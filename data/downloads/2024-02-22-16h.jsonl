{"created":"2024-02-20 18:59:55","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples","abstract":"We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.","sentences":["We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models.","In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning.","Our work pioneers an approach that addresses these gaps.","We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning.","We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.","Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V."],"url":"http://arxiv.org/abs/2402.13254v1","category":"cs.CV"}
{"created":"2024-02-20 18:58:54","title":"Video ReCap: Recursive Captioning of Hour-Long Videos","abstract":"Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap","sentences":["Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions).","However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities.","We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels.","The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently.","We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos.","Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries.","Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema.","Data, code, and models are available at: https://sites.google.com/view/vidrecap"],"url":"http://arxiv.org/abs/2402.13250v1","category":"cs.CV"}
{"created":"2024-02-20 18:58:49","title":"TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization","abstract":"Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.","sentences":["Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations.","We ask whether these advances carry over to other text summarization domains.","We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes.","We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences.","Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size.","On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics.","Finally, we conducted an analysis of hallucination types with a curated error taxonomy.","We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators."],"url":"http://arxiv.org/abs/2402.13249v1","category":"cs.CL"}
{"created":"2024-02-20 18:53:53","title":"Federated Causal Discovery from Heterogeneous Data","abstract":"Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method. The code is available at \\url{https://github.com/lokali/FedCDH.git}.","sentences":["Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations.","This discrepancy has motivated the development of federated causal discovery (FCD) approaches.","However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios.","In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data.","We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients.","We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions.","These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy.","Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models.","We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method.","The code is available at \\url{https://github.com/lokali/FedCDH.git}."],"url":"http://arxiv.org/abs/2402.13241v1","category":"cs.LG"}
{"created":"2024-02-20 18:51:35","title":"Interaction-enhanced nesting in Spin-Fermion and Fermi-Hubbard models","abstract":"The spin-fermion (SF) model postulates that the dominant coupling between low-energy fermions in near critical metals is mediated by collective spin fluctuations (paramagnons) peaked at the N\\'{e}el wave vector, ${\\bf Q}_N$, connecting hot spots on opposite sides of the Fermi surface. It has been argued that strong correlations at hot spots lead to a Fermi surface deformation (FSD) featuring flat regions and increased nesting. This conjecture was confirmed in the perturbative self-consistent calculations when the paramagnon propagator dependence on momentum deviation from ${\\bf Q}_N$ is given by $\\chi^{-1} \\propto |\\Delta q|$. Using diagrammatic Monte Carlo (diagMC) technique we show that such a dependence holds only at temperatures orders of magnitude smaller than any other energy scale in the problem, indicating that a different mechanism may be at play. Instead, we find that a $\\chi^{-1} \\propto |\\Delta q|^{2}$ dependence yields a robust finite-$T$ scenario for achieving FSD. To link phenomenological and microscopic descriptions, we applied the connected determinant diagMC method to the $(t-t')$ Hubbard model and found that in this case: (i) the FSD is not very pronounced, and, instead, it is the lines of zeros of the renormalized dispersion relation that deform towards nesting; (ii) this phenomenon appears at large $U/t>5.5$ before the formation of electron and hole pockets; (iii) the static spin susceptibility is well described by $\\chi^{-1} \\propto |\\Delta q|^{2}$. Flat FS regions yield a non-trivial scenario for realizing a non-Fermi liquid state.","sentences":["The spin-fermion (SF) model postulates that the dominant coupling between low-energy fermions in near critical metals is mediated by collective spin fluctuations (paramagnons) peaked at the N\\'{e}el wave vector, ${\\bf Q}_N$, connecting hot spots on opposite sides of the Fermi surface.","It has been argued that strong correlations at hot spots lead to a Fermi surface deformation (FSD) featuring flat regions and increased nesting.","This conjecture was confirmed in the perturbative self-consistent calculations when the paramagnon propagator dependence on momentum deviation from ${\\bf Q}_N$ is given by $\\chi^{-1} \\propto |\\Delta q|$. Using diagrammatic Monte Carlo (diagMC) technique we show that such a dependence holds only at temperatures orders of magnitude smaller than any other energy scale in the problem, indicating that a different mechanism may be at play.","Instead, we find that a $\\chi^{-1} \\propto |\\Delta q|^{2}$ dependence yields a robust finite-$T$ scenario for achieving FSD.","To link phenomenological and microscopic descriptions, we applied the connected determinant diagMC method to the $(t-t')$ Hubbard model and found that in this case: (i) the FSD is not very pronounced, and, instead, it is the lines of zeros of the renormalized dispersion relation that deform towards nesting; (ii) this phenomenon appears at large $U/t>5.5$ before the formation of electron and hole pockets; (iii) the static spin susceptibility is well described by $\\chi^{-1} \\propto |\\Delta q|^{2}$. Flat FS regions yield a non-trivial scenario for realizing a non-Fermi liquid state."],"url":"http://arxiv.org/abs/2402.13238v1","category":"cond-mat.str-el"}
{"created":"2024-02-20 18:48:49","title":"SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification","abstract":"Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate the negative impacts of domain shifts. Our evaluation on a variety of multi-sensor time series classification tasks shows that SMORE achieves on average 1.98% higher accuracy than state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and 4.63x faster inference.","sentences":["Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors.","However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance.","Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices.","In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing.","SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate the negative impacts of domain shifts.","Our evaluation on a variety of multi-sensor time series classification tasks shows that SMORE achieves on average 1.98% higher accuracy than state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and 4.63x faster inference."],"url":"http://arxiv.org/abs/2402.13233v1","category":"cs.LG"}
{"created":"2024-02-20 18:47:28","title":"Investigating Cultural Alignment of Large Language Models","abstract":"The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.","sentences":["The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology.","Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures?","Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture.","We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references.","Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions.","Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values.","Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment.","Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer."],"url":"http://arxiv.org/abs/2402.13231v1","category":"cs.CL"}
{"created":"2024-02-20 18:42:34","title":"Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive","abstract":"Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \\textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \\textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. By fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly 2\\% better than any other open-source model on the HuggingFace Open LLM Leaderboard and becomes the first open-source LLM to surpass an average accuracy of 80\\%.","sentences":["Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment.","Using pairs of preferred and dispreferred data, DPO models the \\textit{relative} probability of picking one response over another.","In this work, first we show theoretically that the standard DPO loss can lead to a \\textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases.","We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low.","Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode.","Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions.","By fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance.","Notably, Smaug-72B is nearly 2\\% better than any other open-source model on the HuggingFace Open LLM Leaderboard and becomes the first open-source LLM to surpass an average accuracy of 80\\%."],"url":"http://arxiv.org/abs/2402.13228v1","category":"cs.CL"}
{"created":"2024-02-20 18:37:42","title":"NeRF Solves Undersampled MRI Reconstruction","abstract":"This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF). With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled $k$-space data by taking advantage of implicit neural representation. A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image. Effective undersampling strategies for high-quality neural representation are investigated. The proposed method serves two benefits: (i) The learning is based fully on single undersampled $k$-space data, not a bunch of measured data and target image sets. It can be used potentially for diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively rare or limited against diversity of clinical images while undersampled reconstruction is highly demanded. (ii) A reconstructed MR image is a scan-specific representation highly adaptive to the given $k$-space measurement. Numerous experiments validate the feasibility and capability of the proposed approach.","sentences":["This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF).","With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled $k$-space data by taking advantage of implicit neural representation.","A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image.","Effective undersampling strategies for high-quality neural representation are investigated.","The proposed method serves two benefits: (i)","The learning is based fully on single undersampled $k$-space data, not a bunch of measured data and target image sets.","It can be used potentially for diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively rare or limited against diversity of clinical images while undersampled reconstruction is highly demanded.","(ii) A reconstructed MR image is a scan-specific representation highly adaptive to the given $k$-space measurement.","Numerous experiments validate the feasibility and capability of the proposed approach."],"url":"http://arxiv.org/abs/2402.13226v1","category":"eess.IV"}
{"created":"2024-02-20 18:37:19","title":"AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning","abstract":"Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.","sentences":["Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis.","Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality.","Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge.","In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts.","Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs.","Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics.","At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description.","On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy).","Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics.","In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care."],"url":"http://arxiv.org/abs/2402.13225v1","category":"cs.CL"}
{"created":"2024-02-20 18:37:11","title":"Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming","abstract":"This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more significant number of uncertainty scenarios for optimization. The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline. Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics.","sentences":["This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs).","We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming.","The model addresses uncertainties in charging session start and end times, as well as in energy demand.","A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction.","The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset.","The two-stage approach proves robust against early disconnections, considering a more significant number of uncertainty scenarios for optimization.","The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline.","Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics."],"url":"http://arxiv.org/abs/2402.13224v2","category":"math.OC"}
{"created":"2024-02-20 18:33:27","title":"Metallicities and Refined Stellar Parameters for 52 Cool Dwarfs with Transiting Planets and Planet Candidates","abstract":"We collected near-infrared spectra of 65 cool stars with the NASA InfraRed Telescope Facility (IRTF) and analyze them to calculate accurate metallicities and stellar parameters. The sample of 55 M dwarfs and 10 K dwarfs includes 25 systems with confirmed planets and 27 systems with planet candidates identified by the K2 and TESS missions. Three of the 25 confirmed planetary systems host multiple confirmed planets and two of the 27 planet candidate systems host multiple planet candidates. Using the new stellar parameters, we re-fit the K2 and TESS light curves to calculate updated planet properties. In general, our updated stellar properties are more precise than those previously reported and our updated planet properties agree well with those in the literature. Lastly, we briefly examine the relationship between stellar mass, stellar metallicity, and planetary system properties for targets in our sample and for previously characterized planet-hosting low-mass stars. We provide our spectra, stellar parameters, and new planetary fits to the community, expanding the sample available with which to investigate correlations between stellar and planetary properties for low-mass stars.","sentences":["We collected near-infrared spectra of 65 cool stars with the NASA InfraRed Telescope Facility (IRTF) and analyze them to calculate accurate metallicities and stellar parameters.","The sample of 55 M dwarfs and 10 K dwarfs includes 25 systems with confirmed planets and 27 systems with planet candidates identified by the K2 and TESS missions.","Three of the 25 confirmed planetary systems host multiple confirmed planets and two of the 27 planet candidate systems host multiple planet candidates.","Using the new stellar parameters, we re-fit the K2 and TESS light curves to calculate updated planet properties.","In general, our updated stellar properties are more precise than those previously reported and our updated planet properties agree well with those in the literature.","Lastly, we briefly examine the relationship between stellar mass, stellar metallicity, and planetary system properties for targets in our sample and for previously characterized planet-hosting low-mass stars.","We provide our spectra, stellar parameters, and new planetary fits to the community, expanding the sample available with which to investigate correlations between stellar and planetary properties for low-mass stars."],"url":"http://arxiv.org/abs/2402.13223v1","category":"astro-ph.EP"}
{"created":"2024-02-20 18:32:47","title":"RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian","abstract":"Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.","sentences":["Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language.","However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language.","Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs.","Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English.","In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem.","The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models.","Through our results and review of related works, we argue for the need to develop code models for languages other than English."],"url":"http://arxiv.org/abs/2402.13222v1","category":"cs.CL"}
{"created":"2024-02-20 18:31:27","title":"Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies","abstract":"In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires. The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered. Additionally, the results provide valuable insights to compare differences between styles of information gathering when using the system by individual participants. These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time. These predictions enable the development of more effective intervention strategies.","sentences":["In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency.","The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep reinforcement learning.","The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance.","Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training.","A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires.","The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered.","Additionally, the results provide valuable insights to compare differences between styles of information gathering when using the system by individual participants.","These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time.","These predictions enable the development of more effective intervention strategies."],"url":"http://arxiv.org/abs/2402.13219v1","category":"cs.AI"}
{"created":"2024-02-20 18:29:49","title":"VideoPrism: A Foundational Visual Encoder for Video Understanding","abstract":"We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.","sentences":["We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model.","We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts).","The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos.","We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks."],"url":"http://arxiv.org/abs/2402.13217v1","category":"cs.CV"}
{"created":"2024-02-20 18:24:47","title":"Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A","abstract":"Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task. For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results.","sentences":["Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem.","We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers.","We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task.","For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances.","Among those six LLMs, the average AUROC ranged from 60% to 69%.","Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response.","We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results."],"url":"http://arxiv.org/abs/2402.13213v1","category":"cs.CL"}
{"created":"2024-02-20 18:22:38","title":"Soft Self-Consistency Improves Language Model Agents","abstract":"Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and black-box models.","sentences":["Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer.","Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers.","However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples.","This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially.","After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion.","We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed.","Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance.","For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld).","Finally, we show that Soft-SC can be applied to both open-source and black-box models."],"url":"http://arxiv.org/abs/2402.13212v1","category":"cs.CL"}
{"created":"2024-02-20 18:21:32","title":"Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation","abstract":"Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.","sentences":["Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation.","Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses.","Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support.","Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy.","Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy.","Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters.","Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) LLMs alone cannot become good emotional supporters.","These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs."],"url":"http://arxiv.org/abs/2402.13211v1","category":"cs.CL"}
{"created":"2024-02-20 18:19:08","title":"How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena","abstract":"The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant.","sentences":["The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity.","Consequently, research efforts in the last few years focused on finding more efficient alternatives.","Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity.","Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs.","Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant."],"url":"http://arxiv.org/abs/2402.13208v1","category":"cs.CL"}
{"created":"2024-02-20 18:15:11","title":"SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search","abstract":"Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency. HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms. However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly. Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front. Addressing these issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS. Our method leverages adaptive evolutionary operators guided by the learned importance of NN design parameters. Specifically, through tree-based surrogate models and a Reinforcement Learning agent, we aspire to gather knowledge on 'How' and 'When' to evolve NN architectures. Comprehensive evaluations across various NAS search spaces and hardware devices on the ImageNet-1k dataset have shown the merit of SONATA with up to 0.25% improvement in accuracy and up to 2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto dominance over the native NSGA-II, further stipulating the importance of self-adaptive evolution operators in HW-aware NAS.","sentences":["Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency.","HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms.","However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly.","Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front.","Addressing these issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS.","Our method leverages adaptive evolutionary operators guided by the learned importance of NN design parameters.","Specifically, through tree-based surrogate models and a Reinforcement Learning agent, we aspire to gather knowledge on 'How' and 'When' to evolve NN architectures.","Comprehensive evaluations across various NAS search spaces and hardware devices on the ImageNet-1k dataset have shown the merit of SONATA with up to 0.25% improvement in accuracy and up to 2.42x gains in latency and energy.","Our SONATA has seen up to sim$93.6","%","Pareto dominance over the native NSGA-II, further stipulating the importance of self-adaptive evolution operators in HW-aware NAS."],"url":"http://arxiv.org/abs/2402.13204v1","category":"cs.NE"}
{"created":"2024-02-20 18:10:39","title":"Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers","abstract":"Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in simulation using Isaac Gym, a realistic physics simulation environment designed for reinforcement learning. We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot. We also run multiple simulations to show the effects of pruning and quantization on the performance of the model. Our results show that quantization (down to 4 bits) and pruning reduce model size by around 30\\% while maintaining a competitive reward, making the model deployable in a resource-constrained system.","sentences":["Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics.","For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms.","We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms.","Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward.","Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning.","We test our method in simulation using Isaac Gym, a realistic physics simulation environment designed for reinforcement learning.","We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot.","We also run multiple simulations to show the effects of pruning and quantization on the performance of the model.","Our results show that quantization (down to 4 bits) and pruning reduce model size by around 30\\% while maintaining a competitive reward, making the model deployable in a resource-constrained system."],"url":"http://arxiv.org/abs/2402.13201v1","category":"cs.RO"}
{"created":"2024-02-20 18:06:00","title":"Design and Flight Demonstration of a Quadrotor for Urban Mapping and Target Tracking Research","abstract":"This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research. The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection. An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate. The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots. A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm. The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests.","sentences":["This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research.","The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera.","An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection.","An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate.","The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots.","A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm.","The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests."],"url":"http://arxiv.org/abs/2402.13195v1","category":"cs.RO"}
{"created":"2024-02-20 18:00:13","title":"Integrating Blockchain technology within an Information Ecosystem","abstract":"Context: Blockchain-based Information Ecosystems (BBIEs) are a type of information ecosystem in which blockchain technology is used to provide a trust mechanism among parties and to manage shared business logic, breaking the traditional scheme of Information Ecosystems dominated by a leading company and leveraging the decentralization of data management, information flow, and business logic. Objective: In this paper, we propose architecture and technical aspects concerning the creation of a BBIE, underlining the advantages supplied and the logic decomposition among the business and storage components. Method: The requirements are derived from the current needs of the collaborative business and the data collected by surveying practitioners. To get these needs we followed the Grounded Theory research approach. We validate our architectural schema against a case study dealing with the management of a wine supply chain, also involving different companies and supervision authorities. Results: The proposed solution integrates blockchain-based applications with the existing information system as a module of the ecosystem, leveraging on the low costs, scalability, and high-level security because of the restricted access to the network. Conclusion: We must go a long way in deepening and refining the possibilities offered by technology in supporting innovative multi-organizational business models. BBIEs can contribute substantially to paving the way in such a direction.","sentences":["Context: Blockchain-based Information Ecosystems (BBIEs) are a type of information ecosystem in which blockchain technology is used to provide a trust mechanism among parties and to manage shared business logic, breaking the traditional scheme of Information Ecosystems dominated by a leading company and leveraging the decentralization of data management, information flow, and business logic.","Objective:","In this paper, we propose architecture and technical aspects concerning the creation of a BBIE, underlining the advantages supplied and the logic decomposition among the business and storage components.","Method: The requirements are derived from the current needs of the collaborative business and the data collected by surveying practitioners.","To get these needs we followed the Grounded Theory research approach.","We validate our architectural schema against a case study dealing with the management of a wine supply chain, also involving different companies and supervision authorities.","Results:","The proposed solution integrates blockchain-based applications with the existing information system as a module of the ecosystem, leveraging on the low costs, scalability, and high-level security because of the restricted access to the network.","Conclusion: We must go a long way in deepening and refining the possibilities offered by technology in supporting innovative multi-organizational business models.","BBIEs can contribute substantially to paving the way in such a direction."],"url":"http://arxiv.org/abs/2402.13191v1","category":"cs.DC"}
{"created":"2024-02-20 17:49:46","title":"What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents","abstract":"In this study, we introduce \"CosmoAgent,\" an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.","sentences":["In this study, we introduce \"CosmoAgent,\" an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe.","The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations.","Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation.","Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations.","Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles.","This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts.","We have also released the code and datasets to enable further academic investigation into this interesting area of research.","The code is available at https://github.com/agiresearch/AlienAgent."],"url":"http://arxiv.org/abs/2402.13184v2","category":"cs.CL"}
{"created":"2024-02-20 17:44:06","title":"Benchmarking Retrieval-Augmented Generation for Medicine","abstract":"While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.","sentences":["While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge.","Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted.","However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes.","To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets.","Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work.","Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level.","Our results show that the combination of various medical corpora and retrievers achieves the best performance.","In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG.","We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine."],"url":"http://arxiv.org/abs/2402.13178v1","category":"cs.CL"}
{"created":"2024-02-20 17:07:08","title":"AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies","abstract":"More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.","sentences":["More than 7,000 known languages are spoken around the world.","However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies.","Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English.","This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed.","In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription.","In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task.","The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub."],"url":"http://arxiv.org/abs/2402.13152v1","category":"cs.CV"}
{"created":"2024-02-20 17:02:48","title":"SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations","abstract":"We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that the imitation policy is trained towards sub-optimal policies. In this paper, to address these issues, we propose a new approach based on inverse soft-Q learning, where a regularization term is added to the training objective, with the aim of aligning the learned rewards with a pre-assigned reward function that allocates higher weights to state-action pairs from expert demonstrations, and lower weights to those from lower expertise levels. On standard benchmarks, our inverse soft-Q learning significantly outperforms other offline IL baselines by a large margin.","sentences":["We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment.","One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces.","In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels.","Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy.","Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution.","On the other hand, since sub-optimal sets are much larger, there is a high chance that the imitation policy is trained towards sub-optimal policies.","In this paper, to address these issues, we propose a new approach based on inverse soft-Q learning, where a regularization term is added to the training objective, with the aim of aligning the learned rewards with a pre-assigned reward function that allocates higher weights to state-action pairs from expert demonstrations, and lower weights to those from lower expertise levels.","On standard benchmarks, our inverse soft-Q learning significantly outperforms other offline IL baselines by a large margin."],"url":"http://arxiv.org/abs/2402.13147v1","category":"cs.LG"}
{"created":"2024-02-20 17:00:41","title":"CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation","abstract":"Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition. We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus. These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research. The code is available in https://github.com/JasonShao55/Chinese_Metaphor_Explanation.","sentences":["Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication.","This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc.","To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines.","These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles.","Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles.","By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition.","We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus.","These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research.","The code is available in https://github.com/JasonShao55/Chinese_Metaphor_Explanation."],"url":"http://arxiv.org/abs/2402.13145v2","category":"cs.CL"}
{"created":"2024-02-20 16:46:06","title":"Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks","abstract":"Reconfigurable intelligent surface (RIS) used as infrastructure in wireless networks has been a trend, thanks to its low cost and high flexibility. Working in many ways including reflective mirrors and phase-shifted surfaces, RIS is able to enhance the coverage in communications and provide more degrees of freedom for sensing. However, the key issue lies in how to place RIS in accordance with the regulations for electromagnetic field (EMF) exposure, which requires refined evaluations. In this paper, we first investigate the regulations in terms of E-field. Then, relevant deployment characteristics are evaluated jointly: the minimum distance from the base station (BS) to the RIS, and the minimum height of the RIS are given for a given BS power limit and as function of the number of RIS elements. The ray-tracing simulations verify the correctness of our analysis. Besides, different frequency ranges (FRs) and radiation patterns of RIS elements are investigated. The results show that the EMF exposure risk is negligible when RIS works in the reflective-only (RO) mode. However, when it works in the beamforming (BO) mode, its placement should be well specified based on our analytical framework to comply with the regulations of E-field limit in general public scenarios. Finally, we provide an E-field measurement methodology and low-cost solutions in terms of general wireless networks and 5G standalone networks, which pave the way for real-world evaluation in future work.","sentences":["Reconfigurable intelligent surface (RIS) used as infrastructure in wireless networks has been a trend, thanks to its low cost and high flexibility.","Working in many ways including reflective mirrors and phase-shifted surfaces, RIS is able to enhance the coverage in communications and provide more degrees of freedom for sensing.","However, the key issue lies in how to place RIS in accordance with the regulations for electromagnetic field (EMF) exposure, which requires refined evaluations.","In this paper, we first investigate the regulations in terms of E-field.","Then, relevant deployment characteristics are evaluated jointly: the minimum distance from the base station (BS) to the RIS, and the minimum height of the RIS are given for a given BS power limit and as function of the number of RIS elements.","The ray-tracing simulations verify the correctness of our analysis.","Besides, different frequency ranges (FRs) and radiation patterns of RIS elements are investigated.","The results show that the EMF exposure risk is negligible when RIS works in the reflective-only (RO) mode.","However, when it works in the beamforming (BO) mode, its placement should be well specified based on our analytical framework to comply with the regulations of E-field limit in general public scenarios.","Finally, we provide an E-field measurement methodology and low-cost solutions in terms of general wireless networks and 5G standalone networks, which pave the way for real-world evaluation in future work."],"url":"http://arxiv.org/abs/2402.13132v1","category":"eess.SY"}
{"created":"2024-02-20 16:39:23","title":"VGMShield: Mitigating Misuse of Video Generative Models","abstract":"With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \\textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \\textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy.   Furthermore, anticipating future generative model improvements, we propose a {\\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal. Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.","sentences":["With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires.","Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   ","In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation.","We start from \\textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \\textit{tracing} problem, which maps a fake video back to a model that generates it.","Towards these, we propose to leverage pre-trained models that focus on {\\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos.","Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy.   ","Furthermore, anticipating future generative model improvements, we propose a {\\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal.","Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models."],"url":"http://arxiv.org/abs/2402.13126v1","category":"cs.CR"}
{"created":"2024-02-20 16:38:33","title":"TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning","abstract":"Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided https://github.com/Ashura5/TreeEval.","sentences":["Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge.","However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process.","To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage.","Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process.","We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions.","We also conduct more analysis to show the robustness and reliability of TreeEval.","Our code can be accessed via the provided https://github.com/Ashura5/TreeEval."],"url":"http://arxiv.org/abs/2402.13125v1","category":"cs.CL"}
{"created":"2024-02-20 16:11:59","title":"BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes","abstract":"Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs). To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced. However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings. Code is available at https://anonymous.4open.science/r/BuffGraph-730A.","sentences":["Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs).","To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced.","However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes.","To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation.","Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings.","Code is available at https://anonymous.4open.science/r/BuffGraph-730A."],"url":"http://arxiv.org/abs/2402.13114v1","category":"cs.LG"}
{"created":"2024-02-20 16:02:12","title":"CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models","abstract":"The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts. This work aims to uncover the current limitations of LLMs in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and benchmark (https://yizhilll.github.io/CIF-Bench/).","sentences":["The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following.","Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories.","In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language.","CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories.","To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances.","Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts.","This work aims to uncover the current limitations of LLMs in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and benchmark (https://yizhilll.github.io/CIF-Bench/)."],"url":"http://arxiv.org/abs/2402.13109v1","category":"cs.CL"}
{"created":"2024-02-20 15:47:59","title":"ELAD: Explanation-Guided Large Language Models Active Distillation","abstract":"The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning. Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation.","sentences":["The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences.","Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation.","In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance.","To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps.","Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning.","Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation."],"url":"http://arxiv.org/abs/2402.13098v1","category":"cs.CL"}
{"created":"2024-02-20 15:36:41","title":"Event-level Knowledge Editing","abstract":"Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset are publicly released to facilitate further research.","sentences":["Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated.","Existing work edits LLMs at the level of factual knowledge triplets.","However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets.","In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency.","A single event edit leads to updates in multiple entailed knowledge triplets.","(2) Completeness.","Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends.","We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies.","We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark.","We find that ELKEN poses significant challenges to existing knowledge editing approaches.","Our codes and dataset are publicly released to facilitate further research."],"url":"http://arxiv.org/abs/2402.13093v1","category":"cs.CL"}
{"created":"2024-02-20 15:31:44","title":"Towards an empirical understanding of MoE design choices","abstract":"In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.","sentences":["In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels.","We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential.","Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing."],"url":"http://arxiv.org/abs/2402.13089v1","category":"cs.LG"}
{"created":"2024-02-20 15:23:24","title":"Mechanistic Neural Networks for Scientific Machine Learning","abstract":"This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences. It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scientific data across various applications, showing significant performance against specialized state-of-the-art methods.","sentences":["This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences.","It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling.","Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs.","This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing.","Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling.","We prove their comprehensive capabilities in analyzing and interpreting complex scientific data across various applications, showing significant performance against specialized state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.13077v1","category":"cs.LG"}
{"created":"2024-02-20 15:20:32","title":"Josephson Signatures of the Superconducting Higgs/Amplitude Mode","abstract":"The Higgs/amplitude collective mode in superconductors corresponds to oscillations of the amplitude of the order parameter. While its detection typically relies on optical techniques with an external electromagnetic field resonant with the Higgs mode, we present a purely transport-based setup wherein it is excited in a voltage biased Josephson junction. Demonstrating the importance of the order parameter dynamics, we find that in highly transparent junctions featuring single-band s-wave superconductors, the interplay of the Higgs resonance and the Josephson physics enhances the second harmonic Josephson current oscillating at twice the Josephson frequency. If the leads have unequal equilibrium superconducting gaps, this second harmonic component may even eclipse its usual first harmonic counterpart, thus furnishing a unique hallmark of the Higgs oscillations.","sentences":["The Higgs/amplitude collective mode in superconductors corresponds to oscillations of the amplitude of the order parameter.","While its detection typically relies on optical techniques with an external electromagnetic field resonant with the Higgs mode, we present a purely transport-based setup wherein it is excited in a voltage biased Josephson junction.","Demonstrating the importance of the order parameter dynamics, we find that in highly transparent junctions featuring single-band s-wave superconductors, the interplay of the Higgs resonance and the Josephson physics enhances the second harmonic Josephson current oscillating at twice the Josephson frequency.","If the leads have unequal equilibrium superconducting gaps, this second harmonic component may even eclipse its usual first harmonic counterpart, thus furnishing a unique hallmark of the Higgs oscillations."],"url":"http://arxiv.org/abs/2402.13074v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 15:19:52","title":"Towards Intelligent Communications: Large Model Empowered Semantic Communications","abstract":"Deep learning enabled semantic communications have shown great potential to significantly improve transmission efficiency and alleviate spectrum scarcity, by effectively exchanging the semantics behind the data. Recently, the emergence of large models, boasting billions of parameters, has unveiled remarkable human-like intelligence, offering a promising avenue for advancing semantic communication by enhancing semantic understanding and contextual understanding. This article systematically investigates the large model-empowered semantic communication systems from potential applications to system design. First, we propose a new semantic communication architecture that seamlessly integrates large models into semantic communication through the introduction of a memory module. Then, the typical applications are illustrated to show the benefits of the new architecture. Besides, we discuss the key designs in implementing the new semantic communication systems from module design to system training. Finally, the potential research directions are identified to boost the large model-empowered semantic communications.","sentences":["Deep learning enabled semantic communications have shown great potential to significantly improve transmission efficiency and alleviate spectrum scarcity, by effectively exchanging the semantics behind the data.","Recently, the emergence of large models, boasting billions of parameters, has unveiled remarkable human-like intelligence, offering a promising avenue for advancing semantic communication by enhancing semantic understanding and contextual understanding.","This article systematically investigates the large model-empowered semantic communication systems from potential applications to system design.","First, we propose a new semantic communication architecture that seamlessly integrates large models into semantic communication through the introduction of a memory module.","Then, the typical applications are illustrated to show the benefits of the new architecture.","Besides, we discuss the key designs in implementing the new semantic communication systems from module design to system training.","Finally, the potential research directions are identified to boost the large model-empowered semantic communications."],"url":"http://arxiv.org/abs/2402.13073v1","category":"eess.SP"}
{"created":"2024-02-20 14:56:58","title":"3D high-resolution imaging algorithm using 1D MIMO array for autonomous driving application","abstract":"The problem of 3D high-resolution imaging in automotive multiple-input multiple-output (MIMO) side-looking radar using a 1D array is considered. The concept of motion-enhanced snapshots is introduced for generating larger apertures in the azimuth dimension. For the first time, 3D imaging capabilities can be achieved with high angular resolution using a 1D MIMO antenna array, which can alleviate the requirement for large radar systems in autonomous vehicles. The robustness to variations in the vehicle's movement trajectory is also considered and addressed with relevant compensations in the steering vector. The available degrees of freedom as well as the Signal to Noise Ratio (SNR) are shown to increase with the proposed method compared to conventional imaging approaches. The performance of the algorithm has been studied in simulations, and validated with experimental data collected in a realistic driving scenario.","sentences":["The problem of 3D high-resolution imaging in automotive multiple-input multiple-output (MIMO) side-looking radar using a 1D array is considered.","The concept of motion-enhanced snapshots is introduced for generating larger apertures in the azimuth dimension.","For the first time, 3D imaging capabilities can be achieved with high angular resolution using a 1D MIMO antenna array, which can alleviate the requirement for large radar systems in autonomous vehicles.","The robustness to variations in the vehicle's movement trajectory is also considered and addressed with relevant compensations in the steering vector.","The available degrees of freedom as well as the Signal to Noise Ratio (SNR) are shown to increase with the proposed method compared to conventional imaging approaches.","The performance of the algorithm has been studied in simulations, and validated with experimental data collected in a realistic driving scenario."],"url":"http://arxiv.org/abs/2402.13062v1","category":"eess.SP"}
{"created":"2024-02-20 14:52:52","title":"Random Graph Set and Evidence Pattern Reasoning Model","abstract":"Evidence theory is widely used in decision-making and reasoning systems. In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model. In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed. By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks. Random Permutation Set (RPS) expands order information for evidence theory. It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types. In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated. The implementation of EPRM called Conflict Resolution Decision optimized 18.17\\% of the cases compared to Mean Velocity Decision, effectively improving the aircraft velocity ranking. EPRM provides a unified solution for evidence-based decision making.","sentences":["Evidence theory is widely used in decision-making and reasoning systems.","In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model.","In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed.","By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks.","Random Permutation Set (RPS) expands order information for evidence theory.","It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships.","Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types.","In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated.","The implementation of EPRM called Conflict Resolution Decision optimized 18.17\\% of the cases compared to Mean Velocity Decision, effectively improving the aircraft velocity ranking.","EPRM provides a unified solution for evidence-based decision making."],"url":"http://arxiv.org/abs/2402.13058v1","category":"cs.AI"}
{"created":"2024-02-20 14:51:02","title":"Edge Computing for IoT","abstract":"Over the past few years, The idea of edge computing has seen substantial expansion in both academic and industrial circles. This computing approach has garnered attention due to its integrating role in advancing various state-of-the-art technologies such as Internet of Things (IoT) , 5G, artificial intelligence, and augmented reality. In this chapter, we introduce computing paradigms for IoT, offering an overview of the current cutting-edge computing approaches that can be used with IoT. Furthermore, we go deeper into edge computing paradigms, specifically focusing on cloudlet and mobile edge computing. After that, we investigate the architecture of edge computing-based IoT, its advantages, and the technologies that make Edge computing-based IoT possible, including artificial intelligence and lightweight virtualization. Additionally, we review real-life case studies of how edge computing is applied in IoT-based Intelligent Systems, including areas like healthcare, manufacturing, agriculture, and transportation. Finally, we discuss current research obstacles and outline potential future directions for further investigation in this domain.","sentences":["Over the past few years, The idea of edge computing has seen substantial expansion in both academic and industrial circles.","This computing approach has garnered attention due to its integrating role in advancing various state-of-the-art technologies such as Internet of Things (IoT) , 5G, artificial intelligence, and augmented reality.","In this chapter, we introduce computing paradigms for IoT, offering an overview of the current cutting-edge computing approaches that can be used with IoT.","Furthermore, we go deeper into edge computing paradigms, specifically focusing on cloudlet and mobile edge computing.","After that, we investigate the architecture of edge computing-based IoT, its advantages, and the technologies that make Edge computing-based IoT possible, including artificial intelligence and lightweight virtualization.","Additionally, we review real-life case studies of how edge computing is applied in IoT-based Intelligent Systems, including areas like healthcare, manufacturing, agriculture, and transportation.","Finally, we discuss current research obstacles and outline potential future directions for further investigation in this domain."],"url":"http://arxiv.org/abs/2402.13056v1","category":"cs.NI"}
{"created":"2024-02-20 14:43:39","title":"Identifying Semantic Induction Heads to Understand In-Context Learning","abstract":"Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.","sentences":["Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness.","To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs.","Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs.","We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens.","More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models.","The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs."],"url":"http://arxiv.org/abs/2402.13055v1","category":"cs.CL"}
{"created":"2024-02-20 14:39:35","title":"Measurements of Lund subjet multiplicities in 13 TeV proton-proton collisions with the ATLAS detector","abstract":"This Letter presents a differential cross-section measurement of Lund subjet multiplicities, suitable for testing current and future parton shower Monte Carlo algorithms. This measurement is made in dijet events in 140 fb$^{-1}$ of $\\sqrt{s}=13$ TeV proton-proton collision data collected with the ATLAS detector at CERN's Large Hadron Collider. The data are unfolded to account for acceptance and detector-related effects, and are then compared with several Monte Carlo models and to recent resummed analytical calculations. The experimental precision achieved in the measurement allows tests of higher-order effects in QCD predictions. Most predictions fail to accurately describe the measured data, particularly at large values of jet transverse momentum accessible at the Large Hadron Collider, indicating the measurement's utility as an input to future parton shower developments and other studies probing fundamental properties of QCD and the production of hadronic final states up to the TeV-scale.","sentences":["This Letter presents a differential cross-section measurement of Lund subjet multiplicities, suitable for testing current and future parton shower Monte Carlo algorithms.","This measurement is made in dijet events in 140 fb$^{-1}$ of $\\sqrt{s}=13$ TeV proton-proton collision data collected with the ATLAS detector at CERN's Large Hadron Collider.","The data are unfolded to account for acceptance and detector-related effects, and are then compared with several Monte Carlo models and to recent resummed analytical calculations.","The experimental precision achieved in the measurement allows tests of higher-order effects in QCD predictions.","Most predictions fail to accurately describe the measured data, particularly at large values of jet transverse momentum accessible at the Large Hadron Collider, indicating the measurement's utility as an input to future parton shower developments and other studies probing fundamental properties of QCD and the production of hadronic final states up to the TeV-scale."],"url":"http://arxiv.org/abs/2402.13052v1","category":"hep-ex"}
{"created":"2024-02-20 14:29:02","title":"Text-Guided Molecule Generation with Diffusion Language Model","abstract":"Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.","sentences":["Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions.","Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture.","In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods.","TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process.","The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations.","We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources.","Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains.","Code will be released at: https://github.com/Deno-V/tgm-dlm."],"url":"http://arxiv.org/abs/2402.13040v1","category":"cs.LG"}
{"created":"2024-02-20 14:24:00","title":"Align Your Intents: Offline Imitation Learning via Optimal Transport","abstract":"Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms in the sparse-reward tasks.","sentences":["Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment.","As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively.","Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels.","In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data.","Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories.","We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms in the sparse-reward tasks."],"url":"http://arxiv.org/abs/2402.13037v1","category":"cs.LG"}
{"created":"2024-02-20 14:23:23","title":"Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models","abstract":"Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction. We compare the performance of models fine-tuned with the ``Step CoT Check'' prompt against those refined using other promps within the context of checking-correction data. The ``Step CoT Check'' outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness. For reproducibility, all the datasets and codes are provided in \\url{https://github.com/bammt/Learn-to-check}.","sentences":["Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction.","However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction.","In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction.","We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''.","Then we construct a checking-correction dataset for training models.","After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction.","We compare the performance of models fine-tuned with the ``Step CoT Check'' prompt against those refined using other promps within the context of checking-correction data.","The ``Step CoT Check'' outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness.","For reproducibility, all the datasets and codes are provided in \\url{https://github.com/bammt/Learn-to-check}."],"url":"http://arxiv.org/abs/2402.13035v1","category":"cs.CL"}
{"created":"2024-02-20 14:23:06","title":"Ray Tracing Algorithm for Reconfigurable Intelligent Surfaces","abstract":"Ray tracing accelerated with graphics processing units (GPUs) is an accurate and efficient simulation technique of wireless communication channels. In this paper, we extend a GPU-accelerated ray tracer (RT) to support the effects of reconfigurable intelligent surfaces (RISs). To evaluate the electric field, we derived a RIS path loss model that can be integrated into a RT and enables further extensions for the implementation of additional features and incorporation into complex reflective scenarios. We verify the derivation and implementation of our model by comparison with empirical measurements in a lab environment. We demonstrate the capabilities of our model to support higher-order reflections from the RIS to the receiver. We find that such components have a significant effect on the received signal strength, concluding that the extensions of advanced functionality enabled by our model play an important role in the accurate modeling of radio wave propagation in an environment including RISs.","sentences":["Ray tracing accelerated with graphics processing units (GPUs) is an accurate and efficient simulation technique of wireless communication channels.","In this paper, we extend a GPU-accelerated ray tracer (RT) to support the effects of reconfigurable intelligent surfaces (RISs).","To evaluate the electric field, we derived a RIS path loss model that can be integrated into a RT and enables further extensions for the implementation of additional features and incorporation into complex reflective scenarios.","We verify the derivation and implementation of our model by comparison with empirical measurements in a lab environment.","We demonstrate the capabilities of our model to support higher-order reflections from the RIS to the receiver.","We find that such components have a significant effect on the received signal strength, concluding that the extensions of advanced functionality enabled by our model play an important role in the accurate modeling of radio wave propagation in an environment including RISs."],"url":"http://arxiv.org/abs/2402.13034v1","category":"eess.SP"}
{"created":"2024-02-20 14:10:40","title":"Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables","abstract":"Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence. An attention-based method is utilized to integrate information, combined with a language model for generating predictions. We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval. Comprehensive experiments on the large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC. Code will be released at: https://github.com/Deno-V/HeterFC.","sentences":["Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces.","It usually involves evidence retrieval and veracity reasoning.","In this paper, we focus on the latter, reasoning over unstructured text and structured table information.","Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models.","Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures.","To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC.","Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties.","We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence.","An attention-based method is utilized to integrate information, combined with a language model for generating predictions.","We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval.","Comprehensive experiments on the large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC.","Code will be released at: https://github.com/Deno-V/HeterFC."],"url":"http://arxiv.org/abs/2402.13028v1","category":"cs.CL"}
{"created":"2024-02-20 14:09:28","title":"Solving the decision-making analysis differential equation using eye fixation data in Unity software with Hermite Long-Short-Term Memory","abstract":"Decision-making is a fundamental component of our personal and professional lives. To analyze decision-making accuracy, this study proposes a virtual environment designed as an industrial town to investigate the relationship between eye movements and decision-making. Eye tracking provides a tool to examine eye movements, which contain information related to eye position, head position, and gaze direction. The game is designed using Unity software, with the collected data being analyzed using a differential equation and the Hermite neural network method. The game is used to identify the behaviors exhibited by bad and good individuals and differentiate between them before taking action. This paper investigates the accuracy of an individual's decision-making process by analyzing their eye movements and the correctness of the decisions made.","sentences":["Decision-making is a fundamental component of our personal and professional lives.","To analyze decision-making accuracy, this study proposes a virtual environment designed as an industrial town to investigate the relationship between eye movements and decision-making.","Eye tracking provides a tool to examine eye movements, which contain information related to eye position, head position, and gaze direction.","The game is designed using Unity software, with the collected data being analyzed using a differential equation and the Hermite neural network method.","The game is used to identify the behaviors exhibited by bad and good individuals and differentiate between them before taking action.","This paper investigates the accuracy of an individual's decision-making process by analyzing their eye movements and the correctness of the decisions made."],"url":"http://arxiv.org/abs/2402.13027v1","category":"cs.HC"}
{"created":"2024-02-20 14:08:24","title":"CFEVER: A Chinese Fact Extraction and VERification Dataset","abstract":"We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as \"Supports\", \"Refutes\", or \"Not Enough Info\" to depict its degree of factualness. Similar to the FEVER dataset, claims in the \"Supports\" and \"Refutes\" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts. CFEVER is available at https://ikmlab.github.io/CFEVER.","sentences":["We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification.","CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia.","Each claim in CFEVER is labeled as \"Supports\", \"Refutes\", or \"Not Enough Info\" to depict its degree of factualness.","Similar to the FEVER dataset, claims in the \"Supports\" and \"Refutes\" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia.","Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement.","In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts.","CFEVER is available at https://ikmlab.github.io/CFEVER."],"url":"http://arxiv.org/abs/2402.13025v1","category":"cs.CL"}
{"created":"2024-02-20 14:07:18","title":"SmartEx: A Framework for Generating User-Centric Explanations in Smart Environments","abstract":"Explainability is crucial for complex systems like pervasive smart environments, as they collect and analyze data from various sensors, follow multiple rules, and control different devices resulting in behavior that is not trivial and, thus, should be explained to the users. The current approaches, however, offer flat, static, and algorithm-focused explanations. User-centric explanations, on the other hand, consider the recipient and context, providing personalized and context-aware explanations. To address this gap, we propose an approach to incorporate user-centric explanations into smart environments. We introduce a conceptual model and a reference architecture for characterizing and generating such explanations. Our work is the first technical solution for generating context-aware and granular explanations in smart environments. Our architecture implementation demonstrates the feasibility of our approach through various scenarios.","sentences":["Explainability is crucial for complex systems like pervasive smart environments, as they collect and analyze data from various sensors, follow multiple rules, and control different devices resulting in behavior that is not trivial and, thus, should be explained to the users.","The current approaches, however, offer flat, static, and algorithm-focused explanations.","User-centric explanations, on the other hand, consider the recipient and context, providing personalized and context-aware explanations.","To address this gap, we propose an approach to incorporate user-centric explanations into smart environments.","We introduce a conceptual model and a reference architecture for characterizing and generating such explanations.","Our work is the first technical solution for generating context-aware and granular explanations in smart environments.","Our architecture implementation demonstrates the feasibility of our approach through various scenarios."],"url":"http://arxiv.org/abs/2402.13024v1","category":"cs.HC"}
{"created":"2024-02-20 14:01:26","title":"Improving Neural-based Classification with Logical Background Knowledge","abstract":"Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs.","sentences":["Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems.","This hybridization can take many shapes.","In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge.","We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected.","We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization.","We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network.","We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets.","Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs."],"url":"http://arxiv.org/abs/2402.13019v1","category":"cs.AI"}
{"created":"2024-02-20 13:51:27","title":"An evolutionary game with reputation-based imitation-mutation dynamics","abstract":"Reputation plays a crucial role in social interactions by affecting the fitness of individuals during an evolutionary process. Previous works have extensively studied the result of imitation dynamics without focusing on potential irrational choices in strategy updates. We now fill this gap and explore the consequence of such kind of randomness, or one may interpret it as an autonomous thinking. In particular, we study how this extended dynamics alters the evolution of cooperation when individual reputation is directly linked to collected payoff, hence providing a general fitness function. For a broadly valid conclusion, our spatial populations cover different types of interaction topologies, including lattices, small-world and scale-free graphs. By means of intensive simulations we can detect substantial increase in cooperation level that shows a reasonable stability in the presence of a notable strategy mutation.","sentences":["Reputation plays a crucial role in social interactions by affecting the fitness of individuals during an evolutionary process.","Previous works have extensively studied the result of imitation dynamics without focusing on potential irrational choices in strategy updates.","We now fill this gap and explore the consequence of such kind of randomness, or one may interpret it as an autonomous thinking.","In particular, we study how this extended dynamics alters the evolution of cooperation when individual reputation is directly linked to collected payoff, hence providing a general fitness function.","For a broadly valid conclusion, our spatial populations cover different types of interaction topologies, including lattices, small-world and scale-free graphs.","By means of intensive simulations we can detect substantial increase in cooperation level that shows a reasonable stability in the presence of a notable strategy mutation."],"url":"http://arxiv.org/abs/2402.13011v1","category":"physics.soc-ph"}
{"created":"2024-02-20 13:38:04","title":"SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms","abstract":"The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning benchmark based on public datasets converted to a standardized format. This benchmark defines the machine-learning task as well as reporting metrics. We illustrate the use of the benchmark by evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure Community Open-source Research Evaluation) framework and benchmark are made publicly available along with an open-source software library to facilitate research use, while enabling rigorous evaluation of the clinical significance of the algorithms, fostering a collective effort to more optimally detect seizures to improve the lives of people with epilepsy.","sentences":["The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring.","Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging.","This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics.","In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms.","Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics.","We also propose the 10-20 seizure detection benchmark, a machine-learning benchmark based on public datasets converted to a standardized format.","This benchmark defines the machine-learning task as well as reporting metrics.","We illustrate the use of the benchmark by evaluating a set of existing seizure detection algorithms.","The SzCORE (Seizure Community Open-source Research Evaluation) framework and benchmark are made publicly available along with an open-source software library to facilitate research use, while enabling rigorous evaluation of the clinical significance of the algorithms, fostering a collective effort to more optimally detect seizures to improve the lives of people with epilepsy."],"url":"http://arxiv.org/abs/2402.13005v1","category":"eess.SP"}
{"created":"2024-02-20 13:33:33","title":"Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition","abstract":"Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.","sentences":["Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR).","Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures.","While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm.","However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders.","Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation.","We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language.","Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters."],"url":"http://arxiv.org/abs/2402.13004v1","category":"cs.CV"}
{"created":"2024-02-20 13:25:39","title":"Phonotactic Complexity across Dialects","abstract":"Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed.","sentences":["Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012).","We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties.","Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level.","A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity.","We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed."],"url":"http://arxiv.org/abs/2402.12998v1","category":"cs.CL"}
{"created":"2024-02-20 13:21:57","title":"Distributionally Robust Graph-based Recommendation System","abstract":"With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts. The code is available at https://github.com/WANGBohaO-jpg/DR-GNN.","sentences":["With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS).","However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts.","Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS.","Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse.","To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation.","DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support.","Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently.","Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts.","The code is available at https://github.com/WANGBohaO-jpg/DR-GNN."],"url":"http://arxiv.org/abs/2402.12994v2","category":"cs.IR"}
{"created":"2024-02-20 13:21:46","title":"An Autonomous Large Language Model Agent for Chemical Literature Data Mining","abstract":"Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency. The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry.","sentences":["Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare.","The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes.","Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields.","However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature.","To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature.","This AI agent employs large language models (LLMs) for prompt generation and iterative optimization.","It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance.","Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency.","The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry."],"url":"http://arxiv.org/abs/2402.12993v1","category":"cs.IR"}
{"created":"2024-02-20 13:20:39","title":"TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification","abstract":"Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.","sentences":["Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them.","Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse.","In this context, we describe the novel problem of Black-box Identity Verification (BBIV).","The goal is to determine whether a third-party application uses a certain LLM through its chat function.","We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use.","We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers.","TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction.","TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function."],"url":"http://arxiv.org/abs/2402.12991v1","category":"cs.LG"}
{"created":"2024-02-20 13:13:13","title":"Can GNN be Good Adapter for LLMs?","abstract":"Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\\% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.","sentences":["Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains.","In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text.","These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc.","Thus, this paper explores how to utilize LLMs to model TAGs.","Previous methods for TAG modeling are based on million-scale LMs.","When scaled up to billion-scale LLMs, they face huge challenges in computational costs.","Additionally, they also ignore the zero-shot inference capabilities of LLMs.","Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs.","In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs.","The entire framework is trained using auto-regression on node text (next token prediction).","Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks.","Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\\% in terms of node classification.","Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2.","The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling."],"url":"http://arxiv.org/abs/2402.12984v1","category":"cs.CL"}
{"created":"2024-02-20 12:53:31","title":"The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis","abstract":"In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.","sentences":["In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates.","Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context.","To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages.","Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages.","We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations.","Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether.","These findings show that the importance of demonstrations might be overestimated.","Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning."],"url":"http://arxiv.org/abs/2402.12976v1","category":"cs.CL"}
{"created":"2024-02-20 12:36:40","title":"Gl\u00f3rIA -- A Generative and Open Large Language Model for Portuguese","abstract":"Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluation shows that Gl\\'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text. The model also exhibits strong potential for various downstream tasks.","sentences":["Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs).","These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language.","Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese.","We introduce Gl\\'orIA, a robust European Portuguese decoder LLM.","To pre-train Gl\\'orIA",", we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources.","We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks.","Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark.","Evaluation shows that Gl\\'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text.","The model also exhibits strong potential for various downstream tasks."],"url":"http://arxiv.org/abs/2402.12969v1","category":"cs.CL"}
{"created":"2024-02-20 12:17:01","title":"Conditional Logical Message Passing Transformer for Complex Query Answering","abstract":"Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type. We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding. Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model.","sentences":["Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task.","Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning.","However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously.","Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors.","While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph.","In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear.","In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type.","We empirically verified that this approach can reduce computational costs without affecting performance.","Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding.","Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements.","Experimental results show that CLMPT is a new state-of-the-art neural CQA model."],"url":"http://arxiv.org/abs/2402.12954v1","category":"cs.LG"}
{"created":"2024-02-20 12:11:28","title":"QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems","abstract":"Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs. Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement sufficiency and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples. Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48%-96.05% more test samples than the random noise under the same perturbation size constraints). The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%).","sentences":["Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration.","Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems.","There is an urgent need for ways to test their correctness and security.","However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing.","These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons.","In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems.","We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs.","Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement sufficiency and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples.","Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48%-96.05% more test samples than the random noise under the same perturbation size constraints).","The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%)."],"url":"http://arxiv.org/abs/2402.12950v1","category":"cs.SE"}
{"created":"2024-02-20 11:50:50","title":"Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space","abstract":"Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space.","sentences":["Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability.","However, the complexity of their policies often makes them challenging to understand.","In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks.","Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task.","Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements.","We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space."],"url":"http://arxiv.org/abs/2402.12939v1","category":"cs.LG"}
{"created":"2024-02-20 11:28:50","title":"A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence","abstract":"By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.","sentences":["By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic.","However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers.","In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives.","First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically.","To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews.","Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords.","Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals.","The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects.","Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews.","This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews.","Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development."],"url":"http://arxiv.org/abs/2402.12928v1","category":"cs.DL"}
{"created":"2024-02-20 11:06:42","title":"Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models","abstract":"Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products. With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems. This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks. By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape. This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more significant outcomes in increasingly intricate data domains. Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning","sentences":["Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products.","With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems.","This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline.","We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks.","By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape.","This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more significant outcomes in increasingly intricate data domains.","Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning"],"url":"http://arxiv.org/abs/2402.12916v1","category":"cs.LG"}
{"created":"2024-02-20 10:52:57","title":"Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects","abstract":"The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.","sentences":["The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety.","While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts.","To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP).","We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts.","We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions."],"url":"http://arxiv.org/abs/2402.12907v1","category":"cs.AI"}
{"created":"2024-02-20 10:43:01","title":"Evidence of apsidal motion and a possible co-moving companion star detected in the WASP-19 system","abstract":"Love numbers measure the reaction of a celestial body to perturbing forces, such as the centrifugal force caused by rotation, or tidal forces resulting from the interaction with a companion body. These parameters are related to the interior density profile. The non-point mass nature of the host star and a planet orbiting around each other contributes to the periastron precession. The rate of this precession is characterized mainly by the second-order Love number, which offers an opportunity to determine its value. We collected all available radial velocity (RV) data, along with the transit and occultation times from the previous investigations of the system. We supplemented the data set with 19 new RV data points of the host star WASP-19A obtained by HARPS. Here, we summarize the technique for modeling the RV observations and the photometric transit timing variations (TTVs) to determine the rate of periastron precession in this system for the first time. We excluded the presence of a second possible planet up to a period of ~4200 d and with a radial velocity amplitude bigger than ~1 m/s. We show that a constant period is not able to reproduce the observed radial velocities. We also investigated and excluded the possibility of tidal decay and long-term acceleration in the system. However, the inclusion of a small periastron precession term did indeed improve the quality of the fit. We measured the periastron precession rate to be 233 $^{+25}_{-35}$''/. By assuming synchronous rotation for the planet, it indicates a k2 Love number of 0.20 $^{+0.02}_{-0.03}$ for WASP-19Ab. The derived k2 value of the planet has the same order of magnitude as the estimated fluid Love number of other Jupiter-sized exoplanets (WASP-18Ab, WASP-103b, and WASP-121b). A low value of k2 indicates a higher concentration of mass toward the planetary nucleus.","sentences":["Love numbers measure the reaction of a celestial body to perturbing forces, such as the centrifugal force caused by rotation, or tidal forces resulting from the interaction with a companion body.","These parameters are related to the interior density profile.","The non-point mass nature of the host star and a planet orbiting around each other contributes to the periastron precession.","The rate of this precession is characterized mainly by the second-order Love number, which offers an opportunity to determine its value.","We collected all available radial velocity (RV) data, along with the transit and occultation times from the previous investigations of the system.","We supplemented the data set with 19 new RV data points of the host star WASP-19A obtained by HARPS.","Here, we summarize the technique for modeling the RV observations and the photometric transit timing variations (TTVs) to determine the rate of periastron precession in this system for the first time.","We excluded the presence of a second possible planet up to a period of ~4200 d and with a radial velocity amplitude bigger than ~1 m/s. We show that a constant period is not able to reproduce the observed radial velocities.","We also investigated and excluded the possibility of tidal decay and long-term acceleration in the system.","However, the inclusion of a small periastron precession term did indeed improve the quality of the fit.","We measured the periastron precession rate to be 233 $^{+25}_{-35}$''/. By assuming synchronous rotation for the planet, it indicates a k2 Love number of 0.20 $^{+0.02}_{-0.03}$ for WASP-19Ab.","The derived k2 value of the planet has the same order of magnitude as the estimated fluid Love number of other Jupiter-sized exoplanets (WASP-18Ab, WASP-103b, and WASP-121b).","A low value of k2 indicates a higher concentration of mass toward the planetary nucleus."],"url":"http://arxiv.org/abs/2402.12896v1","category":"astro-ph.EP"}
{"created":"2024-02-20 10:30:36","title":"The practice of qualitative parameterisation in the development of Bayesian networks","abstract":"The typical phases of Bayesian network (BN) structured development include specification of purpose and scope, structure development, parameterisation and validation. Structure development is typically focused on qualitative issues and parameterisation quantitative issues, however there are qualitative and quantitative issues that arise in both phases. A common step that occurs after the initial structure has been developed is to perform a rough parameterisation that only captures and illustrates the intended qualitative behaviour of the model. This is done prior to a more rigorous parameterisation, ensuring that the structure is fit for purpose, as well as supporting later development and validation. In our collective experience and in discussions with other modellers, this step is an important part of the development process, but is under-reported in the literature. Since the practice focuses on qualitative issues, despite being quantitative in nature, we call this step qualitative parameterisation and provide an outline of its role in the BN development process.","sentences":["The typical phases of Bayesian network (BN) structured development include specification of purpose and scope, structure development, parameterisation and validation.","Structure development is typically focused on qualitative issues and parameterisation quantitative issues, however there are qualitative and quantitative issues that arise in both phases.","A common step that occurs after the initial structure has been developed is to perform a rough parameterisation that only captures and illustrates the intended qualitative behaviour of the model.","This is done prior to a more rigorous parameterisation, ensuring that the structure is fit for purpose, as well as supporting later development and validation.","In our collective experience and in discussions with other modellers, this step is an important part of the development process, but is under-reported in the literature.","Since the practice focuses on qualitative issues, despite being quantitative in nature, we call this step qualitative parameterisation and provide an outline of its role in the BN development process."],"url":"http://arxiv.org/abs/2402.12887v1","category":"cs.AI"}
{"created":"2024-02-20 18:57:59","title":"Explicit expressions for the gamma vector leading to connections to upper/lower bounds and structural properties","abstract":"We find an explicit formula for the gamma vector in terms of the input polynomial in a way that extends it to arbitrary polynomials. More specifically, we find explicit linear combination in terms of coefficients of the input polynomial (using Catalan numbers and binomial coefficients) and an expression involving the derivative of the input polynomial. In the case where the input is the $h$-polynomial of a simplicial complex, this gives an interpretation of the gamma vector as a measure of local-global contributions. We also apply them to connect signs/inequalities of (shifts of) the gamma vector to upper/lower bound conditions on coefficients of the input polynomial. Finally, we make use of the shape of the sums used to make these estimates and connections with intersection numbers to relate these properties of the gamma vector to algebraic structures.","sentences":["We find an explicit formula for the gamma vector in terms of the input polynomial in a way that extends it to arbitrary polynomials.","More specifically, we find explicit linear combination in terms of coefficients of the input polynomial (using Catalan numbers and binomial coefficients) and an expression involving the derivative of the input polynomial.","In the case where the input is the $h$-polynomial of a simplicial complex, this gives an interpretation of the gamma vector as a measure of local-global contributions.","We also apply them to connect signs/inequalities of (shifts of) the gamma vector to upper/lower bound conditions on coefficients of the input polynomial.","Finally, we make use of the shape of the sums used to make these estimates and connections with intersection numbers to relate these properties of the gamma vector to algebraic structures."],"url":"http://arxiv.org/abs/2402.13248v1","category":"math.CO"}
{"created":"2024-02-20 18:56:17","title":"Quantized shift response in multi-gap topological phases","abstract":"We show that certain 3D multi-gap topological insulators can host quantized shift photoconductivities due to bulk invariants that are defined under reality conditions imposed by additional symmetries. We recast the quantization in terms of the integrated torsion tensor and the non-Abelian Berry connection constituting Chern-Simons forms. Physically, we recognize that the topological quantization emerges purely from virtual transitions contributing to the optical response. Our findings provide another quantized electromagnetic DC response due to the non-trivial band topology, beyond the quantum anomalous Hall effect of Chern insulators and quantized circular photogalvanic effect found in Weyl semimetals.","sentences":["We show that certain 3D multi-gap topological insulators can host quantized shift photoconductivities due to bulk invariants that are defined under reality conditions imposed by additional symmetries.","We recast the quantization in terms of the integrated torsion tensor and the non-Abelian Berry connection constituting Chern-Simons forms.","Physically, we recognize that the topological quantization emerges purely from virtual transitions contributing to the optical response.","Our findings provide another quantized electromagnetic DC response due to the non-trivial band topology, beyond the quantum anomalous Hall effect of Chern insulators and quantized circular photogalvanic effect found in Weyl semimetals."],"url":"http://arxiv.org/abs/2402.13245v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 18:55:09","title":"VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning","abstract":"Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.","sentences":["Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging.","In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning.","VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle.","Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods.","It runs stably in a fully end-to-end manner, even without the rule-based wrapper.","Closed-loop demos are presented at https://hgao-cv.github.io/VADv2."],"url":"http://arxiv.org/abs/2402.13243v1","category":"cs.CV"}
{"created":"2024-02-20 18:51:20","title":"Continuous Pushdown VASS in One Dimension are Easy","abstract":"A pushdown vector addition system with states (PVASS) extends the model of vector addition systems with a pushdown stack. The algorithmic analysis of PVASS has applications such as static analysis of recursive programs manipulating integer variables. Unfortunately, reachability analysis, even for one-dimensional PVASS is not known to be decidable. We relax the model of one-dimensional PVASS to make the counter updates continuous and show that in this case reachability, coverability, and boundedness are decidable in polynomial time. In addition, for the extension of the model with lower-bound guards on the states, we show that coverability and reachability are in NP, and boundedness is in coNP.","sentences":["A pushdown vector addition system with states (PVASS) extends the model of vector addition systems with a pushdown stack.","The algorithmic analysis of PVASS has applications such as static analysis of recursive programs manipulating integer variables.","Unfortunately, reachability analysis, even for one-dimensional PVASS is not known to be decidable.","We relax the model of one-dimensional PVASS to make the counter updates continuous and show that in this case reachability, coverability, and boundedness are decidable in polynomial time.","In addition, for the extension of the model with lower-bound guards on the states, we show that coverability and reachability are in NP, and boundedness is in coNP."],"url":"http://arxiv.org/abs/2402.13237v1","category":"cs.LO"}
{"created":"2024-02-20 18:41:11","title":"Online Matching on $3$-Uniform Hypergraphs","abstract":"The online matching problem was introduced by Karp, Vazirani and Vazirani (STOC 1990) on bipartite graphs with vertex arrivals. It is well-known that the optimal competitive ratio is $1-1/e$ for both integral and fractional versions of the problem. Since then, there has been considerable effort to find optimal competitive ratios for other related settings. In this work, we go beyond the graph case and study the online matching problem on $k$-uniform hypergraphs. For $k=3$, we provide an optimal primal-dual fractional algorithm, which achieves a competitive ratio of $(e-1)/(e+1)\\approx 0.4621$. As our main technical contribution, we present a carefully constructed adversarial instance, which shows that this ratio is in fact optimal. It combines ideas from known hard instances for bipartite graphs under the edge-arrival and vertex-arrival models. For $k\\geq 3$, we give a simple integral algorithm which performs better than greedy when the online nodes have bounded degree. As a corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform hypergraphs when every online node has degree at most 2. This is because the special case where every online node has degree 1 is equivalent to the edge-arrival model on graphs, for which an upper bound of 1/2 is known.","sentences":["The online matching problem was introduced by Karp, Vazirani and Vazirani (STOC 1990) on bipartite graphs with vertex arrivals.","It is well-known that the optimal competitive ratio is $1-1/e$ for both integral and fractional versions of the problem.","Since then, there has been considerable effort to find optimal competitive ratios for other related settings.","In this work, we go beyond the graph case and study the online matching problem on $k$-uniform hypergraphs.","For $k=3$, we provide an optimal primal-dual fractional algorithm, which achieves a competitive ratio of $(e-1)/(e+1)\\approx 0.4621$. As our main technical contribution, we present a carefully constructed adversarial instance, which shows that this ratio is in fact optimal.","It combines ideas from known hard instances for bipartite graphs under the edge-arrival and vertex-arrival models.","For $k\\geq 3$, we give a simple integral algorithm which performs better than greedy when the online nodes have bounded degree.","As a corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform hypergraphs when every online node has degree at most 2.","This is because the special case where every online node has degree 1 is equivalent to the edge-arrival model on graphs, for which an upper bound of 1/2 is known."],"url":"http://arxiv.org/abs/2402.13227v1","category":"cs.DS"}
{"created":"2024-02-20 18:31:27","title":"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts","abstract":"The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.","sentences":["The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions.","To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion.","We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM.","Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark.","While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%.","We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question.","Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory.","We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts."],"url":"http://arxiv.org/abs/2402.13220v1","category":"cs.CV"}
{"created":"2024-02-20 18:30:51","title":"Spontaneous-emission induced ratchet in atom-optics kicked rotor quantum walks","abstract":"Quantum walks have gained significant attention over the past decades, mainly because of their variety of implementations and applications. Atomic quantum walks are typically subject to spontaneous emissions arising from the control fields. We investigate spontaneous emission in an atom optics kicked rotor quantum walk. Here, spontaneous emission occurs naturally due to the driving by the kicks, and it is generally viewed as a nuisance in the experiment. We find, however, that spontaneous emission may induce asymmetries in an otherwise symmetric quantum walk. Our results underscore the utility of spontaneous emission and the application of the asymmetric evolution in the walker's space, i.e. for the construction of a quantum walk ratchet or for Parrondo-like quantum games. This highlights the potential for reinterpreting seemingly adverse effects as beneficial under certain conditions, thus broadening the scope of quantum walks and their applications.","sentences":["Quantum walks have gained significant attention over the past decades, mainly because of their variety of implementations and applications.","Atomic quantum walks are typically subject to spontaneous emissions arising from the control fields.","We investigate spontaneous emission in an atom optics kicked rotor quantum walk.","Here, spontaneous emission occurs naturally due to the driving by the kicks, and it is generally viewed as a nuisance in the experiment.","We find, however, that spontaneous emission may induce asymmetries in an otherwise symmetric quantum walk.","Our results underscore the utility of spontaneous emission and the application of the asymmetric evolution in the walker's space, i.e. for the construction of a quantum walk ratchet or for Parrondo-like quantum games.","This highlights the potential for reinterpreting seemingly adverse effects as beneficial under certain conditions, thus broadening the scope of quantum walks and their applications."],"url":"http://arxiv.org/abs/2402.13218v1","category":"quant-ph"}
{"created":"2024-02-20 18:20:59","title":"Bayesian Reward Models for LLM Alignment","abstract":"To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling.","sentences":["To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data.","We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback).","However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference.","This is especially problematic as the prompt or response diverges from the training data.","It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution.","Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling."],"url":"http://arxiv.org/abs/2402.13210v1","category":"cs.LG"}
{"created":"2024-02-20 18:15:20","title":"Momentum-space Observation of Optically Excited Non-Thermal Electrons in Graphene with Persistent Pseudospin Polarization","abstract":"The unique optical properties of graphene, with broadband absorption and ultrafast response, make it a critical component of optoelectronic and spintronic devices. Using time-resolved momentum microscopy with high data rate and high dynamic range, we report momentum-space measurements of electrons promoted to the graphene conduction band with visible light, and their subsequent relaxation. We observe a pronounced non-thermal distribution of nascent photoexcited electrons with lattice pseudospin polarization in remarkable agreement with results of simple tight-binding theory. By varying the excitation fluence, we vary the relative importance of electron-electron vs. electron-phonon scattering in the relaxation of the initial distribution. Increasing the excitation fluence results in increased noncollinear electron-electron scattering and reduced pseudospin polarization, although up-scattered electrons retain a degree of polarization. These detailed momentum-resolved electron dynamics in graphene demonstrate the capabilities of high-performance time-resolved momentum microscopy in the study of 2D materials and can inform the design of graphene devices.","sentences":["The unique optical properties of graphene, with broadband absorption and ultrafast response, make it a critical component of optoelectronic and spintronic devices.","Using time-resolved momentum microscopy with high data rate and high dynamic range, we report momentum-space measurements of electrons promoted to the graphene conduction band with visible light, and their subsequent relaxation.","We observe a pronounced non-thermal distribution of nascent photoexcited electrons with lattice pseudospin polarization in remarkable agreement with results of simple tight-binding theory.","By varying the excitation fluence, we vary the relative importance of electron-electron vs. electron-phonon scattering in the relaxation of the initial distribution.","Increasing the excitation fluence results in increased noncollinear electron-electron scattering and reduced pseudospin polarization, although up-scattered electrons retain a degree of polarization.","These detailed momentum-resolved electron dynamics in graphene demonstrate the capabilities of high-performance time-resolved momentum microscopy in the study of 2D materials and can inform the design of graphene devices."],"url":"http://arxiv.org/abs/2402.13205v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 18:09:54","title":"Un crible minorant effectif pour les entiers friables","abstract":"Let $\\mathcal{A}$ be a finite set of positive integers and $y\\geq 1$. We give an effective lower bound of the cardinality of the set $\\{n\\in\\mathcal{A};\\,p|n\\Longrightarrow p\\leq y\\}$ under the condition of a good knowledge of the level of distribution of the set $\\mathcal{A}$. Some consequences are studied: an application to the smooth values of irreducible polynomials or binary forms with integer coefficients, and an application to shifted smooth numbers.","sentences":["Let $\\mathcal{A}$ be a finite set of positive integers and $y\\geq 1$.","We give an effective lower bound of the cardinality of the set $\\{n\\in\\mathcal{A};\\,p|n\\Longrightarrow p\\leq y\\}$ under the condition of a good knowledge of the level of distribution of the set $\\mathcal{A}$. Some consequences are studied: an application to the smooth values of irreducible polynomials or binary forms with integer coefficients, and an application to shifted smooth numbers."],"url":"http://arxiv.org/abs/2402.13198v1","category":"math.NT"}
{"created":"2024-02-20 17:56:24","title":"Question Calibration and Multi-Hop Modeling for Temporal Question Answering","abstract":"Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering. To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach. Specifically, We first calibrate the question representation by fusing the question and the time-constrained concepts in KG. Then, we construct the GNN layer to complete multi-hop message passing. Finally, the question representation is combined with the embedding output by the GNN to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset's complex questions are absolutely improved by 5.1% and 1.2% compared to the best-performing baseline. Moreover, QC-MHM can generate interpretable and trustworthy predictions.","sentences":["Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks.","In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention.","Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations.","(I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities.","(II)","They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering.","To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach.","Specifically, We first calibrate the question representation by fusing the question and the time-constrained concepts in KG.","Then, we construct the GNN layer to complete multi-hop message passing.","Finally, the question representation is combined with the embedding output by the GNN to generate the final prediction.","Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset.","Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset's complex questions are absolutely improved by 5.1% and 1.2% compared to the best-performing baseline.","Moreover, QC-MHM can generate interpretable and trustworthy predictions."],"url":"http://arxiv.org/abs/2402.13188v1","category":"cs.CL"}
{"created":"2024-02-20 17:53:24","title":"Testing Calibration in Subquadratic Time","abstract":"In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $O(n^{1.5} \\log(n))$. This improves upon state-of-the-art black-box linear program solvers requiring $\\Omega(n^\\omega)$ time, where $\\omega > 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem, and give sample complexity lower bounds for alternative calibration distances to the one considered in this work. Finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate sample sizes.","sentences":["In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models.","However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored.","Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing.","We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration.   ","We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $O(n^{1.5} \\log(n))$. This improves upon state-of-the-art black-box linear program solvers requiring $\\Omega(n^\\omega)$ time, where $\\omega > 2$ is the exponent of matrix multiplication.","We also develop algorithms for tolerant variants of our testing problem, and give sample complexity lower bounds for alternative calibration distances to the one considered in this work.","Finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate sample sizes."],"url":"http://arxiv.org/abs/2402.13187v1","category":"cs.LG"}
{"created":"2024-02-20 16:53:26","title":"The Hidden Space of Transformer Language Adapters","abstract":"We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.","sentences":["We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages.","We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model.","Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance.","Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace.","Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency."],"url":"http://arxiv.org/abs/2402.13137v1","category":"cs.CL"}
{"created":"2024-02-20 16:43:20","title":"Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity","abstract":"While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size. Finally, we observe further boosts by combining TMFT with a word similarity task or domain adaptive pre-training.","sentences":["While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback.","In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings.","The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS).","We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers.","We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method.","TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset.","We extend our analysis to various model sizes and languages.","Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size.","Finally, we observe further boosts by combining TMFT with a word similarity task or domain adaptive pre-training."],"url":"http://arxiv.org/abs/2402.13130v1","category":"cs.CL"}
{"created":"2024-02-20 16:35:14","title":"Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model","abstract":"Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware. Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons. In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions. Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels. We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution.","sentences":["Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware.","Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data.","Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons.","In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions.","Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels.","We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution."],"url":"http://arxiv.org/abs/2402.13122v1","category":"cs.CV"}
{"created":"2024-02-20 16:25:41","title":"Causal and Stable Relativistic Hydrodynamic Fluctuations","abstract":"When two nuclei collide close to the speed of light, a fluid state known as the quark-gluon plasma is formed. Attempts to understand the dynamics of this fluid have generated significant research into dissipative relativistic fluid dynamics. The fluctuation-dissipation theorem implies that any dissipative dynamical system will also experience thermal fluctuations; however, such fluctuations are not typically included in the modeling of the quark-gluon plasma. This work discusses a new method of determining whether a hydrodynamic framework is consistent with thermal fluctuations. We develop a new method for calculating the noise correlator of relativistic hydrodynamic systems and apply it to Israel-Stewart theory in a general hydrodynamic frame.","sentences":["When two nuclei collide close to the speed of light, a fluid state known as the quark-gluon plasma is formed.","Attempts to understand the dynamics of this fluid have generated significant research into dissipative relativistic fluid dynamics.","The fluctuation-dissipation theorem implies that any dissipative dynamical system will also experience thermal fluctuations; however, such fluctuations are not typically included in the modeling of the quark-gluon plasma.","This work discusses a new method of determining whether a hydrodynamic framework is consistent with thermal fluctuations.","We develop a new method for calculating the noise correlator of relativistic hydrodynamic systems and apply it to Israel-Stewart theory in a general hydrodynamic frame."],"url":"http://arxiv.org/abs/2402.13119v1","category":"nucl-th"}
{"created":"2024-02-20 16:09:29","title":"Observation of time crystal comb in a driven-dissipative system","abstract":"We report the observation of a time crystal comb in the continuously driven-dissipative and strongly interacting Rydberg thermal gas, in which continuous time crystal and discrete time crystal as well as the higher-order harmonic oscillation phases are observed in the same system by tuning the Rydberg excitation Rabi frequency. Remarkably, our work establishes the fundamental relation of time crystalline and driven-dissipative system, and provides new ways to explore the nonequilibrium phases of matter in open systems. Such time crystals with persistent oscillation rooted in emergent quantum correlations, may emerge as a ubiquitous tool in quantum metrology, for instance, continuous sensing and parameter estimation surpassing the standard quantum limit.","sentences":["We report the observation of a time crystal comb in the continuously driven-dissipative and strongly interacting Rydberg thermal gas, in which continuous time crystal and discrete time crystal as well as the higher-order harmonic oscillation phases are observed in the same system by tuning the Rydberg excitation Rabi frequency.","Remarkably, our work establishes the fundamental relation of time crystalline and driven-dissipative system, and provides new ways to explore the nonequilibrium phases of matter in open systems.","Such time crystals with persistent oscillation rooted in emergent quantum correlations, may emerge as a ubiquitous tool in quantum metrology, for instance, continuous sensing and parameter estimation surpassing the standard quantum limit."],"url":"http://arxiv.org/abs/2402.13112v1","category":"physics.atom-ph"}
{"created":"2024-02-20 15:35:02","title":"Gravitational Wave Signal Extraction Against Non-Stationary Instrumental Noises with Deep Neural Network","abstract":"Sapce-borne gravitational wave antennas, such as LISA and LISA-like mission (Taiji and Tianqin), will offer novel perspectives for exploring our Universe while introduce new challenges, especially in data analysis. Aside from the known challenges like high parameter space dimension, superposition of large number of signals and etc., gravitational wave detections in space would be more seriously affected by anomalies or non-stationarities in the science measurements. Considering the three types of foreseeable non-stationarities including data gaps, transients (glitches), and time-varying noise auto-correlations, which may come from routine maintenance or unexpected disturbances during science operations, we developed a deep learning model for accurate signal extractions confronted with such anomalous scenarios. Our model exhibits the same performance as the current state-of-the-art models do for the ideal and anomaly free scenario, while shows remarkable adaptability in extractions of coalescing massive black hole binary signal against all three types of non-stationarities and even their mixtures. This also provide new explorations into the robustness studies of deep learning models for data processing in space-borne gravitational wave missions.","sentences":["Sapce-borne gravitational wave antennas, such as LISA and LISA-like mission (Taiji and Tianqin), will offer novel perspectives for exploring our Universe while introduce new challenges, especially in data analysis.","Aside from the known challenges like high parameter space dimension, superposition of large number of signals and etc., gravitational wave detections in space would be more seriously affected by anomalies or non-stationarities in the science measurements.","Considering the three types of foreseeable non-stationarities including data gaps, transients (glitches), and time-varying noise auto-correlations, which may come from routine maintenance or unexpected disturbances during science operations, we developed a deep learning model for accurate signal extractions confronted with such anomalous scenarios.","Our model exhibits the same performance as the current state-of-the-art models do for the ideal and anomaly free scenario, while shows remarkable adaptability in extractions of coalescing massive black hole binary signal against all three types of non-stationarities and even their mixtures.","This also provide new explorations into the robustness studies of deep learning models for data processing in space-borne gravitational wave missions."],"url":"http://arxiv.org/abs/2402.13091v1","category":"gr-qc"}
{"created":"2024-02-20 15:30:09","title":"Slot-VLM: SlowFast Slots for Video-Language Modeling","abstract":"Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.","sentences":["Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding.","A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs.","In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference.","Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots.","In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure.","The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information.","Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features.","These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering.","Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering."],"url":"http://arxiv.org/abs/2402.13088v1","category":"cs.CV"}
{"created":"2024-02-20 15:24:21","title":"Mode Estimation with Partial Feedback","abstract":"The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.","sentences":["The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments.","These new learning pipelines call for new theoretical frameworks.","In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback.","We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting.","Finally, we combine those contributions into a statistically and computationally efficient solution to our problem."],"url":"http://arxiv.org/abs/2402.13079v1","category":"stat.ML"}
{"created":"2024-02-20 14:32:48","title":"A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction","abstract":"This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing. Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions. Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces. Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model. Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF. A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework. These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time. This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions. Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions.","sentences":["This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing.","Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions.","Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces.","Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model.","Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF.","A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework.","These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time.","This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions.","Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions."],"url":"http://arxiv.org/abs/2402.13045v1","category":"cs.RO"}
{"created":"2024-02-20 14:29:37","title":"Data Repository of Finite Element Models of Normal and Deformed Thoracolumbar Spine","abstract":"Adult spine deformity (ASD) is prevalent and leads to a sagittal misalignment in the vertebral column. Computational methods, including Finite Element (FE) Models, have emerged as valuable tools for investigating the causes and treatment of ASD through biomechanical simulations. However, the process of generating personalized FE models is often complex and time-consuming. To address this challenge, we present a repository of FE models with diverse spine morphologies that statistically represent real geometries from a cohort of patients. These models are generated using EOS images, which are utilized to reconstruct 3D surface spine models. Subsequently, a Statistical Shape Model (SSM) is constructed, enabling the adaptation of a FE hexahedral mesh template for both the bone and soft tissues of the spine through mesh morphing. The SSM deformation fields facilitate the personalization of the mean hexahedral FE model based on sagittal balance measurements. Ultimately, this new hexahedral SSM tool offers a means to generate a virtual cohort of 16807 thoracolumbar FE spine models, which are openly shared in a public repository.","sentences":["Adult spine deformity (ASD) is prevalent and leads to a sagittal misalignment in the vertebral column.","Computational methods, including Finite Element (FE) Models, have emerged as valuable tools for investigating the causes and treatment of ASD through biomechanical simulations.","However, the process of generating personalized FE models is often complex and time-consuming.","To address this challenge, we present a repository of FE models with diverse spine morphologies that statistically represent real geometries from a cohort of patients.","These models are generated using EOS images, which are utilized to reconstruct 3D surface spine models.","Subsequently, a Statistical Shape Model (SSM) is constructed, enabling the adaptation of a FE hexahedral mesh template for both the bone and soft tissues of the spine through mesh morphing.","The SSM deformation fields facilitate the personalization of the mean hexahedral FE model based on sagittal balance measurements.","Ultimately, this new hexahedral SSM tool offers a means to generate a virtual cohort of 16807 thoracolumbar FE spine models, which are openly shared in a public repository."],"url":"http://arxiv.org/abs/2402.13041v1","category":"physics.bio-ph"}
{"created":"2024-02-20 14:23:34","title":"SiLLM: Large Language Models for Simultaneous Machine Translation","abstract":"Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence. The two agents collaborate to accomplish SiMT. To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM. Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance.","sentences":["Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words.","Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations.","However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model.","Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks.","We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT.","The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy.","The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence.","The two agents collaborate to accomplish SiMT.","To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM.","Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.13036v1","category":"cs.CL"}
{"created":"2024-02-20 14:06:39","title":"Bridging Methodologies: Angrist and Imbens' Contributions to Causal Identification","abstract":"In the 1990s, Joshua Angrist and Guido Imbens studied the causal interpretation of Instrumental Variable estimates (a widespread methodology in economics) through the lens of potential outcomes (a classical framework to formalize causality in statistics). Bridging a gap between those two strands of literature, they stress the importance of treatment effect heterogeneity and show that, under defendable assumptions in various applications, this method recovers an average causal effect for a specific subpopulation of individuals whose treatment is affected by the instrument. They were awarded the Nobel Prize primarily for this Local Average Treatment Effect (LATE). The first part of this article presents that methodological contribution in-depth: the origination in earlier applied articles, the different identification results and extensions, and related debates on the relevance of LATEs for public policy decisions. The second part reviews the main contributions of the authors beyond the LATE. J. Angrist has pursued the search for informative and varied empirical research designs in several fields, particularly in education. G. Imbens has complemented the toolbox for treatment effect estimation in many ways, notably through propensity score reweighting, matching, and, more recently, adapting machine learning procedures.","sentences":["In the 1990s, Joshua Angrist and Guido Imbens studied the causal interpretation of Instrumental Variable estimates (a widespread methodology in economics) through the lens of potential outcomes (a classical framework to formalize causality in statistics).","Bridging a gap between those two strands of literature, they stress the importance of treatment effect heterogeneity and show that, under defendable assumptions in various applications, this method recovers an average causal effect for a specific subpopulation of individuals whose treatment is affected by the instrument.","They were awarded the Nobel Prize primarily for this Local Average Treatment Effect (LATE).","The first part of this article presents that methodological contribution in-depth: the origination in earlier applied articles, the different identification results and extensions, and related debates on the relevance of LATEs for public policy decisions.","The second part reviews the main contributions of the authors beyond the LATE.","J. Angrist has pursued the search for informative and varied empirical research designs in several fields, particularly in education.","G. Imbens has complemented the toolbox for treatment effect estimation in many ways, notably through propensity score reweighting, matching, and, more recently, adapting machine learning procedures."],"url":"http://arxiv.org/abs/2402.13023v1","category":"econ.EM"}
{"created":"2024-02-20 13:17:37","title":"Towards Robust Graph Incremental Learning on Evolving Graphs","abstract":"Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem. We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting. Through comprehensive empirical studies with several benchmark datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art GNN incremental learning frameworks in the inductive setting.","sentences":["Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once.","This ability to learn incrementally from a stream of tasks is crucial for many real-world applications.","However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL).","This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added.","In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks.","We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem.","We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting.","Through comprehensive empirical studies with several benchmark datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art GNN incremental learning frameworks in the inductive setting."],"url":"http://arxiv.org/abs/2402.12987v1","category":"cs.LG"}
{"created":"2024-02-20 13:02:51","title":"Efficient adjustment for complex covariates: Gaining efficiency with DOPE","abstract":"Covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ATE) from observational data. Assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ATE. However, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-Euclidean data such as texts. We propose an general framework that accommodates adjustment for any subset of information expressed by the covariates. We generalize prior works and leverage these results to identify the optimal covariate information for efficient adjustment. This information is minimally sufficient for prediction of the outcome conditionally on treatment.   Based on our theoretical results, we propose the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of the ATE, and we provide asymptotic results for the DOPE under general conditions. Compared to the augmented inverse propensity weighted (AIPW) estimator, the DOPE can retain its efficiency even when the covariates are highly predictive of treatment. We illustrate this with a single-index model, and with an implementation of the DOPE based on neural networks, we demonstrate its performance on simulated and real data. Our results show that the DOPE provides an efficient and robust methodology for ATE estimation in various observational settings.","sentences":["Covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ATE) from observational data.","Assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ATE.","However, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-Euclidean data such as texts.","We propose an general framework that accommodates adjustment for any subset of information expressed by the covariates.","We generalize prior works and leverage these results to identify the optimal covariate information for efficient adjustment.","This information is minimally sufficient for prediction of the outcome conditionally on treatment.   ","Based on our theoretical results, we propose the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of the ATE, and we provide asymptotic results for the DOPE under general conditions.","Compared to the augmented inverse propensity weighted (AIPW) estimator, the DOPE can retain its efficiency even when the covariates are highly predictive of treatment.","We illustrate this with a single-index model, and with an implementation of the DOPE based on neural networks, we demonstrate its performance on simulated and real data.","Our results show that the DOPE provides an efficient and robust methodology for ATE estimation in various observational settings."],"url":"http://arxiv.org/abs/2402.12980v1","category":"math.ST"}
{"created":"2024-02-20 12:35:23","title":"MapTrack: Tracking in the Map","abstract":"Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.","sentences":["Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target.","Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities.","Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations.","In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes.","Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter.","The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons.","Trajectories of undetected targets that are still within the probability map are extended by state estimations directly.","The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter.","The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20.","Despite its superior performance, our method remains simple, online, and real-time.","The code will be open-sourced later."],"url":"http://arxiv.org/abs/2402.12968v1","category":"cs.CV"}
{"created":"2024-02-20 12:25:36","title":"Inferring Non-Failure Conditions for Declarative Programs","abstract":"Unintended failures during a computation are painful but frequent during software development. Failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers. Programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct. This paper presents an approach to verify such assumptions. For this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations. In the positive case, the absence of such failures is ensured. In the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again. Our method is fully automatic and can be applied to larger declarative programs. The results of an implementation for functional logic Curry programs are presented.","sentences":["Unintended failures during a computation are painful but frequent during software development.","Failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers.","Programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct.","This paper presents an approach to verify such assumptions.","For this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations.","In the positive case, the absence of such failures is ensured.","In the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again.","Our method is fully automatic and can be applied to larger declarative programs.","The results of an implementation for functional logic Curry programs are presented."],"url":"http://arxiv.org/abs/2402.12960v1","category":"cs.PL"}
{"created":"2024-02-20 12:22:42","title":"Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization","abstract":"Federated learning (FL) has been recognized as a viable distributed learning paradigm for training a machine learning model across distributed clients without uploading raw data. However, FL in wireless networks still faces two major challenges, i.e., large communication overhead and high energy consumption, which are exacerbated by client heterogeneity in dataset sizes and wireless channels. While model quantization is effective for energy reduction, existing works ignore adapting quantization to heterogeneous clients and FL convergence. To address these challenges, this paper develops an energy optimization problem of jointly designing quantization levels, scheduling clients, allocating channels, and controlling computation frequencies (QCCF) in wireless FL. Specifically, we derive an upper bound identifying the influence of client scheduling and quantization errors on FL convergence. Under the longterm convergence constraints and wireless constraints, the problem is established and transformed into an instantaneous problem with Lyapunov optimization. Solving Karush-Kuhn-Tucker conditions, our closed-form solution indicates that the doubly adaptive quantization level rises with the training process and correlates negatively with dataset sizes. Experiment results validate our theoretical results, showing that QCCF consumes less energy with faster convergence compared with state-of-the-art baselines.","sentences":["Federated learning (FL) has been recognized as a viable distributed learning paradigm for training a machine learning model across distributed clients without uploading raw data.","However, FL in wireless networks still faces two major challenges, i.e., large communication overhead and high energy consumption, which are exacerbated by client heterogeneity in dataset sizes and wireless channels.","While model quantization is effective for energy reduction, existing works ignore adapting quantization to heterogeneous clients and FL convergence.","To address these challenges, this paper develops an energy optimization problem of jointly designing quantization levels, scheduling clients, allocating channels, and controlling computation frequencies (QCCF) in wireless FL.","Specifically, we derive an upper bound identifying the influence of client scheduling and quantization errors on FL convergence.","Under the longterm convergence constraints and wireless constraints, the problem is established and transformed into an instantaneous problem with Lyapunov optimization.","Solving Karush-Kuhn-Tucker conditions, our closed-form solution indicates that the doubly adaptive quantization level rises with the training process and correlates negatively with dataset sizes.","Experiment results validate our theoretical results, showing that QCCF consumes less energy with faster convergence compared with state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.12957v1","category":"cs.DC"}
{"created":"2024-02-20 11:52:29","title":"Normalized Orthography for Tunisian Arabic","abstract":"Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences. This research introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency. The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology. This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic.","sentences":["Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences.","This research introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency.","The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology.","This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic."],"url":"http://arxiv.org/abs/2402.12940v1","category":"cs.CL"}
{"created":"2024-02-20 11:50:27","title":"UniCell: Universal Cell Nucleus Classification via Prompt Learning","abstract":"The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks. Code and models are available at https://github.com/lhaof/UniCell","sentences":["The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis.","Numerous pathological datasets are currently available, but their annotations are inconsistent.","Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition.","In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains.","In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets.","Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features.","The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources.","Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks.","Code and models are available at https://github.com/lhaof/UniCell"],"url":"http://arxiv.org/abs/2402.12938v1","category":"cs.CV"}
{"created":"2024-02-20 11:34:18","title":"Enhancement of the critical current by surface irregularities in Fe-based superconductors","abstract":"The critical current $I_c$ of single crystals of the iron pnictide superconductor BaFe$_2$(As$_{1-x}$P$_x$)$_2$, has been studied through measurements of magnetic hysteresis cycles. We show that the introduction of surface irregularities in the $\\mu$m scale significantly increase $I_c$, primarily near the irreversibility magnetic field $H_{irr}$, where the surface currents are the main contribution to $I_c$. Such an increase is consistent with a theoretical estimate for the maximum non-dissipative current that a rough surface can sustain, based on Mathieu-Simon continuum theory for the vortex state.","sentences":["The critical current $I_c$ of single crystals of the iron pnictide superconductor BaFe$_2$(As$_{1-x}$P$_x$)$_2$, has been studied through measurements of magnetic hysteresis cycles.","We show that the introduction of surface irregularities in the $\\mu$m scale significantly increase $I_c$, primarily near the irreversibility magnetic field $H_{irr}$, where the surface currents are the main contribution to $I_c$. Such an increase is consistent with a theoretical estimate for the maximum non-dissipative current that a rough surface can sustain, based on Mathieu-Simon continuum theory for the vortex state."],"url":"http://arxiv.org/abs/2402.12933v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 11:26:42","title":"CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection","abstract":"The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.","sentences":["The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content.","As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes.","In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection.","Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection.","However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial.","Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k).","To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios.","This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools."],"url":"http://arxiv.org/abs/2402.12927v1","category":"cs.CV"}
{"created":"2024-02-20 10:33:35","title":"Transformer-based Learned Image Compression for Joint Decoding and Denoising","abstract":"This work introduces a Transformer-based image compression system. It has the flexibility to switch between the standard image reconstruction and the denoising reconstruction from a single compressed bitstream. Instead of training separate decoders for these tasks, we incorporate two add-on modules to adapt a pre-trained image decoder from performing the standard image reconstruction to joint decoding and denoising. Our scheme adopts a two-pronged approach. It features a latent refinement module to refine the latent representation of a noisy input image for reconstructing a noise-free image. Additionally, it incorporates an instance-specific prompt generator that adapts the decoding process to improve on the latent refinement. Experimental results show that our method achieves a similar level of denoising quality to training a separate decoder for joint decoding and denoising at the expense of only a modest increase in the decoder's model size and computational complexity.","sentences":["This work introduces a Transformer-based image compression system.","It has the flexibility to switch between the standard image reconstruction and the denoising reconstruction from a single compressed bitstream.","Instead of training separate decoders for these tasks, we incorporate two add-on modules to adapt a pre-trained image decoder from performing the standard image reconstruction to joint decoding and denoising.","Our scheme adopts a two-pronged approach.","It features a latent refinement module to refine the latent representation of a noisy input image for reconstructing a noise-free image.","Additionally, it incorporates an instance-specific prompt generator that adapts the decoding process to improve on the latent refinement.","Experimental results show that our method achieves a similar level of denoising quality to training a separate decoder for joint decoding and denoising at the expense of only a modest increase in the decoder's model size and computational complexity."],"url":"http://arxiv.org/abs/2402.12888v1","category":"eess.IV"}
{"created":"2024-02-20 09:59:33","title":"Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets","abstract":"In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments. Here, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets. Additionally, it achieves an $O(\\sqrt{T})$ regret even in adversarial environments where FTL suffers an $\\Omega(T)$ regret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in corrupted stochastic environments with corruption level $C$. Furthermore, by extending our analysis, we establish a regret upper bound of $O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\\ell_p$-balls for $p \\in [1,\\infty)$. This bound bridges the gap between the $O(\\log T)$ regret bound for strongly convex sets ($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$).","sentences":["In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets.","In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret.","This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets.","We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments.","Here, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set.","Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets.","Additionally, it achieves an $O(\\sqrt{T})$ regret even in adversarial environments where FTL suffers an $\\Omega(T)$ regret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in corrupted stochastic environments with corruption level $C$.","Furthermore, by extending our analysis, we establish a regret upper bound of $O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\\ell_p$-balls for $p \\in","[1,\\infty)$. This bound bridges the gap between the $O(\\log T)$ regret bound for strongly convex sets ($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$)."],"url":"http://arxiv.org/abs/2402.12868v1","category":"cs.LG"}
{"created":"2024-02-20 09:55:20","title":"A Novel Protocol Using Captive Portals for FIDO2 Network Authentication","abstract":"FIDO2 authentication is starting to be applied in numerous web authentication services, aiming to replace passwords and their known vulnerabilities. However, this new authentication method has not been integrated yet with network authentication systems. In this paper, we introduce FIDO2CAP: FIDO2 Captive-portal Authentication Protocol. Our proposal describes a novel protocol for captive-portal network authentication using FIDO2 authenticators, as security keys and passkeys. For validating our proposal, we have developed a prototype of FIDO2CAP authentication in a mock scenario. Using this prototype, we performed an usability experiment with 15 real users. This work makes the first systematic approach for adapting network authentication to the new authentication paradigm relying on FIDO2 authentication.","sentences":["FIDO2 authentication is starting to be applied in numerous web authentication services, aiming to replace passwords and their known vulnerabilities.","However, this new authentication method has not been integrated yet with network authentication systems.","In this paper, we introduce FIDO2CAP:","FIDO2 Captive-portal Authentication Protocol.","Our proposal describes a novel protocol for captive-portal network authentication using FIDO2 authenticators, as security keys and passkeys.","For validating our proposal, we have developed a prototype of FIDO2CAP authentication in a mock scenario.","Using this prototype, we performed an usability experiment with 15 real users.","This work makes the first systematic approach for adapting network authentication to the new authentication paradigm relying on FIDO2 authentication."],"url":"http://arxiv.org/abs/2402.12864v1","category":"cs.CR"}
{"created":"2024-02-20 09:36:18","title":"A new simplified MOPSO based on Swarm Elitism and Swarm Memory: MO-ETPSO","abstract":"This paper presents an algorithm based on Particle Swarm Optimization (PSO), adapted for multi-objective optimization problems: the Elitist PSO (MO-ETPSO). The proposed algorithm integrates core strategies from the well-established NSGA-II approach, such as the Crowding Distance Algorithm, while leveraging the advantages of Swarm Intelligence in terms of individual and social cognition. A novel aspect of the algorithm is the introduction of a swarm memory and swarm elitism, which may turn the adoption of NSGA-II strategies in PSO. These features enhance the algorithm's ability to retain and utilize high-quality solutions throughout optimization. Furthermore, all operators within the algorithm are intentionally designed for simplicity, ensuring ease of replication and implementation in various settings. Preliminary comparisons with the NSGA-II algorithm for the Green Vehicle Routing Problem, both in terms of solutions found and convergence, have yielded promising results in favor of MO-ETPSO.","sentences":["This paper presents an algorithm based on Particle Swarm Optimization (PSO), adapted for multi-objective optimization problems: the Elitist PSO (MO-ETPSO).","The proposed algorithm integrates core strategies from the well-established NSGA-II approach, such as the Crowding Distance Algorithm, while leveraging the advantages of Swarm Intelligence in terms of individual and social cognition.","A novel aspect of the algorithm is the introduction of a swarm memory and swarm elitism, which may turn the adoption of NSGA-II strategies in PSO.","These features enhance the algorithm's ability to retain and utilize high-quality solutions throughout optimization.","Furthermore, all operators within the algorithm are intentionally designed for simplicity, ensuring ease of replication and implementation in various settings.","Preliminary comparisons with the NSGA-II algorithm for the Green Vehicle Routing Problem, both in terms of solutions found and convergence, have yielded promising results in favor of MO-ETPSO."],"url":"http://arxiv.org/abs/2402.12856v1","category":"cs.NE"}
{"created":"2024-02-20 09:30:48","title":"MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models","abstract":"Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features. We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks. With the same number of parameters, our approach outperforms LoRA significantly. In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks.","sentences":["Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks.","Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios.","To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.","However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks.","In this work, we introduce a novel PEFT method: MoELoRA.","We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features.","We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks.","With the same number of parameters, our approach outperforms LoRA significantly.","In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks."],"url":"http://arxiv.org/abs/2402.12851v1","category":"cs.CL"}
{"created":"2024-02-20 09:20:32","title":"Instruction-tuned Language Models are Better Knowledge Learners","abstract":"In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.","sentences":["In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data.","The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs.","However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized.","We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner.","Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions.","Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents.","This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents.","Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%."],"url":"http://arxiv.org/abs/2402.12847v1","category":"cs.CL"}
{"created":"2024-02-20 09:13:11","title":"SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets","abstract":"The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.","sentences":["The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations.","A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency.","This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning.","We explore and apply Self-Supervised Learning (SSL) to solve these challenges.","We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions."],"url":"http://arxiv.org/abs/2402.12843v1","category":"cs.CV"}
{"created":"2024-02-20 09:07:18","title":"Critical thresholds in pressureless Euler--Poisson equations with background states","abstract":"We investigate the critical threshold phenomena in a large class of one dimensional pressureless Euler--Poisson (EP) equations, with non-vanishing background states. First, we establish local-in-time well-posedness in proper regularity spaces, which are adapted for a certain \\textit{neutrality condition} to hold. The neutrality condition is shown to be necessary: we construct smooth solutions that exhibit instantaneous failure of the neutrality condition, which in turn yields non-existence of solutions, even locally in time, in the classical Sobolev spaces $H^s({\\mathbb R})$, $s \\geq 2$. Next, we study the critical threshold phenomena in the neutrality-condition-satisfying pressureless EP systems, where we distinguish between two cases. We prove that in the case of attractive forcing, the neutrality condition can further restrict the sub-critical region into its borderline, namely -- the sub-critical region is reduced to a single line in the phase plane. We then turn to provide a rather definitive answer for the critical thresholds in the case of repulsive EP systems with variable backgrounds. As an application, we analyze the critical thresholds for the damped EP system for cold plasma ion dynamics, where the density of electrons is given by the \\textit{Maxwell--Boltzmann relation}.","sentences":["We investigate the critical threshold phenomena in a large class of one dimensional pressureless Euler--Poisson (EP) equations, with non-vanishing background states.","First, we establish local-in-time well-posedness in proper regularity spaces, which are adapted for a certain \\textit{neutrality condition} to hold.","The neutrality condition is shown to be necessary: we construct smooth solutions that exhibit instantaneous failure of the neutrality condition, which in turn yields non-existence of solutions, even locally in time, in the classical Sobolev spaces $H^s({\\mathbb R})$, $s \\geq 2$.","Next, we study the critical threshold phenomena in the neutrality-condition-satisfying pressureless EP systems, where we distinguish between two cases.","We prove that in the case of attractive forcing, the neutrality condition can further restrict the sub-critical region into its borderline, namely -- the sub-critical region is reduced to a single line in the phase plane.","We then turn to provide a rather definitive answer for the critical thresholds in the case of repulsive EP systems with variable backgrounds.","As an application, we analyze the critical thresholds for the damped EP system for cold plasma ion dynamics, where the density of electrons is given by the \\textit{Maxwell--Boltzmann relation}."],"url":"http://arxiv.org/abs/2402.12839v1","category":"math.AP"}
{"created":"2024-02-20 09:02:55","title":"PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs","abstract":"While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization.","sentences":["While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models.","One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets.","However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs.","In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning.","Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks.","Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld.","This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization."],"url":"http://arxiv.org/abs/2402.12835v1","category":"cs.CL"}
{"created":"2024-02-20 08:55:38","title":"The Fundamental Parameters of Astrophysical Plasma Turbulence and its Dissipation: Nonrelativistic Limit","abstract":"A specific set of dimensionless plasma and turbulence parameters is introduced to characterize the nature of turbulence and its dissipation in weakly collisional space and astrophysical plasmas. Key considerations are discussed for the development of predictive models of the turbulent plasma heating that characterize the partitioning of dissipated turbulent energy between the ion and electron species and between the perpendicular and parallel degrees of freedom for each species. Identifying the kinetic physical mechanisms that govern the damping of the turbulent fluctuations is a critical first step in constructing such turbulent heating models. A set of ten general plasma and turbulence parameters are defined, and reasonable approximations along with the exploitation of existing scaling theories for magnetohydrodynamic turbulence are used to reduce this general set of ten parameters to just three parameters in the isotropic temperature case. A critical step forward in this study is to identify the dependence of all of the proposed kinetic mechanisms for turbulent damping in terms of the same set of fundamental plasma and turbulence parameters. Analytical estimations of the scaling of each damping mechanism on these fundamental parameters are presented, and this information is synthesized to produce the first phase diagram for the turbulent damping mechanisms as a function of driving scale and ion plasma beta.","sentences":["A specific set of dimensionless plasma and turbulence parameters is introduced to characterize the nature of turbulence and its dissipation in weakly collisional space and astrophysical plasmas.","Key considerations are discussed for the development of predictive models of the turbulent plasma heating that characterize the partitioning of dissipated turbulent energy between the ion and electron species and between the perpendicular and parallel degrees of freedom for each species.","Identifying the kinetic physical mechanisms that govern the damping of the turbulent fluctuations is a critical first step in constructing such turbulent heating models.","A set of ten general plasma and turbulence parameters are defined, and reasonable approximations along with the exploitation of existing scaling theories for magnetohydrodynamic turbulence are used to reduce this general set of ten parameters to just three parameters in the isotropic temperature case.","A critical step forward in this study is to identify the dependence of all of the proposed kinetic mechanisms for turbulent damping in terms of the same set of fundamental plasma and turbulence parameters.","Analytical estimations of the scaling of each damping mechanism on these fundamental parameters are presented, and this information is synthesized to produce the first phase diagram for the turbulent damping mechanisms as a function of driving scale and ion plasma beta."],"url":"http://arxiv.org/abs/2402.12829v1","category":"astro-ph.SR"}
{"created":"2024-02-20 08:33:41","title":"OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding","abstract":"Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency. However, the domain shift between training and test scenarios due to dataset limitations poses a challenge. This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs. Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance. To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method. It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec. Our OMRA is an online, inference technique. It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction. Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets.","sentences":["Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency.","However, the domain shift between training and test scenarios due to dataset limitations poses a challenge.","This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs.","Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance.","To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method.","It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec.","Our OMRA is an online, inference technique.","It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction.","Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets."],"url":"http://arxiv.org/abs/2402.12816v1","category":"eess.IV"}
{"created":"2024-02-20 08:27:50","title":"Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes","abstract":"The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.","sentences":["The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications.","Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods.","In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem.","We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited.","We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters.","Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness."],"url":"http://arxiv.org/abs/2402.12808v1","category":"cs.LG"}
{"created":"2024-02-20 08:27:30","title":"Principle of least action for quasi-adiabatic state transfers with dissipation","abstract":"We discuss a general formalism to optimize quasi-adiabatic state-transfer protocols, where high fidelity is achieved by maintaining the system in a dark subspace protected from the dominant dissipative channels. We cast the residual fidelity loss, induced by a combination of dissipation and non-adiabatic transitions, in the form of a classical action where the time-dependent control parameters act as coordinates. This allows us to apply the least action principle, yielding the fidelity upper-bound and the corresponding optimal transfer time. As an application, we analyze a system of two qubits subject to weak relaxation and dephasing, interacting through a strongly dissipative quantum bus. In this case, our formalism, we obtain a full characterization of the optimal state-transfer fidelity.","sentences":["We discuss a general formalism to optimize quasi-adiabatic state-transfer protocols, where high fidelity is achieved by maintaining the system in a dark subspace protected from the dominant dissipative channels.","We cast the residual fidelity loss, induced by a combination of dissipation and non-adiabatic transitions, in the form of a classical action where the time-dependent control parameters act as coordinates.","This allows us to apply the least action principle, yielding the fidelity upper-bound and the corresponding optimal transfer time.","As an application, we analyze a system of two qubits subject to weak relaxation and dephasing, interacting through a strongly dissipative quantum bus.","In this case, our formalism, we obtain a full characterization of the optimal state-transfer fidelity."],"url":"http://arxiv.org/abs/2402.12807v1","category":"quant-ph"}
{"created":"2024-02-20 07:49:46","title":"Stochastic Graph Heat Modelling for Diffusion-based Connectivity Retrieval","abstract":"Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures. This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology. Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph. We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals. Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data. On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer's disease. We find that the graph thermal diffusivity is lower for Alzheimer's patients than healthy controls and correlates with dementia scores, suggesting structural impairment in patients in line with previous findings.","sentences":["Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures.","This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology.","Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph.","We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals.","Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data.","On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer's disease.","We find that the graph thermal diffusivity is lower for Alzheimer's patients than healthy controls and correlates with dementia scores, suggesting structural impairment in patients in line with previous findings."],"url":"http://arxiv.org/abs/2402.12785v1","category":"eess.SP"}
{"created":"2024-02-20 07:20:03","title":"Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue","abstract":"In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions. This study introduces the first framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules. Further validation of our model's efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the ChatGPT. This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication.","sentences":["In the realm of human-AI dialogue, the facilitation of empathetic responses is important.","Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions.","This study introduces the first framework designed to engender empathetic dialogue with validating responses.","Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation.","Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules.","Further validation of our model's efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the ChatGPT.","This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication."],"url":"http://arxiv.org/abs/2402.12770v1","category":"cs.CL"}
{"created":"2024-02-20 06:58:49","title":"A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis","abstract":"Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.","sentences":["Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images.","Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts.","To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization.","For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods.","For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts.","Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs.","Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts.","Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results.","Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics."],"url":"http://arxiv.org/abs/2402.12760v1","category":"cs.MM"}
{"created":"2024-02-20 06:01:31","title":"CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning","abstract":"Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects. However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions. This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks. Besides, this paper carried out extensive experiments using five benchmark datasets. The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved.","sentences":["Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects.","However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference.","This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions.","This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet.","The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process.","Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks.","Besides, this paper carried out extensive experiments using five benchmark datasets.","The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved."],"url":"http://arxiv.org/abs/2402.12736v1","category":"cs.CV"}
{"created":"2024-02-20 05:57:01","title":"BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation","abstract":"In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency. Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets. In addition, its pure MLP architecture leads to a linear time complexity.","sentences":["In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying.","Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors.","However, most multi-behavior approaches have limitations in learning the relationship between different behaviors.","In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP).","Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent.","Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency.","Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets.","In addition, its pure MLP architecture leads to a linear time complexity."],"url":"http://arxiv.org/abs/2402.12733v1","category":"cs.IR"}
{"created":"2024-02-20 05:11:20","title":"Structural Knowledge Informed Continual Multivariate Time Series Forecasting","abstract":"Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay. Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data. As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context. Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime. Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks.","sentences":["Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations.","However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages).","Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance.","To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay.","Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data.","As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context.","Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime.","Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks."],"url":"http://arxiv.org/abs/2402.12722v1","category":"cs.LG"}
{"created":"2024-02-20 04:09:58","title":"Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition","abstract":"Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance). We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer. To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training. Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders. Then we pre-train the model with the self-supervised signals to learn the representation. After that, we fix the whole representation model and tune the classifier. During adaptation, we fix the transferable temporal dynamics and update the image encoder. The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets. Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities.","sentences":["Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples.","Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model.","Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance).","We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer.","To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training.","Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders.","Then we pre-train the model with the self-supervised signals to learn the representation.","After that, we fix the whole representation model and tune the classifier.","During adaptation, we fix the transferable temporal dynamics and update the image encoder.","The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets.","Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities."],"url":"http://arxiv.org/abs/2402.12706v1","category":"cs.CV"}
{"created":"2024-02-20 03:59:27","title":"From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges","abstract":"Generative Artificial Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design. However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service. In this position paper, we consider the potential, challenges, and promising approaches for generative AI for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited. Adapting generative AI for such settings involves overcoming significant hurdles, primarily in how to streamline complex models to function efficiently in low-resource environments. This necessitates innovative approaches in model compression, efficient algorithmic design, and perhaps even leveraging edge computing. The objective is to harness the power of generative AI in creating bespoke solutions for design problems, such as medical interventions, farm equipment maintenance, and educational material design, tailored to the unique constraints and needs of remote areas. These efforts could democratize access to advanced technology and foster sustainable development, ensuring universal accessibility and environmental consideration of AI-driven design benefits.","sentences":["Generative Artificial Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design.","However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service.","In this position paper, we consider the potential, challenges, and promising approaches for generative AI for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited.","Adapting generative AI for such settings involves overcoming significant hurdles, primarily in how to streamline complex models to function efficiently in low-resource environments.","This necessitates innovative approaches in model compression, efficient algorithmic design, and perhaps even leveraging edge computing.","The objective is to harness the power of generative AI in creating bespoke solutions for design problems, such as medical interventions, farm equipment maintenance, and educational material design, tailored to the unique constraints and needs of remote areas.","These efforts could democratize access to advanced technology and foster sustainable development, ensuring universal accessibility and environmental consideration of AI-driven design benefits."],"url":"http://arxiv.org/abs/2402.12702v1","category":"cs.AI"}
{"created":"2024-02-20 03:07:57","title":"On reduced modeling of the modulational dynamics in magnetohydrodynamics","abstract":"This paper explores structure formation in two-dimensional magnetohydrodynamic (MHD) turbulence as a modulational instability (MI) of turbulent fluctuations. We focus on the early stages of structure formation and consider simple backgrounds that allow for a tractable model of the MI while retaining the full chain of modulational harmonics. This approach allows us to systematically examine the validity of popular closures such as the quasilinear approximation and other low-order truncations. We find that, although such simple closures can provide quantitatively accurate approximations of the MI growth rates in some regimes, they can fail to capture the modulational dynamics in adjacent regimes even qualitatively, falsely predicting MI when the system is actually stable. We find that this discrepancy is due to the excitation of propagating spectral waves (PSWs) which can ballistically transport energy along the modulational spectrum, unimpeded until dissipative scales, thereby breaking the feedback loops that would otherwise sustain MIs. PSWs can be self-maintained as global modes with real frequencies and drain energy from the primary structure at a constant rate until the primary structure is depleted. To describe these waves within a reduced model, we propose an approximate spectral closure that captures them and MIs on the same footing. We also find that introducing corrections to ideal MHD, conservative or dissipative, can suppress PSWs and reinstate the accuracy of the quasilinear approximation. In this sense, ideal MHD is a `singular' system that is particularly sensitive to the accuracy of the closure within mean-field models.","sentences":["This paper explores structure formation in two-dimensional magnetohydrodynamic (MHD) turbulence as a modulational instability (MI) of turbulent fluctuations.","We focus on the early stages of structure formation and consider simple backgrounds that allow for a tractable model of the MI while retaining the full chain of modulational harmonics.","This approach allows us to systematically examine the validity of popular closures such as the quasilinear approximation and other low-order truncations.","We find that, although such simple closures can provide quantitatively accurate approximations of the MI growth rates in some regimes, they can fail to capture the modulational dynamics in adjacent regimes even qualitatively, falsely predicting MI when the system is actually stable.","We find that this discrepancy is due to the excitation of propagating spectral waves (PSWs) which can ballistically transport energy along the modulational spectrum, unimpeded until dissipative scales, thereby breaking the feedback loops that would otherwise sustain MIs.","PSWs can be self-maintained as global modes with real frequencies and drain energy from the primary structure at a constant rate until the primary structure is depleted.","To describe these waves within a reduced model, we propose an approximate spectral closure that captures them and MIs on the same footing.","We also find that introducing corrections to ideal MHD, conservative or dissipative, can suppress PSWs and reinstate the accuracy of the quasilinear approximation.","In this sense, ideal MHD is a `singular' system that is particularly sensitive to the accuracy of the closure within mean-field models."],"url":"http://arxiv.org/abs/2402.12680v1","category":"physics.plasm-ph"}
{"created":"2024-02-20 02:45:20","title":"Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies","abstract":"In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a finite policy class $\\Tilde{\\Pi}$, which can resort to an adversarial bandit subroutine. In light of the importance of a small, finite $\\Tilde{\\Pi}$, we propose a novel training-time algorithm to iteratively discover \\textit{non-dominated policies}, forming a near-optimal and minimal $\\Tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios.","sentences":["In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time.","Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios.","While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks.","To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks.","We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a finite policy class $\\Tilde{\\Pi}$, which can resort to an adversarial bandit subroutine.","In light of the importance of a small, finite $\\Tilde{\\Pi}$, we propose a novel training-time algorithm to iteratively discover \\textit{non-dominated policies}, forming a near-optimal and minimal $\\Tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency.","Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios."],"url":"http://arxiv.org/abs/2402.12673v1","category":"cs.LG"}
{"created":"2024-02-20 02:32:27","title":"Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning for End-to-end Navigation of Autonomous Vehicles","abstract":"Autonomous driving (AD) technology, leveraging artificial intelligence, strives for vehicle automation. End-toend strategies, emerging to simplify traditional driving systems by integrating perception, decision-making, and control, offer new avenues for advanced driving functionalities. Despite their potential, current challenges include data efficiency, training complexities, and poor generalization. This study addresses these issues with a novel end-to-end AD training model, enhancing system adaptability and intelligence. The model incorporates a Transformer module into the policy network, undergoing initial behavior cloning (BC) pre-training for update gradients. Subsequently, fine-tuning through reinforcement learning with human guidance (RLHG) adapts the model to specific driving environments, aiming to surpass the performance limits of imitation learning (IL). The fine-tuning process involves human interactions, guiding the model to acquire more efficient and safer driving behaviors through supervision, intervention, demonstration, and reward feedback. Simulation results demonstrate that this framework accelerates learning, achieving precise control and significantly enhancing safety and reliability. Compared to other advanced baseline methods, the proposed approach excels in challenging AD tasks. The introduction of the Transformer module and human-guided fine-tuning provides valuable insights and methods for research and applications in the AD field.","sentences":["Autonomous driving (AD) technology, leveraging artificial intelligence, strives for vehicle automation.","End-toend strategies, emerging to simplify traditional driving systems by integrating perception, decision-making, and control, offer new avenues for advanced driving functionalities.","Despite their potential, current challenges include data efficiency, training complexities, and poor generalization.","This study addresses these issues with a novel end-to-end AD training model, enhancing system adaptability and intelligence.","The model incorporates a Transformer module into the policy network, undergoing initial behavior cloning (BC) pre-training for update gradients.","Subsequently, fine-tuning through reinforcement learning with human guidance (RLHG) adapts the model to specific driving environments, aiming to surpass the performance limits of imitation learning (IL).","The fine-tuning process involves human interactions, guiding the model to acquire more efficient and safer driving behaviors through supervision, intervention, demonstration, and reward feedback.","Simulation results demonstrate that this framework accelerates learning, achieving precise control and significantly enhancing safety and reliability.","Compared to other advanced baseline methods, the proposed approach excels in challenging AD tasks.","The introduction of the Transformer module and human-guided fine-tuning provides valuable insights and methods for research and applications in the AD field."],"url":"http://arxiv.org/abs/2402.12666v1","category":"cs.RO"}
{"created":"2024-02-20 01:49:15","title":"Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation","abstract":"Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized \"trick tests\" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no correspondence between trick tests and RUTEd evaluations. Specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on RUTEd evaluations only as often as random chance. We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.","sentences":["Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world.","In this work, we study the correspondence between such decontextualized \"trick tests\" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations).","We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation.","We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation.","We conduct each evaluation for seven instruction-tuned LLMs.","For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises.","We found no correspondence between trick tests and RUTEd evaluations.","Specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on RUTEd evaluations only as often as random chance.","We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms."],"url":"http://arxiv.org/abs/2402.12649v1","category":"cs.CL"}
{"created":"2024-02-20 01:12:59","title":"Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors","abstract":"Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors. Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks to directly craft poisoned data in the input space. However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space. Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation. Empirical results reveal that transfer learning is more vulnerable to our attacks. Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks.","sentences":["Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible.","Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data.","However, such a process may also raise concerns regarding data poisoning attacks.","For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning.","In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors.","Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks to directly craft poisoned data in the input space.","However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space.","Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation.","Empirical results reveal that transfer learning is more vulnerable to our attacks.","Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks."],"url":"http://arxiv.org/abs/2402.12626v1","category":"cs.LG"}
{"created":"2024-02-19 23:08:12","title":"Ultrafast dynamics of a fermion chain in a terahertz field-driven optical cavity","abstract":"We study the effect of a terahertz field-driven single cavity mode for ultrafast control of a fermion chain with dissipation-induced nonlinearity and quadratic coupling to an infrared-active phonon mode. Without photon loss from the cavity, we uncover a first-order phase transition in the nonequilibrium steady state only for the lower phonon-polariton, accompanied by polaritons whose frequency response is asymmetric with respect to the photon frequency due to the direct laser-induced dressing effect on the photon. A weak laser field fails to induce the phase transition but renders the polaritons symmetrical. Finally, we show that sufficiently strong photon loss from the cavity eliminates the polaritons and the associated phase transition. The experimental feasibility of these phenomena is also proposed.","sentences":["We study the effect of a terahertz field-driven single cavity mode for ultrafast control of a fermion chain with dissipation-induced nonlinearity and quadratic coupling to an infrared-active phonon mode.","Without photon loss from the cavity, we uncover a first-order phase transition in the nonequilibrium steady state only for the lower phonon-polariton, accompanied by polaritons whose frequency response is asymmetric with respect to the photon frequency due to the direct laser-induced dressing effect on the photon.","A weak laser field fails to induce the phase transition but renders the polaritons symmetrical.","Finally, we show that sufficiently strong photon loss from the cavity eliminates the polaritons and the associated phase transition.","The experimental feasibility of these phenomena is also proposed."],"url":"http://arxiv.org/abs/2402.12591v1","category":"cond-mat.str-el"}
{"created":"2024-02-19 22:55:56","title":"On The Fourier Coefficients of High-Dimensional Random Geometric Graphs","abstract":"The random geometric graph $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ is formed by sampling $n$ i.i.d. vectors $\\{V_i\\}_{i = 1}^n$ uniformly on $\\mathbb{S}^{d-1}$ and placing an edge between pairs of vertices $i$ and $j$ for which $\\langle V_i,V_j\\rangle \\ge \\tau^p_d,$ where $\\tau^p_d$ is such that the expected density is $p.$ We study the low-degree Fourier coefficients of the distribution $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ and its Gaussian analogue.   Our main conceptual contribution is a novel two-step strategy for bounding Fourier coefficients which we believe is more widely applicable to studying latent space distributions. First, we localize the dependence among edges to few fragile edges. Second, we partition the space of latent vector configurations $(\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p))^{\\otimes n}$ based on the set of fragile edges and on each subset of configurations, we define a noise operator acting independently on edges not incident (in an appropriate sense) to fragile edges.   We apply the resulting bounds to: 1) Settle the low-degree polynomial complexity of distinguishing spherical and Gaussian random geometric graphs from Erdos-Renyi both in the case of observing a complete set of edges and in the non-adaptively chosen mask $\\mathcal{M}$ model recently introduced by [MVW24]; 2) Exhibit a statistical-computational gap for distinguishing $\\mathsf{RGG}$ and the planted coloring model [KVWX23] in a regime when $\\mathsf{RGG}$ is distinguishable from Erdos-Renyi; 3) Reprove known bounds on the second eigenvalue of random geometric graphs.","sentences":["The random geometric graph $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ is formed by sampling $n$ i.i.d. vectors $\\{V_i\\}_{i = 1}^n$ uniformly on $\\mathbb{S}^{d-1}$ and placing an edge between pairs of vertices $i$ and $j$ for which $\\langle V_i,V_j\\rangle \\ge \\tau^p_d,$ where $\\tau^p_d$ is such that the expected density is $p.$ We study the low-degree Fourier coefficients of the distribution $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ and its Gaussian analogue.   ","Our main conceptual contribution is a novel two-step strategy for bounding Fourier coefficients which we believe is more widely applicable to studying latent space distributions.","First, we localize the dependence among edges to few fragile edges.","Second, we partition the space of latent vector configurations $(\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p))^{\\otimes n}$ based on the set of fragile edges and on each subset of configurations, we define a noise operator acting independently on edges not incident (in an appropriate sense) to fragile edges.   ","We apply the resulting bounds to: 1) Settle the low-degree polynomial complexity of distinguishing spherical and Gaussian random geometric graphs from Erdos-Renyi both in the case of observing a complete set of edges and in the non-adaptively chosen mask $\\mathcal{M}$ model recently introduced by [MVW24]; 2) Exhibit a statistical-computational gap for distinguishing $\\mathsf{RGG}$ and the planted coloring model","[KVWX23] in a regime when $\\mathsf{RGG}$ is distinguishable from Erdos-Renyi; 3) Reprove known bounds on the second eigenvalue of random geometric graphs."],"url":"http://arxiv.org/abs/2402.12589v1","category":"math.ST"}
{"created":"2024-02-19 21:35:56","title":"CausalGym: Benchmarking causal interpretability methods on linguistic tasks","abstract":"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.","sentences":["Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons).","At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior.","To help bring these strands of research closer together, we introduce CausalGym.","We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour.","To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS).","We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies.","Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually."],"url":"http://arxiv.org/abs/2402.12560v1","category":"cs.CL"}
{"created":"2024-02-19 20:53:27","title":"Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning","abstract":"Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local sample size). We also develop a theoretical framework for personalized diffusion models, which shows the benefits of collaboration even under heterogeneity. We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective sample amplification for personalized tasks, induced through collaboration, despite data heterogeneity.","sentences":["Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics.","Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored.","We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework.","We develop adaptive algorithms that discover the balance between using limited local data and collaborative information.","We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models.","We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local sample size).","We also develop a theoretical framework for personalized diffusion models, which shows the benefits of collaboration even under heterogeneity.","We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective sample amplification for personalized tasks, induced through collaboration, despite data heterogeneity."],"url":"http://arxiv.org/abs/2402.12537v1","category":"cs.LG"}
{"created":"2024-02-19 20:40:48","title":"Parallel Structures in Pre-training Data Yield In-Context Learning","abstract":"Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data.","sentences":["Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update.","However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts.","In this work, we study what patterns of the pre-training data contribute to ICL.","We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window.","Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL.","We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation).","This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures.","A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data."],"url":"http://arxiv.org/abs/2402.12530v1","category":"cs.CL"}
{"created":"2024-02-19 20:08:13","title":"Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification","abstract":"Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks. Additionally, we assess the method's robustness in continual learning and data removal scenarios. The approach exhibits great promise for bridging the gap between foundation models' performance and challenges tied to data privacy. The source code is available at https://github.com/TobArc/privacy-aware-image-classification-with-kNN.","sentences":["Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes.","Yet, this adaptability is vital for addressing user data privacy concerns.","We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining.","Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability.","We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions.","Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks.","Additionally, we assess the method's robustness in continual learning and data removal scenarios.","The approach exhibits great promise for bridging the gap between foundation models' performance and challenges tied to data privacy.","The source code is available at https://github.com/TobArc/privacy-aware-image-classification-with-kNN."],"url":"http://arxiv.org/abs/2402.12500v1","category":"cs.CV"}
{"created":"2024-02-19 20:06:15","title":"Automated Security Response through Online Learning with Adaptive Conjectures","abstract":"We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Simulation studies based on testbed measurements show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.","sentences":["We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game.","We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0.","This formulation allows us to capture uncertainty about the infrastructure and the intents of the players.","To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout.","We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model.","To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium.","We present our method through an advanced persistent threat use case.","Simulation studies based on testbed measurements show that our method produces effective security strategies that adapt to a changing environment.","We also find that our method enables faster convergence than current reinforcement learning techniques."],"url":"http://arxiv.org/abs/2402.12499v1","category":"cs.GT"}
{"created":"2024-02-19 19:38:10","title":"Exploring the Interplay of Excitatory and Inhibitory Interactions in the Kuramoto Model on Circle Topologies","abstract":"In the field of collective dynamics, the Kuramoto model serves as a benchmark for the investigation of synchronization phenomena. While mean-field approaches and complex networks have been widely studied, the simple topology of a circle is still relatively unexplored, especially in the context of excitatory and inhibitory interactions. In this work, we focus on the dynamics of the Kuramoto model on a circle with positive and negative connections paying attention to the existence of new attractors different from the synchronized state. Using analytical and computational methods, we find that even for identical oscillators, the introduction of inhibitory interactions significantly modifies the structure of the attractors of the system. Our results extend the current understanding of synchronization in simple topologies and open new avenues for the study of collective dynamics in physical systems.","sentences":["In the field of collective dynamics, the Kuramoto model serves as a benchmark for the investigation of synchronization phenomena.","While mean-field approaches and complex networks have been widely studied, the simple topology of a circle is still relatively unexplored, especially in the context of excitatory and inhibitory interactions.","In this work, we focus on the dynamics of the Kuramoto model on a circle with positive and negative connections paying attention to the existence of new attractors different from the synchronized state.","Using analytical and computational methods, we find that even for identical oscillators, the introduction of inhibitory interactions significantly modifies the structure of the attractors of the system.","Our results extend the current understanding of synchronization in simple topologies and open new avenues for the study of collective dynamics in physical systems."],"url":"http://arxiv.org/abs/2402.12481v1","category":"nlin.AO"}
{"created":"2024-02-19 19:10:23","title":"An Adaptive Cubic Regularization quasi-Newton Method on Riemannian Manifolds","abstract":"A quasi-Newton method with cubic regularization is designed for solving Riemannian unconstrained nonconvex optimization problems. The proposed algorithm is fully adaptive with at most ${\\cal O} (\\epsilon_g^{-3/2})$ iterations to achieve a gradient smaller than $\\epsilon_g$ for given $\\epsilon_g$, and at most $\\mathcal O(\\max\\{ \\epsilon_g^{-\\frac{3}{2}}, \\epsilon_H^{-3} \\})$ iterations to reach a second-order stationary point respectively. Notably, the proposed algorithm remains applicable even in cases of the gradient and Hessian of the objective function unknown. Numerical experiments are performed with gradient and Hessian being approximated by forward finite-differences to illustrate the theoretical results and numerical comparison.","sentences":["A quasi-Newton method with cubic regularization is designed for solving Riemannian unconstrained nonconvex optimization problems.","The proposed algorithm is fully adaptive with at most ${\\cal O} (\\epsilon_g^{-3/2})$ iterations to achieve a gradient smaller than $\\epsilon_g$ for given $\\epsilon_g$, and at most $\\mathcal O(\\max\\{ \\epsilon_g^{-\\frac{3}{2}}, \\epsilon_H^{-3} \\})$ iterations to reach a second-order stationary point respectively.","Notably, the proposed algorithm remains applicable even in cases of the gradient and Hessian of the objective function unknown.","Numerical experiments are performed with gradient and Hessian being approximated by forward finite-differences to illustrate the theoretical results and numerical comparison."],"url":"http://arxiv.org/abs/2402.12464v1","category":"math.OC"}
{"created":"2024-02-19 19:01:02","title":"Numerical Challenges in Modeling Gravothermal Collapse in Self-Interacting Dark Matter Halos","abstract":"When dark matter has a large cross section for self scattering, halos can undergo a process known as gravothermal core collapse, where the inner core rapidly increases in density and temperature. To date, several methods have been used to implement Self-Interacting Dark Matter~(SIDM) in N-body codes, but there has been no systematic study of these different methods or their accuracy in the core-collapse phase. In this paper, we compare three different numerical implementations of SIDM, including the standard methods from the GIZMO and Arepo codes, by simulating idealized dwarf halos undergoing significant dark matter self interactions ($\\sigma/m = 50$~cm$^2$/g). When simulating these halos, we also vary the mass resolution, time-stepping criteria, and gravitational force-softening scheme. The various SIDM methods lead to distinct differences in a halo's evolution during the core-collapse phase, as each results in slightly different scattering rates and spurious energy gains/losses. The use of adaptive force softening for gravity can lead to numerical heating that artificially accelerates core collapse, while an insufficiently small simulation time step can cause core evolution to stall or completely reverse. Additionally, particle numbers must be large enough to ensure that the simulated halos are not sensitive to noise in the initial conditions. Even for the highest-resolution simulations tested in this study ($10^6$ particles per halo), we find that variations of order $10\\%$ in collapse time are still present. The results of this work underscore the sensitivity of SIDM modeling on the choice of numerical implementation and motivate a careful study of how these results generalize to halos in a cosmological context.","sentences":["When dark matter has a large cross section for self scattering, halos can undergo a process known as gravothermal core collapse, where the inner core rapidly increases in density and temperature.","To date, several methods have been used to implement Self-Interacting Dark Matter~(SIDM) in N-body codes, but there has been no systematic study of these different methods or their accuracy in the core-collapse phase.","In this paper, we compare three different numerical implementations of SIDM, including the standard methods from the GIZMO and Arepo codes, by simulating idealized dwarf halos undergoing significant dark matter self interactions ($\\sigma/m = 50$~cm$^2$/g).","When simulating these halos, we also vary the mass resolution, time-stepping criteria, and gravitational force-softening scheme.","The various SIDM methods lead to distinct differences in a halo's evolution during the core-collapse phase, as each results in slightly different scattering rates and spurious energy gains/losses.","The use of adaptive force softening for gravity can lead to numerical heating that artificially accelerates core collapse, while an insufficiently small simulation time step can cause core evolution to stall or completely reverse.","Additionally, particle numbers must be large enough to ensure that the simulated halos are not sensitive to noise in the initial conditions.","Even for the highest-resolution simulations tested in this study ($10^6$ particles per halo), we find that variations of order $10\\%$ in collapse time are still present.","The results of this work underscore the sensitivity of SIDM modeling on the choice of numerical implementation and motivate a careful study of how these results generalize to halos in a cosmological context."],"url":"http://arxiv.org/abs/2402.12452v1","category":"astro-ph.CO"}
{"created":"2024-02-19 19:00:00","title":"On Binary Formation from Three Initially Unbound Bodies","abstract":"We explore three-body binary formation (3BBF), the formation of a bound system via gravitational scattering of three initially unbound bodies (3UB), using direct numerical integrations. For the first time, we consider systems with unequal masses, as well as finite-size and post-Newtonian effects. Our analytically derived encounter rates and numerical scattering results reproduce the 3BBF rate predicted by Goodman & Hut (1993) for hard binaries in dense star clusters. We find that 3BBF occurs overwhelmingly through nonresonant encounters and that the two most massive bodies are never the most likely to bind. Instead, 3BBF favors pairing the two least massive bodies (for wide binaries) or the most plus least massive bodies (for hard binaries). 3BBF overwhelmingly favors wide binary formation with super-thermal eccentricities, perhaps helping to explain the eccentric wide binaries observed by Gaia. Hard binaries form much more rarely, but with a thermal eccentricity distribution. The semimajor axis distribution scales cumulatively as $a^3$ for hard and slightly wider binaries. Though mergers are rare between black holes when including relativistic effects, direct collisions occur frequently between main-sequence stars -- more often than hard 3BBF. Yet, these collisions do not significantly suppress hard 3BBF at the low velocity dispersions typical of open or globular clusters. Energy dissipation through gravitational radiation leads to a small probability of a bound, hierarchical triple system forming directly from 3UB.","sentences":["We explore three-body binary formation (3BBF), the formation of a bound system via gravitational scattering of three initially unbound bodies (3UB), using direct numerical integrations.","For the first time, we consider systems with unequal masses, as well as finite-size and post-Newtonian effects.","Our analytically derived encounter rates and numerical scattering results reproduce the 3BBF rate predicted by Goodman & Hut (1993) for hard binaries in dense star clusters.","We find that 3BBF occurs overwhelmingly through nonresonant encounters and that the two most massive bodies are never the most likely to bind.","Instead, 3BBF favors pairing the two least massive bodies (for wide binaries) or the most plus least massive bodies (for hard binaries).","3BBF overwhelmingly favors wide binary formation with super-thermal eccentricities, perhaps helping to explain the eccentric wide binaries observed by Gaia.","Hard binaries form much more rarely, but with a thermal eccentricity distribution.","The semimajor axis distribution scales cumulatively as $a^3$ for hard and slightly wider binaries.","Though mergers are rare between black holes when including relativistic effects, direct collisions occur frequently between main-sequence stars -- more often than hard 3BBF.","Yet, these collisions do not significantly suppress hard 3BBF at the low velocity dispersions typical of open or globular clusters.","Energy dissipation through gravitational radiation leads to a small probability of a bound, hierarchical triple system forming directly from 3UB."],"url":"http://arxiv.org/abs/2402.12429v1","category":"astro-ph.SR"}
{"created":"2024-02-20 18:59:57","title":"How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey","abstract":"Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.","sentences":["Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments.","This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations.","Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields.","It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges."],"url":"http://arxiv.org/abs/2402.13255v1","category":"cs.CV"}
{"created":"2024-02-20 18:51:59","title":"Some obstructions to positive scalar curvature on a noncompact manifold","abstract":"We give obstructions for a noncompact manifold to admit a complete Riemannian metric with (nonuniformly) positive scalar curvature. We treat both the finite volume and infinite volume cases.","sentences":["We give obstructions for a noncompact manifold to admit a complete Riemannian metric with (nonuniformly) positive scalar curvature.","We treat both the finite volume and infinite volume cases."],"url":"http://arxiv.org/abs/2402.13239v1","category":"math.DG"}
{"created":"2024-02-20 18:50:25","title":"Towards audio language modeling -- an overview","abstract":"Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.","sentences":["Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency.","Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs).","Numerous high-performance neural audio codecs and codec-based LMs have been developed.","The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs."],"url":"http://arxiv.org/abs/2402.13236v1","category":"eess.AS"}
{"created":"2024-02-20 18:44:31","title":"On Type II blowups of axisymmetric solutions to the Navier-Stokes equations","abstract":"In the note, the Euler scaling is used to study a certain scenario of potential Type II blowups of axisymmetric solutions to the Navier-Stokes equations.","sentences":["In the note, the Euler scaling is used to study a certain scenario of potential Type II blowups of axisymmetric solutions to the Navier-Stokes equations."],"url":"http://arxiv.org/abs/2402.13229v1","category":"math.AP"}
{"created":"2024-02-20 18:08:11","title":"Polygonal surfaces in pseudo-hyperbolic spaces","abstract":"A polygonal surface in the pseudo-hyperbolic space H 2,n is a complete maximal surface bounded by a lightlike polygon in the Einstein universe Ein 1,n with finitely many vertices. In this article, we give several characterizations of them. Polygonal surfaces are characterized by finiteness of their total curvature, by asymptotic flatness, and also by the fact of having parabolic type and polynomial quartic differential. Our result relies on a comparison between three ideal boundaries associated with a maximal surface, corresponding to three distinct distances naturally defined on the maximal surface.","sentences":["A polygonal surface in the pseudo-hyperbolic space H 2,n is a complete maximal surface bounded by a lightlike polygon in the Einstein universe Ein 1,n with finitely many vertices.","In this article, we give several characterizations of them.","Polygonal surfaces are characterized by finiteness of their total curvature, by asymptotic flatness, and also by the fact of having parabolic type and polynomial quartic differential.","Our result relies on a comparison between three ideal boundaries associated with a maximal surface, corresponding to three distinct distances naturally defined on the maximal surface."],"url":"http://arxiv.org/abs/2402.13197v1","category":"math.DG"}
{"created":"2024-02-20 17:53:02","title":"Effects of pair freeze-out on photon distributions in BBN epoch","abstract":"We investigate the evolution of non-extensivity in the photon distribution during the Big Bang Nucleosynthesis (BBN) epoch using Tsallis statistics. Assuming a minimal deviation from the Planck distribution, we construct the perturbed Boltzmann equation for photons, including the collision terms for pair creation and annihilation processes. We analyze the possibility that these collisions could cause a slight increase in the number of high-frequency photons within the BBN era, and consequently, the primordial plasma might be temporarily placed in a state of chemical non-equilibrium. We also discuss the restoration of the photon distribution to an equilibrium state as the Universe enters the matter-dominated era. These findings, which suggest possible changes in the photon distribution during the epoch between the BBN and the recombination, offer insights that support the previously proposed ansatz solution to the primordial lithium problem in arXiv:1812.09472.","sentences":["We investigate the evolution of non-extensivity in the photon distribution during the Big Bang Nucleosynthesis (BBN) epoch using Tsallis statistics.","Assuming a minimal deviation from the Planck distribution, we construct the perturbed Boltzmann equation for photons, including the collision terms for pair creation and annihilation processes.","We analyze the possibility that these collisions could cause a slight increase in the number of high-frequency photons within the BBN era, and consequently, the primordial plasma might be temporarily placed in a state of chemical non-equilibrium.","We also discuss the restoration of the photon distribution to an equilibrium state as the Universe enters the matter-dominated era.","These findings, which suggest possible changes in the photon distribution during the epoch between the BBN and the recombination, offer insights that support the previously proposed ansatz solution to the primordial lithium problem in arXiv:1812.09472."],"url":"http://arxiv.org/abs/2402.13186v1","category":"astro-ph.CO"}
{"created":"2024-02-20 17:36:56","title":"Geometric structures on the quaternionic unit ball and slice regular M\u00f6bius transformations","abstract":"Building from ideas of hypercomplex analysis on the quaternionic unit ball, we introduce Hermitian, Riemannian and K\\\"ahler-like structures on the latter. These are built from the so-called regular M\\\"obius transformations. Such geometric structures are shown to be natural generalizations of those from the complex setup. Our structures can be considered as more natural, from the analytic viewpoint, than the usual quaternionic hyperbolic geometry. Furthermore, our constructions provide solutions to problems not achieved by hyper-K\\\"ahler and quaternion-K\\\"ahler geometries when applied to the quaternionic unit ball.","sentences":["Building from ideas of hypercomplex analysis on the quaternionic unit ball, we introduce Hermitian, Riemannian and K\\\"ahler-like structures on the latter.","These are built from the so-called regular M\\\"obius transformations.","Such geometric structures are shown to be natural generalizations of those from the complex setup.","Our structures can be considered as more natural, from the analytic viewpoint, than the usual quaternionic hyperbolic geometry.","Furthermore, our constructions provide solutions to problems not achieved by hyper-K\\\"ahler and quaternion-K\\\"ahler geometries when applied to the quaternionic unit ball."],"url":"http://arxiv.org/abs/2402.13175v1","category":"math.CV"}
{"created":"2024-02-20 17:29:19","title":"Nonequilibrium fluctuations of chemical reaction networks at criticality: The Schl\u00f6gl model as paradigmatic case","abstract":"Chemical reaction networks can undergo nonequilibrium phase transitions upon variation of external control parameters like the chemical potential of a species. We investigate the flux in the associated chemostats that is proportional to the entropy production and its critical fluctuations within the Schl\\\"ogl model. Numerical simulations show that the corresponding diffusion coefficient diverges at the critical point as a function of system size. In the vicinity of the critical point, the diffusion coefficient follows a scaling form. We develop an analytical approach based on the chemical Langevin equation and van Kampen's system size expansion that yields the corresponding exponents in the monostable regime. In the bistable regime, we rely on a two-state approximation in order to analytically describe the critical behavior.","sentences":["Chemical reaction networks can undergo nonequilibrium phase transitions upon variation of external control parameters like the chemical potential of a species.","We investigate the flux in the associated chemostats that is proportional to the entropy production and its critical fluctuations within the Schl\\\"ogl model.","Numerical simulations show that the corresponding diffusion coefficient diverges at the critical point as a function of system size.","In the vicinity of the critical point, the diffusion coefficient follows a scaling form.","We develop an analytical approach based on the chemical Langevin equation and van Kampen's system size expansion that yields the corresponding exponents in the monostable regime.","In the bistable regime, we rely on a two-state approximation in order to analytically describe the critical behavior."],"url":"http://arxiv.org/abs/2402.13168v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 17:28:57","title":"Convergence Rate for Moderate Interaction particles and Application to Mean Field Games","abstract":"We study two interacting particle systems, both modeled as a system of $N$ stochastic differential equations driven by Brownian motions with singular kernels and moderate interaction. We show a quantitative result where the convergence rate depends on the moderate scaling parameter, the regularity of the solution of the limit equation and the dimension. Our approach is based on the techniques of stochastic calculus, some properties of Besov and Triebel-Lizorkin space, and the semigroup approach introduced in [9].","sentences":["We study two interacting particle systems, both modeled as a system of $N$ stochastic differential equations driven by Brownian motions with singular kernels and moderate interaction.","We show a quantitative result where the convergence rate depends on the moderate scaling parameter, the regularity of the solution of the limit equation and the dimension.","Our approach is based on the techniques of stochastic calculus, some properties of Besov and Triebel-Lizorkin space, and the semigroup approach introduced in [9]."],"url":"http://arxiv.org/abs/2402.13167v1","category":"math.PR"}
{"created":"2024-02-20 17:27:26","title":"The universal thermodynamic properties of Extremely Compact Objects","abstract":"An extremely compact object (ECO) is defined as a quantum object without horizon, whose radius is just a small distance $s$ outside its Schwarzschild radius. We show that any ECO of mass $M$ in $d+1$ dimensions with $s\\ll (M/m_p)^{2/(d-2)(d+1)}l_p$ must have (at leading order) the same thermodynamic properties -- temperature, entropy and radiation rates -- as the corresponding semiclassical black hole of mass $M$. An essential aspect of the argument involves showing that the Tolman-Oppenheimer-Volkoff equation has no consistent solution in the region just outside the ECO surface, unless this region is filled with radiation at the (appropriately blueshifted) Hawking temperature. In string theory it has been found that black hole microstates are fuzzballs -- objects with no horizon -- which are expected to have a radius that is only a little larger than the horizon radius. Thus the arguments of this paper provide a nice closure to the fuzzball paradigm: the absence of a horizon removes the information paradox, and the thermodynamic properties of the semiclassical hole are nonetheless recovered to an excellent approximation.","sentences":["An extremely compact object (ECO) is defined as a quantum object without horizon, whose radius is just a small distance $s$ outside its Schwarzschild radius.","We show that any ECO of mass $M$ in $d+1$ dimensions with $s\\ll (M/m_p)^{2/(d-2)(d+1)}l_p$ must have (at leading order) the same thermodynamic properties -- temperature, entropy and radiation rates -- as the corresponding semiclassical black hole of mass $M$. An essential aspect of the argument involves showing that the Tolman-Oppenheimer-Volkoff equation has no consistent solution in the region just outside the ECO surface, unless this region is filled with radiation at the (appropriately blueshifted)","Hawking temperature.","In string theory it has been found that black hole microstates are fuzzballs -- objects with no horizon -- which are expected to have a radius that is only a little larger than the horizon radius.","Thus the arguments of this paper provide a nice closure to the fuzzball paradigm: the absence of a horizon removes the information paradox, and the thermodynamic properties of the semiclassical hole are nonetheless recovered to an excellent approximation."],"url":"http://arxiv.org/abs/2402.13166v1","category":"hep-th"}
{"created":"2024-02-20 17:24:24","title":"Asymptotic quantization on Riemannian manifolds via covering growth estimates","abstract":"The quantization problem looks for best approximations of a probability measure on a given metric space by finitely many points, where the approximation error is measured with respect to the Wasserstein distance. On particular smooth domains, such as $\\mathbb{R}^d$ or complete Riemannian manifolds, the quantization error is known to decay polynomially as the number of points is taken to infinity, provided the measure satisfies an integral condition which controls the amount of mass outside compact sets. On Riemannian manifolds, the existing integral condition involves a quantity measuring the growth of the exponential map, for which the only available estimates are in terms of lower bounds on sectional curvature.   In this paper, we provide a more general integral condition for the asymptotics of the quantization error on Riemannian manifolds, given in terms of the growth of the covering numbers of spheres, which is purely metric in nature and concerns only the large-scale growth of the manifold. We further estimate the covering growth of manifolds in two particular cases, namely lower bounds on the Ricci curvature and geometric group actions by a discrete group of isometries. These estimates can themselves generalize beyond manifolds, and hint at a future treatment of asymptotic quantization also on non-smooth metric measure spaces.","sentences":["The quantization problem looks for best approximations of a probability measure on a given metric space by finitely many points, where the approximation error is measured with respect to the Wasserstein distance.","On particular smooth domains, such as $\\mathbb{R}^d$ or complete Riemannian manifolds, the quantization error is known to decay polynomially as the number of points is taken to infinity, provided the measure satisfies an integral condition which controls the amount of mass outside compact sets.","On Riemannian manifolds, the existing integral condition involves a quantity measuring the growth of the exponential map, for which the only available estimates are in terms of lower bounds on sectional curvature.   ","In this paper, we provide a more general integral condition for the asymptotics of the quantization error on Riemannian manifolds, given in terms of the growth of the covering numbers of spheres, which is purely metric in nature and concerns only the large-scale growth of the manifold.","We further estimate the covering growth of manifolds in two particular cases, namely lower bounds on the Ricci curvature and geometric group actions by a discrete group of isometries.","These estimates can themselves generalize beyond manifolds, and hint at a future treatment of asymptotic quantization also on non-smooth metric measure spaces."],"url":"http://arxiv.org/abs/2402.13164v1","category":"math.MG"}
{"created":"2024-02-20 17:22:37","title":"Nonadiabatic Dynamics of Molecules Interacting with Metal Surfaces: An Approach Based on Langevin Dynamics and the Hierarchical Equations of Motion","abstract":"A novel mixed quantum-classical approach to simulating nonadiabatic dynamics of molecules at metal surfaces is presented. The method combines the numerically exact hierarchical equations of motion approach for the quantum electronic degrees of freedom with Langevin dynamics for the classical degrees of freedom, namely low-frequency vibrational modes within the molecule. The approach extends previous mixed quantum-classical methods based on Langevin equations to models containing strong electron-electron or quantum electronic-vibrational interactions, while maintaining a nonperturbative and non-Markovian treatment of the molecule-metal coupling. To demonstrate the approach, nonequilibrium transport observables are calculated for a molecular nanojunction containing strong interactions.","sentences":["A novel mixed quantum-classical approach to simulating nonadiabatic dynamics of molecules at metal surfaces is presented.","The method combines the numerically exact hierarchical equations of motion approach for the quantum electronic degrees of freedom with Langevin dynamics for the classical degrees of freedom, namely low-frequency vibrational modes within the molecule.","The approach extends previous mixed quantum-classical methods based on Langevin equations to models containing strong electron-electron or quantum electronic-vibrational interactions, while maintaining a nonperturbative and non-Markovian treatment of the molecule-metal coupling.","To demonstrate the approach, nonequilibrium transport observables are calculated for a molecular nanojunction containing strong interactions."],"url":"http://arxiv.org/abs/2402.13161v2","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 17:22:36","title":"Measurement-induced phase transitions by matrix product states scaling","abstract":"We study the time evolution of long quantum spin chains subjected to continuous monitoring via matrix product states (MPS) at fixed bond dimension, with the Time-Dependent Variational Principle (TDVP) algorithm. The latter gives an effective classical non-linear evolution with a conserved charge, which approximates the real quantum evolution up to an error. We show that the error rate displays a phase transition in the monitoring strength, which can be well detected by scaling analysis with relatively low values of bond dimensions. The method allows for an efficient numerical determination of the critical measurement-induced phase transition parameters in many-body quantum systems. Moreover, in the presence of U(1) global spin charge, we show the existence of a charge-sharpening transition well separated from the entanglement transition which we detect by studying the charge fluctuations of a local sub-part of the system at very large times. Our work substantiates the TDVP time evolution as a method to identify measured-induced phase transitions in systems of arbitrary dimensions and sizes.","sentences":["We study the time evolution of long quantum spin chains subjected to continuous monitoring via matrix product states (MPS) at fixed bond dimension, with the Time-Dependent Variational Principle (TDVP) algorithm.","The latter gives an effective classical non-linear evolution with a conserved charge, which approximates the real quantum evolution up to an error.","We show that the error rate displays a phase transition in the monitoring strength, which can be well detected by scaling analysis with relatively low values of bond dimensions.","The method allows for an efficient numerical determination of the critical measurement-induced phase transition parameters in many-body quantum systems.","Moreover, in the presence of U(1) global spin charge, we show the existence of a charge-sharpening transition well separated from the entanglement transition which we detect by studying the charge fluctuations of a local sub-part of the system at very large times.","Our work substantiates the TDVP time evolution as a method to identify measured-induced phase transitions in systems of arbitrary dimensions and sizes."],"url":"http://arxiv.org/abs/2402.13160v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 17:13:51","title":"Improved error bounds for approximations of high-frequency wave propagation in nonlinear dispersive media","abstract":"High-frequency wave propagation is often modelled by nonlinear Friedrichs systems where both the differential equation and the initial data contain the inverse of a small parameter $\\varepsilon$, which causes oscillations with wavelengths proportional to $\\varepsilon$ in time and space. A prominent example is the Maxwell--Lorentz system, which is a well-established model for the propagation of light in nonlinear media. In diffractive optics, such problems have to be solved on long time intervals with length proportional to $1/\\varepsilon$. Approximating the solution of such a problem numerically with a standard method is hopeless, because traditional methods require an extremely fine resolution in time and space, which entails unacceptable computational costs. A possible alternative is to replace the original problem by a new system of PDEs which is more suitable for numerical computations but still yields a sufficiently accurate approximation. Such models are often based on the \\emph{slowly varying envelope approximation} or generalizations thereof. Results in the literature state that the error of the slowly varying envelope approximation is of $\\mathcal{O}(\\varepsilon)$. In this work, however, we prove that the error is even proportional to $\\varepsilon^2$, which is a substantial improvement, and which explains the error behavior observed in numerical experiments. For a higher-order generalization of the slowly varying envelope approximation we improve the error bound from $\\mathcal{O}(\\varepsilon^2)$ to $\\mathcal{O}(\\varepsilon^3)$. Both proofs are based on a careful analysis of the nonlinear interaction between oscillatory and non-oscillatory error terms, and on \\textit{a priori} bounds for certain ``parts'' of the approximations which are defined by suitable projections.","sentences":["High-frequency wave propagation is often modelled by nonlinear Friedrichs systems where both the differential equation and the initial data contain the inverse of a small parameter $\\varepsilon$, which causes oscillations with wavelengths proportional to $\\varepsilon$ in time and space.","A prominent example is the Maxwell--Lorentz system, which is a well-established model for the propagation of light in nonlinear media.","In diffractive optics, such problems have to be solved on long time intervals with length proportional to $1/\\varepsilon$. Approximating the solution of such a problem numerically with a standard method is hopeless, because traditional methods require an extremely fine resolution in time and space, which entails unacceptable computational costs.","A possible alternative is to replace the original problem by a new system of PDEs which is more suitable for numerical computations but still yields a sufficiently accurate approximation.","Such models are often based on the \\emph{slowly varying envelope approximation} or generalizations thereof.","Results in the literature state that the error of the slowly varying envelope approximation is of $\\mathcal{O}(\\varepsilon)$. In this work, however, we prove that the error is even proportional to $\\varepsilon^2$, which is a substantial improvement, and which explains the error behavior observed in numerical experiments.","For a higher-order generalization of the slowly varying envelope approximation we improve the error bound from $\\mathcal{O}(\\varepsilon^2)$ to $\\mathcal{O}(\\varepsilon^3)$. Both proofs are based on a careful analysis of the nonlinear interaction between oscillatory and non-oscillatory error terms, and on \\textit{a priori} bounds for certain ``parts'' of the approximations which are defined by suitable projections."],"url":"http://arxiv.org/abs/2402.13155v1","category":"math.AP"}
{"created":"2024-02-20 16:59:03","title":"Neural Network Diffusion","abstract":"Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.","sentences":["Diffusion models have achieved remarkable success in image and video generation.","In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}.","Our approach is simple, utilizing an autoencoder and a standard latent diffusion model.","The autoencoder extracts latent representations of a subset of the trained network parameters.","A diffusion model is then trained to synthesize these latent parameter representations from random noise.","It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters.","Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost.","Notably, we empirically find that the generated models perform differently with the trained networks.","Our results encourage more exploration on the versatile use of diffusion models."],"url":"http://arxiv.org/abs/2402.13144v1","category":"cs.LG"}
{"created":"2024-02-20 16:40:47","title":"Non-local time evolution equation with singular integral and its application to traffic flow model","abstract":"We consider an integro-differential equation model for traffic flow which is an extension of the Burgers equation model. To discuss the model, we first examine general settings for integrable integro-differential equations and find that they are obtained through a simple residue formula from integrable eqations in a complex domain. As demonstration of the efficiency of this approach, we list several integrable equations including a difference equation with double singular integral and an equation with elliptic singular integral. Then, we discuss the traffic model with singular integral and show that the model exhibits interaction between free flow region and congested region depending on the parameter of non-locality.","sentences":["We consider an integro-differential equation model for traffic flow which is an extension of the Burgers equation model.","To discuss the model, we first examine general settings for integrable integro-differential equations and find that they are obtained through a simple residue formula from integrable eqations in a complex domain.","As demonstration of the efficiency of this approach, we list several integrable equations including a difference equation with double singular integral and an equation with elliptic singular integral.","Then, we discuss the traffic model with singular integral and show that the model exhibits interaction between free flow region and congested region depending on the parameter of non-locality."],"url":"http://arxiv.org/abs/2402.13128v1","category":"nlin.SI"}
{"created":"2024-02-20 16:32:47","title":"Embedded minimal surfaces in $\\mathbb{S}^3$ and $\\mathbb{B}^3$ via equivariant eigenvalue optimization","abstract":"In 1970, Lawson solved the topological realization problem for minimal surfaces in the sphere, showing that any closed orientable surface can be minimally embedded in $\\mathbb{S}^3$. The analogous problem for surfaces with boundary was posed by Fraser and Li in 2014, and it has attracted much attention in recent years, stimulating the development of many new constructions for free boundary minimal surfaces. In this paper, we resolve this problem by showing that any compact orientable surface with boundary can be embedded in $\\mathbb{B}^3$ as a free boundary minimal surface with area below $2\\pi$. Furthermore, we show that the number of minimal surfaces in $\\mathbb{S}^3$ of prescribed topology and area below $8\\pi$, and the number of free boundary minimal surfaces in $\\mathbb{B}^3$ with prescribed topology and area below $2\\pi$, grow at least linearly with the genus. This is achieved via a new method for producing minimal surfaces of prescribed topology in low-dimensional balls and spheres, based on the optimization of Laplace and Steklov eigenvalues in the presence of a discrete symmetry group.   As a key ingredient, we develop new techniques for proving the existence of maximizing metrics, which can be used to resolve the existence problem in many symmetric situations and provide at least partial existence results for classical eigenvalue optimization problems.","sentences":["In 1970, Lawson solved the topological realization problem for minimal surfaces in the sphere, showing that any closed orientable surface can be minimally embedded in $\\mathbb{S}^3$. The analogous problem for surfaces with boundary was posed by Fraser and Li in 2014, and it has attracted much attention in recent years, stimulating the development of many new constructions for free boundary minimal surfaces.","In this paper, we resolve this problem by showing that any compact orientable surface with boundary can be embedded in $\\mathbb{B}^3$ as a free boundary minimal surface with area below $2\\pi$. Furthermore, we show that the number of minimal surfaces in $\\mathbb{S}^3$ of prescribed topology and area below $8\\pi$, and the number of free boundary minimal surfaces in $\\mathbb{B}^3$ with prescribed topology and area below $2\\pi$, grow at least linearly with the genus.","This is achieved via a new method for producing minimal surfaces of prescribed topology in low-dimensional balls and spheres, based on the optimization of Laplace and Steklov eigenvalues in the presence of a discrete symmetry group.   ","As a key ingredient, we develop new techniques for proving the existence of maximizing metrics, which can be used to resolve the existence problem in many symmetric situations and provide at least partial existence results for classical eigenvalue optimization problems."],"url":"http://arxiv.org/abs/2402.13121v1","category":"math.DG"}
{"created":"2024-02-20 16:01:42","title":"On the Stability of Gradient Descent for Large Learning Rate","abstract":"There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset in parameter space. Additionally, we prove that if the step-size is too big, then the set of initializations from which gradient descent converges to a critical point has measure zero.","sentences":["There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate).","Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets.","In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption.","More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset in parameter space.","Additionally, we prove that if the step-size is too big, then the set of initializations from which gradient descent converges to a critical point has measure zero."],"url":"http://arxiv.org/abs/2402.13108v1","category":"cs.LG"}
{"created":"2024-02-20 16:01:39","title":"On Generalization Bounds for Deep Compound Gaussian Neural Networks","abstract":"Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error bound is formulated by bounding the Rademacher complexity of the class of compound Gaussian network estimates with Dudley's integral. Under realistic conditions, we show that, at worst, the generalization error scales $\\mathcal{O}(n\\sqrt{\\ln(n)})$ in the signal dimension and $\\mathcal{O}(($Network Size$)^{3/2})$ in network size.","sentences":["Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm.","Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks.","An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs.","These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data.","In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior.","These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems.","The generalization error bound is formulated by bounding the Rademacher complexity of the class of compound Gaussian network estimates with Dudley's integral.","Under realistic conditions, we show that, at worst, the generalization error scales $\\mathcal{O}(n\\sqrt{\\ln(n)})$ in the signal dimension and $\\mathcal{O}(($Network Size$)^{3/2})$ in network size."],"url":"http://arxiv.org/abs/2402.13106v1","category":"stat.ML"}
{"created":"2024-02-20 15:54:24","title":"A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations","abstract":"Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations. However, the computational costs stand in the way of the practical application of this approach. The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point. A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models. In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step. Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest. We achieve this for an elasto-plastic material by predicting full-field microscopic strains using a graph neural network (GNN) while retaining the microscopic constitutive material model to obtain the stresses. This hybrid data-physics graph-based approach avoids the high dimensionality originating from predicting full-field responses while allowing non-locality to arise. By training the GNN on a variety of meshes, it learns to generalize to unseen meshes, allowing a single model to be used for a range of microstructures. The embedded microscopic constitutive model in the GNN implicitly tracks history-dependent variables and leads to improved accuracy. We demonstrate for several challenging scenarios that the surrogate can predict complex macroscopic stress-strain paths. As the computation time of our method scales favorably with the number of elements in the microstructure compared to the FE method, our method can significantly accelerate FE2 simulations.","sentences":["Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations.","However, the computational costs stand in the way of the practical application of this approach.","The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point.","A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models.","In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step.","Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest.","We achieve this for an elasto-plastic material by predicting full-field microscopic strains using a graph neural network (GNN) while retaining the microscopic constitutive material model to obtain the stresses.","This hybrid data-physics graph-based approach avoids the high dimensionality originating from predicting full-field responses while allowing non-locality to arise.","By training the GNN on a variety of meshes, it learns to generalize to unseen meshes, allowing a single model to be used for a range of microstructures.","The embedded microscopic constitutive model in the GNN implicitly tracks history-dependent variables and leads to improved accuracy.","We demonstrate for several challenging scenarios that the surrogate can predict complex macroscopic stress-strain paths.","As the computation time of our method scales favorably with the number of elements in the microstructure compared to the FE method, our method can significantly accelerate FE2 simulations."],"url":"http://arxiv.org/abs/2402.13101v1","category":"cs.LG"}
{"created":"2024-02-20 15:35:40","title":"Contractivity of neural ODEs: an eigenvalue optimization problem","abstract":"We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs. When looking at contractivity properties of a one layer weight-tied neural ODE $\\dot{u}(t)=\\sigma(Au(t)+b)$ (with $u,b \\in {\\mathbb R}^n$, $A$ is a given $n \\times n$ matrix, $\\sigma : {\\mathbb R} \\to {\\mathbb R}^+$ denotes an activation function and for a vector $z \\in {\\mathbb R}^n$, $\\sigma(z) \\in {\\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\\mathrm{diag}}(D) \\in \\sigma'({\\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\\chi\\subseteq \\mathbb [0,\\infty)$ such that the logarithmic norm $\\mu(DA) \\le c$ for all diagonal matrices $D$ with $D_{ii}\\in \\chi$. We propose a two-level nested methodology: an inner level where, for a given $\\chi$, we compute an optimizer $D^\\star(\\chi)$ by a gradient system approach, and an outer level where we tune $\\chi$ so that the value $c$ is reached by $\\mu(D^\\star(\\chi)A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\\dot{u}(t) = \\sigma( A_k(t) \\ldots \\sigma ( A_{1}(t) u(t) + b_{1}(t) ) \\ldots + b_{k}(t) )$ and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset.","sentences":["We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs.","When looking at contractivity properties of a one layer weight-tied neural ODE $\\dot{u}(t)=\\sigma(Au(t)+b)$ (with $u,b \\in {\\mathbb R}^n$, $A$ is a given $n \\times n$ matrix, $\\sigma : {\\mathbb R} \\to {\\mathbb R}^+$ denotes an activation function and for a vector $z \\in {\\mathbb R}^n$, $\\sigma(z) \\in {\\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\\mathrm{diag}}(D) \\in \\sigma'({\\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\\chi\\subseteq \\mathbb [0,\\infty)$ such that the logarithmic norm $\\mu(DA) \\le c$ for all diagonal matrices $D$ with $D_{ii}\\in \\chi$. We propose a two-level nested methodology: an inner level where, for a given $\\chi$, we compute an optimizer $D^\\star(\\chi)$ by a gradient system approach, and an outer level where we tune $\\chi$ so that the value $c$ is reached by $\\mu(D^\\star(\\chi)A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\\dot{u}(t) = \\sigma( A_k(t)","\\ldots \\sigma ( A_{1}(t) u(t) + b_{1}(t) )","\\ldots + b_{k}(t) )","$","and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset."],"url":"http://arxiv.org/abs/2402.13092v2","category":"math.NA"}
{"created":"2024-02-20 15:29:49","title":"How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning","abstract":"We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical bound derived even under the strongest audit setup.   The gap found is not a fluke. Our subsequent study provides an improved privacy result for private hyper-parameter tuning due to its distinct properties. Our privacy results are also more generalizable compared to prior analyses that are only easily applicable in specific setups.","sentences":["We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several.","Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood.","Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   ","This paper contributes both positive and negative answers to this question.","Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense.","However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds.","This is first demonstrated by applying privacy audit on the tuning process.","Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical bound derived even under the strongest audit setup.   ","The gap found is not a fluke.","Our subsequent study provides an improved privacy result for private hyper-parameter tuning due to its distinct properties.","Our privacy results are also more generalizable compared to prior analyses that are only easily applicable in specific setups."],"url":"http://arxiv.org/abs/2402.13087v1","category":"cs.LG"}
{"created":"2024-02-20 15:10:02","title":"Radiation back-reaction during dark-matter freeze-out via metastable bound states","abstract":"The formation and decay of metastable bound states can deplete significantly the density of multi-TeV thermal-relic dark matter. The effect depends on the interplay of bound-state formation, ionisation, transition and decay processes. Existing calculations take into account bound-state ionisation and excitations due to the radiation of the thermal bath. However, the dynamics of Hydrogen recombination suggests that the resonant radiation produced in bound-state formation or de-excitations may backreact, ionising or exciting the bound states thus impeding recombination. In this paper we examine this effect in the context of dark-matter freeze-out. To this end, we employ the generalised Saha equilibrium equation for metastable bound states, and discuss its salient features. We show that, in sharp contrast to Hydrogen recombination, the radiation produced during dark matter freeze-out is more likely to thermalise or redshift, rather than ionise or excite the metastable bound states. This holds not only for the low-energy (resonant) radiation produced in bound-state formation and transition processes, but also for the high-energy radiation produced in dark-matter annihilations and bound-state decays. While our computations are carried out in a minimal dark $U(1)$ model, our conclusions only strengthen in more complex models.","sentences":["The formation and decay of metastable bound states can deplete significantly the density of multi-TeV thermal-relic dark matter.","The effect depends on the interplay of bound-state formation, ionisation, transition and decay processes.","Existing calculations take into account bound-state ionisation and excitations due to the radiation of the thermal bath.","However, the dynamics of Hydrogen recombination suggests that the resonant radiation produced in bound-state formation or de-excitations may backreact, ionising or exciting the bound states thus impeding recombination.","In this paper we examine this effect in the context of dark-matter freeze-out.","To this end, we employ the generalised Saha equilibrium equation for metastable bound states, and discuss its salient features.","We show that, in sharp contrast to Hydrogen recombination, the radiation produced during dark matter freeze-out is more likely to thermalise or redshift, rather than ionise or excite the metastable bound states.","This holds not only for the low-energy (resonant) radiation produced in bound-state formation and transition processes, but also for the high-energy radiation produced in dark-matter annihilations and bound-state decays.","While our computations are carried out in a minimal dark $U(1)$ model, our conclusions only strengthen in more complex models."],"url":"http://arxiv.org/abs/2402.13069v1","category":"hep-ph"}
{"created":"2024-02-20 14:35:54","title":"Optical appearance of black holes surrounded by a dark matter halo","abstract":"Black holes in General Relativity are described by space-time metrics that are simpler in comparison to non-vacuum compact objects. However, given the universality of the gravitational pull, it is expected that dark matter accumulates around astrophysical black holes, which can have an impact in the overall gravitational field, especially at galactic centers, and induce non-negligible effects in their observational imprints. In this work we study the optical appearance of a spherically symmetric black hole both when orbited by isotropically emitting light sources and when surrounded by a (geometrically and optically thin) accretion disk, while immersed in a dark matter halo. The black hole geometry plus the dark matter halo come as a solution of Einstein's field equations coupled to an anisotropic fluid whose density component follows a Hermquist-type distribution. Even in situations in which the geodesic description differs profoundly from the isolated black hole case, we find minor modifications to the primary and secondary tracks of the isotropic orbiting sources, and to the width, location, and relative luminosity of the corresponding photon rings as compared to the Schwarzschild black hole at equal black hole mass and emission models. This fact troubles distinguishing between both geometries using present observations of very-long baseline interferometry.","sentences":["Black holes in General Relativity are described by space-time metrics that are simpler in comparison to non-vacuum compact objects.","However, given the universality of the gravitational pull, it is expected that dark matter accumulates around astrophysical black holes, which can have an impact in the overall gravitational field, especially at galactic centers, and induce non-negligible effects in their observational imprints.","In this work we study the optical appearance of a spherically symmetric black hole both when orbited by isotropically emitting light sources and when surrounded by a (geometrically and optically thin) accretion disk, while immersed in a dark matter halo.","The black hole geometry plus the dark matter halo come as a solution of Einstein's field equations coupled to an anisotropic fluid whose density component follows a Hermquist-type distribution.","Even in situations in which the geodesic description differs profoundly from the isolated black hole case, we find minor modifications to the primary and secondary tracks of the isotropic orbiting sources, and to the width, location, and relative luminosity of the corresponding photon rings as compared to the Schwarzschild black hole at equal black hole mass and emission models.","This fact troubles distinguishing between both geometries using present observations of very-long baseline interferometry."],"url":"http://arxiv.org/abs/2402.13047v1","category":"gr-qc"}
{"created":"2024-02-20 14:31:47","title":"Conversion of Emitted Axionic Dark Matter to Photons for Non-Rotating Magnetized Neutron Stars","abstract":"We attempt to find the impact of a modified Tolman Oppenheimer Volkoff (TOV) system of equations on the luminosities of direct photons, neutrinos and axions for a particular axion mass in the presence of a magnetic field. We employ two different equation of states (EoSs) namely APR and FPS to generate the profiles of mass and pressure for spherically symmetric and non-rotating Neutron stars (NSs). We then compute the axions and neutrino emission rates by employing the Cooper-pair-breaking and formation process (PBF) in the core using the NSCool code. We also examine the possibility of axion to photon conversion in the magnetosphere of NSs. Furthermore, we investigate the impact of the magnetic field on the actual observables, such as the energy spectrum of axions and axion-converted photon flux for three different NSs. Our comparative study indicates that axions energy spectrum and axion-converted photon flux changes significantly due to an intense magnetic field.","sentences":["We attempt to find the impact of a modified Tolman Oppenheimer Volkoff (TOV) system of equations on the luminosities of direct photons, neutrinos and axions for a particular axion mass in the presence of a magnetic field.","We employ two different equation of states (EoSs) namely APR and FPS to generate the profiles of mass and pressure for spherically symmetric and non-rotating Neutron stars (NSs).","We then compute the axions and neutrino emission rates by employing the Cooper-pair-breaking and formation process (PBF) in the core using the NSCool code.","We also examine the possibility of axion to photon conversion in the magnetosphere of NSs.","Furthermore, we investigate the impact of the magnetic field on the actual observables, such as the energy spectrum of axions and axion-converted photon flux for three different NSs.","Our comparative study indicates that axions energy spectrum and axion-converted photon flux changes significantly due to an intense magnetic field."],"url":"http://arxiv.org/abs/2402.13044v1","category":"hep-ph"}
{"created":"2024-02-20 14:26:09","title":"N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images","abstract":"This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source along with the training images.","sentences":["This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs.","The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view.","Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon.","The N-MPC achieves real time control of a UAV with a control frequency of 100Hz.","The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments.","The associated code is released open-source along with the training images."],"url":"http://arxiv.org/abs/2402.13038v1","category":"cs.RO"}
{"created":"2024-02-20 14:18:43","title":"Enhancing Real-World Complex Network Representations with Hyperedge Augmentation","abstract":"Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary node features by extracting from the virtual hyperedge information, which are used for enhancing GNN performances on downstream tasks. We design three diverse virtual hyperedge construction strategies to accompany the augmentation scheme: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality. Furthermore, to facilitate HyperAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce. Our empirical study shows that HyperAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into graph augmentation methods for real-world complex networks.","sentences":["Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs).","Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations.","These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise.","Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges.","Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues.","In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary node features by extracting from the virtual hyperedge information, which are used for enhancing GNN performances on downstream tasks.","We design three diverse virtual hyperedge construction strategies to accompany the augmentation scheme: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality.","Furthermore, to facilitate HyperAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce.","Our empirical study shows that HyperAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into graph augmentation methods for real-world complex networks."],"url":"http://arxiv.org/abs/2402.13033v1","category":"cs.LG"}
{"created":"2024-02-20 14:02:36","title":"Dirichlet Problems in Perforated Domains","abstract":"In this paper we establish $W^{1,p}$ estimates for solutions $u_\\varepsilon$ to Laplace's equation with the Dirichlet condition in a bounded and perforated, not necessarily periodically, $C^1$ domain $\\Omega_{\\varepsilon, \\eta}$ in $\\mathbb{R}^d$. The bounding constants depend explicitly on two small parameters $\\varepsilon$ and $\\eta$, where $\\varepsilon$ represents the scale of the minimal distance between holes, and $\\eta$ denotes the ratio between the size of the holes and $\\varepsilon$. The proof relies on a large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$, whose proof is divided into two parts. In the first part, we show that as $\\varepsilon, \\eta $ approach zero, harmonic functions in $\\Omega_{\\varepsilon, \\eta}$ may be approximated by solutions of an intermediate problem for a Schr\\\"odinger operator in $\\Omega$. In the second part, a real-variable method is employed to establish the large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$ by using the approximation at scales above $\\varepsilon$. The results are sharp except in the case $d\\ge 3$ and $p=d$ or $d^\\prime$.","sentences":["In this paper we establish $W^{1,p}$ estimates for solutions $u_\\varepsilon$ to Laplace's equation with the Dirichlet condition in a bounded and perforated, not necessarily periodically, $C^1$ domain $\\Omega_{\\varepsilon, \\eta}$ in $\\mathbb{R}^d$. The bounding constants depend explicitly on two small parameters $\\varepsilon$ and $\\eta$, where $\\varepsilon$ represents the scale of the minimal distance between holes, and $\\eta$ denotes the ratio between the size of the holes and $\\varepsilon$. The proof relies on a large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$, whose proof is divided into two parts.","In the first part, we show that as $\\varepsilon, \\eta $ approach zero, harmonic functions in $\\Omega_{\\varepsilon, \\eta}$ may be approximated by solutions of an intermediate problem for a Schr\\\"odinger operator in $\\Omega$. In the second part, a real-variable method is employed to establish the large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$ by using the approximation at scales above $\\varepsilon$. The results are sharp except in the case $d\\ge 3$ and $p=d$ or $d^\\prime$."],"url":"http://arxiv.org/abs/2402.13021v1","category":"math.AP"}
{"created":"2024-02-20 14:02:17","title":"Excitons in epitaxially grown WS2 on Graphene: a nanometer-resolved EELS and DFT study","abstract":"In this study, we investigate excitonic properties of epitaxially grown WS2, which is of particular interest for various applications due to its potential for upscaling to wafer sized structures. Understanding the effect of the dielectric environment due to changing layer numbers and multi-material heterostructures on the optical properties is crucial for tailoring device properties. Monochromated electron energy loss spectroscopy in a scanning transmission electron microscope is employed to characterize the excitonic spectrum of WS2 on graphene grown by metal organic chemical vapor deposition. This technique provides the required spatial resolution at the nanometer scale in combination with high quality spectra. To complement the experimental results, theoretical investigations using density functional theory and applying the Bethe-Salpeter equations are conducted. We find that by transitioning from mono- to bi- to multilayers of WS2 the spectra show redshifts for both, the K-valley excitons at about 2.0 and 2.4 eV as well as excitonic features of higher energies. The latter features originate from so called band nesting of transitions between the Gamma- and K-point. In summary, this study provides valuable insights into the excitonic properties of WS2 in different layer configurations and environments, which are realistically needed for future device fabrication and property tuning. Finally, we can show that nanometer scale electron spectroscopy supported by careful theoretical modelling can successfully link atomic structure and optical properties, such as exciton shifts, in non-idealized complex material systems like multilayer 2D heterostructures.","sentences":["In this study, we investigate excitonic properties of epitaxially grown WS2, which is of particular interest for various applications due to its potential for upscaling to wafer sized structures.","Understanding the effect of the dielectric environment due to changing layer numbers and multi-material heterostructures on the optical properties is crucial for tailoring device properties.","Monochromated electron energy loss spectroscopy in a scanning transmission electron microscope is employed to characterize the excitonic spectrum of WS2 on graphene grown by metal organic chemical vapor deposition.","This technique provides the required spatial resolution at the nanometer scale in combination with high quality spectra.","To complement the experimental results, theoretical investigations using density functional theory and applying the Bethe-Salpeter equations are conducted.","We find that by transitioning from mono- to bi- to multilayers of WS2 the spectra show redshifts for both, the K-valley excitons at about 2.0 and 2.4 eV as well as excitonic features of higher energies.","The latter features originate from so called band nesting of transitions between the Gamma- and K-point.","In summary, this study provides valuable insights into the excitonic properties of WS2 in different layer configurations and environments, which are realistically needed for future device fabrication and property tuning.","Finally, we can show that nanometer scale electron spectroscopy supported by careful theoretical modelling can successfully link atomic structure and optical properties, such as exciton shifts, in non-idealized complex material systems like multilayer 2D heterostructures."],"url":"http://arxiv.org/abs/2402.13020v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 13:57:23","title":"An exact stationary axisymmetric vacuum solution within a metric--affine bumblebee gravity","abstract":"Within the framework of the spontaneous Lorentz symmetry breaking, we consider a metric--affine generalization of the gravitational sector of the Standard--Model Extension (SME), including the Lorentz--violating (LV) coefficients $u$ and $s^{\\mu\\nu}$. In this model, we derive the modified Einstein field equations in order to obtain a new axisymmetric vacuum spinning solution for a particular bumblebee's profile. Such a solution has the remarkable property of incorporating the effects of Lorentz symmetry breaking (LSB) through the LV dimensionless parameter $X=\\xi b^2$, as the LSB is turned off, $X=0$, we recover the well--established result, the Kerr solution, as expected. Afterwards, we calculate the geodesics, the radial acceleration and thermodynamic quantities for this new metric. We also estimate an upper bound for $X$ by using astrophysical data of the advance Mercury's perihelion.","sentences":["Within the framework of the spontaneous Lorentz symmetry breaking, we consider a metric--affine generalization of the gravitational sector of the Standard--Model Extension (SME), including the Lorentz--violating (LV) coefficients $u$ and","$s^{\\mu\\nu}$. In this model, we derive the modified Einstein field equations in order to obtain a new axisymmetric vacuum spinning solution for a particular bumblebee's profile.","Such a solution has the remarkable property of incorporating the effects of Lorentz symmetry breaking (LSB) through the LV dimensionless parameter $X=\\xi b^2$, as the LSB is turned off, $X=0$, we recover the well--established result, the Kerr solution, as expected.","Afterwards, we calculate the geodesics, the radial acceleration and thermodynamic quantities for this new metric.","We also estimate an upper bound for $X$ by using astrophysical data of the advance Mercury's perihelion."],"url":"http://arxiv.org/abs/2402.13014v1","category":"gr-qc"}
{"created":"2024-02-20 13:55:53","title":"Asymptotic behavior of the indicator function in the inverse problem of the wave equation for media with multiple types of cavities","abstract":"In this paper, the inverse problem of the wave equation by the enclosure method for a medium with multiple types of cavities is discussed. In the case considered here, the sign of the indicator function of the enclosure method is not determined and sign cancellation may occur, resulting in loss of information. By examining the top terms of the indicator function in detail, we show that the shortest distance to the cavities can be obtained even in such a case.","sentences":["In this paper, the inverse problem of the wave equation by the enclosure method for a medium with multiple types of cavities is discussed.","In the case considered here, the sign of the indicator function of the enclosure method is not determined and sign cancellation may occur, resulting in loss of information.","By examining the top terms of the indicator function in detail, we show that the shortest distance to the cavities can be obtained even in such a case."],"url":"http://arxiv.org/abs/2402.13012v1","category":"math.AP"}
{"created":"2024-02-20 13:32:00","title":"A unifying primary framework for quantum graph neural networks from quantum graph states","abstract":"Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.","sentences":["Graph states are used to represent mathematical graphs as quantum states on quantum computers.","They can be formulated through stabilizer codes or directly quantum gates and quantum states.","In this paper we show that a quantum graph neural network model can be understood and realized based on graph states.","We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers."],"url":"http://arxiv.org/abs/2402.13001v2","category":"quant-ph"}
{"created":"2024-02-20 13:25:16","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism","abstract":"Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.","sentences":["Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems.","Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query.","We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase.","We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism.","We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts."],"url":"http://arxiv.org/abs/2402.12997v1","category":"cs.IR"}
{"created":"2024-02-20 13:21:27","title":"On gravito-inertial surface waves","abstract":"In geophysical environments, wave motions that are shaped by the action of gravity and global rotation bear the name of gravito-inertial waves. We present a geometrical description of gravito-inertial surface waves, which are low-frequency waves existing in the presence of a solid boundary. We consider an idealized fluid model for an incompressible fluid enclosed in a smooth compact three-dimensional domain, subject to a constant rotation vector. The fluid is also stratified in density under a constant Brunt-V{\\\"a}is{\\\"a}l{\\\"a} frequency. The spectral problem is formulated in terms of the pressure, which satisfies a Poincar\\'e equation within the domain, and a Kelvin equation on the boundary. The Poincar\\'e equation is elliptic when the wave frequency is small enough, such that we can use the Dirichlet-to-Neumann operator to reduce the Kelvin equation to a pseudo-differential equation on the boundary. We find that the wave energy is concentrated on the boundary for large covectors, and can exhibit surface wave attractors for generic domains. In an ellipsoid, we show that these waves are square-integrable and reduce to spherical harmonics on the boundary.","sentences":["In geophysical environments, wave motions that are shaped by the action of gravity and global rotation bear the name of gravito-inertial waves.","We present a geometrical description of gravito-inertial surface waves, which are low-frequency waves existing in the presence of a solid boundary.","We consider an idealized fluid model for an incompressible fluid enclosed in a smooth compact three-dimensional domain, subject to a constant rotation vector.","The fluid is also stratified in density under a constant Brunt-V{\\\"a}is{\\\"a}l{\\\"a} frequency.","The spectral problem is formulated in terms of the pressure, which satisfies a Poincar\\'e equation within the domain, and a Kelvin equation on the boundary.","The Poincar\\'e equation is elliptic when the wave frequency is small enough, such that we can use the Dirichlet-to-Neumann operator to reduce the Kelvin equation to a pseudo-differential equation on the boundary.","We find that the wave energy is concentrated on the boundary for large covectors, and can exhibit surface wave attractors for generic domains.","In an ellipsoid, we show that these waves are square-integrable and reduce to spherical harmonics on the boundary."],"url":"http://arxiv.org/abs/2402.12992v1","category":"math.AP"}
{"created":"2024-02-20 13:14:02","title":"L\u00fcscher equation with long-range forces","abstract":"We derive the modified L\\\"uscher equation in the presence of the long-range force caused by the exchange of a light particle. It is shown that the use of this equation enables one to circumvent the problems related to the strong partial-wave mixing and the t-channel sub-threshold singularities. It is also demonstrated that the present method is intrinsically linked to the so-called modified effective-range expansion (MERE) in the infinite volume. A detailed comparison with the two recently proposed alternative approaches is provided.","sentences":["We derive the modified L\\\"uscher equation in the presence of the long-range force caused by the exchange of a light particle.","It is shown that the use of this equation enables one to circumvent the problems related to the strong partial-wave mixing and the t-channel sub-threshold singularities.","It is also demonstrated that the present method is intrinsically linked to the so-called modified effective-range expansion (MERE) in the infinite volume.","A detailed comparison with the two recently proposed alternative approaches is provided."],"url":"http://arxiv.org/abs/2402.12985v1","category":"hep-lat"}
{"created":"2024-02-20 13:03:26","title":"Funktionalanalysis Teil I","abstract":"Roughly spoken, Functionalanalysis means the study of the category of infinite-dimensional vectorspaces over the field of real or complex numbers, together with their linear maps. In most cases, one further needs a topological structure on such a vectorspace, because then, you can consider the continuous linear maps between such spaces. The name Functionalanalysis is due to the fact, that in the beginning of the theory, the authors wanted to expand Calculus onto functionals of spaces of functions. Functionalanalytical results give the possibility to solve problems in the Theory of (Partial) Differential Equations, in Complex Analysis or in Quantum Mechanics. But the aim of this lines is not to explain the applications. We will discuss the mathematical theory of metric spaces, normed vector spaces and algebras, spaces of continuous resp. integrable functions as well as reflexive and uniformly convex spaces.","sentences":["Roughly spoken, Functionalanalysis means the study of the category of infinite-dimensional vectorspaces over the field of real or complex numbers, together with their linear maps.","In most cases, one further needs a topological structure on such a vectorspace, because then, you can consider the continuous linear maps between such spaces.","The name Functionalanalysis is due to the fact, that in the beginning of the theory, the authors wanted to expand Calculus onto functionals of spaces of functions.","Functionalanalytical results give the possibility to solve problems in the Theory of (Partial) Differential Equations, in Complex Analysis or in Quantum Mechanics.","But the aim of this lines is not to explain the applications.","We will discuss the mathematical theory of metric spaces, normed vector spaces and algebras, spaces of continuous resp.","integrable functions as well as reflexive and uniformly convex spaces."],"url":"http://arxiv.org/abs/2402.12981v1","category":"math.FA"}
{"created":"2024-02-20 13:00:38","title":"Minimisation of peak stresses with the shape derivative","abstract":"This paper is concerned with the minimisation of peak stresses occurring in linear elasticity. We propose to minimise the maximal von Mises stress of the elastic body. This leads to a nonsmooth shape functional. We derive the shape derivative and associate it with the Clarke sub-differential. Using a steepest descent algorithm we present numerical simulations. We compare our results to the usual $p$-norm regularisation and show that our algorithm performs better in the presented tests.","sentences":["This paper is concerned with the minimisation of peak stresses occurring in linear elasticity.","We propose to minimise the maximal von Mises stress of the elastic body.","This leads to a nonsmooth shape functional.","We derive the shape derivative and associate it with the Clarke sub-differential.","Using a steepest descent algorithm we present numerical simulations.","We compare our results to the usual $p$-norm regularisation and show that our algorithm performs better in the presented tests."],"url":"http://arxiv.org/abs/2402.12978v1","category":"math.OC"}
{"created":"2024-02-20 18:59:02","title":"Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields","abstract":"In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization.","sentences":["In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision.","First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions.","Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization.","Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead.","To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask.","Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization."],"url":"http://arxiv.org/abs/2402.13252v1","category":"cs.CV"}
{"created":"2024-02-20 18:59:00","title":"FlashTex: Fast Relightable Mesh Texturing with LightControlNet","abstract":"Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting. Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures.","sentences":["Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators.","We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt.","Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment.","We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model.","Our text-to-texture pipeline then constructs the texture in two stages.","The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet.","The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting.","Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures."],"url":"http://arxiv.org/abs/2402.13251v1","category":"cs.GR"}
{"created":"2024-02-20 17:58:43","title":"Force-free identification of minimum-energy pathways and transition states for stochastic electronic structure theories","abstract":"Stochastic electronic structure theories, e.g., Quantum Monte Carlo methods, enable highly accurate total energy calculations which in principle can be used to construct highly accurate potential energy surfaces. However, their stochastic nature poses a challenge to the computation and use of forces and Hessians, which are typically required in algorithms for minimum-energy pathway (MEP) and transition state (TS) identification, such as the nudged-elastic band (NEB) algorithm and its climbing image formulation. Here, we present strategies that utilize the surrogate Hessian line-search method - previously developed for QMC structural optimization - to efficiently identify MEP and TS structures without requiring force calculations at the level of the stochastic electronic structure theory. By modifying the surrogate Hessian algorithm to operate in path-orthogonal subspaces and on saddle points, we show that it is possible to identify MEPs and TSs using a force-free QMC approach. We demonstrate these strategies via two examples, the inversion of the ammonia molecule and an SN2 reaction. We validate our results using Density Functional Theory- and coupled cluster-based NEB calculations. We then introduce a hybrid DFT-QMC approach to compute thermodynamic and kinetic quantities - free energy differences, rate constants, and equilibrium constants - that incorporates stochastically-optimized structures and their energies, and show that this scheme improves upon DFT accuracy. Our methods generalize straightforwardly to other systems and other high-accuracy theories that similarly face challenges computing energy gradients, paving the way for highly accurate PES mapping, transition state determination, and thermodynamic and kinetic calculations, at significantly reduced computational expense.","sentences":["Stochastic electronic structure theories, e.g., Quantum Monte Carlo methods, enable highly accurate total energy calculations which in principle can be used to construct highly accurate potential energy surfaces.","However, their stochastic nature poses a challenge to the computation and use of forces and Hessians, which are typically required in algorithms for minimum-energy pathway (MEP) and transition state (TS) identification, such as the nudged-elastic band (NEB) algorithm and its climbing image formulation.","Here, we present strategies that utilize the surrogate Hessian line-search method - previously developed for QMC structural optimization - to efficiently identify MEP and TS structures without requiring force calculations at the level of the stochastic electronic structure theory.","By modifying the surrogate Hessian algorithm to operate in path-orthogonal subspaces and on saddle points, we show that it is possible to identify MEPs and TSs using a force-free QMC approach.","We demonstrate these strategies via two examples, the inversion of the ammonia molecule and an SN2 reaction.","We validate our results using Density Functional Theory- and coupled cluster-based NEB calculations.","We then introduce a hybrid DFT-QMC approach to compute thermodynamic and kinetic quantities - free energy differences, rate constants, and equilibrium constants - that incorporates stochastically-optimized structures and their energies, and show that this scheme improves upon DFT accuracy.","Our methods generalize straightforwardly to other systems and other high-accuracy theories that similarly face challenges computing energy gradients, paving the way for highly accurate PES mapping, transition state determination, and thermodynamic and kinetic calculations, at significantly reduced computational expense."],"url":"http://arxiv.org/abs/2402.13189v1","category":"physics.chem-ph"}
{"created":"2024-02-20 17:49:43","title":"Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening","abstract":"Robust Model Predictive Control (MPC) for nonlinear systems is a problem that poses significant challenges as highlighted by the diversity of approaches proposed in the last decades. Often compromises with respect to computational load, conservatism, generality, or implementation complexity have to be made, and finding an approach that provides the right balance is still a challenge to the research community. This work provides a contribution by proposing a novel shrinking-horizon robust MPC formulation for nonlinear discrete-time systems. By explicitly accounting for how disturbances and linearization errors are propagated through the nonlinear dynamics, a constraint tightening-based formulation is obtained, with guarantees of robust constraint satisfaction. The proposed controller relies on iteratively solving a Nonlinear Program (NLP) to simultaneously optimize system operation and the required constraint tightening. Numerical experiments show the effectiveness of the proposed controller with three different choices of NLP solvers as well as significantly improved computational speed, better scalability, and generally reduced conservatism when compared to an existing technique from the literature.","sentences":["Robust Model Predictive Control (MPC) for nonlinear systems is a problem that poses significant challenges as highlighted by the diversity of approaches proposed in the last decades.","Often compromises with respect to computational load, conservatism, generality, or implementation complexity have to be made, and finding an approach that provides the right balance is still a challenge to the research community.","This work provides a contribution by proposing a novel shrinking-horizon robust MPC formulation for nonlinear discrete-time systems.","By explicitly accounting for how disturbances and linearization errors are propagated through the nonlinear dynamics, a constraint tightening-based formulation is obtained, with guarantees of robust constraint satisfaction.","The proposed controller relies on iteratively solving a Nonlinear Program (NLP) to simultaneously optimize system operation and the required constraint tightening.","Numerical experiments show the effectiveness of the proposed controller with three different choices of NLP solvers as well as significantly improved computational speed, better scalability, and generally reduced conservatism when compared to an existing technique from the literature."],"url":"http://arxiv.org/abs/2402.13183v1","category":"eess.SY"}
{"created":"2024-02-20 17:49:10","title":"Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness","abstract":"We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.","sentences":["We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space.","Each agent sequentially queries the function to obtain noisy observations at the query points.","Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents.","We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server.","Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication."],"url":"http://arxiv.org/abs/2402.13182v1","category":"cs.LG"}
{"created":"2024-02-20 17:38:37","title":"$h$-Wasserstein barycenters","abstract":"We generalize the notion and theory of Wasserstein barycenters introduced by Agueh and Carlier (2011) from the quadratic cost to general smooth strictly convex costs $h$ with non-degenerate Hessian. We show the equivalence between a coupled two-marginal and a multi-marginal formulation and establish that the multi-marginal optimal plan is unique and of Monge form. To establish the latter result we introduce a new approach which is not based on explicitly solving the optimality system, but instead deriving a quantitative injectivity estimate for the (highly non-injective) map from $N$-point configurations to their $h$-barycenter on the support of an optimal multi-marginal plan.","sentences":["We generalize the notion and theory of Wasserstein barycenters introduced by Agueh and Carlier (2011) from the quadratic cost to general smooth strictly convex costs $h$ with non-degenerate Hessian.","We show the equivalence between a coupled two-marginal and a multi-marginal formulation and establish that the multi-marginal optimal plan is unique and of Monge form.","To establish the latter result we introduce a new approach which is not based on explicitly solving the optimality system, but instead deriving a quantitative injectivity estimate for the (highly non-injective) map from $N$-point configurations to their $h$-barycenter on the support of an optimal multi-marginal plan."],"url":"http://arxiv.org/abs/2402.13176v1","category":"math.AP"}
{"created":"2024-02-20 17:22:11","title":"Barking dogs: A Fr\u00e9chet distance variant for detour detection","abstract":"Imagine you are a dog behind a fence $Q$ and a hiker is passing by at constant speed along the hiking path $P$. In order to fulfil your duties as a watchdog, you desire to bark as long as possible at the human. However, your barks can only be heard in a fixed radius $\\rho$ and, as a dog, you have bounded speed $s$. Can you optimize your route along the fence $Q$ in order to maximize the barking time with radius $\\rho$, assuming you can run backwards and forward at speed at most $s$?   We define the barking distance from a polyline $P$ on $n$ vertices to a polyline $Q$ on $m$ vertices as the time that the hiker stays in your barking radius if you run optimally along $Q$. This asymmetric similarity measure between two curves can be used to detect outliers in $Q$ compared to $P$ that other established measures like the Fr\\'echet distance and Dynamic Time Warping fail to capture at times. We consider this measure in three different settings. In the discrete setting, the traversals of $P$ and $Q$ are both discrete. For this case we show that the barking distance from $P$ to $Q$ can be computed in $O(nm\\log s)$ time. In the semi-discrete setting, the traversal of $Q$ is continuous while the one of $P$ is again discrete. Here, we show how to compute the barking distance in time $O(nm\\log (nm))$. Finally, in the continuous setting in which both traversals are continuous, we show that the problem can be solved in polynomial time. For all the settings we show that, assuming SETH, no truly subquadratic algorithm can exist.","sentences":["Imagine you are a dog behind a fence $Q$ and a hiker is passing by at constant speed along the hiking path $P$.","In order to fulfil your duties as a watchdog, you desire to bark as long as possible at the human.","However, your barks can only be heard in a fixed radius $\\rho$ and, as a dog, you have bounded speed $s$. Can you optimize your route along the fence $Q$ in order to maximize the barking time with radius $\\rho$, assuming you can run backwards and forward at speed at most $s$?   ","We define the barking distance from a polyline $P$ on $n$ vertices to a polyline $Q$ on $m$ vertices as the time that the hiker stays in your barking radius if you run optimally along $Q$. This asymmetric similarity measure between two curves can be used to detect outliers in $Q$ compared to $P$ that other established measures like the Fr\\'echet distance and Dynamic Time Warping fail to capture at times.","We consider this measure in three different settings.","In the discrete setting, the traversals of $P$ and $Q$ are both discrete.","For this case we show that the barking distance from $P$ to $Q$ can be computed in $O(nm\\log s)$ time.","In the semi-discrete setting, the traversal of $Q$ is continuous while the one of $P$ is again discrete.","Here, we show how to compute the barking distance in time $O(nm\\log (nm))$. Finally, in the continuous setting in which both traversals are continuous, we show that the problem can be solved in polynomial time.","For all the settings we show that, assuming SETH, no truly subquadratic algorithm can exist."],"url":"http://arxiv.org/abs/2402.13159v1","category":"cs.CG"}
{"created":"2024-02-20 16:50:47","title":"A Systematic Literature Review on Task Allocation and Performance Management Techniques in Cloud Data Center","abstract":"As cloud computing usage grows, cloud data centers play an increasingly important role. To maximize resource utilization, ensure service quality, and enhance system performance, it is crucial to allocate tasks and manage performance effectively. The purpose of this study is to provide an extensive analysis of task allocation and performance management techniques employed in cloud data centers. The aim is to systematically categorize and organize previous research by identifying the cloud computing methodologies, categories, and gaps. A literature review was conducted, which included the analysis of 463 task allocations and 480 performance management papers. The review revealed three task allocation research topics and seven performance management methods. Task allocation research areas are resource allocation, load-Balancing, and scheduling. Performance management includes monitoring and control, power and energy management, resource utilization optimization, quality of service management, fault management, virtual machine management, and network management. The study proposes new techniques to enhance cloud computing work allocation and performance management. Short-comings in each approach can guide future research. The research's findings on cloud data center task allocation and performance management can assist academics, practitioners, and cloud service providers in optimizing their systems for dependability, cost-effectiveness, and scalability. Innovative methodologies can steer future research to fill gaps in the literature.","sentences":["As cloud computing usage grows, cloud data centers play an increasingly important role.","To maximize resource utilization, ensure service quality, and enhance system performance, it is crucial to allocate tasks and manage performance effectively.","The purpose of this study is to provide an extensive analysis of task allocation and performance management techniques employed in cloud data centers.","The aim is to systematically categorize and organize previous research by identifying the cloud computing methodologies, categories, and gaps.","A literature review was conducted, which included the analysis of 463 task allocations and 480 performance management papers.","The review revealed three task allocation research topics and seven performance management methods.","Task allocation research areas are resource allocation, load-Balancing, and scheduling.","Performance management includes monitoring and control, power and energy management, resource utilization optimization, quality of service management, fault management, virtual machine management, and network management.","The study proposes new techniques to enhance cloud computing work allocation and performance management.","Short-comings in each approach can guide future research.","The research's findings on cloud data center task allocation and performance management can assist academics, practitioners, and cloud service providers in optimizing their systems for dependability, cost-effectiveness, and scalability.","Innovative methodologies can steer future research to fill gaps in the literature."],"url":"http://arxiv.org/abs/2402.13135v1","category":"cs.DC"}
{"created":"2024-02-20 16:19:12","title":"Faster and Deterministic Subtrajectory Clustering","abstract":"Given a trajectory $T$ and a distance $\\Delta$, we wish to find a set $C$ of curves of complexity at most $\\ell$, such that we can cover $T$ with subcurves that each are within Fr\\'echet distance $\\Delta$ to at least one curve in $C$. We call $C$ an $(\\ell,\\Delta)$-clustering and aim to find an $(\\ell,\\Delta)$-clustering of minimum cardinality. This problem was introduced by Akitaya $et$ $al.$ (2021) and shown to be NP-complete. The main focus has therefore been on bicriterial approximation algorithms, allowing for the clustering to be an $(\\ell, \\Theta(\\Delta))$-clustering of roughly optimal size. We present algorithms that construct $(\\ell,4\\Delta)$-clusterings of $\\mathcal{O}(k \\log n)$ size, where $k$ is the size of the optimal $(\\ell, \\Delta)$-clustering. For the discrete Fr\\'echet distance, we use $\\mathcal{O}(n \\ell \\log n)$ space and $\\mathcal{O}(k n^2 \\log^3 n)$ deterministic worst case time. For the continuous Fr\\'echet distance, we use $\\mathcal{O}(n^2 \\log n)$ space and $\\mathcal{O}(k n^3 \\log^3 n)$ time. Our algorithms significantly improve upon the clustering quality (improving the approximation factor in $\\Delta$) and size (whenever $\\ell \\in \\Omega(\\log n)$). We offer deterministic running times comparable to known expected bounds. Additionally, in the continuous setting, we give a near-linear improvement upon the space usage. When compared only to deterministic results, we offer a near-linear speedup and a near-quadratic improvement in the space usage. When we may restrict ourselves to only considering clusters where all subtrajectories are vertex-to-vertex subcurves, we obtain even better results under the continuous Fr\\'echet distance. Our algorithm becomes near quadratic and uses space that is near linear in $n \\ell$.","sentences":["Given a trajectory $T$ and a distance $\\Delta$, we wish to find a set $C$ of curves of complexity at most $\\ell$, such that we can cover $T$ with subcurves that each are within Fr\\'echet distance $\\Delta$ to at least one curve in $C$.","We call $C$ an $(\\ell,\\Delta)$-clustering and aim to find an $(\\ell,\\Delta)$-clustering of minimum cardinality.","This problem was introduced by Akitaya $et$ $al.$ (2021) and shown to be NP-complete.","The main focus has therefore been on bicriterial approximation algorithms, allowing for the clustering to be an $(\\ell, \\Theta(\\Delta))$-clustering of roughly optimal size.","We present algorithms that construct $(\\ell,4\\Delta)$-clusterings of $\\mathcal{O}(k \\log n)$ size, where $k$ is the size of the optimal $(\\ell, \\Delta)$-clustering.","For the discrete Fr\\'echet distance, we use $\\mathcal{O}(n \\ell \\log n)$ space and $\\mathcal{O}(k n^2 \\log^3 n)$ deterministic worst case time.","For the continuous Fr\\'echet distance, we use $\\mathcal{O}(n^2 \\log n)$ space and $\\mathcal{O}(k n^3 \\log^3 n)$ time.","Our algorithms significantly improve upon the clustering quality (improving the approximation factor in $\\Delta$) and size (whenever $\\ell \\in \\Omega(\\log n)$).","We offer deterministic running times comparable to known expected bounds.","Additionally, in the continuous setting, we give a near-linear improvement upon the space usage.","When compared only to deterministic results, we offer a near-linear speedup and a near-quadratic improvement in the space usage.","When we may restrict ourselves to only considering clusters where all subtrajectories are vertex-to-vertex subcurves, we obtain even better results under the continuous Fr\\'echet distance.","Our algorithm becomes near quadratic and uses space that is near linear in $n \\ell$."],"url":"http://arxiv.org/abs/2402.13117v1","category":"cs.CG"}
{"created":"2024-02-20 15:54:50","title":"Are boron-nitride nanobelts capable to capture greenhouse gases?","abstract":"Why is the question in the title pertinent? Toxic gases, which are detrimental to both human health and the environment, have been released in greater quantities as a result of industrial development. These gases necessitate capture, immobilization, and measurement. Consequently, the present study investigates the interactions between boron-nitride nanobelt and M\\\"obius-type boron-nitride nanobelt and nine greenhouse gases, namely ammonia, carbon dioxide, carbon monoxide, hydrogen sulfide, methane, methanol, nitric dioxide, nitric oxide, and phosgene. The adsorption energies calculated for the structures with optimized geometry are all negative, suggesting that all gases are adsorbed favorably in both nanobelts. Furthermore, we discovered that the recovery time of the sensors ranges from two hours to a few nanoseconds, and that the nanobelts exhibit distinct responses to each gas. According to electronic and topological investigations, covalent bonds were exclusively formed by nitric oxide; the remaining gases formed non-covalent bonds. Molecular dynamics ultimately demonstrate that the interaction between a single gas molecule and the nanobelt remains consistent across the vast majority of gases, whereas the interaction between 500 gas molecules and the nanobelts functions as an attraction, notwithstanding the impact of volumetric effects characteristic of high volume gases on the interaction. For the completion of each calculation, semiempirical tight-binding methods were implemented utilizing the xTB software. The outcomes of our study generated a favorable response to the inquiry posed in the title.","sentences":["Why is the question in the title pertinent?","Toxic gases, which are detrimental to both human health and the environment, have been released in greater quantities as a result of industrial development.","These gases necessitate capture, immobilization, and measurement.","Consequently, the present study investigates the interactions between boron-nitride nanobelt and M\\\"obius-type boron-nitride nanobelt and nine greenhouse gases, namely ammonia, carbon dioxide, carbon monoxide, hydrogen sulfide, methane, methanol, nitric dioxide, nitric oxide, and phosgene.","The adsorption energies calculated for the structures with optimized geometry are all negative, suggesting that all gases are adsorbed favorably in both nanobelts.","Furthermore, we discovered that the recovery time of the sensors ranges from two hours to a few nanoseconds, and that the nanobelts exhibit distinct responses to each gas.","According to electronic and topological investigations, covalent bonds were exclusively formed by nitric oxide; the remaining gases formed non-covalent bonds.","Molecular dynamics ultimately demonstrate that the interaction between a single gas molecule and the nanobelt remains consistent across the vast majority of gases, whereas the interaction between 500 gas molecules and the nanobelts functions as an attraction, notwithstanding the impact of volumetric effects characteristic of high volume gases on the interaction.","For the completion of each calculation, semiempirical tight-binding methods were implemented utilizing the xTB software.","The outcomes of our study generated a favorable response to the inquiry posed in the title."],"url":"http://arxiv.org/abs/2402.13102v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 15:40:07","title":"A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks","abstract":"This study investigates the integration of a high altitude platform station (HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm for energy saving. By doing so, the sustainability and ubiquitous connectivity targets can be achieved. Besides, a delay-aware approach is also adopted, where the delay profiles of users are respected in such a way that we attempt to meet the latency requirements of users with a best-effort strategy. To this end, a novel, simple, and lightweight Q-learning algorithm is designed to address the cell-switching optimization problem. During the simulation campaigns, different interference scenarios and delay situations between base stations are examined in terms of energy consumption and quality-of-service (QoS), and the results confirm the efficacy of the proposed Q-learning algorithm.","sentences":["This study investigates the integration of a high altitude platform station (HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm for energy saving.","By doing so, the sustainability and ubiquitous connectivity targets can be achieved.","Besides, a delay-aware approach is also adopted, where the delay profiles of users are respected in such a way that we attempt to meet the latency requirements of users with a best-effort strategy.","To this end, a novel, simple, and lightweight Q-learning algorithm is designed to address the cell-switching optimization problem.","During the simulation campaigns, different interference scenarios and delay situations between base stations are examined in terms of energy consumption and quality-of-service (QoS), and the results confirm the efficacy of the proposed Q-learning algorithm."],"url":"http://arxiv.org/abs/2402.13096v1","category":"cs.NI"}
{"created":"2024-02-20 15:33:48","title":"Fast and memory-efficient optimization for large-scale data-driven predictive control","abstract":"Recently, data-enabled predictive control (DeePC) schemes based on Willems' fundamental lemma have attracted considerable attention. At the core are computations using Hankel-like matrices and their connection to the concept of persistency of excitation. We propose an iterative solver for the underlying data-driven optimal control problems resulting from linear discrete-time systems. To this end, we apply factorizations based on the discrete Fourier transform of the Hankel-like matrices, which enable fast and memory-efficient computations. To take advantage of this factorization in an optimal control solver and to reduce the effect of inherent bad conditioning of the Hankel-like matrices, we propose an augmented Lagrangian lBFGS-method. We illustrate the performance of our method by means of a numerical study.","sentences":["Recently, data-enabled predictive control (DeePC) schemes based on Willems' fundamental lemma have attracted considerable attention.","At the core are computations using Hankel-like matrices and their connection to the concept of persistency of excitation.","We propose an iterative solver for the underlying data-driven optimal control problems resulting from linear discrete-time systems.","To this end, we apply factorizations based on the discrete Fourier transform of the Hankel-like matrices, which enable fast and memory-efficient computations.","To take advantage of this factorization in an optimal control solver and to reduce the effect of inherent bad conditioning of the Hankel-like matrices, we propose an augmented Lagrangian lBFGS-method.","We illustrate the performance of our method by means of a numerical study."],"url":"http://arxiv.org/abs/2402.13090v1","category":"math.OC"}
{"created":"2024-02-20 15:22:25","title":"Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition","abstract":"Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor.","sentences":["Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience.","This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models.","We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory.","Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models.","These guidelines focus on minimizing power use without substantially affecting accuracy.","Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods.","It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor."],"url":"http://arxiv.org/abs/2402.13076v1","category":"cs.SD"}
{"created":"2024-02-20 15:13:38","title":"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models","abstract":"The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.","sentences":["The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance.","Recent years have witnessed significant developments in codec models.","The ideal sound codec should preserve content, paralinguistics, speakers, and audio information.","However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings.","This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark.","It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.","Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs.","Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons.","Finally, we will release codes, the leaderboard, and data to accelerate progress within the community."],"url":"http://arxiv.org/abs/2402.13071v1","category":"eess.AS"}
{"created":"2024-02-20 15:11:31","title":"Design of narrowband infrared emitters by hybridizing guided-mode resonance structures with van der Waals materials","abstract":"In this paper, narrowband emitters have been designed using particle swarm optimization (PSO) in the 10-20 {\\mu}m infrared range. The device structure consists of an anisotropic {\\alpha}-MoO3 layer combined with the one- and two-dimensional guided-mode resonance structures. Well-defined absorption lines are present in the reflection spectrum for both TE and TM polarizations, thereby yielding narrowband emissivity at desired wavelengths. The band structure of the designed emitters under TM polarization demonstrates distinct features unlike its TE counterpart. These features are attributed to the interaction between guided-mode resonances and phonon polaritons. The results are relevant for applications in active and passive photonic elements in mid- and long-wave IR bands.","sentences":["In this paper, narrowband emitters have been designed using particle swarm optimization (PSO) in the 10-20 {\\mu}m infrared range.","The device structure consists of an anisotropic {\\alpha}-MoO3 layer combined with the one- and two-dimensional guided-mode resonance structures.","Well-defined absorption lines are present in the reflection spectrum for both TE and TM polarizations, thereby yielding narrowband emissivity at desired wavelengths.","The band structure of the designed emitters under TM polarization demonstrates distinct features unlike its TE counterpart.","These features are attributed to the interaction between guided-mode resonances and phonon polaritons.","The results are relevant for applications in active and passive photonic elements in mid- and long-wave IR bands."],"url":"http://arxiv.org/abs/2402.13070v1","category":"physics.optics"}
{"created":"2024-02-20 14:41:42","title":"Inverse design of spinodoid structures using Bayesian optimization","abstract":"Tailoring materials to achieve a desired behavior in specific applications is of significant scientific and industrial interest as design of materials is a key driver to innovation. Overcoming the rather slow and expertise-bound traditional forward approaches of trial and error, inverse design is attracting substantial attention. Targeting a property, the design model proposes a candidate structure with the desired property. This concept can be particularly well applied to the field of architected materials as their structures can be directly tuned. The bone-like spinodoid materials are a specific class of architected materials. They are of considerable interest thanks to their non-periodicity, smoothness, and low-dimensional statistical description. Previous work successfully employed machine learning (ML) models for inverse design. The amount of data necessary for most ML approaches poses a severe obstacle for broader application, especially in the context of inelasticity. That is why we propose an inverse-design approach based on Bayesian optimization to operate in the small-data regime. Necessitating substantially less data, a small initial data set is iteratively augmented by in silico generated data until a structure with the targeted properties is found. The application to the inverse design of spinodoid structures of desired elastic properties demonstrates the framework's potential for paving the way for advance in inverse design.","sentences":["Tailoring materials to achieve a desired behavior in specific applications is of significant scientific and industrial interest as design of materials is a key driver to innovation.","Overcoming the rather slow and expertise-bound traditional forward approaches of trial and error, inverse design is attracting substantial attention.","Targeting a property, the design model proposes a candidate structure with the desired property.","This concept can be particularly well applied to the field of architected materials as their structures can be directly tuned.","The bone-like spinodoid materials are a specific class of architected materials.","They are of considerable interest thanks to their non-periodicity, smoothness, and low-dimensional statistical description.","Previous work successfully employed machine learning (ML) models for inverse design.","The amount of data necessary for most ML approaches poses a severe obstacle for broader application, especially in the context of inelasticity.","That is why we propose an inverse-design approach based on Bayesian optimization to operate in the small-data regime.","Necessitating substantially less data, a small initial data set is iteratively augmented by in silico generated data until a structure with the targeted properties is found.","The application to the inverse design of spinodoid structures of desired elastic properties demonstrates the framework's potential for paving the way for advance in inverse design."],"url":"http://arxiv.org/abs/2402.13054v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 14:14:26","title":"Federated Learning for Iot/Edge/Fog Computing Systems","abstract":"With the help of a new architecture called Edge/Fog (E/F) computing, cloud computing services can now be extended nearer to data generator devices. E/F computing in combination with Deep Learning (DL) is a promisedtechnique that is vastly applied in numerous fields. To train their models, data producers in conventional DL architectures with E/F computing enable them to repeatedly transmit and communicate data with third-party servers, like Edge/Fog or cloud servers. Due to the extensive bandwidth needs, legal issues, and privacy risks, this architecture is frequently impractical. Through a centralized server, the models can be co-trained by FL through distributed clients, including cars, hospitals, and mobile phones, while preserving data localization. As it facilitates group learning and model optimization, FL can therefore be seen as a motivating element in the E/F computing paradigm. Although FL applications in E/F computing environments have been considered in previous studies, FL execution and hurdles in the E/F computing framework have not been thoroughly covered. In order to identify advanced solutions, this chapter will provide a review of the application of FL in E/F computing systems. We think that by doing this chapter, researchers will learn more about how E/F computing and FL enable related concepts and technologies. Some case studies about the implementation of federated learning in E/F computing are being investigated. The open issues and future research directions are introduced.","sentences":["With the help of a new architecture called Edge/Fog (E/F) computing, cloud computing services can now be extended nearer to data generator devices.","E/F computing in combination with Deep Learning (DL) is a promisedtechnique that is vastly applied in numerous fields.","To train their models, data producers in conventional DL architectures with E/F computing enable them to repeatedly transmit and communicate data with third-party servers, like Edge/Fog or cloud servers.","Due to the extensive bandwidth needs, legal issues, and privacy risks, this architecture is frequently impractical.","Through a centralized server, the models can be co-trained by FL through distributed clients, including cars, hospitals, and mobile phones, while preserving data localization.","As it facilitates group learning and model optimization, FL can therefore be seen as a motivating element in the E/F computing paradigm.","Although FL applications in E/F computing environments have been considered in previous studies, FL execution and hurdles in the E/F computing framework have not been thoroughly covered.","In order to identify advanced solutions, this chapter will provide a review of the application of FL in E/F computing systems.","We think that by doing this chapter, researchers will learn more about how E/F computing and FL enable related concepts and technologies.","Some case studies about the implementation of federated learning in E/F computing are being investigated.","The open issues and future research directions are introduced."],"url":"http://arxiv.org/abs/2402.13029v1","category":"cs.NI"}
{"created":"2024-02-20 13:23:28","title":"Spectral and temporal metrology with bandlimited functions and finite-time measurements","abstract":"We perform an analysis supplementing the metrology toolbox in the time-frequency domain. While the relevant time-frequency-based metrological protocols can be borrowed from the spatial domain, where they have recently been well developed, their ultimate practical usefulness is shown to be restricted by limits put on the bandwidth of both the signal and measurements, as well as by the finite measurement time. As we demonstrate for the well-known problem of multiparameter estimation for two incoherent, point-like sources, the impact of these experimental limitations on the optimal protocol's efficiency can be detrimental. Nonetheless, we propose necessary operational criteria for attainability of the quantum Cram\\'{e}r-Rao bound under the discussed restrictions.","sentences":["We perform an analysis supplementing the metrology toolbox in the time-frequency domain.","While the relevant time-frequency-based metrological protocols can be borrowed from the spatial domain, where they have recently been well developed, their ultimate practical usefulness is shown to be restricted by limits put on the bandwidth of both the signal and measurements, as well as by the finite measurement time.","As we demonstrate for the well-known problem of multiparameter estimation for two incoherent, point-like sources, the impact of these experimental limitations on the optimal protocol's efficiency can be detrimental.","Nonetheless, we propose necessary operational criteria for attainability of the quantum Cram\\'{e}r-Rao bound under the discussed restrictions."],"url":"http://arxiv.org/abs/2402.12995v1","category":"quant-ph"}
{"created":"2024-02-20 13:19:57","title":"Data Unfolding with Mean Integrated Square Error Optimization","abstract":"Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software. Computer simulation is used to study the measurement process. Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects. Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors. In the article, the histogram method is employed to estimate both the measured and true PDFs. The binning of histograms is determined using the K-means clustering algorithm. The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose. The accuracy of the results is assessed using the Mean Integrated Square Error. To determine the optimal value for the regularization parameter, a bootstrap method is applied. Additionally, a mathematical model of the measurement system is formulated using system identification methods. This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics.","sentences":["Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software.","Computer simulation is used to study the measurement process.","Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects.","Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors.","In the article, the histogram method is employed to estimate both the measured and true PDFs.","The binning of histograms is determined using the K-means clustering algorithm.","The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose.","The accuracy of the results is assessed using the Mean Integrated Square Error.","To determine the optimal value for the regularization parameter, a bootstrap method is applied.","Additionally, a mathematical model of the measurement system is formulated using system identification methods.","This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics."],"url":"http://arxiv.org/abs/2402.12990v1","category":"physics.data-an"}
{"created":"2024-02-20 12:48:16","title":"Between Green Hills and Green Bills: Unveiling the Green Shades of Sustainability and Burden Shifting through Multi-Objective Optimization in Swiss Energy System Planning","abstract":"The Paris agreement is the first-ever universally accepted and legally binding agreement on global climate change. It is a bridge between today's and climate-neutrality policies and strategies before the end of the century. Critical to this endeavor is energy system modeling, which, while adept at devising cost-effective carbon-neutral strategies, often overlooks the broader environmental and social implications. This study introduces an innovative methodology that integrates life-cycle impact assessment indicators into energy system modeling, enabling a comprehensive assessment of both economic and environmental outcomes.   Focusing on Switzerland's energy system as a case study, our model reveals that optimizing key environomic indicators can lead to significant economic advantages, with system costs potentially decreasing by 15% to 47% by minimizing potential impacts from operating fossil technologies to the indirect impact related to the construction of the renewable infrastructure. However, a system optimized solely for economic efficiency, despite achieving 63% reduction in carbon footprint compared to 2020, our results show a potential risk of burden shift to other environmental issues.   The adoption of multi-objective optimization in our approach nuances the exploration of the complex interplay between environomic objectives and technological choices. Our results illuminate pathways towards more holistically optimized energy systems, effectively addressing trade-offs across environmental problems and enhancing societal acceptance of the solutions to this century's defining challenge.","sentences":["The Paris agreement is the first-ever universally accepted and legally binding agreement on global climate change.","It is a bridge between today's and climate-neutrality policies and strategies before the end of the century.","Critical to this endeavor is energy system modeling, which, while adept at devising cost-effective carbon-neutral strategies, often overlooks the broader environmental and social implications.","This study introduces an innovative methodology that integrates life-cycle impact assessment indicators into energy system modeling, enabling a comprehensive assessment of both economic and environmental outcomes.   ","Focusing on Switzerland's energy system as a case study, our model reveals that optimizing key environomic indicators can lead to significant economic advantages, with system costs potentially decreasing by 15% to 47% by minimizing potential impacts from operating fossil technologies to the indirect impact related to the construction of the renewable infrastructure.","However, a system optimized solely for economic efficiency, despite achieving 63% reduction in carbon footprint compared to 2020, our results show a potential risk of burden shift to other environmental issues.   ","The adoption of multi-objective optimization in our approach nuances the exploration of the complex interplay between environomic objectives and technological choices.","Our results illuminate pathways towards more holistically optimized energy systems, effectively addressing trade-offs across environmental problems and enhancing societal acceptance of the solutions to this century's defining challenge."],"url":"http://arxiv.org/abs/2402.12973v1","category":"cs.CE"}
{"created":"2024-02-20 12:40:31","title":"How Temporal Unrolling Supports Neural Physics Simulators","abstract":"Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does not utilize this solver. We also quantify a difference in the accuracy of models trained in a fully differentiable setup compared to their non-differentiable counterparts. While differentiable setups perform best, the accuracy of unrolling without temporal gradients comes comparatively close. Furthermore, we empirically show that these behaviors are invariant to changes in the underlying physical system, the network architecture and size, and the numerical scheme. These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable. We also observe that the convergence rate of common neural architectures is low compared to numerical algorithms. This encourages the use of hybrid approaches combining neural and numerical algorithms to utilize the benefits of both.","sentences":["Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators.","We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories.","In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients.","Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients.","We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios.","It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does not utilize this solver.","We also quantify a difference in the accuracy of models trained in a fully differentiable setup compared to their non-differentiable counterparts.","While differentiable setups perform best, the accuracy of unrolling without temporal gradients comes comparatively close.","Furthermore, we empirically show that these behaviors are invariant to changes in the underlying physical system, the network architecture and size, and the numerical scheme.","These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable.","We also observe that the convergence rate of common neural architectures is low compared to numerical algorithms.","This encourages the use of hybrid approaches combining neural and numerical algorithms to utilize the benefits of both."],"url":"http://arxiv.org/abs/2402.12971v1","category":"physics.comp-ph"}
{"created":"2024-02-20 12:31:13","title":"Non-facial exposedness of copositive cones over symmetric cones","abstract":"In this paper, we consider copositive cones over symmetric cones and show that they are never facially exposed when the underlying cone has dimension at least 2. We do so by explicitly exhibiting a non-exposed extreme ray. Our result extends the known fact that the cone of copositive matrices over the nonnegative orthant is not facially exposed in general.","sentences":["In this paper, we consider copositive cones over symmetric cones and show that they are never facially exposed when the underlying cone has dimension at least 2.","We do so by explicitly exhibiting a non-exposed extreme ray.","Our result extends the known fact that the cone of copositive matrices over the nonnegative orthant is not facially exposed in general."],"url":"http://arxiv.org/abs/2402.12964v1","category":"math.OC"}
{"created":"2024-02-20 12:22:10","title":"Hamilton-Jacobi-Bellman equations for Rydberg-blockade processes","abstract":"We discuss time-optimal control problems for two setups involving globally driven Rydberg atoms in the blockade limit by deriving the associated Hamilton-Jacobi-Bellman equations. From these equations, we extract the globally optimal trajectories and the corresponding controls for several target processes of the atomic system, using a generalized method of characteristics. We apply this method to retrieve known results for CZ and C-phase gates, and to find new optimal pulses for all elementary processes involved in the universal quantum computation scheme introduced in [Physical Review Letters 131, 170601 (2023)].","sentences":["We discuss time-optimal control problems for two setups involving globally driven Rydberg atoms in the blockade limit by deriving the associated Hamilton-Jacobi-Bellman equations.","From these equations, we extract the globally optimal trajectories and the corresponding controls for several target processes of the atomic system, using a generalized method of characteristics.","We apply this method to retrieve known results for CZ and C-phase gates, and to find new optimal pulses for all elementary processes involved in the universal quantum computation scheme introduced in [Physical Review Letters 131, 170601 (2023)]."],"url":"http://arxiv.org/abs/2402.12956v1","category":"quant-ph"}
{"created":"2024-02-20 11:53:08","title":"Apparent color and Raman vibrational modes of the unconventional superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+\u03b4}$ exfoliated flakes","abstract":"Studying and controlling the properties of individual exfoliated materials is one of the first steps towards the fabrication of complex van der Waals systems. However, prolonged exposure to ambient conditions can affect the properties of very thin exfoliated materials altering their physical properties. For this reason, it is imperative to employ versatile characterization strategies compatible with reduced ambient exposure times. In this work, we demonstrate that optical microscopy and Raman spectroscopy are quick and non-invasive techniques to study flakes of the high-temperature superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (BSCCO-2212). The apparent color of BSCCO-2212 exfoliated flakes on SiO$_2$/Si has been studied allowing a rough and fast identification of the number of layers. Moreover, we find that thin flakes have a refractive index of around 1.7 in the visible range and 0.5 for the absorption coefficient near the maximum at 550 nm. We determine the optimal combination of illumination wavelength and substrate properties for the identification of different numbers of unit cells of BSCCO-2212. In addition, we report the hardening of the characteristic Raman modes at 116 cm$^{-1}$ and 460 cm$^{-1}$ as flake thickness decreases, possibly due to strain in the BiO and CuO$_2$ planes, respectively. Moreover, the evolution of the Raman modes establishes a second approach to determine the thickness of BSCCO-2212 thin flakes. As BSCCO-2212 is a challenging material to be due to its sensitivity to ambient conditions deriving in an insulating state, the present work provides a guide for the fabrication and characterization of complex van der Waals systems paving the way for studying heterostructures based on unconventional superconductors in the 2D limit.","sentences":["Studying and controlling the properties of individual exfoliated materials is one of the first steps towards the fabrication of complex van der Waals systems.","However, prolonged exposure to ambient conditions can affect the properties of very thin exfoliated materials altering their physical properties.","For this reason, it is imperative to employ versatile characterization strategies compatible with reduced ambient exposure times.","In this work, we demonstrate that optical microscopy and Raman spectroscopy are quick and non-invasive techniques to study flakes of the high-temperature superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (BSCCO-2212).","The apparent color of BSCCO-2212 exfoliated flakes on SiO$_2$/Si has been studied allowing a rough and fast identification of the number of layers.","Moreover, we find that thin flakes have a refractive index of around 1.7 in the visible range and 0.5 for the absorption coefficient near the maximum at 550 nm.","We determine the optimal combination of illumination wavelength and substrate properties for the identification of different numbers of unit cells of BSCCO-2212.","In addition, we report the hardening of the characteristic Raman modes at 116 cm$^{-1}$ and 460 cm$^{-1}$ as flake thickness decreases, possibly due to strain in the BiO and CuO$_2$ planes, respectively.","Moreover, the evolution of the Raman modes establishes a second approach to determine the thickness of BSCCO-2212 thin flakes.","As BSCCO-2212 is a challenging material to be due to its sensitivity to ambient conditions deriving in an insulating state, the present work provides a guide for the fabrication and characterization of complex van der Waals systems paving the way for studying heterostructures based on unconventional superconductors in the 2D limit."],"url":"http://arxiv.org/abs/2402.12941v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 11:38:52","title":"GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks","abstract":"We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals. Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework. Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy. GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes. A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness. Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient. This approximation is a contribution that is likely to be of interest even beyond the scope of the problem studied in this paper. Unlike other state-of-the-art, GRAPHGINI automatically balances all three optimization objectives (utility, individual, and group fairness) of the GNN and is free from any manual tuning of weight parameters. Extensive experimentation on real-world datasets showcases the efficacy of GRAPHGINI in making significant improvements in individual fairness compared to all currently available state-of-the-art methods while maintaining utility and group equality.","sentences":["We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals.","Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework.","Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy.","GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes.","A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness.","Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient.","This approximation is a contribution that is likely to be of interest even beyond the scope of the problem studied in this paper.","Unlike other state-of-the-art, GRAPHGINI automatically balances all three optimization objectives (utility, individual, and group fairness) of the GNN and is free from any manual tuning of weight parameters.","Extensive experimentation on real-world datasets showcases the efficacy of GRAPHGINI in making significant improvements in individual fairness compared to all currently available state-of-the-art methods while maintaining utility and group equality."],"url":"http://arxiv.org/abs/2402.12937v1","category":"cs.LG"}
{"created":"2024-02-20 11:13:15","title":"Neural-Network-Based Optimal Guidance for Lunar Vertical Landing","abstract":"This paper addresses an optimal guidance problem concerning the vertical landing of a lunar lander with the objective of minimizing fuel consumption. The vertical landing imposes a final attitude constraint, which is treated as a final control constraint. To handle this constraint, we propose a nonnegative small regularization term to augment the original cost functional. This ensures the satisfaction of the final control constraint in accordance with Pontryagin's Minimum Principle. By leveraging the necessary conditions for optimality, we establish a parameterized system that facilitates the generation of numerous optimal trajectories, which contain the nonlinear mapping from the flight state to the optimal guidance command. Subsequently, a neural network is trained to approximate such mapping. Finally, numerical examples are presented to validate the proposed method.","sentences":["This paper addresses an optimal guidance problem concerning the vertical landing of a lunar lander with the objective of minimizing fuel consumption.","The vertical landing imposes a final attitude constraint, which is treated as a final control constraint.","To handle this constraint, we propose a nonnegative small regularization term to augment the original cost functional.","This ensures the satisfaction of the final control constraint in accordance with Pontryagin's Minimum Principle.","By leveraging the necessary conditions for optimality, we establish a parameterized system that facilitates the generation of numerous optimal trajectories, which contain the nonlinear mapping from the flight state to the optimal guidance command.","Subsequently, a neural network is trained to approximate such mapping.","Finally, numerical examples are presented to validate the proposed method."],"url":"http://arxiv.org/abs/2402.12920v1","category":"math.OC"}
{"created":"2024-02-20 11:11:03","title":"Compact sum-of-products form of the molecular electronic Hamiltonian based on canonical polyadic decomposition","abstract":"We propose an approach to represent the second-quantized electronic Hamiltonian in a compact sum-of-products (SOP) form. The approach is based on the canonical polyadic decomposition (CPD) of the original Hamiltonian projected onto the sub-Fock spaces formed by groups of spin orbitals. The algorithm for obtaining the canonical polyadic form starts from an exact sum-of-products, which is then optimally compactified using an alternating least-squares procedure. We discuss the relation of this specific SOP with related forms, namely the Tucker format and the matrix product operator often used in conjunction with matrix product states. We benchmark the method on the electronic dynamics of an excited water molecule, trans-polyenes, and the charge migration in glycine upon inner-valence ionization. The quantum dynamics are performed with the multilayer multi-configuration time-dependent Hartree method in second quantization representation (MCTDH-SQR). Other methods based on tree-tensor Ans\\\"atze may profit from this general approach.","sentences":["We propose an approach to represent the second-quantized electronic Hamiltonian in a compact sum-of-products (SOP) form.","The approach is based on the canonical polyadic decomposition (CPD) of the original Hamiltonian projected onto the sub-Fock spaces formed by groups of spin orbitals.","The algorithm for obtaining the canonical polyadic form starts from an exact sum-of-products, which is then optimally compactified using an alternating least-squares procedure.","We discuss the relation of this specific SOP with related forms, namely the Tucker format and the matrix product operator often used in conjunction with matrix product states.","We benchmark the method on the electronic dynamics of an excited water molecule, trans-polyenes, and the charge migration in glycine upon inner-valence ionization.","The quantum dynamics are performed with the multilayer multi-configuration time-dependent Hartree method in second quantization representation (MCTDH-SQR).","Other methods based on tree-tensor Ans\\\"atze may profit from this general approach."],"url":"http://arxiv.org/abs/2402.12919v1","category":"physics.chem-ph"}
{"created":"2024-02-20 11:01:39","title":"OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data","abstract":"This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4.","sentences":["This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track.","This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data.","We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data.","Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters.","Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4."],"url":"http://arxiv.org/abs/2402.12913v1","category":"cs.CL"}
{"created":"2024-02-20 10:50:01","title":"Locally Rainbow Paths","abstract":"We introduce the algorithmic problem of finding a locally rainbow path of length $\\ell$ connecting two distinguished vertices $s$ and $t$ in a vertex-colored directed graph. Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at least $r$ differently colored vertices. This problem generalizes the well-known problem of finding a rainbow path. It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling. We show that the problem is computationally intractable even if $r=2$ or if one looks for a locally rainbow among the shortest paths. On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if $r$ is small, the problem can be solved efficiently. Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails.","sentences":["We introduce the algorithmic problem of finding a locally rainbow path of length $\\ell$ connecting two distinguished vertices $s$ and $t$ in a vertex-colored directed graph.","Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at least $r$ differently colored vertices.","This problem generalizes the well-known problem of finding a rainbow path.","It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling.","We show that the problem is computationally intractable even if $r=2$ or if one looks for a locally rainbow among the shortest paths.","On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if $r$ is small, the problem can be solved efficiently.","Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails."],"url":"http://arxiv.org/abs/2402.12905v1","category":"cs.DS"}
{"created":"2024-02-20 10:49:17","title":"Lipschitz stability for an inverse source problem of the wave equation with kinetic boundary conditions","abstract":"In this paper, we present a refined approach to establish a global Lipschitz stability for an inverse source problem concerning the determination of forcing terms in the wave equation with mixed boundary conditions. It consists of boundary conditions incorporating a dynamic boundary condition and Dirichlet boundary condition on disjoint subsets of the boundary. The primary contribution of this article is the rigorous derivation of a sharp Carleman estimate for the wave system with a dynamic boundary condition. In particular, our findings complete and drastically improve the earlier results established by Gal and Tebou [SIAM J. Control Optim., 55 (2017), 324-364]. This is achieved by using a different weight function to overcome some relevant difficulties. As for the stability proof, we extend to dynamic boundary conditions a recent argument avoiding cut-off functions. Finally, we also show that our developed Carleman estimate yields a sharp boundary controllability result.","sentences":["In this paper, we present a refined approach to establish a global Lipschitz stability for an inverse source problem concerning the determination of forcing terms in the wave equation with mixed boundary conditions.","It consists of boundary conditions incorporating a dynamic boundary condition and Dirichlet boundary condition on disjoint subsets of the boundary.","The primary contribution of this article is the rigorous derivation of a sharp Carleman estimate for the wave system with a dynamic boundary condition.","In particular, our findings complete and drastically improve the earlier results established by Gal and Tebou","[SIAM J. Control Optim., 55 (2017), 324-364].","This is achieved by using a different weight function to overcome some relevant difficulties.","As for the stability proof, we extend to dynamic boundary conditions a recent argument avoiding cut-off functions.","Finally, we also show that our developed Carleman estimate yields a sharp boundary controllability result."],"url":"http://arxiv.org/abs/2402.12902v1","category":"math.AP"}
{"created":"2024-02-20 10:36:09","title":"Extensive search for axion dark matter over 1\\,GHz with CAPP's Main Axion eXperiment","abstract":"We report an extensive high-sensitivity search for axion dark matter above 1\\,GHz at the Center for Axion and Precision Physics Research (CAPP). The cavity resonant search, exploiting the coupling between axions and photons, explored the frequency (mass) range of 1.025\\,GHz (4.24\\,$\\mu$eV) to 1.185\\,GHz (4.91\\,$\\mu$eV). We have introduced a number of innovations in this field, demonstrating the practical approach of optimizing all the relevant parameters of axion haloscopes, extending presently available technology. The CAPP 12\\,T magnet with an aperture of 320\\,mm made of Nb$_3$Sn and NbTi superconductors surrounding a 37-liter ultralight-weight copper cavity is expected to convert DFSZ axions into approximately $10^2$ microwave photons per second. A powerful dilution refrigerator, capable of keeping the core system below 40\\,mK, combined with quantum-noise limited readout electronics, achieved a total system noise of about 200\\,mK or below, which corresponds to a background of roughly $4\\times 10^3$ photons per second within the axion bandwidth. The combination of all those improvements provides unprecedented search performance, imposing the most stringent exclusion limits on axion--photon coupling in this frequency range to date. These results also suggest an experimental capability suitable for highly-sensitive searches for axion dark matter above 1\\,GHz.","sentences":["We report an extensive high-sensitivity search for axion dark matter above 1\\,GHz at the Center for Axion and Precision Physics Research (CAPP).","The cavity resonant search, exploiting the coupling between axions and photons, explored the frequency (mass) range of 1.025\\,GHz (4.24\\,$\\mu$eV) to 1.185\\,GHz (4.91\\,$\\mu$eV).","We have introduced a number of innovations in this field, demonstrating the practical approach of optimizing all the relevant parameters of axion haloscopes, extending presently available technology.","The CAPP 12\\,T magnet with an aperture of 320\\,mm made of Nb$_3$Sn and NbTi superconductors surrounding a 37-liter ultralight-weight copper cavity is expected to convert DFSZ axions into approximately $10^2$ microwave photons per second.","A powerful dilution refrigerator, capable of keeping the core system below 40\\,mK, combined with quantum-noise limited readout electronics, achieved a total system noise of about 200\\,mK or below, which corresponds to a background of roughly $4\\times 10^3$ photons per second within the axion bandwidth.","The combination of all those improvements provides unprecedented search performance, imposing the most stringent exclusion limits on axion--photon coupling in this frequency range to date.","These results also suggest an experimental capability suitable for highly-sensitive searches for axion dark matter above 1\\,GHz."],"url":"http://arxiv.org/abs/2402.12892v1","category":"hep-ex"}
{"created":"2024-02-20 10:33:45","title":"BFT-DSN: A Byzantine Fault Tolerant Decentralized Storage Network","abstract":"With the rapid development of blockchain and its applications, the amount of data stored on decentralized storage networks (DSNs) has grown exponentially. DSNs bring together affordable storage resources from around the world to provide robust, decentralized storage services for tens of thousands of decentralized applications (dApps). However, existing DSNs do not offer verifiability when implementing erasure coding for redundant storage, making them vulnerable to Byzantine encoders. Additionally, there is a lack of Byzantine fault-tolerant consensus for optimal resilience in DSNs. This paper introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network designed to address these challenges. BFT-DSN combines storage-weighted BFT consensus with erasure coding and incorporates homomorphic fingerprints and weighted threshold signatures for decentralized verification. The implementation of BFT-DSN demonstrates its comparable performance in terms of storage cost and latency as well as superior performance in Byzantine resilience when compared to existing industrial decentralized storage networks.","sentences":["With the rapid development of blockchain and its applications, the amount of data stored on decentralized storage networks (DSNs) has grown exponentially.","DSNs bring together affordable storage resources from around the world to provide robust, decentralized storage services for tens of thousands of decentralized applications (dApps).","However, existing DSNs do not offer verifiability when implementing erasure coding for redundant storage, making them vulnerable to Byzantine encoders.","Additionally, there is a lack of Byzantine fault-tolerant consensus for optimal resilience in DSNs.","This paper introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network designed to address these challenges.","BFT-DSN combines storage-weighted BFT consensus with erasure coding and incorporates homomorphic fingerprints and weighted threshold signatures for decentralized verification.","The implementation of BFT-DSN demonstrates its comparable performance in terms of storage cost and latency as well as superior performance in Byzantine resilience when compared to existing industrial decentralized storage networks."],"url":"http://arxiv.org/abs/2402.12889v1","category":"cs.CR"}
{"created":"2024-02-20 10:13:44","title":"Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study","abstract":"The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable insights into the strengths and limitations of existing baseline methods, contributing to the ongoing discourse on optimal FMTL application in practical scenarios. The source code will be made available for results replication.","sentences":["The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets.","However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field.","This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm.","This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios.","We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption.","Through our exhaustive experiments, we aim to provide valuable insights into the strengths and limitations of existing baseline methods, contributing to the ongoing discourse on optimal FMTL application in practical scenarios.","The source code will be made available for results replication."],"url":"http://arxiv.org/abs/2402.12876v1","category":"cs.LG"}
{"created":"2024-02-20 10:09:00","title":"Skill or Luck? Return Decomposition via Advantage Functions","abstract":"Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance.","sentences":["Learning from off-policy data is essential for sample-efficient reinforcement learning.","In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck).","Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE).","The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions.","We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important.","Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance."],"url":"http://arxiv.org/abs/2402.12874v1","category":"cs.LG"}
{"created":"2024-02-20 10:06:30","title":"Deep, convergent, unrolled half-quadratic splitting for image deconvolution","abstract":"In recent years, algorithm unrolling has emerged as a powerful technique for designing interpretable neural networks based on iterative algorithms. Imaging inverse problems have particularly benefited from unrolling-based deep network design since many traditional model-based approaches rely on iterative optimization. Despite exciting progress, typical unrolling approaches heuristically design layer-specific convolution weights to improve performance. Crucially, convergence properties of the underlying iterative algorithm are lost once layer-specific parameters are learned from training data. We propose an unrolling technique that breaks the trade-off between retaining algorithm properties while simultaneously enhancing performance. We focus on image deblurring and unrolling the widely-applied Half-Quadratic Splitting (HQS) algorithm. We develop a new parametrization scheme which enforces layer-specific parameters to asymptotically approach certain fixed points. Through extensive experimental studies, we verify that our approach achieves competitive performance with state-of-the-art unrolled layer-specific learning and significantly improves over the traditional HQS algorithm. We further establish convergence of the proposed unrolled network as the number of layers approaches infinity, and characterize its convergence rate. Our experimental verification involves simulations that validate the analytical results as well as comparison with state-of-the-art non-blind deblurring techniques on benchmark datasets. The merits of the proposed convergent unrolled network are established over competing alternatives, especially in the regime of limited training.","sentences":["In recent years, algorithm unrolling has emerged as a powerful technique for designing interpretable neural networks based on iterative algorithms.","Imaging inverse problems have particularly benefited from unrolling-based deep network design since many traditional model-based approaches rely on iterative optimization.","Despite exciting progress, typical unrolling approaches heuristically design layer-specific convolution weights to improve performance.","Crucially, convergence properties of the underlying iterative algorithm are lost once layer-specific parameters are learned from training data.","We propose an unrolling technique that breaks the trade-off between retaining algorithm properties while simultaneously enhancing performance.","We focus on image deblurring and unrolling the widely-applied Half-Quadratic Splitting (HQS) algorithm.","We develop a new parametrization scheme which enforces layer-specific parameters to asymptotically approach certain fixed points.","Through extensive experimental studies, we verify that our approach achieves competitive performance with state-of-the-art unrolled layer-specific learning and significantly improves over the traditional HQS algorithm.","We further establish convergence of the proposed unrolled network as the number of layers approaches infinity, and characterize its convergence rate.","Our experimental verification involves simulations that validate the analytical results as well as comparison with state-of-the-art non-blind deblurring techniques on benchmark datasets.","The merits of the proposed convergent unrolled network are established over competing alternatives, especially in the regime of limited training."],"url":"http://arxiv.org/abs/2402.12872v1","category":"eess.IV"}
{"created":"2024-02-20 10:04:51","title":"Interface Identification constrained by Local-to-Nonlocal Coupling","abstract":"Models of physical phenomena that use nonlocal operators are better suited for some applications than their classical counterparts that employ partial differential operators. However, the numerical solution of these nonlocal problems can be quite expensive. Therefore, Local-to-Nonlocal couplings have emerged that combine partial differential operators with nonlocal operators. In this work, we make use of an energy-based Local-to-Nonlocal coupling that serves as a constraint for an interface identification problem.","sentences":["Models of physical phenomena that use nonlocal operators are better suited for some applications than their classical counterparts that employ partial differential operators.","However, the numerical solution of these nonlocal problems can be quite expensive.","Therefore, Local-to-Nonlocal couplings have emerged that combine partial differential operators with nonlocal operators.","In this work, we make use of an energy-based Local-to-Nonlocal coupling that serves as a constraint for an interface identification problem."],"url":"http://arxiv.org/abs/2402.12871v1","category":"math.OC"}
{"created":"2024-02-20 09:54:52","title":"Finding Cross-rule Optimization Bugs in Datalog Engines","abstract":"Datalog is a popular and widely-used declarative logic programming language. Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results. To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem. The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine. Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules. Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated-we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts. We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\\'e, CozoDB, $\\mu$Z, and DDlog, and discovered a total of 30 bugs. Of these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5. Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17$\\times$ (for DDlog) to 31.02$\\times$ (for Souffl\\'e) as many valid test cases with non-empty results as the naive random method. We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice.","sentences":["Datalog is a popular and widely-used declarative logic programming language.","Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results.","To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem.","The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine.","Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules.","Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated-we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts.","We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\\'e, CozoDB, $\\mu$Z, and DDlog, and discovered a total of 30 bugs.","Of these, 13 were logic bugs, while the remaining were crash and error bugs.","Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach.","Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5.","Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17$\\times$ (for DDlog) to 31.02$\\times$ (for Souffl\\'e) as many valid test cases with non-empty results as the naive random method.","We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice."],"url":"http://arxiv.org/abs/2402.12863v1","category":"cs.SE"}
{"created":"2024-02-20 09:43:00","title":"ATLAS: A Model of Short-term European Electricity Market Processes under Uncertainty -- Balancing Modules","abstract":"The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes. ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail. Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints -- both regarding different production units and the different market constraints. This working paper describes the simulated balancing processes in detail and is the second part of the ATLAS documentation.","sentences":["The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes.","ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail.","Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints -- both regarding different production units and the different market constraints.","This working paper describes the simulated balancing processes in detail and is the second part of the ATLAS documentation."],"url":"http://arxiv.org/abs/2402.12859v1","category":"econ.GN"}
{"created":"2024-02-20 09:35:11","title":"Partial null-controllabiliy of evolution equations by one-dimensional control","abstract":"The problem of partial null controllability for linear autonomous evolution equations, which are controlled by a one-dimensional control, is under consideration. The partial null-controllability conditions for coupled abstract evolution systems have been obtained using the moment problem approach.","sentences":["The problem of partial null controllability for linear autonomous evolution equations, which are controlled by a one-dimensional control, is under consideration.","The partial null-controllability conditions for coupled abstract evolution systems have been obtained using the moment problem approach."],"url":"http://arxiv.org/abs/2402.12855v1","category":"math.OC"}
{"created":"2024-02-20 09:33:22","title":"Differentiable Mapper For Topological Optimization Of Data Representation","abstract":"Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself. In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs. In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated. Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones.","sentences":["Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science.","Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself.","While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures.","However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself.","In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs.","In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated.","Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones."],"url":"http://arxiv.org/abs/2402.12854v1","category":"cs.LG"}
{"created":"2024-02-20 09:27:09","title":"ATLAS: A Model of Short-term European Electricity Market Processes under Uncertainty","abstract":"The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes. ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail. Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints--both regarding different production units and the different market constraints.","sentences":["The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes.","ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail.","Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints--both regarding different production units and the different market constraints."],"url":"http://arxiv.org/abs/2402.12848v1","category":"econ.GN"}
{"created":"2024-02-20 09:01:28","title":"SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs","abstract":"Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power architectures designed to accelerate Compute-Intensive Loops (CILs). The effectiveness of CGRAs in providing acceleration relies on the quality of mapping: how efficiently the CIL is compiled onto the platform. State of the Art (SoA) compilation techniques utilize modulo scheduling to minimize the Iteration Interval (II) and use graph algorithms like Max-Clique Enumeration to address mapping challenges. Our work approaches the mapping problem through a satisfiability (SAT) formulation. We introduce the Kernel Mobility Schedule (KMS), an ad-hoc schedule used with the Data Flow Graph and CGRA architectural information to generate Boolean statements that, when satisfied, yield a valid mapping. Experimental results demonstrate SAT-MapIt outperforming SoA alternatives in almost 50\\% of explored benchmarks. Additionally, we evaluated the mapping results in a synthesizable CGRA design and emphasized the run-time metrics trends, i.e. energy efficiency and latency, across different CILs and CGRA sizes. We show that a hardware-agnostic analysis performed on compiler-level metrics can optimally prune the architectural design space, while still retaining Pareto-optimal configurations. Moreover, by exploring how implementation details impact cost and performance on real hardware, we highlight the importance of holistic software-to-hardware mapping flows, as the one presented herein.","sentences":["Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power architectures designed to accelerate Compute-Intensive Loops (CILs).","The effectiveness of CGRAs in providing acceleration relies on the quality of mapping: how efficiently the CIL is compiled onto the platform.","State of the Art (SoA) compilation techniques utilize modulo scheduling to minimize the Iteration Interval (II) and use graph algorithms like Max-Clique Enumeration to address mapping challenges.","Our work approaches the mapping problem through a satisfiability (SAT) formulation.","We introduce the Kernel Mobility Schedule (KMS), an ad-hoc schedule used with the Data Flow Graph and CGRA architectural information to generate Boolean statements that, when satisfied, yield a valid mapping.","Experimental results demonstrate SAT-MapIt outperforming SoA alternatives in almost 50\\% of explored benchmarks.","Additionally, we evaluated the mapping results in a synthesizable CGRA design and emphasized the run-time metrics trends, i.e. energy efficiency and latency, across different CILs and CGRA sizes.","We show that a hardware-agnostic analysis performed on compiler-level metrics can optimally prune the architectural design space, while still retaining Pareto-optimal configurations.","Moreover, by exploring how implementation details impact cost and performance on real hardware, we highlight the importance of holistic software-to-hardware mapping flows, as the one presented herein."],"url":"http://arxiv.org/abs/2402.12834v1","category":"cs.AR"}
{"created":"2024-02-20 08:58:39","title":"Integrating Multi-Preconditioned Conjugate Gradient with Additive Multigrid Strategy","abstract":"Due to its optimal complexity, the multigrid (MG) method is one of the most popular approaches for solving large-scale linear systems arising from the discretization of partial differential equations. However, the parallel implementation of standard MG methods, which are inherently multiplicative, suffers from increasing communication complexity. In such cases, the additive variants of MG methods provide a good alternative due to their inherently parallel nature, although they exhibit slower convergence. This work combines the additive multigrid method with the multipreconditioned conjugate gradient (MPCG) method. In the proposed approach, the MPCG method employs the corrections from the different levels of the MG hierarchy as separate preconditioned search directions. In this approach, the MPCG method updates the current iterate by using the linear combination of the preconditioned search directions, where the optimal coefficients for the linear combination are computed by exploiting the energy norm minimization of the CG method. The idea behind our approach is to combine the $A$-conjugacy of the search directions of the MPCG method and the quasi $H_1$-orthogonality of the corrections from the MG hierarchy. In the numerical section, we study the performance of the proposed method compared to the standard additive and multiplicative MG methods used as preconditioners for the CG method.","sentences":["Due to its optimal complexity, the multigrid (MG) method is one of the most popular approaches for solving large-scale linear systems arising from the discretization of partial differential equations.","However, the parallel implementation of standard MG methods, which are inherently multiplicative, suffers from increasing communication complexity.","In such cases, the additive variants of MG methods provide a good alternative due to their inherently parallel nature, although they exhibit slower convergence.","This work combines the additive multigrid method with the multipreconditioned conjugate gradient (MPCG) method.","In the proposed approach, the MPCG method employs the corrections from the different levels of the MG hierarchy as separate preconditioned search directions.","In this approach, the MPCG method updates the current iterate by using the linear combination of the preconditioned search directions, where the optimal coefficients for the linear combination are computed by exploiting the energy norm minimization of the CG method.","The idea behind our approach is to combine the $A$-conjugacy of the search directions of the MPCG method and the quasi $H_1$-orthogonality of the corrections from the MG hierarchy.","In the numerical section, we study the performance of the proposed method compared to the standard additive and multiplicative MG methods used as preconditioners for the CG method."],"url":"http://arxiv.org/abs/2402.12833v1","category":"math.NA"}
{"created":"2024-02-20 08:58:37","title":"Nearly Optimal Fault Tolerant Distance Oracle","abstract":"We present an $f$-fault tolerant distance oracle for an undirected weighted graph where each edge has an integral weight from $[1 \\dots W]$. Given a set $F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our oracle returns the \\emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf \\log (nW))^{O(f^2)})$ time, where $c > 1$ is a constant. The space complexity of our oracle is $O(f^4n^2\\log^2 (nW))$. For a constant $f$, our oracle is nearly optimal both in terms of space and time (barring some logarithmic factor).","sentences":["We present an $f$-fault tolerant distance oracle for an undirected weighted graph where each edge has an integral weight from $[1 \\dots W]$. Given a set $F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our oracle returns the \\emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf \\log (nW))^{O(f^2)})$ time, where $c > 1$ is a constant.","The space complexity of our oracle is $O(f^4n^2\\log^2 (nW))$. For a constant $f$, our oracle is nearly optimal both in terms of space and time (barring some logarithmic factor)."],"url":"http://arxiv.org/abs/2402.12832v2","category":"cs.DS"}
{"created":"2024-02-20 08:58:10","title":"A sparse hierarchical $hp$-finite element method on disks and annuli","abstract":"We develop a sparse hierarchical $hp$-finite element method ($hp$-FEM) for the Helmholtz equation with rotationally invariant variable coefficients posed on a two-dimensional disk or annulus. The mesh is an inner disk cell (omitted if on an annulus domain) and concentric annuli cells. The discretization preserves the Fourier mode decoupling of rotationally invariant operators, such as the Laplacian, which manifests as block diagonal mass and stiffness matrices. Moreover, the matrices have a sparsity pattern independent of the order of the discretization and admit an optimal complexity factorization. The sparse $hp$-FEM can handle radial discontinuities in the right-hand side and in rotationally invariant Helmholtz coefficients. We consider examples such as a high-frequency Helmholtz equation with radial discontinuities, the time-dependent Schr\\\"odinger equation, and an extension to a three-dimensional cylinder domain, with a quasi-optimal solve, via the Alternating Direction Implicit (ADI) algorithm.","sentences":["We develop a sparse hierarchical $hp$-finite element method ($hp$-FEM) for the Helmholtz equation with rotationally invariant variable coefficients posed on a two-dimensional disk or annulus.","The mesh is an inner disk cell (omitted if on an annulus domain) and concentric annuli cells.","The discretization preserves the Fourier mode decoupling of rotationally invariant operators, such as the Laplacian, which manifests as block diagonal mass and stiffness matrices.","Moreover, the matrices have a sparsity pattern independent of the order of the discretization and admit an optimal complexity factorization.","The sparse $hp$-FEM can handle radial discontinuities in the right-hand side and in rotationally invariant Helmholtz coefficients.","We consider examples such as a high-frequency Helmholtz equation with radial discontinuities, the time-dependent Schr\\\"odinger equation, and an extension to a three-dimensional cylinder domain, with a quasi-optimal solve, via the Alternating Direction Implicit (ADI) algorithm."],"url":"http://arxiv.org/abs/2402.12831v1","category":"math.NA"}
{"created":"2024-02-20 08:54:07","title":"SGD with Clipping is Secretly Estimating the Median Gradient","abstract":"There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.","sentences":["There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient.","For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself.","Here we study SGD with robust gradient estimators based on estimating the median.","We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise.","We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof.","Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework."],"url":"http://arxiv.org/abs/2402.12828v1","category":"stat.ML"}
{"created":"2024-02-20 08:30:46","title":"Scalable Decentralized Algorithms for Online Personalized Mean Estimation","abstract":"In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based approach, with complexity of O( r |A| log |A|) and O(r |A|), respectively. We establish conditions under which both algorithms yield asymptotically optimal estimates and offer a theoretical characterization of their performance.","sentences":["In numerous settings, agents lack sufficient data to directly learn a model.","Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ.","A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved.","This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean.","Existing algorithms face impractical space and time complexities (quadratic in the number of agents A).","To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based approach, with complexity of O( r |A| log |A|) and O(r |A|), respectively.","We establish conditions under which both algorithms yield asymptotically optimal estimates and offer a theoretical characterization of their performance."],"url":"http://arxiv.org/abs/2402.12812v1","category":"cs.LG"}
