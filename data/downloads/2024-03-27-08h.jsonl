{"created":"2024-03-26 17:59:52","title":"ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis","abstract":"Gestures play a key role in human communication. Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance. Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words. Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis. Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing. Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures. To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another. We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks. We urge the reader to watch our supplementary video at our website.","sentences":["Gestures play a key role in human communication.","Recent methods for co-speech gesture generation, while managing to generate beat-aligned motions, struggle generating gestures that are semantically aligned with the utterance.","Compared to beat gestures that align naturally to the audio signal, semantically coherent gestures require modeling the complex interactions between the language and human motion, and can be controlled by focusing on certain words.","Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal gesture synthesis, which can not only generate gestures based on multi-modal speech inputs, but can also facilitate controllability in gesture synthesis.","Our method proposes two guidance objectives that allow the users to modulate the impact of different conditioning modalities (e.g. audio vs text) as well as to choose certain words to be emphasized during gesturing.","Our method is versatile in that it can be trained either for generating monologue gestures or even the conversational gestures.","To further advance the research on multi-party interactive gestures, the DnD Group Gesture dataset is released, which contains 6 hours of gesture data showing 5 people interacting with one another.","We compare our method with several recent works and demonstrate effectiveness of our method on a variety of tasks.","We urge the reader to watch our supplementary video at our website."],"url":"http://arxiv.org/abs/2403.17936v1","category":"cs.CV"}
{"created":"2024-03-26 17:59:24","title":"OmniVid: A Generative Framework for Universal Video Understanding","abstract":"The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding. Code is available at https://github.com/wangjk666/OmniVid.","sentences":["The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution.","Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats.","In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora.","Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens.","In this way, a variety of video tasks could be formulated as video-grounded token generation.","This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework.","Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding.","Code is available at https://github.com/wangjk666/OmniVid."],"url":"http://arxiv.org/abs/2403.17935v1","category":"cs.CV"}
{"created":"2024-03-26 17:58:29","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models","abstract":"SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density. Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan. It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model. Compared to nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it a more accessible option and helping with democratizing future research in this field.","sentences":["SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs.","Its core component is a learned model that is able to generate agent bounding boxes and lane graphs.","The model's outputs serve as an initial state for traffic simulation.","The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial.","Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE).","It encodes agents and the lane graph into distinct channels in a rasterized latent map.","This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer.","Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns or increasing traffic density.","Further, SLEDGE can support 500m long routes, a capability not found in existing data-driven simulators like nuPlan.","It presents new challenges for planning algorithms, evidenced by failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge, when tested on hard routes and dense traffic generated by our model.","Compared to nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it a more accessible option and helping with democratizing future research in this field."],"url":"http://arxiv.org/abs/2403.17933v1","category":"cs.RO"}
{"created":"2024-03-26 17:58:02","title":"The instability mechanism of compact multiplanet systems","abstract":"To improve our understanding of orbital instabilities in compact planetary systems, we compare suites of $N$-body simulations against numerical integrations of simplified dynamical models. We show that, surprisingly, dynamical models that account for small sets of resonant interactions between the planets can accurately recover $N$-body instability times. This points towards a simple physical picture in which a handful of three-body resonances (3BRs), generated by interactions between nearby two-body mean motion resonances (MMRs), overlap and drive chaotic diffusion, leading to instability. Motivated by this, we show that instability times are well-described by a power law relating instability time to planet separations, measured in units of fractional semi-major axis difference divided by the planet-to-star mass ratio to the $1/4$ power, rather than the frequently-adopted $1/3$ power implied by measuring separations in units of mutual Hill radii. For idealized systems, the parameters of this power-law relationship depend only on the ratio of the planets' orbital eccentricities to the orbit-crossing value, and we report an empirical fit to enable quick instability time predictions. This relationship predicts that observed systems comprised of three or more sub-Neptune mass planets must be spaced with period ratios $\\mathcal{P} \\gtrsim 1.35$ and that tightly-spaced systems ($\\mathcal{P} \\lesssim 1.5$) must possess very low eccentricities ($e \\lesssim 0.05$) to be stable for more than $10^9$ orbits.","sentences":["To improve our understanding of orbital instabilities in compact planetary systems, we compare suites of $N$-body simulations against numerical integrations of simplified dynamical models.","We show that, surprisingly, dynamical models that account for small sets of resonant interactions between the planets can accurately recover $N$-body instability times.","This points towards a simple physical picture in which a handful of three-body resonances (3BRs), generated by interactions between nearby two-body mean motion resonances (MMRs), overlap and drive chaotic diffusion, leading to instability.","Motivated by this, we show that instability times are well-described by a power law relating instability time to planet separations, measured in units of fractional semi-major axis difference divided by the planet-to-star mass ratio to the $1/4$ power, rather than the frequently-adopted $1/3$ power implied by measuring separations in units of mutual Hill radii.","For idealized systems, the parameters of this power-law relationship depend only on the ratio of the planets' orbital eccentricities to the orbit-crossing value, and we report an empirical fit to enable quick instability time predictions.","This relationship predicts that observed systems comprised of three or more sub-Neptune mass planets must be spaced with period ratios $\\mathcal{P} \\gtrsim 1.35$ and that tightly-spaced systems ($\\mathcal{P} \\lesssim 1.5$) must possess very low eccentricities ($e \\lesssim 0.05$) to be stable for more than $10^9","$ orbits."],"url":"http://arxiv.org/abs/2403.17928v1","category":"astro-ph.EP"}
{"created":"2024-03-26 17:57:57","title":"MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution","abstract":"In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.","sentences":["In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities.","Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level.","To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors.","Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents.","This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues.","In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2.","MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines.","Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method.","We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc."],"url":"http://arxiv.org/abs/2403.17927v1","category":"cs.SE"}
{"created":"2024-03-26 17:57:20","title":"Testing the $\\mathbf\u039b$CDM Cosmological Model with Forthcoming Measurements of the Cosmic Microwave Background with SPT-3G","abstract":"We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope. The surveys cover separate regions of 1500, 2650, and 6000 ${\\rm deg}^{2}$ to different depths, in total observing 25% of the sky. These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\\mu{\\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024. The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\\sqrt{\\text{2}}$ times higher than the temperature noise. We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers. We demonstrate the ability to test the $\\Lambda{\\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\\Lambda{\\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data. The $\\Lambda{\\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model.","sentences":["We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope.","The surveys cover separate regions of 1500, 2650, and 6000 ${\\rm deg}^{2}$ to different depths, in total observing 25% of the sky.","These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\\mu{\\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024.","The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\\sqrt{\\text{2}}$ times higher than the temperature noise.","We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers.","We demonstrate the ability to test the $\\Lambda{\\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\\Lambda{\\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data.","The $\\Lambda{\\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model."],"url":"http://arxiv.org/abs/2403.17925v1","category":"astro-ph.CO"}
{"created":"2024-03-26 17:57:05","title":"AID: Attention Interpolation of Text-to-Image Diffusion","abstract":"Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation. Our approach demonstrates effectiveness for conceptual and spatial interpolation. Code and demo are available at https://github.com/QY-H00/attention-interpolation-diffusion.","sentences":["Conditional diffusion models can create unseen images in various settings, aiding image interpolation.","Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood.","Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity.","To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID).","Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness.","We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process.","This method enables the creation of new images with greater consistency, smoothness, and efficiency, and offers control over the exact path of interpolation.","Our approach demonstrates effectiveness for conceptual and spatial interpolation.","Code and demo are available at https://github.com/QY-H00/attention-interpolation-diffusion."],"url":"http://arxiv.org/abs/2403.17924v1","category":"cs.CV"}
{"created":"2024-03-26 17:55:58","title":"The Need for Speed: Pruning Transformers with One Recipe","abstract":"We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitive accuracy performance and improved throughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP baselines and a $0.5$% improvement from state-of-the-art methods on image classification at competitive FLOPs reductions. We further demonstrate the generalization of tasks and architecture with comparative performance using Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents one of the first one-shot efficient frameworks for compressing transformer architectures that generalizes well across different class domains, in particular: natural language and image-related tasks, without $\\textit{re-training}$.","sentences":["We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique for $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption.","To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitive accuracy performance and improved throughput.","Particularly, we show a $\\leq 2$% accuracy degradation from NLP baselines and a $0.5$% improvement from state-of-the-art methods on image classification at competitive FLOPs reductions.","We further demonstrate the generalization of tasks and architecture with comparative performance using Mask2Former for semantic segmentation and cnn-style networks.","OPTIN presents one of the first one-shot efficient frameworks for compressing transformer architectures that generalizes well across different class domains, in particular: natural language and image-related tasks, without $\\textit{re-training}$."],"url":"http://arxiv.org/abs/2403.17921v1","category":"cs.LG"}
{"created":"2024-03-26 17:55:11","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","abstract":"Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models. However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering. The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components. We represent the global motion of a scene's bounding box using rigid transformation along a trajectory parameterized by a spline. We learn local deformations that conform to the global trajectory using supervision from a text-to-video model. Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study. Video results can be viewed on our website: https://sherwinbahmani.github.io/tc4d.","sentences":["Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models.","However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering.","The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models.","Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components.","We represent the global motion of a scene's bounding box using rigid transformation along a trajectory parameterized by a spline.","We learn local deformations that conform to the global trajectory using supervision from a text-to-video model.","Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study.","Video results can be viewed on our website: https://sherwinbahmani.github.io/tc4d."],"url":"http://arxiv.org/abs/2403.17920v1","category":"cs.CV"}
{"created":"2024-03-26 17:55:02","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning","abstract":"The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.","sentences":["The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training.","Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings.","Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers.","Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA.","We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization.","Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench scores.","On large models, specifically LLaMA-2-70B, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains."],"url":"http://arxiv.org/abs/2403.17919v1","category":"cs.LG"}
{"created":"2024-03-26 17:54:15","title":"AgentStudio: A Toolkit for Building General Virtual Agents","abstract":"Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings. To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces. Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc. We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future.","sentences":["Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence.","Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities.","To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development.","This includes environment setups, data collection, agent evaluation, and visualization.","The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces.","This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings.","To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both created with our graphical interfaces.","Furthermore, we present several actionable insights derived from AgentStudio, e.g., general visual grounding, open-ended tool creation, learning from videos, etc.","We have open-sourced the environments, datasets, benchmarks, and interfaces to promote research towards developing general virtual agents for the future."],"url":"http://arxiv.org/abs/2403.17918v1","category":"cs.AI"}
{"created":"2024-03-26 17:53:27","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","abstract":"The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction. Through extensive experiments and ablation studies, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction tasks. In particular, CMP reduces the average prediction error by 17.2\\% with fewer missing detections compared with the no cooperation setting. Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios.","sentences":["The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs).","Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction.","Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities.","Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules.","Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations.","We also propose a prediction aggregation module, which unifies the predictions obtained by different CAVs and generates the final prediction.","Through extensive experiments and ablation studies, we demonstrate the effectiveness of our method in cooperative perception, tracking, and motion prediction tasks.","In particular, CMP reduces the average prediction error by 17.2\\% with fewer missing detections compared with the no cooperation setting.","Our work marks a significant step forward in the cooperative capabilities of CAVs, showcasing enhanced performance in complex scenarios."],"url":"http://arxiv.org/abs/2403.17916v1","category":"cs.RO"}
{"created":"2024-03-26 17:51:06","title":"Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports","abstract":"A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we need to understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT. To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels. The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB). It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem.","sentences":["A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety.","To better use those reports, we need to understand the most important events or impact factors according to the accident reports.","However, the increasing number of accident reports requires large efforts from domain experts to label those reports.","In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically.","This article argues that we can identify the events more accurately by leveraging the event taxonomy.","More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information.","We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT.","To further utilize the information from event taxonomy, we regularize the proposed model according to the relationship and distribution among labels.","The effectiveness of our framework is evaluated with the data collected by National Transportation Safety Board (NTSB).","It has been shown that fine-level prediction accuracy is highly improved, and the regularization term can be beneficial to the rare event identification problem."],"url":"http://arxiv.org/abs/2403.17914v1","category":"cs.AI"}
{"created":"2024-03-26 17:51:05","title":"Enhancing Indoor and Outdoor THz Communications with Beyond Diagonal-IRS: Optimization and Performance Analysis","abstract":"This work investigates the application of Beyond Diagonal Intelligent Reflective Surface (BD-IRS) to enhance THz downlink communication systems, operating in a hybrid: reflective and transmissive mode, to simultaneously provide services to indoor and outdoor users. We propose an optimization framework that jointly optimizes the beamforming vectors and phase shifts in the hybrid reflective/transmissive mode, aiming to maximize the system sum rate. To tackle the challenges in solving the joint design problem, we employ the conjugate gradient method and propose an iterative algorithm that successively optimizes the hybrid beamforming vectors and the phase shifts. Through comprehensive numerical simulations, our findings demonstrate a significant improvement in rate when compared to existing benchmark schemes, including time- and frequency-divided approaches, by approximately $30.5\\%$ and $70.28\\%$ respectively. This underscores the significant influence of IRS elements on system performance relative to that of base station antennas, highlighting their pivotal role in advancing the communication system efficacy.","sentences":["This work investigates the application of Beyond Diagonal Intelligent Reflective Surface (BD-IRS) to enhance THz downlink communication systems, operating in a hybrid: reflective and transmissive mode, to simultaneously provide services to indoor and outdoor users.","We propose an optimization framework that jointly optimizes the beamforming vectors and phase shifts in the hybrid reflective/transmissive mode, aiming to maximize the system sum rate.","To tackle the challenges in solving the joint design problem, we employ the conjugate gradient method and propose an iterative algorithm that successively optimizes the hybrid beamforming vectors and the phase shifts.","Through comprehensive numerical simulations, our findings demonstrate a significant improvement in rate when compared to existing benchmark schemes, including time- and frequency-divided approaches, by approximately $30.5\\%$ and $70.28\\%$ respectively.","This underscores the significant influence of IRS elements on system performance relative to that of base station antennas, highlighting their pivotal role in advancing the communication system efficacy."],"url":"http://arxiv.org/abs/2403.17913v1","category":"eess.SP"}
{"created":"2024-03-26 17:47:25","title":"Domain-Specific Evaluation Strategies for AI in Journalism","abstract":"News organizations today rely on AI tools to increase efficiency and productivity across various tasks in news production and distribution. These tools are oriented towards stakeholders such as reporters, editors, and readers. However, practitioners also express reservations around adopting AI technologies into the newsroom, due to the technical and ethical challenges involved in evaluating AI technology and its return on investments. This is to some extent a result of the lack of domain-specific strategies to evaluate AI models and applications. In this paper, we consider different aspects of AI evaluation (model outputs, interaction, and ethics) that can benefit from domain-specific tailoring, and suggest examples of how journalistic considerations can lead to specialized metrics or strategies. In doing so, we lay out a potential framework to guide AI evaluation in journalism, such as seen in other disciplines (e.g. law, healthcare). We also consider directions for future work, as well as how our approach might generalize to other domains.","sentences":["News organizations today rely on AI tools to increase efficiency and productivity across various tasks in news production and distribution.","These tools are oriented towards stakeholders such as reporters, editors, and readers.","However, practitioners also express reservations around adopting AI technologies into the newsroom, due to the technical and ethical challenges involved in evaluating AI technology and its return on investments.","This is to some extent a result of the lack of domain-specific strategies to evaluate AI models and applications.","In this paper, we consider different aspects of AI evaluation (model outputs, interaction, and ethics) that can benefit from domain-specific tailoring, and suggest examples of how journalistic considerations can lead to specialized metrics or strategies.","In doing so, we lay out a potential framework to guide AI evaluation in journalism, such as seen in other disciplines (e.g. law, healthcare).","We also consider directions for future work, as well as how our approach might generalize to other domains."],"url":"http://arxiv.org/abs/2403.17911v1","category":"cs.CY"}
{"created":"2024-03-26 17:45:48","title":"Multi-Agent Resilient Consensus under Intermittent Faulty and Malicious Transmissions (Extended Version)","abstract":"In this work, we consider the consensus problem in which legitimate agents share their values over an undirected communication network in the presence of malicious or faulty agents. Different from the previous works, we characterize the conditions that generalize to several scenarios such as intermittent faulty or malicious transmissions, based on trust observations. As the standard trust aggregation approach based on a constant threshold fails to distinguish intermittent malicious/faulty activity, we propose a new detection algorithm utilizing time-varying thresholds and the random trust values available to legitimate agents. Under these conditions, legitimate agents almost surely determine their trusted neighborhood correctly with geometrically decaying misclassification probabilities. We further prove that the consensus process converges almost surely even in the presence of malicious agents. We also derive the probabilistic bounds on the deviation from the nominal consensus value that would have been achieved with no malicious agents in the system. Numerical results verify the convergence among agents and exemplify the deviation under different scenarios.","sentences":["In this work, we consider the consensus problem in which legitimate agents share their values over an undirected communication network in the presence of malicious or faulty agents.","Different from the previous works, we characterize the conditions that generalize to several scenarios such as intermittent faulty or malicious transmissions, based on trust observations.","As the standard trust aggregation approach based on a constant threshold fails to distinguish intermittent malicious/faulty activity, we propose a new detection algorithm utilizing time-varying thresholds and the random trust values available to legitimate agents.","Under these conditions, legitimate agents almost surely determine their trusted neighborhood correctly with geometrically decaying misclassification probabilities.","We further prove that the consensus process converges almost surely even in the presence of malicious agents.","We also derive the probabilistic bounds on the deviation from the nominal consensus value that would have been achieved with no malicious agents in the system.","Numerical results verify the convergence among agents and exemplify the deviation under different scenarios."],"url":"http://arxiv.org/abs/2403.17907v1","category":"eess.SY"}
{"created":"2024-03-26 17:45:30","title":"WKB asymptotics of Stokes matrices, spectral curves and rhombus inequalities","abstract":"We consider an $n\\times n$ system of ODEs on $\\mathbb{P}^1$ with a simple pole $A$ at $z=0$ and a double pole $u={\\rm diag}(u_1, \\dots, u_n)$ at $z=\\infty$. This is the simplest situation in which the monodromy data of the system are described by upper and lower triangular Stokes matrices $S_\\pm$, and we impose reality conditions which imply $S_-=S_+^\\dagger$. We study leading WKB exponents of Stokes matrices in parametrizations given by generalized minors and by spectral coordinates, and we show that for $u$ on the caterpillar line (which corresponds to the limit $(u_{j+1}-u_j)/(u_j- u_{j-1}) \\to \\infty$ for $j=2, \\cdots, n-1$), the real parts of these exponents are given by periods of certain cycles on the degenerate spectral curve $\\Gamma(u_{\\rm cat}(t), A)$.   These cycles admit unique deformations for $u$ near the caterpillar line. Using the spectral network theory, we give for $n=2$, and $n=3$ exact WKB predictions for asymptotics of generalized minors in terms of periods of these cycles. Boalch's theorem from Poisson geometry implies that real parts of leading WKB exponents satisfy the rhombus (or interlacing) inequalities. We show that these inequalities are in correspondence with finite webs of the canonical foliation on the root curve $\\Gamma^r(u, A)$, and that they follow from the positivity of the corresponding periods. We conjecture that a similar mechanism applies for $n>3$.   We also outline the relation of the spectral coordinates with the cluster structures considered by Goncharov-Shen, and with ${\\mathcal N}=2$ supersymmetric quantum field theories in dimension four associated with some simple quivers.","sentences":["We consider an $n\\times n$ system of ODEs on $\\mathbb{P}^1$ with a simple pole $A$ at $z=0$ and a double pole $u={\\rm diag}(u_1, \\dots, u_n)$ at $z=\\infty$. This is the simplest situation in which the monodromy data of the system are described by upper and lower triangular Stokes matrices $S_\\pm$, and we impose reality conditions which imply $S_-=S_+^\\dagger$. We study leading WKB exponents of Stokes matrices in parametrizations given by generalized minors and by spectral coordinates, and we show that for $u$ on the caterpillar line (which corresponds to the limit $(u_{j+1}-u_j)/(u_j- u_{j-1}) \\to \\infty$ for $j=2, \\cdots, n-1$), the real parts of these exponents are given by periods of certain cycles on the degenerate spectral curve $\\Gamma(u_{\\rm cat}(t), A)$.   These cycles admit unique deformations for $u$ near the caterpillar line.","Using the spectral network theory, we give for $n=2$, and $n=3$ exact WKB predictions for asymptotics of generalized minors in terms of periods of these cycles.","Boalch's theorem from Poisson geometry implies that real parts of leading WKB exponents satisfy the rhombus (or interlacing) inequalities.","We show that these inequalities are in correspondence with finite webs of the canonical foliation on the root curve $\\Gamma^r(u, A)$, and that they follow from the positivity of the corresponding periods.","We conjecture that a similar mechanism applies for $n>3$.   We also outline the relation of the spectral coordinates with the cluster structures considered by Goncharov-Shen, and with ${\\mathcal N}=2$ supersymmetric quantum field theories in dimension four associated with some simple quivers."],"url":"http://arxiv.org/abs/2403.17906v1","category":"math-ph"}
{"created":"2024-03-26 17:43:08","title":"Search and Society: Reimagining Information Access for Radical Futures","abstract":"Information retrieval (IR) technologies and research are undergoing transformative changes. It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research. Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries. The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives. Our guiding future imaginaries must be informed by other academic fields, such as democratic theory and critical theory, and should be co-developed with social science scholars, legal scholars, civil rights and social justice activists, and artists, among others. In this perspective paper, we motivate why the community must consider this radical shift in how we do research and what we work on, and sketch a path forward towards this transformation.","sentences":["Information retrieval (IR) technologies and research are undergoing transformative changes.","It is our perspective that the community should accept this opportunity to re-center our research agendas on societal needs while dismantling the artificial separation between the work on fairness, accountability, transparency, and ethics in IR and the rest of IR research.","Instead of adopting a reactionary strategy of trying to mitigate potential social harms from emerging technologies, the community should aim to proactively set the research agenda for the kinds of systems we should build inspired by diverse explicitly stated sociotechnical imaginaries.","The sociotechnical imaginaries that underpin the design and development of information access technologies needs to be explicitly articulated, and we need to develop theories of change in context of these diverse perspectives.","Our guiding future imaginaries must be informed by other academic fields, such as democratic theory and critical theory, and should be co-developed with social science scholars, legal scholars, civil rights and social justice activists, and artists, among others.","In this perspective paper, we motivate why the community must consider this radical shift in how we do research and what we work on, and sketch a path forward towards this transformation."],"url":"http://arxiv.org/abs/2403.17901v1","category":"cs.IR"}
{"created":"2024-03-26 17:43:04","title":"On the Dynamics of Point-Vortices with Positive Intensities collapsing with the boundary","abstract":"In this paper we study the point-vortex dynamics with positive intensities. We show that in the half-plane and in a disk, collapses of point-vortices with the boundary in finite time are impossible, hence the solution of the dynamics is global in time. We also give some necessary conditions for the existence of collapses with the boundary in general smooth bounded domains, in particular that the trajectory of at least one point-vortex has no limit. Some minor results are obtained with signed intensities.","sentences":["In this paper we study the point-vortex dynamics with positive intensities.","We show that in the half-plane and in a disk, collapses of point-vortices with the boundary in finite time are impossible, hence the solution of the dynamics is global in time.","We also give some necessary conditions for the existence of collapses with the boundary in general smooth bounded domains, in particular that the trajectory of at least one point-vortex has no limit.","Some minor results are obtained with signed intensities."],"url":"http://arxiv.org/abs/2403.17900v1","category":"math.AP"}
{"created":"2024-03-26 17:41:48","title":"A severe local flood and social events show a similar impact on human mobility","abstract":"While a social event, such as a concert or a food festival, is a common experience to people, a natural disaster is experienced by a fewer individuals. The ordinary and common ground experience of social events could be therefore used to better understand the complex impacts of uncommon, but devastating natural events on society, such as floods. Based on this idea, we present a comparison - in terms of human mobility -, between an extreme local flood that occurred in 2017 in Switzerland, and social events which took place in the same region, in the weeks before and after the inundation. Using mobile phone location data, we show that the severe local flood and social events have a similar impact on human mobility, both at the national scale and at a local scale. At the national level, we found a small difference between the distributions of visitors and their travelled distances among the several weeks in which the events took place. At the local level, instead, we detected the anomalies (in time series) in the number of people travelling each road and railway, and we found that the distributions of anomalies, and of their clusters, are comparable between the flood and the social events. Hence, our findings suggest that the knowledge on ubiquitous social events can be employed to characterise the impacts of rare natural disasters on human mobility. The proposed methods at the local level can thus be used to analyse the disturbances in complex spatial networks and, in general, as complementary approaches for the analyses of complex systems.","sentences":["While a social event, such as a concert or a food festival, is a common experience to people, a natural disaster is experienced by a fewer individuals.","The ordinary and common ground experience of social events could be therefore used to better understand the complex impacts of uncommon, but devastating natural events on society, such as floods.","Based on this idea, we present a comparison - in terms of human mobility -, between an extreme local flood that occurred in 2017 in Switzerland, and social events which took place in the same region, in the weeks before and after the inundation.","Using mobile phone location data, we show that the severe local flood and social events have a similar impact on human mobility, both at the national scale and at a local scale.","At the national level, we found a small difference between the distributions of visitors and their travelled distances among the several weeks in which the events took place.","At the local level, instead, we detected the anomalies (in time series) in the number of people travelling each road and railway, and we found that the distributions of anomalies, and of their clusters, are comparable between the flood and the social events.","Hence, our findings suggest that the knowledge on ubiquitous social events can be employed to characterise the impacts of rare natural disasters on human mobility.","The proposed methods at the local level can thus be used to analyse the disturbances in complex spatial networks and, in general, as complementary approaches for the analyses of complex systems."],"url":"http://arxiv.org/abs/2403.17899v1","category":"physics.soc-ph"}
{"created":"2024-03-26 17:22:29","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels","abstract":"One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.","sentences":["One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types.","This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers.","Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance.","To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection.","Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods."],"url":"http://arxiv.org/abs/2403.17891v1","category":"cs.LG"}
{"created":"2024-03-26 17:22:27","title":"Searching for large dark matter clumps using the Galileo Satnav clock variations","abstract":"This study presents bounds on transient variations of fundamental constants, with typical timescales ranging from minutes to months, using clocks in space. The underlying phenomenology describing such transient variations relies on models for Dark Matter (DM) which suggest possible encounters of macroscopic compact objects with the Earth, due to the motion of the solar system in the galactic halo. If such compact objects possess an effective feeble interaction with the ordinary matter beyond the gravitational one, it may result in effective transient variations of fundamental constants. Such variations leave signatures on clocks onboard GNSS satellites. In this paper, we introduce a phenomenological study dedicated to the search for such DM transient objects using the network of passive hydrogen masers (H-Masers) onboard Galileo satellites. We first model the signature of transient variations of fundamental constants as a frequency modulation in the difference between two satellite clocks, considering the satellite trajectories relative to the transient event. Then, we present first results based on a fast analysis method, the maximum reach analysis. The main result is a significant extension of the discovery range for DM transients, with a sensitivity never achieved before. We investigate indeed the range of transient sizes from $10^5$ to $10^9$ kilometres, which, apart from indirect and model-dependent non-transient effects, has never been explored previously.","sentences":["This study presents bounds on transient variations of fundamental constants, with typical timescales ranging from minutes to months, using clocks in space.","The underlying phenomenology describing such transient variations relies on models for Dark Matter (DM) which suggest possible encounters of macroscopic compact objects with the Earth, due to the motion of the solar system in the galactic halo.","If such compact objects possess an effective feeble interaction with the ordinary matter beyond the gravitational one, it may result in effective transient variations of fundamental constants.","Such variations leave signatures on clocks onboard GNSS satellites.","In this paper, we introduce a phenomenological study dedicated to the search for such DM transient objects using the network of passive hydrogen masers (H-Masers) onboard Galileo satellites.","We first model the signature of transient variations of fundamental constants as a frequency modulation in the difference between two satellite clocks, considering the satellite trajectories relative to the transient event.","Then, we present first results based on a fast analysis method, the maximum reach analysis.","The main result is a significant extension of the discovery range for DM transients, with a sensitivity never achieved before.","We investigate indeed the range of transient sizes from $10^5$ to $10^9$ kilometres, which, apart from indirect and model-dependent non-transient effects, has never been explored previously."],"url":"http://arxiv.org/abs/2403.17890v1","category":"astro-ph.CO"}
{"created":"2024-03-26 17:21:54","title":"Large scale paired antibody language models","abstract":"Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics. With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data. To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input. These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset. We show that our models outperform existing antibody and protein language models on a diverse range of design and regression tasks relevant to antibody engineering. This advancement marks a significant leap forward in leveraging machine learning, large scale data sets and high-performance computing for enhancing antibody design for therapeutic development.","sentences":["Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics.","With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data.","To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input.","These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset.","We show that our models outperform existing antibody and protein language models on a diverse range of design and regression tasks relevant to antibody engineering.","This advancement marks a significant leap forward in leveraging machine learning, large scale data sets and high-performance computing for enhancing antibody design for therapeutic development."],"url":"http://arxiv.org/abs/2403.17889v1","category":"q-bio.BM"}
{"created":"2024-03-26 17:19:23","title":"Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation","abstract":"As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling.","sentences":["As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources.","We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data.","We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility.","We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training).","We evaluate NEC on two EO tasks: scene classification and semantic segmentation.","Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data.","Even at 99.7% compression, performance drops by only 5% on the scene classification task.","Overall, NEC is a data-efficient yet performant approach for multi-task EO modelling."],"url":"http://arxiv.org/abs/2403.17886v1","category":"cs.LG"}
{"created":"2024-03-26 17:13:17","title":"Superior and Pragmatic Talking Face Generation with Teacher-Student Framework","abstract":"Talking face generation technology creates talking videos from arbitrary appearance and motion signal, with the \"arbitrary\" offering ease of use but also introducing challenges in practical applications. Existing methods work well with standard inputs but suffer serious performance degradation with intricate real-world ones. Moreover, efficiency is also an important concern in deployment. To comprehensively address these issues, we introduce SuperFace, a teacher-student framework that balances quality, robustness, cost and editability. We first propose a simple but effective teacher model capable of handling inputs of varying qualities to generate high-quality results. Building on this, we devise an efficient distillation strategy to acquire an identity-specific student model that maintains quality with significantly reduced computational load. Our experiments validate that SuperFace offers a more comprehensive solution than existing methods for the four mentioned objectives, especially in reducing FLOPs by 99\\% with the student model. SuperFace can be driven by both video and audio and allows for localized facial attributes editing.","sentences":["Talking face generation technology creates talking videos from arbitrary appearance and motion signal, with the \"arbitrary\" offering ease of use but also introducing challenges in practical applications.","Existing methods work well with standard inputs but suffer serious performance degradation with intricate real-world ones.","Moreover, efficiency is also an important concern in deployment.","To comprehensively address these issues, we introduce SuperFace, a teacher-student framework that balances quality, robustness, cost and editability.","We first propose a simple but effective teacher model capable of handling inputs of varying qualities to generate high-quality results.","Building on this, we devise an efficient distillation strategy to acquire an identity-specific student model that maintains quality with significantly reduced computational load.","Our experiments validate that SuperFace offers a more comprehensive solution than existing methods for the four mentioned objectives, especially in reducing FLOPs by 99\\% with the student model.","SuperFace can be driven by both video and audio and allows for localized facial attributes editing."],"url":"http://arxiv.org/abs/2403.17883v1","category":"cs.CV"}
{"created":"2024-03-26 17:12:34","title":"Double polytropic cosmic acceleration from the Murnaghan equation of state","abstract":"We consider a double polytropic cosmological fluid and demonstrate that, when one constituent resembles a bare cosmological constant while the other emulates a generalized Chaplygin gas, a good description of the Universe's large-scale dynamics is obtained. In particular, our double polytropic reduces to the Murnaghan equation of state, whose applications are already well established in solid state physics and classical thermodynamics. Intriguingly, our model approximates the conventional $\\Lambda$CDM paradigm while reproducing the collective effects of logotropic and generalized Chaplygin fluids across different regimes. To check the goodness of our fluid description, we analyze first order density perturbations, refining our model through various orders of approximation, utilizing $\\sigma_8$ data alongside other cosmological data sets. Encouraging results suggest that our model, based on the Murnaghan equation of state, outperforms the standard cosmological background within specific approximate regimes and, on the whole, surpasses the standard phenomenological reconstruction of dark energy.","sentences":["We consider a double polytropic cosmological fluid and demonstrate that, when one constituent resembles a bare cosmological constant while the other emulates a generalized Chaplygin gas, a good description of the Universe's large-scale dynamics is obtained.","In particular, our double polytropic reduces to the Murnaghan equation of state, whose applications are already well established in solid state physics and classical thermodynamics.","Intriguingly, our model approximates the conventional $\\Lambda$CDM paradigm while reproducing the collective effects of logotropic and generalized Chaplygin fluids across different regimes.","To check the goodness of our fluid description, we analyze first order density perturbations, refining our model through various orders of approximation, utilizing $\\sigma_8$ data alongside other cosmological data sets.","Encouraging results suggest that our model, based on the Murnaghan equation of state, outperforms the standard cosmological background within specific approximate regimes and, on the whole, surpasses the standard phenomenological reconstruction of dark energy."],"url":"http://arxiv.org/abs/2403.17880v1","category":"gr-qc"}
{"created":"2024-03-26 17:12:34","title":"Deepfake Generation and Detection: A Benchmark and Survey","abstract":"In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.","sentences":["In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks.","This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field.","We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks.","Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection.","Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals.","Finally, we analyze the challenges and future research directions of the discussed fields.","We closely follow the latest developments in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection."],"url":"http://arxiv.org/abs/2403.17881v1","category":"cs.CV"}
{"created":"2024-03-26 17:10:15","title":"Empowering Data Mesh with Federated Learning","abstract":"The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.","sentences":["The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making.","However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing.","A new data paradigm, Data Mesh, is proposed to overcome these challenges.","Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products.","Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture.","In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations.","To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh.","To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture."],"url":"http://arxiv.org/abs/2403.17878v1","category":"cs.LG"}
{"created":"2024-03-26 17:07:02","title":"Identifying codes in graphs of given maximum degree. II. Triangle-free graphs","abstract":"An $\\textit{identifying code}$ of a closed-twin-free graph $G$ is a set $S$ of vertices of $G$ such that any two vertices in $G$ have a distinct intersection between their closed neighborhood and $S$. It was conjectured that there exists a constant $c$ such that for every connected closed-twin-free graph $G$ of order $n$ and maximum degree $\\Delta$, the graph $G$ admits an identifying code of size at most $\\left( \\frac{\\Delta-1}{\\Delta} \\right) n+c$. In a companion paper, we proved the conjecture for all trees. We show that the conjecture holds for all triangle-free graphs, with the same lists of exceptional graphs needing $c>0$ as for trees: for $\\Delta\\ge 3$, $c=1/3$ suffices and there is only a set of 12 trees requiring $c>0$ for $\\Delta=3$, and when $\\Delta\\ge 4$ this set is reduced to the $\\Delta$-star only. Our proof is by induction, whose starting point is the above result for trees. Along the way, we prove a generalized version of Bondy's theorem on induced subsets [J. A. Bondy. Induced subsets. Journal of Combinatorial Theory, Series B, 1972] that we use as a tool in our proofs. We also use our main result for triangle-free graphs, to prove the upper bound $\\left( \\frac{\\Delta-1}{\\Delta} \\right) n+1/\\Delta+4t$ for graphs that can be made triangle-free by the removal of $t$ edges.","sentences":["An $\\textit{identifying code}$ of a closed-twin-free graph $G$ is a set $S$ of vertices of $G$ such that any two vertices in $G$ have a distinct intersection between their closed neighborhood and $S$. It was conjectured that there exists a constant $c$ such that for every connected closed-twin-free graph $G$ of order $n$ and maximum degree $\\Delta$, the graph $G$ admits an identifying code of size at most $\\left( \\frac{\\Delta-1}{\\Delta} \\right) n+c$. In a companion paper, we proved the conjecture for all trees.","We show that the conjecture holds for all triangle-free graphs, with the same lists of exceptional graphs needing $c>0$ as for trees: for $\\Delta\\ge 3$, $c=1/3$ suffices and there is only a set of 12 trees requiring $c>0$ for $\\Delta=3$, and when $\\Delta\\ge 4$ this set is reduced to the $\\Delta$-star only.","Our proof is by induction, whose starting point is the above result for trees.","Along the way, we prove a generalized version of Bondy's theorem on induced subsets [J. A. Bondy.","Induced subsets.","Journal of Combinatorial Theory, Series B, 1972] that we use as a tool in our proofs.","We also use our main result for triangle-free graphs, to prove the upper bound $\\left( \\frac{\\Delta-1}{\\Delta} \\right) n+1/\\Delta+4t$ for graphs that can be made triangle-free by the removal of $t$ edges."],"url":"http://arxiv.org/abs/2403.17877v1","category":"math.CO"}
{"created":"2024-03-26 17:03:55","title":"The Solution to an Impulse Control Problem Motivated by Optimal Harvesting","abstract":"We consider a stochastic impulse control problem that is motivated by applications such as the optimal exploitation of a natural resource. In particular, we consider a stochastic system whose uncontrolled state dynamics are modelled by a non-explosive positive linear diffusion. The control that can be applied to this system takes the form of one-sided impulsive action. The objective of the control problem is to maximise a discounted performance criterion that rewards the effect of control action but involves a fixed cost at each time of a control intervention. We derive the complete solution to this problem under general assumptions. It turns out that the solution can take four qualitatively different forms, several of which have not been observed in the literature. In two of the four cases, there exist only $\\varepsilon$-optimal control strategies. We also show that the boundary classification of 0 may play a critical role in the solution of the problem. Furthermore, we develop a way for establishing the strong solution to a stochastic impulse control problem's optimally controlled SDE.","sentences":["We consider a stochastic impulse control problem that is motivated by applications such as the optimal exploitation of a natural resource.","In particular, we consider a stochastic system whose uncontrolled state dynamics are modelled by a non-explosive positive linear diffusion.","The control that can be applied to this system takes the form of one-sided impulsive action.","The objective of the control problem is to maximise a discounted performance criterion that rewards the effect of control action but involves a fixed cost at each time of a control intervention.","We derive the complete solution to this problem under general assumptions.","It turns out that the solution can take four qualitatively different forms, several of which have not been observed in the literature.","In two of the four cases, there exist only $\\varepsilon$-optimal control strategies.","We also show that the boundary classification of 0 may play a critical role in the solution of the problem.","Furthermore, we develop a way for establishing the strong solution to a stochastic impulse control problem's optimally controlled SDE."],"url":"http://arxiv.org/abs/2403.17875v1","category":"math.OC"}
{"created":"2024-03-26 17:02:42","title":"Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach","abstract":"Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims to bridge the gap between LLM capabilities and user perceptions, promoting the ethically responsible development and use of LLM-based technology.","sentences":["Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations.","Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users.","In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health.","In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust.","To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users.","This addition aims to bridge the gap between LLM capabilities and user perceptions, promoting the ethically responsible development and use of LLM-based technology."],"url":"http://arxiv.org/abs/2403.17873v1","category":"cs.AI"}
{"created":"2024-03-26 16:59:09","title":"On a combinatorial classification of fine compactified universal Jacobians","abstract":"Extending the definition of $V$-stability conditions, given by Viviani in a recent preprint, we introduce the notion of \\emph{universal stability conditions}. Building on results by Pagani and Tommasi, we show that fine compactified universal Jacobians, that is, fine compactified Jacobians over the moduli spaces of stable pointed curves $\\overline{\\mathcal{M}}_{g,n}$, are combinatorially classified by universal stability conditions.   We use these stability conditions to show the following. The inclusion of fine compactified universal Jacobians of type $(g,n)$ whose fibres over geometric points are classical, that is, they are constructed by some numerical polarisation, into the class of all fine compactified universal Jacobians, is strict, in general, for any $g\\geq 2$. This answers a question of Pagani and Tommasi.","sentences":["Extending the definition of $V$-stability conditions, given by Viviani in a recent preprint, we introduce the notion of \\emph{universal stability conditions}.","Building on results by Pagani and Tommasi, we show that fine compactified universal Jacobians, that is, fine compactified Jacobians over the moduli spaces of stable pointed curves $\\overline{\\mathcal{M}}_{g,n}$, are combinatorially classified by universal stability conditions.   ","We use these stability conditions to show the following.","The inclusion of fine compactified universal Jacobians of type $(g,n)$ whose fibres over geometric points are classical, that is, they are constructed by some numerical polarisation, into the class of all fine compactified universal Jacobians, is strict, in general, for any $g\\geq 2$.","This answers a question of Pagani and Tommasi."],"url":"http://arxiv.org/abs/2403.17871v1","category":"math.AG"}
{"created":"2024-03-26 16:57:55","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency Domain","abstract":"Diffusion models have recently brought a powerful revolution in image generation. Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability. In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples. Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps. In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component. We name the complete approach \"Moving Average Sampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules. Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost.","sentences":["Diffusion models have recently brought a powerful revolution in image generation.","Despite showing impressive generative capabilities, most of these models rely on the current sample to denoise the next one, possibly resulting in denoising instability.","In this paper, we reinterpret the iterative denoising process as model optimization and leverage a moving average mechanism to ensemble all the prior samples.","Instead of simply applying moving average to the denoised samples at different timesteps, we first map the denoised samples to data space and then perform moving average to avoid distribution shift across timesteps.","In view that diffusion models evolve the recovery from low-frequency components to high-frequency details, we further decompose the samples into different frequency components and execute moving average separately on each component.","We name the complete approach \"Moving Average Sampling in Frequency domain (MASF)\".","MASF could be seamlessly integrated into mainstream pre-trained diffusion models and sampling schedules.","Extensive experiments on both unconditional and conditional diffusion models demonstrate that our MASF leads to superior performances compared to the baselines, with almost negligible additional complexity cost."],"url":"http://arxiv.org/abs/2403.17870v1","category":"cs.CV"}
{"created":"2024-03-26 16:53:32","title":"A Floquet analysis perspective of driven light-matter interaction models","abstract":"In this paper, we analyze the harmonically driven Jaynes-Cummings and Lipkin-Meshkov-Glick models using both numerical integration of time-dependent Hamiltonians and Floquet theory. For a separation of time-scales between the drive and intrinsic Rabi oscillations in the former model, the driving results in an effective periodic reversal of time. The corresponding Floquet Hamilto- nian is a Wannier-Stark model, which can be analytically solved. Despite the chaotic nature of the driven Lipkin-Meshkov-Glick model, moderate system sizes can display qualitatively different behaviors under varying system parameters. Ergodicity arises in systems that are neither adiabatic nor diabatic, owing to repeated multi-level Landau-Zener transitions. Chaotic behavior, observed in slow driving, manifests as random jumps in the magnetization, suggesting potential utility as a random number generator. Furthermore, we discuss both models in terms of what we call Floquet Fock state lattices.","sentences":["In this paper, we analyze the harmonically driven Jaynes-Cummings and Lipkin-Meshkov-Glick models using both numerical integration of time-dependent Hamiltonians and Floquet theory.","For a separation of time-scales between the drive and intrinsic Rabi oscillations in the former model, the driving results in an effective periodic reversal of time.","The corresponding Floquet Hamilto- nian is a Wannier-Stark model, which can be analytically solved.","Despite the chaotic nature of the driven Lipkin-Meshkov-Glick model, moderate system sizes can display qualitatively different behaviors under varying system parameters.","Ergodicity arises in systems that are neither adiabatic nor diabatic, owing to repeated multi-level Landau-Zener transitions.","Chaotic behavior, observed in slow driving, manifests as random jumps in the magnetization, suggesting potential utility as a random number generator.","Furthermore, we discuss both models in terms of what we call Floquet Fock state lattices."],"url":"http://arxiv.org/abs/2403.17866v1","category":"quant-ph"}
{"created":"2024-03-26 16:51:21","title":"Synthesizing Soundscapes: Leveraging Text-to-Audio Models for Environmental Sound Classification","abstract":"In the past few years, text-to-audio models have emerged as a significant advancement in automatic audio gener- ation. Although they represent impressive technological progress, the effectiveness of their use in the development of audio applications remains uncertain. This paper aims to investigate these aspects, specifically focusing on the task of classification of environmental sounds. This study analyzes the performance of two different environmental classification systems when data generated from text-to-audio models is used for training. Two cases are considered: a) when the training dataset is augmented by data coming from two different text-to-audio models; and b) when the training dataset consists solely of synthetic audio generated. In both cases, the performance of the classification task is tested on real data. Results indicate that text-to-audio models are effective for dataset augmentation, whereas the performance of the models drops when relying on only generated audio.","sentences":["In the past few years, text-to-audio models have emerged as a significant advancement in automatic audio gener- ation.","Although they represent impressive technological progress, the effectiveness of their use in the development of audio applications remains uncertain.","This paper aims to investigate these aspects, specifically focusing on the task of classification of environmental sounds.","This study analyzes the performance of two different environmental classification systems when data generated from text-to-audio models is used for training.","Two cases are considered: a) when the training dataset is augmented by data coming from two different text-to-audio models; and b) when the training dataset consists solely of synthetic audio generated.","In both cases, the performance of the classification task is tested on real data.","Results indicate that text-to-audio models are effective for dataset augmentation, whereas the performance of the models drops when relying on only generated audio."],"url":"http://arxiv.org/abs/2403.17864v1","category":"eess.AS"}
{"created":"2024-03-26 16:50:44","title":"An AI-Native Runtime for Multi-Wearable Environments","abstract":"The miniaturization of AI accelerators is paving the way for next-generation wearable applications within wearable technologies. We introduce Mojito, an AI-native runtime with advanced MLOps designed to facilitate the development and deployment of these applications on wearable devices. It emphasizes the necessity of dynamic orchestration of distributed resources equipped with ultra-low-power AI accelerators to overcome challenges associated with unpredictable runtime environments. Through its innovative approaches, Mojito demonstrates how future wearable technologies can evolve to be more autonomous.","sentences":["The miniaturization of AI accelerators is paving the way for next-generation wearable applications within wearable technologies.","We introduce Mojito, an AI-native runtime with advanced MLOps designed to facilitate the development and deployment of these applications on wearable devices.","It emphasizes the necessity of dynamic orchestration of distributed resources equipped with ultra-low-power AI accelerators to overcome challenges associated with unpredictable runtime environments.","Through its innovative approaches, Mojito demonstrates how future wearable technologies can evolve to be more autonomous."],"url":"http://arxiv.org/abs/2403.17863v1","category":"cs.DC"}
{"created":"2024-03-26 16:49:25","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications","abstract":"Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.","sentences":["Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data.","Existing work has mainly focused on mitigation of such errors using either humans or an automated approach.","In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks.","We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure.","For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set.","We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy.","Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable."],"url":"http://arxiv.org/abs/2403.17860v1","category":"cs.CL"}
{"created":"2024-03-26 16:45:58","title":"Ill-posedness of the hydrostatic Euler-Boussinesq equations and failure of hydrostatic limit","abstract":"We investigate the hydrostatic approximation for inviscid stratified fluids, described by the two-dimensional Euler-Boussinesq equations in a periodic channel. Through a perturbative analysis of the hydrostatic homogeneous setting, we exhibit a stratified steady state violating the Miles-Howard criterion and generating a growing mode, both for the linearized hydrostatic and non-hydrostatic equations. By leveraging long-wave nonlinear instability for the original Euler-Boussinesq system, we demonstrate the breakdown of the hydrostatic limit around such unstable profiles. Finally, we establish the generic nonlinear ill-posedness of the limiting hydrostatic system in Sobolev spaces.","sentences":["We investigate the hydrostatic approximation for inviscid stratified fluids, described by the two-dimensional Euler-Boussinesq equations in a periodic channel.","Through a perturbative analysis of the hydrostatic homogeneous setting, we exhibit a stratified steady state violating the Miles-Howard criterion and generating a growing mode, both for the linearized hydrostatic and non-hydrostatic equations.","By leveraging long-wave nonlinear instability for the original Euler-Boussinesq system, we demonstrate the breakdown of the hydrostatic limit around such unstable profiles.","Finally, we establish the generic nonlinear ill-posedness of the limiting hydrostatic system in Sobolev spaces."],"url":"http://arxiv.org/abs/2403.17857v1","category":"math.AP"}
{"created":"2024-03-26 16:45:27","title":"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs","abstract":"Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4.","sentences":["Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology.","In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech.","However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization.","This paper reports the first study on the behavior of large language models with reference to conversion.","We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech.","This task is situated within a natural language inference paradigm.","We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and Llama 2 70B).","We find that GPT-4 performs best on the task, followed by GPT-3.5, but that the open source language models are also able to perform it and that the 7B parameter Mistral displays as little difference between its baseline performance on the natural language inference task and the non-prototypical syntactic category task, as the massive GPT-4."],"url":"http://arxiv.org/abs/2403.17856v1","category":"cs.CL"}
{"created":"2024-03-26 16:42:30","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic","abstract":"Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.","sentences":["Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog.","It is a critical component for modern dialog system design and discourse analysis.","Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain.","This work explores a neural-symbolic approach as a potential solution to these problems.","We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model.","We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance.","Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines."],"url":"http://arxiv.org/abs/2403.17853v1","category":"cs.CL"}
{"created":"2024-03-26 16:36:50","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections","abstract":"Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to drastically geographical elevation changes and uneven distribution through the year. Thus, it is crucial to track and predict the rainfall to make the most use of it and to prevent the floods. However, climate models have limited resolution and require intensive computational power for local-scale use. Therefore, we proposed a deep convolutional neural network with skip connections, attention blocks, and auxiliary data concatenation, in order to downscale the low-resolution precipitation data into high-resolution one. Eventually, we compare with other climate downscaling methods and show better performance in metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation, structural similarity index (SSIM), and forecast indicators.","sentences":["Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change.","These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties.","To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes.","Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness.","One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands.","Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics.","In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to drastically geographical elevation changes and uneven distribution through the year.","Thus, it is crucial to track and predict the rainfall to make the most use of it and to prevent the floods.","However, climate models have limited resolution and require intensive computational power for local-scale use.","Therefore, we proposed a deep convolutional neural network with skip connections, attention blocks, and auxiliary data concatenation, in order to downscale the low-resolution precipitation data into high-resolution one.","Eventually, we compare with other climate downscaling methods and show better performance in metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation, structural similarity index (SSIM), and forecast indicators."],"url":"http://arxiv.org/abs/2403.17847v1","category":"cs.LG"}
{"created":"2024-03-26 16:36:43","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation","abstract":"Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps. In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments. We provide code and trial video data at http://hovsg.github.io/.","sentences":["Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features.","While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation.","In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation.","Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features.","Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph.","HOV-SG is evaluated on three distinct datasets and surpasses previous baselines in open-vocabulary semantic accuracy on the object, room, and floor level while producing a 75% reduction in representation size compared to dense open-vocabulary maps.","In order to prove the efficacy and generalization capabilities of HOV-SG, we showcase successful long-horizon language-conditioned robot navigation within real-world multi-storage environments.","We provide code and trial video data at http://hovsg.github.io/."],"url":"http://arxiv.org/abs/2403.17846v1","category":"cs.RO"}
{"created":"2024-03-26 16:34:05","title":"TractOracle: towards an anatomically-informed reward function for RL-based tractography","abstract":"Reinforcement learning (RL)-based tractography is a competitive alternative to machine learning and classical tractography algorithms due to its high anatomical accuracy obtained without the need for any annotated data. However, the reward functions so far used to train RL agents do not encapsulate anatomical knowledge which causes agents to generate spurious false positives tracts. In this paper, we propose a new RL tractography system, TractOracle, which relies on a reward network trained for streamline classification. This network is used both as a reward function during training as well as a mean for stopping the tracking process early and thus reduce the number of false positive streamlines. This makes our system a unique method that evaluates and reconstructs WM streamlines at the same time. We report an improvement of true positive ratios by almost 20\\% and a reduction of 3x of false positive ratios on one dataset and an increase between 2x and 7x in the number true positive streamlines on another dataset.","sentences":["Reinforcement learning (RL)-based tractography is a competitive alternative to machine learning and classical tractography algorithms due to its high anatomical accuracy obtained without the need for any annotated data.","However, the reward functions so far used to train RL agents do not encapsulate anatomical knowledge which causes agents to generate spurious false positives tracts.","In this paper, we propose a new RL tractography system, TractOracle, which relies on a reward network trained for streamline classification.","This network is used both as a reward function during training as well as a mean for stopping the tracking process early and thus reduce the number of false positive streamlines.","This makes our system a unique method that evaluates and reconstructs WM streamlines at the same time.","We report an improvement of true positive ratios by almost 20\\% and a reduction of 3x of false positive ratios on one dataset and an increase between 2x and 7x in the number true positive streamlines on another dataset."],"url":"http://arxiv.org/abs/2403.17845v1","category":"cs.LG"}
{"created":"2024-03-26 16:27:37","title":"ReMamber: Referring Image Segmentation with Mamba Twister","abstract":"Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba. These provide valuable perspectives for future research.","sentences":["Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks.","However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies.","Fortunately, Mamba addresses this with efficient linear complexity in processing.","However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data.","In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block.","The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism.","We achieve the state-of-the-art on three challenging benchmarks.","Moreover, we conduct thorough analyses of ReMamber and discuss other fusion designs using Mamba.","These provide valuable perspectives for future research."],"url":"http://arxiv.org/abs/2403.17839v1","category":"cs.CV"}
{"created":"2024-03-26 16:24:42","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","abstract":"High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications. However, capturing HDR content from real-world scenes is expensive and time- consuming. Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community. A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance). To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods. Furthermore, we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation, human body part segmentation, and holistic scene segmentation. The dataset, data collection pipeline, and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR.","sentences":["High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications.","However, capturing HDR content from real-world scenes is expensive and time- consuming.","Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community.","A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance).","To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game.","We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the state-of-the-art HDR image reconstruction methods.","Furthermore, we demonstrate the effectiveness of the proposed dataset and its impact on additional computer vision tasks including 3D human pose estimation, human body part segmentation, and holistic scene segmentation.","The dataset, data collection pipeline, and evaluation code are available at: https://github.com/HrishavBakulBarua/GTA-HDR."],"url":"http://arxiv.org/abs/2403.17837v1","category":"cs.CV"}
{"created":"2024-03-26 16:14:15","title":"The memory of Rayleigh-Taylor turbulence","abstract":"In this work, we consider the problem of inferring the initial conditions of a Rayleigh-Taylor mixing zone by measuring the 0D turbulent quantities at an unspecified time. To this aim, we have generated a comprehensive dataset through direct numerical simulations (DNS), focusing on miscible fluids with slight density contrasts. The initial interface deformations in these simulations are characterized by an annular spectrum which is parametrized by four non dimensional numbers. %In order to study the sensitivity of 0D turbulent quantities to the initial interface perturbation distributions, we build a surrogate model for the simulations using a physics-informed neural network (PINN). This allows us to compute the Sobol indices for the turbulent quantities, disentangling the effects of the initial parameters on the growth of the mixing layer. Within a Bayesian framework, we use a Markov chain Monte-Carlo method to determine the posterior distributions of initial conditions given various state variables. %This sheds light on the inertial or diffusive trajectories along with how the initial conditions are progressively forgotten during transition to turbulence. Moreover, it identifies which turbulent quantities are better predictors for the dynamics of Rayleigh-Taylor mixing zones by more effectively retaining the memory of the flow. By inferring the initial conditions and forward propagating its maximum a posteriori (MAP) estimate, we propose a strategy to model the Rayleigh-Taylor transition to turbulence.","sentences":["In this work, we consider the problem of inferring the initial conditions of a Rayleigh-Taylor mixing zone by measuring the 0D turbulent quantities at an unspecified time.","To this aim, we have generated a comprehensive dataset through direct numerical simulations (DNS), focusing on miscible fluids with slight density contrasts.","The initial interface deformations in these simulations are characterized by an annular spectrum which is parametrized by four non dimensional numbers.","%In order to study the sensitivity of 0D turbulent quantities to the initial interface perturbation distributions, we build a surrogate model for the simulations using a physics-informed neural network (PINN).","This allows us to compute the Sobol indices for the turbulent quantities, disentangling the effects of the initial parameters on the growth of the mixing layer.","Within a Bayesian framework, we use a Markov chain Monte-Carlo method to determine the posterior distributions of initial conditions given various state variables.","%This sheds light on the inertial or diffusive trajectories along with how the initial conditions are progressively forgotten during transition to turbulence.","Moreover, it identifies which turbulent quantities are better predictors for the dynamics of Rayleigh-Taylor mixing zones by more effectively retaining the memory of the flow.","By inferring the initial conditions and forward propagating its maximum a posteriori (MAP) estimate, we propose a strategy to model the Rayleigh-Taylor transition to turbulence."],"url":"http://arxiv.org/abs/2403.17832v1","category":"physics.flu-dyn"}
{"created":"2024-03-26 16:09:29","title":"A modular framework of generalized Hurwitz class numbers","abstract":"We discover a non-trivial rather simple relation between the mock modular generating functions of the level $1$ and level $N$ Hurwitz class numbers. This relation yields a holomorphic modular form of weight $\\frac{3}{2}$ and level $4N$, where $N > 1$ is stipulated to be odd and square-free. We extend this observation to a non-holomorphic framework and obtain a higher level non-holomorphic Zagier Eisenstein series as well as a higher level preimage of it under the Bruinier--Funke operator $\\xi_{\\frac{1}{2}}$. All of these observations are deduced from a more general inspection of a certain weight $\\frac{1}{2}$ Maass--Eisenstein series of level $4N$ at its spectral point $s=\\frac{3}{4}$. This idea goes back to Duke, Imamo\\={g}lu and T\\'{o}th in level $4$ and relies on the theory of so-called sesquiharmonic Maass forms. We calculate the Fourier expansion of our sesquiharmonic preimage and of its shadow. We conclude by offering an example if $N=5$ or $N=7$ and we provide the SAGE code to compute the Fourier coefficients involved.","sentences":["We discover a non-trivial rather simple relation between the mock modular generating functions of the level $1$ and level $N$ Hurwitz class numbers.","This relation yields a holomorphic modular form of weight $\\frac{3}{2}$ and level $4N$, where $N > 1$ is stipulated to be odd and square-free.","We extend this observation to a non-holomorphic framework and obtain a higher level non-holomorphic Zagier Eisenstein series as well as a higher level preimage of it under the Bruinier--Funke operator $\\xi_{\\frac{1}{2}}$. All of these observations are deduced from a more general inspection of a certain weight $\\frac{1}{2}$ Maass--Eisenstein series of level $4N$ at its spectral point $s=\\frac{3}{4}$. This idea goes back to Duke, Imamo\\={g}lu and T\\'{o}th in level $4$ and relies on the theory of so-called sesquiharmonic Maass forms.","We calculate the Fourier expansion of our sesquiharmonic preimage and of its shadow.","We conclude by offering an example if $N=5$ or $N=7$ and we provide the SAGE code to compute the Fourier coefficients involved."],"url":"http://arxiv.org/abs/2403.17829v1","category":"math.NT"}
{"created":"2024-03-26 16:06:56","title":"The Relativistic Spin Precession in the Compact Double Neutron Star System PSR~J1946+2052","abstract":"We observe systematic profile changes in the visible pulsar of the compact double neutron star system PSR~J1946+2052 using observations with the Five-hundred-meter Aperture Spherical radio Telescope (FAST). The interpulse of PSR~J1946+2052 changed from single-peak to double-peak shape from 2018 to 2021. We attribute this evolution as the result of the relativistic spin precession of the pulsar. With the high sensitivity of FAST, we also measure significant polarization for the first time, allowing us to model this with the precessional rotating vector model. Assuming, to the first order, a circular hollow-cone-like emission beam pattern and taking the validity of general relativity, we derive the binary's orbital inclination angle (${63^\\circ}^{+5^\\circ}_{-3^\\circ}$) and pulsar's spin geometry. Pulsar's spin vector and the orbital angular momentum vector are found to be only slightly misaligned (${0.21^\\circ}^{+0.28^\\circ}_{-0.10^\\circ}$).The quoted uncertainties do not reflect the systematic uncertainties introduced by our model assumptions. By simulating future observations of profile and polarization evolution, we estimate that we could constrain the precession rate within a $43\\%$ uncertainty in 9 years. Hence, we suggest that the system's profile evolution could be combined with precise pulsar timing to test general relativity in the future.","sentences":["We observe systematic profile changes in the visible pulsar of the compact double neutron star system PSR~J1946+2052 using observations with the Five-hundred-meter Aperture Spherical radio Telescope (FAST).","The interpulse of PSR~J1946+2052 changed from single-peak to double-peak shape from 2018 to 2021.","We attribute this evolution as the result of the relativistic spin precession of the pulsar.","With the high sensitivity of FAST, we also measure significant polarization for the first time, allowing us to model this with the precessional rotating vector model.","Assuming, to the first order, a circular hollow-cone-like emission beam pattern and taking the validity of general relativity, we derive the binary's orbital inclination angle (${63^\\circ}^{+5^\\circ}_{-3^\\circ}$) and pulsar's spin geometry.","Pulsar's spin vector and the orbital angular momentum vector are found to be only slightly misaligned (${0.21^\\circ}^{+0.28^\\circ}_{-0.10^\\circ}$).The quoted uncertainties do not reflect the systematic uncertainties introduced by our model assumptions.","By simulating future observations of profile and polarization evolution, we estimate that we could constrain the precession rate within a $43\\%$ uncertainty in 9 years.","Hence, we suggest that the system's profile evolution could be combined with precise pulsar timing to test general relativity in the future."],"url":"http://arxiv.org/abs/2403.17828v1","category":"astro-ph.HE"}
{"created":"2024-03-26 16:06:42","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions","abstract":"Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase. For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions. Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage.","sentences":["Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful.","Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets.","We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object.","The method introduces three techniques that enable effective learning from limited data.","First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each.","In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized.","Second, we propose a compact representation that tightly couples hand and object poses.","Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance.","Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose.","Given a grasping motion from this stage, multiple different actions can be prompted in the interaction phase.","For textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions.","Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions.","Moreover, we demonstrate the practicality of our framework by utilizing a hand pose estimate from an off-the-shelf pose estimator for guidance, and then sampling multiple different actions in the interaction stage."],"url":"http://arxiv.org/abs/2403.17827v1","category":"cs.CV"}
{"created":"2024-03-26 16:06:33","title":"On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report","abstract":"Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal. This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games. But, where exactly? All investigations of Stackelberg planning so far focused on practical aspects. We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning. We show that in general Stackelberg planning is actually no harder than classical planning. Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase. In attempts to identify tractable fragments, we further study its complexity under various planning task restrictions, showing that Stackelberg planning remains intractable where classical planning is not. We finally inspect the complexity of meta-operator verification, a problem that has been recently connected to Stackelberg planning.","sentences":["Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal.","This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games.","But, where exactly?","All investigations of Stackelberg planning so far focused on practical aspects.","We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning.","We show that in general Stackelberg planning is actually no harder than classical planning.","Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase.","In attempts to identify tractable fragments, we further study its complexity under various planning task restrictions, showing that Stackelberg planning remains intractable where classical planning is not.","We finally inspect the complexity of meta-operator verification, a problem that has been recently connected to Stackelberg planning."],"url":"http://arxiv.org/abs/2403.17826v1","category":"cs.AI"}
{"created":"2024-03-26 16:06:07","title":"Band Structure and Fermi Surface Nesting in $LaSb_2$","abstract":"We use high-resolution angle resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) to investigate the electronic structure of the charge density wave (CDW) system LaSb$_2$. This compound is among an interesting group of materials that manifests both a CDW transition and lower temperature superconductivity. We find the DFT calculations to be in good agreement with our ARPES data. The Fermi surface of LaSb$_2$ consists of two small hole pockets close to $\\Gamma$ and four larger pockets near the Brillouin zone (BZ) boundary. The overall features of the Fermi surface do not vary with temperature. A saddle point is present at -0.19 $eV$ below the Fermi level at $\\Gamma$. Critical points in band structure have more pronounced effects on a materials properties when they are located closer to the Fermi level, making doped LaSb$_2$ compounds a potential interesting subject of future research. Multiple peaks are present in the generalized, electronic susceptibility calculations indicating the presence of possible nesting vectors. We were not able to detect any signatures of the CDW transition at 355 K, pointing to the subtle nature of this transition. This is unusual, given that such a high transition temperature is expected to be associated with the presence of a large CDW gap. This is confirmed through investigation of the Fermi surface and through analysis of momentum distribution curves (MDC). It is possible that changes are subtle and occur below current sensitivity of our measurements.","sentences":["We use high-resolution angle resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) to investigate the electronic structure of the charge density wave (CDW) system LaSb$_2$.","This compound is among an interesting group of materials that manifests both a CDW transition and lower temperature superconductivity.","We find the DFT calculations to be in good agreement with our ARPES data.","The Fermi surface of LaSb$_2$ consists of two small hole pockets close to $\\Gamma$ and four larger pockets near the Brillouin zone (BZ) boundary.","The overall features of the Fermi surface do not vary with temperature.","A saddle point is present at -0.19 $eV$ below the Fermi level at $\\Gamma$. Critical points in band structure have more pronounced effects on a materials properties when they are located closer to the Fermi level, making doped LaSb$_2$ compounds a potential interesting subject of future research.","Multiple peaks are present in the generalized, electronic susceptibility calculations indicating the presence of possible nesting vectors.","We were not able to detect any signatures of the CDW transition at 355 K, pointing to the subtle nature of this transition.","This is unusual, given that such a high transition temperature is expected to be associated with the presence of a large CDW gap.","This is confirmed through investigation of the Fermi surface and through analysis of momentum distribution curves (MDC).","It is possible that changes are subtle and occur below current sensitivity of our measurements."],"url":"http://arxiv.org/abs/2403.17824v1","category":"cond-mat.str-el"}
{"created":"2024-03-26 15:55:54","title":"Towards Multilevel Modelling of Train Passing Events on the Staffordshire Bridge","abstract":"We suggest a multilevel model, to represent aggregate train-passing events from the Staffordshire bridge monitoring system. We formulate a combined model from simple units, representing strain envelopes (of each train passing) for two types of commuter train. The measurements are treated as a longitudinal dataset and represented with a (low-rank approximation) hierarchical Gaussian process. For each unit in the combined model, we encode domain expertise as boundary condition constraints and work towards a general representation of the strain response. Looking forward, this should allow for the simulation of train types that were previously unobserved in the training data. For example, trains with more passengers or freights with a heavier payload. The strain event simulations are valuable since they can inform further experiments (including FEM calibration, fatigue analysis, or design) to test the bridge in hypothesised scenarios.","sentences":["We suggest a multilevel model, to represent aggregate train-passing events from the Staffordshire bridge monitoring system.","We formulate a combined model from simple units, representing strain envelopes (of each train passing) for two types of commuter train.","The measurements are treated as a longitudinal dataset and represented with a (low-rank approximation) hierarchical Gaussian process.","For each unit in the combined model, we encode domain expertise as boundary condition constraints and work towards a general representation of the strain response.","Looking forward, this should allow for the simulation of train types that were previously unobserved in the training data.","For example, trains with more passengers or freights with a heavier payload.","The strain event simulations are valuable since they can inform further experiments (including FEM calibration, fatigue analysis, or design) to test the bridge in hypothesised scenarios."],"url":"http://arxiv.org/abs/2403.17820v1","category":"stat.AP"}
{"created":"2024-03-26 15:54:48","title":"Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)","abstract":"Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks.   In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.","sentences":["Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications.","To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks.   ","In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes.","We explore various roles that LLMs can play in this context while identifying some of the challenges to address.","The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management."],"url":"http://arxiv.org/abs/2403.17819v1","category":"cs.NI"}
{"created":"2024-03-26 15:53:53","title":"CSSTs: A Dynamic Data Structure for Partial Orders in Concurrent Execution Analysis","abstract":"Dynamic analyses are a standard approach to analyzing and testing concurrent programs. Such techniques observe program traces and analyze them to infer the presence or absence of bugs. At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$. The standard data structure for this task has thus far been vector clocks. These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\\sigma$, while they cannot handle the deletion of existing orderings.   In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time. For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting. Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature.","sentences":["Dynamic analyses are a standard approach to analyzing and testing concurrent programs.","Such techniques observe program traces and analyze them to infer the presence or absence of bugs.","At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$.","The standard data structure for this task has thus far been vector clocks.","These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\\sigma$, while they cannot handle the deletion of existing orderings.   ","In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time.","For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting.","Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature."],"url":"http://arxiv.org/abs/2403.17818v1","category":"cs.PL"}
{"created":"2024-03-26 15:52:36","title":"D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting","abstract":"In time series forecasting, effectively disentangling intricate temporal patterns is crucial. While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal. Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability. In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting. Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the \"shallow\" aspect. A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the \"deep\" aspect. After that, an interaction and fusion (IF) module is used to further analyze the components. Extensive experiments on seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art performance, outperforming the best baseline by an average of 9.48% and 7.15% in MSE and MAE, respectively.","sentences":["In time series forecasting, effectively disentangling intricate temporal patterns is crucial.","While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal.","Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability.","In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting.","Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the \"shallow\" aspect.","A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the \"deep\" aspect.","After that, an interaction and fusion (IF) module is used to further analyze the components.","Extensive experiments on seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art performance, outperforming the best baseline by an average of 9.48% and 7.15% in MSE and MAE, respectively."],"url":"http://arxiv.org/abs/2403.17814v1","category":"cs.AI"}
{"created":"2024-03-26 15:51:20","title":"Estimating the detection of antineutrinos in the future Brazilian neutron source","abstract":"In line with the tradition of employing nuclear reactors for neutrino physics research, we present potential experiments that could be undertaken using the new Brazilian neutron source, the Brazilian Multipurpose Reactor (RMB). This upcoming facility has clearly defined objectives, including the production of radiopharmaceuticals for the healthcare system, material irradiation testing, and research using neutron beams to investigate the micro and mesostructures of various materials. However, we anticipate that the RMB will generate up to $15,500$ antineutrinos daily, potentially expanding the facility's scope to include neutrino physics experiments involving beta and inverse beta reactions. The modern operational conditions of the RMB and its location in the Iper\\'o-Brazil region may also enable neutrino oscillation experiments and the determination of the $\\theta_{13}$ parameter, as well as support safeguards activities by the International Atomic Energy Agency (IAEA), allowing for the monitoring of nuclear fuel burning cycles and isotope fractions. We discuss the potential utilization of the RMB as an antineutrino source, proposing experiments and addressing associated technical limitations.","sentences":["In line with the tradition of employing nuclear reactors for neutrino physics research, we present potential experiments that could be undertaken using the new Brazilian neutron source, the Brazilian Multipurpose Reactor (RMB).","This upcoming facility has clearly defined objectives, including the production of radiopharmaceuticals for the healthcare system, material irradiation testing, and research using neutron beams to investigate the micro and mesostructures of various materials.","However, we anticipate that the RMB will generate up to $15,500$ antineutrinos daily, potentially expanding the facility's scope to include neutrino physics experiments involving beta and inverse beta reactions.","The modern operational conditions of the RMB and its location in the Iper\\'o-Brazil region may also enable neutrino oscillation experiments and the determination of the $\\theta_{13}$ parameter, as well as support safeguards activities by the International Atomic Energy Agency (IAEA), allowing for the monitoring of nuclear fuel burning cycles and isotope fractions.","We discuss the potential utilization of the RMB as an antineutrino source, proposing experiments and addressing associated technical limitations."],"url":"http://arxiv.org/abs/2403.17812v1","category":"physics.ins-det"}
{"created":"2024-03-26 15:46:56","title":"Environment Reconstruction based on Multi-User Selection and Multi-Modal Fusion in ISAC","abstract":"Integrated sensing and communications (ISAC) has been deemed as a key technology for the sixth generation (6G) wireless communications systems. In this paper, we explore the inherent clustered nature of wireless users and design a multi-user based environment reconstruction scheme. Specifically, we first select users based on the estimation precision of channel's multipath, including the line-of-sight (LOS) and the non-line-of-sight (NLOS) paths, to enhance the accuracy of environment reconstruction. Then, we develop a fusion strategy that merges communications signalling with camera image to increase the accuracy and robustness of environment reconstruction. The simulation results demonstrate that the proposed algorithm can achieve a remarkable sensing accuracy of centimeter level, which is about 17 times better than the scheme without user selection. Meanwhile, the fusion of communications data and vision data leads to a threefold accuracy improvement over the image only method, especially under challenging weather conditions like raining and snowing.","sentences":["Integrated sensing and communications (ISAC) has been deemed as a key technology for the sixth generation (6G) wireless communications systems.","In this paper, we explore the inherent clustered nature of wireless users and design a multi-user based environment reconstruction scheme.","Specifically, we first select users based on the estimation precision of channel's multipath, including the line-of-sight (LOS) and the non-line-of-sight (NLOS) paths, to enhance the accuracy of environment reconstruction.","Then, we develop a fusion strategy that merges communications signalling with camera image to increase the accuracy and robustness of environment reconstruction.","The simulation results demonstrate that the proposed algorithm can achieve a remarkable sensing accuracy of centimeter level, which is about 17 times better than the scheme without user selection.","Meanwhile, the fusion of communications data and vision data leads to a threefold accuracy improvement over the image only method, especially under challenging weather conditions like raining and snowing."],"url":"http://arxiv.org/abs/2403.17810v1","category":"eess.SP"}
{"created":"2024-03-26 15:45:29","title":"Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields","abstract":"The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated data in the biomedical imaging domain. To address this limitation, we propose Biomedical Video Diffusion Model (BVDM), capable of generating realistic-looking synthetic microscopy videos. Trained only on a single real video, BVDM can generate videos of arbitrary length with pixel-level annotations that can be used for training data-hungry models. It is composed of a denoising diffusion probabilistic model (DDPM) generating high-fidelity synthetic cell microscopy images and a flow prediction model (FPM) predicting the non-rigid transformation between consecutive video frames. During inference, initially, the DDPM imposes realistic cell textures on synthetic cell masks which are generated based on real data statistics. The flow prediction model predicts the flow field between consecutive masks and applies that to the DDPM output from the previous time frame to create the next one while keeping temporal consistency. BVDM outperforms state-of-the-art synthetic live cell microscopy video generation models. Furthermore, we demonstrate that a sufficiently large synthetic dataset enhances the performance of cell segmentation and tracking models compared to using a limited amount of available real data.","sentences":["The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology.","These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts.","Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed.","These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated data in the biomedical imaging domain.","To address this limitation, we propose Biomedical Video Diffusion Model (BVDM), capable of generating realistic-looking synthetic microscopy videos.","Trained only on a single real video, BVDM can generate videos of arbitrary length with pixel-level annotations that can be used for training data-hungry models.","It is composed of a denoising diffusion probabilistic model (DDPM) generating high-fidelity synthetic cell microscopy images and a flow prediction model (FPM) predicting the non-rigid transformation between consecutive video frames.","During inference, initially, the DDPM imposes realistic cell textures on synthetic cell masks which are generated based on real data statistics.","The flow prediction model predicts the flow field between consecutive masks and applies that to the DDPM output from the previous time frame to create the next one while keeping temporal consistency.","BVDM outperforms state-of-the-art synthetic live cell microscopy video generation models.","Furthermore, we demonstrate that a sufficiently large synthetic dataset enhances the performance of cell segmentation and tracking models compared to using a limited amount of available real data."],"url":"http://arxiv.org/abs/2403.17808v1","category":"eess.IV"}
{"created":"2024-03-26 15:45:07","title":"Towards Inclusive Video Commenting: Introducing Signmaku for the Deaf and Hard-of-Hearing","abstract":"Previous research underscored the potential of danmaku--a text-based commenting feature on videos--in engaging hearing audiences. Yet, for many Deaf and hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes precedence over English. To improve inclusivity, we introduce \"Signmaku,\" a new commenting mechanism that uses ASL, serving as a sign language counterpart to danmaku. Through a need-finding study (N=12) and a within-subject experiment (N=20), we evaluated three design styles: real human faces, cartoon-like figures, and robotic representations. The results showed that cartoon-like signmaku not only entertained but also encouraged participants to create and share ASL comments, with fewer privacy concerns compared to the other designs. Conversely, the robotic representations faced challenges in accurately depicting hand movements and facial expressions, resulting in higher cognitive demands on users. Signmaku featuring real human faces elicited the lowest cognitive load and was the most comprehensible among all three types. Our findings offered novel design implications for leveraging generative AI to create signmaku comments, enriching co-learning experiences for DHH individuals.","sentences":["Previous research underscored the potential of danmaku--a text-based commenting feature on videos--in engaging hearing audiences.","Yet, for many Deaf and hard-of-hearing (DHH) individuals, American Sign Language (ASL) takes precedence over English.","To improve inclusivity, we introduce \"Signmaku,\" a new commenting mechanism that uses ASL, serving as a sign language counterpart to danmaku.","Through a need-finding study (N=12) and a within-subject experiment (N=20), we evaluated three design styles: real human faces, cartoon-like figures, and robotic representations.","The results showed that cartoon-like signmaku not only entertained but also encouraged participants to create and share ASL comments, with fewer privacy concerns compared to the other designs.","Conversely, the robotic representations faced challenges in accurately depicting hand movements and facial expressions, resulting in higher cognitive demands on users.","Signmaku featuring real human faces elicited the lowest cognitive load and was the most comprehensible among all three types.","Our findings offered novel design implications for leveraging generative AI to create signmaku comments, enriching co-learning experiences for DHH individuals."],"url":"http://arxiv.org/abs/2403.17807v1","category":"cs.HC"}
{"created":"2024-03-26 15:44:58","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms","abstract":"Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions. We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.","sentences":["Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task.","Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size.","Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem.","In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness.","A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model.","Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions.","We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured."],"url":"http://arxiv.org/abs/2403.17806v1","category":"cs.LG"}
{"created":"2024-03-26 15:42:04","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","abstract":"The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks. Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments. In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator. MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents. This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated with techniques from unsupervised environment design to automate the generation of adaptive auto-curricula. The code is available at https://github.com/AutonomousDrivingExaminer/mats-gym.","sentences":["The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks.","Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies.","However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments.","In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator.","MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents.","This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated with techniques from unsupervised environment design to automate the generation of adaptive auto-curricula.","The code is available at https://github.com/AutonomousDrivingExaminer/mats-gym."],"url":"http://arxiv.org/abs/2403.17805v1","category":"cs.RO"}
{"created":"2024-03-26 15:42:01","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","abstract":"Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score. Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data. Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs.","sentences":["Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images.","Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly.","Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency.","In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models.","Our framework starts from a user prompt and iteratively generates revised prompts with the goal of maximizing a consistency score.","Our extensive validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost the initial consistency score by up to 24.9% in terms of DSG score while preserving the FID and increasing the recall between generated and real data.","Our work paves the way toward building more reliable and robust T2I systems by harnessing the power of LLMs."],"url":"http://arxiv.org/abs/2403.17804v1","category":"cs.CV"}
{"created":"2024-03-26 15:39:59","title":"Steering Feedback in Dynamic Driving Simulators: The Influence of Steering Wheel Vibration and Vehicle Motion Frequency","abstract":"The validity of the subjective evaluation of steering feedback in driving simulators is crucial for modern vehicle development. Although there are established objective steering characteristics for the assessment of both stationary and dynamic feedback behaviour, factors such as steering wheel vibrations and vehicle body motion, particularly in high-frequency ranges, present challenges in simulator fidelity. This work investigates the influence of steering wheel vibration and vehicle body motion frequency content on the subjective evaluation of steering feedback during closed-loop driving in a dynamic driving simulator. A controlled subject study with 30 participants consisting of a back-to-back comparison of a reference vehicle with an electrical power steering system and three variants of its virtual representation on a dynamic driving simulator was performed. Subjective evaluation focused on the representation of road feedback in comparison to the reference vehicle. The statistical analysis of subjective results show that there is a significant influence of the frequency content of both steering wheel torque and vehicle motion on the subjective evaluation of steering feedback in a dynamic driving simulator. The results suggest an influence of frequency content on the subjective evaluation quality of steering feedback characteristics that are not associated with the dynamic feedback behaviour in the context of established performance indicators.","sentences":["The validity of the subjective evaluation of steering feedback in driving simulators is crucial for modern vehicle development.","Although there are established objective steering characteristics for the assessment of both stationary and dynamic feedback behaviour, factors such as steering wheel vibrations and vehicle body motion, particularly in high-frequency ranges, present challenges in simulator fidelity.","This work investigates the influence of steering wheel vibration and vehicle body motion frequency content on the subjective evaluation of steering feedback during closed-loop driving in a dynamic driving simulator.","A controlled subject study with 30 participants consisting of a back-to-back comparison of a reference vehicle with an electrical power steering system and three variants of its virtual representation on a dynamic driving simulator was performed.","Subjective evaluation focused on the representation of road feedback in comparison to the reference vehicle.","The statistical analysis of subjective results show that there is a significant influence of the frequency content of both steering wheel torque and vehicle motion on the subjective evaluation of steering feedback in a dynamic driving simulator.","The results suggest an influence of frequency content on the subjective evaluation quality of steering feedback characteristics that are not associated with the dynamic feedback behaviour in the context of established performance indicators."],"url":"http://arxiv.org/abs/2403.17800v1","category":"eess.SY"}
{"created":"2024-03-26 15:32:35","title":"Solution Generation of a Capped Black Hole","abstract":"Utilizing the electric Harrison transformation developed in five-dimensional minimal supergravity, we construct an exact solution characterizing non-BPS charged rotating black holes with a horizon cross-section of a lens space L(n;1). Among these solutions, only the ones corresponding to n=0 and n=1 do not have any curvature singularities, conical singularities, Dirac-Misner string singularities, and orbifold singularities both on and outside the horizon; additionally, it is free from closed timelike curves. The solution for n=0 corresponds to the charged dipole black ring that we constructed in the previous paper. The specific solution for n=1, referred to as the ``capped black hole,\" was introduced in our previous letter. This provides the first example of a non-BPS exact solution, representing an asymptotically flat, stationary spherical black hole with a domain of outer communication (DOC) having a nontrivial topology in five-dimensional minimal supergravity. We demonstrate that the DOC on a timeslice has the topology of $[R^4\\# CP^2 ]\\setminus B^4$. Differing from the well-known Myers-Perry and Cveti\\v{c}-Youm black holes describing a spherical horizon topology and a DOC with a trivial topology of $R^4 \\setminus B^4$ on a timeslice, the capped black hole's horizon is capped by a disc-shaped bubble. We explicitly demonstrate that the capped black hole carries mass, two angular momenta, an electric charge, and a magnetic flux, with only three of these quantities being independent. Furthermore, we reveal that this black hole can possess identical conserved charges as the Cveti\\v{c}-Youm black hole. The existence of this solution challenges black hole uniqueness beyond both the black ring and the BPS spherical black hole. Moreover, within specific parameter regions, the capped black hole can exhibit a larger entropy than the Cveti\\v{c}-Youm black hole.","sentences":["Utilizing the electric Harrison transformation developed in five-dimensional minimal supergravity, we construct an exact solution characterizing non-BPS charged rotating black holes with a horizon cross-section of a lens space L(n;1).","Among these solutions, only the ones corresponding to n=0 and n=1 do not have any curvature singularities, conical singularities, Dirac-Misner string singularities, and orbifold singularities both on and outside the horizon; additionally, it is free from closed timelike curves.","The solution for n=0 corresponds to the charged dipole black ring that we constructed in the previous paper.","The specific solution for n=1, referred to as the ``capped black hole,\" was introduced in our previous letter.","This provides the first example of a non-BPS exact solution, representing an asymptotically flat, stationary spherical black hole with a domain of outer communication (DOC) having a nontrivial topology in five-dimensional minimal supergravity.","We demonstrate that the DOC on a timeslice has the topology of $[R^4\\# CP^2 ]\\setminus B^4$. Differing from the well-known Myers-Perry and Cveti\\v{c}-Youm black holes describing a spherical horizon topology and a DOC with a trivial topology of $R^4 \\setminus B^4$ on a timeslice, the capped black hole's horizon is capped by a disc-shaped bubble.","We explicitly demonstrate that the capped black hole carries mass, two angular momenta, an electric charge, and a magnetic flux, with only three of these quantities being independent.","Furthermore, we reveal that this black hole can possess identical conserved charges as the Cveti\\v{c}-Youm black hole.","The existence of this solution challenges black hole uniqueness beyond both the black ring and the BPS spherical black hole.","Moreover, within specific parameter regions, the capped black hole can exhibit a larger entropy than the Cveti\\v{c}-Youm black hole."],"url":"http://arxiv.org/abs/2403.17796v1","category":"hep-th"}
{"created":"2024-03-26 15:21:18","title":"A PAC-Bayesian Framework for Optimal Control with Stability Guarantees","abstract":"Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost function that averages out the random uncertainties affecting the dynamics of nonlinear systems. For tractability reasons, this problem is typically addressed by minimizing an empirical cost, which represents the average cost across a finite dataset of sampled disturbances. However, this approach raises the challenge of quantifying the control performance against out-of-sample uncertainties. Particularly, in scenarios where the training dataset is small, SNOC policies are prone to overfitting, resulting in significant discrepancies between the empirical cost and the true cost, i.e., the average SNOC cost incurred during control deployment. Therefore, establishing generalization bounds on the true cost is crucial for ensuring reliability in real-world applications. In this paper, we introduce a novel approach that leverages PAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on these bounds, we propose a new method for designing optimal controllers, offering a principled way to incorporate prior knowledge into the synthesis process, which aids in improving the control policy and mitigating overfitting. Furthermore, by leveraging recent parametrizations of stabilizing controllers for nonlinear systems, our framework inherently ensures closed-loop stability. The effectiveness of our proposed method in incorporating prior knowledge and combating overfitting is shown by designing neural network controllers for tasks in cooperative robotics.","sentences":["Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost function that averages out the random uncertainties affecting the dynamics of nonlinear systems.","For tractability reasons, this problem is typically addressed by minimizing an empirical cost, which represents the average cost across a finite dataset of sampled disturbances.","However, this approach raises the challenge of quantifying the control performance against out-of-sample uncertainties.","Particularly, in scenarios where the training dataset is small, SNOC policies are prone to overfitting, resulting in significant discrepancies between the empirical cost and the true cost, i.e., the average SNOC cost incurred during control deployment.","Therefore, establishing generalization bounds on the true cost is crucial for ensuring reliability in real-world applications.","In this paper, we introduce a novel approach that leverages PAC-Bayes theory to provide rigorous generalization bounds for SNOC.","Based on these bounds, we propose a new method for designing optimal controllers, offering a principled way to incorporate prior knowledge into the synthesis process, which aids in improving the control policy and mitigating overfitting.","Furthermore, by leveraging recent parametrizations of stabilizing controllers for nonlinear systems, our framework inherently ensures closed-loop stability.","The effectiveness of our proposed method in incorporating prior knowledge and combating overfitting is shown by designing neural network controllers for tasks in cooperative robotics."],"url":"http://arxiv.org/abs/2403.17790v1","category":"eess.SY"}
{"created":"2024-03-26 15:21:12","title":"Molecular groundstate determination via short pulses on superconducting qubits","abstract":"Quantum computing is currently hindered by hardware noise. We present a freestyle superconducting pulse optimization method, incorporating two-qubit channels, which enhances flexibility, execution speed, and noise resilience. A minimal 0.22 ns pulse is shown to determine the H2 groundstate to within chemical accuracy upon real-hardware, approaching the quantum speed limit. Similarly, a pulse significantly shorter than circuit-based counterparts is found for the LiH molecule, attaining state-of-the-art accuracy. The method is general and can potentially accelerate performance across various quantum computing components and hardware.","sentences":["Quantum computing is currently hindered by hardware noise.","We present a freestyle superconducting pulse optimization method, incorporating two-qubit channels, which enhances flexibility, execution speed, and noise resilience.","A minimal 0.22 ns pulse is shown to determine the H2 groundstate to within chemical accuracy upon real-hardware, approaching the quantum speed limit.","Similarly, a pulse significantly shorter than circuit-based counterparts is found for the LiH molecule, attaining state-of-the-art accuracy.","The method is general and can potentially accelerate performance across various quantum computing components and hardware."],"url":"http://arxiv.org/abs/2403.17789v1","category":"quant-ph"}
{"created":"2024-03-26 15:20:49","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","abstract":"The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models. The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks. This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of fine-tuned ViT models for precise and dependable tasks.","sentences":["The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications.","These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications.","This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges.","We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations.","Our results highlight a significant divergence in performance, with Gemini-pro falling short in accuracy and reliability when compared to fine-tuned ViT models.","The ViT models, on the other hand, demonstrate exceptional accuracy, achieving near-perfect performance on both tasks.","This study not only showcases the strengths and limitations of prompt-engineered LMMs in cybersecurity applications but also emphasizes the unmatched efficacy of fine-tuned ViT models for precise and dependable tasks."],"url":"http://arxiv.org/abs/2403.17787v1","category":"cs.AI"}
{"created":"2024-03-26 15:16:14","title":"SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings","abstract":"Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure's message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing. Participants' feedback further offers valuable design insights for future systems aiming to enhance caption writing.","sentences":["Crafting effective captions for figures is important.","Readers heavily depend on these captions to grasp the figure's message.","However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing.","This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition.","SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference.","Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them.","A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing.","Participants' feedback further offers valuable design insights for future systems aiming to enhance caption writing."],"url":"http://arxiv.org/abs/2403.17784v1","category":"cs.HC"}
{"created":"2024-03-26 15:15:44","title":"Intersecting subsets in finite permutation groups","abstract":"A subset (subgroup) $S$ of a transitive permutation group $G\\leq Sym(\\Omega)$ is called an intersecting subset (subgroup, resp.) if the ratio $xy^{-1}$ of any elements $x,y\\in S$ fixes some point. A transitive group is said to have the EKR property if the size of each intersecting subset is at most the order of the point stabilizer. A nice result of Meagher-Spiga-Tiep (2016) says that 2-transitive permutation groups have the EKR property. In this paper, we systematically study intersecting subsets in more general transitive permutation groups, including primitive (quasiprimitive) groups, rank-3 groups, Suzuki groups, and some special solvable groups. We present new families of groups that have the EKR property, and various families of groups that do not have the EKR property. This paper significantly improves the unpublished version of this paper, and particularly solves Problem 1.4 of it.","sentences":["A subset (subgroup) $S$ of a transitive permutation group $G\\leq Sym(\\Omega)$ is called an intersecting subset (subgroup, resp.)","if the ratio $xy^{-1}$ of any elements $x,y\\in S$ fixes some point.","A transitive group is said to have the EKR property if the size of each intersecting subset is at most the order of the point stabilizer.","A nice result of Meagher-Spiga-Tiep (2016) says that 2-transitive permutation groups have the EKR property.","In this paper, we systematically study intersecting subsets in more general transitive permutation groups, including primitive (quasiprimitive) groups, rank-3 groups, Suzuki groups, and some special solvable groups.","We present new families of groups that have the EKR property, and various families of groups that do not have the EKR property.","This paper significantly improves the unpublished version of this paper, and particularly solves Problem 1.4 of it."],"url":"http://arxiv.org/abs/2403.17783v1","category":"math.GR"}
{"created":"2024-03-26 15:12:46","title":"Optical Flow Based Detection and Tracking of Moving Objects for Autonomous Vehicles","abstract":"Accurate velocity estimation of surrounding moving objects and their trajectories are critical elements of perception systems in Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These are non-trivial problems due to the diverse types and sizes of such objects and their dynamic and random behaviour. Recent point cloud based solutions often use Iterative Closest Point (ICP) techniques, which are known to have certain limitations. For example, their computational costs are high due to their iterative nature, and their estimation error often deteriorates as the relative velocities of the target objects increase (>2 m/sec). Motivated by such shortcomings, this paper first proposes a novel Detection and Tracking of Moving Objects (DATMO) for AVs based on an optical flow technique, which is proven to be computationally efficient and highly accurate for such problems. \\textcolor{black}{This is achieved by representing the driving scenario as a vector field and applying vector calculus theories to ensure spatiotemporal continuity.} We also report the results of a comprehensive performance evaluation of the proposed DATMO technique, carried out in this study using synthetic and real-world data. The results of this study demonstrate the superiority of the proposed technique, compared to the DATMO techniques in the literature, in terms of estimation accuracy and processing time in a wide range of relative velocities of moving objects. Finally, we evaluate and discuss the sensitivity of the estimation error of the proposed DATMO technique to various system and environmental parameters, as well as the relative velocities of the moving objects.","sentences":["Accurate velocity estimation of surrounding moving objects and their trajectories are critical elements of perception systems in Automated/Autonomous Vehicles (AVs) with a direct impact on their safety.","These are non-trivial problems due to the diverse types and sizes of such objects and their dynamic and random behaviour.","Recent point cloud based solutions often use Iterative Closest Point (ICP) techniques, which are known to have certain limitations.","For example, their computational costs are high due to their iterative nature, and their estimation error often deteriorates as the relative velocities of the target objects increase (>2 m/sec).","Motivated by such shortcomings, this paper first proposes a novel Detection and Tracking of Moving Objects (DATMO) for AVs based on an optical flow technique, which is proven to be computationally efficient and highly accurate for such problems.","\\textcolor{black}{This is achieved by representing the driving scenario as a vector field and applying vector calculus theories to ensure spatiotemporal continuity.","}","We also report the results of a comprehensive performance evaluation of the proposed DATMO technique, carried out in this study using synthetic and real-world data.","The results of this study demonstrate the superiority of the proposed technique, compared to the DATMO techniques in the literature, in terms of estimation accuracy and processing time in a wide range of relative velocities of moving objects.","Finally, we evaluate and discuss the sensitivity of the estimation error of the proposed DATMO technique to various system and environmental parameters, as well as the relative velocities of the moving objects."],"url":"http://arxiv.org/abs/2403.17779v1","category":"cs.RO"}
{"created":"2024-03-26 15:11:18","title":"Towards a FAIR Documentation of Workflows and Models in Applied Mathematics","abstract":"Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.","sentences":["Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics.","The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows.","MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template.","Central to these workflows are mathematical models.","MaRDI addresses them with the MathModDB ontology, offering a structured formal model description.","Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities.","This demonstration underscores the versatility of both services beyond their original numerical domain."],"url":"http://arxiv.org/abs/2403.17778v1","category":"cs.AI"}
{"created":"2024-03-26 15:09:33","title":"Exploring the Boundaries of Ambient Awareness in Twitter","abstract":"Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter). Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks. However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network. In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible. First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter. Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network. Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility. This analysis shows that (in line with \\citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA. We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness.","sentences":["Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter).","Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks.","However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network.","In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible.","First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter.","Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network.","Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility.","This analysis shows that (in line with \\citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA.","We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness."],"url":"http://arxiv.org/abs/2403.17776v1","category":"cs.SI"}
{"created":"2024-03-26 15:00:20","title":"Coupled Nonlinear Schr\u00f6dinger (CNLS) Equations for two interacting electrostatic wavepackets in a non-Maxwellian fluid plasma model","abstract":"The nonlinear dynamics of two co-propagating electrostatic wavepackets, characterized by different wavenumbers and amplitudes, in a 1D non-magnetized plasma fluid model is considered, from first principles. The original plasma model, consisting of \\kappa-distributed electrons evolving against a cold ion background, is reduced, by means of a multiple-scale perturbation method to a pair of asymmetric coupled nonlinear Schr\\\"odinger (CNLS) equations for the dynamics of the wavepacket envelopes.   Exact analytical expressions are derived for the dispersion, self-modulation, and cross-modulation coefficients involved in the CNLS equations, as functions of the wavenumbers and the spectral index \\kappa characterizing the electron profile. An analytical investigation of the modulational instability (MI) properties of this pair of wavepackets reveals that MI occurs in most parts of the parameter space.   The instability windows and the corresponding growth rate are calculated in a number of case studies. Two-wave interaction favors MI by extending its range of occurrence and by enhancing its growth rate. Growth rate patterns obtained for different \\kappa suggest that deviation from Maxwellian equilibrium, for low \\kappa values, leads to enhanced MI of the interacting wave pair.   To the best of our knowledge, the dynamics of two co-propagating wavepackets in a plasma described by a fluid model with \\kappa-distributed electrons is investigated thoroughly with respect to their MI properties as a function of \\kappa for the first time, in the framework of an asymmetric CNLS system. Although we have focused on electrostatic wavepacket propagation in non-Maxwellian plasma, the results are generic and may be used as basis to model energy localization in nonlinear optics, in hydrodynamics or in dispersive media with Kerr-type nonlinearities where MI is relevant.","sentences":["The nonlinear dynamics of two co-propagating electrostatic wavepackets, characterized by different wavenumbers and amplitudes, in a 1D non-magnetized plasma fluid model is considered, from first principles.","The original plasma model, consisting of \\kappa-distributed electrons evolving against a cold ion background, is reduced, by means of a multiple-scale perturbation method to a pair of asymmetric coupled nonlinear Schr\\\"odinger (CNLS) equations for the dynamics of the wavepacket envelopes.   ","Exact analytical expressions are derived for the dispersion, self-modulation, and cross-modulation coefficients involved in the CNLS equations, as functions of the wavenumbers and the spectral index \\kappa characterizing the electron profile.","An analytical investigation of the modulational instability (MI) properties of this pair of wavepackets reveals that MI occurs in most parts of the parameter space.   ","The instability windows and the corresponding growth rate are calculated in a number of case studies.","Two-wave interaction favors MI by extending its range of occurrence and by enhancing its growth rate.","Growth rate patterns obtained for different \\kappa suggest that deviation from Maxwellian equilibrium, for low \\kappa values, leads to enhanced MI of the interacting wave pair.   ","To the best of our knowledge, the dynamics of two co-propagating wavepackets in a plasma described by a fluid model with \\kappa-distributed electrons is investigated thoroughly with respect to their MI properties as a function of \\kappa for the first time, in the framework of an asymmetric CNLS system.","Although we have focused on electrostatic wavepacket propagation in non-Maxwellian plasma, the results are generic and may be used as basis to model energy localization in nonlinear optics, in hydrodynamics or in dispersive media with Kerr-type nonlinearities where MI is relevant."],"url":"http://arxiv.org/abs/2403.17772v1","category":"physics.plasm-ph"}
{"created":"2024-03-26 14:59:11","title":"CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation","abstract":"Despite the significant success achieved by deep learning methods in medical image segmentation, researchers still struggle in the computer-aided diagnosis of abdominal lymph nodes due to the complex abdominal environment, small and indistinguishable lesions, and limited annotated data. To address these problems, we present a pipeline that integrates the conditional diffusion model for lymph node generation and the nnU-Net model for lymph node segmentation to improve the segmentation performance of abdominal lymph nodes through synthesizing a diversity of realistic abdominal lymph node data. We propose LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical structure masks as model conditions. These conditions work in two conditioning mechanisms: global structure conditioning and local detail conditioning, to distinguish between lymph nodes and their surroundings and better capture lymph node characteristics. The obtained paired abdominal lymph node images and masks are used for the downstream segmentation task. Experimental results on the abdominal lymph node datasets demonstrate that LN-DDPM outperforms other generative methods in the abdominal lymph node image synthesis and better assists the downstream abdominal lymph node segmentation task.","sentences":["Despite the significant success achieved by deep learning methods in medical image segmentation, researchers still struggle in the computer-aided diagnosis of abdominal lymph nodes due to the complex abdominal environment, small and indistinguishable lesions, and limited annotated data.","To address these problems, we present a pipeline that integrates the conditional diffusion model for lymph node generation and the nnU-Net model for lymph node segmentation to improve the segmentation performance of abdominal lymph nodes through synthesizing a diversity of realistic abdominal lymph node data.","We propose LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph node (LN) generation.","LN-DDPM utilizes lymph node masks and anatomical structure masks as model conditions.","These conditions work in two conditioning mechanisms: global structure conditioning and local detail conditioning, to distinguish between lymph nodes and their surroundings and better capture lymph node characteristics.","The obtained paired abdominal lymph node images and masks are used for the downstream segmentation task.","Experimental results on the abdominal lymph node datasets demonstrate that LN-DDPM outperforms other generative methods in the abdominal lymph node image synthesis and better assists the downstream abdominal lymph node segmentation task."],"url":"http://arxiv.org/abs/2403.17770v1","category":"eess.IV"}
{"created":"2024-03-26 14:54:48","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation","abstract":"Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related to this work are available at https://dongqi.me/projects/SciNews.","sentences":["Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public.","The automated generation of such narratives enhances the accessibility of scholarly insights.","In this paper, we present a new corpus to facilitate this paradigm development.","Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines.","To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts.","We benchmark our dataset employing state-of-the-art text generation models.","The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports.","The dataset and code related to this work are available at https://dongqi.me/projects/SciNews."],"url":"http://arxiv.org/abs/2403.17768v1","category":"cs.CL"}
{"created":"2024-03-26 14:54:02","title":"Counting Stars is Constant-Degree Optimal For Detecting Any Planted Subgraph","abstract":"We study the computational limits of the following general hypothesis testing problem. Let H=H_n be an \\emph{arbitrary} undirected graph on n vertices. We study the detection task between a ``null'' Erd\\H{o}s-R\\'{e}nyi random graph G(n,p) and a ``planted'' random graph which is the union of G(n,p) together with a random copy of H=H_n. Our notion of planted model is a generalization of a plethora of recently studied models initiated with the study of the planted clique model (Jerrum 1992), which corresponds to the special case where H is a k-clique and p=1/2.   Over the last decade, several papers have studied the power of low-degree polynomials for limited choices of H's in the above task. In this work, we adopt a unifying perspective and characterize the power of \\emph{constant degree} polynomials for the detection task, when \\emph{H=H_n is any arbitrary graph} and for \\emph{any p=\\Omega(1).} Perhaps surprisingly, we prove that the optimal constant degree polynomial is always given by simply \\emph{counting stars} in the input random graph. As a direct corollary, we conclude that the class of constant-degree polynomials is only able to ``sense'' the degree distribution of the planted graph H, and no other graph theoretic property of it.","sentences":["We study the computational limits of the following general hypothesis testing problem.","Let H=H_n be an \\emph{arbitrary} undirected graph on n vertices.","We study the detection task between a ``null'' Erd\\H{o}s-R\\'{e}nyi random graph G(n,p) and a ``planted'' random graph which is the union of G(n,p) together with a random copy of H=H_n.","Our notion of planted model is a generalization of a plethora of recently studied models initiated with the study of the planted clique model (Jerrum 1992), which corresponds to the special case where H is a k-clique and p=1/2.   ","Over the last decade, several papers have studied the power of low-degree polynomials for limited choices of H's in the above task.","In this work, we adopt a unifying perspective and characterize the power of \\emph{constant degree} polynomials for the detection task, when \\emph{H=H_n is any arbitrary graph} and for \\emph{any p=\\Omega(1).}","Perhaps surprisingly, we prove that the optimal constant degree polynomial is always given by simply \\emph{counting stars} in the input random graph.","As a direct corollary, we conclude that the class of constant-degree polynomials is only able to ``sense'' the degree distribution of the planted graph H, and no other graph theoretic property of it."],"url":"http://arxiv.org/abs/2403.17766v1","category":"math.ST"}
{"created":"2024-03-26 14:51:55","title":"On the uniqueness of the infinite cluster and the cluster density in the Poisson driven random connection model","abstract":"We consider a random connection model (RCM) on a general space driven by a Poisson process whose intensity measure is scaled by a parameter $t\\ge 0$. We say that the infinite clusters are deletion stable if the removal of a Poisson point cannot split a cluster in two or more infinite clusters. We prove that this stability together with a natural irreducibility assumption implies uniqueness of the infinite cluster. Conversely, if the infinite cluster is unique then this stability property holds. Several criteria for irreducibility will be established. We also study the analytic properties of expectations of functions of clusters as a function of $t$. In particular we show that the position dependent cluster density is differentiable. A significant part of this paper is devoted to the important case of a stationary marked RCM (in Euclidean space), containing the Boolean model with general compact grains and the so-called weighted RCM as special cases. In this case we establish differentiability and a convexity property of the cluster density $\\kappa(t)$. These properties are crucial for our proof of deletion stability of the infinite clusters but are also of interest in their own right. It then follows that an irreducible stationary marked RCM can have at most one infinite cluster. This extends and unifies several results in the literature.","sentences":["We consider a random connection model (RCM) on a general space driven by a Poisson process whose intensity measure is scaled by a parameter $t\\ge 0$.","We say that the infinite clusters are deletion stable if the removal of a Poisson point cannot split a cluster in two or more infinite clusters.","We prove that this stability together with a natural irreducibility assumption implies uniqueness of the infinite cluster.","Conversely, if the infinite cluster is unique then this stability property holds.","Several criteria for irreducibility will be established.","We also study the analytic properties of expectations of functions of clusters as a function of $t$. In particular we show that the position dependent cluster density is differentiable.","A significant part of this paper is devoted to the important case of a stationary marked RCM (in Euclidean space), containing the Boolean model with general compact grains and the so-called weighted RCM as special cases.","In this case we establish differentiability and a convexity property of the cluster density $\\kappa(t)$. These properties are crucial for our proof of deletion stability of the infinite clusters but are also of interest in their own right.","It then follows that an irreducible stationary marked RCM can have at most one infinite cluster.","This extends and unifies several results in the literature."],"url":"http://arxiv.org/abs/2403.17762v1","category":"math.PR"}
{"created":"2024-03-26 14:44:51","title":"DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection","abstract":"In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant. Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable. This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase. DataCook operates by \"cooking\" the raw data before distribution, enabling the development of models that perform normally on this processed data. However, during the deployment phase, the original test data must be also \"cooked\" through DataCook to ensure normal model performance. This process grants copyright holders control over authorization during the deployment phase. The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable. We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants. The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios. Code and data are available at https://github.com/MedMNIST/DataCook.","sentences":["In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant.","Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable.","This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase.","DataCook operates by \"cooking\" the raw data before distribution, enabling the development of models that perform normally on this processed data.","However, during the deployment phase, the original test data must be also \"cooked\" through DataCook to ensure normal model performance.","This process grants copyright holders control over authorization during the deployment phase.","The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to standard adversarial examples (Adv) that aim to confuse models.","Similar to Adv, AntiAdv introduces imperceptible perturbations, ensuring that the data processed by DataCook remains easily understandable.","We conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D data and the high-resolution variants.","The outcomes indicate that DataCook effectively meets its objectives, preventing models trained on AntiAdv from analyzing unauthorized data effectively, without compromising the validity and accuracy of the data in legitimate scenarios.","Code and data are available at https://github.com/MedMNIST/DataCook."],"url":"http://arxiv.org/abs/2403.17755v1","category":"cs.AI"}
{"created":"2024-03-26 14:43:48","title":"Robust Analysis of Full-Duplex Two-Way Space Shift Keying With RIS Systems","abstract":"Reconfigurable intelligent surface (RIS)-assisted index modulation system schemes are considered a promising technology for sixth-generation (6G) wireless communication systems, which can enhance various system capabilities such as coverage and reliability. However, obtaining perfect channel state information (CSI) is challenging due to the lack of a radio frequency chain in RIS. In this paper, we investigate the RIS-assisted full-duplex (FD) two-way space shift keying (SSK) system under imperfect CSI, where the signal emissions are augmented by deploying RISs in the vicinity of two FD users. The maximum likelihood detector is utilized to recover the transmit antenna index. With this in mind, we derive closed-form average bit error probability (ABEP) expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide the upper bound and asymptotic ABEP expressions in the presence of channel estimation errors. To gain more insights, we also derive the outage probability and provide the throughput of the proposed scheme with imperfect CSI. The correctness of the analytical derivation results is confirmed via Monte Carlo simulations. It is demonstrated that increasing the number of elements of RIS can significantly improve the ABEP performance of the FD system over the half-duplex (HD) system. Furthermore, in the high SNR region, the ABEP performance of the FD system is better than that of the HD system.","sentences":["Reconfigurable intelligent surface (RIS)-assisted index modulation system schemes are considered a promising technology for sixth-generation (6G) wireless communication systems, which can enhance various system capabilities such as coverage and reliability.","However, obtaining perfect channel state information (CSI) is challenging due to the lack of a radio frequency chain in RIS.","In this paper, we investigate the RIS-assisted full-duplex (FD) two-way space shift keying (SSK) system under imperfect CSI, where the signal emissions are augmented by deploying RISs in the vicinity of two FD users.","The maximum likelihood detector is utilized to recover the transmit antenna index.","With this in mind, we derive closed-form average bit error probability (ABEP) expression based on the Gaussian-Chebyshev quadrature (GCQ) method and provide the upper bound and asymptotic ABEP expressions in the presence of channel estimation errors.","To gain more insights, we also derive the outage probability and provide the throughput of the proposed scheme with imperfect CSI.","The correctness of the analytical derivation results is confirmed via Monte Carlo simulations.","It is demonstrated that increasing the number of elements of RIS can significantly improve the ABEP performance of the FD system over the half-duplex (HD) system.","Furthermore, in the high SNR region, the ABEP performance of the FD system is better than that of the HD system."],"url":"http://arxiv.org/abs/2403.17751v1","category":"cs.IT"}
{"created":"2024-03-26 14:43:48","title":"Can multiple-choice questions really be useful in detecting the abilities of LLMs?","abstract":"Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs' output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.","sentences":["Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency.","However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required.","The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English.","We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position.","We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings.","Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions.","Additionally, we propose two methods to quantify the consistency and confidence of LLMs' output, which can be generalized to other QA evaluation benchmarks.","Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy.","We also find MCQs to be less reliable than LFGQs in terms of expected calibration error.","Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space.","Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs."],"url":"http://arxiv.org/abs/2403.17752v1","category":"cs.CL"}
{"created":"2024-03-26 14:40:17","title":"Multi-Task Dense Prediction via Mixture of Low-Rank Experts","abstract":"Previous multi-task dense prediction methods based on the Mixture of Experts (MoE) have received great performance but they neglect the importance of explicitly modeling the global relations among all tasks. In this paper, we present a novel decoder-focused method for multi-task dense prediction, called Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships, MLoRE adds a generic convolution path to the original MoE structure, where each task feature can go through this path for explicit parameter sharing. Furthermore, to control the parameters and computational cost brought by the increase in the number of experts, we take inspiration from LoRA and propose to leverage the low-rank format of a vanilla convolution in the expert network. Since the low-rank experts have fewer parameters and can be dynamically parameterized into the generic convolution, the parameters and computational cost do not change much with the increase of experts. Benefiting from this design, we increase the number of experts and its reception field to enlarge the representation capacity, facilitating multiple dense tasks learning in a unified network. Extensive experiments on the PASCAL-Context and NYUD-v2 benchmarks show that our MLoRE achieves superior performance compared to previous state-of-the-art methods on all metrics. Our code is available at https://github.com/YuqiYang213/MLoRE.","sentences":["Previous multi-task dense prediction methods based on the Mixture of Experts (MoE) have received great performance but they neglect the importance of explicitly modeling the global relations among all tasks.","In this paper, we present a novel decoder-focused method for multi-task dense prediction, called Mixture-of-Low-Rank-Experts (MLoRE).","To model the global task relationships, MLoRE adds a generic convolution path to the original MoE structure, where each task feature can go through this path for explicit parameter sharing.","Furthermore, to control the parameters and computational cost brought by the increase in the number of experts, we take inspiration from LoRA and propose to leverage the low-rank format of a vanilla convolution in the expert network.","Since the low-rank experts have fewer parameters and can be dynamically parameterized into the generic convolution, the parameters and computational cost do not change much with the increase of experts.","Benefiting from this design, we increase the number of experts and its reception field to enlarge the representation capacity, facilitating multiple dense tasks learning in a unified network.","Extensive experiments on the PASCAL-Context and NYUD-v2 benchmarks show that our MLoRE achieves superior performance compared to previous state-of-the-art methods on all metrics.","Our code is available at https://github.com/YuqiYang213/MLoRE."],"url":"http://arxiv.org/abs/2403.17749v1","category":"cs.CV"}
{"created":"2024-03-26 14:40:10","title":"UCxn: Typologically Informed Annotation of Constructions Atop Universal Dependencies","abstract":"The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages. However, the UD annotations do not tell the full story. Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements -- for example, interrogative sentences with special markers and/or word orders -- are not labeled holistically. We argue for (i) augmenting UD annotations with a 'UCxn' annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages. As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns. In addition to findings regarding these particular constructions, our study yields important insights on methodology for describing and identifying constructions in language-general and language-particular ways, and lays the foundation for future constructional enrichment of UD treebanks.","sentences":["The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages.","However, the UD annotations do not tell the full story.","Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements -- for example, interrogative sentences with special markers and/or word orders -- are not labeled holistically.","We argue for (i) augmenting UD annotations with a 'UCxn' annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages.","As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns.","In addition to findings regarding these particular constructions, our study yields important insights on methodology for describing and identifying constructions in language-general and language-particular ways, and lays the foundation for future constructional enrichment of UD treebanks."],"url":"http://arxiv.org/abs/2403.17748v1","category":"cs.CL"}
{"created":"2024-03-26 14:39:07","title":"Weighted Ehrhart theory via mixed Hodge modules on toric varieties","abstract":"We give a cohomological and geometrical interpretation for the weighted Ehrhart theory of a full-dimensional lattice polytope $P$, with Laurent polynomial weights of geometric origin. For this purpose, we calculate the motivic Chern and Hirzebruch characteristic classes of a mixed Hodge module complex $\\mathcal{M}$ whose underlying cohomology sheaves are constant on the $\\mathbb{T}$-orbits of the toric variety $X_P$ associated to $P$. Besides motivic coefficients, this also applies to the intersection cohomology Hodge module. We introduce a corresponding generalized Hodge $\\chi_y$-polynomial of the ample divisor $D_P$ on $X_P$. Motivic properties of these characteristic classes are used to express this Hodge polynomial in terms of a very general weighed lattice point counting and the corresponding weighted Ehrhart theory. We introduce, for such a mixed Hodge modules complex $\\mathcal{M}$ on $X$, an Ehrhart polynomial $E_{P,\\mathcal{M}}$ generalizing the Hodge polynomial of $\\mathcal{M}$ and satisfying a reciprocity formula and a purity formula fitting with the duality for mixed Hodge modules. This Ehrhart polynomial and its properties depend only on a Laurent polynomial weight function on the faces $Q$ of $P$. In the special case of the intersection cohomology mixed Hodge module, the weight function corresponds to Stanley's $g$-function of the polar polytope of $P$, hence it depends only on the combinatorics of $P$. In particular, we obtain a combinatorial formula for the intersection cohomology signature.","sentences":["We give a cohomological and geometrical interpretation for the weighted Ehrhart theory of a full-dimensional lattice polytope $P$, with Laurent polynomial weights of geometric origin.","For this purpose, we calculate the motivic Chern and Hirzebruch characteristic classes of a mixed Hodge module complex $\\mathcal{M}$ whose underlying cohomology sheaves are constant on the $\\mathbb{T}$-orbits of the toric variety $X_P$ associated to $P$. Besides motivic coefficients, this also applies to the intersection cohomology Hodge module.","We introduce a corresponding generalized Hodge $\\chi_y$-polynomial of the ample divisor $D_P$ on $X_P$. Motivic properties of these characteristic classes are used to express this Hodge polynomial in terms of a very general weighed lattice point counting and the corresponding weighted Ehrhart theory.","We introduce, for such a mixed Hodge modules complex $\\mathcal{M}$ on $X$, an Ehrhart polynomial $E_{P,\\mathcal{M}}$ generalizing the Hodge polynomial of $\\mathcal{M}$ and satisfying a reciprocity formula and a purity formula fitting with the duality for mixed Hodge modules.","This Ehrhart polynomial and its properties depend only on a Laurent polynomial weight function on the faces $Q$ of $P$. In the special case of the intersection cohomology mixed Hodge module, the weight function corresponds to Stanley's $g$-function of the polar polytope of $P$, hence it depends only on the combinatorics of $P$. In particular, we obtain a combinatorial formula for the intersection cohomology signature."],"url":"http://arxiv.org/abs/2403.17747v1","category":"math.AG"}
{"created":"2024-03-26 14:31:33","title":"Low-energy elastic (anti)neutrino-nucleon scattering in covariant baryon chiral perturbation theory","abstract":"The low-energy antineutrino- and neutrino-nucleon neutral current elastic scattering is studied within the framework of the relativistic SU(2) baryon chiral perturbation theory up to the order of $\\mathcal{O}(p^3)$. We have derived the model-independent hadronic amplitudes and extracted the form factors from them. It is found that differential cross sections ${{\\rm d} \\sigma}/{{\\rm d} Q^2}$ for the processes of (anti)neutrino-proton scattering are in good agreement with the existing MiniBooNE data in the $Q^2$ region $[0.13,0.20]$ GeV$^2$, where nuclear effects are expected to be negligible. For $Q^2\\leq 0.13$ GeV$^2$, large deviation is observed, which is mainly owing to the sizeable Pauli blocking effect. Comparisons with the simulation data produced by the NuWro and GIENE Mento Carlo events generators are also discussed. The chiral results obtained in this work can be utilized as inputs in various nuclear models to achieve the goal of precise determination of the strangeness axial vector form factor, in particular when the low-energy MicroBooNE data are available in the near future.","sentences":["The low-energy antineutrino- and neutrino-nucleon neutral current elastic scattering is studied within the framework of the relativistic SU(2) baryon chiral perturbation theory up to the order of $\\mathcal{O}(p^3)$. We have derived the model-independent hadronic amplitudes and extracted the form factors from them.","It is found that differential cross sections ${{\\rm d} \\sigma}/{{\\rm d} Q^2}$ for the processes of (anti)neutrino-proton scattering are in good agreement with the existing MiniBooNE data in the $Q^2$ region $[0.13,0.20]$ GeV$^2$, where nuclear effects are expected to be negligible.","For $Q^2\\leq 0.13$ GeV$^2$, large deviation is observed, which is mainly owing to the sizeable Pauli blocking effect.","Comparisons with the simulation data produced by the NuWro and GIENE Mento Carlo events generators are also discussed.","The chiral results obtained in this work can be utilized as inputs in various nuclear models to achieve the goal of precise determination of the strangeness axial vector form factor, in particular when the low-energy MicroBooNE data are available in the near future."],"url":"http://arxiv.org/abs/2403.17743v1","category":"hep-ph"}
{"created":"2024-03-26 14:30:23","title":"Using Stratified Sampling to Improve LIME Image Explanations","abstract":"We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach.","sentences":["We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling.","Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data.","We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past.","We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator.","Experiments show the efficacy of the proposed approach."],"url":"http://arxiv.org/abs/2403.17742v1","category":"cs.AI"}
{"created":"2024-03-26 14:29:51","title":"The low multipoles in the Pantheon+SH0ES data","abstract":"In previous work we have shown that the dipole in the low redshift supernovae of the Pantheon+SH0ES data does not agree with the one inferred from the velocity of the solar system as obtained from CMB data. We interpreted this as the presence of significant bulk velocities. In this paper we study the monopole, dipole and quadrupole in the Pantheon+SH0ES data. We find that in addition to the dipole also both, the monopole and the quadrupole are detected with high significance. They are of similar amplitudes as the bulk flow. While the monopole is only significant at very low redshift, the quadrupole even increases with redshift.","sentences":["In previous work we have shown that the dipole in the low redshift supernovae of the Pantheon+SH0ES data does not agree with the one inferred from the velocity of the solar system as obtained from CMB data.","We interpreted this as the presence of significant bulk velocities.","In this paper we study the monopole, dipole and quadrupole in the Pantheon+SH0ES data.","We find that in addition to the dipole also both, the monopole and the quadrupole are detected with high significance.","They are of similar amplitudes as the bulk flow.","While the monopole is only significant at very low redshift, the quadrupole even increases with redshift."],"url":"http://arxiv.org/abs/2403.17741v1","category":"astro-ph.CO"}
{"created":"2024-03-26 14:29:34","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction","abstract":"Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data. In the experiments, we evaluate our model under three cold-start settings on three real-world datasets. The experimental results show that HIRE outperforms other baselines by a large margin. Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model.","sentences":["Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied.","Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items.","However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task.","Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE).","HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network.","Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important interactions via the observed data.","In the experiments, we evaluate our model under three cold-start settings on three real-world datasets.","The experimental results show that HIRE outperforms other baselines by a large margin.","Furthermore, we visualize the inferred interactions of HIRE to confirm the contribution of our model."],"url":"http://arxiv.org/abs/2403.17740v1","category":"cs.IR"}
{"created":"2024-03-26 14:24:01","title":"Out-of-distribution Rumor Detection via Test-Time Adaptation","abstract":"Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training. However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity. This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations. To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and robustness when facing OOD problems. Extensive experiments conducted on two group datasets collected from real-world social platforms demonstrate that our framework outperforms the state-of-the-art methods in performance.","sentences":["Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge.","Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training.","However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity.","This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations.","To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD).","This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and robustness when facing OOD problems.","Extensive experiments conducted on two group datasets collected from real-world social platforms demonstrate that our framework outperforms the state-of-the-art methods in performance."],"url":"http://arxiv.org/abs/2403.17735v1","category":"cs.AI"}
{"created":"2024-03-26 14:21:49","title":"Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation scans using Linked Denoising Diffusion Probabilistic Models","abstract":"The rapid advancement of Artificial Intelligence (AI) in biomedical imaging and radiotherapy is hindered by the limited availability of large imaging data repositories. With recent research and improvements in denoising diffusion probabilistic models (DDPM), high quality synthetic medical scans are now possible. Despite this, there is currently no way of generating multiple related images, such as a corresponding ground truth which can be used to train models, so synthetic scans are often manually annotated before use. This research introduces a novel architecture that is able to generate multiple, related PET-CT-tumour mask pairs using paired networks and conditional encoders. Our approach includes innovative, time step-controlled mechanisms and a `noise-seeding' strategy to improve DDPM sampling consistency. While our model requires a modified perceptual loss function to ensure accurate feature alignment we show generation of clearly aligned synthetic images and improvement in segmentation accuracy with generated images.","sentences":["The rapid advancement of Artificial Intelligence (AI) in biomedical imaging and radiotherapy is hindered by the limited availability of large imaging data repositories.","With recent research and improvements in denoising diffusion probabilistic models (DDPM), high quality synthetic medical scans are now possible.","Despite this, there is currently no way of generating multiple related images, such as a corresponding ground truth which can be used to train models, so synthetic scans are often manually annotated before use.","This research introduces a novel architecture that is able to generate multiple, related PET-CT-tumour mask pairs using paired networks and conditional encoders.","Our approach includes innovative, time step-controlled mechanisms and a `noise-seeding' strategy to improve DDPM sampling consistency.","While our model requires a modified perceptual loss function to ensure accurate feature alignment we show generation of clearly aligned synthetic images and improvement in segmentation accuracy with generated images."],"url":"http://arxiv.org/abs/2403.17734v1","category":"eess.IV"}
{"created":"2024-03-26 14:20:16","title":"On a class of nonautonomous quasilinear systems with general time-gradually-degenerate damping","abstract":"In this paper, we study two systems with a time-variable coefficient and general time-gradually-degenerate damping. More explicitly, we construct the Riemann solutions to the time-variable coefficient Zeldovich approximation and time-variable coefficient pressureless gas systems both with general time-gradually- degenerate damping. Applying the method of similar variables and nonlinear viscosity, we obtain classical Riemann solutions and delta shock wave solutions.","sentences":["In this paper, we study two systems with a time-variable coefficient and general time-gradually-degenerate damping.","More explicitly, we construct the Riemann solutions to the time-variable coefficient Zeldovich approximation and time-variable coefficient pressureless gas systems both with general time-gradually- degenerate damping.","Applying the method of similar variables and nonlinear viscosity, we obtain classical Riemann solutions and delta shock wave solutions."],"url":"http://arxiv.org/abs/2403.17732v1","category":"math.AP"}
{"created":"2024-03-26 14:18:43","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention","abstract":"To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.Furthermore, a phase contrastive learning task is proposed to improve the anisotropy of contextual representations in EulerFormer. Our theoretical framework possesses a high degree of completeness and generality. It is more robust to semantic variations and possesses moresuperior theoretical properties in principle. Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach.","sentences":["To capture user preference, transformer models have been widely applied to model sequential user behavior data.","The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence.","Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations.","In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference.","However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling.","To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference.","The EulerFormer involves two key technical improvements.","First, it employs a new transformation function for efficiently transforming the sequence tokens into polar-form complex vectors using Euler's formula, enabling the unified modeling of both semantic and positional information in a complex rotation form.","Secondly, it develops a differential rotation mechanism, where the semantic rotation angles can be controlled by an adaptation function, enabling the adaptive integration of the semantic and positional information according to the semantic contexts.","Furthermore, a phase contrastive learning task is proposed to improve the anisotropy of contextual representations in EulerFormer.","Our theoretical framework possesses a high degree of completeness and generality.","It is more robust to semantic variations and possesses moresuperior theoretical properties in principle.","Extensive experiments conducted on four public datasets demonstrate the effectiveness and efficiency of our approach."],"url":"http://arxiv.org/abs/2403.17729v1","category":"cs.IR"}
{"created":"2024-03-26 14:14:30","title":"Tiny Models are the Computational Saver for Large Models","abstract":"This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding will help the research community in exploring new compression methods to address the escalating computational demands posed by rapidly evolving AI models. Our evaluation of this approach in ImageNet-1k classification demonstrates its potential to reduce the number of compute operations by up to 90%, with only negligible losses in performance, across various modern vision models. The code of this work will be available.","sentences":["This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively.","Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources.","Most existing early exit designs are implemented by attaching additional network branches to the model's backbone.","Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance.","Employing them as the first exit can remarkably enhance computational efficiency.","By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression.","This finding will help the research community in exploring new compression methods to address the escalating computational demands posed by rapidly evolving AI models.","Our evaluation of this approach in ImageNet-1k classification demonstrates its potential to reduce the number of compute operations by up to 90%, with only negligible losses in performance, across various modern vision models.","The code of this work will be available."],"url":"http://arxiv.org/abs/2403.17726v1","category":"cs.AI"}
{"created":"2024-03-26 14:12:36","title":"Magnonic inverse-design processor","abstract":"Artificial Intelligence (AI) technology has revolutionized our everyday lives and research. The concept of inverse design, which involves defining a functionality by a human and then using an algorithm to search for the device's design, opened new perspectives for information processing. A specialized AI-driven processor capable of solving an inverse problem in real-time offers a compelling alternative to the time and energy-intensive CMOS computations. Here, we report on a magnon-based processor that uses a complex reconfigurable medium to process data in the gigahertz range, catering to the demands of 5G and 6G telecommunication. Demonstrating its versatility, the processor solves inverse problems using two algorithms to realize RF notch filters and demultiplexers. The processor also exhibits potential for binary, reservoir, and neuromorphic computing paradigms.","sentences":["Artificial Intelligence (AI) technology has revolutionized our everyday lives and research.","The concept of inverse design, which involves defining a functionality by a human and then using an algorithm to search for the device's design, opened new perspectives for information processing.","A specialized AI-driven processor capable of solving an inverse problem in real-time offers a compelling alternative to the time and energy-intensive CMOS computations.","Here, we report on a magnon-based processor that uses a complex reconfigurable medium to process data in the gigahertz range, catering to the demands of 5G and 6G telecommunication.","Demonstrating its versatility, the processor solves inverse problems using two algorithms to realize RF notch filters and demultiplexers.","The processor also exhibits potential for binary, reservoir, and neuromorphic computing paradigms."],"url":"http://arxiv.org/abs/2403.17724v1","category":"physics.app-ph"}
{"created":"2024-03-26 14:00:45","title":"Testing beyond-Kerr spacetimes with GWTC-3","abstract":"The Kerr spacetime is a fundamental solution of general relativity (GR), describing the gravitational field around a rotating, uncharged black hole (BH). Kerr spacetime has been crucial in modern astrophysics and it serves as a foundation for the study of gravitational waves (GWs). Possible deviations in Kerr geometry may indicate deviations from GR predictions. In this work, we consider the Johannsen-Psaltis metric, which is a beyond-Kerr metric characterized by a single free parameter, and then we probe this theory framework using several GWs observations from the third Gravitational-wave Transient Catalog (GWTC-3). We find that, for most of the events analyzed, there are no significant deviations from the null hypothesis, i.e. the Kerr metric. Our main findings demonstrate alignment and certain enhancements when compared to previous estimates documented in the literature.","sentences":["The Kerr spacetime is a fundamental solution of general relativity (GR), describing the gravitational field around a rotating, uncharged black hole (BH).","Kerr spacetime has been crucial in modern astrophysics and it serves as a foundation for the study of gravitational waves (GWs).","Possible deviations in Kerr geometry may indicate deviations from GR predictions.","In this work, we consider the Johannsen-Psaltis metric, which is a beyond-Kerr metric characterized by a single free parameter, and then we probe this theory framework using several GWs observations from the third Gravitational-wave Transient Catalog (GWTC-3).","We find that, for most of the events analyzed, there are no significant deviations from the null hypothesis, i.e. the Kerr metric.","Our main findings demonstrate alignment and certain enhancements when compared to previous estimates documented in the literature."],"url":"http://arxiv.org/abs/2403.17718v1","category":"gr-qc"}
{"created":"2024-03-26 14:00:07","title":"A Comprehensive Study of Massive Compact Star Admitting Conformal Motion Under Bardeen Geometry","abstract":"This article primarily investigates the existence of the charged compact star under the conformal motion treatment within the context of f(Q) gravity. We have developed two models by implementing the power-law and linear form of conformal factor, enabling an in-depth comparison in our study. We have selected the MIT Bag model equation of state to describe the connection between pressure and energy density and matched the interior spherically symmetric space-time with the Bardeen space-time. In addition, the present research examines various physically valid characteristics of realistic stars, such as PSR J1614-2230, PSR J1903+327, Vela X-1, Cen X-3, and SMC X-1. We compare two constructed models by attributing the behavior of density, pressure, equilibrium conditions, and the adiabatic index. We have additionally included a brief analysis of the scenario involving Reissner-Nordstrom spacetime as an external geometry for the matching condition. In contrast to the Reissner-Nordstrom instance, the Bardeen model with the extra term in the asymptotic representations yields a more intriguing and viable result. The current analysis reveals that the resulting compact star solutions are physically acceptable and authentic when considering the presence of charge with conformal motion in f(Q) gravity.","sentences":["This article primarily investigates the existence of the charged compact star under the conformal motion treatment within the context of f(Q) gravity.","We have developed two models by implementing the power-law and linear form of conformal factor, enabling an in-depth comparison in our study.","We have selected the MIT Bag model equation of state to describe the connection between pressure and energy density and matched the interior spherically symmetric space-time with the Bardeen space-time.","In addition, the present research examines various physically valid characteristics of realistic stars, such as PSR J1614-2230, PSR J1903+327, Vela X-1, Cen X-3, and SMC X-1.","We compare two constructed models by attributing the behavior of density, pressure, equilibrium conditions, and the adiabatic index.","We have additionally included a brief analysis of the scenario involving Reissner-Nordstrom spacetime as an external geometry for the matching condition.","In contrast to the Reissner-Nordstrom instance, the Bardeen model with the extra term in the asymptotic representations yields a more intriguing and viable result.","The current analysis reveals that the resulting compact star solutions are physically acceptable and authentic when considering the presence of charge with conformal motion in f(Q) gravity."],"url":"http://arxiv.org/abs/2403.17716v1","category":"gr-qc"}
{"created":"2024-03-26 13:58:00","title":"Optimization-based Prompt Injection Attack to LLM-as-a-Judge","abstract":"LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.","sentences":["LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs).","Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment.","However, the robustness of these systems against prompt injection attacks remains an open question.","In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge.","Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations.","Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems.","Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack."],"url":"http://arxiv.org/abs/2403.17710v1","category":"cs.CR"}
{"created":"2024-03-26 13:54:52","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","abstract":"With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.   Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.   The dataset is available on website: https://dianvrlab.github.io/Panonut360/.","sentences":["With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge.","Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment.","At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos.","However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.   ","Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos.","The dataset provides details on the viewport and gaze attention locations of users.","Besides, we present some statistics samples extracted from the dataset.","For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution.","Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos.","That's why we name the dataset Panonut, a saliency weighting shaped like a donut.","Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.   ","The dataset is available on website: https://dianvrlab.github.io/Panonut360/."],"url":"http://arxiv.org/abs/2403.17708v1","category":"cs.CV"}
{"created":"2024-03-26 13:51:10","title":"Effect of light-assisted tunable interaction on the position response function of cold atoms","abstract":"The position response of a particle subjected to a perturbation is of general interest in physics. We study the modification of the position response function of an ensemble of cold atoms in a magneto-optical trap in the presence of tunable light-assisted interactions. We subject the cold atoms to an intense laser light tuned near the photoassociation resonance and observe the position response of the atoms subjected to a sudden displacement. Surprisingly, we observe that the entire cold atomic cloud undergoes collective oscillations. We use a generalised quantum Langevin approach to theoretically analyse the results of the experiments and find good agreement.","sentences":["The position response of a particle subjected to a perturbation is of general interest in physics.","We study the modification of the position response function of an ensemble of cold atoms in a magneto-optical trap in the presence of tunable light-assisted interactions.","We subject the cold atoms to an intense laser light tuned near the photoassociation resonance and observe the position response of the atoms subjected to a sudden displacement.","Surprisingly, we observe that the entire cold atomic cloud undergoes collective oscillations.","We use a generalised quantum Langevin approach to theoretically analyse the results of the experiments and find good agreement."],"url":"http://arxiv.org/abs/2403.17707v1","category":"physics.atom-ph"}
{"created":"2024-03-26 13:50:34","title":"Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement","abstract":"Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed \"Topic Refinement\". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by various models. Our comprehensive evaluation across three unique datasets has shown that our topic refinement approach significantly enhances the semantic coherence of topics.","sentences":["Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics.","Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data.","In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".","This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined.","By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically.","This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by various models.","Our comprehensive evaluation across three unique datasets has shown that our topic refinement approach significantly enhances the semantic coherence of topics."],"url":"http://arxiv.org/abs/2403.17706v1","category":"cs.CL"}
{"created":"2024-03-26 13:49:48","title":"Prioritize Team Actions: Multi-Agent Temporal Logic Task Planning with Ordering Constraints","abstract":"In this paper, we investigate the problem of linear temporal logic (LTL) path planning for multi-agent systems, introducing the new concept of \\emph{ordering constraints}. Specifically, we consider a generic objective function that is defined for the path of each individual agent. The primary objective is to find a global plan for the team of agents, ensuring they collectively meet the specified LTL requirements. Simultaneously, we aim to maintain a pre-determined order in the values of the objective function for each agent, which we refer to as the ordering constraints. This new requirement stems from scenarios like security-aware planning, where relative orders outweigh absolute values in importance. We present an efficient algorithm to solve this problem, supported by proofs of correctness that demonstrate the optimality of our solution. Additionally, we provide a case study in security-aware path planning to illustrate the practicality and effectiveness of our proposed approach.","sentences":["In this paper, we investigate the problem of linear temporal logic (LTL) path planning for multi-agent systems, introducing the new concept of \\emph{ordering constraints}.","Specifically, we consider a generic objective function that is defined for the path of each individual agent.","The primary objective is to find a global plan for the team of agents, ensuring they collectively meet the specified LTL requirements.","Simultaneously, we aim to maintain a pre-determined order in the values of the objective function for each agent, which we refer to as the ordering constraints.","This new requirement stems from scenarios like security-aware planning, where relative orders outweigh absolute values in importance.","We present an efficient algorithm to solve this problem, supported by proofs of correctness that demonstrate the optimality of our solution.","Additionally, we provide a case study in security-aware path planning to illustrate the practicality and effectiveness of our proposed approach."],"url":"http://arxiv.org/abs/2403.17704v1","category":"eess.SY"}
{"created":"2024-03-26 13:45:20","title":"Residual finiteness of fundamental $n$-quandles of links","abstract":"In this paper, we investigate residual finiteness and subquandle separability of quandles. The existence of these finiteness properties implies the solvability of the word problem and the generalised word problem for quandles. We prove that the fundamental $n$-quandle of any link in the 3-sphere is residually finite for each $n \\ge 2$. This supplements the recent result on residual finiteness of link quandles and the classification of links whose fundamental $n$-quandles are finite for some $n$. We also establish several general results on these finiteness properties and give many families of quandles admitting them.","sentences":["In this paper, we investigate residual finiteness and subquandle separability of quandles.","The existence of these finiteness properties implies the solvability of the word problem and the generalised word problem for quandles.","We prove that the fundamental $n$-quandle of any link in the 3-sphere is residually finite for each $n \\ge 2$.","This supplements the recent result on residual finiteness of link quandles and the classification of links whose fundamental $n$-quandles are finite for some $n$. We also establish several general results on these finiteness properties and give many families of quandles admitting them."],"url":"http://arxiv.org/abs/2403.17703v1","category":"math.GT"}
{"created":"2024-03-26 13:38:06","title":"MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation","abstract":"When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kernel function adheres to a consistent mean weight coefficient, harnessing the synergistic advantages of different kernels to formulate an innovative bias function. Subsequently, specific slopes are tailored for each kernel function, applying penalties at varying rates, to enhance the model's extrapolation capabilities. Finally, this bias is seamlessly incorporated as a penalty to the post-softmax scores. We present two distinct versions of our method: a parameter-free variant that requires no new learnable parameters, which enhances length extrapolation capabilities without compromising training efficiency, and a parameterized variant capable of integrating state-of-the-art techniques. Empirical evaluations across diverse datasets have demonstrated that both variants of our method achieve state-of-the-art performance, outperforming traditional parameter-free and parameterized approaches.","sentences":["When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes.","Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance.","These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge.","Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores.","Initially, the framework utilizes various kernel functions to construct multiple kernel functions.","Each kernel function adheres to a consistent mean weight coefficient, harnessing the synergistic advantages of different kernels to formulate an innovative bias function.","Subsequently, specific slopes are tailored for each kernel function, applying penalties at varying rates, to enhance the model's extrapolation capabilities.","Finally, this bias is seamlessly incorporated as a penalty to the post-softmax scores.","We present two distinct versions of our method: a parameter-free variant that requires no new learnable parameters, which enhances length extrapolation capabilities without compromising training efficiency, and a parameterized variant capable of integrating state-of-the-art techniques.","Empirical evaluations across diverse datasets have demonstrated that both variants of our method achieve state-of-the-art performance, outperforming traditional parameter-free and parameterized approaches."],"url":"http://arxiv.org/abs/2403.17698v1","category":"cs.LG"}
{"created":"2024-03-26 13:35:10","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","abstract":"We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the need for special tokens. We evaluate PlainMamba on a variety of visual recognition tasks including image classification, semantic segmentation, object detection, and instance segmentation. Our method achieves performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives. For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance. Code and models are available at https://github.com/ChenhongyiYang/PlainMamba","sentences":["We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition.","The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images.","In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information.","Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers.","The architecture is further simplified by removing the need for special tokens.","We evaluate PlainMamba on a variety of visual recognition tasks including image classification, semantic segmentation, object detection, and instance segmentation.","Our method achieves performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives.","For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance.","Code and models are available at https://github.com/ChenhongyiYang/PlainMamba"],"url":"http://arxiv.org/abs/2403.17695v1","category":"cs.CV"}
{"created":"2024-03-26 13:35:02","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","abstract":"In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. Our methodology is divided into two stages. Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks. Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation. Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience. Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment. We release code and model weights at https://github.com/scutzzj/AniPortrait","sentences":["In this study, we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image.","Our methodology is divided into two stages.","Initially, we extract 3D intermediate representations from audio and project them into a sequence of 2D facial landmarks.","Subsequently, we employ a robust diffusion model, coupled with a motion module, to convert the landmark sequence into photorealistic and temporally consistent portrait animation.","Experimental results demonstrate the superiority of AniPortrait in terms of facial naturalness, pose diversity, and visual quality, thereby offering an enhanced perceptual experience.","Moreover, our methodology exhibits considerable potential in terms of flexibility and controllability, which can be effectively applied in areas such as facial motion editing or face reenactment.","We release code and model weights at https://github.com/scutzzj/AniPortrait"],"url":"http://arxiv.org/abs/2403.17694v1","category":"cs.CV"}
{"created":"2024-03-26 13:34:21","title":"ExpressEdit: Video Editing with Natural Language and Sketching","abstract":"Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user's multimodal edit commands and supporting iterations on the editing commands. This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing.","sentences":["Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike.","When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging.","However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas.","To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas.","We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents.","Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame.","Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching.","The system implements the interpreted edits, which then the user can iterate on.","An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas.","The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user's multimodal edit commands and supporting iterations on the editing commands.","This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing."],"url":"http://arxiv.org/abs/2403.17693v1","category":"cs.HC"}
{"created":"2024-03-26 13:33:16","title":"Manifold-Guided Lyapunov Control with Diffusion Models","abstract":"This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models. The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability.","sentences":["This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models.","The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding.","To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions.","Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability."],"url":"http://arxiv.org/abs/2403.17692v1","category":"cs.CV"}
{"created":"2024-03-26 13:32:32","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes","abstract":"The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Sc\\`enes \\`a faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing \"data-driven bias\" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.","sentences":["The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains.","This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement.","To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models.","Copyright law distinguishes between original expressions and generic ones (Sc\\`enes \\`a faire), protecting the former and permitting reproduction of the latter.","However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works.","GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works.","We propose a data-driven approach to identify the genericity of works created by GenAI, employing \"data-driven bias\" to assess the genericity of expressive compositions.","This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset.","The potential implications of measuring expressive genericity for copyright law are profound.","Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals.","More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI."],"url":"http://arxiv.org/abs/2403.17691v1","category":"cs.CV"}
{"created":"2024-03-26 13:31:33","title":"Large Language Models Enhanced Collaborative Filtering","abstract":"Recent advancements in Large Language Models (LLMs) have attracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs). Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs. Al- though the extensive world knowledge embedded in LLMs generally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collaborative filtering information. Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs. In this paper, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which distils the world knowledge and reasoning capabilities of LLMs into collaborative filtering. We also explored a concise and efficient instruction-tuning method, which improves the recommendation capabilities of LLMs while preserving their general functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive experiments on three real-world datasets demonstrate that LLM-CF significantly enhances several backbone recommendation models and consistently outperforms competitive baselines, showcasing its effectiveness in distilling the world knowledge and reasoning capabilities of LLM into collaborative filtering.","sentences":["Recent advancements in Large Language Models (LLMs) have attracted considerable interest among researchers to leverage these models to enhance Recommender Systems (RSs).","Existing work predominantly utilizes LLMs to generate knowledge-rich texts or utilizes LLM-derived embeddings as features to improve RSs.","Al- though the extensive world knowledge embedded in LLMs generally benefits RSs, the application can only take limited number of users and items as inputs, without adequately exploiting collaborative filtering information.","Considering its crucial role in RSs, one key challenge in enhancing RSs with LLMs lies in providing better collaborative filtering information through LLMs.","In this paper, drawing inspiration from the in-context learning and chain of thought reasoning in LLMs, we propose the Large Language Models enhanced Collaborative Filtering (LLM-CF) framework, which distils the world knowledge and reasoning capabilities of LLMs into collaborative filtering.","We also explored a concise and efficient instruction-tuning method, which improves the recommendation capabilities of LLMs while preserving their general functionalities (e.g., not decreasing on the LLM benchmark).","Comprehensive experiments on three real-world datasets demonstrate that LLM-CF significantly enhances several backbone recommendation models and consistently outperforms competitive baselines, showcasing its effectiveness in distilling the world knowledge and reasoning capabilities of LLM into collaborative filtering."],"url":"http://arxiv.org/abs/2403.17688v1","category":"cs.IR"}
{"created":"2024-03-26 13:24:52","title":"Assessing the similarity of real matrices with arbitrary shape","abstract":"Assessing the similarity of matrices is valuable for analyzing the extent to which data sets exhibit common features in tasks such as data clustering, dimensionality reduction, pattern recognition, group comparison, and graph analysis. Methods proposed for comparing vectors, such as cosine similarity, can be readily generalized to matrices. However, this approach usually neglects the inherent two-dimensional structure of matrices. Here, we propose singular angle similarity (SAS), a measure for evaluating the structural similarity between two arbitrary, real matrices of the same shape based on singular value decomposition. After introducing the measure, we compare SAS with standard measures for matrix comparison and show that only SAS captures the two-dimensional structure of matrices. Further, we characterize the behavior of SAS in the presence of noise and as a function of matrix dimensionality. Finally, we apply SAS to two use cases: square non-symmetric matrices of probabilistic network connectivity, and non-square matrices representing neural brain activity. For synthetic data of network connectivity, SAS matches intuitive expectations and allows for a robust assessment of similarities and differences. For experimental data of brain activity, SAS captures differences in the structure of high-dimensional responses to different stimuli. We conclude that SAS is a suitable measure for quantifying the shared structure of matrices with arbitrary shape.","sentences":["Assessing the similarity of matrices is valuable for analyzing the extent to which data sets exhibit common features in tasks such as data clustering, dimensionality reduction, pattern recognition, group comparison, and graph analysis.","Methods proposed for comparing vectors, such as cosine similarity, can be readily generalized to matrices.","However, this approach usually neglects the inherent two-dimensional structure of matrices.","Here, we propose singular angle similarity (SAS), a measure for evaluating the structural similarity between two arbitrary, real matrices of the same shape based on singular value decomposition.","After introducing the measure, we compare SAS with standard measures for matrix comparison and show that only SAS captures the two-dimensional structure of matrices.","Further, we characterize the behavior of SAS in the presence of noise and as a function of matrix dimensionality.","Finally, we apply SAS to two use cases: square non-symmetric matrices of probabilistic network connectivity, and non-square matrices representing neural brain activity.","For synthetic data of network connectivity, SAS matches intuitive expectations and allows for a robust assessment of similarities and differences.","For experimental data of brain activity, SAS captures differences in the structure of high-dimensional responses to different stimuli.","We conclude that SAS is a suitable measure for quantifying the shared structure of matrices with arbitrary shape."],"url":"http://arxiv.org/abs/2403.17687v1","category":"q-bio.NC"}
{"created":"2024-03-26 13:14:18","title":"Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI","abstract":"This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt. Our approach ranked first in the final test with a score of 0.627.","sentences":["This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment.","The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures.","The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem.","In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem.","To clarify, our approach contains two main blocks: (1)XLM-R\\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt.","Our approach ranked first in the final test with a score of 0.627."],"url":"http://arxiv.org/abs/2403.17683v1","category":"cs.AI"}
{"created":"2024-03-26 13:12:57","title":"Ueda foliation problem for complex tori","abstract":"We consider an embedded general complex torus $C_n$ into a complex manifold $M_{n+d}$ with a unitary flat normal bundle $N_C$. We show the existence of (non-singular) holomorphic foliation in a neighborhood of $C$ in $M$ having $C$ as leaf under some conditions.","sentences":["We consider an embedded general complex torus $C_n$ into a complex manifold $M_{n+d}$ with a unitary flat normal bundle $N_C$. We show the existence of (non-singular) holomorphic foliation in a neighborhood of $C$ in $M$ having $C$ as leaf under some conditions."],"url":"http://arxiv.org/abs/2403.17682v1","category":"math.CV"}
{"created":"2024-03-26 13:12:35","title":"A Wall Crossing Formula for Motivic Enumerative Invariants","abstract":"We prove an analog of the wall crossing formula for Welschinger invariants relating the difference of signed curve counting of real curves passing through configurations that differ by a pair of complex conjugated points, and a correspondence Welschinger invariant of the blow up.   We prove this analogue for the motivic count of rational curves of fixed degree passing through a generic configuration of points, counted with a motivic multiplicity in the Grothendieck-Witt ring of a base field, extending the notions in the correspondence theorem between motivic invariants for $k$-rational point conditions and tropical curves.   We use this formula to compute the degree 4 motivic enumerative invariants of the projective plane counting curves passing through configurations of points defined over quadratic extensions of a base field.","sentences":["We prove an analog of the wall crossing formula for Welschinger invariants relating the difference of signed curve counting of real curves passing through configurations that differ by a pair of complex conjugated points, and a correspondence Welschinger invariant of the blow up.   ","We prove this analogue for the motivic count of rational curves of fixed degree passing through a generic configuration of points, counted with a motivic multiplicity in the Grothendieck-Witt ring of a base field, extending the notions in the correspondence theorem between motivic invariants for $k$-rational point conditions and tropical curves.   ","We use this formula to compute the degree 4 motivic enumerative invariants of the projective plane counting curves passing through configurations of points defined over quadratic extensions of a base field."],"url":"http://arxiv.org/abs/2403.17681v1","category":"math.AG"}
{"created":"2024-03-26 13:11:21","title":"Echoes in genus three of Teichm\u00fcller curves in genus two","abstract":"We classify the Teichm\\\"uller curves in the moduli space of genus three Riemann surfaces $\\mathcal M_3$ that are obtained by a covering construction from a primitive Teichm\\\"uller curve in $\\mathcal M_2$. We describe the action on homology modulo two of the affine groups of translation surfaces generating these primitive curves. We also classify the $\\mathrm{SL}_2(\\mathbb Z)$-orbits of square-tiled surfaces in $\\mathcal H(2, 2)$ that are a cover of a genus two one.","sentences":["We classify the Teichm\\\"uller curves in the moduli space of genus three Riemann surfaces $\\mathcal M_3$ that are obtained by a covering construction from a primitive Teichm\\\"uller curve in $\\mathcal M_2$. We describe the action on homology modulo two of the affine groups of translation surfaces generating these primitive curves.","We also classify the $\\mathrm{SL}_2(\\mathbb Z)$-orbits of square-tiled surfaces in $\\mathcal H(2, 2)$ that are a cover of a genus two one."],"url":"http://arxiv.org/abs/2403.17680v1","category":"math.GT"}
{"created":"2024-03-26 13:05:02","title":"Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention","abstract":"Deep learning methods have traditionally been difficult to apply to compression of hyperspectral images onboard of spacecrafts, due to the large computational complexity needed to achieve adequate representational power, as well as the lack of suitable datasets for training and testing. In this paper, we depart from the traditional autoencoder approach and we design a predictive neural network, called LineRWKV, that works recursively line-by-line to limit memory consumption. In order to achieve that, we adopt a novel hybrid attentive-recursive operation that combines the representational advantages of Transformers with the linear complexity and recursive implementation of recurrent neural networks. The compression algorithm performs prediction of each pixel using LineRWKV, followed by entropy coding of the residual. Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV is the first deep-learning method to outperform CCSDS-123.0-B-2 at lossless and near-lossless compression. Promising throughput results are also evaluated on a 7W embedded system.","sentences":["Deep learning methods have traditionally been difficult to apply to compression of hyperspectral images onboard of spacecrafts, due to the large computational complexity needed to achieve adequate representational power, as well as the lack of suitable datasets for training and testing.","In this paper, we depart from the traditional autoencoder approach and we design a predictive neural network, called LineRWKV, that works recursively line-by-line to limit memory consumption.","In order to achieve that, we adopt a novel hybrid attentive-recursive operation that combines the representational advantages of Transformers with the linear complexity and recursive implementation of recurrent neural networks.","The compression algorithm performs prediction of each pixel using LineRWKV, followed by entropy coding of the residual.","Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV is the first deep-learning method to outperform CCSDS-123.0-B-2 at lossless and near-lossless compression.","Promising throughput results are also evaluated on a 7W embedded system."],"url":"http://arxiv.org/abs/2403.17677v1","category":"eess.IV"}
{"created":"2024-03-26 13:04:00","title":"Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices","abstract":"Reservoir computing is a recurrent neural network that has been applied across various domains in machine learning. The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir. Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge. Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation. The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material. We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation. Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems. Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography. As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption.","sentences":["Reservoir computing is a recurrent neural network that has been applied across various domains in machine learning.","The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir.","Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge.","Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation.","The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material.","We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation.","Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems.","Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography.","As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption."],"url":"http://arxiv.org/abs/2403.17676v1","category":"physics.app-ph"}
{"created":"2024-03-26 13:02:46","title":"Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games","abstract":"Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, achieving the complementary performance in the cybersecurity games.","sentences":["Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations.","In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations.","In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space.","Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking.","Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, achieving the complementary performance in the cybersecurity games."],"url":"http://arxiv.org/abs/2403.17674v1","category":"cs.CR"}
{"created":"2024-03-26 13:02:33","title":"Revealing the Microscopic Mechanism of Elementary Vortex Pinning in Superconductors","abstract":"Vortex pinning is a crucial factor that determines the critical current of practical superconductors. However, the understanding of its underlying mechanism has long been phenomenological without a clear microscopic description. Here using high-resolution scanning tunneling microscopy, we studied single vortex pinning induced by point defect in layered FeSe-based superconductors. We found the defect-vortex interaction drives low-energy vortex bound states away from EF, resulting a mini gap which effectively lowered the energy of vortex and caused the pinning. By measuring the local density-of-states, we directly obtained the elementary pinning energy and estimated the pinning force through the spatial gradient of pinning energy. The results align with the bulk critical current measurement. We further show that a general microscopic quantum model with considering defect-vortex interaction can well capture our observation. It indicates the local pairing near pinned vortex core is actually enhanced, which is beyond the traditional understanding that non-superconducting regions pin vortices. Our study thus revealed a general microscopic mechanism of vortex pinning in superconductors.","sentences":["Vortex pinning is a crucial factor that determines the critical current of practical superconductors.","However, the understanding of its underlying mechanism has long been phenomenological without a clear microscopic description.","Here using high-resolution scanning tunneling microscopy, we studied single vortex pinning induced by point defect in layered FeSe-based superconductors.","We found the defect-vortex interaction drives low-energy vortex bound states away from EF, resulting a mini gap which effectively lowered the energy of vortex and caused the pinning.","By measuring the local density-of-states, we directly obtained the elementary pinning energy and estimated the pinning force through the spatial gradient of pinning energy.","The results align with the bulk critical current measurement.","We further show that a general microscopic quantum model with considering defect-vortex interaction can well capture our observation.","It indicates the local pairing near pinned vortex core is actually enhanced, which is beyond the traditional understanding that non-superconducting regions pin vortices.","Our study thus revealed a general microscopic mechanism of vortex pinning in superconductors."],"url":"http://arxiv.org/abs/2403.17671v1","category":"cond-mat.supr-con"}
{"created":"2024-03-26 13:01:26","title":"A family of Chatterjee's correlation coefficients and their properties","abstract":"Quantifying the strength of functional dependence between random scalars $X$ and $Y$ is an important statistical problem. While many existing correlation coefficients excel in identifying linear or monotone functional dependence, they fall short in capturing general non-monotone functional relationships. In response, we propose a family of correlation coefficients $\\xi^{(h,F)}_n$, characterized by a continuous bivariate function $h$ and a cdf function $F$. By offering a range of selections for $h$ and $F$, $\\xi^{(h,F)}_n$ encompasses a diverse class of novel correlation coefficients, while also incorporates the Chatterjee's correlation coefficient (Chatterjee, 2021) as a special case. We prove that $\\xi^{(h,F)}_n$ converges almost surely to a deterministic limit $\\xi^{(h,F)}$ as sample size $n$ approaches infinity. In addition, under appropriate conditions imposed on $h$ and $F$, the limit $\\xi^{(h,F)}$ satisfies the three appealing properties: (P1). it belongs to the range of $[0,1]$; (P2). it equals 1 if and only if $Y$ is a measurable function of $X$; and (P3). it equals 0 if and only if $Y$ is independent of $X$. As amplified by our numerical experiments, our proposals provide practitioners with a variety of options to choose the most suitable correlation coefficient tailored to their specific practical needs.","sentences":["Quantifying the strength of functional dependence between random scalars $X$ and $Y$ is an important statistical problem.","While many existing correlation coefficients excel in identifying linear or monotone functional dependence, they fall short in capturing general non-monotone functional relationships.","In response, we propose a family of correlation coefficients $\\xi^{(h,F)}_n$, characterized by a continuous bivariate function $h$ and a cdf function $F$. By offering a range of selections for $h$ and $F$, $\\xi^{(h,F)}_n$ encompasses a diverse class of novel correlation coefficients, while also incorporates the Chatterjee's correlation coefficient (Chatterjee, 2021) as a special case.","We prove that $\\xi^{(h,F)}_n$ converges almost surely to a deterministic limit $\\xi^{(h,F)}$ as sample size $n$ approaches infinity.","In addition, under appropriate conditions imposed on $h$ and $F$, the limit $\\xi^{(h,F)}$ satisfies the three appealing properties: (P1).","it belongs to the range of $[0,1]$; (P2).","it equals 1 if and only if $Y$ is a measurable function of $X$; and (P3).","it equals 0 if and only if $Y$ is independent of $X$. As amplified by our numerical experiments, our proposals provide practitioners with a variety of options to choose the most suitable correlation coefficient tailored to their specific practical needs."],"url":"http://arxiv.org/abs/2403.17670v1","category":"stat.ME"}
{"created":"2024-03-26 12:57:05","title":"Learning Goal-Directed Object Pushing in Cluttered Scenes with Location-Based Attention","abstract":"Non-prehensile planar pushing is a challenging task due to its underactuated nature with hybrid-dynamics, where a robot needs to reason about an object's long-term behaviour and contact-switching, while being robust to contact uncertainty. The presence of clutter in the environment further complicates this task, introducing the need to include more sophisticated spatial analysis to avoid collisions. Building upon prior work on reinforcement learning (RL) with multimodal categorical exploration for planar pushing, in this paper we incorporate location-based attention to enable robust navigation through clutter. Unlike previous RL literature addressing this obstacle avoidance pushing task, our framework requires no predefined global paths and considers the target orientation of the manipulated object. Our results demonstrate that the learned policies successfully navigate through a wide range of complex obstacle configurations, including dynamic obstacles, with smooth motions, achieving the desired target object pose. We also validate the transferability of the learned policies to robotic hardware using the KUKA iiwa robot arm.","sentences":["Non-prehensile planar pushing is a challenging task due to its underactuated nature with hybrid-dynamics, where a robot needs to reason about an object's long-term behaviour and contact-switching, while being robust to contact uncertainty.","The presence of clutter in the environment further complicates this task, introducing the need to include more sophisticated spatial analysis to avoid collisions.","Building upon prior work on reinforcement learning (RL) with multimodal categorical exploration for planar pushing, in this paper we incorporate location-based attention to enable robust navigation through clutter.","Unlike previous RL literature addressing this obstacle avoidance pushing task, our framework requires no predefined global paths and considers the target orientation of the manipulated object.","Our results demonstrate that the learned policies successfully navigate through a wide range of complex obstacle configurations, including dynamic obstacles, with smooth motions, achieving the desired target object pose.","We also validate the transferability of the learned policies to robotic hardware using the KUKA iiwa robot arm."],"url":"http://arxiv.org/abs/2403.17667v1","category":"cs.RO"}
{"created":"2024-03-26 12:55:12","title":"When View- and Conflict-Robustness Coincide for Multiversion Concurrency Control","abstract":"A DBMS allows trading consistency for efficiency through the allocation of isolation levels that are strictly weaker than serializability. The robustness problem asks whether, for a given set of transactions and a given allocation of isolation levels, every possible interleaved execution of those transactions that is allowed under the provided allocation, is always safe. In the literature, safe is interpreted as conflict-serializable (to which we refer here as conflict-robustness). In this paper, we study the view-robustness problem, interpreting safe as view-serializable. View-serializability is a more permissive notion that allows for a greater number of schedules to be serializable and aligns more closely with the intuitive understanding of what it means for a database to be consistent. However, view-serializability is more complex to analyze (e.g., conflict-serializability can be decided in polynomial time whereas deciding view-serializability is NP-complete). While conflict-robustness implies view-robustness, the converse does not hold in general. In this paper, we provide a sufficient condition for isolation levels guaranteeing that conflict- and view-robustness coincide and show that this condition is satisfied by the isolation levels occurring in Postgres and Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot isolation (SSI). It hence follows that for these systems, widening from conflict- to view-serializability does not allow for more sets of transactions to become robust. Interestingly, the complexity of deciding serializability within these isolation levels is still quite different. Indeed, deciding conflict-serializability for schedules allowed under RC and SI remains in polynomial time, while we show that deciding view-serializability within these isolation levels remains NP-complete.","sentences":["A DBMS allows trading consistency for efficiency through the allocation of isolation levels that are strictly weaker than serializability.","The robustness problem asks whether, for a given set of transactions and a given allocation of isolation levels, every possible interleaved execution of those transactions that is allowed under the provided allocation, is always safe.","In the literature, safe is interpreted as conflict-serializable (to which we refer here as conflict-robustness).","In this paper, we study the view-robustness problem, interpreting safe as view-serializable.","View-serializability is a more permissive notion that allows for a greater number of schedules to be serializable and aligns more closely with the intuitive understanding of what it means for a database to be consistent.","However, view-serializability is more complex to analyze (e.g., conflict-serializability can be decided in polynomial time whereas deciding view-serializability is NP-complete).","While conflict-robustness implies view-robustness, the converse does not hold in general.","In this paper, we provide a sufficient condition for isolation levels guaranteeing that conflict- and view-robustness coincide and show that this condition is satisfied by the isolation levels occurring in Postgres and Oracle: read committed (RC), snapshot isolation (SI) and serializable snapshot isolation (SSI).","It hence follows that for these systems, widening from conflict- to view-serializability does not allow for more sets of transactions to become robust.","Interestingly, the complexity of deciding serializability within these isolation levels is still quite different.","Indeed, deciding conflict-serializability for schedules allowed under RC and SI remains in polynomial time, while we show that deciding view-serializability within these isolation levels remains NP-complete."],"url":"http://arxiv.org/abs/2403.17665v1","category":"cs.DB"}
{"created":"2024-03-26 12:53:10","title":"DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with Space-sensitive Customization and Semantic Preservation","abstract":"Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph. In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference. To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE. For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM). In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC). This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background. We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model. Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing.","sentences":["Facial Appearance Editing (FAE) aims to modify physical attributes, such as pose, expression and lighting, of human facial images while preserving attributes like identity and background, showing great importance in photograph.","In spite of the great progress in this area, current researches generally meet three challenges: low generation fidelity, poor attribute preservation, and inefficient inference.","To overcome above challenges, this paper presents DiffFAE, a one-stage and highly-efficient diffusion-based framework tailored for high-fidelity FAE.","For high-fidelity query attributes transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures the fidelity and generalization ability by utilizing rendering texture derived from 3D Morphable Model (3DMM).","In order to preserve source attributes, we introduce the Region-responsive Semantic Composition (RSC).","This module is guided to learn decoupled source-regarding features, thereby better preserving the identity and alleviating artifacts from non-facial attributes such as hair, clothes, and background.","We further introduce a consistency regularization for our pipeline to enhance editing controllability by leveraging prior knowledge in the attention matrices of diffusion model.","Extensive experiments demonstrate the superiority of DiffFAE over existing methods, achieving state-of-the-art performance in facial appearance editing."],"url":"http://arxiv.org/abs/2403.17664v1","category":"cs.CV"}
{"created":"2024-03-26 12:48:51","title":"Rapid non-destructive inspection of sub-surface defects in 3D printed alumina through 30 layers with 7 \u03bcm depth resolution","abstract":"The use of additive manufacturing (AM) processes for industrial fabrication has grown rapidly over the last ten years. The most well-known AM technologies are fused deposition modelling and stereolithography techniques. One particular industry where 3D printing is advantageous over traditional fabrication techniques is within ceramic components due to its flexibility. To establish a new and improved level of print quality and reduce resource consumption in the 3D printing ceramics industry, there is a need for fast integrated, sub-surface and non-destructive inspection (NDI) with high resolution. Several techniques have already been developed for high-resolution NDI, such as X-ray computed tomography (XCT), but none of them are both fast, integrable, and non-destructive while allowing deep penetration with high resolution. \\newline In this study, we demonstrate sub-surface monitoring of 3D printed alumina parts to a depth of $\\sim$0.7 mm in images of 400$\\times$2048 pixels with a lateral resolution of 30~$\\mu$m and depth (or axial) resolution of 7$~\\mu$m . The results were achieved using mid-infrared optical coherence tomography (MIR OCT) based on a MIR supercontinuum laser with a 4$~\\mu$m center wavelength. We find that it is possible to detect individual printed ceramic layers and track predefined defects through all four processing steps: green, preconditioned, debinded, and sintered. Our results also demonstrate how a defect in the green phase could affect the final product. Based on the understanding of how defects develop in maturing printed parts, we pave the way for NDI integration in AM, which can be combined with artificial intelligence and machine learning algorithms for automatic defect classification in volume production of a new standard of high quality ceramic components.","sentences":["The use of additive manufacturing (AM) processes for industrial fabrication has grown rapidly over the last ten years.","The most well-known AM technologies are fused deposition modelling and stereolithography techniques.","One particular industry where 3D printing is advantageous over traditional fabrication techniques is within ceramic components due to its flexibility.","To establish a new and improved level of print quality and reduce resource consumption in the 3D printing ceramics industry, there is a need for fast integrated, sub-surface and non-destructive inspection (NDI) with high resolution.","Several techniques have already been developed for high-resolution NDI, such as X-ray computed tomography (XCT), but none of them are both fast, integrable, and non-destructive while allowing deep penetration with high resolution.","\\newline In this study, we demonstrate sub-surface monitoring of 3D printed alumina parts to a depth of $\\sim$0.7 mm in images of 400$\\times$2048 pixels with a lateral resolution of 30~$\\mu$m and depth (or axial) resolution of 7$~\\mu$m .","The results were achieved using mid-infrared optical coherence tomography (MIR OCT) based on a MIR supercontinuum laser with a 4$~\\mu$m center wavelength.","We find that it is possible to detect individual printed ceramic layers and track predefined defects through all four processing steps: green, preconditioned, debinded, and sintered.","Our results also demonstrate how a defect in the green phase could affect the final product.","Based on the understanding of how defects develop in maturing printed parts, we pave the way for NDI integration in AM, which can be combined with artificial intelligence and machine learning algorithms for automatic defect classification in volume production of a new standard of high quality ceramic components."],"url":"http://arxiv.org/abs/2403.17662v1","category":"physics.optics"}
{"created":"2024-03-26 12:47:39","title":"Language Models for Text Classification: Is In-Context Learning Enough?","abstract":"Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models. We also analyse the results by prompt, classification type, domain, and number of labels. In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification.","sentences":["Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings.","An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data.","This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances.","However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models.","In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems.","In particular, we compare zero- and few-shot approaches of large language models to fine-tuning smaller language models.","We also analyse the results by prompt, classification type, domain, and number of labels.","In general, the results show how fine-tuning smaller and more efficient language models can still outperform few-shot approaches of larger language models, which have room for improvement when it comes to text classification."],"url":"http://arxiv.org/abs/2403.17661v1","category":"cs.CL"}
{"created":"2024-03-26 12:47:04","title":"CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations","abstract":"Optimal Power Flow (OPF) refers to a wide range of related optimization problems with the goal of operating power systems efficiently and securely. In the simplest setting, OPF determines how much power to generate in order to minimize costs while meeting demand for power and satisfying physical and operational constraints. In even the simplest case, power grid operators use approximations of the AC-OPF problem because solving the exact problem is prohibitively slow with state-of-the-art solvers. These approximations sacrifice accuracy and operational feasibility in favor of speed. This trade-off leads to costly \"uplift payments\" and increased carbon emissions, especially for large power grids. In the present work, we train a deep learning system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF cost) without compromising speed (running in as little as 33--65 ms). Importantly, CANOS scales to realistic grid sizes with promising empirical results on grids containing as many as 10,000 buses. Finally, because CANOS is a Graph Neural Network, it is robust to changes in topology. We show that CANOS is accurate across N-1 topological perturbations of a base grid typically used in security-constrained analysis. This paves the way for more efficient optimization of more complex OPF problems which alter grid connectivity such as unit commitment, topology optimization and security-constrained OPF.","sentences":["Optimal Power Flow (OPF) refers to a wide range of related optimization problems with the goal of operating power systems efficiently and securely.","In the simplest setting, OPF determines how much power to generate in order to minimize costs while meeting demand for power and satisfying physical and operational constraints.","In even the simplest case, power grid operators use approximations of the AC-OPF problem because solving the exact problem is prohibitively slow with state-of-the-art solvers.","These approximations sacrifice accuracy and operational feasibility in favor of speed.","This trade-off leads to costly \"uplift payments\" and increased carbon emissions, especially for large power grids.","In the present work, we train a deep learning system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF cost) without compromising speed (running in as little as 33--65 ms).","Importantly, CANOS scales to realistic grid sizes with promising empirical results on grids containing as many as 10,000 buses.","Finally, because CANOS is a Graph Neural Network, it is robust to changes in topology.","We show that CANOS is accurate across N-1 topological perturbations of a base grid typically used in security-constrained analysis.","This paves the way for more efficient optimization of more complex OPF problems which alter grid connectivity such as unit commitment, topology optimization and security-constrained OPF."],"url":"http://arxiv.org/abs/2403.17660v1","category":"cs.LG"}
{"created":"2024-03-26 12:45:55","title":"Tracking the motion of a shock along a channel in the low solar corona","abstract":"Shock waves are excited by coronal mass ejections (CMEs) and large-scale extreme-ultraviolet (EUV) wave fronts and can result in low-frequency radio emission under certain coronal conditions. In this work, we investigate a moving source of low-frequency radio emission as a CME and an associated EUV wave front move along a channel of a lower density, magnetic field, and Alfv\\'en speed in the solar corona. Observations from the Atmospheric Imaging Assembly on board the Solar Dynamics Observatory, the Nan\\c{c}ay Radio Heliograph (NRH), and the Irish Low Frequency Array(I-LOFAR) were analysed. Differential emission measure maps were generated to determine densities and Alfv\\'en maps, and the kinematics of the EUV wave front was tracked using CorPITA. The radio sources' positions and velocity were calculated from NRH images and I-LOFAR dynamic spectra. The EUV wave expanded radially with a uniform velocity of $\\sim$ 500 km s$^{-1}$. However, the radio source was observed to be deflected and appeared to move along a channel of a lower Alfv\\'en speed, abruptly slowing from 1700 km s$^{-1}$ to 250 km s$^{-1}$ as it entered a quiet-Sun region. A shock wave with an apparent radial velocity of > 420 km s$^{-1}$ was determined from the drift rate of the associated Type II radio burst. The apparent motion of the radio source may have resulted from a wave front moving along a coronal wave guide or by different points along the wave front emitting at locations with favourable conditions for shock formation.","sentences":["Shock waves are excited by coronal mass ejections (CMEs) and large-scale extreme-ultraviolet (EUV) wave fronts and can result in low-frequency radio emission under certain coronal conditions.","In this work, we investigate a moving source of low-frequency radio emission as a CME and an associated EUV wave front move along a channel of a lower density, magnetic field, and Alfv\\'en speed in the solar corona.","Observations from the Atmospheric Imaging Assembly on board the Solar Dynamics Observatory, the Nan\\c{c}ay Radio Heliograph (NRH), and the Irish Low Frequency Array(I-LOFAR) were analysed.","Differential emission measure maps were generated to determine densities and Alfv\\'en maps, and the kinematics of the EUV wave front was tracked using CorPITA.","The radio sources' positions and velocity were calculated from NRH images and I-LOFAR dynamic spectra.","The EUV wave expanded radially with a uniform velocity of $\\sim$ 500 km s$^{-1}$.","However, the radio source was observed to be deflected and appeared to move along a channel of a lower Alfv\\'en speed, abruptly slowing from 1700 km s$^{-1}$ to 250 km s$^{-1}$ as it entered a quiet-Sun region.","A shock wave with an apparent radial velocity of > 420 km s$^{-1}$ was determined from the drift rate of the associated Type II radio burst.","The apparent motion of the radio source may have resulted from a wave front moving along a coronal wave guide or by different points along the wave front emitting at locations with favourable conditions for shock formation."],"url":"http://arxiv.org/abs/2403.17659v1","category":"astro-ph.SR"}
{"created":"2024-03-26 12:39:02","title":"SGHormer: An Energy-Saving Graph Transformer Driven by Spikes","abstract":"Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHormer explicitly capture global structure information and recover the expressive power of spiking embeddings, respectively. In experiments, SGHormer achieves comparable performances to other full-precision GTs with extremely low computational energy consumption. The results show that SGHomer makes a remarkable progress in the field of low-energy GTs.","sentences":["Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks.","However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead.","The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data.","Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework.","To this end, we propose a new spiking-based graph transformer (SGHormer).","It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs.","The spiking graph self-attention and spiking rectify blocks in SGHormer explicitly capture global structure information and recover the expressive power of spiking embeddings, respectively.","In experiments, SGHormer achieves comparable performances to other full-precision GTs with extremely low computational energy consumption.","The results show that SGHomer makes a remarkable progress in the field of low-energy GTs."],"url":"http://arxiv.org/abs/2403.17656v1","category":"cs.NE"}
{"created":"2024-03-26 12:36:11","title":"An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation","abstract":"We present an extension-based approach for computing and verifying preferences in an abstract argumentation system. Although numerous argumentation semantics have been developed previously for identifying acceptable sets of arguments from an argumentation framework, there is a lack of justification behind their acceptability based on implicit argument preferences. Preference-based argumentation frameworks allow one to determine what arguments are justified given a set of preferences. Our research considers the inverse of the standard reasoning problem, i.e., given an abstract argumentation framework and a set of justified arguments, we compute what the possible preferences over arguments are. Furthermore, there is a need to verify (i.e., assess) that the computed preferences would lead to the acceptable sets of arguments. This paper presents a novel approach and algorithm for exhaustively computing and enumerating all possible sets of preferences (restricted to three identified cases) for a conflict-free set of arguments in an abstract argumentation framework. We prove the soundness, completeness and termination of the algorithm. The research establishes that preferences are determined using an extension-based approach after the evaluation phase (acceptability of arguments) rather than stated beforehand. In this work, we focus our research study on grounded, preferred and stable semantics. We show that the complexity of computing sets of preferences is exponential in the number of arguments, and thus, describe an approximate approach and algorithm to compute the preferences. Furthermore, we present novel algorithms for verifying (i.e., assessing) the computed preferences. We provide details of the implementation of the algorithms (source code has been made available), various experiments performed to evaluate the algorithms and the analysis of the results.","sentences":["We present an extension-based approach for computing and verifying preferences in an abstract argumentation system.","Although numerous argumentation semantics have been developed previously for identifying acceptable sets of arguments from an argumentation framework, there is a lack of justification behind their acceptability based on implicit argument preferences.","Preference-based argumentation frameworks allow one to determine what arguments are justified given a set of preferences.","Our research considers the inverse of the standard reasoning problem, i.e., given an abstract argumentation framework and a set of justified arguments, we compute what the possible preferences over arguments are.","Furthermore, there is a need to verify (i.e., assess) that the computed preferences would lead to the acceptable sets of arguments.","This paper presents a novel approach and algorithm for exhaustively computing and enumerating all possible sets of preferences (restricted to three identified cases) for a conflict-free set of arguments in an abstract argumentation framework.","We prove the soundness, completeness and termination of the algorithm.","The research establishes that preferences are determined using an extension-based approach after the evaluation phase (acceptability of arguments) rather than stated beforehand.","In this work, we focus our research study on grounded, preferred and stable semantics.","We show that the complexity of computing sets of preferences is exponential in the number of arguments, and thus, describe an approximate approach and algorithm to compute the preferences.","Furthermore, we present novel algorithms for verifying (i.e., assessing) the computed preferences.","We provide details of the implementation of the algorithms (source code has been made available), various experiments performed to evaluate the algorithms and the analysis of the results."],"url":"http://arxiv.org/abs/2403.17653v1","category":"cs.AI"}
{"created":"2024-03-26 12:36:00","title":"Leveraging A Variety of Anchors in Cellular Network for Ubiquitous Sensing","abstract":"Integrated sensing and communication (ISAC) has recently attracted tremendous attention from both academia and industry, being envisioned as a key part of the standards for the sixth-generation (6G) cellular network. A key challenge of 6G-oriented ISAC lies in how to perform ubiquitous sensing based on the communication signals and devices. Previous works have made great progresses on studying the signal waveform design that leads to optimal communication-sensing performance tradeoff. In this article, we aim to focus on issues arising from the exploitation of the communication devices for sensing in 6G network. Particularly, we will discuss about how to leverage various nodes available in the cellular network as anchors to perform ubiquitous sensing. On one hand, the base stations (BSs) will be the most important anchors in the future 6G ISAC network, since they can generate/process radio signals with high range/angle resolutions, and their positions are precisely known. Correspondingly, we will first study the BS-based sensing technique. On the other hand, the BSs alone may not enable ubiquitous sensing, since they cannot cover all the places with strong line-of-sight (LOS) links. This motivates us to investigate the possibility of using other nodes that are with higher density in the network to act as the anchors. Along this line, we are interested in two types of new anchors - user equipments (UEs) and reconfigurable intelligent surfaces (RISs). This paper will shed light on the opportunities and challenges brought by UE-assisted sensing and RIS-assisted sensing. Our goal is to devise a novel 6G-oriented sensing architecture where BSs, UEs, and RISs can work together to provide ubiquitous sensing services.","sentences":["Integrated sensing and communication (ISAC) has recently attracted tremendous attention from both academia and industry, being envisioned as a key part of the standards for the sixth-generation (6G) cellular network.","A key challenge of 6G-oriented ISAC lies in how to perform ubiquitous sensing based on the communication signals and devices.","Previous works have made great progresses on studying the signal waveform design that leads to optimal communication-sensing performance tradeoff.","In this article, we aim to focus on issues arising from the exploitation of the communication devices for sensing in 6G network.","Particularly, we will discuss about how to leverage various nodes available in the cellular network as anchors to perform ubiquitous sensing.","On one hand, the base stations (BSs) will be the most important anchors in the future 6G ISAC network, since they can generate/process radio signals with high range/angle resolutions, and their positions are precisely known.","Correspondingly, we will first study the BS-based sensing technique.","On the other hand, the BSs alone may not enable ubiquitous sensing, since they cannot cover all the places with strong line-of-sight (LOS) links.","This motivates us to investigate the possibility of using other nodes that are with higher density in the network to act as the anchors.","Along this line, we are interested in two types of new anchors - user equipments (UEs) and reconfigurable intelligent surfaces (RISs).","This paper will shed light on the opportunities and challenges brought by UE-assisted sensing and RIS-assisted sensing.","Our goal is to devise a novel 6G-oriented sensing architecture where BSs, UEs, and RISs can work together to provide ubiquitous sensing services."],"url":"http://arxiv.org/abs/2403.17652v1","category":"eess.SP"}
{"created":"2024-03-26 12:29:18","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering","abstract":"The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the evaluations of human assessors, acting as automatic metrics for the generated explanatory subgraphs. Our implementation is available at https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.","sentences":["The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods.","Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model.","In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset.","This approach bridges the gap between interpretability and performance.","Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making.","To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation.","Moreover, we present quantitative metrics that correlate with the evaluations of human assessors, acting as automatic metrics for the generated explanatory subgraphs.","Our implementation is available at https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA."],"url":"http://arxiv.org/abs/2403.17647v1","category":"cs.CL"}
{"created":"2024-03-26 12:23:34","title":"S+t-SNE - Bringing dimensionality reduction to data streams","abstract":"We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.","sentences":["We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams.","The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios.","By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations.","Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics.","Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE.","The results highlight its ability to capture patterns in a streaming scenario.","We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data."],"url":"http://arxiv.org/abs/2403.17643v1","category":"cs.AI"}
{"created":"2024-03-26 12:22:55","title":"Magnetic structure and properties of the honeycomb antiferromagnet [Na(OH$_2$)$_3$]Mn(NCS)$_3$","abstract":"We report the magnetic structure and properties of a thiocyanate-based honeycomb magnet [Na(OH$_2$)$_3$]Mn(NCS)$_3$ which crystallises in the unusual low-symmetry trigonal space group $P\\overline{3}$. Magnetic measurements on powder samples show this material is an antiferromagnet (ordering temperature ($T_\\mathrm{N,mag} = 18.1(6)\\,$)K) and can be described by nearest neighbour antiferromagnetic interactions $J=-11.07(4)\\,$K. A method for growing neutron-diffraction sized single crystals (\\textgreater10 mm$^3$) is demonstrated. Low temperature neutron single crystal diffraction shows that the compound adopts the collinear antiferromagnetic structure with $T_\\mathrm{N,neut}= 18.94(7)\\,$K, magnetic space group $P \\bar{3}'$. Low temperature second-harmonic generation (SHG) measurements provide no evidence of breaking of the centre of symmetry.","sentences":["We report the magnetic structure and properties of a thiocyanate-based honeycomb magnet","[Na(OH$_2$)$_3$]Mn(NCS)$_3$ which crystallises in the unusual low-symmetry trigonal space group $P\\overline{3}$. Magnetic measurements on powder samples show this material is an antiferromagnet (ordering temperature ($T_\\mathrm{N,mag} = 18.1(6)\\,$)K) and can be described by nearest neighbour antiferromagnetic interactions $J=-11.07(4)\\,$K. A method for growing neutron-diffraction sized single crystals (\\textgreater10 mm$^3$) is demonstrated.","Low temperature neutron single crystal diffraction shows that the compound adopts the collinear antiferromagnetic structure with $T_\\mathrm{N,neut}= 18.94(7)\\,$K, magnetic space group $P \\bar{3}'$.","Low temperature second-harmonic generation (SHG) measurements provide no evidence of breaking of the centre of symmetry."],"url":"http://arxiv.org/abs/2403.17642v1","category":"cond-mat.str-el"}
{"created":"2024-03-26 12:21:57","title":"Share the Sugar","abstract":"We provide a general argument against value incomparability, based on a new style of impossibility result. In particular, we show that, against plausible background assumptions, value incomparability creates an incompatibility between two very plausible principles for ranking lotteries: a weak ``negative dominance'' principle (to the effect that Lottery 1 can be better than Lottery 2 only if some possible outcome of Lottery 1 is better than some possible outcome of Lottery 2) and a weak form of ex ante Pareto (to the effect that, if Lottery 1 gives an unambiguously better prospect to some individuals than Lottery 2, and equally good prospects to everyone else, then Lottery 1 is better than Lottery 2). After spelling out our results, and the arguments based on them, we consider which principle the proponent of incomparability ought to reject.","sentences":["We provide a general argument against value incomparability, based on a new style of impossibility result.","In particular, we show that, against plausible background assumptions, value incomparability creates an incompatibility between two very plausible principles for ranking lotteries: a weak ``negative dominance'' principle (to the effect that Lottery 1 can be better than Lottery 2 only if some possible outcome of Lottery 1 is better than some possible outcome of Lottery 2) and a weak form of ex ante Pareto (to the effect that, if Lottery 1 gives an unambiguously better prospect to some individuals than Lottery 2, and equally good prospects to everyone else, then Lottery 1 is better than Lottery 2).","After spelling out our results, and the arguments based on them, we consider which principle the proponent of incomparability ought to reject."],"url":"http://arxiv.org/abs/2403.17641v1","category":"econ.TH"}
{"created":"2024-03-26 12:21:47","title":"High-Resolution Image Translation Model Based on Grayscale Redefinition","abstract":"Image-to-image translation is a technique that focuses on transferring images from one domain to another while maintaining the essential content representations. In recent years, image-to-image translation has gained significant attention and achieved remarkable advancements due to its diverse applications in computer vision and image processing tasks. In this work, we propose an innovative method for image translation between different domains. For high-resolution image translation tasks, we use a grayscale adjustment method to achieve pixel-level translation. For other tasks, we utilize the Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and improved loss to enhance the image translation performance. On the other hand, to tackle the issue of sparse training data, we adopt model weight initialization from other task to optimize the performance of the current task.","sentences":["Image-to-image translation is a technique that focuses on transferring images from one domain to another while maintaining the essential content representations.","In recent years, image-to-image translation has gained significant attention and achieved remarkable advancements due to its diverse applications in computer vision and image processing tasks.","In this work, we propose an innovative method for image translation between different domains.","For high-resolution image translation tasks, we use a grayscale adjustment method to achieve pixel-level translation.","For other tasks, we utilize the Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and improved loss to enhance the image translation performance.","On the other hand, to tackle the issue of sparse training data, we adopt model weight initialization from other task to optimize the performance of the current task."],"url":"http://arxiv.org/abs/2403.17639v1","category":"eess.IV"}
{"created":"2024-03-26 12:12:44","title":"PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning","abstract":"Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \\textit{PettingZoo}-based interface for RL agent deployment in both solo and multi-agent setups. Furthermore, we demonstrate the utility of the environment through experiments with Deep Reinforcement Learning agents, showcasing the potential of RL-based approaches to significantly enhance offloading strategies in distributed computing settings. PeersimGym thus bridges the gap between theoretical RL models and their practical applications, paving the way for advancements in efficient task offloading methodologies.","sentences":["Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints.","While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions.","However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments.","To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks.","PeersimGym supports a wide range of network topologies and computational constraints and integrates a \\textit{PettingZoo}-based interface for RL agent deployment in both solo and multi-agent setups.","Furthermore, we demonstrate the utility of the environment through experiments with Deep Reinforcement Learning agents, showcasing the potential of RL-based approaches to significantly enhance offloading strategies in distributed computing settings.","PeersimGym thus bridges the gap between theoretical RL models and their practical applications, paving the way for advancements in efficient task offloading methodologies."],"url":"http://arxiv.org/abs/2403.17637v1","category":"cs.LG"}
{"created":"2024-03-26 12:11:29","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","abstract":"Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in guiding generation dynamically. The prefix parameters can be tuned towards accurate initiative prediction as well as mix-initiative response generation. Extensive experiments on two public dialogue datasets show that the proposed IDPT outperforms previous baselines on both automatic metrics and human evaluations. It also manages to generate appropriate responses with manipulated initiatives.","sentences":["Mixed initiative serves as one of the key factors in controlling conversation directions.","For a speaker, responding passively or leading proactively would result in rather different responses.","However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives.","It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses.","Moreover, obtaining plenty of human annotations for initiative labels can be expensive.","To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings.","Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in guiding generation dynamically.","The prefix parameters can be tuned towards accurate initiative prediction as well as mix-initiative response generation.","Extensive experiments on two public dialogue datasets show that the proposed IDPT outperforms previous baselines on both automatic metrics and human evaluations.","It also manages to generate appropriate responses with manipulated initiatives."],"url":"http://arxiv.org/abs/2403.17636v1","category":"cs.CL"}
{"created":"2024-03-26 12:08:14","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps","abstract":"In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.","sentences":["In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets.","We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations.","We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D).","UADA3D does not depend on pre-trained source models or teacher-student architectures.","Instead, it uses an adversarial approach to directly learn domain-invariant features.","We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains.","Our code is open-source and will be available soon."],"url":"http://arxiv.org/abs/2403.17633v1","category":"cs.CV"}
{"created":"2024-03-26 12:08:05","title":"Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset","abstract":"The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for energy modelling research related to E-Scooters and E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption modelling based on the dataset using a set of representative machine learning algorithms and compare their performance against the contemporary mathematical models as a baseline. Our results demonstrate a notable advantage for data-driven models in comparison to the corresponding mathematical models for estimating energy consumption. Specifically, data-driven models outperform physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for E-Scooters based on an in-depth analysis of the dataset under certain assumptions.","sentences":["The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces.","In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters.","However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these.","To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification.","To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for energy modelling research related to E-Scooters and E-Bikes.","Furthermore, we provide a comprehensive analysis of energy consumption modelling based on the dataset using a set of representative machine learning algorithms and compare their performance against the contemporary mathematical models as a baseline.","Our results demonstrate a notable advantage for data-driven models in comparison to the corresponding mathematical models for estimating energy consumption.","Specifically, data-driven models outperform physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for E-Scooters based on an in-depth analysis of the dataset under certain assumptions."],"url":"http://arxiv.org/abs/2403.17632v1","category":"cs.AI"}
{"created":"2024-03-26 12:08:04","title":"AniArtAvatar: Animatable 3D Art Avatar from a Single Image","abstract":"We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space. Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network. We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks. To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages. The head and torso can be animated by transforming the two cages. Our approach is a one-shot pipeline that can be applied to various styles. Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions.","sentences":["We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements.","Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression.","With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface.","For avatar animation, we extract control points, transfer the motion with these points, and deform the implicit canonical space.","Firstly, we render the front image of the avatar, extract the 2D landmarks, and project them to the 3D space using a trained SDF network.","We extract 3D driving landmarks using 3DMM and transfer the motion to the avatar landmarks.","To animate the avatar pose, we manually set the body height and bound the head and torso of an avatar with two cages.","The head and torso can be animated by transforming the two cages.","Our approach is a one-shot pipeline that can be applied to various styles.","Experiments demonstrate that our method can generate high-quality 3D art avatars with desired control over different motions."],"url":"http://arxiv.org/abs/2403.17631v1","category":"cs.CV"}
{"created":"2024-03-26 12:06:34","title":"Interior Controls on the Habitability of Rocky Planets","abstract":"No matter how fascinating and exotic other terrestrial planets are revealed to be, nothing generates more excitement than announcements regarding their habitability. From the observation of Mars to present-day efforts toward Venus and the characterization of exoplanets, the search for life, or at least environments that could accommodate life, has been a major drive for space exploration. So far, we have found no other unquestionably habitable world besides Earth. The conditions of the habitability of terrestrial planets have proved elusive, as surface conditions depend on the complex interplay of many processes throughout the evolution of a planet. Here, we review how the interior of a rocky planet can drive the evolution of surface conditions and the atmosphere. Instead of listing criteria assumed to be critical for life, we discuss how the bulk-silicate planet can affect the onset, continuation and cessation of habitability. We then consider how it can be observed and current efforts towards this end.","sentences":["No matter how fascinating and exotic other terrestrial planets are revealed to be, nothing generates more excitement than announcements regarding their habitability.","From the observation of Mars to present-day efforts toward Venus and the characterization of exoplanets, the search for life, or at least environments that could accommodate life, has been a major drive for space exploration.","So far, we have found no other unquestionably habitable world besides Earth.","The conditions of the habitability of terrestrial planets have proved elusive, as surface conditions depend on the complex interplay of many processes throughout the evolution of a planet.","Here, we review how the interior of a rocky planet can drive the evolution of surface conditions and the atmosphere.","Instead of listing criteria assumed to be critical for life, we discuss how the bulk-silicate planet can affect the onset, continuation and cessation of habitability.","We then consider how it can be observed and current efforts towards this end."],"url":"http://arxiv.org/abs/2403.17630v1","category":"astro-ph.EP"}
{"created":"2024-03-26 11:51:58","title":"Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System","abstract":"Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.","sentences":["Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories.","However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data.","In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots.","Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm.","Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem.","Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system.","This enables us to refine our estimates of the tree traits if an area is revisited later during a mission.","We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests.","Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete."],"url":"http://arxiv.org/abs/2403.17622v1","category":"cs.RO"}
{"created":"2024-03-26 11:50:27","title":"Linear dynamics and classical tests of the gravitational quantum field theory","abstract":"We explore the new physics phenomena of gravidynamics governed by the inhomogeneous spin gauge symmetry based on the gravitational quantum field theory. Such a gravidynamics enables us to derive the generalized Einstein equation and an equation beyond it. To simplify the analyses, we linearize the dynamic equations of gravitational interaction by keeping terms up to the leading order in the dual gravigauge field. We then apply the linearized dynamic equations into two particular gravitational phenomena. First, we consider the linearized equations in the absence of source fields, which is shown to have five physical propagating polarizations as gravitational waves, i.e., two tensor modes, two vector modes, and one scalar, instead of two tensor polarizations in the general relativity. Second, we examine the Newtonian limit in which the gravitational fields and the matter source distribution are weak and static. By deriving the associated Poisson equation, we obtain the exact relation of the fundamental interaction coupling in the gravidynamics with the experimentally measured Newtonian constant. We also make use of nonrelativistic objects and relativistic photons to probe the Newtonian field configurations. In particular, the experiments from the gravitational deflection of light rays and the Shapiro time delay can place stringent constraints on the linearized gravidynamics in the gravitational quantum field theory.","sentences":["We explore the new physics phenomena of gravidynamics governed by the inhomogeneous spin gauge symmetry based on the gravitational quantum field theory.","Such a gravidynamics enables us to derive the generalized Einstein equation and an equation beyond it.","To simplify the analyses, we linearize the dynamic equations of gravitational interaction by keeping terms up to the leading order in the dual gravigauge field.","We then apply the linearized dynamic equations into two particular gravitational phenomena.","First, we consider the linearized equations in the absence of source fields, which is shown to have five physical propagating polarizations as gravitational waves, i.e., two tensor modes, two vector modes, and one scalar, instead of two tensor polarizations in the general relativity.","Second, we examine the Newtonian limit in which the gravitational fields and the matter source distribution are weak and static.","By deriving the associated Poisson equation, we obtain the exact relation of the fundamental interaction coupling in the gravidynamics with the experimentally measured Newtonian constant.","We also make use of nonrelativistic objects and relativistic photons to probe the Newtonian field configurations.","In particular, the experiments from the gravitational deflection of light rays and the Shapiro time delay can place stringent constraints on the linearized gravidynamics in the gravitational quantum field theory."],"url":"http://arxiv.org/abs/2403.17619v1","category":"gr-qc"}
{"created":"2024-03-26 11:48:51","title":"Symmetry groups of hyperbolic links and their complements","abstract":"We explicitly construct a sequence of hyperbolic links $\\{ L_{4n} \\}$ where the number of symmetries of each $\\mathbb{S}^{3} \\setminus L_{4n}$ that are not induced by symmetries of the pair $(\\mathbb{S}^{3}, L_{4n})$ grows linearly with n. Specifically, $[Sym(\\mathbb{S}^{3} \\setminus L_{4n}) : Sym(\\mathbb{S}^{3}, L_{4n})] =8n \\rightarrow \\infty$ as $n \\rightarrow \\infty$. For this construction, we start with a family of minimally twisted chain links, $\\{ C_{4n} \\}$, where $Sym(\\mathbb{S}^{3}, C_{4n})$ and $Sym(\\mathbb{S}^{3} \\setminus C_{4n})$ coincide and grow linearly with $n$. We then perform a particular type of homeomorphism on $\\mathbb{S}^{3} \\setminus C_{4n}$ to produce another link complement $\\mathbb{S}^{3} \\setminus L_{4n}$ where we can uniformly bound $|Sym(\\mathbb{S}^{3}, L_{4n})|$ using a combinatorial condition based on linking number. A more general result highlighting how to control symmetry groups of hyperbolic links is provided, which has potential for further application.","sentences":["We explicitly construct a sequence of hyperbolic links $\\{ L_{4n} \\}$ where the number of symmetries of each $\\mathbb{S}^{3} \\setminus L_{4n}$ that are not induced by symmetries of the pair $(\\mathbb{S}^{3}, L_{4n})$ grows linearly with n. Specifically, $[Sym(\\mathbb{S}^{3} \\setminus L_{4n}) : Sym(\\mathbb{S}^{3}, L_{4n})]","=8n \\rightarrow \\infty$ as $n \\rightarrow \\infty$. For this construction, we start with a family of minimally twisted chain links, $\\{ C_{4n} \\}$, where $Sym(\\mathbb{S}^{3}, C_{4n})$ and $Sym(\\mathbb{S}^{3} \\setminus C_{4n})$ coincide and grow linearly with $n$. We then perform a particular type of homeomorphism on $\\mathbb{S}^{3} \\setminus C_{4n}$ to produce another link complement $\\mathbb{S}^{3} \\setminus L_{4n}$ where we can uniformly bound $|Sym(\\mathbb{S}^{3}, L_{4n})|$ using a combinatorial condition based on linking number.","A more general result highlighting how to control symmetry groups of hyperbolic links is provided, which has potential for further application."],"url":"http://arxiv.org/abs/2403.17616v1","category":"math.GT"}
{"created":"2024-03-26 11:48:23","title":"New Physics Pathways from B Processes","abstract":"We re-consider recent measures of $R_{K}$ and $R_{K^*}$, now compatible with the Standard Model expectations, as well as the results for the process $\\text{BR}(B_s \\rightarrow \\mu^+ \\mu^-)$ alongside earlier determinations of $R_{D^{(\\ast)}}$ and $\\text{BR}(B_c \\rightarrow \\tau \\nu)$. We provide analytic constraints on the associated Wilson coefficients in both the $b \\to s$ and the $b \\to c$ sectors. These allow us to estimate the scale of potential New Physics for generic extensions of the Standard Model. We then use the results to constrain the leptoquark landscape.","sentences":["We re-consider recent measures of $R_{K}$ and $R_{K^*}$, now compatible with the Standard Model expectations, as well as the results for the process $\\text{BR}(B_s \\rightarrow \\mu^+ \\mu^-)$ alongside earlier determinations of $R_{D^{(\\ast)}}$ and $\\text{BR}(B_c \\rightarrow \\tau \\nu)$.","We provide analytic constraints on the associated Wilson coefficients in both the $b \\to s$ and the $b \\to c$ sectors.","These allow us to estimate the scale of potential New Physics for generic extensions of the Standard Model.","We then use the results to constrain the leptoquark landscape."],"url":"http://arxiv.org/abs/2403.17614v1","category":"hep-ph"}
{"created":"2024-03-26 11:48:19","title":"A Globally Convergent Gradient Method with Momentum","abstract":"In this work, we consider smooth unconstrained optimization problems and we deal with the class of gradient methods with momentum, i.e., descent algorithms where the search direction is defined as a linear combination of the current gradient and the preceding search direction. This family of algorithms includes nonlinear conjugate gradient methods and Polyak's heavy-ball approach, and is thus of high practical and theoretical interest in large-scale nonlinear optimization. We propose a general framework where the scalars of the linear combination defining the search direction are computed simultaneously by minimizing the approximate quadratic model in the 2 dimensional subspace. This strategy allows us to define a class of gradient methods with momentum enjoying global convergence guarantees and an optimal worst-case complexity bound in the nonconvex setting. Differently than all related works in the literature, the convergence conditions are stated in terms of the Hessian matrix of the bi-dimensional quadratic model. To the best of our knowledge, these results are novel to the literature. Moreover, extensive computational experiments show that the gradient methods with momentum here presented outperform classical conjugate gradient methods and are (at least) competitive with the state-of-art method for unconstrained optimization, i.e, L-BFGS method.","sentences":["In this work, we consider smooth unconstrained optimization problems and we deal with the class of gradient methods with momentum, i.e., descent algorithms where the search direction is defined as a linear combination of the current gradient and the preceding search direction.","This family of algorithms includes nonlinear conjugate gradient methods and Polyak's heavy-ball approach, and is thus of high practical and theoretical interest in large-scale nonlinear optimization.","We propose a general framework where the scalars of the linear combination defining the search direction are computed simultaneously by minimizing the approximate quadratic model in the 2 dimensional subspace.","This strategy allows us to define a class of gradient methods with momentum enjoying global convergence guarantees and an optimal worst-case complexity bound in the nonconvex setting.","Differently than all related works in the literature, the convergence conditions are stated in terms of the Hessian matrix of the bi-dimensional quadratic model.","To the best of our knowledge, these results are novel to the literature.","Moreover, extensive computational experiments show that the gradient methods with momentum here presented outperform classical conjugate gradient methods and are (at least) competitive with the state-of-art method for unconstrained optimization, i.e, L-BFGS method."],"url":"http://arxiv.org/abs/2403.17613v1","category":"math.OC"}
{"created":"2024-03-26 11:44:49","title":"Denoising Table-Text Retrieval for Open-Domain Question Answering","abstract":"In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder to identify minimum and maximum values within a column. Experimental results demonstrate that DoTTeR significantly outperforms strong baselines on both retrieval recall and downstream QA tasks. Our code is available at https://github.com/deokhk/DoTTeR.","sentences":["In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions.","Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table.","To address these issues, we propose Denoised Table-Text Retriever (DoTTeR).","Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model.","Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table.","To encode this ranking information, we fine-tune a rank-aware column encoder to identify minimum and maximum values within a column.","Experimental results demonstrate that DoTTeR significantly outperforms strong baselines on both retrieval recall and downstream QA tasks.","Our code is available at https://github.com/deokhk/DoTTeR."],"url":"http://arxiv.org/abs/2403.17611v1","category":"cs.CL"}
{"created":"2024-03-26 11:43:05","title":"MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors","abstract":"Foot contact is an important cue not only for human motion capture but also for motion understanding and physically plausible motion generation. However, most of the foot-contact annotations in existing datasets are estimated by purely visual matching and distance thresholding, which results in low accuracy and coarse granularity. Even though existing multimodal datasets synergistically capture plantar pressure (foot contact) and visual signals, they are specifically designed for small-range and slow motion such as Taiji Quan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation. To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking. To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture. Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy. We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains. Project page: https://haolyuan.github.io/MMVP-Dataset/.","sentences":["Foot contact is an important cue not only for human motion capture but also for motion understanding and physically plausible motion generation.","However, most of the foot-contact annotations in existing datasets are estimated by purely visual matching and distance thresholding, which results in low accuracy and coarse granularity.","Even though existing multimodal datasets synergistically capture plantar pressure (foot contact) and visual signals, they are specifically designed for small-range and slow motion such as Taiji Quan and Yoga.","Therefore, there is still a lack of a vision-pressure multimodal dataset with large-range and fast human motion, as well as accurate and dense foot-contact annotation.","To fill this gap, we propose a Multimodal MoCap Dataset with Vision and Pressure sensors, named MMVP.","MMVP provides accurate and dense plantar pressure signals synchronized with RGBD observations, which is especially useful for both plausible shape estimation, robust pose fitting without foot drifting, and accurate global translation tracking.","To validate the dataset, we propose an RGBD-P SMPL fitting method and also a monocular-video-based baseline framework, VP-MoCap, for human motion capture.","Experiments demonstrate that our RGBD-P SMPL Fitting results significantly outperform pure visual motion capture.","Moreover, VP-MoCap outperforms SOTA methods in foot-contact and global translation estimation accuracy.","We believe the configuration of the dataset and the baseline frameworks will stimulate the research in this direction and also provide a good reference for MoCap applications in various domains.","Project page: https://haolyuan.github.io/MMVP-Dataset/."],"url":"http://arxiv.org/abs/2403.17610v1","category":"cs.CV"}
{"created":"2024-03-26 11:41:09","title":"A location Invariant Statistic-Based Consistent Estimation Method for Three-Parameter Generalized Exponential Distribution","abstract":"In numerous instances, the generalized exponential distribution can be used as an alternative to the gamma distribution or the Weibull distribution when analyzing lifetime or skewed data. This article offers a consistent method for estimating the parameters of a three-parameter generalized exponential distribution that sidesteps the issue of an unbounded likelihood function. The method is hinged on a maximum likelihood estimation of shape and scale parameters that uses a location-invariant statistic. Important estimator properties, such as uniqueness and consistency, are demonstrated. In addition, quantile estimates for the lifetime distribution are provided. We present a Monte Carlo simulation study along with comparisons to a number of well-known estimation techniques in terms of bias and root mean square error. For illustrative purposes, a real-world lifetime data set is analyzed.","sentences":["In numerous instances, the generalized exponential distribution can be used as an alternative to the gamma distribution or the Weibull distribution when analyzing lifetime or skewed data.","This article offers a consistent method for estimating the parameters of a three-parameter generalized exponential distribution that sidesteps the issue of an unbounded likelihood function.","The method is hinged on a maximum likelihood estimation of shape and scale parameters that uses a location-invariant statistic.","Important estimator properties, such as uniqueness and consistency, are demonstrated.","In addition, quantile estimates for the lifetime distribution are provided.","We present a Monte Carlo simulation study along with comparisons to a number of well-known estimation techniques in terms of bias and root mean square error.","For illustrative purposes, a real-world lifetime data set is analyzed."],"url":"http://arxiv.org/abs/2403.17609v1","category":"stat.ME"}
{"created":"2024-03-26 11:39:00","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets","abstract":"The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and Swin-T detectors on the GenImage dataset, achieving state-of-the-art results.   We provide the dataset and source codes of this paper on the anonymous website: https://www.unbiased-genimage.org","sentences":["The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation.","Consequently, numerous detectors and associated datasets have emerged.","However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors.","In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size.","Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors.","Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors.","Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and Swin-T detectors on the GenImage dataset, achieving state-of-the-art results.   ","We provide the dataset and source codes of this paper on the anonymous website: https://www.unbiased-genimage.org"],"url":"http://arxiv.org/abs/2403.17608v1","category":"cs.CV"}
{"created":"2024-03-26 11:38:39","title":"Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs","abstract":"This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning. In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidia's H100 GPU by up to a factor 19. The code can be found at https://github.com/intel/tiny-dpcpp-nn.","sentences":["This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550.","To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP.","We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference.","We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training.","The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed Machine Learning.","In all cases, our implementation outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on Nvidia's H100 GPU by up to a factor 19.","The code can be found at https://github.com/intel/tiny-dpcpp-nn."],"url":"http://arxiv.org/abs/2403.17607v1","category":"cs.AI"}
{"created":"2024-03-26 11:38:19","title":"Interactive Identification of Granular Materials using Force Measurements","abstract":"The ability to identify granular materials facilitates the emergence of various new applications in robotics, ranging from cooking at home to truck loading at mining sites. However, granular material identification remains a challenging and underexplored area. In this work, we present a novel interactive material identification framework that enables robots to identify a wide range of granular materials using only a force-torque sensor for perception. Our framework, comprising interactive exploration, feature extraction, and classification stages, prioritizes simplicity and transparency for seamless integration into various manipulation pipelines. We evaluate the proposed approach through extensive experiments with a real-world dataset comprising 11 granular materials, which we also make publicly available. Additionally, we conducted a comprehensive qualitative analysis of the dataset to offer deeper insights into its nature, aiding future development. Our results show that the proposed method is capable of accurately identifying a wide range of granular materials solely relying on force measurements obtained from direct interaction with the materials. Code and dataset are available at: https://irobotics.aalto.fi/indentify_granular/.","sentences":["The ability to identify granular materials facilitates the emergence of various new applications in robotics, ranging from cooking at home to truck loading at mining sites.","However, granular material identification remains a challenging and underexplored area.","In this work, we present a novel interactive material identification framework that enables robots to identify a wide range of granular materials using only a force-torque sensor for perception.","Our framework, comprising interactive exploration, feature extraction, and classification stages, prioritizes simplicity and transparency for seamless integration into various manipulation pipelines.","We evaluate the proposed approach through extensive experiments with a real-world dataset comprising 11 granular materials, which we also make publicly available.","Additionally, we conducted a comprehensive qualitative analysis of the dataset to offer deeper insights into its nature, aiding future development.","Our results show that the proposed method is capable of accurately identifying a wide range of granular materials solely relying on force measurements obtained from direct interaction with the materials.","Code and dataset are available at: https://irobotics.aalto.fi/indentify_granular/."],"url":"http://arxiv.org/abs/2403.17606v1","category":"cs.RO"}
{"created":"2024-03-26 11:13:35","title":"LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation","abstract":"Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware of learner state distribution. Our method, applied to urban traffic simulation, demonstrates significant improvements over existing state-of-the-art baselines in both short-term microscopic and long-term macroscopic realism when evaluated on the real-world dataset pNEUMA.","sentences":["Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow.","However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges.","Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments.","Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations.","In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning.","By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware of learner state distribution.","Our method, applied to urban traffic simulation, demonstrates significant improvements over existing state-of-the-art baselines in both short-term microscopic and long-term macroscopic realism when evaluated on the real-world dataset pNEUMA."],"url":"http://arxiv.org/abs/2403.17601v1","category":"cs.AI"}
{"created":"2024-03-26 11:05:48","title":"Symmetry restoration and vacuum decay from accretion around black holes","abstract":"Vacuum decay and symmetry breaking play an important role in the fundamental structure of the matter and the evolution of the universe. In this work we study how the purely classical effect of accretion of fundamental fields onto black holes can lead to shells of symmetry restoration in the midst of a symmetry broken phase. We also show how it can catalyze vacuum decay, forming a bubble that expands asymptotically at the speed of light. These effects offer an alternative, purely classical mechanism to quantum tunnelling for seeding phase transitions in the universe.","sentences":["Vacuum decay and symmetry breaking play an important role in the fundamental structure of the matter and the evolution of the universe.","In this work we study how the purely classical effect of accretion of fundamental fields onto black holes can lead to shells of symmetry restoration in the midst of a symmetry broken phase.","We also show how it can catalyze vacuum decay, forming a bubble that expands asymptotically at the speed of light.","These effects offer an alternative, purely classical mechanism to quantum tunnelling for seeding phase transitions in the universe."],"url":"http://arxiv.org/abs/2403.17595v1","category":"gr-qc"}
{"created":"2024-03-26 11:05:32","title":"Lattice dynamics of LiNb$_{\\text{1-x}}$Ta$_{\\text{x}}$O$_{\\text{3}}$ solid solutions: Theory and experiment","abstract":"Lithium niobate (LNO) and lithium tantalate (LTO) see widespread use in fundamental research and commercial technologies reaching from electronics over classical optics to integrated quantum communication. In recent years, the mixed crystal system lithium niobate tantalate (LNT) allows for the dedicate engineering of material properties by combining the advantages of the two parental materials LNO and LTO. Vibrational spectroscopies such as Raman spectroscopy or (Fourier transform) infrared spectroscopy are vital techniques to provide detailed insight into the material properties, which is central to the analysis and optimization of devices. In this work, we present a joint experimental-theoretical approach allowing to unambiguously assign the spectral features in the LNT material family through both Raman and IR spectroscopy, as well as to provide an in-depth explanation for the observed scattering efficiencies based on first-principles calculations. The phononic contribution to the static dielectric tensor is calculated from the experimental and theoretical data using the generalized Lyddane-Sachs-Teller relation and compared with the results of the first-principles calculations. The joint methodology can be readily expanded to other materials and serves, e.g., as the basis for studying the role of point defects or doping.","sentences":["Lithium niobate (LNO) and lithium tantalate (LTO) see widespread use in fundamental research and commercial technologies reaching from electronics over classical optics to integrated quantum communication.","In recent years, the mixed crystal system lithium niobate tantalate (LNT) allows for the dedicate engineering of material properties by combining the advantages of the two parental materials LNO and LTO.","Vibrational spectroscopies such as Raman spectroscopy or (Fourier transform) infrared spectroscopy are vital techniques to provide detailed insight into the material properties, which is central to the analysis and optimization of devices.","In this work, we present a joint experimental-theoretical approach allowing to unambiguously assign the spectral features in the LNT material family through both Raman and IR spectroscopy, as well as to provide an in-depth explanation for the observed scattering efficiencies based on first-principles calculations.","The phononic contribution to the static dielectric tensor is calculated from the experimental and theoretical data using the generalized Lyddane-Sachs-Teller relation and compared with the results of the first-principles calculations.","The joint methodology can be readily expanded to other materials and serves, e.g., as the basis for studying the role of point defects or doping."],"url":"http://arxiv.org/abs/2403.17594v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 11:01:53","title":"On the Benefits of Over-parameterization for Out-of-Distribution Generalization","abstract":"In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess in-distribution (ID) loss. We demonstrate that in this scenario, further increasing the model's parameterization can significantly reduce the OOD loss. Intuitively, the variance term of ID loss remains low due to orthogonality of long-tail features, meaning overfitting noise during training generally doesn't raise testing loss. However, in OOD cases, distributional shift increases the variance term. Thankfully, the inherent shift is unrelated to individual x, maintaining the orthogonality of long-tail features. Expanding the hidden dimension can additionally improve this orthogonality by mapping the features into higher-dimensional spaces, thereby reducing the variance term. We further show that model ensembles also improve OOD loss, akin to increasing model capacity. These insights explain the empirical phenomenon of enhanced OOD generalization through model ensembles, supported by consistent simulations with theoretical results.","sentences":["In recent years, machine learning models have achieved success based on the independently and identically distributed assumption.","However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem.","Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient.","Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings.","To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions.","Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess in-distribution (ID) loss.","We demonstrate that in this scenario, further increasing the model's parameterization can significantly reduce the OOD loss.","Intuitively, the variance term of ID loss remains low due to orthogonality of long-tail features, meaning overfitting noise during training generally doesn't raise testing loss.","However, in OOD cases, distributional shift increases the variance term.","Thankfully, the inherent shift is unrelated to individual x, maintaining the orthogonality of long-tail features.","Expanding the hidden dimension can additionally improve this orthogonality by mapping the features into higher-dimensional spaces, thereby reducing the variance term.","We further show that model ensembles also improve OOD loss, akin to increasing model capacity.","These insights explain the empirical phenomenon of enhanced OOD generalization through model ensembles, supported by consistent simulations with theoretical results."],"url":"http://arxiv.org/abs/2403.17592v1","category":"cs.LG"}
{"created":"2024-03-26 10:54:07","title":"Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models","abstract":"Random Forest (RF) is well-known as an efficient ensemble learning method in terms of predictive performance. It is also considered a Black Box because of its hundreds of deep decision trees. This lack of interpretability can be a real drawback for acceptance of RF models in several real-world applications, especially those affecting one's lives, such as in healthcare, security, and law. In this work, we present Forest-ORE, a method that makes RF interpretable via an optimized rule ensemble (ORE) for local and global interpretation. Unlike other rule-based approaches aiming at interpreting the RF model, this method simultaneously considers several parameters that influence the choice of an interpretable rule ensemble. Existing methods often prioritize predictive performance over interpretability coverage and do not provide information about existing overlaps or interactions between rules. Forest-ORE uses a mixed-integer optimization program to build an ORE that considers the trade-off between predictive performance, interpretability coverage, and model size (size of the rule ensemble, rule lengths, and rule overlaps). In addition to providing an ORE competitive in predictive performance with RF, this method enriches the ORE through other rules that afford complementary information. It also enables monitoring of the rule selection process and delivers various metrics that can be used to generate a graphical representation of the final model. This framework is illustrated through an example, and its robustness is assessed through 36 benchmark datasets. A comparative analysis of well-known methods shows that Forest-ORE provides an excellent trade-off between predictive performance, interpretability coverage, and model size.","sentences":["Random Forest (RF) is well-known as an efficient ensemble learning method in terms of predictive performance.","It is also considered a Black Box because of its hundreds of deep decision trees.","This lack of interpretability can be a real drawback for acceptance of RF models in several real-world applications, especially those affecting one's lives, such as in healthcare, security, and law.","In this work, we present Forest-ORE, a method that makes RF interpretable via an optimized rule ensemble (ORE) for local and global interpretation.","Unlike other rule-based approaches aiming at interpreting the RF model, this method simultaneously considers several parameters that influence the choice of an interpretable rule ensemble.","Existing methods often prioritize predictive performance over interpretability coverage and do not provide information about existing overlaps or interactions between rules.","Forest-ORE uses a mixed-integer optimization program to build an ORE that considers the trade-off between predictive performance, interpretability coverage, and model size (size of the rule ensemble, rule lengths, and rule overlaps).","In addition to providing an ORE competitive in predictive performance with RF, this method enriches the ORE through other rules that afford complementary information.","It also enables monitoring of the rule selection process and delivers various metrics that can be used to generate a graphical representation of the final model.","This framework is illustrated through an example, and its robustness is assessed through 36 benchmark datasets.","A comparative analysis of well-known methods shows that Forest-ORE provides an excellent trade-off between predictive performance, interpretability coverage, and model size."],"url":"http://arxiv.org/abs/2403.17588v1","category":"cs.LG"}
{"created":"2024-03-26 10:54:07","title":"Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models","abstract":"With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set. This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data. The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers. Our approach is tested across 11 datasets under the three task settings. Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\\% and even shows superior results against methods utilizing external training data. Additionally, our method exhibits robust performance against natural distribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.","sentences":["With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research.","The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation.","Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms.","In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings.","Specifically, we propose the dual memory networks that comprise dynamic and static memory components.","The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the training set.","This novel capability enhances model performance in the few-shot setting and enables model usability in the absence of training data.","The two memory networks employ the same flexible memory interactive strategy, which can operate in a training-free mode and can be further enhanced by incorporating learnable projection layers.","Our approach is tested across 11 datasets under the three task settings.","Remarkably, in the zero-shot scenario, it outperforms existing methods by over 3\\% and even shows superior results against methods utilizing external training data.","Additionally, our method exhibits robust performance against natural distribution shifts.","Codes are available at \\url{https://github.com/YBZh/DMN}."],"url":"http://arxiv.org/abs/2403.17589v1","category":"cs.CV"}
{"created":"2024-03-26 10:53:25","title":"Parameterized Analysis of Bribery in Challenge the Champ Tournaments","abstract":"Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.   We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players. On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values. To this end, we establish several results that are of independent interest. In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players. Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers.","sentences":["Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players.","If a player beats the champ, then that player is considered the new (current) champ.","Each player in the competition challenges the current champ once in a fixed order.","The champ of the last round is considered the winner of the tournament.","We investigate a setting where players can be bribed to lower their winning probability against the initial champ.","The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes.","Mattei et al.","[Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.   ","We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players.","On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values.","To this end, we establish several results that are of independent interest.","In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players.","Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers."],"url":"http://arxiv.org/abs/2403.17587v1","category":"cs.DS"}
{"created":"2024-03-26 10:46:22","title":"Point potentials on Euclidean space, hyperbolic space and sphere in any dimension","abstract":"In dimensions d= 1, 2, 3 the Laplacian can be perturbed by a point potential. In higher dimensions the Laplacian with a point potential cannot be defined as a self-adjoint operator. However, for any dimension there exists a natural family of functions that can be interpreted as Green's functions of the Laplacian with a spherically symmetric point potential. In dimensions 1, 2, 3 they are the integral kernels of the resolvent of well-defined self-adjoint operators. In higher dimensions they are not even integral kernels of bounded operators. Their construction uses the so-called generalized integral, a concept going back to Riesz and Hadamard.   We consider the Laplace(-Beltrami) operator on the Euclidean space, the hyperbolic space and the sphere in any dimension. We describe the corresponding Green's functions, also perturbed by a point potential. We describe their limit as the scaled hyperbolic space and the scaled sphere approach the Euclidean space. Especially interesting is the behavior of positive eigenvalues of the spherical Laplacian, which undergo a shift proportional to a negative power of the radius of the sphere.   We expect that in any dimension our constructions yield possible behaviors of the integral kernel of the resolvent of a perturbed Laplacian far from the support of the perturbation. Besides, they can be viewed as toy models illustrating various aspects of renormalization in Quantum Field Theory, especially the point-splitting method and dimensional regularization.","sentences":["In dimensions d= 1, 2, 3 the Laplacian can be perturbed by a point potential.","In higher dimensions the Laplacian with a point potential cannot be defined as a self-adjoint operator.","However, for any dimension there exists a natural family of functions that can be interpreted as Green's functions of the Laplacian with a spherically symmetric point potential.","In dimensions 1, 2, 3 they are the integral kernels of the resolvent of well-defined self-adjoint operators.","In higher dimensions they are not even integral kernels of bounded operators.","Their construction uses the so-called generalized integral, a concept going back to Riesz and Hadamard.   ","We consider the Laplace(-Beltrami) operator on the Euclidean space, the hyperbolic space and the sphere in any dimension.","We describe the corresponding Green's functions, also perturbed by a point potential.","We describe their limit as the scaled hyperbolic space and the scaled sphere approach the Euclidean space.","Especially interesting is the behavior of positive eigenvalues of the spherical Laplacian, which undergo a shift proportional to a negative power of the radius of the sphere.   ","We expect that in any dimension our constructions yield possible behaviors of the integral kernel of the resolvent of a perturbed Laplacian far from the support of the perturbation.","Besides, they can be viewed as toy models illustrating various aspects of renormalization in Quantum Field Theory, especially the point-splitting method and dimensional regularization."],"url":"http://arxiv.org/abs/2403.17583v1","category":"math-ph"}
{"created":"2024-03-26 10:45:11","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","abstract":"Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.","sentences":["Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree.","The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users.","However, the need for additional training data hinders deployment in new domains.","To address this, we explore approaches to generate this data directly from dialog trees.","We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU.","We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms.","Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data."],"url":"http://arxiv.org/abs/2403.17582v1","category":"cs.CL"}
{"created":"2024-03-26 10:31:32","title":"Tactical decompositions in finite polar spaces and non-spreading classical group actions","abstract":"For finite classical groups acting naturally on the set of points of their ambient polar spaces, the symmetry properties of \\emph{synchronising} and \\emph{separating} are equivalent to natural and well-studied problems on the existence of certain configurations in finite geometry. The more general class of \\emph{spreading} permutation groups is harder to describe, and it is the purpose of this paper to explore this property for finite classical groups. In particular, we show that for most finite classical groups, their natural action on the points of its polar space is non-spreading. We develop and use a result on tactical decompositions (an \\emph{AB-Lemma}) that provides a useful technique for finding witnesses for non-spreading permutation groups. We also consider some of the other primitive actions of the classical groups.","sentences":["For finite classical groups acting naturally on the set of points of their ambient polar spaces, the symmetry properties of \\emph{synchronising} and \\emph{separating} are equivalent to natural and well-studied problems on the existence of certain configurations in finite geometry.","The more general class of \\emph{spreading} permutation groups is harder to describe, and it is the purpose of this paper to explore this property for finite classical groups.","In particular, we show that for most finite classical groups, their natural action on the points of its polar space is non-spreading.","We develop and use a result on tactical decompositions (an \\emph{AB-Lemma}) that provides a useful technique for finding witnesses for non-spreading permutation groups.","We also consider some of the other primitive actions of the classical groups."],"url":"http://arxiv.org/abs/2403.17576v1","category":"math.GR"}
{"created":"2024-03-26 10:22:05","title":"Ransomware: Analysis and Evaluation of Live Forensic Techniques and the Impact on Linux based IoT Systems","abstract":"Ransomware has been predominantly a threat to Windows systems. But, Linux systems became interesting for cybercriminals and this trend is expected to continue. This endangers IoT ecosystems, whereas many IoT systems are based on Linux (e.g. cloud infrastructure and gateways). This paper researches how currently employed forensic techniques can be applied to Linux ransomware and evaluates the maturity as well as the impact on the system. While Windows-based ransomware predominantly uses RSA and AES for key management, a variety of approaches was identified for Linux. Cybercriminals appear to be deliberately moving away from RSA and AES to make Live forensic investigations more difficult. Linux ransomware is developed for a predefined goal and does not exploit the full potential of damage. It appears in an early stage and is expected to reach a similar potential to Windows-based malware. The results generated provided an excellent basic understanding to discuss and assess implications on the IoT industry at an early stage of development.","sentences":["Ransomware has been predominantly a threat to Windows systems.","But, Linux systems became interesting for cybercriminals and this trend is expected to continue.","This endangers IoT ecosystems, whereas many IoT systems are based on Linux (e.g. cloud infrastructure and gateways).","This paper researches how currently employed forensic techniques can be applied to Linux ransomware and evaluates the maturity as well as the impact on the system.","While Windows-based ransomware predominantly uses RSA and AES for key management, a variety of approaches was identified for Linux.","Cybercriminals appear to be deliberately moving away from RSA and AES to make Live forensic investigations more difficult.","Linux ransomware is developed for a predefined goal and does not exploit the full potential of damage.","It appears in an early stage and is expected to reach a similar potential to Windows-based malware.","The results generated provided an excellent basic understanding to discuss and assess implications on the IoT industry at an early stage of development."],"url":"http://arxiv.org/abs/2403.17571v1","category":"cs.CR"}
{"created":"2024-03-26 10:20:38","title":"A Caro-Wei bound for induced linear forests in graphs","abstract":"A well-known result due to Caro (1979) and Wei (1981) states that every graph $G$ has an independent set of size at least $\\sum_{v\\in V(G)} \\frac{1}{d(v) + 1}$, where $d(v)$ denotes the degree of vertex $v$. Alon, Kahn, and Seymour (1987) showed the following generalization: For every $k\\geq 0$, every graph $G$ has a $k$-degenerate induced subgraph with at least $\\sum_{v \\in V(G)}\\min\\{1, \\frac {k+1}{d(v)+1}\\}$ vertices. In particular, for $k=1$, every graph $G$ with no isolated vertices has an induced forest with at least $\\sum_{v\\in V(G)} \\frac{2}{d(v) + 1}$ vertices. Akbari, Amanihamedani, Mousavi, Nikpey, and Sheybani (2019) conjectured that, if $G$ has minimum degree at least $2$, then one can even find an induced linear forest of that order in $G$, that is, a forest where each component is a path.   In this paper, we prove this conjecture and show a number of related results. In particular, if there is no restriction on the minimum degree of $G$, we show that there are infinitely many ``best possible'' functions $f$ such that $\\sum_{v\\in V(G)} f(d(v))$ is a lower bound on the maximum order of a linear forest in $G$, and we give a full characterization of all such functions $f$.","sentences":["A well-known result due to Caro (1979) and Wei (1981) states that every graph $G$ has an independent set of size at least $\\sum_{v\\in V(G)}","\\frac{1}{d(v) + 1}$, where $d(v)$ denotes the degree of vertex $v$. Alon, Kahn, and Seymour (1987) showed the following generalization:","For every $k\\geq 0$, every graph $G$ has a $k$-degenerate induced subgraph with at least $\\sum_{v \\in V(G)}\\min\\{1, \\frac {k+1}{d(v)+1}\\}$ vertices.","In particular, for $k=1$, every graph $G$ with no isolated vertices has an induced forest with at least $\\sum_{v\\in V(G)} \\frac{2}{d(v) + 1}$ vertices.","Akbari, Amanihamedani, Mousavi, Nikpey, and Sheybani (2019) conjectured that, if $G$ has minimum degree at least $2$, then one can even find an induced linear forest of that order in $G$, that is, a forest where each component is a path.   ","In this paper, we prove this conjecture and show a number of related results.","In particular, if there is no restriction on the minimum degree of $G$, we show that there are infinitely many ``best possible'' functions $f$ such that $\\sum_{v\\in V(G)} f(d(v))$ is a lower bound on the maximum order of a linear forest in $G$, and we give a full characterization of all such functions $f$."],"url":"http://arxiv.org/abs/2403.17568v1","category":"math.CO"}
{"created":"2024-03-26 10:20:35","title":"Piecewise Linear Expectation Analysis via $k$-Induction for Probabilistic Programs","abstract":"Quantitative analysis of probabilistic programs aims at deriving tight numerical bounds for probabilistic properties such as expectation and assertion probability, and plays a crucial role in the verification of probabilistic programs. Along this line of research, most existing works consider numerical bounds over the whole state space monolithically and do not consider piecewise bounds. Clearly, monolithic bounds are either conservative, or not expressive and succinct enough in general. To derive more succinct, expressive and precise numerical bounds for probabilistic properties, we propose a novel approach for synthesizing piecewise linear bounds in this work. To this end, we first show how to extract a piecewise feature w.r.t. a given quantitative property from a probabilistic program using latticed $k$-induction that captures a wide and representative class of piecewise bound functions. Second, we develop an algorithmic approach to synthesize piecewise linear upper and lower bounds from the piecewise feature, for which we show that the synthesis of piecewise linear bounds can be reduced to bilinear programming. Third, we implement our approach with the bilinear programming solver Gurobi. The experimental results indicate that our approach is capable of generating tight or even accurate piecewise linear bounds for an extensive set of benchmarks compared with the state of the art.","sentences":["Quantitative analysis of probabilistic programs aims at deriving tight numerical bounds for probabilistic properties such as expectation and assertion probability, and plays a crucial role in the verification of probabilistic programs.","Along this line of research, most existing works consider numerical bounds over the whole state space monolithically and do not consider piecewise bounds.","Clearly, monolithic bounds are either conservative, or not expressive and succinct enough in general.","To derive more succinct, expressive and precise numerical bounds for probabilistic properties, we propose a novel approach for synthesizing piecewise linear bounds in this work.","To this end, we first show how to extract a piecewise feature w.r.t.","a given quantitative property from a probabilistic program using latticed $k$-induction that captures a wide and representative class of piecewise bound functions.","Second, we develop an algorithmic approach to synthesize piecewise linear upper and lower bounds from the piecewise feature, for which we show that the synthesis of piecewise linear bounds can be reduced to bilinear programming.","Third, we implement our approach with the bilinear programming solver Gurobi.","The experimental results indicate that our approach is capable of generating tight or even accurate piecewise linear bounds for an extensive set of benchmarks compared with the state of the art."],"url":"http://arxiv.org/abs/2403.17567v1","category":"cs.PL"}
{"created":"2024-03-26 10:10:53","title":"A Survey on Deep Learning and State-of-the-arts Applications","abstract":"Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.","sentences":["Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data.","Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations.","Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems.","Several studies have reviewed deep learning concepts and applications.","However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains.","Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing.","We highlight the key features of the models and their effectiveness in solving the problems within each domain.","Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures.","Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers."],"url":"http://arxiv.org/abs/2403.17561v1","category":"cs.LG"}
{"created":"2024-03-26 10:04:58","title":"Some Generalizations of Mercer inequality and its operator extensions","abstract":"We study the Mercer inequality and its operator extension for superquadratic functions. In particular, we give a more general form of the Mercer inequality by replacing some constants by positive operators. As some consequences, our results produce a Jensen operator inequality for superquadratic functions. Moreover, we present some Mercer inequalities of Hermite-Hadamard's type.","sentences":["We study the Mercer inequality and its operator extension for superquadratic functions.","In particular, we give a more general form of the Mercer inequality by replacing some constants by positive operators.","As some consequences, our results produce a Jensen operator inequality for superquadratic functions.","Moreover, we present some Mercer inequalities of Hermite-Hadamard's type."],"url":"http://arxiv.org/abs/2403.17557v1","category":"math.FA"}
{"created":"2024-03-26 10:04:24","title":"m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt","abstract":"Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results show that m3P outperforms previous text-only baselines and multilingual multimodal methods by a large margin. Furthermore, the probing experiments validate the effectiveness of our method in enhancing translation under the low-resource and massively multilingual scenario.","sentences":["Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large.","To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation.","In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation.","We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages.","Our method aims to minimize the representation distance of different languages by regarding the image as a central language.","Experimental results show that m3P outperforms previous text-only baselines and multilingual multimodal methods by a large margin.","Furthermore, the probing experiments validate the effectiveness of our method in enhancing translation under the low-resource and massively multilingual scenario."],"url":"http://arxiv.org/abs/2403.17556v1","category":"cs.CL"}
{"created":"2024-03-26 09:59:45","title":"Naive Bayes-based Context Extension for Large Language Models","abstract":"Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes' theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master","sentences":["Large Language Models (LLMs) have shown promising in-context learning abilities.","However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples.","In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size.","Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency.","NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length.","Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context.","Finally, it employs Bayes' theorem to generate the test task.","Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods.","The NBCE code will be made publicly accessible.","The code NBCE is available at: https://github.com/amurtadha/NBCE-master"],"url":"http://arxiv.org/abs/2403.17552v1","category":"cs.CL"}
{"created":"2024-03-26 09:55:49","title":"Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis","abstract":"The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results. By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes. However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.","sentences":["The medical field is one of the important fields in the application of artificial intelligence technology.","With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field.","Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.","The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation.","Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results.","By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes.","However, ethical considerations and collaboration among stakeholders are essential for responsible implementation.","Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care."],"url":"http://arxiv.org/abs/2403.17549v1","category":"cs.AI"}
{"created":"2024-03-26 09:54:25","title":"Properties of graphs of neural codes","abstract":"A neural code on $ n $ neurons is a collection of subsets of the set $ [n]=\\{1,2,\\dots,n\\} $. In this paper, we study some properties of graphs of neural codes. In particular, we study codeword containment graph (CCG) given by Chan et al. (SIAM J. on Dis. Math., 37(1):114-145,2017) and general relationship graph (GRG) given by Gross et al. (Adv. in App. Math., 95:65-95, 2018). We provide a sufficient condition for CCG to be connected. We also show that the connectedness and completeness of CCG are preserved under surjective morphisms between neural codes defined by A. Jeffs (SIAM J. on App. Alg. and Geo., 4(1):99-122,2020). Further, we show that if CCG of any neural code $\\mathcal{C}$ is complete with $|\\mathcal{C}|=m$, then $\\mathcal{C} \\cong \\{\\emptyset,1,12,\\dots,123\\cdots m\\}$ as neural codes. We also prove that a code whose CCG is complete is open convex. Later, we show that if a code $\\mathcal{C}$ with $|\\mathcal{C}|>3$ has its CCG to be connected 2-regular then $|\\mathcal{C}| $ is even. The GRG was defined only for degree two neural codes using the canonical forms of its neural ideal. We first define GRG for any neural code. Then, we show the behaviour of GRGs under the various elementary code maps. At last, we compare these two graphs for certain classes of codes and see their properties.","sentences":["A neural code on $ n $ neurons is a collection of subsets of the set $","[n]=\\{1,2,\\dots,n\\} $.","In this paper, we study some properties of graphs of neural codes.","In particular, we study codeword containment graph (CCG) given by Chan et al.","(SIAM J. on Dis.","Math., 37(1):114-145,2017) and general relationship graph (GRG) given by Gross et al. (Adv. in App.","Math., 95:65-95, 2018).","We provide a sufficient condition for CCG to be connected.","We also show that the connectedness and completeness of CCG are preserved under surjective morphisms between neural codes defined by A. Jeffs (SIAM J. on App.","Alg. and Geo., 4(1):99-122,2020).","Further, we show that if CCG of any neural code $\\mathcal{C}$ is complete with $|\\mathcal{C}|=m$, then $\\mathcal{C} \\cong \\{\\emptyset,1,12,\\dots,123\\cdots m\\}$ as neural codes.","We also prove that a code whose CCG is complete is open convex.","Later, we show that if a code $\\mathcal{C}$ with $|\\mathcal{C}|>3$ has its CCG to be connected 2-regular then $|\\mathcal{C}| $ is even.","The GRG was defined only for degree two neural codes using the canonical forms of its neural ideal.","We first define GRG for any neural code.","Then, we show the behaviour of GRGs under the various elementary code maps.","At last, we compare these two graphs for certain classes of codes and see their properties."],"url":"http://arxiv.org/abs/2403.17548v1","category":"math.CO"}
{"created":"2024-03-26 09:51:43","title":"Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining","abstract":"The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools of text mining and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, text mining techniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances. Finally, topic modeling is used to examine key components and their relative significance in job descriptions. By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals. This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals.","sentences":["The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals.","We use innovative methods and tools of text mining and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics.","The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, text mining techniques, and an innovative measure, namely, the Semantic Brand Score.","We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics.","In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances.","Finally, topic modeling is used to examine key components and their relative significance in job descriptions.","By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals.","This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals."],"url":"http://arxiv.org/abs/2403.17546v1","category":"cs.CL"}
{"created":"2024-03-26 09:44:57","title":"VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts","abstract":"Despite the considerable attention given to the questions of \\textit{how much} and \\textit{how to} explore in deep reinforcement learning, the investigation into \\textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \\textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a novel approach for efficient exploration timing. Experimental results on the Atari suite demonstrate the superiority of our strategy over traditional methods such as $\\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like Noisy Nets.","sentences":["Despite the considerable attention given to the questions of \\textit{how much} and \\textit{how to} explore in deep reinforcement learning, the investigation into \\textit{when} to explore remains relatively less researched.","While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains.","The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains.","The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state.","In this paper, we propose to leverage the agent's internal state to decide \\textit{when} to explore, addressing the shortcomings of blind switching mechanisms.","We present Value Discrepancy and State Counts through homeostasis (VDSC), a novel approach for efficient exploration timing.","Experimental results on the Atari suite demonstrate the superiority of our strategy over traditional methods such as $\\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like Noisy Nets."],"url":"http://arxiv.org/abs/2403.17542v1","category":"cs.LG"}
{"created":"2024-03-26 09:44:34","title":"WordRobe: Text-Guided Generation of Textured 3D Garments","abstract":"In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures. We propose \"WordRobe\", a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts. We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation. Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing. For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods. We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study. The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing.","sentences":["In this paper, we tackle a new and challenging problem of text-driven generation of 3D garments with high-quality textures.","We propose \"WordRobe\", a novel framework for the generation of unposed & textured 3D garment meshes from user-friendly text prompts.","We achieve this by first learning a latent representation of 3D garments using a novel coarse-to-fine training strategy and a loss for latent disentanglement, promoting better latent interpolation.","Subsequently, we align the garment latent space to the CLIP embedding space in a weakly supervised manner, enabling text-driven 3D garment generation and editing.","For appearance modeling, we leverage the zero-shot generation capability of ControlNet to synthesize view-consistent texture maps in a single feed-forward inference step, thereby drastically decreasing the generation time as compared to existing methods.","We demonstrate superior performance over current SOTAs for learning 3D garment latent space, garment interpolation, and text-driven texture synthesis, supported by quantitative evaluation and qualitative user study.","The unposed 3D garment meshes generated using WordRobe can be directly fed to standard cloth simulation & animation pipelines without any post-processing."],"url":"http://arxiv.org/abs/2403.17541v1","category":"cs.CV"}
{"created":"2024-03-26 09:43:08","title":"A misleading naming convention: de Sitter `tachyonic' scalar fields","abstract":"We revisit the so-called `tachyonic' scalar fields in de Sitter (dS) spacetime. Through a rigorous group-theoretical analysis, we argue that labeling these fields as `tachyonic' lacks physical relevance, frequently resulting in misinterpretations in scholarly discourse, such as the misconception of faster-than-light travel associated with the corresponding particles.","sentences":["We revisit the so-called `tachyonic' scalar fields in de Sitter (dS) spacetime.","Through a rigorous group-theoretical analysis, we argue that labeling these fields as `tachyonic' lacks physical relevance, frequently resulting in misinterpretations in scholarly discourse, such as the misconception of faster-than-light travel associated with the corresponding particles."],"url":"http://arxiv.org/abs/2403.17539v1","category":"gr-qc"}
{"created":"2024-03-26 09:41:21","title":"ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler","abstract":"State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation study demonstrates that parameter-efficient fine-tuning requires less than 6% of training data to yield comparable performance with traditional full-weight fine-tuning.","sentences":["State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications.","Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks.","This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples.","We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work.","A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points.","Additionally, our in-depth ablation study demonstrates that parameter-efficient fine-tuning requires less than 6% of training data to yield comparable performance with traditional full-weight fine-tuning."],"url":"http://arxiv.org/abs/2403.17536v1","category":"cs.CL"}
{"created":"2024-03-26 09:36:59","title":"KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion","abstract":"The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs. To tackle the misordering issue, we develop a knowledge-guided interactive training method that enhances the identification and ranking of candidates. To address the omission issue, we design a knowledge-augmented constrained inference method that enables contextual prompting and controlled generation, so as to obtain valid rankings. Experimental results show that KG-GenRe achieves state-of-the-art performance on four datasets, with gains of up to 6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and 9.0% and 11.1% compared to that without re-ranking. Extensive analysis demonstrates the effectiveness of components in KG-GenRe.","sentences":["The goal of knowledge graph completion (KGC) is to predict missing facts among entities.","Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate.","Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems.","Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities.","However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission.","To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC.","To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs.","To tackle the misordering issue, we develop a knowledge-guided interactive training method that enhances the identification and ranking of candidates.","To address the omission issue, we design a knowledge-augmented constrained inference method that enables contextual prompting and controlled generation, so as to obtain valid rankings.","Experimental results show that KG-GenRe achieves state-of-the-art performance on four datasets, with gains of up to 6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and 9.0% and 11.1% compared to that without re-ranking.","Extensive analysis demonstrates the effectiveness of components in KG-GenRe."],"url":"http://arxiv.org/abs/2403.17532v1","category":"cs.AI"}
{"created":"2024-03-26 09:36:26","title":"Design and Preliminary Evaluation of a Torso Stabiliser for Individuals with Spinal Cord Injury","abstract":"Spinal cord injuries (SCIs) generally result in sensory and mobility impairments, with torso instability being particularly debilitating. Existing torso stabilisers are often rigid and restrictive. This paper presents an early investigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso stabiliser inspired by devices such as centrifugal clutches and seat-belt mechanisms. Firstly, the paper presents a motion-capture (MoCap) and OpenSim-based kinematic analysis of the cable-based system to understand requisite device characteristics. The simulated evaluation resulted in the cable-based device to require 55-60cm of unrestricted travel, and to lock at a threshold cable velocity of 80-100cm/sec. Next, the developed 1-DoF device is introduced. The proposed mechanical device is transparent during activities of daily living, and transitions to compliant blocking when incipient fall is detected. Prototype behaviour was then validated using a MoCap-based kinematic analysis to verify non-restrictive movement, reliable transition to blocking, and compliance of the blocking.","sentences":["Spinal cord injuries (SCIs) generally result in sensory and mobility impairments, with torso instability being particularly debilitating.","Existing torso stabilisers are often rigid and restrictive.","This paper presents an early investigation into a non-restrictive 1 degree-of-freedom (DoF) mechanical torso stabiliser inspired by devices such as centrifugal clutches and seat-belt mechanisms.","Firstly, the paper presents a motion-capture (MoCap) and OpenSim-based kinematic analysis of the cable-based system to understand requisite device characteristics.","The simulated evaluation resulted in the cable-based device to require 55-60cm of unrestricted travel, and to lock at a threshold cable velocity of 80-100cm/sec.","Next, the developed 1-DoF device is introduced.","The proposed mechanical device is transparent during activities of daily living, and transitions to compliant blocking when incipient fall is detected.","Prototype behaviour was then validated using a MoCap-based kinematic analysis to verify non-restrictive movement, reliable transition to blocking, and compliance of the blocking."],"url":"http://arxiv.org/abs/2403.17531v1","category":"cs.RO"}
{"created":"2024-03-26 09:36:20","title":"Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification","abstract":"Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the proposed approach through a series of experiments exploring several backbones, as well as diverse pre-training and fine-tuning schemes, on two distinct medical tasks, i.e., classification of prostate cancer aggressiveness from MRI data and classification of breast cancer malignity from microscopic images. Results: Our results indicate that the proposed approach consistently yields superior performance w.r.t. ablation experiments, maintaining competitiveness even when a distribution shift between training and evaluation data occurs. Conclusion: Extensive experiments demonstrate the effectiveness and wide applicability of the proposed approach. We hope that this work will add another solution to the arsenal of addressing learning issues in data-scarce imaging domains.","sentences":["Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data.","We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes.","Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks.","We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level.","This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing.","We demonstrate the effectiveness of the proposed approach through a series of experiments exploring several backbones, as well as diverse pre-training and fine-tuning schemes, on two distinct medical tasks, i.e., classification of prostate cancer aggressiveness from MRI data and classification of breast cancer malignity from microscopic images.","Results:","Our results indicate that the proposed approach consistently yields superior performance w.r.t. ablation experiments, maintaining competitiveness even when a distribution shift between training and evaluation data occurs.","Conclusion: Extensive experiments demonstrate the effectiveness and wide applicability of the proposed approach.","We hope that this work will add another solution to the arsenal of addressing learning issues in data-scarce imaging domains."],"url":"http://arxiv.org/abs/2403.17530v1","category":"cs.CV"}
{"created":"2024-03-26 09:35:16","title":"Detection of Deepfake Environmental Audio","abstract":"With the ever-rising quality of deep generative models, it is increasingly important to be able to discern whether the audio data at hand have been recorded or synthesized. Although the detection of fake speech signals has been studied extensively, this is not the case for the detection of fake environmental audio.   We propose a simple and efficient pipeline for detecting fake environmental sounds based on the CLAP audio embedding. We evaluate this detector using audio data from the 2023 DCASE challenge task on Foley sound synthesis.   Our experiments show that fake sounds generated by 44 state-of-the-art synthesizers can be detected on average with 98% accuracy. We show that using an audio embedding learned on environmental audio is beneficial over a standard VGGish one as it provides a 10% increase in detection performance. Informal listening to Incorrect Negative examples demonstrates audible features of fake sounds missed by the detector such as distortion and implausible background noise.","sentences":["With the ever-rising quality of deep generative models, it is increasingly important to be able to discern whether the audio data at hand have been recorded or synthesized.","Although the detection of fake speech signals has been studied extensively, this is not the case for the detection of fake environmental audio.   ","We propose a simple and efficient pipeline for detecting fake environmental sounds based on the CLAP audio embedding.","We evaluate this detector using audio data from the 2023 DCASE challenge task on Foley sound synthesis.   ","Our experiments show that fake sounds generated by 44 state-of-the-art synthesizers can be detected on average with 98% accuracy.","We show that using an audio embedding learned on environmental audio is beneficial over a standard VGGish one as it provides a 10% increase in detection performance.","Informal listening to Incorrect Negative examples demonstrates audible features of fake sounds missed by the detector such as distortion and implausible background noise."],"url":"http://arxiv.org/abs/2403.17529v1","category":"cs.SD"}
{"created":"2024-03-26 09:26:12","title":"Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation","abstract":"The drawing order of a sketch records how it is created stroke-by-stroke by a human being. For graphic sketch representation learning, recent studies have injected sketch drawing orders into graph edge construction by linking each patch to another in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since a sketch could have variants of drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for learning graphic sketch representation. Instead of injecting sketch drawings into graph edges, we embed these sequential information into graph nodes only. More specifically, each patch embedding is equipped with a sinusoidal absolute PE to highlight the sequential position in the drawing order. And its neighboring patches, ranked by the values of self-attention scores between patch embeddings, are equipped with learnable relative PEs to restore the contextual positions within a neighborhood. During message aggregation via graph convolutional networks, a node receives both semantic contents from patch embeddings and contextual patterns from PEs by its neighbors, arriving at drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis.","sentences":["The drawing order of a sketch records how it is created stroke-by-stroke by a human being.","For graphic sketch representation learning, recent studies have injected sketch drawing orders into graph edge construction by linking each patch to another in accordance to a temporal-based nearest neighboring strategy.","However, such constructed graph edges may be unreliable, since a sketch could have variants of drawings.","In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for learning graphic sketch representation.","Instead of injecting sketch drawings into graph edges, we embed these sequential information into graph nodes only.","More specifically, each patch embedding is equipped with a sinusoidal absolute PE to highlight the sequential position in the drawing order.","And its neighboring patches, ranked by the values of self-attention scores between patch embeddings, are equipped with learnable relative PEs to restore the contextual positions within a neighborhood.","During message aggregation via graph convolutional networks, a node receives both semantic contents from patch embeddings and contextual patterns from PEs by its neighbors, arriving at drawing-order-enhanced sketch representations.","Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis."],"url":"http://arxiv.org/abs/2403.17525v1","category":"cs.CV"}
{"created":"2024-03-26 09:25:57","title":"Provably Secure Disambiguating Neural Linguistic Steganography","abstract":"Recent research in provably secure neural linguistic steganography has overlooked a crucial aspect: the sender must detokenize stegotexts to avoid raising suspicion from the eavesdropper. The segmentation ambiguity problem, which arises when using language models based on subwords, leads to occasional decoding failures in all neural language steganography implementations based on these models. Current solutions to this issue involve altering the probability distribution of candidate words, rendering them incompatible with provably secure steganography. We propose a novel secure disambiguation method named SyncPool, which effectively addresses the segmentation ambiguity problem. We group all tokens with prefix relationships in the candidate pool before the steganographic embedding algorithm runs to eliminate uncertainty among ambiguous tokens. To enable the receiver to synchronize the sampling process of the sender, a shared cryptographically-secure pseudorandom number generator (CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does not change the size of the candidate pool or the distribution of tokens and thus is applicable to provably secure language steganography methods. We provide theoretical proofs and experimentally demonstrate the applicability of our solution to various languages and models, showing its potential to significantly improve the reliability and security of neural linguistic steganography systems.","sentences":["Recent research in provably secure neural linguistic steganography has overlooked a crucial aspect: the sender must detokenize stegotexts to avoid raising suspicion from the eavesdropper.","The segmentation ambiguity problem, which arises when using language models based on subwords, leads to occasional decoding failures in all neural language steganography implementations based on these models.","Current solutions to this issue involve altering the probability distribution of candidate words, rendering them incompatible with provably secure steganography.","We propose a novel secure disambiguation method named SyncPool, which effectively addresses the segmentation ambiguity problem.","We group all tokens with prefix relationships in the candidate pool before the steganographic embedding algorithm runs to eliminate uncertainty among ambiguous tokens.","To enable the receiver to synchronize the sampling process of the sender, a shared cryptographically-secure pseudorandom number generator (CSPRNG) is deployed to select a token from the ambiguity pool.","SyncPool does not change the size of the candidate pool or the distribution of tokens and thus is applicable to provably secure language steganography methods.","We provide theoretical proofs and experimentally demonstrate the applicability of our solution to various languages and models, showing its potential to significantly improve the reliability and security of neural linguistic steganography systems."],"url":"http://arxiv.org/abs/2403.17524v1","category":"cs.CR"}
{"created":"2024-03-26 09:25:25","title":"Simplified functional flow equation","abstract":"We propose to adapt the precise definition of the flowing effective action in order to obtain a functional flow equation with simple properties close to physical intuition. The simplified flow equation is invariant under local gauge transformations and suitable for both euclidean and Minkowski signature and analytic continuation. The cutoff always removes fluctuations close to zeros of the inverse full propagator. A formulation of the simplified flow equation in terms of renormalized scale invariant fields permits direct access to scaling solutions and associated fixed points. Corrections to the simplified flow equation involve a field-dependent modification of the cutoff for which we discuss a systematic expansion. Truncated solutions for a scalar field theory in four dimensions suggest a new fixed point with a field-dependent coefficient of the kinetic term.","sentences":["We propose to adapt the precise definition of the flowing effective action in order to obtain a functional flow equation with simple properties close to physical intuition.","The simplified flow equation is invariant under local gauge transformations and suitable for both euclidean and Minkowski signature and analytic continuation.","The cutoff always removes fluctuations close to zeros of the inverse full propagator.","A formulation of the simplified flow equation in terms of renormalized scale invariant fields permits direct access to scaling solutions and associated fixed points.","Corrections to the simplified flow equation involve a field-dependent modification of the cutoff for which we discuss a systematic expansion.","Truncated solutions for a scalar field theory in four dimensions suggest a new fixed point with a field-dependent coefficient of the kinetic term."],"url":"http://arxiv.org/abs/2403.17523v1","category":"hep-th"}
{"created":"2024-03-26 09:24:31","title":"Jacob's ladders, Hardy-Littlewood integral (1918) and new asymptotic functional equations for Euler's Gamma function together with the tenth equivalent of the Fermat-Wiles theorem","abstract":"In this paper new $\\Gamma$-functional is constructed upon the basis of the set of almost linear increments of the Hardy-Littlewood integral. This functional generates a $\\Gamma$-equivalent of the Fermat-Wiles theorem and also new set of factorization formulae for Euler's $\\Gamma$-function.","sentences":["In this paper new $\\Gamma$-functional is constructed upon the basis of the set of almost linear increments of the Hardy-Littlewood integral.","This functional generates a $\\Gamma$-equivalent of the Fermat-Wiles theorem and also new set of factorization formulae for Euler's $\\Gamma$-function."],"url":"http://arxiv.org/abs/2403.17522v1","category":"math.NT"}
{"created":"2024-03-26 09:22:37","title":"Boosting Adversarial Training via Fisher-Rao Norm-based Regularization","abstract":"Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training process. Building upon this observation, we propose a novel regularization framework, called Logit-Oriented Adversarial Training (LOAT), which can mitigate the trade-off between robustness and accuracy while imposing only a negligible increase in computational overhead. Our extensive experiments demonstrate that the proposed regularization strategy can boost the performance of the prevalent adversarial training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT, across various network architectures. Our code will be available at https://github.com/TrustAI/LOAT.","sentences":["Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks.","Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem.","This paper attempts to resolve this issue through the lens of model complexity.","First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron.","Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training.","Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training process.","Building upon this observation, we propose a novel regularization framework, called Logit-Oriented Adversarial Training (LOAT), which can mitigate the trade-off between robustness and accuracy while imposing only a negligible increase in computational overhead.","Our extensive experiments demonstrate that the proposed regularization strategy can boost the performance of the prevalent adversarial training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT, across various network architectures.","Our code will be available at https://github.com/TrustAI/LOAT."],"url":"http://arxiv.org/abs/2403.17520v1","category":"cs.LG"}
{"created":"2024-03-26 09:20:46","title":"Active drive towards elastic spinodals","abstract":"Active renewable matter, a distinctive feature of adaptive living materials, can exhibit highly unusual mechanical responses by actively navigating the space of material parameters. In particular, it can self-drive towards elastic spinodals where the presence of inhomogeneous floppy modes, makes the matter elastically degenerate. The main effect of the implied marginality is stress localization leading to the emergence of force chains which can be actively assembled and disassembled. In this Letter we formalize the concept of spinodal states for general elastic solids and show how such extreme mechanical regimes can be actively navigated.","sentences":["Active renewable matter, a distinctive feature of adaptive living materials, can exhibit highly unusual mechanical responses by actively navigating the space of material parameters.","In particular, it can self-drive towards elastic spinodals where the presence of inhomogeneous floppy modes, makes the matter elastically degenerate.","The main effect of the implied marginality is stress localization leading to the emergence of force chains which can be actively assembled and disassembled.","In this Letter we formalize the concept of spinodal states for general elastic solids and show how such extreme mechanical regimes can be actively navigated."],"url":"http://arxiv.org/abs/2403.17517v1","category":"cond-mat.soft"}
{"created":"2024-03-26 09:18:59","title":"MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities","abstract":"Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and highlight a critical correlation: the more precisely we map brain activities to text embeddings, the better the text reconstruction results. Such insight can simplify the task of reconstructing language from brain activities for future work, emphasizing the importance of improving brain-to-text-embedding mapping techniques.","sentences":["Decoding continuous language from brain activity is a formidable yet promising field of research.","It is particularly significant for aiding people with speech disabilities to communicate through brain signals.","This field addresses the complex task of mapping brain signals to text.","The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses.","In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities.","Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores.","We further validate the proposed modules through detailed ablation studies and case analyses and highlight a critical correlation: the more precisely we map brain activities to text embeddings, the better the text reconstruction results.","Such insight can simplify the task of reconstructing language from brain activities for future work, emphasizing the importance of improving brain-to-text-embedding mapping techniques."],"url":"http://arxiv.org/abs/2403.17516v1","category":"cs.CL"}
{"created":"2024-03-26 09:18:50","title":"Prediction-sharing During Training and Inference","abstract":"Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data.","sentences":["Two firms are engaged in a competitive prediction task.","Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances.","We study data-sharing contracts between the firms.","The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both.","Our analysis proceeds on three levels.","First, we develop a general Bayesian framework that facilitates our study.","Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it.","Within these two settings we study optimal contract choice.","More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal.","Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data."],"url":"http://arxiv.org/abs/2403.17515v1","category":"econ.TH"}
{"created":"2024-03-26 09:14:25","title":"A unified framework for coarse grained molecular dynamics of proteins","abstract":"Understanding protein dynamics is crucial for elucidating their biological functions. While all-atom molecular dynamics (MD) simulations provide detailed information, coarse-grained (CG) MD simulations capture the essential collective motions of proteins at significantly lower computational cost. In this article, we present a unified framework for coarse-grained molecular dynamics simulation of proteins. Our approach utilizes a tree-structured representation of collective variables, enabling reconstruction of protein Cartesian coordinates with high fidelity. The force field is constructed using a deep neural network trained on trajectories generated from conventional all-atom MD simulations. We demonstrate the framework's effectiveness using the 168-amino protein target T1027 from CASP14. Statistical distributions of the collective variables and time series of root mean square deviation (RMSD) obtained from our coarse-grained simulations closely resemble those from all-atom MD simulations. This method is not only useful for studying the movements of complex proteins, but also has the potential to be adapted for simulating other biomolecules like DNA, RNA, and even electrolytes in batteries.","sentences":["Understanding protein dynamics is crucial for elucidating their biological functions.","While all-atom molecular dynamics (MD) simulations provide detailed information, coarse-grained (CG) MD simulations capture the essential collective motions of proteins at significantly lower computational cost.","In this article, we present a unified framework for coarse-grained molecular dynamics simulation of proteins.","Our approach utilizes a tree-structured representation of collective variables, enabling reconstruction of protein Cartesian coordinates with high fidelity.","The force field is constructed using a deep neural network trained on trajectories generated from conventional all-atom MD simulations.","We demonstrate the framework's effectiveness using the 168-amino protein target T1027 from CASP14.","Statistical distributions of the collective variables and time series of root mean square deviation (RMSD) obtained from our coarse-grained simulations closely resemble those from all-atom MD simulations.","This method is not only useful for studying the movements of complex proteins, but also has the potential to be adapted for simulating other biomolecules like DNA, RNA, and even electrolytes in batteries."],"url":"http://arxiv.org/abs/2403.17513v1","category":"physics.chem-ph"}
{"created":"2024-03-26 09:03:40","title":"SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder","abstract":"The data bottleneck has emerged as a fundamental challenge in learning based image restoration methods. Researchers have attempted to generate synthesized training data using paired or unpaired samples to address this challenge. This study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages both paired and unpaired datasets to generate realistic degraded data. Our approach is based on modeling the conditional distribution of degraded and clean images with a specially designed graphical model. Under the variational inference framework, we develop an objective function for handling both paired and unpaired data. We employ our method to generate paired training samples for real-world image denoising and super-resolution tasks. Our approach excels in the quality of synthetic degraded images compared to other unpaired and paired noise modeling methods. Furthermore, our approach demonstrates remarkable performance in downstream image restoration tasks, even with limited paired data. With more paired data, our method achieves the best performance on the SIDD dataset.","sentences":["The data bottleneck has emerged as a fundamental challenge in learning based image restoration methods.","Researchers have attempted to generate synthesized training data using paired or unpaired samples to address this challenge.","This study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages both paired and unpaired datasets to generate realistic degraded data.","Our approach is based on modeling the conditional distribution of degraded and clean images with a specially designed graphical model.","Under the variational inference framework, we develop an objective function for handling both paired and unpaired data.","We employ our method to generate paired training samples for real-world image denoising and super-resolution tasks.","Our approach excels in the quality of synthetic degraded images compared to other unpaired and paired noise modeling methods.","Furthermore, our approach demonstrates remarkable performance in downstream image restoration tasks, even with limited paired data.","With more paired data, our method achieves the best performance on the SIDD dataset."],"url":"http://arxiv.org/abs/2403.17502v1","category":"cs.CV"}
{"created":"2024-03-26 08:59:37","title":"Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification","abstract":"Graph representation learning is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen graph structures during inference. In recent years, graph neural networks (GNNs) have emerged as powerful graph models for inductive learning tasks such as node classification, whereas they typically heavily rely on the annotated nodes under a fully supervised training setting. Compared with the GNN-based methods, variational graph auto-encoders (VGAEs) are known to be more generalizable to capture the internal structural information of graphs independent of node labels and have achieved prominent performance on multiple unsupervised learning tasks. However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a supervised manner and avoiding over-fitting the proximity information of graphs. To solve these problems and improve the model performance of VGAEs for inductive graph representation learning, in this work, we propose the Self-Label Augmented VGAE model. To leverage the label information for training, our model takes node labels as one-hot encoded inputs and then performs label reconstruction in model training. To overcome the scarcity problem of node labels for semi-supervised settings, we further propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels generated by our model with a node-wise masking approach to enhance the label information. Experiments on benchmark inductive learning graph datasets verify that our proposed model archives promising results on node classification with particular superiority under semi-supervised learning settings.","sentences":["Graph representation learning is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen graph structures during inference.","In recent years, graph neural networks (GNNs) have emerged as powerful graph models for inductive learning tasks such as node classification, whereas they typically heavily rely on the annotated nodes under a fully supervised training setting.","Compared with the GNN-based methods, variational graph auto-encoders (VGAEs) are known to be more generalizable to capture the internal structural information of graphs independent of node labels and have achieved prominent performance on multiple unsupervised learning tasks.","However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a supervised manner and avoiding over-fitting the proximity information of graphs.","To solve these problems and improve the model performance of VGAEs for inductive graph representation learning, in this work, we propose the Self-Label Augmented VGAE model.","To leverage the label information for training, our model takes node labels as one-hot encoded inputs and then performs label reconstruction in model training.","To overcome the scarcity problem of node labels for semi-supervised settings, we further propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels generated by our model with a node-wise masking approach to enhance the label information.","Experiments on benchmark inductive learning graph datasets verify that our proposed model archives promising results on node classification with particular superiority under semi-supervised learning settings."],"url":"http://arxiv.org/abs/2403.17500v1","category":"cs.LG"}
{"created":"2024-03-26 08:59:36","title":"Time-dependent nuclear energy-density functional theory toolkit for neutron star crust: Dynamics of a nucleus in a neutron superfluid","abstract":"We present a new numerical tool designed to probe the dense layers of neutron star crusts. It is based on the Time-Dependent Hartree-Fock-Bogoliubov theory with generalized Skyrme nuclear energy density functionals, such as the Brussels-Montreal ones. We use it to study the time evolution of a nucleus accelerating through superfluid neutron medium in the inner crust of a neutron star. We extract an effective mass in the low velocity limit. We observe a threshold velocity and specify mechanisms of dissipation: phonon emission, Cooper pairs breaking, and vortex rings creation. The microscopic effects we study have impact on neutron star. Moreover, the mechanisms, we described, are general and apply also to other fermionic superfluid mixtures like liquid helium, or ultracold gases.","sentences":["We present a new numerical tool designed to probe the dense layers of neutron star crusts.","It is based on the Time-Dependent Hartree-Fock-Bogoliubov theory with generalized Skyrme nuclear energy density functionals, such as the Brussels-Montreal ones.","We use it to study the time evolution of a nucleus accelerating through superfluid neutron medium in the inner crust of a neutron star.","We extract an effective mass in the low velocity limit.","We observe a threshold velocity and specify mechanisms of dissipation: phonon emission, Cooper pairs breaking, and vortex rings creation.","The microscopic effects we study have impact on neutron star.","Moreover, the mechanisms, we described, are general and apply also to other fermionic superfluid mixtures like liquid helium, or ultracold gases."],"url":"http://arxiv.org/abs/2403.17499v1","category":"nucl-th"}
{"created":"2024-03-26 08:59:08","title":"Surface figure correction using differential deposition of WSi2","abstract":"The surface figure of an x-ray mirror was improved by differential deposition of WSi2 layers. DC magnetron sputtering through beam-defining apertures was applied on moving substrates to generate thin films with arbitrary longitudinal thickness variations. The required velocity profiles were calculated using a deconvolution algorithm. Height errors were evaluated after each correction iteration using offline visible light surface metrology. WSi2 was selected as a promising material since it conserves the initial substrate surface roughness and limits the film stress to acceptable levels. On a 300 mm long flat Si mirror the shape error was reduced to less than 0.2 nm RMS.","sentences":["The surface figure of an x-ray mirror was improved by differential deposition of WSi2 layers.","DC magnetron sputtering through beam-defining apertures was applied on moving substrates to generate thin films with arbitrary longitudinal thickness variations.","The required velocity profiles were calculated using a deconvolution algorithm.","Height errors were evaluated after each correction iteration using offline visible light surface metrology.","WSi2 was selected as a promising material since it conserves the initial substrate surface roughness and limits the film stress to acceptable levels.","On a 300 mm long flat Si mirror the shape error was reduced to less than 0.2 nm RMS."],"url":"http://arxiv.org/abs/2403.17498v1","category":"physics.app-ph"}
{"created":"2024-03-26 08:52:54","title":"Quantum Optimization for the Future Energy Grid: Summary and Quantum Utility Prospects","abstract":"In this project summary paper, we summarize the key results and use-cases explored in the German Federal Ministry of Education and Research (BMBF) funded project \"Q-GRID\" which aims to assess potential quantum utility optimization applications in the electrical grid. The project focuses on two layers of optimization problems relevant to decentralized energy generation and transmission as well as novel energy transportation/exchange methods such as Peer-2-Peer energy trading and microgrid formation. For select energy grid optimization problems, we demonstrate exponential classical optimizer runtime scaling even for small problem instances, and present initial findings that variational quantum algorithms such as QAOA and hybrid quantum annealing solvers may provide more favourable runtime scaling to obtain similar solution quality. These initial results suggest that quantum computing may be a key enabling technology in the future energy transition insofar that they may be able to solve business problems which are already challenging at small problem instance sizes.","sentences":["In this project summary paper, we summarize the key results and use-cases explored in the German Federal Ministry of Education and Research (BMBF) funded project \"Q-GRID\" which aims to assess potential quantum utility optimization applications in the electrical grid.","The project focuses on two layers of optimization problems relevant to decentralized energy generation and transmission as well as novel energy transportation/exchange methods such as Peer-2-Peer energy trading and microgrid formation.","For select energy grid optimization problems, we demonstrate exponential classical optimizer runtime scaling even for small problem instances, and present initial findings that variational quantum algorithms such as QAOA and hybrid quantum annealing solvers may provide more favourable runtime scaling to obtain similar solution quality.","These initial results suggest that quantum computing may be a key enabling technology in the future energy transition insofar that they may be able to solve business problems which are already challenging at small problem instance sizes."],"url":"http://arxiv.org/abs/2403.17495v1","category":"quant-ph"}
{"created":"2024-03-26 08:51:23","title":"FaultGuard: A Generative Approach to Resilient Fault Prediction in Smart Electrical Grids","abstract":"Predicting and classifying faults in electricity networks is crucial for uninterrupted provision and keeping maintenance costs at a minimum. Thanks to the advancements in the field provided by the smart grid, several data-driven approaches have been proposed in the literature to tackle fault prediction tasks. Implementing these systems brought several improvements, such as optimal energy consumption and quick restoration. Thus, they have become an essential component of the smart grid. However, the robustness and security of these systems against adversarial attacks have not yet been extensively investigated. These attacks can impair the whole grid and cause additional damage to the infrastructure, deceiving fault detection systems and disrupting restoration. In this paper, we present FaultGuard, the first framework for fault type and zone classification resilient to adversarial attacks. To ensure the security of our system, we employ an Anomaly Detection System (ADS) leveraging a novel Generative Adversarial Network training layer to identify attacks. Furthermore, we propose a low-complexity fault prediction model and an online adversarial training technique to enhance robustness. We comprehensively evaluate the framework's performance against various adversarial attacks using the IEEE13-AdvAttack dataset, which constitutes the state-of-the-art for resilient fault prediction benchmarking. Our model outclasses the state-of-the-art even without considering adversaries, with an accuracy of up to 0.958. Furthermore, our ADS shows attack detection capabilities with an accuracy of up to 1.000. Finally, we demonstrate how our novel training layers drastically increase performances across the whole framework, with a mean increase of 154% in ADS accuracy and 118% in model accuracy.","sentences":["Predicting and classifying faults in electricity networks is crucial for uninterrupted provision and keeping maintenance costs at a minimum.","Thanks to the advancements in the field provided by the smart grid, several data-driven approaches have been proposed in the literature to tackle fault prediction tasks.","Implementing these systems brought several improvements, such as optimal energy consumption and quick restoration.","Thus, they have become an essential component of the smart grid.","However, the robustness and security of these systems against adversarial attacks have not yet been extensively investigated.","These attacks can impair the whole grid and cause additional damage to the infrastructure, deceiving fault detection systems and disrupting restoration.","In this paper, we present FaultGuard, the first framework for fault type and zone classification resilient to adversarial attacks.","To ensure the security of our system, we employ an Anomaly Detection System (ADS) leveraging a novel Generative Adversarial Network training layer to identify attacks.","Furthermore, we propose a low-complexity fault prediction model and an online adversarial training technique to enhance robustness.","We comprehensively evaluate the framework's performance against various adversarial attacks using the IEEE13-AdvAttack dataset, which constitutes the state-of-the-art for resilient fault prediction benchmarking.","Our model outclasses the state-of-the-art even without considering adversaries, with an accuracy of up to 0.958.","Furthermore, our ADS shows attack detection capabilities with an accuracy of up to 1.000.","Finally, we demonstrate how our novel training layers drastically increase performances across the whole framework, with a mean increase of 154% in ADS accuracy and 118% in model accuracy."],"url":"http://arxiv.org/abs/2403.17494v1","category":"cs.CR"}
{"created":"2024-03-26 08:47:23","title":"DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation","abstract":"The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts. However, such models face problems of generalization and expensive training costs. The use of large language models (LLMs) to solve the task of generating paper abstracts saves the cost of model training. However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs. In this paper, we propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages of the existing GoT prompt approach, but also dynamically adjust the graph structure according to data characteristics while reducing model reasoning cost. Experimental results show that our method's cost-effectiveness in abstract generation tasks is only 43.7% to 56.4% of other multi-round query prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.","sentences":["The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts.","However, such models face problems of generalization and expensive training costs.","The use of large language models (LLMs) to solve the task of generating paper abstracts saves the cost of model training.","However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs.","In this paper, we propose a Dynamic Graph of Thought (DGoT).","It not only inherits the advantages of the existing GoT prompt approach, but also dynamically adjust the graph structure according to data characteristics while reducing model reasoning cost.","Experimental results show that our method's cost-effectiveness in abstract generation tasks is only 43.7% to 56.4% of other multi-round query prompt approaches.","Our code is available at https://github.com/JayceNing/DGoT."],"url":"http://arxiv.org/abs/2403.17491v1","category":"cs.CL"}
{"created":"2024-03-26 08:42:57","title":"Covariant reconstruction of forms from their invariants","abstract":"Let $K$ be an algebraically closed field of characteristic $0$ and $f$ be a homogeneous polynomial. We provide an explicit algorithm, which, given the invariants of a generic $f$ under the action of $\\mathrm{GL}_n(K)$ returns a polynomial in the orbit of $f$. We derive a specific algorithm for the reconstruction of a generic non-hyperelliptic curve of genus 4 from its invariants, as well as a direct reconstruction algorithm for generic non-hyperelliptic curves of genus 3 from their Dixmier-Ohno invariants.","sentences":["Let $K$ be an algebraically closed field of characteristic $0$ and $f$ be a homogeneous polynomial.","We provide an explicit algorithm, which, given the invariants of a generic $f$ under the action of $\\mathrm{GL}_n(K)$ returns a polynomial in the orbit of $f$. We derive a specific algorithm for the reconstruction of a generic non-hyperelliptic curve of genus 4 from its invariants, as well as a direct reconstruction algorithm for generic non-hyperelliptic curves of genus 3 from their Dixmier-Ohno invariants."],"url":"http://arxiv.org/abs/2403.17490v1","category":"math.AC"}
{"created":"2024-03-26 08:33:44","title":"Adaptive Bayesian Structure Learning of DAGs With Non-conjugate Prior","abstract":"Directed Acyclic Graphs (DAGs) are solid structures used to describe and infer the dependencies among variables in multivariate scenarios. Having a thorough comprehension of the accurate DAG-generating model is crucial for causal discovery and estimation. Our work suggests utilizing a non-conjugate prior for Gaussian DAG structure learning to enhance the posterior probability. We employ the idea of using the Bessel function to address the computational burden, providing faster MCMC computation compared to the use of conjugate priors. In addition, our proposal exhibits a greater rate of adaptation when compared to the conjugate prior, specifically for the inclusion of nodes in the DAG-generating model. Simulation studies demonstrate the superior accuracy of DAG learning, and we obtain the same maximum a posteriori and median probability model estimate for the AML data, using the non-conjugate prior.","sentences":["Directed Acyclic Graphs (DAGs) are solid structures used to describe and infer the dependencies among variables in multivariate scenarios.","Having a thorough comprehension of the accurate DAG-generating model is crucial for causal discovery and estimation.","Our work suggests utilizing a non-conjugate prior for Gaussian DAG structure learning to enhance the posterior probability.","We employ the idea of using the Bessel function to address the computational burden, providing faster MCMC computation compared to the use of conjugate priors.","In addition, our proposal exhibits a greater rate of adaptation when compared to the conjugate prior, specifically for the inclusion of nodes in the DAG-generating model.","Simulation studies demonstrate the superior accuracy of DAG learning, and we obtain the same maximum a posteriori and median probability model estimate for the AML data, using the non-conjugate prior."],"url":"http://arxiv.org/abs/2403.17489v1","category":"stat.ME"}
{"created":"2024-03-26 08:33:19","title":"Dynamical origin of neutrino masses and dark matter from a new confining sector","abstract":"A dynamical mechanism, based on a confining non-abelian dark symmetry, which generates Majorana masses for hypercharge-less fermions, is proposed. We apply it to the inverse seesaw scenario, which allows to generate light neutrino masses from the interplay of TeV-scale Pseudo-Dirac mass terms and a small explicit breaking of lepton number. A single generation of vector-like dark quarks, transforming under a $\\text{SU}(3)_\\text{D}$ gauge symmetry, is coupled to a real singlet scalar, which serves as a portal between the dark quark condensate and three generations of heavy sterile neutrinos. Such a dark sector and the Standard Model (SM) are kept in thermal equilibrium with each other via sizeable Yukawa couplings to the heavy neutrinos. In this framework the lightest dark baryon, which has spin $3/2$ and is stabilized at the renormalizable level by an accidental dark baryon number symmetry, can account for the observed relic density via thermal freeze-out from annihilations into the lightest dark mesons. These mesons in turn decay to heavy neutrinos, which produce SM final states upon decay. This model may be probed by next generation neutrino telescopes via neutrino lines produced from dark matter annihilations.","sentences":["A dynamical mechanism, based on a confining non-abelian dark symmetry, which generates Majorana masses for hypercharge-less fermions, is proposed.","We apply it to the inverse seesaw scenario, which allows to generate light neutrino masses from the interplay of TeV-scale Pseudo-Dirac mass terms and a small explicit breaking of lepton number.","A single generation of vector-like dark quarks, transforming under a $\\text{SU}(3)_\\text{D}$ gauge symmetry, is coupled to a real singlet scalar, which serves as a portal between the dark quark condensate and three generations of heavy sterile neutrinos.","Such a dark sector and the Standard Model (SM) are kept in thermal equilibrium with each other via sizeable Yukawa couplings to the heavy neutrinos.","In this framework the lightest dark baryon, which has spin $3/2$ and is stabilized at the renormalizable level by an accidental dark baryon number symmetry, can account for the observed relic density via thermal freeze-out from annihilations into the lightest dark mesons.","These mesons in turn decay to heavy neutrinos, which produce SM final states upon decay.","This model may be probed by next generation neutrino telescopes via neutrino lines produced from dark matter annihilations."],"url":"http://arxiv.org/abs/2403.17488v1","category":"hep-ph"}
{"created":"2024-03-26 08:32:40","title":"Greybody Factors Imprinted on Black Hole Ringdowns. II. Merging Binary Black Holes","abstract":"The spectral amplitude of the merger-ringdown gravitational wave (GW) emitted by a comparable mass-ratio black hole merger is modeled by the greybody factor of the remnant black hole. Our model does not include fitting parameters except for a single overall spectral amplitude. We perform the mass-spin inference from the SXS data without introducing fitting parameters and without tuning the data range of each SXS template. Also, we find that the exponential damping in the ringdown spectral amplitude can be modeled well with the exponential damping in the greybody factor at high frequencies. Based on the findings, we propose a conjecture that the light ring of the remnant black hole, which sources the ringdown, forms as early as during the merger stage. We discuss the formation of the light ring in the static binary solution as a first step towards the understanding of how the separation of merging black holes may affect the formation of the light ring.","sentences":["The spectral amplitude of the merger-ringdown gravitational wave (GW) emitted by a comparable mass-ratio black hole merger is modeled by the greybody factor of the remnant black hole.","Our model does not include fitting parameters except for a single overall spectral amplitude.","We perform the mass-spin inference from the SXS data without introducing fitting parameters and without tuning the data range of each SXS template.","Also, we find that the exponential damping in the ringdown spectral amplitude can be modeled well with the exponential damping in the greybody factor at high frequencies.","Based on the findings, we propose a conjecture that the light ring of the remnant black hole, which sources the ringdown, forms as early as during the merger stage.","We discuss the formation of the light ring in the static binary solution as a first step towards the understanding of how the separation of merging black holes may affect the formation of the light ring."],"url":"http://arxiv.org/abs/2403.17487v1","category":"gr-qc"}
{"created":"2024-03-26 08:30:10","title":"Hairy black holes in extended Einstein-Maxwell-scalar theories with magnetic charge and kinetic couplings","abstract":"We study black hole (BH) solutions in extended Einstein-Maxwell-scalar theories which is classified in a subclass of the $U(1)$ gauge-invariant scalar-vector-tensor theories. The scalar field is coupled to the vector field, which has electric and magnetic charges. For the static and spherically symmetric spacetime, we investigate modifications to the Reissner-Nordstr\\\"{o}m solutions focusing on the three types of scalar-vector interactions, including derivative couplings. We solve the field equations analytically in two asymptotic regions which are the vicinity of the BH horizon and the spatial infinity, and clarify the condition for the existence of scalar hair. To understand the behaviors of solutions in intermediate scales, the field equations are integrated numerically for concrete models with different types of couplings. We find new hairy BH solutions with scalar hair in the presence of magnetic charge and kinetic coupling. The magnetic charge plays an important role in distinguishing hairy BH solutions originated from three types of different interactions at a large coupling limit.","sentences":["We study black hole (BH) solutions in extended Einstein-Maxwell-scalar theories which is classified in a subclass of the $U(1)$ gauge-invariant scalar-vector-tensor theories.","The scalar field is coupled to the vector field, which has electric and magnetic charges.","For the static and spherically symmetric spacetime, we investigate modifications to the Reissner-Nordstr\\\"{o}m solutions focusing on the three types of scalar-vector interactions, including derivative couplings.","We solve the field equations analytically in two asymptotic regions which are the vicinity of the BH horizon and the spatial infinity, and clarify the condition for the existence of scalar hair.","To understand the behaviors of solutions in intermediate scales, the field equations are integrated numerically for concrete models with different types of couplings.","We find new hairy BH solutions with scalar hair in the presence of magnetic charge and kinetic coupling.","The magnetic charge plays an important role in distinguishing hairy BH solutions originated from three types of different interactions at a large coupling limit."],"url":"http://arxiv.org/abs/2403.17484v1","category":"gr-qc"}
{"created":"2024-03-26 08:27:25","title":"Double Holography of Entangled Universes","abstract":"We employ double holography to examine a system of two entangled gravitating universes that live on two codimension-one branes in an asymptotically AdS$_3$ spacetime with two disjoint conformal boundaries. There are distinct brane configurations depending on the temperature of the thermofield double (TFD) state between the left and right systems. The topology transition between two branes is naturally identified with the emergence of an Einstein-Rosen bridge connecting the two entangled universes. This doubly holographic construction offers a holographic perspective on gravitational collapse and black hole formation in brane universes. Through this holographic framework, we analyze the quantum information structure of the two gravitating universes. Specifically, we calculate the mutual information between defects present in the boundary theories on the left and right sides. Furthermore, we investigate the decoupling process in the Hayden-Preskill protocol applied to the two copies of the defect field theory and discuss the interpretation of the Yoshida-Kitaev decoding protocol.","sentences":["We employ double holography to examine a system of two entangled gravitating universes that live on two codimension-one branes in an asymptotically AdS$_3$ spacetime with two disjoint conformal boundaries.","There are distinct brane configurations depending on the temperature of the thermofield double (TFD) state between the left and right systems.","The topology transition between two branes is naturally identified with the emergence of an Einstein-Rosen bridge connecting the two entangled universes.","This doubly holographic construction offers a holographic perspective on gravitational collapse and black hole formation in brane universes.","Through this holographic framework, we analyze the quantum information structure of the two gravitating universes.","Specifically, we calculate the mutual information between defects present in the boundary theories on the left and right sides.","Furthermore, we investigate the decoupling process in the Hayden-Preskill protocol applied to the two copies of the defect field theory and discuss the interpretation of the Yoshida-Kitaev decoding protocol."],"url":"http://arxiv.org/abs/2403.17483v1","category":"hep-th"}
{"created":"2024-03-26 17:57:20","title":"FastCAR: Fast Classification And Regression Multi-Task Learning via Task Consolidation for Modelling a Continuous Property Variable of Object Classes","abstract":"FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite task heterogeneity with only subtle correlation. It addresses object classification and continuous property variable regression, a crucial use case in science and engineering. FastCAR involves a labeling transformation approach that can be used with a single-task regression network architecture. FastCAR outperforms traditional MTL model families, parametrized in the landscape of architecture and loss weighting schemes, when learning of both tasks are collectively considered (classification accuracy of 99.54%, regression mean absolute percentage error of 2.3%). The experiments performed used an Advanced Steel Property dataset contributed by us. The dataset comprises 4536 images of 224x224 pixels, annotated with object classes and hardness properties that take continuous values. With the labeling transformation and single-task regression network architecture, FastCAR achieves reduced latency and time efficiency.","sentences":["FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite task heterogeneity with only subtle correlation.","It addresses object classification and continuous property variable regression, a crucial use case in science and engineering.","FastCAR involves a labeling transformation approach that can be used with a single-task regression network architecture.","FastCAR outperforms traditional MTL model families, parametrized in the landscape of architecture and loss weighting schemes, when learning of both tasks are collectively considered (classification accuracy of 99.54%, regression mean absolute percentage error of 2.3%).","The experiments performed used an Advanced Steel Property dataset contributed by us.","The dataset comprises 4536 images of 224x224 pixels, annotated with object classes and hardness properties that take continuous values.","With the labeling transformation and single-task regression network architecture, FastCAR achieves reduced latency and time efficiency."],"url":"http://arxiv.org/abs/2403.17926v1","category":"cs.CV"}
{"created":"2024-03-26 17:54:05","title":"Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes","abstract":"This paper presents two algorithms for multi-agent dynamic coverage in spatiotemporal environments, where the coverage algorithms are informed by the method of data assimilation. In particular, we show that by considering the information assimilation algorithm, here a Numerical Gaussian Process Kalman Filter, the influence of measurements taken at one position on the uncertainty of the estimate at another location can be computed. We use this relationship to propose new coverage algorithms. Furthermore, we show that the controllers naturally extend to the multi-agent context, allowing for a distributed-control central-information paradigm for multi-agent coverage. Finally, we demonstrate the algorithms through a realistic simulation of a team of UAVs collecting wind data over a region in Austria.","sentences":["This paper presents two algorithms for multi-agent dynamic coverage in spatiotemporal environments, where the coverage algorithms are informed by the method of data assimilation.","In particular, we show that by considering the information assimilation algorithm, here a Numerical Gaussian Process Kalman Filter, the influence of measurements taken at one position on the uncertainty of the estimate at another location can be computed.","We use this relationship to propose new coverage algorithms.","Furthermore, we show that the controllers naturally extend to the multi-agent context, allowing for a distributed-control central-information paradigm for multi-agent coverage.","Finally, we demonstrate the algorithms through a realistic simulation of a team of UAVs collecting wind data over a region in Austria."],"url":"http://arxiv.org/abs/2403.17917v1","category":"eess.SY"}
{"created":"2024-03-26 16:48:13","title":"ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages","abstract":"Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models. At the same time, many benchmark datasets have become available for QA and MRC tasks. However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models. To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years. One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text. Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource.","sentences":["Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models.","At the same time, many benchmark datasets have become available for QA and MRC tasks.","However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web.","Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models.","To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America.","Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years.","One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text.","Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages.","This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource."],"url":"http://arxiv.org/abs/2403.17859v1","category":"cs.CL"}
{"created":"2024-03-26 16:13:55","title":"Learning the Optimal Power Flow: Environment Design Matters","abstract":"To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.","sentences":["To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach.","However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment.","In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice.","In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance.","Further, we derive some first recommendations regarding the choice of these design decisions.","The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field."],"url":"http://arxiv.org/abs/2403.17831v1","category":"cs.LG"}
{"created":"2024-03-26 16:10:21","title":"Assessment of Multimodal Large Language Models in Alignment with Human Values","abstract":"Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations. To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations. Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives. Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.","sentences":["Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh).","However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations.","To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations.","Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle.","We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives.","Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field."],"url":"http://arxiv.org/abs/2403.17830v1","category":"cs.CV"}
{"created":"2024-03-26 14:57:30","title":"Spectra of correlators in the relaxation time approximation of kinetic theory","abstract":"The relaxation time approximation (RTA) of the kinetic Boltzmann equation is likely the simplest window into the microscopic properties of collective real-time transport. Within this framework, we analytically compute all retarded two-point Green's functions of the energy-momentum tensor and a conserved $U(1)$ current in thermal states with classical massless particles (a `CFT') at non-zero density, and in the absence and presence of broken translational symmetry. This is done in $2+1$ and $3+1$ dimensions. RTA allows a full explicit analysis of the analytic structure of different correlators (poles versus branch cuts) and the transport properties that they imply (the thermoelectric conductivities, and the hydrodynamic, quasihydrodynamic and gapped mode dispersion relations). Our inherently weakly coupled analysis thereby also enables a direct comparison with previously known strongly coupled results in holographic CFTs dual to the Einstein-Maxwell-axion theories.","sentences":["The relaxation time approximation (RTA) of the kinetic Boltzmann equation is likely the simplest window into the microscopic properties of collective real-time transport.","Within this framework, we analytically compute all retarded two-point Green's functions of the energy-momentum tensor and a conserved $U(1)$ current in thermal states with classical massless particles (a `CFT') at non-zero density, and in the absence and presence of broken translational symmetry.","This is done in $2+1$ and $3+1$ dimensions.","RTA allows a full explicit analysis of the analytic structure of different correlators (poles versus branch cuts) and the transport properties that they imply (the thermoelectric conductivities, and the hydrodynamic, quasihydrodynamic and gapped mode dispersion relations).","Our inherently weakly coupled analysis thereby also enables a direct comparison with previously known strongly coupled results in holographic CFTs dual to the Einstein-Maxwell-axion theories."],"url":"http://arxiv.org/abs/2403.17769v1","category":"hep-th"}
{"created":"2024-03-26 14:44:00","title":"Optimal Euclidean Tree Covers","abstract":"A $(1+\\varepsilon)\\textit{-stretch tree cover}$ of a metric space is a collection of trees, where every pair of points has a $(1+\\varepsilon)$-stretch path in one of the trees. The celebrated $\\textit{Dumbbell Theorem}$ [Arya et~al. STOC'95] states that any set of $n$ points in $d$-dimensional Euclidean space admits a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d} \\cdot \\log(1/\\varepsilon))$ trees, where the $O_d$ notation suppresses terms that depend solely on the dimension~$d$. The running time of their construction is $O_d(n \\log n \\cdot \\frac{\\log(1/\\varepsilon)}{\\varepsilon^{d}} + n \\cdot \\varepsilon^{-2d})$. Since the same point may occur in multiple levels of the tree, the $\\textit{maximum degree}$ of a point in the tree cover may be as large as $\\Omega(\\log \\Phi)$, where $\\Phi$ is the aspect ratio of the input point set.   In this work we present a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d+1} \\cdot \\log(1/\\varepsilon))$ trees, which is optimal (up to the $\\log(1/\\varepsilon)$ factor). Moreover, the maximum degree of points in any tree is an $\\textit{absolute constant}$ for any $d$. As a direct corollary, we obtain an optimal {routing scheme} in low-dimensional Euclidean spaces. We also present a $(1+\\varepsilon)$-stretch $\\textit{Steiner}$ tree cover (that may use Steiner points) with $O_d(\\varepsilon^{(-d+1)/{2}} \\cdot \\log(1/\\varepsilon))$ trees, which too is optimal. The running time of our two constructions is linear in the number of edges in the respective tree covers, ignoring an additive $O_d(n \\log n)$ term; this improves over the running time underlying the Dumbbell Theorem.","sentences":["A $(1+\\varepsilon)\\textit{-stretch tree cover}$ of a metric space is a collection of trees, where every pair of points has a $(1+\\varepsilon)$-stretch path in one of the trees.","The celebrated $\\textit{Dumbbell Theorem}$","[Arya et~al. STOC'95] states that any set of $n$ points in $d$-dimensional Euclidean space admits a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d} \\cdot \\log(1/\\varepsilon))$ trees, where the $O_d$ notation suppresses terms that depend solely on the dimension~$d$.","The running time of their construction is $O_d(n \\log n \\cdot \\frac{\\log(1/\\varepsilon)}{\\varepsilon^{d}} + n \\cdot \\varepsilon^{-2d})$. Since the same point may occur in multiple levels of the tree, the $\\textit{maximum degree}$ of a point in the tree cover may be as large as $\\Omega(\\log \\Phi)$, where $\\Phi$ is the aspect ratio of the input point set.   ","In this work we present a $(1+\\varepsilon)$-stretch tree cover with $O_d(\\varepsilon^{-d+1} \\cdot \\log(1/\\varepsilon))$ trees, which is optimal (up to the $\\log(1/\\varepsilon)$ factor).","Moreover, the maximum degree of points in any tree is an $\\textit{absolute constant}$ for any $d$. As a direct corollary, we obtain an optimal {routing scheme} in low-dimensional Euclidean spaces.","We also present a $(1+\\varepsilon)$-stretch $\\textit{Steiner}$ tree cover (that may use Steiner points) with $O_d(\\varepsilon^{(-d+1)/{2}} \\cdot \\log(1/\\varepsilon))$ trees, which too is optimal.","The running time of our two constructions is linear in the number of edges in the respective tree covers, ignoring an additive $O_d(n \\log n)$ term; this improves over the running time underlying the Dumbbell Theorem."],"url":"http://arxiv.org/abs/2403.17754v1","category":"cs.CG"}
{"created":"2024-03-26 13:58:47","title":"Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A New Benchmark","abstract":"The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity. Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas. However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets. In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images. Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes. Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively. The code and data will be made available soon.","sentences":["The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity.","Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas.","However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets.","In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images.","Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes.","Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively.","The code and data will be made available soon."],"url":"http://arxiv.org/abs/2403.17712v1","category":"cs.CV"}
{"created":"2024-03-26 10:05:03","title":"Neural category","abstract":"A neural code on $ n $ neurons is a collection of subsets of the set $ [n]=\\{1,2,\\dots,n\\} $. Curto et al. \\cite{curto2013neural} associated a ring $\\mathcal{R}_{\\mathcal{C}}$ (neural ring) to a neural code $\\mathcal{C}$. A special class of ring homomorphisms between two neural rings, called neural ring homomorphism, was introduced by Curto and Youngs \\cite{curto2020neural}. The main work in this paper comprises constructing two categories. First is the $\\mathfrak{C}$ category, a subcategory of SETS consisting of neural codes and code maps. Second is the neural category $\\mathfrak{N}$, a subcategory of \\textit{Rngs} consisting of neural rings and neural ring homomorphisms. Then, the rest of the paper characterizes the properties of these two categories like initial and final objects, products, coproducts, limits, etc. Also, we show that these two categories are in dual equivalence.","sentences":["A neural code on $ n $ neurons is a collection of subsets of the set $","[n]=\\{1,2,\\dots,n\\} $.","Curto et al. \\cite{curto2013neural} associated a ring $\\mathcal{R}_{\\mathcal{C}}$ (neural ring) to a neural code $\\mathcal{C}$. A special class of ring homomorphisms between two neural rings, called neural ring homomorphism, was introduced by Curto and Youngs \\cite{curto2020neural}.","The main work in this paper comprises constructing two categories.","First is the $\\mathfrak{C}$ category, a subcategory of SETS consisting of neural codes and code maps.","Second is the neural category $\\mathfrak{N}$, a subcategory of \\textit{Rngs} consisting of neural rings and neural ring homomorphisms.","Then, the rest of the paper characterizes the properties of these two categories like initial and final objects, products, coproducts, limits, etc.","Also, we show that these two categories are in dual equivalence."],"url":"http://arxiv.org/abs/2403.17558v1","category":"math.CT"}
{"created":"2024-03-26 08:19:29","title":"Natural Language Requirements Testability Measurement Based on Requirement Smells","abstract":"Requirements form the basis for defining software systems' obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary is required to detect such words. We offer a neural word-embedding technique to generate such a dictionary automatically. Using the dictionary, we could automatically detect Polysemy smell (domain-specific ambiguity) for the first time in 10 application domains. Our empirical study on nearly 1000 software requirements from six well-known industrial and academic projects demonstrates that the proposed smell detection approach outperforms Smella, a state-of-the-art tool, in detecting requirements smells. The precision and recall of smell detection are improved with an average of 0.03 and 0.33, respectively, compared to the state-of-the-art. The proposed requirement testability model measures the testability of 985 requirements with a mean absolute error of 0.12 and a mean squared error of 0.03, demonstrating the model's potential for practical use.","sentences":["Requirements form the basis for defining software systems' obligations and tasks.","Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests.","However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability.","This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain.","Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words.","A comprehensive dictionary is required to detect such words.","We offer a neural word-embedding technique to generate such a dictionary automatically.","Using the dictionary, we could automatically detect Polysemy smell (domain-specific ambiguity) for the first time in 10 application domains.","Our empirical study on nearly 1000 software requirements from six well-known industrial and academic projects demonstrates that the proposed smell detection approach outperforms Smella, a state-of-the-art tool, in detecting requirements smells.","The precision and recall of smell detection are improved with an average of 0.03 and 0.33, respectively, compared to the state-of-the-art.","The proposed requirement testability model measures the testability of 985 requirements with a mean absolute error of 0.12 and a mean squared error of 0.03, demonstrating the model's potential for practical use."],"url":"http://arxiv.org/abs/2403.17479v1","category":"cs.SE"}
{"created":"2024-03-26 08:07:33","title":"Document Set Expansion with Positive-Unlabelled Learning Using Intractable Density Estimation","abstract":"The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents. Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task. However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection. To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models. Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE. Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE.","sentences":["The Document Set Expansion (DSE) task involves identifying relevant documents from large collections based on a limited set of example documents.","Previous research has highlighted Positive and Unlabeled (PU) learning as a promising approach for this task.","However, most PU methods rely on the unrealistic assumption of knowing the class prior for positive samples in the collection.","To address this limitation, this paper introduces a novel PU learning framework that utilizes intractable density estimation models.","Experiments conducted on PubMed and Covid datasets in a transductive setting showcase the effectiveness of the proposed method for DSE.","Code is available from https://github.com/Beautifuldog01/Document-set-expansion-puDE."],"url":"http://arxiv.org/abs/2403.17473v1","category":"cs.IR"}
{"created":"2024-03-26 07:55:45","title":"A Unified Kernel for Neural Network Learning","abstract":"Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoretically characterize the uniform tightness and learning convergence of the UNK kernel, providing comprehensive insights into this unified kernel. Experimental results underscore the effectiveness of our proposed method.","sentences":["Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning.","Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes.","Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK).","The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel.","In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization.","The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity.","Besides, we also theoretically characterize the uniform tightness and learning convergence of the UNK kernel, providing comprehensive insights into this unified kernel.","Experimental results underscore the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2403.17467v1","category":"cs.LG"}
{"created":"2024-03-26 07:55:16","title":"LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection","abstract":"The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives. Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators. LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times.","sentences":["The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images.","This development, while impressive, also raises significant privacy and security concerns.","In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images.","We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection.","LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake.","To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature.","Our EGRE utilizes an align-then-refine mechanism, which effectively refines the image feature for generated-image detection from both spatial and channel perspectives.","Extensive experiments on the large-scale GenImage benchmark demonstrate the superiority of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1% average ACC/AP across 8 different image generators.","LaRE also surpasses existing methods in terms of feature extraction cost, delivering an impressive speed enhancement of 8 times."],"url":"http://arxiv.org/abs/2403.17465v1","category":"cs.CV"}
{"created":"2024-03-26 07:41:54","title":"Imitating Cost-Constrained Behaviors in Reinforcement Learning","abstract":"Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the time available. In such problems, imitation learning is challenging as decisions are not only dictated by the reward model but are also dependent on a cost-constrained model. In this paper, we provide multiple methods that match expert distributions in the presence of trajectory cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to find a good trade-off between expected return and minimizing constraint violation; and (c) Cost-violation-based alternating gradient. We empirically show that leading imitation learning approaches imitate cost-constrained behaviors poorly and our meta-gradient-based approach achieves the best performance.","sentences":["Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches.","In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems.","Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert.","Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle).","However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints.","For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the time available.","In such problems, imitation learning is challenging as decisions are not only dictated by the reward model but are also dependent on a cost-constrained model.","In this paper, we provide multiple methods that match expert distributions in the presence of trajectory cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to find a good trade-off between expected return and minimizing constraint violation; and (c) Cost-violation-based alternating gradient.","We empirically show that leading imitation learning approaches imitate cost-constrained behaviors poorly and our meta-gradient-based approach achieves the best performance."],"url":"http://arxiv.org/abs/2403.17456v1","category":"cs.LG"}
{"created":"2024-03-26 07:23:46","title":"Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model","abstract":"Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.","sentences":["Modeling long-range dependencies in sequential data is a crucial step in sequence learning.","A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences.","However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs).","To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP.","We augment simple ETS with additional parameters and complex field to reduce the inductive bias.","Despite increasing less than 1\\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark."],"url":"http://arxiv.org/abs/2403.17445v1","category":"cs.LG"}
{"created":"2024-03-26 07:06:54","title":"An Empirical Study of ChatGPT-related projects on GitHub","abstract":"As ChatGPT possesses powerful capabilities in natural language processing and code analysis, it has received widespread attention since its launch. Developers have applied its powerful capabilities to various domains through software projects which are hosted on the largest open-source platform (GitHub) worldwide. Simultaneously, these projects have triggered extensive discussions. In order to comprehend the research content of these projects and understand the potential requirements discussed, we collected ChatGPT-related projects from the GitHub platform and utilized the LDA topic model to identify the discussion topics. Specifically, we selected 200 projects, categorizing them into three primary categories through analyzing their descriptions: ChatGPT implementation & training, ChatGPT application, ChatGPT improvement & extension. Subsequently, we employed the LDA topic model to identify 10 topics from issue texts, and compared the distribution and evolution trend of the discovered topics within the three primary project categories. Our observations include (1) The number of projects growing in a single month for the three primary project categories are closely associated with the development of ChatGPT. (2) There exist significant variations in the popularity of each topic for the three primary project categories. (3) The monthly changes in the absolute impact of each topic for the three primary project categories are diverse, which is often closely associated with the variation in the number of projects owned by that category. (4) With the passage of time, the relative impact of each topic exhibits different development trends in the three primary project categories. Based on these findings, we discuss implications for developers and users.","sentences":["As ChatGPT possesses powerful capabilities in natural language processing and code analysis, it has received widespread attention since its launch.","Developers have applied its powerful capabilities to various domains through software projects which are hosted on the largest open-source platform (GitHub) worldwide.","Simultaneously, these projects have triggered extensive discussions.","In order to comprehend the research content of these projects and understand the potential requirements discussed, we collected ChatGPT-related projects from the GitHub platform and utilized the LDA topic model to identify the discussion topics.","Specifically, we selected 200 projects, categorizing them into three primary categories through analyzing their descriptions: ChatGPT implementation & training, ChatGPT application, ChatGPT improvement & extension.","Subsequently, we employed the LDA topic model to identify 10 topics from issue texts, and compared the distribution and evolution trend of the discovered topics within the three primary project categories.","Our observations include (1) The number of projects growing in a single month for the three primary project categories are closely associated with the development of ChatGPT.","(2) There exist significant variations in the popularity of each topic for the three primary project categories.","(3) The monthly changes in the absolute impact of each topic for the three primary project categories are diverse, which is often closely associated with the variation in the number of projects owned by that category.","(4) With the passage of time, the relative impact of each topic exhibits different development trends in the three primary project categories.","Based on these findings, we discuss implications for developers and users."],"url":"http://arxiv.org/abs/2403.17437v1","category":"cs.SE"}
{"created":"2024-03-26 07:05:06","title":"Particle identification with machine learning from incomplete data in the ALICE experiment","abstract":"The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.","sentences":["The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions.","Such studies require accurate particle identification (PID).","ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts.","Acmuch better performance can be achieved with machine learning (ML) methods.","Our solution uses multiple neural networks (NN) serving as binary classifiers.","Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples.","We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data."],"url":"http://arxiv.org/abs/2403.17436v1","category":"hep-ex"}
{"created":"2024-03-26 06:57:23","title":"Robust and Scalable Model Editing for Large Language Models","abstract":"Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing. To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets. Empirical results show that our method outperforms current state-of-the-art methods by a large margin. Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa). The source code can be found at https://github.com/thunlp/EREN.","sentences":["Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context.","In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant.","This enables updating and correcting the model's knowledge by in-context editing instead of retraining.","Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context.","In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context.","Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalability and robustness of LLM editing.","To better evaluate the robustness of model editors, we collect a new dataset, that contains irrelevant questions that are more challenging than the ones in existing datasets.","Empirical results show that our method outperforms current state-of-the-art methods by a large margin.","Unlike existing techniques, it can integrate knowledge from multiple edits, and correctly respond to syntactically similar but semantically unrelated inputs (and vice versa).","The source code can be found at https://github.com/thunlp/EREN."],"url":"http://arxiv.org/abs/2403.17431v1","category":"cs.CL"}
{"created":"2024-03-26 06:50:04","title":"Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization","abstract":"Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the symptom delineation task and the summarization task. This research contributes to the nascent field of applying LLMs to psychiatric interview and demonstrates their potential effectiveness in aiding mental health practitioners.","sentences":["Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains.","Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value.","Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues.","Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript.","Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs.","Our experimental results show that appropriately prompted LLMs can achieve high performance on both the symptom delineation task and the summarization task.","This research contributes to the nascent field of applying LLMs to psychiatric interview and demonstrates their potential effectiveness in aiding mental health practitioners."],"url":"http://arxiv.org/abs/2403.17428v1","category":"cs.AI"}
{"created":"2024-03-26 06:47:17","title":"Knowledge-Powered Recommendation for an Improved Diet Water Footprint","abstract":"According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year. By 2025, two-thirds of the world's population may be facing water shortages. This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food. This paper proposes a recommendation engine, powered by knowledge graphs, aiming to facilitate sustainable and healthy food consumption. The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint. The system architecture includes source identification, information extraction, schema alignment, knowledge graph construction, and user interface development. The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts.","sentences":["According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year.","By 2025, two-thirds of the world's population may be facing water shortages.","This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food.","This paper proposes a recommendation engine, powered by knowledge graphs, aiming to facilitate sustainable and healthy food consumption.","The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint.","The system architecture includes source identification, information extraction, schema alignment, knowledge graph construction, and user interface development.","The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts."],"url":"http://arxiv.org/abs/2403.17426v1","category":"cs.AI"}
{"created":"2024-03-26 06:34:23","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification","abstract":"The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experiments on public TREC datasets to demonstrate the effectiveness and potential of MA4DIV. Considering the limited number of queries in public TREC datasets, we construct a large-scale dataset from industry sources and show that MA4DIV achieves substantial improvements in both effectiveness and efficiency than existing baselines on a industrial scale dataset.","sentences":["The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible.","Existing methods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one document with the highest diversity score at a time.","These approaches tend to be inefficient and are easily trapped in a suboptimal state.","In addition, some other methods aim to approximately optimize the diversity metric, such as $\\alpha$-NDCG, but the results still remain suboptimal.","To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV.","In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents.","This approach allows for directly optimizing the diversity metrics, such as $\\alpha$-NDCG, while achieving high training efficiency.","We conducted preliminary experiments on public TREC datasets to demonstrate the effectiveness and potential of MA4DIV.","Considering the limited number of queries in public TREC datasets, we construct a large-scale dataset from industry sources and show that MA4DIV achieves substantial improvements in both effectiveness and efficiency than existing baselines on a industrial scale dataset."],"url":"http://arxiv.org/abs/2403.17421v1","category":"cs.IR"}
{"created":"2024-03-26 06:18:42","title":"AI Safety: Necessary, but insufficient and possibly problematic","abstract":"This article critically examines the recent hype around AI safety. We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good. We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with. We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense. We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions. We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety.","sentences":["This article critically examines the recent hype around AI safety.","We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good.","We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with.","We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense.","We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions.","We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety."],"url":"http://arxiv.org/abs/2403.17419v1","category":"cs.AI"}
{"created":"2024-03-26 06:14:58","title":"Cyclic pursuit formation control for arbitrary desired shapes","abstract":"A multi-agent system comprises numerous agents that autonomously make decisions to collectively accomplish tasks, drawing significant attention for their wide-ranging applications. Within this context, formation control emerges as a prominent task, wherein agents collaboratively shape and maneuver while preserving formation integrity. Our focus centers on cyclic pursuit, a method facilitating the formation of circles, ellipses, and figure-eights under the assumption that agents can only perceive the relative positions of those preceding them. However, this method's scope has been restricted to these specific shapes, leaving the feasibility of forming other shapes uncertain. In response, our study proposes a novel method based on cyclic pursuit capable of forming a broader array of shapes, enabling agents to individually shape while pursuing preceding agents, thereby extending the repertoire of achievable formations. We present two scenarios concerning the information available to agents and devise formation control methods tailored to each scenario. Through extensive simulations, we demonstrate the efficacy of our proposed method in forming multiple shapes, including those represented as Fourier series, thereby underscoring the versatility and effectiveness of our approach.","sentences":["A multi-agent system comprises numerous agents that autonomously make decisions to collectively accomplish tasks, drawing significant attention for their wide-ranging applications.","Within this context, formation control emerges as a prominent task, wherein agents collaboratively shape and maneuver while preserving formation integrity.","Our focus centers on cyclic pursuit, a method facilitating the formation of circles, ellipses, and figure-eights under the assumption that agents can only perceive the relative positions of those preceding them.","However, this method's scope has been restricted to these specific shapes, leaving the feasibility of forming other shapes uncertain.","In response, our study proposes a novel method based on cyclic pursuit capable of forming a broader array of shapes, enabling agents to individually shape while pursuing preceding agents, thereby extending the repertoire of achievable formations.","We present two scenarios concerning the information available to agents and devise formation control methods tailored to each scenario.","Through extensive simulations, we demonstrate the efficacy of our proposed method in forming multiple shapes, including those represented as Fourier series, thereby underscoring the versatility and effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.17417v1","category":"eess.SY"}
{"created":"2024-03-26 06:12:38","title":"The Privacy Policy Permission Model: A Unified View of Privacy Policies","abstract":"Organizations use privacy policies to communicate their data collection practices to their clients. A privacy policy is a set of statements that specifies how an organization gathers, uses, discloses, and maintains a client's data. However, most privacy policies lack a clear, complete explanation of how data providers' information is used. We propose a modeling methodology, called the Privacy Policy Permission Model (PPPM), that provides a uniform, easy-to-understand representation of privacy policies, which can accurately and clearly show how data is used within an organization's practice. Using this methodology, a privacy policy is captured as a diagram. The diagram is capable of highlighting inconsistencies and inaccuracies in the privacy policy. The methodology supports privacy officers in properly and clearly articulating an organization's privacy policy.","sentences":["Organizations use privacy policies to communicate their data collection practices to their clients.","A privacy policy is a set of statements that specifies how an organization gathers, uses, discloses, and maintains a client's data.","However, most privacy policies lack a clear, complete explanation of how data providers' information is used.","We propose a modeling methodology, called the Privacy Policy Permission Model (PPPM), that provides a uniform, easy-to-understand representation of privacy policies, which can accurately and clearly show how data is used within an organization's practice.","Using this methodology, a privacy policy is captured as a diagram.","The diagram is capable of highlighting inconsistencies and inaccuracies in the privacy policy.","The methodology supports privacy officers in properly and clearly articulating an organization's privacy policy."],"url":"http://arxiv.org/abs/2403.17414v1","category":"cs.CR"}
{"created":"2024-03-26 06:06:01","title":"On permutation-invariant neural networks","abstract":"Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of the diverse problem settings and ongoing research efforts pertaining to neural networks that approximate set functions. By delving into the intricacies of these approaches and elucidating the associated challenges, the survey aims to equip readers with a comprehensive understanding of the field. Through this comprehensive perspective, we hope that researchers can gain valuable insights into the potential applications, inherent limitations, and future directions of set-based neural networks. Indeed, from this survey we gain two insights: i) Deep Sets and its variants can be generalized by differences in the aggregation function, and ii) the behavior of Deep Sets is sensitive to the choice of the aggregation function. From these observations, we show that Deep Sets, one of the well-known permutation-invariant neural networks, can be generalized in the sense of a quasi-arithmetic mean.","sentences":["Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms.","However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges.","In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data.","These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures.","Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions.","This comprehensive survey aims to provide an overview of the diverse problem settings and ongoing research efforts pertaining to neural networks that approximate set functions.","By delving into the intricacies of these approaches and elucidating the associated challenges, the survey aims to equip readers with a comprehensive understanding of the field.","Through this comprehensive perspective, we hope that researchers can gain valuable insights into the potential applications, inherent limitations, and future directions of set-based neural networks.","Indeed, from this survey we gain two insights: i) Deep Sets and its variants can be generalized by differences in the aggregation function, and ii) the behavior of Deep Sets is sensitive to the choice of the aggregation function.","From these observations, we show that Deep Sets, one of the well-known permutation-invariant neural networks, can be generalized in the sense of a quasi-arithmetic mean."],"url":"http://arxiv.org/abs/2403.17410v1","category":"cs.LG"}
{"created":"2024-03-26 05:55:21","title":"Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens","abstract":"Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or \"district\" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each district. The DGT technique is applied to fine-tune several transformer-based models, on this new dataset. Experimental results demonstrate the effectiveness of DGT, with the ByT5 model achieving superior performance over word-based models like mT5, BanglaT5, and umT5. This is attributed to ByT5's ability to handle a high percentage of out-of-vocabulary words in the test set. The proposed approach highlights the importance of incorporating regional dialect information into ubiquitous natural language processing systems for languages with diverse phonological variations. The following work was a result of the \"Bhashamul\" challenge, which is dedicated to solving the problem of Bengali text with regional dialects to IPA transcription https://www.kaggle.com/competitions/regipa/. The training and inference notebooks are available through the competition link.","sentences":["Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes.","This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions.","This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh.","The key idea is to provide the model with explicit information about the regional dialect or \"district\" of the input text before generating the IPA transcription.","This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each district.","The DGT technique is applied to fine-tune several transformer-based models, on this new dataset.","Experimental results demonstrate the effectiveness of DGT, with the ByT5 model achieving superior performance over word-based models like mT5, BanglaT5, and umT5.","This is attributed to ByT5's ability to handle a high percentage of out-of-vocabulary words in the test set.","The proposed approach highlights the importance of incorporating regional dialect information into ubiquitous natural language processing systems for languages with diverse phonological variations.","The following work was a result of the \"Bhashamul\" challenge, which is dedicated to solving the problem of Bengali text with regional dialects to IPA transcription https://www.kaggle.com/competitions/regipa/.","The training and inference notebooks are available through the competition link."],"url":"http://arxiv.org/abs/2403.17407v1","category":"cs.CL"}
{"created":"2024-03-26 05:51:05","title":"The recessionary pressures of generative AI: A threat to wellbeing","abstract":"Generative Artificial Intelligence (AI) stands as a transformative force that presents a paradox; it offers unprecedented opportunities for productivity growth while potentially posing significant threats to economic stability and societal wellbeing. Many consider generative AI as akin to previous technological advancements, using historical precedent to argue that fears of widespread job displacement are unfounded, while others contend that generative AI`s unique capacity to undertake non-routine cognitive tasks sets it apart from other forms of automation capital and presents a threat to the quality and availability of work that underpin stable societies. This paper explores the conditions under which both may be true. We posit the existence of an AI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of recessionary pressures could be triggered, exacerbating social disparities, reducing social cohesion, heightening tensions, and requiring sustained government intervention to maintain stability. To prevent this, the paper underscores the urgent need for proactive policy responses, making recommendations to reduce these risks through robust regulatory frameworks and a new social contract characterised by progressive social and economic policies. This approach aims to ensure a sustainable, inclusive, and resilient economic future where human contribution to the economy is retained and integrated with generative AI to enhance the Mental Wealth of nations.","sentences":["Generative Artificial Intelligence (AI) stands as a transformative force that presents a paradox; it offers unprecedented opportunities for productivity growth while potentially posing significant threats to economic stability and societal wellbeing.","Many consider generative AI as akin to previous technological advancements, using historical precedent to argue that fears of widespread job displacement are unfounded, while others contend that generative AI`s unique capacity to undertake non-routine cognitive tasks sets it apart from other forms of automation capital and presents a threat to the quality and availability of work that underpin stable societies.","This paper explores the conditions under which both may be true.","We posit the existence of an AI-capital-to-labour ratio threshold beyond which a self-reinforcing cycle of recessionary pressures could be triggered, exacerbating social disparities, reducing social cohesion, heightening tensions, and requiring sustained government intervention to maintain stability.","To prevent this, the paper underscores the urgent need for proactive policy responses, making recommendations to reduce these risks through robust regulatory frameworks and a new social contract characterised by progressive social and economic policies.","This approach aims to ensure a sustainable, inclusive, and resilient economic future where human contribution to the economy is retained and integrated with generative AI to enhance the Mental Wealth of nations."],"url":"http://arxiv.org/abs/2403.17405v1","category":"cs.CY"}
{"created":"2024-03-26 05:40:15","title":"A Survey on Resource Management in Joint Communication and Computing-Embedded SAGIN","abstract":"The advent of the 6G era aims for ubiquitous connectivity, with the integration of non-terrestrial networks (NTN) offering extensive coverage and enhanced capacity. As manufacturing advances and user demands evolve, space-air-ground integrated networks (SAGIN) with computational capabilities emerge as a viable solution for services requiring low latency and high computational power. Resource management within joint communication and computing-embedded SAGIN (JCC-SAGIN) presents greater complexity than traditional terrestrial networks. This complexity arises from the spatiotemporal dynamics of network topology and service demand, the interdependency of large-scale resource variables, and intricate tradeoffs among various performance metrics. Thus, a thorough examination of resource management strategies in JCC-SAGIN is crucial, emphasizing the role of non-terrestrial platforms with processing capabilities in 6G. This paper begins by reviewing the architecture, enabling technologies, and applications in JCC-SAGIN. Then, we offer a detailed overview of resource management modeling and optimization methods, encompassing both traditional optimization approaches and learning-based intelligent decision-making frameworks. Finally, we outline the prospective research directions in JCC-SAGIN.","sentences":["The advent of the 6G era aims for ubiquitous connectivity, with the integration of non-terrestrial networks (NTN) offering extensive coverage and enhanced capacity.","As manufacturing advances and user demands evolve, space-air-ground integrated networks (SAGIN) with computational capabilities emerge as a viable solution for services requiring low latency and high computational power.","Resource management within joint communication and computing-embedded SAGIN (JCC-SAGIN) presents greater complexity than traditional terrestrial networks.","This complexity arises from the spatiotemporal dynamics of network topology and service demand, the interdependency of large-scale resource variables, and intricate tradeoffs among various performance metrics.","Thus, a thorough examination of resource management strategies in JCC-SAGIN is crucial, emphasizing the role of non-terrestrial platforms with processing capabilities in 6G.","This paper begins by reviewing the architecture, enabling technologies, and applications in JCC-SAGIN.","Then, we offer a detailed overview of resource management modeling and optimization methods, encompassing both traditional optimization approaches and learning-based intelligent decision-making frameworks.","Finally, we outline the prospective research directions in JCC-SAGIN."],"url":"http://arxiv.org/abs/2403.17400v1","category":"cs.NI"}
{"created":"2024-03-26 05:25:01","title":"An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning","abstract":"We propose an open-source end-to-end logic optimization framework for large-scale boolean network with reinforcement learning.","sentences":["We propose an open-source end-to-end logic optimization framework for large-scale boolean network with reinforcement learning."],"url":"http://arxiv.org/abs/2403.17395v1","category":"cs.AI"}
{"created":"2024-03-26 05:23:12","title":"Natural-artificial hybrid swarm: Cyborg-insect group navigation in unknown obstructed soft terrain","abstract":"Navigating multi-robot systems in complex terrains has always been a challenging task. This is due to the inherent limitations of traditional robots in collision avoidance, adaptation to unknown environments, and sustained energy efficiency. In order to overcome these limitations, this research proposes a solution by integrating living insects with miniature electronic controllers to enable robotic-like programmable control, and proposing a novel control algorithm for swarming. Although these creatures, called cyborg insects, have the ability to instinctively avoid collisions with neighbors and obstacles while adapting to complex terrains, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input. In response to this issue, we propose a novel swarm navigation algorithm addressing these challenges. The effectiveness of the algorithm is demonstrated through an experimental validation in which a cyborg swarm was successfully navigated through an unknown sandy field with obstacles and hills. This research contributes to the domain of swarm robotics and showcases the potential of integrating biological organisms with robotics and control theory to create more intelligent autonomous systems with real-world applications.","sentences":["Navigating multi-robot systems in complex terrains has always been a challenging task.","This is due to the inherent limitations of traditional robots in collision avoidance, adaptation to unknown environments, and sustained energy efficiency.","In order to overcome these limitations, this research proposes a solution by integrating living insects with miniature electronic controllers to enable robotic-like programmable control, and proposing a novel control algorithm for swarming.","Although these creatures, called cyborg insects, have the ability to instinctively avoid collisions with neighbors and obstacles while adapting to complex terrains, there is a lack of literature on the control of multi-cyborg systems.","This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input.","In response to this issue, we propose a novel swarm navigation algorithm addressing these challenges.","The effectiveness of the algorithm is demonstrated through an experimental validation in which a cyborg swarm was successfully navigated through an unknown sandy field with obstacles and hills.","This research contributes to the domain of swarm robotics and showcases the potential of integrating biological organisms with robotics and control theory to create more intelligent autonomous systems with real-world applications."],"url":"http://arxiv.org/abs/2403.17392v1","category":"cs.RO"}
{"created":"2024-03-26 05:11:51","title":"ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition","abstract":"In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also achieves over 75% of the performance of a strong, fully supervised model trained on gold data. Our code is available at: https://github.com/hriaz17/ELLEN.","sentences":["In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class.","We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules.","These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context.","ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above.","It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data).","Further, we evaluate our CoNLL-2003 model in a zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and achieves comparable performance to GPT-4.","In a zero-shot setting, ELLEN also achieves over 75% of the performance of a strong, fully supervised model trained on gold data.","Our code is available at: https://github.com/hriaz17/ELLEN."],"url":"http://arxiv.org/abs/2403.17385v1","category":"cs.CL"}
{"created":"2024-03-26 05:10:47","title":"Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation","abstract":"This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.","sentences":["This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods.","We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points.","Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii.","The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process.","Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting."],"url":"http://arxiv.org/abs/2403.17384v1","category":"cs.AI"}
{"created":"2024-03-26 04:59:27","title":"Application-Driven Innovation in Machine Learning","abstract":"As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.","sentences":["As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important.","Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself.","In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research.","We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work.","Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation.","We outline how these processes may be improved."],"url":"http://arxiv.org/abs/2403.17381v1","category":"cs.LG"}
{"created":"2024-03-26 04:49:11","title":"Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance","abstract":"Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms' ability to capture structural information, and guiding the denoising process away from these degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves sample quality in conditional and even unconditional scenarios. Moreover, PAG significantly improves the baseline performance in various downstream tasks where existing guidances such as CG or CFG cannot be fully utilized, including ControlNet with empty prompts and image restoration such as inpainting and deblurring.","sentences":["Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG).","These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration.","In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules.","PAG is designed to progressively enhance the structure of samples throughout the denoising process.","It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms' ability to capture structural information, and guiding the denoising process away from these degraded samples.","In both ADM and Stable Diffusion, PAG surprisingly improves sample quality in conditional and even unconditional scenarios.","Moreover, PAG significantly improves the baseline performance in various downstream tasks where existing guidances such as CG or CFG cannot be fully utilized, including ControlNet with empty prompts and image restoration such as inpainting and deblurring."],"url":"http://arxiv.org/abs/2403.17377v1","category":"cs.CV"}
{"created":"2024-03-26 04:27:56","title":"AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving","abstract":"Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduced cost.","sentences":["Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance.","However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model.","This necessitates an expensive process of continuously curating and annotating data with significant human effort.","We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios.","This process operates iteratively, allowing for continuous self-improvement of the model.","We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduced cost."],"url":"http://arxiv.org/abs/2403.17373v1","category":"cs.CV"}
{"created":"2024-03-26 04:11:47","title":"Robust Containment Queries over Collections of Rational Parametric Curves via Generalized Winding Numbers","abstract":"Point containment queries for regions bound by watertight geometric surfaces, i.e. closed and without self-intersections, can be evaluated straightforwardly with a number of well-studied algorithms. However, when such assumptions on domain geometry are not met, these methods are theoretically unfounded at best and practically unusable at worst. More robust classification schemes utilize generalized winding numbers, a mathematical construction that is indifferent to imperfections in the often human-defined geometric model. We extend this methodology to more general curved shapes, defining a robust containment query for regions whose boundary elements are defined by a collection of rational parametric curves. In doing so, we devise an algorithm that is stable and accurate at arbitrary points in space, circumventing the typical difficulties for queries that are arbitrarily close or coincident with the model. This is done by reducing the generalized winding number problem to an integer winding number problem, which is solved by approximating each curve with a polyline that provably has the same winding number at the point of interest. We demonstrate the improvements in computational complexity granted by this method over conventional techniques, as well as the robustness induced by its application","sentences":["Point containment queries for regions bound by watertight geometric surfaces, i.e. closed and without self-intersections, can be evaluated straightforwardly with a number of well-studied algorithms.","However, when such assumptions on domain geometry are not met, these methods are theoretically unfounded at best and practically unusable at worst.","More robust classification schemes utilize generalized winding numbers, a mathematical construction that is indifferent to imperfections in the often human-defined geometric model.","We extend this methodology to more general curved shapes, defining a robust containment query for regions whose boundary elements are defined by a collection of rational parametric curves.","In doing so, we devise an algorithm that is stable and accurate at arbitrary points in space, circumventing the typical difficulties for queries that are arbitrarily close or coincident with the model.","This is done by reducing the generalized winding number problem to an integer winding number problem, which is solved by approximating each curve with a polyline that provably has the same winding number at the point of interest.","We demonstrate the improvements in computational complexity granted by this method over conventional techniques, as well as the robustness induced by its application"],"url":"http://arxiv.org/abs/2403.17371v1","category":"cs.CG"}
{"created":"2024-03-26 04:07:08","title":"ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?","abstract":"As AI becomes more integral in our lives, the need for transparency and responsibility grows. While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that ChatGPT aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment. This research advances our understanding of large language models' capabilities to assess the text explanation quality in different configurations for responsible AI development.","sentences":["As AI becomes more integral in our lives, the need for transparency and responsibility grows.","While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings.","This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale).","We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement.","We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations.","Our results show that ChatGPT aligns better with humans in more coarse-grained scales.","Also, paired comparisons and dynamic prompting (i.e., providing semantically similar examples in the prompt) improve the alignment.","This research advances our understanding of large language models' capabilities to assess the text explanation quality in different configurations for responsible AI development."],"url":"http://arxiv.org/abs/2403.17368v1","category":"cs.CL"}
{"created":"2024-03-26 03:58:52","title":"Extracting Biomedical Entities from Noisy Audio Transcripts","abstract":"Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems. Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied. Named Entity Recognition (NER), an essential clinical task, is particularly affected by such noise, often termed the ASR-NLP gap. Prior works have primarily studied ASR's efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments. This paper introduces a novel dataset, BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a comprehensive collection of almost 2,000 clean and noisy recordings. In addressing the noise challenge, we present an innovative transcript-cleaning method using GPT4, investigating both zero-shot and few-shot methodologies. Our study further delves into an error analysis, shedding light on the types of errors in transcription software, corrections by GPT4, and the challenges GPT4 faces. This paper aims to foster improved understanding and potential solutions for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation practices.","sentences":["Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems.","Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied.","Named Entity Recognition (NER), an essential clinical task, is particularly affected by such noise, often termed the ASR-NLP gap.","Prior works have primarily studied ASR's efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments.","This paper introduces a novel dataset, BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Telephone (BTACT) exam.","Our dataset offers a comprehensive collection of almost 2,000 clean and noisy recordings.","In addressing the noise challenge, we present an innovative transcript-cleaning method using GPT4, investigating both zero-shot and few-shot methodologies.","Our study further delves into an error analysis, shedding light on the types of errors in transcription software, corrections by GPT4, and the challenges GPT4 faces.","This paper aims to foster improved understanding and potential solutions for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation practices."],"url":"http://arxiv.org/abs/2403.17363v1","category":"cs.CL"}
{"created":"2024-03-26 03:54:25","title":"Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model","abstract":"FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that our approach exhibits competitive performance, aligning itself closely with top-tier models on the FEVEROUS benchmark.","sentences":["FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data.","In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings.","This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context.","By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions.","The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised.","Comparative analyses reveal that our approach exhibits competitive performance, aligning itself closely with top-tier models on the FEVEROUS benchmark."],"url":"http://arxiv.org/abs/2403.17361v1","category":"cs.CL"}
{"created":"2024-03-26 03:46:33","title":"Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent","abstract":"Lagrangian-guided Monte Carlo tree search with global dual ascent has been applied to solve large constrained partially observable Markov decision processes (CPOMDPs) online. In this work, we demonstrate that these global dual parameters can lead to myopic action selection during exploration, ultimately leading to suboptimal decision making. To address this, we introduce history-dependent dual variables that guide local action selection and are optimized with recursive dual ascent. We empirically compare the performance of our approach on a motivating toy example and two large CPOMDPs, demonstrating improved exploration, and ultimately, safer outcomes.","sentences":["Lagrangian-guided Monte Carlo tree search with global dual ascent has been applied to solve large constrained partially observable Markov decision processes (CPOMDPs) online.","In this work, we demonstrate that these global dual parameters can lead to myopic action selection during exploration, ultimately leading to suboptimal decision making.","To address this, we introduce history-dependent dual variables that guide local action selection and are optimized with recursive dual ascent.","We empirically compare the performance of our approach on a motivating toy example and two large CPOMDPs, demonstrating improved exploration, and ultimately, safer outcomes."],"url":"http://arxiv.org/abs/2403.17358v1","category":"cs.AI"}
{"created":"2024-03-26 03:44:51","title":"MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation","abstract":"Code comments are important for developers in program comprehension. In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. However, the extent of such supplementary information varies a lot in different code comments. In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide. With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments. Our experimental results demonstrate the value of our proposed work with a number of findings. (1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category. (2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate. (3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set. (4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower BLEU score in evaluation. These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation.","sentences":["Code comments are important for developers in program comprehension.","In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature.","However, the extent of such supplementary information varies a lot in different code comments.","In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide.","With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments.","Our experimental results demonstrate the value of our proposed work with a number of findings.","(1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category.","(2) Being able to provide various kinds of essential information, large-MESIA comments in the dataset are difficult for existing neural approaches to generate.","(3) We can improve the capability of existing neural approaches to generate large-MESIA comments by reducing the proportion of small-MESIA comments in the training set.","(4) The retrained model can generate large-MESIA comments that convey essential meaningful supplementary information for methods in the small-MESIA test set, but will get a lower BLEU score in evaluation.","These findings indicate that with good training data, auto-generated comments can sometimes even surpass human-written reference comments, and having no appropriate ground truth for evaluation is an issue that needs to be addressed by future work on automatic comment generation."],"url":"http://arxiv.org/abs/2403.17357v1","category":"cs.SE"}
{"created":"2024-03-26 03:28:02","title":"The Solution of the Zodiac Killer's 340-Character Cipher","abstract":"The case of the Zodiac Killer is one of the most widely known unsolved serial killer cases in history. The unidentified killer murdered five known victims and terrorized the state of California. He also communicated extensively with the press and law enforcement. Besides his murders, Zodiac was known for his use of ciphers. The first Zodiac cipher was solved within a week of its publication, while the second cipher was solved by the authors after 51 years, when it was discovered to be a transposition and homophonic substitution cipher with unusual qualities. In this paper, we detail the historical significance of this cipher and the numerous efforts which culminated in its solution.","sentences":["The case of the Zodiac Killer is one of the most widely known unsolved serial killer cases in history.","The unidentified killer murdered five known victims and terrorized the state of California.","He also communicated extensively with the press and law enforcement.","Besides his murders, Zodiac was known for his use of ciphers.","The first Zodiac cipher was solved within a week of its publication, while the second cipher was solved by the authors after 51 years, when it was discovered to be a transposition and homophonic substitution cipher with unusual qualities.","In this paper, we detail the historical significance of this cipher and the numerous efforts which culminated in its solution."],"url":"http://arxiv.org/abs/2403.17350v1","category":"cs.AI"}
{"created":"2024-03-26 03:03:50","title":"The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge","abstract":"In this paper, we propose a solution for improving the quality of captions generated for figures in papers. We adopt the approach of summarizing the textual content in the paper to generate image captions. Throughout our study, we encounter discrepancies in the OCR information provided in the official dataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR information from all images. Moreover, we observe that certain textual content in the official paper pertains to images that are not relevant for captioning, thereby introducing noise during caption generation. To mitigate this issue, we leverage LLaMA to extract image-specific information by querying the textual content based on image mentions, effectively filtering out extraneous information. Additionally, we recognize a discrepancy between the primary use of maximum likelihood estimation during text generation and the evaluation metrics such as ROUGE employed to assess the quality of generated captions. To bridge this gap, we integrate the BRIO model framework, enabling a more coherent alignment between the generation and evaluation processes. Our approach ranked first in the final test with a score of 4.49.","sentences":["In this paper, we propose a solution for improving the quality of captions generated for figures in papers.","We adopt the approach of summarizing the textual content in the paper to generate image captions.","Throughout our study, we encounter discrepancies in the OCR information provided in the official dataset.","To rectify this, we employ the PaddleOCR toolkit to extract OCR information from all images.","Moreover, we observe that certain textual content in the official paper pertains to images that are not relevant for captioning, thereby introducing noise during caption generation.","To mitigate this issue, we leverage LLaMA to extract image-specific information by querying the textual content based on image mentions, effectively filtering out extraneous information.","Additionally, we recognize a discrepancy between the primary use of maximum likelihood estimation during text generation and the evaluation metrics such as ROUGE employed to assess the quality of generated captions.","To bridge this gap, we integrate the BRIO model framework, enabling a more coherent alignment between the generation and evaluation processes.","Our approach ranked first in the final test with a score of 4.49."],"url":"http://arxiv.org/abs/2403.17342v1","category":"cs.CV"}
{"created":"2024-03-26 02:53:26","title":"Measurement Uncertainty Impact on Koopman Operator Estimation of Power System Dynamics","abstract":"Sensor measurements are mission-critical for monitoring and controlling power systems because they provide real-time insight into the grid operating condition; however, confidence in these insights depends greatly on the quality of the sensor data. Uncertainty in sensor measurements is an intrinsic aspect of the measurement process. In this paper, we develop an analytical method to quantify the impact of measurement uncertainties in numerical methods that employ the Koopman operator to identify nonlinear dynamics based on recorded data. In particular, we quantify the confidence interval of each element in the push-forward matrix from which a subset of the Koopman operator's discrete spectrum is estimated. We provide a detailed numerical analysis of the developed method applied to numerical simulations and field data collected from experiments conducted in a megawatt-scale facility at the National Renewable Energy Laboratory.","sentences":["Sensor measurements are mission-critical for monitoring and controlling power systems because they provide real-time insight into the grid operating condition; however, confidence in these insights depends greatly on the quality of the sensor data.","Uncertainty in sensor measurements is an intrinsic aspect of the measurement process.","In this paper, we develop an analytical method to quantify the impact of measurement uncertainties in numerical methods that employ the Koopman operator to identify nonlinear dynamics based on recorded data.","In particular, we quantify the confidence interval of each element in the push-forward matrix from which a subset of the Koopman operator's discrete spectrum is estimated.","We provide a detailed numerical analysis of the developed method applied to numerical simulations and field data collected from experiments conducted in a megawatt-scale facility at the National Renewable Energy Laboratory."],"url":"http://arxiv.org/abs/2403.17339v1","category":"stat.AP"}
{"created":"2024-03-26 02:49:08","title":"Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems","abstract":"Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input. We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways. Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method.","sentences":["Optimal control methods provide solutions to safety-critical problems but easily become intractable.","Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss.","This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced.","Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness.","%as well as infeasibility.","To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF).","In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input.","We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways.","Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.17338v1","category":"eess.SY"}
{"created":"2024-03-26 02:33:36","title":"The Pursuit of Fairness in Artificial Intelligence Models: A Survey","abstract":"Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems. We explore the different definitions of fairness existing in the current literature. We create a comprehensive taxonomy by categorizing different types of bias and investigate cases of biased AI in different application domains. A thorough study is conducted of the approaches and techniques employed by researchers to mitigate bias in AI models. Moreover, we also delve into the impact of biased models on user experience and the ethical considerations to contemplate when developing and deploying such models. We hope this survey helps researchers and practitioners understand the intricate details of fairness and bias in AI systems. By sharing this thorough survey, we aim to promote additional discourse in the domain of equitable and responsible AI.","sentences":["Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment.","Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter.","Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people.","With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them.","Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias.","This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems.","We explore the different definitions of fairness existing in the current literature.","We create a comprehensive taxonomy by categorizing different types of bias and investigate cases of biased AI in different application domains.","A thorough study is conducted of the approaches and techniques employed by researchers to mitigate bias in AI models.","Moreover, we also delve into the impact of biased models on user experience and the ethical considerations to contemplate when developing and deploying such models.","We hope this survey helps researchers and practitioners understand the intricate details of fairness and bias in AI systems.","By sharing this thorough survey, we aim to promote additional discourse in the domain of equitable and responsible AI."],"url":"http://arxiv.org/abs/2403.17333v1","category":"cs.AI"}
{"created":"2024-03-26 02:30:50","title":"FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling","abstract":"Many AI platforms, including traffic monitoring systems, use Federated Learning (FL) for decentralized sensor data processing for learning-based applications while preserving privacy and ensuring secured information transfer. On the other hand, applying supervised learning to large data samples, like high-resolution images requires intensive human labor to label different parts of a data sample. Multiple Instance Learning (MIL) alleviates this challenge by operating over labels assigned to the 'bag' of instances. In this paper, we introduce Federated Multiple-Instance Learning (FedMIL). This framework applies federated learning to boost the training performance in video-based MIL tasks such as vehicle accident detection using distributed CCTV networks. However, data sources in decentralized settings are not typically Independently and Identically Distributed (IID), making client selection imperative to collectively represent the entire dataset with minimal clients. To address this challenge, we propose DPPQ, a framework based on the Determinantal Point Process (DPP) with a quality-based kernel to select clients with the most diverse datasets that achieve better performance compared to both random selection and current DPP-based client selection methods even with less data utilization in the majority of non-IID cases. This offers a significant advantage for deployment on edge devices with limited computational resources, providing a reliable solution for training AI models in massive smart sensor networks.","sentences":["Many AI platforms, including traffic monitoring systems, use Federated Learning (FL) for decentralized sensor data processing for learning-based applications while preserving privacy and ensuring secured information transfer.","On the other hand, applying supervised learning to large data samples, like high-resolution images requires intensive human labor to label different parts of a data sample.","Multiple Instance Learning (MIL) alleviates this challenge by operating over labels assigned to the 'bag' of instances.","In this paper, we introduce Federated Multiple-Instance Learning (FedMIL).","This framework applies federated learning to boost the training performance in video-based MIL tasks such as vehicle accident detection using distributed CCTV networks.","However, data sources in decentralized settings are not typically Independently and Identically Distributed (IID), making client selection imperative to collectively represent the entire dataset with minimal clients.","To address this challenge, we propose DPPQ, a framework based on the Determinantal Point Process (DPP) with a quality-based kernel to select clients with the most diverse datasets that achieve better performance compared to both random selection and current DPP-based client selection methods even with less data utilization in the majority of non-IID cases.","This offers a significant advantage for deployment on edge devices with limited computational resources, providing a reliable solution for training AI models in massive smart sensor networks."],"url":"http://arxiv.org/abs/2403.17331v1","category":"cs.DC"}
{"created":"2024-03-26 02:24:32","title":"Deep Support Vectors","abstract":"While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.","sentences":["While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored.","This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models.","We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning.","Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models.","Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM.","The code will be available."],"url":"http://arxiv.org/abs/2403.17329v1","category":"cs.LG"}
{"created":"2024-03-26 02:22:08","title":"Learning Traffic Signal Control via Genetic Programming","abstract":"The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic programming is adopted to perform gradient-free optimization of the urgency function. We test our algorithm on multiple public traffic signal control datasets. The experimental results indicate that the tree-shaped urgency function evolved by genetic programming outperforms the baselines, including a state-of-the-art method in the transportation field and a well-known DRL-based method.","sentences":["The control of traffic signals is crucial for improving transportation efficiency.","Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies.","However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability.","In this work, a new learning-based method for signal control in complex intersections is proposed.","In our approach, we design a concept of phase urgency for each signal phase.","During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency.","We then proposed to represent the urgency function as an explainable tree structure.","The urgency function can calculate the phase urgency for a specific phase based on the current road conditions.","Genetic programming is adopted to perform gradient-free optimization of the urgency function.","We test our algorithm on multiple public traffic signal control datasets.","The experimental results indicate that the tree-shaped urgency function evolved by genetic programming outperforms the baselines, including a state-of-the-art method in the transportation field and a well-known DRL-based method."],"url":"http://arxiv.org/abs/2403.17328v1","category":"cs.AI"}
{"created":"2024-03-26 02:17:07","title":"Unveiling the origin of unconventional moire ferroelectricity","abstract":"Interfacial ferroelectricity emerges in heterostructures consisting of nonpolar van der Waals (vdW) layers, greatly expanding the scope of two dimensional ferroelectrics. In particular, the unconventional moire ferroelectricity observed in bilayer graphene/boron nitride (BN) heterostructures, exhibits promising functionalities with topological current, superconductivity and synaptic responses. However, the debate about its mechanism - correlation driven charge transfer between two graphene layers - limits device reproducibility and hence large-scale production. Here by designing a single-layer graphene encapsulated by lattice-mismatched WSe2, we identify the ferroelectricity as stemming from - instead of graphene moire bands - the particular BN, where interfacial sliding ferroelectricity must play a role. With similar structures, multilayer twisted MoS2 is found to reproduce the ferroelectricity. The key is a conductive moire ferroelectric, where the screened gate and the pinned domain wall together result in unchanged electronic states, i.e. anomalous screening. The intimate connection to interfacial sliding ferroelectricity thus provides advantages of diverse choices of constituent materials and robust polarization switching while preserving the unique anomalous screening, paving the way to reproducible and reliable memory-based devices in artificial intelligence.","sentences":["Interfacial ferroelectricity emerges in heterostructures consisting of nonpolar van der Waals (vdW) layers, greatly expanding the scope of two dimensional ferroelectrics.","In particular, the unconventional moire ferroelectricity observed in bilayer graphene/boron nitride (BN) heterostructures, exhibits promising functionalities with topological current, superconductivity and synaptic responses.","However, the debate about its mechanism - correlation driven charge transfer between two graphene layers - limits device reproducibility and hence large-scale production.","Here by designing a single-layer graphene encapsulated by lattice-mismatched WSe2, we identify the ferroelectricity as stemming from - instead of graphene moire bands - the particular BN, where interfacial sliding ferroelectricity must play a role.","With similar structures, multilayer twisted MoS2 is found to reproduce the ferroelectricity.","The key is a conductive moire ferroelectric, where the screened gate and the pinned domain wall together result in unchanged electronic states, i.e. anomalous screening.","The intimate connection to interfacial sliding ferroelectricity thus provides advantages of diverse choices of constituent materials and robust polarization switching while preserving the unique anomalous screening, paving the way to reproducible and reliable memory-based devices in artificial intelligence."],"url":"http://arxiv.org/abs/2403.17326v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 02:09:07","title":"Unsupervised Learning for Joint Beamforming Design in RIS-aided ISAC Systems","abstract":"It is critical to design efficient beamforming in reconfigurable intelligent surface (RIS)-aided integrated sensing and communication (ISAC) systems for enhancing spectrum utilization. However, conventional methods often have limitations, either incurring high computational complexity due to iterative algorithms or sacrificing performance when using heuristic methods. To achieve both low complexity and high spectrum efficiency, an unsupervised learning-based beamforming design is proposed in this work. We tailor image-shaped channel samples and develop an ISAC beamforming neural network (IBF-Net) model for beamforming. By leveraging unsupervised learning, the loss function incorporates key performance metrics like sensing and communication channel correlation and sensing channel gain, eliminating the need of labeling. Simulations show that the proposed method achieves competitive performance compared to benchmarks while significantly reduces computational complexity.","sentences":["It is critical to design efficient beamforming in reconfigurable intelligent surface (RIS)-aided integrated sensing and communication (ISAC) systems for enhancing spectrum utilization.","However, conventional methods often have limitations, either incurring high computational complexity due to iterative algorithms or sacrificing performance when using heuristic methods.","To achieve both low complexity and high spectrum efficiency, an unsupervised learning-based beamforming design is proposed in this work.","We tailor image-shaped channel samples and develop an ISAC beamforming neural network (IBF-Net) model for beamforming.","By leveraging unsupervised learning, the loss function incorporates key performance metrics like sensing and communication channel correlation and sensing channel gain, eliminating the need of labeling.","Simulations show that the proposed method achieves competitive performance compared to benchmarks while significantly reduces computational complexity."],"url":"http://arxiv.org/abs/2403.17324v1","category":"eess.SP"}
{"created":"2024-03-26 02:08:49","title":"On the Impact of Random Node Sampling on Adaptive Diffusion Networks","abstract":"In this paper, we analyze the effects of random sampling on adaptive diffusion networks. These networks consist in a collection of nodes that can measure and process data, and that can communicate with each other to pursue a common goal of estimating an unknown system. In particular, we consider in our theoretical analysis the diffusion least-mean-squares algorithm in a scenario in which the nodes are randomly sampled. Hence, each node may or may not adapt its local estimate at a certain iteration. Our model shows that, if the nodes cooperate, a reduction in the sampling probability leads to a slight decrease in the steady-state Network Mean-Square Deviation (NMSD), assuming that the environment is stationary and that all other parameters of the algorithm are kept fixed. Furthermore, under certain circumstances, this can also ensure the stability of the algorithm in situations in which it would otherwise be unstable. Although counter-intuitive, our findings are backed by simulation results, which match the theoretical curves well.","sentences":["In this paper, we analyze the effects of random sampling on adaptive diffusion networks.","These networks consist in a collection of nodes that can measure and process data, and that can communicate with each other to pursue a common goal of estimating an unknown system.","In particular, we consider in our theoretical analysis the diffusion least-mean-squares algorithm in a scenario in which the nodes are randomly sampled.","Hence, each node may or may not adapt its local estimate at a certain iteration.","Our model shows that, if the nodes cooperate, a reduction in the sampling probability leads to a slight decrease in the steady-state Network Mean-Square Deviation (NMSD), assuming that the environment is stationary and that all other parameters of the algorithm are kept fixed.","Furthermore, under certain circumstances, this can also ensure the stability of the algorithm in situations in which it would otherwise be unstable.","Although counter-intuitive, our findings are backed by simulation results, which match the theoretical curves well."],"url":"http://arxiv.org/abs/2403.17323v1","category":"eess.SP"}
{"created":"2024-03-26 02:05:46","title":"A Bayesian shrinkage estimator for transfer learning","abstract":"Transfer learning (TL) has emerged as a powerful tool to supplement data collected for a target task with data collected for a related source task. The Bayesian framework is natural for TL because information from the source data can be incorporated in the prior distribution for the target data analysis. In this paper, we propose and study Bayesian TL methods for the normal-means problem and multiple linear regression. We propose two classes of prior distributions. The first class assumes the difference in the parameters for the source and target tasks is sparse, i.e., many parameters are shared across tasks. The second assumes that none of the parameters are shared across tasks, but the differences are bounded in $\\ell_2$-norm. For the sparse case, we propose a Bayes shrinkage estimator with theoretical guarantees under mild assumptions. The proposed methodology is tested on synthetic data and outperforms state-of-the-art TL methods. We then use this method to fine-tune the last layer of a neural network model to predict the molecular gap property in a material science application. We report improved performance compared to classical fine tuning and methods using only the target data.","sentences":["Transfer learning (TL) has emerged as a powerful tool to supplement data collected for a target task with data collected for a related source task.","The Bayesian framework is natural for TL because information from the source data can be incorporated in the prior distribution for the target data analysis.","In this paper, we propose and study Bayesian TL methods for the normal-means problem and multiple linear regression.","We propose two classes of prior distributions.","The first class assumes the difference in the parameters for the source and target tasks is sparse, i.e., many parameters are shared across tasks.","The second assumes that none of the parameters are shared across tasks, but the differences are bounded in $\\ell_2$-norm.","For the sparse case, we propose a Bayes shrinkage estimator with theoretical guarantees under mild assumptions.","The proposed methodology is tested on synthetic data and outperforms state-of-the-art TL methods.","We then use this method to fine-tune the last layer of a neural network model to predict the molecular gap property in a material science application.","We report improved performance compared to classical fine tuning and methods using only the target data."],"url":"http://arxiv.org/abs/2403.17321v1","category":"stat.ME"}
{"created":"2024-03-26 02:01:18","title":"JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset","abstract":"Dialogue datasets are crucial for deep learning-based task-oriented dialogue system research. While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2.2. In addition, through evaluation experiments of interactive dialogues with the models and human participants, we identified limitations in the task completion capabilities of LLMs in Japanese.","sentences":["Dialogue datasets are crucial for deep learning-based task-oriented dialogue system research.","While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English.","In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset.","Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods.","Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2.2.","In addition, through evaluation experiments of interactive dialogues with the models and human participants, we identified limitations in the task completion capabilities of LLMs in Japanese."],"url":"http://arxiv.org/abs/2403.17319v1","category":"cs.CL"}
{"created":"2024-03-26 01:46:34","title":"ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching","abstract":"The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching. On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss. On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems. In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X, respectively.","sentences":["The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks.","Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature.","Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses.","Yet, this approach requires increasing memory as demand grows for processing longer sequences.","The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU.","In this paper, we propose ALISA, a novel algorithm-system co-design solution to address the challenges imposed by KV caching.","On the algorithm level, ALISA prioritizes tokens that are most important in generating a new token via a Sparse Window Attention (SWA) algorithm.","SWA introduces high sparsity in attention layers and reduces the memory footprint of KV caching at negligible accuracy loss.","On the system level, ALISA employs three-phase token-level dynamical scheduling and optimizes the trade-off between caching and recomputation, thus maximizing the overall performance in resource-constrained systems.","In a single GPU-CPU system, we demonstrate that under varying workloads, ALISA improves the throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X, respectively."],"url":"http://arxiv.org/abs/2403.17312v1","category":"cs.AI"}
{"created":"2024-03-26 01:29:46","title":"Neural Multimodal Topic Modeling: A Comprehensive Evaluation","abstract":"Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the potential for their application in guiding future multimodal topic modeling endeavors.","sentences":["Neural topic models can successfully find coherent and diverse topics in textual data.","However, they are limited in dealing with multimodal datasets (e.g., images and text).","This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images.","In the process, we propose two novel topic modeling solutions and two novel evaluation metrics.","Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics.","Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future.","Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics.","This alignment not only reinforces the credibility of our metrics but also highlights the potential for their application in guiding future multimodal topic modeling endeavors."],"url":"http://arxiv.org/abs/2403.17308v1","category":"cs.CL"}
{"created":"2024-03-26 01:28:42","title":"Visual Hallucination: Definition, Quantification, and Prescriptive Remediations","abstract":"The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier.","sentences":["The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI.","In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs).","However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs).","In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA).","We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii)","Numeric Discrepancy.","We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier."],"url":"http://arxiv.org/abs/2403.17306v1","category":"cs.AI"}
{"created":"2024-03-26 00:53:24","title":"InternLM2 Technical Report","abstract":"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.","sentences":["The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI).","However, replicating such advancements in open-source models has been challenging.","This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques.","The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data.","InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.","InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking.","By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution."],"url":"http://arxiv.org/abs/2403.17297v1","category":"cs.CL"}
{"created":"2024-03-26 00:29:52","title":"Cognitively Biased Users Interacting with Algorithmically Biased Results in Whole-Session Search on Controversial Topics","abstract":"When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues. To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs). We designed three-query search sessions on debated topics under various bias conditions. We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias. Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) confirmation bias and result presentation on SERPs affect search behaviors in the current query and perceived familiarity with clicked results in subsequent queries. The bias position also affect attitude changes of users with lower perceived openness to conflicting opinions; 3) Interactions in the first query and and dwell time throughout the session are associated with users' attitude changes in different forms. Our study goes beyond traditional simulation-based evaluation settings and simulated rational users, sheds light on the mixed effects of human biases and algorithmic biases in controversial information retrieval tasks, and can inform the design of bias-aware user models, human-centered bias mitigation techniques, and socially responsible intelligent IR systems.","sentences":["When interacting with information retrieval (IR) systems, users, affected by confirmation biases, tend to select search results that confirm their existing beliefs on socially significant contentious issues.","To understand the judgments and attitude changes of users searching online, our study examined how cognitively biased users interact with algorithmically biased search engine result pages (SERPs).","We designed three-query search sessions on debated topics under various bias conditions.","We recruited 1,321 crowdsourcing participants and explored their attitude changes, search interactions, and the effects of confirmation bias.","Three key findings emerged: 1) most attitude changes occur in the initial query of a search session; 2) confirmation bias and result presentation on SERPs affect search behaviors in the current query and perceived familiarity with clicked results in subsequent queries.","The bias position also affect attitude changes of users with lower perceived openness to conflicting opinions; 3) Interactions in the first query and and dwell time throughout the session are associated with users' attitude changes in different forms.","Our study goes beyond traditional simulation-based evaluation settings and simulated rational users, sheds light on the mixed effects of human biases and algorithmic biases in controversial information retrieval tasks, and can inform the design of bias-aware user models, human-centered bias mitigation techniques, and socially responsible intelligent IR systems."],"url":"http://arxiv.org/abs/2403.17286v1","category":"cs.IR"}
{"created":"2024-03-26 00:09:38","title":"Automate Knowledge Concept Tagging on Math Questions with LLMs","abstract":"Knowledge concept tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been conducted manually with help from pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts. In this paper, we explore automating the tagging task using Large Language Models (LLMs), in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications. Moreover, the zero/few-shot learning capability of LLMs makes them well-suited for application in educational scenarios, which often face challenges in collecting large-scale, expertise-annotated datasets. By conducting extensive experiments with a variety of representative LLMs, we demonstrate that LLMs are a promising tool for concept tagging in math questions. Furthermore, through case studies examining the results from different LLMs, we draw some empirical conclusions about the key factors for success in applying LLMs to the automatic concept tagging task.","sentences":["Knowledge concept tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization.","Traditionally, these annotations have been conducted manually with help from pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts.","In this paper, we explore automating the tagging task using Large Language Models (LLMs), in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications.","Moreover, the zero/few-shot learning capability of LLMs makes them well-suited for application in educational scenarios, which often face challenges in collecting large-scale, expertise-annotated datasets.","By conducting extensive experiments with a variety of representative LLMs, we demonstrate that LLMs are a promising tool for concept tagging in math questions.","Furthermore, through case studies examining the results from different LLMs, we draw some empirical conclusions about the key factors for success in applying LLMs to the automatic concept tagging task."],"url":"http://arxiv.org/abs/2403.17281v1","category":"cs.CL"}
{"created":"2024-03-25 23:19:19","title":"Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning","abstract":"This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters. The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications.","sentences":["This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers.","By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation.","To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture.","Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks.","Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated.","To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn.","The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters.","The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications."],"url":"http://arxiv.org/abs/2403.17266v1","category":"cs.RO"}
{"created":"2024-03-25 23:15:13","title":"EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions","abstract":"Investigating child-computer interactions within their contexts is vital for designing technology that caters to children's needs. However, determining what aspects of context are relevant for designing child-centric technology remains a challenge. We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning. Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children's characteristics and contexts. This informs subsequent online observations, allowing researchers to focus on frequent interactions. Furthermore, researchers can validate preliminary assumptions with children. A means-ends analysis framework aids in the systematic analysis of data, shedding light on context, agency and homework-information searching processes children employ in their activities. To illustrate EXPLORA's capabilities, we present nine single case studies investigating Brazilian child-caregiver dyads' (children ages 9-11) use of technology in homework information-searching.","sentences":["Investigating child-computer interactions within their contexts is vital for designing technology that caters to children's needs.","However, determining what aspects of context are relevant for designing child-centric technology remains a challenge.","We introduce EXPLORA, a multimodal, multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning.","Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children's characteristics and contexts.","This informs subsequent online observations, allowing researchers to focus on frequent interactions.","Furthermore, researchers can validate preliminary assumptions with children.","A means-ends analysis framework aids in the systematic analysis of data, shedding light on context, agency and homework-information searching processes children employ in their activities.","To illustrate EXPLORA's capabilities, we present nine single case studies investigating Brazilian child-caregiver dyads' (children ages 9-11) use of technology in homework information-searching."],"url":"http://arxiv.org/abs/2403.17264v1","category":"cs.HC"}
{"created":"2024-03-25 22:50:43","title":"Methodological Problems in Every Black-Box Study of Forensic Firearm Comparisons","abstract":"Reviews conducted by the National Academy of Sciences (2009) and the President's Council of Advisors on Science and Technology (2016) concluded that the field of forensic firearm comparisons has not been demonstrated to be scientifically valid. Scientific validity requires adequately designed studies of firearm examiner performance in terms of accuracy, repeatability, and reproducibility. Researchers have performed ``black-box'' studies with the goal of estimating these performance measures. As statisticians with expertise in experimental design, we conducted a literature search of such studies to date and then evaluated the design and statistical analysis methods used in each study. Our conclusion is that all studies in our literature search have methodological flaws that are so grave that they render the studies invalid, that is, incapable of establishing scientific validity of the field of firearms examination. Notably, error rates among firearms examiners, both collectively and individually, remain unknown. Therefore, statements about the common origin of bullets or cartridge cases that are based on examination of ``individual\" characteristics do not have a scientific basis. We provide some recommendations for the design and analysis of future studies.","sentences":["Reviews conducted by the National Academy of Sciences (2009) and the President's Council of Advisors on Science and Technology (2016) concluded that the field of forensic firearm comparisons has not been demonstrated to be scientifically valid.","Scientific validity requires adequately designed studies of firearm examiner performance in terms of accuracy, repeatability, and reproducibility.","Researchers have performed ``black-box'' studies with the goal of estimating these performance measures.","As statisticians with expertise in experimental design, we conducted a literature search of such studies to date and then evaluated the design and statistical analysis methods used in each study.","Our conclusion is that all studies in our literature search have methodological flaws that are so grave that they render the studies invalid, that is, incapable of establishing scientific validity of the field of firearms examination.","Notably, error rates among firearms examiners, both collectively and individually, remain unknown.","Therefore, statements about the common origin of bullets or cartridge cases that are based on examination of ``individual\" characteristics do not have a scientific basis.","We provide some recommendations for the design and analysis of future studies."],"url":"http://arxiv.org/abs/2403.17248v1","category":"stat.AP"}
{"created":"2024-03-25 22:49:56","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","abstract":"We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \\texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \\texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA applications, including multi-agent and distributed temporal difference (TD) learning, Q-learning and stochastic optimization with correlated data.","sentences":["We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server.","We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays.","To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation.","We provide a finite-time analysis of \\texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains.","Significantly advancing existing results, \\texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling.","Our work is relevant for various SA applications, including multi-agent and distributed temporal difference (TD) learning, Q-learning and stochastic optimization with correlated data."],"url":"http://arxiv.org/abs/2403.17247v1","category":"cs.AI"}
{"created":"2024-03-25 22:47:13","title":"TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models","abstract":"Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences. We combine the strengths of classical planning and LLMs by approximating human intuitions for two-agent planning goal decomposition. We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone and preserving execution success. Additionally, we find that LLM-based approximations of subgoals can achieve similar multi-agent execution steps than those specified by human experts. Website and resources at https://glamor-usc.github.io/twostep","sentences":["Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible.","However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other.","A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning.","By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences.","We combine the strengths of classical planning and LLMs by approximating human intuitions for two-agent planning goal decomposition.","We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone and preserving execution success.","Additionally, we find that LLM-based approximations of subgoals can achieve similar multi-agent execution steps than those specified by human experts.","Website and resources at https://glamor-usc.github.io/twostep"],"url":"http://arxiv.org/abs/2403.17246v1","category":"cs.AI"}
{"created":"2024-03-25 22:34:05","title":"DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion","abstract":"We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions.","sentences":["We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions.","While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness.","This problem becomes particularly noticeable for methods that work with text input alone.","To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views.","Initially, a coarse 3D generation undergoes refinement via geometric optimization.","Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset.","Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions."],"url":"http://arxiv.org/abs/2403.17237v1","category":"cs.CV"}
{"created":"2024-03-25 22:21:23","title":"Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking","abstract":"In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle's outcomes, we are able to model a value estimator and a policy generator for given states. By doing that, we build up a balancing mechanism between exploration and exploitation, speeding up the path planning process while maintaining its quality without using human expert driver data.","sentences":["In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks.","Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming.","State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system.","Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way.","To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework.","By iteratively learning the value of a state and the best action among samples from its previous cycle's outcomes, we are able to model a value estimator and a policy generator for given states.","By doing that, we build up a balancing mechanism between exploration and exploitation, speeding up the path planning process while maintaining its quality without using human expert driver data."],"url":"http://arxiv.org/abs/2403.17234v1","category":"cs.AI"}
{"created":"2024-03-25 22:17:51","title":"Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination","abstract":"This paper presents a self-supervised learning method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation. For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while reinforcement learning becomes inefficient due to the high probability of collision during exploration. To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles. In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample dynamic obstacles from it, so the generated training data can be used to learn a motion planner to navigate in dynamic environments. Dyna-LfLH is evaluated on a ground robot in both simulated and physical environments and achieves up to 25% better success rate compared to baselines.","sentences":["This paper presents a self-supervised learning method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles.","When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation.","For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while reinforcement learning becomes inefficient due to the high probability of collision during exploration.","To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles.","In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample dynamic obstacles from it, so the generated training data can be used to learn a motion planner to navigate in dynamic environments.","Dyna-LfLH is evaluated on a ground robot in both simulated and physical environments and achieves up to 25% better success rate compared to baselines."],"url":"http://arxiv.org/abs/2403.17231v1","category":"cs.RO"}
{"created":"2024-03-25 21:56:02","title":"Uncertainty Quantification for Gradient-based Explanations in Neural Networks","abstract":"Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to evaluate the quality of the generated explanations.","sentences":["Explanation methods help understand the reasons for a model's prediction.","These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model.","With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods.","In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods.","We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets.","By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them.","Additionally, we compute modified pixel insertion/deletion metrics to evaluate the quality of the generated explanations."],"url":"http://arxiv.org/abs/2403.17224v1","category":"cs.LG"}
{"created":"2024-03-25 21:53:36","title":"Co-Occurring of Object Detection and Identification towards unlabeled object discovery","abstract":"In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories. Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns. These frequent patterns will show base classes and their corresponding co-occurring classes. We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results on public benchmark dataset is reported in Sec 4. Further we extend this work by considering all frequently objects as unlabeled and what if they are occluded as well.","sentences":["In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories.","Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes.","The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis.","In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns.","These frequent patterns will show base classes and their corresponding co-occurring classes.","We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO.","The experimental results on public benchmark dataset is reported in Sec 4.","Further we extend this work by considering all frequently objects as unlabeled and what if they are occluded as well."],"url":"http://arxiv.org/abs/2403.17223v1","category":"cs.CV"}
{"created":"2024-03-25 21:48:22","title":"SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies","abstract":"Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate their responses on psychological scales. We demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses on one scale using responses from another as behavioral information. We also evaluate the alignment between human and SeSaMe-simulated responses to psychological scales. Then, we present experiments to inspect the utility of SeSaMe-simulated responses as ground truth in training ML models by replicating established depression and anxiety screening tasks from a previous study. Our results indicate SeSaMe to be a promising approach, but its alignment may vary across scales and specific prediction objectives. We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges researchers face with ground-truth collection in passive sensing studies.","sentences":["Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health.","These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models.","However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses.","In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies.","By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales.","In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate their responses on psychological scales.","We demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses on one scale using responses from another as behavioral information.","We also evaluate the alignment between human and SeSaMe-simulated responses to psychological scales.","Then, we present experiments to inspect the utility of SeSaMe-simulated responses as ground truth in training ML models by replicating established depression and anxiety screening tasks from a previous study.","Our results indicate SeSaMe to be a promising approach, but its alignment may vary across scales and specific prediction objectives.","We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios.","We conclude by discussing the potential implications of SeSaMe in addressing some challenges researchers face with ground-truth collection in passive sensing studies."],"url":"http://arxiv.org/abs/2403.17219v1","category":"cs.HC"}
{"created":"2024-03-25 21:46:53","title":"DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment","abstract":"Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning. We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance.","sentences":["Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions.","Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed.","Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images.","To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment.","Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions.","Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific fine-tuning.","We compare against state-of-the-art GAN-, StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment performance."],"url":"http://arxiv.org/abs/2403.17217v1","category":"cs.CV"}
{"created":"2024-03-25 21:43:43","title":"An Undergraduate Consortium for Addressing the Leaky Pipeline to Computing Research","abstract":"Despite an increasing number of successful interventions designed to broaden participation in computing research, there is still significant attrition among historically marginalized groups in the computing research pipeline. This experience report describes a first-of-its-kind Undergraduate Consortium (UC) that addresses this challenge by empowering students with a culmination of their undergraduate research in a conference setting. The UC, conducted at the AAAI Conference on Artificial Intelligence (AAAI), aims to broaden participation in the AI research community by recruiting students, particularly those from historically marginalized groups, supporting them with mentorship, advising, and networking as an accelerator toward graduate school, AI research, and their scientific identity. This paper presents our program design, inspired by a rich set of evidence-based practices, and a preliminary evaluation of the first years that points to the UC achieving many of its desired outcomes. We conclude by discussing insights to improve our program and expand to other computing communities.","sentences":["Despite an increasing number of successful interventions designed to broaden participation in computing research, there is still significant attrition among historically marginalized groups in the computing research pipeline.","This experience report describes a first-of-its-kind Undergraduate Consortium (UC) that addresses this challenge by empowering students with a culmination of their undergraduate research in a conference setting.","The UC, conducted at the AAAI Conference on Artificial Intelligence (AAAI), aims to broaden participation in the AI research community by recruiting students, particularly those from historically marginalized groups, supporting them with mentorship, advising, and networking as an accelerator toward graduate school, AI research, and their scientific identity.","This paper presents our program design, inspired by a rich set of evidence-based practices, and a preliminary evaluation of the first years that points to the UC achieving many of its desired outcomes.","We conclude by discussing insights to improve our program and expand to other computing communities."],"url":"http://arxiv.org/abs/2403.17215v1","category":"cs.CY"}
{"created":"2024-03-25 21:41:31","title":"Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation","abstract":"Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation.","sentences":["Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance.","With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models.","In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python.","Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code.","Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance.","This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA).","Our results demonstrate that a strategic combination of prompt engineering and regular expression can effectively extract the source code from the model generation output.","In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%.","Our findings shed light on and motivate future research to conduct more reliable benchmarks of LLMs for code translation."],"url":"http://arxiv.org/abs/2403.17214v1","category":"cs.SE"}
{"created":"2024-03-25 21:39:33","title":"Sanity Checks for Explanation Uncertainty","abstract":"Explanations for machine learning models can be hard to interpret or be wrong. Combining an explanation method with an uncertainty estimation method produces explanation uncertainty. Evaluating explanation uncertainty is difficult. In this paper we propose sanity checks for uncertainty explanation methods, where a weight and data randomization tests are defined for explanations with uncertainty, allowing for quick tests to combinations of uncertainty and explanation methods. We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests with Guided Backpropagation, Integrated Gradients, and LIME explanations.","sentences":["Explanations for machine learning models can be hard to interpret or be wrong.","Combining an explanation method with an uncertainty estimation method produces explanation uncertainty.","Evaluating explanation uncertainty is difficult.","In this paper we propose sanity checks for uncertainty explanation methods, where a weight and data randomization tests are defined for explanations with uncertainty, allowing for quick tests to combinations of uncertainty and explanation methods.","We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests with Guided Backpropagation, Integrated Gradients, and LIME explanations."],"url":"http://arxiv.org/abs/2403.17212v1","category":"cs.LG"}
{"created":"2024-03-25 21:37:31","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions","abstract":"Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Our customized VGAE consists of a graph encoder, a latent information encoder, and an MLP decoder. CADGL surpasses other state-of-the-art DDI prediction models, excelling in predicting clinically valuable novel DDIs, supported by rigorous case studies.","sentences":["Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development.","DDIs occur when one drug's properties are affected by the inclusion of other drugs.","Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings.","However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities.","We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL.","Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure.","Our customized VGAE consists of a graph encoder, a latent information encoder, and an MLP decoder.","CADGL surpasses other state-of-the-art DDI prediction models, excelling in predicting clinically valuable novel DDIs, supported by rigorous case studies."],"url":"http://arxiv.org/abs/2403.17210v1","category":"cs.LG"}
{"created":"2024-03-25 21:37:30","title":"Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node","abstract":"This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a \"semantic node\" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process \"semantic node\" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts. Our findings emphasize LLMs' capability in automating AAS instance creation, enhancing semantic interoperability, and contributing to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are released on our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM","sentences":["This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort.","We construct a \"semantic node\" data structure to capture the semantic essence of textual data.","Then, a system powered by large language models is designed and implemented to process \"semantic node\" and generate AAS instance models from textual technical data.","Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models.","In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts.","Our findings emphasize LLMs' capability in automating AAS instance creation, enhancing semantic interoperability, and contributing to the broader field of semantic interoperability for digital twins in industrial applications.","The prototype implementation and evaluation results are released on our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM"],"url":"http://arxiv.org/abs/2403.17209v1","category":"cs.AI"}
{"created":"2024-03-25 21:28:45","title":"Observation of polarization density waves in SrTiO3","abstract":"The nature of the \"failed\" ferroelectric transition in SrTiO3 has been a long-standing puzzle in condensed matter physics. A compelling explanation is the competition between ferroelectricity and an instability with a mesoscopic modulation of the polarization. These polarization density waves, which should become especially strong near the quantum critical point, break local inversion symmetry and are difficult to probe with conventional x-ray scattering methods. Here we combine a femtosecond x-ray free electron laser (XFEL) with THz coherent control methods to probe inversion symmetry breaking at finite momenta and visualize the instability of the polarization on nanometer lengthscales in SrTiO3. We find polar-acoustic collective modes that are soft particularly at the tens of nanometer lengthscale. These precursor collective excitations provide evidence for the conjectured mesoscopic modulated phase in SrTiO3.","sentences":["The nature of the \"failed\" ferroelectric transition in SrTiO3 has been a long-standing puzzle in condensed matter physics.","A compelling explanation is the competition between ferroelectricity and an instability with a mesoscopic modulation of the polarization.","These polarization density waves, which should become especially strong near the quantum critical point, break local inversion symmetry and are difficult to probe with conventional x-ray scattering methods.","Here we combine a femtosecond x-ray free electron laser (XFEL) with THz coherent control methods to probe inversion symmetry breaking at finite momenta and visualize the instability of the polarization on nanometer lengthscales in SrTiO3.","We find polar-acoustic collective modes that are soft particularly at the tens of nanometer lengthscale.","These precursor collective excitations provide evidence for the conjectured mesoscopic modulated phase in SrTiO3."],"url":"http://arxiv.org/abs/2403.17203v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-25 20:43:23","title":"Engagement Measurement Based on Facial Landmarks and Spatial-Temporal Graph Convolutional Networks","abstract":"Engagement in virtual learning is crucial for a variety of factors including learner satisfaction, performance, and compliance with learning programs, but measuring it is a challenging task. There is therefore considerable interest in utilizing artificial intelligence and affective computing to measure engagement in natural settings as well as on a large scale. This paper introduces a novel, privacy-preserving method for engagement measurement from videos. It uses facial landmarks, which carry no personally identifiable information, extracted from videos via the MediaPipe deep learning solution. The extracted facial landmarks are fed to a Spatial-Temporal Graph Convolutional Network (ST-GCN) to output the engagement level of the learner in the video. To integrate the ordinal nature of the engagement variable into the training process, ST-GCNs undergo training in a novel ordinal learning framework based on transfer learning. Experimental results on two video student engagement measurement datasets show the superiority of the proposed method compared to previous methods with improved state-of-the-art on the EngageNet dataset with a %3.1 improvement in four-class engagement level classification accuracy and on the Online Student Engagement dataset with a %1.5 improvement in binary engagement classification accuracy. The relatively lightweight ST-GCN and its integration with the real-time MediaPipe deep learning solution make the proposed approach capable of being deployed on virtual learning platforms and measuring engagement in real time.","sentences":["Engagement in virtual learning is crucial for a variety of factors including learner satisfaction, performance, and compliance with learning programs, but measuring it is a challenging task.","There is therefore considerable interest in utilizing artificial intelligence and affective computing to measure engagement in natural settings as well as on a large scale.","This paper introduces a novel, privacy-preserving method for engagement measurement from videos.","It uses facial landmarks, which carry no personally identifiable information, extracted from videos via the MediaPipe deep learning solution.","The extracted facial landmarks are fed to a Spatial-Temporal Graph Convolutional Network (ST-GCN) to output the engagement level of the learner in the video.","To integrate the ordinal nature of the engagement variable into the training process, ST-GCNs undergo training in a novel ordinal learning framework based on transfer learning.","Experimental results on two video student engagement measurement datasets show the superiority of the proposed method compared to previous methods with improved state-of-the-art on the EngageNet dataset with a %3.1 improvement in four-class engagement level classification accuracy and on the Online Student Engagement dataset with a %1.5 improvement in binary engagement classification accuracy.","The relatively lightweight ST-GCN and its integration with the real-time MediaPipe deep learning solution make the proposed approach capable of being deployed on virtual learning platforms and measuring engagement in real time."],"url":"http://arxiv.org/abs/2403.17175v1","category":"cs.CV"}
{"created":"2024-03-25 20:43:17","title":"Belief Samples Are All You Need For Social Learning","abstract":"In this paper, we consider the problem of social learning, where a group of agents embedded in a social network are interested in learning an underlying state of the world. Agents have incomplete, noisy, and heterogeneous sources of information, providing them with recurring private observations of the underlying state of the world. Agents can share their learning experience with their peers by taking actions observable to them, with values from a finite feasible set of states. Actions can be interpreted as samples from the beliefs which agents may form and update on what the true state of the world is. Sharing samples, in place of full beliefs, is motivated by the limited communication, cognitive, and information-processing resources available to agents especially in large populations. Previous work (Salhab et al.) poses the question as to whether learning with probability one is still achievable if agents are only allowed to communicate samples from their beliefs. We provide a definite positive answer to this question, assuming a strongly connected network and a ``collective distinguishability'' assumption, which are both required for learning even in full-belief-sharing settings. In our proposed belief update mechanism, each agent's belief is a normalized weighted geometric interpolation between a fully Bayesian private belief -- aggregating information from the private source -- and an ensemble of empirical distributions of the samples shared by her neighbors over time. By carefully constructing asymptotic almost-sure lower/upper bounds on the frequency of shared samples matching the true state/or not, we rigorously prove the convergence of all the beliefs to the true state, with probability one.","sentences":["In this paper, we consider the problem of social learning, where a group of agents embedded in a social network are interested in learning an underlying state of the world.","Agents have incomplete, noisy, and heterogeneous sources of information, providing them with recurring private observations of the underlying state of the world.","Agents can share their learning experience with their peers by taking actions observable to them, with values from a finite feasible set of states.","Actions can be interpreted as samples from the beliefs which agents may form and update on what the true state of the world is.","Sharing samples, in place of full beliefs, is motivated by the limited communication, cognitive, and information-processing resources available to agents especially in large populations.","Previous work (Salhab et al.) poses the question as to whether learning with probability one is still achievable if agents are only allowed to communicate samples from their beliefs.","We provide a definite positive answer to this question, assuming a strongly connected network and a ``collective distinguishability'' assumption, which are both required for learning even in full-belief-sharing settings.","In our proposed belief update mechanism, each agent's belief is a normalized weighted geometric interpolation between a fully Bayesian private belief -- aggregating information from the private source -- and an ensemble of empirical distributions of the samples shared by her neighbors over time.","By carefully constructing asymptotic almost-sure lower/upper bounds on the frequency of shared samples matching the true state/or not, we rigorously prove the convergence of all the beliefs to the true state, with probability one."],"url":"http://arxiv.org/abs/2403.17174v1","category":"cs.LG"}
{"created":"2024-03-25 20:36:03","title":"NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions","abstract":"Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp serves as a challenging evaluation set for numerical claim verification.","sentences":["Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era.","Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims.","In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage.","This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims.","We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims.","We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32.","This demonstrates that Numtemp serves as a challenging evaluation set for numerical claim verification."],"url":"http://arxiv.org/abs/2403.17169v1","category":"cs.CL"}
{"created":"2024-03-25 20:29:06","title":"Building an Open-Source Community to Enhance Autonomic Nervous System Signal Analysis: DBDP-Autonomic","abstract":"Smartphones and wearable sensors offer an unprecedented ability to collect peripheral psychophysiological signals across diverse timescales, settings, populations, and modalities. However, open-source software development has yet to keep pace with rapid advancements in hardware technology and availability, creating an analytical barrier that limits the scientific usefulness of acquired data. We propose a community-driven, open-source peripheral psychophysiological signal pre-processing and analysis software framework that could advance biobehavioral health by enabling more robust, transparent, and reproducible inferences involving autonomic nervous system data.","sentences":["Smartphones and wearable sensors offer an unprecedented ability to collect peripheral psychophysiological signals across diverse timescales, settings, populations, and modalities.","However, open-source software development has yet to keep pace with rapid advancements in hardware technology and availability, creating an analytical barrier that limits the scientific usefulness of acquired data.","We propose a community-driven, open-source peripheral psychophysiological signal pre-processing and analysis software framework that could advance biobehavioral health by enabling more robust, transparent, and reproducible inferences involving autonomic nervous system data."],"url":"http://arxiv.org/abs/2403.17165v1","category":"cs.HC"}
{"created":"2024-03-25 20:29:04","title":"Multi-Objective Quality-Diversity for Crystal Structure Prediction","abstract":"Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness the power of Multi-Objective Quality-Diversity algorithms in order to find crystal structures which have diverse features and achieve different trade-offs of objectives. We analyse our approach on 5 crystal systems and demonstrate that it is not only able to re-discover known real-life structures, but also find promising new ones. Moreover, we propose a method for illuminating the objective space to gain an understanding of what trade-offs can be achieved.","sentences":["Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations.","However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function.","This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation.","By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics.","However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency.","Therefore, in this work, we harness the power of Multi-Objective Quality-Diversity algorithms in order to find crystal structures which have diverse features and achieve different trade-offs of objectives.","We analyse our approach on 5 crystal systems and demonstrate that it is not only able to re-discover known real-life structures, but also find promising new ones.","Moreover, we propose a method for illuminating the objective space to gain an understanding of what trade-offs can be achieved."],"url":"http://arxiv.org/abs/2403.17164v1","category":"cs.NE"}
{"created":"2024-03-25 20:16:16","title":"Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP","abstract":"Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that for GNNs appropriate sparsification and ensembles of different sparsification levels lead to substantial performance increases of the overall architecture. We also design a new, state-of-the-art transformer encoder with ensembles of attention masking. These transformers increase model performance from a gap of $0.16\\%$ to $0.10\\%$ for TSP instances of size 100 and from $0.02\\%$ to $0.00\\%$ for TSP instances of size 50.","sentences":["Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture.","However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances.","We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only.","In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations.","Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance.","In the experimental studies, we show that for GNNs appropriate sparsification and ensembles of different sparsification levels lead to substantial performance increases of the overall architecture.","We also design a new, state-of-the-art transformer encoder with ensembles of attention masking.","These transformers increase model performance from a gap of $0.16\\%$ to $0.10\\%$ for TSP instances of size 100 and from $0.02\\%$ to $0.00\\%$ for TSP instances of size 50."],"url":"http://arxiv.org/abs/2403.17159v1","category":"cs.LG"}
{"created":"2024-03-25 19:50:07","title":"Hearing the shape of an arena with spectral swarm robotics","abstract":"Swarm robotics promises adaptability to unknown situations and robustness against failures. However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded. Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues. This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm. Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function. Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian. Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Laplacian operator - enabling them to \"hear\" the spectrum of their arena. We reveal a universal scaling that links the optimal number of robots (a global parameter) with their optimal radius of interaction (a local parameter). We validate experimentally spectral swarm robotics under challenging conditions with the one-shot classification of arena shapes using a sparse swarm of Kilobots. Spectral methods can assist with challenging tasks where robots need to build an emergent consensus on their environment, such as adaptation to unknown terrains, division of labor, or quorum sensing. Spectral methods may extend beyond robotics to analyze and coordinate swarms of agents of various natures, such as traffic or crowds, and to better understand the long-range dynamics of natural systems emerging from short-range interactions.","sentences":["Swarm robotics promises adaptability to unknown situations and robustness against failures.","However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded.","Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues.","This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm.","Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function.","Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian.","Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Laplacian operator - enabling them to \"hear\" the spectrum of their arena.","We reveal a universal scaling that links the optimal number of robots (a global parameter) with their optimal radius of interaction (a local parameter).","We validate experimentally spectral swarm robotics under challenging conditions with the one-shot classification of arena shapes using a sparse swarm of Kilobots.","Spectral methods can assist with challenging tasks where robots need to build an emergent consensus on their environment, such as adaptation to unknown terrains, division of labor, or quorum sensing.","Spectral methods may extend beyond robotics to analyze and coordinate swarms of agents of various natures, such as traffic or crowds, and to better understand the long-range dynamics of natural systems emerging from short-range interactions."],"url":"http://arxiv.org/abs/2403.17147v1","category":"cs.RO"}
{"created":"2024-03-25 19:28:10","title":"MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models","abstract":"Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 11 policy models with up to 63x more parameters, and outperforms previous alignment methods with down to 22.27x less computational resources. The model also accurately aligns with unseen objectives, marking the first step towards generalizable multi-objective preference alignment.","sentences":["Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment.","However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives.","In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses.","MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning.","Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 11 policy models with up to 63x more parameters, and outperforms previous alignment methods with down to 22.27x less computational resources.","The model also accurately aligns with unseen objectives, marking the first step towards generalizable multi-objective preference alignment."],"url":"http://arxiv.org/abs/2403.17141v1","category":"cs.CL"}
{"created":"2024-03-25 19:17:43","title":"RepairAgent: An Autonomous, LLM-Based Agent for Program Repair","abstract":"Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering.","sentences":["Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience.","This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM).","Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools.","RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts.","Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interact with these tools, and a finite state machine that guides the agent in invoking the tools.","Our evaluation on the popular Defects4J dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques.","Interacting with the LLM imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug.","To the best of our knowledge, this work is the first to present an autonomous, LLM-based agent for program repair, paving the way for future agent-based techniques in software engineering."],"url":"http://arxiv.org/abs/2403.17134v1","category":"cs.SE"}
{"created":"2024-03-25 19:15:19","title":"Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification","abstract":"Dataset distillation aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset. Consequently, data distillation methods are usually tied to a specific ML algorithm. While recent literature deals mainly with distillation of large collections of images in the context of neural network models, tabular data distillation is much less represented and mainly focused on a theoretical perspective. The current paper explores the potential of a simple distillation technique previously proposed in the context of Less-than-one shot learning. The main goal is to push further the performance of prototype-based soft-labels distillation in terms of classification accuracy, by integrating optimization steps in the distillation process. The analysis is performed on real-world data sets with various degrees of imbalance. Experimental studies trace the capability of the method to distill the data, but also the opportunity to act as an augmentation method, i.e. to generate new data that is able to increase model accuracy when used in conjunction with - as opposed to instead of - the original data.","sentences":["Dataset distillation aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset.","Consequently, data distillation methods are usually tied to a specific ML algorithm.","While recent literature deals mainly with distillation of large collections of images in the context of neural network models, tabular data distillation is much less represented and mainly focused on a theoretical perspective.","The current paper explores the potential of a simple distillation technique previously proposed in the context of Less-than-one shot learning.","The main goal is to push further the performance of prototype-based soft-labels distillation in terms of classification accuracy, by integrating optimization steps in the distillation process.","The analysis is performed on real-world data sets with various degrees of imbalance.","Experimental studies trace the capability of the method to distill the data, but also the opportunity to act as an augmentation method, i.e. to generate new data that is able to increase model accuracy when used in conjunction with - as opposed to instead of - the original data."],"url":"http://arxiv.org/abs/2403.17130v1","category":"cs.LG"}
{"created":"2024-03-25 19:07:32","title":"The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition","abstract":"In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning. The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to explicitly quantify the consistency of proxies of LLM priors and their pull on the posteriors. We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions. We also find that the larger the model, the stronger these effects become. Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results.","sentences":["In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning.","The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost.","The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors).","However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors.","This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations.","In this work, we design experiments and propose measurements to explicitly quantify the consistency of proxies of LLM priors and their pull on the posteriors.","We show that LLMs have strong yet inconsistent priors in emotion recognition that ossify their predictions.","We also find that the larger the model, the stronger these effects become.","Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results."],"url":"http://arxiv.org/abs/2403.17125v1","category":"cs.CL"}
{"created":"2024-03-25 19:04:59","title":"Grounding Language Plans in Demonstrations Through Counterfactual Perturbations","abstract":"Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://sites.google.com/view/grounding-plans","sentences":["Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI.","Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations.","Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot.","By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task.","Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling.","The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner.","We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks.","Website: https://sites.google.com/view/grounding-plans"],"url":"http://arxiv.org/abs/2403.17124v1","category":"cs.RO"}
{"created":"2024-03-25 18:56:07","title":"Competition between allowed and first-forbidden $\u03b2$ decay in $r$-process waiting-point nuclei within a relativistic beyond-mean-field approach","abstract":"We compute $\\beta$-decay half-lives of isotonic nuclear chains located at neutron shell closures $N=50$, $82$, $126$ and $184$, which are of particular importance for the $r$-process nucleosynthesis, and study the role of first-forbidden transitions in a framework that includes complex nucleonic correlations beyond the quasiparticle random phase approximation. Such correlations are accounted for by coupling single nucleons to collective degrees of freedom (nuclear vibrations), and are found essential to reproduce available experimental $\\beta$-decay rates and more precise many-body methods. We find that the nucleon-vibration correlations tend to decrease the probability of decay via first-forbidden transition near stability, as they enhance Gamow-Teller transitions at low energy. While in the lighter systems allowed transitions dominate, the decay of $N=126$ and $N=184$ nuclei is found to occur to a large extent via first-forbidden transitions, and in particular those induced by $1^-$ and $0^-$ operators. Overall the many-body method based on nucleon-vibration coupling provides an ideal framework for future large-scale calculations. Upcoming experimental measurements of $\\beta$-decay rates in the $N=126$ region by radioactive-beam facilities will be crucial in order to validate the approach.","sentences":["We compute $\\beta$-decay half-lives of isotonic nuclear chains located at neutron shell closures $N=50$, $82$, $126$ and $184$, which are of particular importance for the $r$-process nucleosynthesis, and study the role of first-forbidden transitions in a framework that includes complex nucleonic correlations beyond the quasiparticle random phase approximation.","Such correlations are accounted for by coupling single nucleons to collective degrees of freedom (nuclear vibrations), and are found essential to reproduce available experimental $\\beta$-decay rates and more precise many-body methods.","We find that the nucleon-vibration correlations tend to decrease the probability of decay via first-forbidden transition near stability, as they enhance Gamow-Teller transitions at low energy.","While in the lighter systems allowed transitions dominate, the decay of $N=126$ and $N=184$ nuclei is found to occur to a large extent via first-forbidden transitions, and in particular those induced by $1^-$ and $0^-$ operators.","Overall the many-body method based on nucleon-vibration coupling provides an ideal framework for future large-scale calculations.","Upcoming experimental measurements of $\\beta$-decay rates in the $N=126$ region by radioactive-beam facilities will be crucial in order to validate the approach."],"url":"http://arxiv.org/abs/2403.17115v1","category":"nucl-th"}
{"created":"2024-03-25 18:46:13","title":"Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach","abstract":"This work focuses on developing an effective meta-heuristic approach to protect against simultaneous attacks on nodes of a network modeled using a graph. Specifically, we focus on the $k$-strong Roman domination problem, a generalization of the well-known Roman domination problem on graphs. This general problem is about assigning integer weights to nodes that represent the number of field armies stationed at each node in order to satisfy the protection constraints while minimizing the total weights. These constraints concern the protection of a graph against any simultaneous attack consisting of $k \\in \\mathbb{N}$ nodes. An attack is considered repelled if each node labeled 0 can be defended by borrowing an army from one of its neighboring nodes, ensuring that the neighbor retains at least one army for self-defense. The $k$-SRD problem has practical applications in various areas, such as developing counter-terrorism strategies or managing supply chain disruptions. The solution to this problem is notoriously difficult to find, as even checking the feasibility of the proposed solution requires an exponential number of steps. We propose a variable neighborhood search algorithm in which the feasibility of the solution is checked by introducing the concept of quasi-feasibility, which is realized by careful sampling within the set of all possible attacks. Extensive experimental evaluations show the scalability and robustness of the proposed approach compared to the two exact approaches from the literature. Experiments are conducted with random networks from the literature and newly introduced random wireless networks as well as with real-world networks. A practical application scenario, using real-world networks, involves applying our approach to graphs extracted from GeoJSON files containing geographic features of hundreds of cities or larger regions.","sentences":["This work focuses on developing an effective meta-heuristic approach to protect against simultaneous attacks on nodes of a network modeled using a graph.","Specifically, we focus on the $k$-strong Roman domination problem, a generalization of the well-known Roman domination problem on graphs.","This general problem is about assigning integer weights to nodes that represent the number of field armies stationed at each node in order to satisfy the protection constraints while minimizing the total weights.","These constraints concern the protection of a graph against any simultaneous attack consisting of $k \\in \\mathbb{N}$ nodes.","An attack is considered repelled if each node labeled 0 can be defended by borrowing an army from one of its neighboring nodes, ensuring that the neighbor retains at least one army for self-defense.","The $k$-SRD problem has practical applications in various areas, such as developing counter-terrorism strategies or managing supply chain disruptions.","The solution to this problem is notoriously difficult to find, as even checking the feasibility of the proposed solution requires an exponential number of steps.","We propose a variable neighborhood search algorithm in which the feasibility of the solution is checked by introducing the concept of quasi-feasibility, which is realized by careful sampling within the set of all possible attacks.","Extensive experimental evaluations show the scalability and robustness of the proposed approach compared to the two exact approaches from the literature.","Experiments are conducted with random networks from the literature and newly introduced random wireless networks as well as with real-world networks.","A practical application scenario, using real-world networks, involves applying our approach to graphs extracted from GeoJSON files containing geographic features of hundreds of cities or larger regions."],"url":"http://arxiv.org/abs/2403.17108v1","category":"cs.AI"}
{"created":"2024-03-25 18:38:54","title":"AI Consciousness is Inevitable: A Theoretical Computer Science Perspective","abstract":"We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.","sentences":["We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations.","From this perspective, we develop a formal machine model for consciousness.","The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness.","Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable."],"url":"http://arxiv.org/abs/2403.17101v1","category":"cs.AI"}
{"created":"2024-03-25 18:32:22","title":"Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis","abstract":"In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs), the utmost importance lies in guaranteeing resilient and lucid security measures. This study highlights the necessity of implementing a Zero Trust Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs), hence departing from conventional perimeter defences that may expose vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous and continuous process of authenticating all network entities and communications. The accuracy of our methodology in detecting and identifying unmanned aerial vehicles (UAVs) is 84.59\\%. This is achieved by utilizing Radio Frequency (RF) signals within a Deep Learning framework, a unique method. Precise identification is crucial in Zero Trust Architecture (ZTA), as it determines network access. In addition, the use of eXplainable Artificial Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) contributes to the improvement of the model's transparency and interpretability. Adherence to Zero Trust Architecture (ZTA) standards guarantees that the classifications of unmanned aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security within the UAV field.","sentences":["In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs), the utmost importance lies in guaranteeing resilient and lucid security measures.","This study highlights the necessity of implementing a Zero Trust Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs), hence departing from conventional perimeter defences that may expose vulnerabilities.","The Zero Trust Architecture (ZTA) paradigm requires a rigorous and continuous process of authenticating all network entities and communications.","The accuracy of our methodology in detecting and identifying unmanned aerial vehicles (UAVs) is 84.59\\%.","This is achieved by utilizing Radio Frequency (RF) signals within a Deep Learning framework, a unique method.","Precise identification is crucial in Zero Trust Architecture (ZTA), as it determines network access.","In addition, the use of eXplainable Artificial Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) contributes to the improvement of the model's transparency and interpretability.","Adherence to Zero Trust Architecture (ZTA) standards guarantees that the classifications of unmanned aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security within the UAV field."],"url":"http://arxiv.org/abs/2403.17093v1","category":"cs.LG"}
{"created":"2024-03-25 18:28:45","title":"Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data","abstract":"We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The concentrability coefficient in the aggregated Markov Transition Model may grow exponentially with the horizon length, even when the concentrability coefficient in the original MDP is small and the offline data is admissible (i.e., the data distribution equals the occupancy measure of some policy), 3) Under value function realizability, there is a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data, implying that trajectory data offers no extra benefits over admissible data. These three pieces jointly resolve the open problem, though each of them could be of independent interest.","sentences":["We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness.","Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity.","In this work, we provide a negative answer to this question for the task of offline policy evaluation.","In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability.","Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP.","This unifies and generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The concentrability coefficient in the aggregated Markov Transition Model may grow exponentially with the horizon length, even when the concentrability coefficient in the original MDP is small and the offline data is admissible (i.e., the data distribution equals the occupancy measure of some policy), 3) Under value function realizability, there is a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data, implying that trajectory data offers no extra benefits over admissible data.","These three pieces jointly resolve the open problem, though each of them could be of independent interest."],"url":"http://arxiv.org/abs/2403.17091v1","category":"cs.LG"}
{"created":"2024-03-25 18:25:10","title":"GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration","abstract":"The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simulation study to test the framework's efficacy, followed by model and human evaluations to develop a dataset benchmark for long-term life tasks, and experiments across different models and settings. By shifting the focus from short-term tasks to the broader spectrum of long-term life goals, this research underscores the transformative potential of LLMs in enhancing human decision-making processes and task management, marking a significant step forward in the evolution of human-AI collaboration.","sentences":["The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process.","Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources.","This shift underscores the potential of LLMs in redefining information access paradigms.","Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks.","It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning.","The methodology encompasses a comprehensive simulation study to test the framework's efficacy, followed by model and human evaluations to develop a dataset benchmark for long-term life tasks, and experiments across different models and settings.","By shifting the focus from short-term tasks to the broader spectrum of long-term life goals, this research underscores the transformative potential of LLMs in enhancing human decision-making processes and task management, marking a significant step forward in the evolution of human-AI collaboration."],"url":"http://arxiv.org/abs/2403.17089v1","category":"cs.HC"}
{"created":"2024-03-25 18:18:12","title":"A Comparative Analysis of Visual Odometry in Virtual and Real-World Railways Environments","abstract":"Perception tasks play a crucial role in the development of automated operations and systems across multiple application fields. In the railway transportation domain, these tasks can improve the safety, reliability, and efficiency of various perations, including train localization, signal recognition, and track discrimination. However, collecting considerable and precisely labeled datasets for testing such novel algorithms poses extreme challenges in the railway environment due to the severe restrictions in accessing the infrastructures and the practical difficulties associated with properly equipping trains with the required sensors, such as cameras and LiDARs. The remarkable innovations of graphic engine tools offer new solutions to craft realistic synthetic datasets. To illustrate the advantages of employing graphic simulation for early-stage testing of perception tasks in the railway domain, this paper presents a comparative analysis of the performance of a SLAM algorithm applied both in a virtual synthetic environment and a real-world scenario. The analysis leverages virtual railway environments created with the latest version of Unreal Engine, facilitating data collection and allowing the examination of challenging scenarios, including low-visibility, dangerous operational modes, and complex environments. The results highlight the feasibility and potentiality of graphic simulation to advance perception tasks in the railway domain.","sentences":["Perception tasks play a crucial role in the development of automated operations and systems across multiple application fields.","In the railway transportation domain, these tasks can improve the safety, reliability, and efficiency of various perations, including train localization, signal recognition, and track discrimination.","However, collecting considerable and precisely labeled datasets for testing such novel algorithms poses extreme challenges in the railway environment due to the severe restrictions in accessing the infrastructures and the practical difficulties associated with properly equipping trains with the required sensors, such as cameras and LiDARs.","The remarkable innovations of graphic engine tools offer new solutions to craft realistic synthetic datasets.","To illustrate the advantages of employing graphic simulation for early-stage testing of perception tasks in the railway domain, this paper presents a comparative analysis of the performance of a SLAM algorithm applied both in a virtual synthetic environment and a real-world scenario.","The analysis leverages virtual railway environments created with the latest version of Unreal Engine, facilitating data collection and allowing the examination of challenging scenarios, including low-visibility, dangerous operational modes, and complex environments.","The results highlight the feasibility and potentiality of graphic simulation to advance perception tasks in the railway domain."],"url":"http://arxiv.org/abs/2403.17084v1","category":"cs.RO"}
{"created":"2024-03-25 18:16:34","title":"A Study in Dataset Pruning for Image Super-Resolution","abstract":"In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new perspectives to the untapped potential of dataset pruning in image SR. It suggests that careful selection of training data based on loss-value metrics can lead to better SR models, challenging the conventional wisdom that more data inevitably leads to better performance.","sentences":["In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword.","While offering rich training material, they also demand substantial computational and storage resources.","In this work, we analyze dataset pruning as a solution to these challenges.","We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model.","By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset.","Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process.","Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes.","Our work opens new perspectives to the untapped potential of dataset pruning in image SR.","It suggests that careful selection of training data based on loss-value metrics can lead to better SR models, challenging the conventional wisdom that more data inevitably leads to better performance."],"url":"http://arxiv.org/abs/2403.17083v1","category":"eess.IV"}
{"created":"2024-03-25 18:05:19","title":"Quantum Liquids: Emergent higher-rank gauge theory and fractons","abstract":"Fracton emerges from strongly interacting many-body systems whose excitations, referred to as sub-dimensional particles, have restricted mobility or kinetic motions. These entities have garnered significant interest due to their interdisciplinary implications spanning topological quantum codes, quantum field theory, emergent gravity, quantum information, and more, revealing unique nonequilibrium behaviors such as nonergodicity and glassy dynamics. This review presents a structured and educational overview of fracton phenomena, specifically focusing on gapless fracton liquids. Noteworthy for their compressibility and gapless excitations, fracton liquids facilitate collective modes reminiscent of those gauge fluctuations found in Maxwell's electromagnetic framework. However, they are distinct due to an additional higher-moment conservation law that restricts the mobility of individual charges and monopoles. Our exploration begins with the theoretical foundation of 3D fracton liquids, presenting a variety of emergent symmetric tensor gauge theories and reviewing their equilibrium and dynamic properties. Following this theoretical groundwork, we discuss the material realization of various fracton liquids in Yb-based breathing pyrochlore lattices, close-packed tiling structures and other synthetic quantum matter platforms. Additionally, we introduce a general protocol to foster and manipulate emergent fracton spin liquids in the realm of frustrated magnetism.","sentences":["Fracton emerges from strongly interacting many-body systems whose excitations, referred to as sub-dimensional particles, have restricted mobility or kinetic motions.","These entities have garnered significant interest due to their interdisciplinary implications spanning topological quantum codes, quantum field theory, emergent gravity, quantum information, and more, revealing unique nonequilibrium behaviors such as nonergodicity and glassy dynamics.","This review presents a structured and educational overview of fracton phenomena, specifically focusing on gapless fracton liquids.","Noteworthy for their compressibility and gapless excitations, fracton liquids facilitate collective modes reminiscent of those gauge fluctuations found in Maxwell's electromagnetic framework.","However, they are distinct due to an additional higher-moment conservation law that restricts the mobility of individual charges and monopoles.","Our exploration begins with the theoretical foundation of 3D fracton liquids, presenting a variety of emergent symmetric tensor gauge theories and reviewing their equilibrium and dynamic properties.","Following this theoretical groundwork, we discuss the material realization of various fracton liquids in Yb-based breathing pyrochlore lattices, close-packed tiling structures and other synthetic quantum matter platforms.","Additionally, we introduce a general protocol to foster and manipulate emergent fracton spin liquids in the realm of frustrated magnetism."],"url":"http://arxiv.org/abs/2403.17074v1","category":"cond-mat.str-el"}
{"created":"2024-03-25 18:03:58","title":"Semantic Ranking for Automated Adversarial Technique Annotation in Security Text","abstract":"We introduce a new method for extracting structured threat behaviors from threat intelligence text. Our method is based on a multi-stage ranking architecture that allows jointly optimizing for efficiency and effectiveness. Therefore, we believe this problem formulation better aligns with the real-world nature of the task considering the large number of adversary techniques and the extensive body of threat intelligence created by security analysts. Our findings show that the proposed system yields state-of-the-art performance results for this task. Results show that our method has a top-3 recall performance of 81\\% in identifying the relevant technique among 193 top-level techniques. Our tests also demonstrate that our system performs significantly better (+40\\%) than the widely used large language models when tested under a zero-shot setting.","sentences":["We introduce a new method for extracting structured threat behaviors from threat intelligence text.","Our method is based on a multi-stage ranking architecture that allows jointly optimizing for efficiency and effectiveness.","Therefore, we believe this problem formulation better aligns with the real-world nature of the task considering the large number of adversary techniques and the extensive body of threat intelligence created by security analysts.","Our findings show that the proposed system yields state-of-the-art performance results for this task.","Results show that our method has a top-3 recall performance of 81\\% in identifying the relevant technique among 193 top-level techniques.","Our tests also demonstrate that our system performs significantly better (+40\\%) than the widely used large language models when tested under a zero-shot setting."],"url":"http://arxiv.org/abs/2403.17068v1","category":"cs.CR"}
{"created":"2024-03-25 18:00:42","title":"Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions","abstract":"In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts. We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model. Project page: https://compvis.github.io/attribute-control. Code is available at https://github.com/CompVis/attribute-control.","sentences":["In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images.","However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person'').","Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously.","We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models.","Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive text prompts.","We demonstrate that these directions can be used to augment the prompt text input with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the diffusion model.","Project page: https://compvis.github.io/attribute-control.","Code is available at https://github.com/CompVis/attribute-control."],"url":"http://arxiv.org/abs/2403.17064v1","category":"cs.CV"}
{"created":"2024-03-25 18:00:01","title":"$\u03a6^p$ Amplitudes from the Positive Tropical Grassmannian: Triangulations of Extended Diagrams","abstract":"The global Schwinger formula, introduced by Cachazo and Early as a single integral over the positive tropical Grassmannian, provides a way to uncover properties of scattering amplitudes which are hard to see in their standard Feynman diagram formulation. In a recent work, Cachazo and one of the authors extended the global Schwinger formula to general $\\phi^p$ theories. When $p=4$, it was conjectured that the integral decomposes as a sum over cones which are in bijection with non-crossing chord diagrams, and further that these can be obtained by finding the zeroes of a piece-wise linear function, $H(x)$. In this note we give a proof of this conjecture. We also present a purely combinatorial way of computing $\\phi^p$ amplitudes by triangulating a trivial extended version of non-crossing $(p-2)$-chord diagrams, called extended diagrams, and present a proof of the bijection between triangulated extended diagrams and Feynman diagrams when $p=4$. This is reminiscent of recent constructions using Stokes polytopes and accordiohedra. However, the $\\phi^p$ amplitude is now partitioned by a new collection of objects, each of which characterizes a polyhedral cone in the positive tropical Grassmannian in the form of an associahedron or of an intersection of two associahedra. Moreover, we comment on the bijection between extended diagrams and double-ordered biadjoint scalar amplitudes. We also conjecture the form of the general piece-wise linear function, $H^{\\phi^p}(x)$, whose zeroes generate the regions in which the $\\phi^p$ global Schwinger formula decomposes into.","sentences":["The global Schwinger formula, introduced by Cachazo and Early as a single integral over the positive tropical Grassmannian, provides a way to uncover properties of scattering amplitudes which are hard to see in their standard Feynman diagram formulation.","In a recent work, Cachazo and one of the authors extended the global Schwinger formula to general $\\phi^p$ theories.","When $p=4$, it was conjectured that the integral decomposes as a sum over cones which are in bijection with non-crossing chord diagrams, and further that these can be obtained by finding the zeroes of a piece-wise linear function, $H(x)$. In this note we give a proof of this conjecture.","We also present a purely combinatorial way of computing $\\phi^p$ amplitudes by triangulating a trivial extended version of non-crossing $(p-2)$-chord diagrams, called extended diagrams, and present a proof of the bijection between triangulated extended diagrams and Feynman diagrams when $p=4$. This is reminiscent of recent constructions using Stokes polytopes and accordiohedra.","However, the $\\phi^p$ amplitude is now partitioned by a new collection of objects, each of which characterizes a polyhedral cone in the positive tropical Grassmannian in the form of an associahedron or of an intersection of two associahedra.","Moreover, we comment on the bijection between extended diagrams and double-ordered biadjoint scalar amplitudes.","We also conjecture the form of the general piece-wise linear function, $H^{\\phi^p}(x)$, whose zeroes generate the regions in which the $\\phi^p$ global Schwinger formula decomposes into."],"url":"http://arxiv.org/abs/2403.17051v1","category":"hep-th"}
{"created":"2024-03-25 17:59:58","title":"Optimizing LiDAR Placements for Robust Driving Perception in Adverse Conditions","abstract":"The robustness of driving perception systems under unprecedented conditions is crucial for safety-critical usages. Latest advancements have prompted increasing interests towards multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce a Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 364,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional robustness in both 3D object detection and LiDAR semantic segmentation tasks, under diverse adverse weather and sensor failure conditions. Code and benchmark toolkit are publicly available.","sentences":["The robustness of driving perception systems under unprecedented conditions is crucial for safety-critical usages.","Latest advancements have prompted increasing interests towards multi-LiDAR perception.","However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately.","Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations.","Our framework makes three appealing contributions.","1) To identify the most effective configurations for multi-LiDAR systems, we introduce a Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality.","2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements.","3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 364,000-frame dataset from both clean and adverse conditions.","Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines.","We showcase exceptional robustness in both 3D object detection and LiDAR semantic segmentation tasks, under diverse adverse weather and sensor failure conditions.","Code and benchmark toolkit are publicly available."],"url":"http://arxiv.org/abs/2403.17009v1","category":"cs.CV"}
{"created":"2024-03-25 17:59:23","title":"Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models","abstract":"This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning. While MLLMs have shown promise in various visual tasks, they often lack interpretability and struggle with complex visual inputs. To address these challenges, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We collect and introduce the Visual CoT dataset comprising 373k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Importantly, the introduced benchmark is capable of evaluating MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available to foster further research in this direction.","sentences":["This paper presents Visual CoT, a novel pipeline that leverages the reasoning capabilities of multi-modal large language models (MLLMs) by incorporating visual Chain-of-Thought (CoT) reasoning.","While MLLMs have shown promise in various visual tasks, they often lack interpretability and struggle with complex visual inputs.","To address these challenges, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts.","We collect and introduce the Visual CoT dataset comprising 373k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions.","Importantly, the introduced benchmark is capable of evaluating MLLMs in scenarios requiring specific local region identification.","Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies.","The Visual CoT dataset, benchmark, and pre-trained models are available to foster further research in this direction."],"url":"http://arxiv.org/abs/2403.16999v1","category":"cs.CV"}
{"created":"2024-03-25 17:59:01","title":"DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving","abstract":"End-to-end driving has made significant progress in recent years, demonstrating benefits such as system simplicity and competitive driving performance under both open-loop and closed-loop settings. Nevertheless, the lack of interpretability and controllability in its driving decisions hinders real-world deployment for end-to-end driving systems. In this paper, we collect a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA simulator. It contains sensor data, control decisions, and chain-of-thought labels to indicate the reasoning process. We utilize the challenging driving scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and lane-changing, and propose a rule-based expert policy to control the vehicle and generate ground truth labels for its reasoning process across different driving aspects and the final decisions. This dataset can serve as an open-loop end-to-end driving benchmark, enabling the evaluation of accuracy in various chain-of-thought aspects and the final decision. In addition, we propose a baseline model called DriveCoT-Agent, trained on our dataset, to generate chain-of-thought predictions and final decisions. The trained model exhibits strong performance in both open-loop and closed-loop evaluations, demonstrating the effectiveness of our proposed dataset.","sentences":["End-to-end driving has made significant progress in recent years, demonstrating benefits such as system simplicity and competitive driving performance under both open-loop and closed-loop settings.","Nevertheless, the lack of interpretability and controllability in its driving decisions hinders real-world deployment for end-to-end driving systems.","In this paper, we collect a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA simulator.","It contains sensor data, control decisions, and chain-of-thought labels to indicate the reasoning process.","We utilize the challenging driving scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and lane-changing, and propose a rule-based expert policy to control the vehicle and generate ground truth labels for its reasoning process across different driving aspects and the final decisions.","This dataset can serve as an open-loop end-to-end driving benchmark, enabling the evaluation of accuracy in various chain-of-thought aspects and the final decision.","In addition, we propose a baseline model called DriveCoT-Agent, trained on our dataset, to generate chain-of-thought predictions and final decisions.","The trained model exhibits strong performance in both open-loop and closed-loop evaluations, demonstrating the effectiveness of our proposed dataset."],"url":"http://arxiv.org/abs/2403.16996v1","category":"cs.CV"}
{"created":"2024-03-25 17:58:22","title":"Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows","abstract":"Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effectively decreases the inference time. Experiments on three challenging fine-grained control tasks and multiple high-quality text editing show that our method consistently outperforms its baselines. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.","sentences":["Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model.","A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps.","While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications.","This paper proposes Language Rectified Flow ({\\ours}).","Our method is based on the reformulation of the standard probabilistic flow models.","Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer.","From the source distribution, our language rectified flow yields fast simulation and effectively decreases the inference time.","Experiments on three challenging fine-grained control tasks and multiple high-quality text editing show that our method consistently outperforms its baselines.","Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks."],"url":"http://arxiv.org/abs/2403.16995v1","category":"cs.CL"}
{"created":"2024-03-25 17:52:07","title":"Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation","abstract":"Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for bounding the information flow in the sampling process. Bounded Attention prevents detrimental leakage among subjects and enables guiding the generation to promote each subject's individuality, even with complex multi-subject conditioning. Through extensive experimentation, we demonstrate that our method empowers the generation of multiple subjects that better align with given prompts and layouts.","sentences":["Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images.","However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects.","Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens.","Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects.","In this work, we study and analyze the causes of these limitations.","Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process.","This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects.","To address these issues, we introduce Bounded Attention, a training-free method for bounding the information flow in the sampling process.","Bounded Attention prevents detrimental leakage among subjects and enables guiding the generation to promote each subject's individuality, even with complex multi-subject conditioning.","Through extensive experimentation, we demonstrate that our method empowers the generation of multiple subjects that better align with given prompts and layouts."],"url":"http://arxiv.org/abs/2403.16990v1","category":"cs.CV"}
{"created":"2024-03-25 17:44:45","title":"Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings","abstract":"Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks. Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e.\\ sets of concepts which share some property of interest. Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust. Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g.\\ the colour of objects or the materials they are made of). In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings. We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves results in downstream tasks such as ultra-fine entity typing and ontology completion.","sentences":["Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks.","Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e.\\ sets of concepts which share some property of interest.","Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust.","Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g.\\ the colour of objects or the materials they are made of).","In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings.","We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves results in downstream tasks such as ultra-fine entity typing and ontology completion."],"url":"http://arxiv.org/abs/2403.16984v1","category":"cs.AI"}
{"created":"2024-03-25 17:38:32","title":"VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild","abstract":"We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web.","sentences":["We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts.","VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence.","On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2.","Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings.","In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit.","We encourage readers to listen to the demos at https://jasonppy.github.io/VoiceCraft_web."],"url":"http://arxiv.org/abs/2403.16973v1","category":"eess.AS"}
{"created":"2024-03-25 17:32:23","title":"AIOS: LLM Agent Operating System","abstract":"The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system \"with soul\" -- an important step towards AGI. Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents. We present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS. Our experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of our AIOS modules. Through this, we aim to not only improve the performance and efficiency of LLM agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future. The project is open-source at https://github.com/agiresearch/AIOS.","sentences":["The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy.","Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations.","The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources.","Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS) as the brain of the OS, enabling an operating system \"with soul\" -- an important step towards AGI.","Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents.","We present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS.","Our experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of our AIOS modules.","Through this, we aim to not only improve the performance and efficiency of LLM agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future.","The project is open-source at https://github.com/agiresearch/AIOS."],"url":"http://arxiv.org/abs/2403.16971v2","category":"cs.OS"}
{"created":"2024-03-25 17:17:35","title":"Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy","abstract":"Multi-agent, collaborative sensor fusion is a vital component of a multi-national intelligence toolkit. In safety-critical and/or contested environments, adversaries may infiltrate and compromise a number of agents. We analyze state of the art multi-target tracking algorithms under this compromised agent threat model. We prove that the track existence probability test (\"track score\") is significantly vulnerable to even small numbers of adversaries. To add security awareness, we design a trust estimation framework using hierarchical Bayesian updating. Our framework builds beliefs of trust on tracks and agents by mapping sensor measurements to trust pseudomeasurements (PSMs) and incorporating prior trust beliefs in a Bayesian context. In case studies, our trust estimation algorithm accurately estimates the trustworthiness of tracks/agents, subject to observability limitations.","sentences":["Multi-agent, collaborative sensor fusion is a vital component of a multi-national intelligence toolkit.","In safety-critical and/or contested environments, adversaries may infiltrate and compromise a number of agents.","We analyze state of the art multi-target tracking algorithms under this compromised agent threat model.","We prove that the track existence probability test (\"track score\") is significantly vulnerable to even small numbers of adversaries.","To add security awareness, we design a trust estimation framework using hierarchical Bayesian updating.","Our framework builds beliefs of trust on tracks and agents by mapping sensor measurements to trust pseudomeasurements (PSMs) and incorporating prior trust beliefs in a Bayesian context.","In case studies, our trust estimation algorithm accurately estimates the trustworthiness of tracks/agents, subject to observability limitations."],"url":"http://arxiv.org/abs/2403.16956v1","category":"cs.RO"}
{"created":"2024-03-25 17:14:00","title":"Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance","abstract":"Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 1B model trained for 100B tokens in RedPajama, reaching a performance comparable to the one trained for 48% more steps on the default mixture. Extending the application of data mixing laws to continual training accurately predicts the critical mixture proportion that avoids catastrophic forgetting and outlooks the potential for dynamic data schedules","sentences":["Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models.","While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws.","Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture.","Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training.","Moreover, experimental results verify that our method effectively optimizes the training mixture of a 1B model trained for 100B tokens in RedPajama, reaching a performance comparable to the one trained for 48% more steps on the default mixture.","Extending the application of data mixing laws to continual training accurately predicts the critical mixture proportion that avoids catastrophic forgetting and outlooks the potential for dynamic data schedules"],"url":"http://arxiv.org/abs/2403.16952v1","category":"cs.CL"}
{"created":"2024-03-25 17:11:28","title":"Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators","abstract":"Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PairS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PairS benefits from calibration.","sentences":["Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language.","However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments.","In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators.","Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts.","PairS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring.","Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of LLMs and demonstrate how PairS benefits from calibration."],"url":"http://arxiv.org/abs/2403.16950v2","category":"cs.CL"}
{"created":"2024-03-25 17:04:02","title":"SPACE-IDEAS: A Dataset for Salient Information Detection in Space Innovation","abstract":"Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow. Nevertheless, most of the datasets available for this task are derived mainly from academic publications. We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain. The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles. In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model. We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers.","sentences":["Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow.","Nevertheless, most of the datasets available for this task are derived mainly from academic publications.","We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain.","The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles.","In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model.","We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers."],"url":"http://arxiv.org/abs/2403.16941v1","category":"cs.CL"}
{"created":"2024-03-25 16:57:37","title":"The Costs of Competition in Distributing Scarce Research Funds","abstract":"Research funding systems are not isolated systems - they are embedded in a larger scientific system with an enormous influence on the system. This paper aims to analyze the allocation of competitive research funding from different perspectives: How reliable are decision processes for funding? What are the economic costs of competitive funding? How does competition for funds affect doing risky research? How do competitive funding environments affect scientists themselves, and which ethical issues must be considered? We attempt to identify gaps in our knowledge of research funding systems; we propose recommendations for policymakers and funding agencies, including empirical experiments of decision processes and the collection of data on these processes. With our recommendations we hope to contribute to developing improved ways of organizing research funding.","sentences":["Research funding systems are not isolated systems - they are embedded in a larger scientific system with an enormous influence on the system.","This paper aims to analyze the allocation of competitive research funding from different perspectives: How reliable are decision processes for funding?","What are the economic costs of competitive funding?","How does competition for funds affect doing risky research?","How do competitive funding environments affect scientists themselves, and which ethical issues must be considered?","We attempt to identify gaps in our knowledge of research funding systems; we propose recommendations for policymakers and funding agencies, including empirical experiments of decision processes and the collection of data on these processes.","With our recommendations we hope to contribute to developing improved ways of organizing research funding."],"url":"http://arxiv.org/abs/2403.16934v1","category":"econ.GN"}
{"created":"2024-03-25 16:57:02","title":"Backpropagation through space, time, and the brain","abstract":"Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates.","sentences":["Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task.","However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality.","How such networks can perform efficient credit assignment, remains, to a large extent, an open question.","In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT).","However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints.","We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons.","We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent.","The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity.","In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation.","For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal convolution.","For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates."],"url":"http://arxiv.org/abs/2403.16933v1","category":"q-bio.NC"}
{"created":"2024-03-25 16:49:38","title":"FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN","abstract":"Federated Learning (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network. Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client's system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes. In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL. First, we leverage Generative Adversarial Networks (GANs) to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data. Then, we use synthetic data to enhance the robustness and completeness of datasets across nodes. Our methodology adheres to FL's privacy requirements by generating synthetic data in a federated manner without sharing the actual data in the process. We incorporate techniques such as classwise sampling and node grouping, designed to improve the federated GAN's performance, enabling the creation of high-quality synthetic datasets and facilitating efficient FL training. Empirical results from our experiments demonstrate that FLIGAN significantly improves the model accuracy, especially in scenarios with high class imbalances, achieving up to a 20% increase in model accuracy over traditional FL baselines.","sentences":["Federated Learning (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes).","It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network.","Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client's system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes.","In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL.","First, we leverage Generative Adversarial Networks (GANs) to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data.","Then, we use synthetic data to enhance the robustness and completeness of datasets across nodes.","Our methodology adheres to FL's privacy requirements by generating synthetic data in a federated manner without sharing the actual data in the process.","We incorporate techniques such as classwise sampling and node grouping, designed to improve the federated GAN's performance, enabling the creation of high-quality synthetic datasets and facilitating efficient FL training.","Empirical results from our experiments demonstrate that FLIGAN significantly improves the model accuracy, especially in scenarios with high class imbalances, achieving up to a 20% increase in model accuracy over traditional FL baselines."],"url":"http://arxiv.org/abs/2403.16930v1","category":"cs.LG"}
{"created":"2024-03-25 16:46:08","title":"Searches for Higgs boson production through decays of heavy resonances","abstract":"The discovery of the Higgs boson has led to new possible signatures for heavy resonance searches at the LHC. Since then, search channels including at least one Higgs boson plus another particle have formed an important part of the program of new physics searches. In this report, the status of these searches by the CMS Collaboration is reviewed. Searches are discussed for resonances decaying to two Higgs bosons, a Higgs and a vector boson, or a Higgs boson and another new resonance, with proton-proton collision data collected at $\\sqrt{s}$ = 13 TeV in the years 2016-2018. A combination of the results of these searches is presented together with constraints on different beyond-the-standard model scenarios, including scenarios with extended Higgs sectors, heavy vector bosons and extra dimensions. Studies are shown for the first time by CMS on the validity of the narrow-width approximation in searches for the resonant production of a pair of Higgs bosons. The potential for a discovery at the High Luminosity LHC is also discussed.","sentences":["The discovery of the Higgs boson has led to new possible signatures for heavy resonance searches at the LHC.","Since then, search channels including at least one Higgs boson plus another particle have formed an important part of the program of new physics searches.","In this report, the status of these searches by the CMS Collaboration is reviewed.","Searches are discussed for resonances decaying to two Higgs bosons, a Higgs and a vector boson, or a Higgs boson and another new resonance, with proton-proton collision data collected at $\\sqrt{s}$ = 13 TeV in the years 2016-2018.","A combination of the results of these searches is presented together with constraints on different beyond-the-standard model scenarios, including scenarios with extended Higgs sectors, heavy vector bosons and extra dimensions.","Studies are shown for the first time by CMS on the validity of the narrow-width approximation in searches for the resonant production of a pair of Higgs bosons.","The potential for a discovery at the High Luminosity LHC is also discussed."],"url":"http://arxiv.org/abs/2403.16926v1","category":"hep-ex"}
{"created":"2024-03-25 16:32:50","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models","abstract":"Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.","sentences":["Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning.","This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning.","By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks.","We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs.","Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets.","Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations."],"url":"http://arxiv.org/abs/2403.16915v2","category":"cs.IR"}
{"created":"2024-03-25 16:21:25","title":"Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data","abstract":"Synthetic data generation has the potential to impact applications and domains with scarce data. However, before such data is used for sensitive tasks such as mental health, we need an understanding of how different demographics are represented in it. In our paper, we analyze the potential of producing synthetic data using GPT-3 by exploring the various stressors it attributes to different race and gender combinations, to provide insight for future researchers looking into using LLMs for data generation. Using GPT-3, we develop HEADROOM, a synthetic dataset of 3,120 posts about depression-triggering stressors, by controlling for race, gender, and time frame (before and after COVID-19). Using this dataset, we conduct semantic and lexical analyses to (1) identify the predominant stressors for each demographic group; and (2) compare our synthetic data to a human-generated dataset. We present the procedures to generate queries to develop depression data using GPT-3, and conduct analyzes to uncover the types of stressors it assigns to demographic groups, which could be used to test the limitations of LLMs for synthetic data generation for depression data. Our findings show that synthetic data mimics some of the human-generated data distribution for the predominant depression stressors across diverse demographics.","sentences":["Synthetic data generation has the potential to impact applications and domains with scarce data.","However, before such data is used for sensitive tasks such as mental health, we need an understanding of how different demographics are represented in it.","In our paper, we analyze the potential of producing synthetic data using GPT-3 by exploring the various stressors it attributes to different race and gender combinations, to provide insight for future researchers looking into using LLMs for data generation.","Using GPT-3, we develop HEADROOM, a synthetic dataset of 3,120 posts about depression-triggering stressors, by controlling for race, gender, and time frame (before and after COVID-19).","Using this dataset, we conduct semantic and lexical analyses to (1) identify the predominant stressors for each demographic group; and (2) compare our synthetic data to a human-generated dataset.","We present the procedures to generate queries to develop depression data using GPT-3, and conduct analyzes to uncover the types of stressors it assigns to demographic groups, which could be used to test the limitations of LLMs for synthetic data generation for depression data.","Our findings show that synthetic data mimics some of the human-generated data distribution for the predominant depression stressors across diverse demographics."],"url":"http://arxiv.org/abs/2403.16909v1","category":"cs.AI"}
{"created":"2024-03-25 16:19:33","title":"Towards Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations","abstract":"Understanding driving scenes and communicating automated vehicle decisions are key requirements for trustworthy automated driving. In this article, we introduce the Qualitative Explainable Graph (QXG), which is a unified symbolic and qualitative representation for scene understanding in urban mobility. The QXG enables interpreting an automated vehicle's environment using sensor data and machine learning models. It utilizes spatio-temporal graphs and qualitative constraints to extract scene semantics from raw sensor inputs, such as LiDAR and camera data, offering an interpretable scene model. A QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations across various sensor types. Our research showcases the potential of QXG, particularly in the context of automated driving, where it can rationalize decisions by linking the graph with observed actions. These explanations can serve diverse purposes, from informing passengers and alerting vulnerable road users to enabling post-hoc analysis of prior behaviors.","sentences":["Understanding driving scenes and communicating automated vehicle decisions are key requirements for trustworthy automated driving.","In this article, we introduce the Qualitative Explainable Graph (QXG), which is a unified symbolic and qualitative representation for scene understanding in urban mobility.","The QXG enables interpreting an automated vehicle's environment using sensor data and machine learning models.","It utilizes spatio-temporal graphs and qualitative constraints to extract scene semantics from raw sensor inputs, such as LiDAR and camera data, offering an interpretable scene model.","A QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations across various sensor types.","Our research showcases the potential of QXG, particularly in the context of automated driving, where it can rationalize decisions by linking the graph with observed actions.","These explanations can serve diverse purposes, from informing passengers and alerting vulnerable road users to enabling post-hoc analysis of prior behaviors."],"url":"http://arxiv.org/abs/2403.16908v1","category":"cs.AI"}
{"created":"2024-03-25 16:14:45","title":"Multi-Agent Optimization for Safety Analysis of Cyber-Physical Systems: Position Paper","abstract":"Failure Mode, Effects and Criticality Analysis (FMECA) is one of the safety analysis methods recommended by most of the international standards. The classical FMECA is made in a form of a table filled in either manually or by using safety analysis tools. In both cases, the design engineers have to choose the trade-offs between safety and other development constraints. In the case of complex cyber-physical systems (CPS) with thousands of specified constraints, this may lead to severe problems and significantly impact the overall criticality of CPS. In this paper, we propose to adopt optimization techniques to automate the decision making process conducted after FMECA of CPS. We describe a multi-agent based optimization method which extends classical FMECA for offering optimal solutions in terms of criticality and development constraints of CPS.","sentences":["Failure Mode, Effects and Criticality Analysis (FMECA) is one of the safety analysis methods recommended by most of the international standards.","The classical FMECA is made in a form of a table filled in either manually or by using safety analysis tools.","In both cases, the design engineers have to choose the trade-offs between safety and other development constraints.","In the case of complex cyber-physical systems (CPS) with thousands of specified constraints, this may lead to severe problems and significantly impact the overall criticality of CPS.","In this paper, we propose to adopt optimization techniques to automate the decision making process conducted after FMECA of CPS.","We describe a multi-agent based optimization method which extends classical FMECA for offering optimal solutions in terms of criticality and development constraints of CPS."],"url":"http://arxiv.org/abs/2403.16904v1","category":"cs.AI"}
{"created":"2024-03-26 17:58:28","title":"Controlling the chaotic wake of a flapping foil by tuning its chordwise flexibility","abstract":"Effects of chord-wise flexibility as an instrument to control chaotic transitions in the wake of a flexible flapping foil have been studied here using an immersed boundary method-based in-house fluid-structure-interaction solver. The ability of the flapping foil at an optimum level of flexibility to inhibit chaotic transition, otherwise encountered in a similar but rigid configuration, has been highlighted. The rigid foil manifests chaotic transition through a quasi-periodic-intermittency route at high dynamic plunge velocities; whereas, increasing the level of flexibility gradually regularises the aperiodic behaviour through a variety of interesting wake patterns. If flexibility is increased beyond an optimum level, aperiodicity sets in again and robust chaos is restored at very high flexibility levels. The mechanisms of triggering the order-to-chaos transition are different between the rigid and the high flexibility cases. Along the route to order and back to chaos, the flexible foil exhibits different flow-field behaviours, including far-wake switching, primary \\& secondary vortex streets, bifurcated wakes and interactive vortices between the bifurcated wakes. The underlying interaction mechanisms of the flow-field vortices responsible for the associated dynamical signatures of the wake have been closely tracked. This study further examines the optimum propulsive performance range of the flexible flapper and investigates its connection with the periodicity/regularity of the system.","sentences":["Effects of chord-wise flexibility as an instrument to control chaotic transitions in the wake of a flexible flapping foil have been studied here using an immersed boundary method-based in-house fluid-structure-interaction solver.","The ability of the flapping foil at an optimum level of flexibility to inhibit chaotic transition, otherwise encountered in a similar but rigid configuration, has been highlighted.","The rigid foil manifests chaotic transition through a quasi-periodic-intermittency route at high dynamic plunge velocities; whereas, increasing the level of flexibility gradually regularises the aperiodic behaviour through a variety of interesting wake patterns.","If flexibility is increased beyond an optimum level, aperiodicity sets in again and robust chaos is restored at very high flexibility levels.","The mechanisms of triggering the order-to-chaos transition are different between the rigid and the high flexibility cases.","Along the route to order and back to chaos, the flexible foil exhibits different flow-field behaviours, including far-wake switching, primary \\& secondary vortex streets, bifurcated wakes and interactive vortices between the bifurcated wakes.","The underlying interaction mechanisms of the flow-field vortices responsible for the associated dynamical signatures of the wake have been closely tracked.","This study further examines the optimum propulsive performance range of the flexible flapper and investigates its connection with the periodicity/regularity of the system."],"url":"http://arxiv.org/abs/2403.17932v1","category":"physics.flu-dyn"}
{"created":"2024-03-26 17:58:22","title":"Track Everything Everywhere Fast and Robustly","abstract":"We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video. The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications. OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence. To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions. While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models. Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process. Our experiments demonstrate a substantial improvement in training speed (more than \\textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion.","sentences":["We propose a novel test-time optimization approach for efficiently and robustly tracking any pixel at any time in a video.","The latest state-of-the-art optimization-based tracking technique, OmniMotion, requires a prohibitively long optimization time, rendering it impractical for downstream applications.","OmniMotion is sensitive to the choice of random seeds, leading to unstable convergence.","To improve efficiency and robustness, we introduce a novel invertible deformation network, CaDeX++, which factorizes the function representation into a local spatial-temporal feature grid and enhances the expressivity of the coupling blocks with non-linear functions.","While CaDeX++ incorporates a stronger geometric bias within its architectural design, it also takes advantage of the inductive bias provided by the vision foundation models.","Our system utilizes monocular depth estimation to represent scene geometry and enhances the objective by incorporating DINOv2 long-term semantics to regulate the optimization process.","Our experiments demonstrate a substantial improvement in training speed (more than \\textbf{10 times} faster), robustness, and accuracy in tracking over the SoTA optimization-based method OmniMotion."],"url":"http://arxiv.org/abs/2403.17931v1","category":"cs.CV"}
{"created":"2024-03-26 17:56:15","title":"Proofs that Modify Proofs","abstract":"In this paper we give an ordinal analysis of the theory of second order arithmetic. We do this by working with proof trees -- that is, \"deductions\" which may not be well-founded. Working in a suitable theory, we are able to represent functions on proof trees as yet further proof trees satisfying a suitable analog of well-foundedness. Iterating this process allows us to represent higher order functions as well: since functions on proof trees are just proof trees themselves, these functions can easily be extended to act on proof trees which are themselves understood as functions. The corresponding system of ordinals parallels this, using higher order collapsing function.","sentences":["In this paper we give an ordinal analysis of the theory of second order arithmetic.","We do this by working with proof trees -- that is, \"deductions\" which may not be well-founded.","Working in a suitable theory, we are able to represent functions on proof trees as yet further proof trees satisfying a suitable analog of well-foundedness.","Iterating this process allows us to represent higher order functions as well: since functions on proof trees are just proof trees themselves, these functions can easily be extended to act on proof trees which are themselves understood as functions.","The corresponding system of ordinals parallels this, using higher order collapsing function."],"url":"http://arxiv.org/abs/2403.17922v1","category":"math.LO"}
{"created":"2024-03-26 17:50:04","title":"Emergent Anomalous Hydrodynamics at Infinite Temperature in a Long-Range XXZ Model","abstract":"The conventional wisdom suggests that transports of conserved quantities in non-integrable quantum many-body systems at high temperatures are diffusive. However, we discover a counterexample of this paradigm by uncovering anomalous hydrodynamics in a spin-1/2 XXZ chain with power-law couplings. This model, classified as non-integrable due to its Wigner-Dyson level-spacing statistics in the random matrix theory, exhibits a surprising superdiffusive-ballistic-superdiffusive transport transition by varying the power-law exponent of couplings for a fixed anisotropy. Our findings are verified by multiple observables, including the spin-spin autocorrelator, mean-square displacement, and spin conductivity. Interestingly, we further quantify the degree of quantum chaos using the Kullback-Leibler divergence between the entanglement entropy distributions of the model's eigenstates and a random state. Remarkably, an observed local maximum in the divergence near the transition boundary suggests a link between anomalous hydrodynamics and a suppression of quantum chaos. This work offers another deep understanding of emergent anomalous transport phenomena in a wider range of non-integrable quantum many-body systems","sentences":["The conventional wisdom suggests that transports of conserved quantities in non-integrable quantum many-body systems at high temperatures are diffusive.","However, we discover a counterexample of this paradigm by uncovering anomalous hydrodynamics in a spin-1/2 XXZ chain with power-law couplings.","This model, classified as non-integrable due to its Wigner-Dyson level-spacing statistics in the random matrix theory, exhibits a surprising superdiffusive-ballistic-superdiffusive transport transition by varying the power-law exponent of couplings for a fixed anisotropy.","Our findings are verified by multiple observables, including the spin-spin autocorrelator, mean-square displacement, and spin conductivity.","Interestingly, we further quantify the degree of quantum chaos using the Kullback-Leibler divergence between the entanglement entropy distributions of the model's eigenstates and a random state.","Remarkably, an observed local maximum in the divergence near the transition boundary suggests a link between anomalous hydrodynamics and a suppression of quantum chaos.","This work offers another deep understanding of emergent anomalous transport phenomena in a wider range of non-integrable quantum many-body systems"],"url":"http://arxiv.org/abs/2403.17912v1","category":"quant-ph"}
{"created":"2024-03-26 17:46:25","title":"ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing Change Detection","abstract":"Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps. However, existing convolutional neural network and transformer-based frameworks often struggle to accurately segment semantic change regions. Moreover, transformers-based methods with standard self-attention suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data. To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size. Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise convolution, respectively. The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention. Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods. Compared to the recent transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters. Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection benchmarks. Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance. Project url https://github.com/techmn/elgcnet.","sentences":["Deep learning has shown remarkable success in remote sensing change detection (CD), aiming to identify semantic change regions between co-registered satellite image pairs acquired at distinct time stamps.","However, existing convolutional neural network and transformer-based frameworks often struggle to accurately segment semantic change regions.","Moreover, transformers-based methods with standard self-attention suffer from quadratic computational complexity with respect to the image resolution, making them less practical for CD tasks with limited training data.","To address these issues, we propose an efficient change detection framework, ELGC-Net, which leverages rich contextual information to precisely estimate change regions while reducing the model size.","Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder.","The focus of our design is the introduction of an Efficient Local-Global Context Aggregator module within the encoder, capturing enhanced global context and local spatial information through a novel pooled-transpose (PT) attention and depthwise convolution, respectively.","The PT attention employs pooling operations for robust feature extraction and minimizes computational cost with transposed attention.","Extensive experiments on three challenging CD datasets demonstrate that ELGC-Net outperforms existing methods.","Compared to the recent transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in intersection over union metric on the LEVIR-CD dataset, while significantly reducing trainable parameters.","Our proposed ELGC-Net sets a new state-of-the-art performance in remote sensing change detection benchmarks.","Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly reduced computational complexity, suitable for resource-constrained settings, while achieving comparable performance.","Project url https://github.com/techmn/elgcnet."],"url":"http://arxiv.org/abs/2403.17909v1","category":"cs.CV"}
{"created":"2024-03-26 17:46:03","title":"Reciprocity Calibration of Dual-Antenna Repeaters","abstract":"We present a reciprocity calibration method for dual-antenna repeaters in wireless networks. The method uses bi-directional measurements between two network nodes, A and B, where for each bi-directional measurement, the repeaters are configured in different states. The nodes A and B could be two access points in a distributed MIMO system, or they could be a base station and a mobile user terminal, for example. From the calibration measurements, the differences between the repeaters' forward and reverse gains are estimated. The repeaters are then (re-)configured to compensate for these differences such that the repeaters appear, transparently to the network, as reciprocal components of the propagation environment, enabling reciprocity-based beamforming in the network.","sentences":["We present a reciprocity calibration method for dual-antenna repeaters in wireless networks.","The method uses bi-directional measurements between two network nodes, A and B, where for each bi-directional measurement, the repeaters are configured in different states.","The nodes A and B could be two access points in a distributed MIMO system, or they could be a base station and a mobile user terminal, for example.","From the calibration measurements, the differences between the repeaters' forward and reverse gains are estimated.","The repeaters are then (re-)configured to compensate for these differences such that the repeaters appear, transparently to the network, as reciprocal components of the propagation environment, enabling reciprocity-based beamforming in the network."],"url":"http://arxiv.org/abs/2403.17908v1","category":"eess.SP"}
{"created":"2024-03-26 17:44:04","title":"Quasi-rigid operators and hyper-recurrence","abstract":"We study recurrent operators from a new perspective by introducing the notion of hyper-recurrent operators and establish robust connections with quasi-rigid operators. For example, we prove that a recurrent operator on a separable Banach space is quasi-rigid if and only if it is a linear factor of a hyper-recurrent operator, and show that the quasi-rigid operators found in Costakis, Manoussos and Parissis's work, along with many others, are, in fact, hyper-recurrent operators. Furthermore, we provide a negative answer, using a class of operators introduced by Tapia, to the question by Costakis et al. whether $T \\oplus T$ is recurrent whenever $T$ is.","sentences":["We study recurrent operators from a new perspective by introducing the notion of hyper-recurrent operators and establish robust connections with quasi-rigid operators.","For example, we prove that a recurrent operator on a separable Banach space is quasi-rigid if and only if it is a linear factor of a hyper-recurrent operator, and show that the quasi-rigid operators found in Costakis, Manoussos and Parissis's work, along with many others, are, in fact, hyper-recurrent operators.","Furthermore, we provide a negative answer, using a class of operators introduced by Tapia, to the question by Costakis et al.","whether $T \\oplus T$ is recurrent whenever $T$ is."],"url":"http://arxiv.org/abs/2403.17904v1","category":"math.FA"}
{"created":"2024-03-26 17:39:36","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians","abstract":"The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations. While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum. This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details. Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation. Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results. Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results.","sentences":["The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering fidelity and efficiency compared to NeRF-based neural scene representations.","While demonstrating the potential for real-time rendering, 3D-GS encounters rendering bottlenecks in large scenes with complex details due to an excessive number of Gaussian primitives located within the viewing frustum.","This limitation is particularly noticeable in zoom-out views and can lead to inconsistent rendering speeds in scenes with varying details.","Moreover, it often struggles to capture the corresponding level of details at different scales with its heuristic density control operation.","Inspired by the Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an LOD-structured 3D Gaussian approach supporting level-of-detail decomposition for scene representation that contributes to the final rendering results.","Our model dynamically selects the appropriate level from the set of multi-resolution anchor points, ensuring consistent rendering performance with adaptive LOD adjustments while maintaining high-fidelity rendering results."],"url":"http://arxiv.org/abs/2403.17898v1","category":"cs.CV"}
{"created":"2024-03-26 17:35:15","title":"Dynamical evolution of the Uranian satellite system II. Crossing of the 5/3 Ariel-Umbriel mean motion resonance","abstract":"At present, the main satellites of Uranus are not involved in any low order mean motion resonance (MMR). However, owing to tides raised in the planet, Ariel and Umbriel most likely crossed the 5/3 MMR in the past. Previous studies on this resonance passage relied on limited time-consuming N-body simulations or simplified models focusing solely on the effects of the eccentricity or the inclination. In this paper, we aim to provide a more comprehensive view on how the system evaded capture in the 5/3 MMR. For that purpose, we developed a secular resonant two-satellite model with low eccentricities and low inclinations, including tides using the weak friction model. By performing a large number of numerical simulations, we show that capture in the 5/3 MMR is certain if the initial eccentricities of Ariel, $e_1$, and Umbriel, $e_2$, are related through $(e_1^2 + e_2^2)^{1/2} < 0.007$. Moreover, we observe that the eccentricity of Ariel is the key variable to evade the 5/3 MMR with a high probability. We determine that for $e_1 > 0.015$ and $e_2 < 0.01$, the system avoids capture in at least 60\\% of the cases. We also show that, to replicate the currently observed system, the initial inclinations of Ariel and Umbriel must lay within $I_1 \\leq 0.05^{\\circ}$ and $0.06^{\\circ} \\leq I_2 \\leq 0.11^{\\circ}$, respectively. We checked these results using a complete N-body model with the five main satellites and did not observe any significant differences.","sentences":["At present, the main satellites of Uranus are not involved in any low order mean motion resonance (MMR).","However, owing to tides raised in the planet, Ariel and Umbriel most likely crossed the 5/3 MMR in the past.","Previous studies on this resonance passage relied on limited time-consuming N-body simulations or simplified models focusing solely on the effects of the eccentricity or the inclination.","In this paper, we aim to provide a more comprehensive view on how the system evaded capture in the 5/3 MMR.","For that purpose, we developed a secular resonant two-satellite model with low eccentricities and low inclinations, including tides using the weak friction model.","By performing a large number of numerical simulations, we show that capture in the 5/3 MMR is certain if the initial eccentricities of Ariel, $e_1$, and Umbriel, $e_2$, are related through $(e_1^2 + e_2^2)^{1/2} < 0.007$.","Moreover, we observe that the eccentricity of Ariel is the key variable to evade the 5/3 MMR with a high probability.","We determine that for $e_1 > 0.015$ and $e_2 < 0.01$, the system avoids capture in at least 60\\% of the cases.","We also show that, to replicate the currently observed system, the initial inclinations of Ariel and Umbriel must lay within $I_1 \\leq 0.05^{\\circ}$ and $0.06^{\\circ} \\leq I_2 \\leq 0.11^{\\circ}$, respectively.","We checked these results using a complete N-body model with the five main satellites and did not observe any significant differences."],"url":"http://arxiv.org/abs/2403.17897v1","category":"astro-ph.EP"}
{"created":"2024-03-26 17:34:41","title":"Dynamical evolution of the Uranian satellite system I. From the 5/3 Ariel-Umbriel mean motion resonance to the present","abstract":"Mutual gravitational interactions between the five major Uranian satellites raise small quasi-periodic fluctuations on their orbital elements. At the same time, tidal interactions between the satellites and the planet induce a slow outward drift of the orbits, while damping the eccentricities and the inclinations. In this paper, we revisit the current and near past evolution of this system using a N-body integrator, including spin evolution and tidal dissipation with the weak friction model. We update the secular eigenmodes of the system and show that it is unlikely that any of the main satellites were recently captured into a high obliquity Cassini state. We rather expect that the Uranian satellites are in a low obliquity Cassini state and compute their values. We also show that the current eccentricities of the satellites are not forced, and estimate the free eccentricities and inclinations. We constrain the quality factor of Uranus to be $Q_U = (8.6 \\pm 2.9)\\times10^3$, and that of the satellites to be $Q_S \\sim 500$. We find that the system most likely encountered the 5/3 mean motion resonance between Ariel and Umbriel in the past, at about ($0.7\\pm0.2$) Gyr ago. We additionally determine the eccentricities and inclinations of all satellites just after the resonance passage that comply with the current system. We finally show that, from the crossing of the 5/3 MMR to the present, the evolution of the system is mostly peaceful and dominated by tides raised on Uranus by the satellites.","sentences":["Mutual gravitational interactions between the five major Uranian satellites raise small quasi-periodic fluctuations on their orbital elements.","At the same time, tidal interactions between the satellites and the planet induce a slow outward drift of the orbits, while damping the eccentricities and the inclinations.","In this paper, we revisit the current and near past evolution of this system using a N-body integrator, including spin evolution and tidal dissipation with the weak friction model.","We update the secular eigenmodes of the system and show that it is unlikely that any of the main satellites were recently captured into a high obliquity Cassini state.","We rather expect that the Uranian satellites are in a low obliquity Cassini state and compute their values.","We also show that the current eccentricities of the satellites are not forced, and estimate the free eccentricities and inclinations.","We constrain the quality factor of Uranus to be $Q_U = (8.6 \\pm 2.9)\\times10^3$, and that of the satellites to be $Q_S \\sim 500$.","We find that the system most likely encountered the 5/3 mean motion resonance between Ariel and Umbriel in the past, at about ($0.7\\pm0.2$) Gyr ago.","We additionally determine the eccentricities and inclinations of all satellites just after the resonance passage that comply with the current system.","We finally show that, from the crossing of the 5/3 MMR to the present, the evolution of the system is mostly peaceful and dominated by tides raised on Uranus by the satellites."],"url":"http://arxiv.org/abs/2403.17896v1","category":"astro-ph.EP"}
{"created":"2024-03-26 17:27:20","title":"Density of group languages in shift spaces","abstract":"We study the density of group languages (i.e. rational languages recognized by morphisms onto finite groups) inside shift spaces. The density of a rational language can be understood as the frequency of some \"pattern\" in the shift space, for example a pattern like \"words with an even number of a given letter.\" In this paper, we handle density of group languages via ergodicity of skew products between the shift space and the recognizing group. We consider both the cases of shifts of finite type (with a suitable notion of irreducibility), and of minimal shifts. In the latter case, our main result is a closed formula for the density which holds whenever the skew product has minimal closed invariant subsets which are ergodic under the product of the original measure and the uniform probability measure on the group. The formula is derived in part from a characterization of minimal closed invariant subsets for skew products relying on notions of cocycles and coboundaries. In the case where the whole skew product itself is ergodic under the product measure, then the density is completely determined by the cardinality of the image of the language inside the recognizing group. We provide sufficient conditions for the skew product to have minimal closed invariant subsets that are ergodic under the product measure. Finally, we investigate the link between minimal closed invariant subsets, return words and bifix codes.","sentences":["We study the density of group languages (i.e. rational languages recognized by morphisms onto finite groups) inside shift spaces.","The density of a rational language can be understood as the frequency of some \"pattern\" in the shift space, for example a pattern like \"words with an even number of a given letter.\"","In this paper, we handle density of group languages via ergodicity of skew products between the shift space and the recognizing group.","We consider both the cases of shifts of finite type (with a suitable notion of irreducibility), and of minimal shifts.","In the latter case, our main result is a closed formula for the density which holds whenever the skew product has minimal closed invariant subsets which are ergodic under the product of the original measure and the uniform probability measure on the group.","The formula is derived in part from a characterization of minimal closed invariant subsets for skew products relying on notions of cocycles and coboundaries.","In the case where the whole skew product itself is ergodic under the product measure, then the density is completely determined by the cardinality of the image of the language inside the recognizing group.","We provide sufficient conditions for the skew product to have minimal closed invariant subsets that are ergodic under the product measure.","Finally, we investigate the link between minimal closed invariant subsets, return words and bifix codes."],"url":"http://arxiv.org/abs/2403.17892v1","category":"math.DS"}
{"created":"2024-03-26 17:03:17","title":"Scaling Mixed-Integer Programming for Certification of Neural Network Controllers Using Bounds Tightening","abstract":"Neural networks offer a computationally efficient approximation of model predictive control, but they lack guarantees on the resulting controlled system's properties. Formal certification of neural networks is crucial for ensuring safety, particularly in safety-critical domains such as autonomous vehicles. One approach to formally certify properties of neural networks is to solve a mixed-integer program based on the network. This approach suffers from scalability issues due to the complexity of solving the resulting mixed-integer programs. Nevertheless, these issues can be (partially) mitigated via bound-tightening techniques prior to forming the mixed-integer program, which results in tighter formulations and faster optimisation. This paper presents bound-tightening techniques in the context of neural network explicit control policies. Bound tightening is particularly important when considering problems spanning multiple time steps of a controlled system, as the bounds must be propagated through the problem depth. Several strategies for bound tightening are evaluated in terms of both computational complexity and tightness of the bounds.","sentences":["Neural networks offer a computationally efficient approximation of model predictive control, but they lack guarantees on the resulting controlled system's properties.","Formal certification of neural networks is crucial for ensuring safety, particularly in safety-critical domains such as autonomous vehicles.","One approach to formally certify properties of neural networks is to solve a mixed-integer program based on the network.","This approach suffers from scalability issues due to the complexity of solving the resulting mixed-integer programs.","Nevertheless, these issues can be (partially) mitigated via bound-tightening techniques prior to forming the mixed-integer program, which results in tighter formulations and faster optimisation.","This paper presents bound-tightening techniques in the context of neural network explicit control policies.","Bound tightening is particularly important when considering problems spanning multiple time steps of a controlled system, as the bounds must be propagated through the problem depth.","Several strategies for bound tightening are evaluated in terms of both computational complexity and tightness of the bounds."],"url":"http://arxiv.org/abs/2403.17874v1","category":"math.OC"}
{"created":"2024-03-26 16:57:01","title":"Sample complexity of quantum hypothesis testing","abstract":"Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary quantum hypothesis testing depends logarithmically on the inverse type~II error probability and inversely on the quantum relative entropy. Finally, we provide lower and upper bounds on the sample complexity of multiple quantum hypothesis testing, with it remaining an intriguing open question to improve these bounds.","sentences":["Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state.","In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability.","By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing.","In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity.","As a counterpart of the quantum Stein's lemma, we also find that the sample complexity of asymmetric binary quantum hypothesis testing depends logarithmically on the inverse type~II error probability and inversely on the quantum relative entropy.","Finally, we provide lower and upper bounds on the sample complexity of multiple quantum hypothesis testing, with it remaining an intriguing open question to improve these bounds."],"url":"http://arxiv.org/abs/2403.17868v1","category":"quant-ph"}
{"created":"2024-03-26 16:49:46","title":"Proceedings Sixth Workshop on Models for Formal Analysis of Real Systems","abstract":"This volume contains the proceedings of MARS 2024, the sixth workshop on Models for Formal Analysis of Real Systems, held as part of ETAPS 2024, the European Joint Conferences on Theory and Practice of Software.   The MARS workshops bring together researchers from different communities who are developing formal models of real systems in areas where complex models occur, such as networks, cyber-physical systems, hardware/software co-design, biology, etc. The motivation and aim for MARS stem from the following two observations:   (1) Large case studies are essential to show that specification formalisms and modelling techniques are applicable to real systems, whereas many research papers only consider toy examples or tiny case studies.   (2) Developing an accurate model of a real system takes a large amount of time, often months or years. In most scientific papers, however, salient details of the model need to be skipped due to lack of space, and to leave room for formal verification methodologies and results.   The MARS workshops aim at remedying these issues, emphasising modelling over verification, so as to retain lessons learnt from formal modelling, which are not usually discussed elsewhere.","sentences":["This volume contains the proceedings of MARS 2024, the sixth workshop on Models for Formal Analysis of Real Systems, held as part of ETAPS 2024, the European Joint Conferences on Theory and Practice of Software.   ","The MARS workshops bring together researchers from different communities who are developing formal models of real systems in areas where complex models occur, such as networks, cyber-physical systems, hardware/software co-design, biology, etc.","The motivation and aim for MARS stem from the following two observations:   (1) Large case studies are essential to show that specification formalisms and modelling techniques are applicable to real systems, whereas many research papers only consider toy examples or tiny case studies.   ","(2) Developing an accurate model of a real system takes a large amount of time, often months or years.","In most scientific papers, however, salient details of the model need to be skipped due to lack of space, and to leave room for formal verification methodologies and results.   ","The MARS workshops aim at remedying these issues, emphasising modelling over verification, so as to retain lessons learnt from formal modelling, which are not usually discussed elsewhere."],"url":"http://arxiv.org/abs/2403.17862v1","category":"cs.LO"}
{"created":"2024-03-26 16:49:31","title":"Stealthy Deactivation of Safety Filters","abstract":"Safety filters ensure that only safe control actions are executed. We propose a simple and stealthy false-data injection attack for deactivating such safety filters; in particular, we focus on deactivating safety filters that are based on control-barrier functions. The attack injects false sensor measurements to bias state estimates to the interior of a safety region, which makes the safety filter accept unsafe control actions. To detect such attacks, we also propose a detector that detects biases manufactured by the proposed attack policy, which complements conventional detectors when safety filters are used. The proposed attack policy and detector are illustrated on a double integrator example.","sentences":["Safety filters ensure that only safe control actions are executed.","We propose a simple and stealthy false-data injection attack for deactivating such safety filters; in particular, we focus on deactivating safety filters that are based on control-barrier functions.","The attack injects false sensor measurements to bias state estimates to the interior of a safety region, which makes the safety filter accept unsafe control actions.","To detect such attacks, we also propose a detector that detects biases manufactured by the proposed attack policy, which complements conventional detectors when safety filters are used.","The proposed attack policy and detector are illustrated on a double integrator example."],"url":"http://arxiv.org/abs/2403.17861v1","category":"cs.SY"}
{"created":"2024-03-26 16:46:05","title":"Parallelizable Parametric Nonlinear System Identification via tuning of a Moving Horizon State Estimator","abstract":"This paper introduces a novel optimization-based approach for parametric nonlinear system identification. Building upon the prediction error method framework, traditionally used for linear system identification, we extend its capabilities to nonlinear systems. The predictions are computed using a moving horizon state estimator with a constant arrival cost. Eventually, both the system parameters and the arrival cost are estimated by minimizing the sum of the squared prediction errors. Since the predictions are induced by the state estimator, the method can be viewed as the tuning of a state estimator, based on its predictive capacities. The present extension of the prediction error method not only enhances performance for nonlinear systems but also enables learning from multiple trajectories with unknown initial states, broadening its applicability in practical scenarios. Additionally, the novel formulation leaves room for the design of efficient and parallelizable optimization algorithms, since each output prediction only depends on a fixed window of past actions and measurements. In the special case of linear time-invariant systems, we show an important property of the proposed method which suggests asymptotic consistency under reasonable assumptions. Numerical examples illustrate the effectiveness and practicality of the approach, and one of the examples also highlights the necessity for the arrival cost.","sentences":["This paper introduces a novel optimization-based approach for parametric nonlinear system identification.","Building upon the prediction error method framework, traditionally used for linear system identification, we extend its capabilities to nonlinear systems.","The predictions are computed using a moving horizon state estimator with a constant arrival cost.","Eventually, both the system parameters and the arrival cost are estimated by minimizing the sum of the squared prediction errors.","Since the predictions are induced by the state estimator, the method can be viewed as the tuning of a state estimator, based on its predictive capacities.","The present extension of the prediction error method not only enhances performance for nonlinear systems but also enables learning from multiple trajectories with unknown initial states, broadening its applicability in practical scenarios.","Additionally, the novel formulation leaves room for the design of efficient and parallelizable optimization algorithms, since each output prediction only depends on a fixed window of past actions and measurements.","In the special case of linear time-invariant systems, we show an important property of the proposed method which suggests asymptotic consistency under reasonable assumptions.","Numerical examples illustrate the effectiveness and practicality of the approach, and one of the examples also highlights the necessity for the arrival cost."],"url":"http://arxiv.org/abs/2403.17858v1","category":"math.OC"}
{"created":"2024-03-26 16:40:08","title":"Counterfactual Fairness through Transforming Data Orthogonal to Bias","abstract":"Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine learning models and tasks, and includes a sparse variant to enhance numerical stability through regularization. Through empirical evaluation on simulated and real-world datasets - including the adult income and the COMPAS recidivism datasets - our methodology demonstrates its capacity to enable fairer outcomes without compromising accuracy.","sentences":["Machine learning models have shown exceptional prowess in solving complex issues across various domains.","Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups.","Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied.","We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications.","Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables.","The OB algorithm is model-agnostic, catering to a wide array of machine learning models and tasks, and includes a sparse variant to enhance numerical stability through regularization.","Through empirical evaluation on simulated and real-world datasets - including the adult income and the COMPAS recidivism datasets - our methodology demonstrates its capacity to enable fairer outcomes without compromising accuracy."],"url":"http://arxiv.org/abs/2403.17852v1","category":"cs.LG"}
{"created":"2024-03-26 16:38:13","title":"An Integer Linear Program to create the shifts in a supermarket","abstract":"The shift design and the personnel scheduling problem is known to be a difficult problem. It is a real-world problem which has lots of applications in the organization of companies. Solutions are usually found by dividing the problem in two steps: first the shifts are created, then the employees are assigned to them by respecting a bunch of constraints. The assignment of different tasks increases the complexity, since we have to consider the skills of the single employee necessary to perform any activity. In this paper we present aa integer linear programming formulation which models together the shift creation and the construction of rosters for employees, with the objective of minimizing the amount of uncovered demand. Finally we provide the results for three real-world instances, confirming that this approach is promising.","sentences":["The shift design and the personnel scheduling problem is known to be a difficult problem.","It is a real-world problem which has lots of applications in the organization of companies.","Solutions are usually found by dividing the problem in two steps: first the shifts are created, then the employees are assigned to them by respecting a bunch of constraints.","The assignment of different tasks increases the complexity, since we have to consider the skills of the single employee necessary to perform any activity.","In this paper we present aa integer linear programming formulation which models together the shift creation and the construction of rosters for employees, with the objective of minimizing the amount of uncovered demand.","Finally we provide the results for three real-world instances, confirming that this approach is promising."],"url":"http://arxiv.org/abs/2403.17850v1","category":"math.OC"}
{"created":"2024-03-26 16:30:36","title":"A Sociotechnical Framework For Addressing Stigma and Designing Personalized Digital Health Products","abstract":"Stigma, a recognized global barrier to effective disease management, impacts social interactions, resource access, and psychological well-being. In this study, we developed a patient-centered framework for deriving design requirements and interventions for health conditions subject to social stigma. This study introduces a patient-centered framework, grounded in sociotechnical systems theory, to create tailored interventions and design requirements for health conditions influenced by social stigma. We tested this framework through a mixed-method study on chronic pelvic pain patients. Our approach led to the identification of ten design requirements that encompass behavioral and psychological support and strategies for day-to-day living. The findings reveal a preference among CPP patients for priming and social support interventions. This study underscores the value of a systems-based perspective in healthcare, advocating for a nuanced, patient-centered approach that addresses the complex nature of health conditions affected by social stigma. It contributes to the ongoing discourse on integrating STS theory into healthcare frameworks, highlighting the need for targeted strategies to combat the complexities of stigma in patient care.","sentences":["Stigma, a recognized global barrier to effective disease management, impacts social interactions, resource access, and psychological well-being.","In this study, we developed a patient-centered framework for deriving design requirements and interventions for health conditions subject to social stigma.","This study introduces a patient-centered framework, grounded in sociotechnical systems theory, to create tailored interventions and design requirements for health conditions influenced by social stigma.","We tested this framework through a mixed-method study on chronic pelvic pain patients.","Our approach led to the identification of ten design requirements that encompass behavioral and psychological support and strategies for day-to-day living.","The findings reveal a preference among CPP patients for priming and social support interventions.","This study underscores the value of a systems-based perspective in healthcare, advocating for a nuanced, patient-centered approach that addresses the complex nature of health conditions affected by social stigma.","It contributes to the ongoing discourse on integrating STS theory into healthcare frameworks, highlighting the need for targeted strategies to combat the complexities of stigma in patient care."],"url":"http://arxiv.org/abs/2403.17843v1","category":"cs.HC"}
{"created":"2024-03-26 16:29:03","title":"Experimental Realization of Discrete Time Quasi-Crystals","abstract":"Floquet (periodically driven) systems can give rise to unique non-equilibrium phases of matter without equilibrium analogs. The most prominent example is the realization of discrete time crystals. An intriguing question emerges: what other novel phases can manifest when the constraint of time periodicity is relaxed? In this study, we explore quantum systems subjected to a quasi-periodic drive. Leveraging a strongly interacting spin ensemble in diamond, we identify the emergence of long-lived discrete time quasi-crystals. Unlike conventional time crystals, time quasi-crystals exhibit robust sub-harmonic responses at multiple incommensurate frequencies. Furthermore, we show that the multi-frequency nature of the quasi-periodic drive allows for the formation of diverse patterns associated with different discrete time quasi-crystalline phases. Our findings demonstrate the existence of non-equilibrium phases in quasi-Floquet settings, significantly broadening the catalog of novel phenomena in driven many-body quantum systems.","sentences":["Floquet (periodically driven) systems can give rise to unique non-equilibrium phases of matter without equilibrium analogs.","The most prominent example is the realization of discrete time crystals.","An intriguing question emerges: what other novel phases can manifest when the constraint of time periodicity is relaxed?","In this study, we explore quantum systems subjected to a quasi-periodic drive.","Leveraging a strongly interacting spin ensemble in diamond, we identify the emergence of long-lived discrete time quasi-crystals.","Unlike conventional time crystals, time quasi-crystals exhibit robust sub-harmonic responses at multiple incommensurate frequencies.","Furthermore, we show that the multi-frequency nature of the quasi-periodic drive allows for the formation of diverse patterns associated with different discrete time quasi-crystalline phases.","Our findings demonstrate the existence of non-equilibrium phases in quasi-Floquet settings, significantly broadening the catalog of novel phenomena in driven many-body quantum systems."],"url":"http://arxiv.org/abs/2403.17842v1","category":"quant-ph"}
{"created":"2024-03-26 16:27:36","title":"Universal entropy transport far from equilibrium across the BCS-BEC crossover","abstract":"The transport properties of strongly interacting fermionic systems can reveal exotic states of matter, but experiments and theory have predominantly focused on bulk systems in the hydrodynamic limit describable with linear response coefficients such as electrical and thermal conductivity. In a ballistic channel connecting two superfluid reservoirs, recent experiments revealed a far-from-equilibrium regime beyond linear hydrodynamics where particle and entropy currents respond nonlinearly to biases of chemical potential and temperature, and their ratio is robust to the channel geometry. However, the origin of this robustness and its relation to the strong interparticle interactions remain unknown. Here, we study the coupled transport of particles and entropy tuning the interaction across the Bardeen-Cooper-Schrieffer to Bose-Einstein condensate (BCS-BEC) crossover, the reservoir degeneracy across the superfluid phase transition, as well as the local potentials and confinement of the channel. Surprisingly, the entropy advectively transported per particle depends only on the interactions and reservoir degeneracy and not on the details of the channel, suggesting that this property has its origin in the universal equilibrium properties of the reservoirs. In contrast, the magnitudes of the advective and diffusive entropy currents vary significantly with the channel details. The advective current increases monotonically towards the BEC side, which can be largely explained by the estimated superfluid gap in the channel. The Wiedemann-Franz law that links the advective and diffusive currents in Fermi liquids is most egregiously violated at unitarity, suggesting a change in the nature of the excitations responsible for entropy diffusion near unitarity. These observations pose fundamental questions regarding transport phenomena in strongly interacting Fermi systems far from equilibrium.","sentences":["The transport properties of strongly interacting fermionic systems can reveal exotic states of matter, but experiments and theory have predominantly focused on bulk systems in the hydrodynamic limit describable with linear response coefficients such as electrical and thermal conductivity.","In a ballistic channel connecting two superfluid reservoirs, recent experiments revealed a far-from-equilibrium regime beyond linear hydrodynamics where particle and entropy currents respond nonlinearly to biases of chemical potential and temperature, and their ratio is robust to the channel geometry.","However, the origin of this robustness and its relation to the strong interparticle interactions remain unknown.","Here, we study the coupled transport of particles and entropy tuning the interaction across the Bardeen-Cooper-Schrieffer to Bose-Einstein condensate (BCS-BEC) crossover, the reservoir degeneracy across the superfluid phase transition, as well as the local potentials and confinement of the channel.","Surprisingly, the entropy advectively transported per particle depends only on the interactions and reservoir degeneracy and not on the details of the channel, suggesting that this property has its origin in the universal equilibrium properties of the reservoirs.","In contrast, the magnitudes of the advective and diffusive entropy currents vary significantly with the channel details.","The advective current increases monotonically towards the BEC side, which can be largely explained by the estimated superfluid gap in the channel.","The Wiedemann-Franz law that links the advective and diffusive currents in Fermi liquids is most egregiously violated at unitarity, suggesting a change in the nature of the excitations responsible for entropy diffusion near unitarity.","These observations pose fundamental questions regarding transport phenomena in strongly interacting Fermi systems far from equilibrium."],"url":"http://arxiv.org/abs/2403.17838v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-26 16:22:06","title":"An MBE-CASSCF Approach for the Accurate Treatment of Large Active Spaces","abstract":"We present a novel implementation of the complete active space self-consistent field (CASSCF) method that makes use of the many-body expanded full configuration interaction (MBE-FCI) method to incrementally approximate electronic structures within large active spaces. On the basis of a hybrid first-order algorithm employing both Super-CI and quasi-Newton strategies for the optimization of molecular orbitals, we demonstrate both computational efficacy and high accuracy of the resulting MBE-CASSCF method. We assess the performance of our implementation on a set of established numerical tests before applying MBE-CASSCF in the investigation of the triplet-quintet spin gap of an iron(II) tetraphenylporphyrin model system with active spaces as large as 50 electrons in 50 orbitals.","sentences":["We present a novel implementation of the complete active space self-consistent field (CASSCF) method that makes use of the many-body expanded full configuration interaction (MBE-FCI) method to incrementally approximate electronic structures within large active spaces.","On the basis of a hybrid first-order algorithm employing both Super-CI and quasi-Newton strategies for the optimization of molecular orbitals, we demonstrate both computational efficacy and high accuracy of the resulting MBE-CASSCF method.","We assess the performance of our implementation on a set of established numerical tests before applying MBE-CASSCF in the investigation of the triplet-quintet spin gap of an iron(II) tetraphenylporphyrin model system with active spaces as large as 50 electrons in 50 orbitals."],"url":"http://arxiv.org/abs/2403.17836v1","category":"physics.chem-ph"}
{"created":"2024-03-26 16:20:54","title":"Implementing photometric stereo for scanning helium microscopy (SHeM) to reconstruct true-to-size 3D surfaces","abstract":"Scanning Helium Microscopy (SHeM) offers a combination of spatial and angular resolution via a pinhole-collimated beam of thermal energy, neutral helium-4 atoms for non-destructive imaging. This thesis introduces a novel 3D imaging mode, \"heliometric stereo\", enabling true-to-size 3D surface reconstruction using an adapted photometric stereo algorithm. Stereolithography (SLA) 3D printed plastics are explored for SHeM pinhole plates due to limitations in traditional machining. FormLabs \"Clear Resin\" via SLA printing proves ideal for rapid prototyping of vacuum components, with a developed baking protocol ensuring vacuum compatibility. The study indicates re-wetting of such plastics is a surface process over weeks.   Developing 3D image reconstruction for both single and multi-detector setups required a real-space point tracking method. The point tracking method facilitates facet angle measurement in various materials, including technological and biological crystals. It has since become integral to SHeM imaging protocols for sample manipulator debugging.   The thesis also details a multi-detector SHeM instrument, referred to as B-SHeM. While primarily designed to perform heliometric stereo reconstructions, the instrument also enables the range of novel SHeM experiments such as mixed-species beams to investigate inelastic scattering.   The heliometric stereo methods implemented in the work have motivated the development of a GPU accelerated version of the in-house Monte-Carlo based ray tracing framework, which is the de-facto standard for SHeM image analysis. GPU parallelisation was explored as a method for decreasing simulation time and enabling previously inaccessible simulations involving complex scattering distributions and high resolution, realistic sample geometries. Preliminary testing on an analogous problem yielded a potential performance increase of up to 380 times.","sentences":["Scanning Helium Microscopy (SHeM) offers a combination of spatial and angular resolution via a pinhole-collimated beam of thermal energy, neutral helium-4 atoms for non-destructive imaging.","This thesis introduces a novel 3D imaging mode, \"heliometric stereo\", enabling true-to-size 3D surface reconstruction using an adapted photometric stereo algorithm.","Stereolithography (SLA) 3D printed plastics are explored for SHeM pinhole plates due to limitations in traditional machining.","FormLabs \"Clear Resin\" via SLA printing proves ideal for rapid prototyping of vacuum components, with a developed baking protocol ensuring vacuum compatibility.","The study indicates re-wetting of such plastics is a surface process over weeks.   ","Developing 3D image reconstruction for both single and multi-detector setups required a real-space point tracking method.","The point tracking method facilitates facet angle measurement in various materials, including technological and biological crystals.","It has since become integral to SHeM imaging protocols for sample manipulator debugging.   ","The thesis also details a multi-detector SHeM instrument, referred to as B-SHeM.","While primarily designed to perform heliometric stereo reconstructions, the instrument also enables the range of novel SHeM experiments such as mixed-species beams to investigate inelastic scattering.   ","The heliometric stereo methods implemented in the work have motivated the development of a GPU accelerated version of the in-house Monte-Carlo based ray tracing framework, which is the de-facto standard for SHeM image analysis.","GPU parallelisation was explored as a method for decreasing simulation time and enabling previously inaccessible simulations involving complex scattering distributions and high resolution, realistic sample geometries.","Preliminary testing on an analogous problem yielded a potential performance increase of up to 380 times."],"url":"http://arxiv.org/abs/2403.17835v1","category":"physics.app-ph"}
{"created":"2024-03-26 15:53:02","title":"Graph Language Model (GLM): A new graph-based approach to detect social instabilities","abstract":"This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.","sentences":["This scientific report presents a novel methodology for the early prediction of important political events using News datasets.","The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data.","Initially, we designed a preliminary version of the method and tested it on a few events.","This analysis revealed limitations in the initial research phase.","We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data.","After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests.","Results demonstrate the superiority of our approach compared to baseline methods.","Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data."],"url":"http://arxiv.org/abs/2403.17816v1","category":"cs.CL"}
{"created":"2024-03-26 15:40:05","title":"Towards 3D Vision with Low-Cost Single-Photon Cameras","abstract":"We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras. These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution. We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D geometry from a set of spatially distributed sensors with known poses. We show that our approach can successfully recover complex 3D shapes from simulated data. We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor. Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras.","sentences":["We present a method for reconstructing 3D shape of arbitrary Lambertian objects based on measurements by miniature, energy-efficient, low-cost single-photon cameras.","These cameras, operating as time resolved image sensors, illuminate the scene with a very fast pulse of diffuse light and record the shape of that pulse as it returns back from the scene at a high temporal resolution.","We propose to model this image formation process, account for its non-idealities, and adapt neural rendering to reconstruct 3D geometry from a set of spatially distributed sensors with known poses.","We show that our approach can successfully recover complex 3D shapes from simulated data.","We further demonstrate 3D object reconstruction from real-world captures, utilizing measurements from a commodity proximity sensor.","Our work draws a connection between image-based modeling and active range scanning and is a step towards 3D vision with single-photon cameras."],"url":"http://arxiv.org/abs/2403.17801v1","category":"cs.CV"}
{"created":"2024-03-26 15:38:35","title":"Discovery and timing of ten new millisecond pulsars in the globular cluster Terzan 5","abstract":"We report the discovery of ten new pulsars in the globular cluster Terzan 5 as part of the Transients and Pulsars with MeerKAT (TRAPUM) Large Survey Project. We observed Terzan 5 at L-band (856--1712 MHz) with the MeerKAT radio telescope for four hours on two epochs, and performed acceleration searches of 45 out of 288 tied-array beams covering the core of the cluster. We obtained phase-connected timing solutions for nine discoveries, covering nearly two decades of archival observations from the Green Bank Telescope for all but one. Highlights include PSR J1748$-$2446ao which is an eccentric ($e = 0.32$) wide-orbit (orbital period $P_{\\rm b} = 57.55$ d) system. We were able to measure the rate of advance of periastron ($\\dot{\\omega}$) for this system allowing us to determine a total mass of $3.17 \\pm \\, 0.02\\, \\rm M_{\\odot}$. With a minimum companion mass ($M_{\\rm c}$) of $\\sim 0.8\\, \\rm M_{\\odot}$, PSR J1748$-$2446ao is a candidate double neutron star (DNS) system. If confirmed to be a DNS, it would be the fastest spinning pulsar ($P = 2.27$ ms) and the longest orbital period measured for any known DNS system. PSR J1748$-$2446ap has the second highest eccentricity for any recycled pulsar ($e \\sim 0.905$) and for this system we can measure the total mass ($1.997 \\pm 0.006\\, \\rm M_{\\odot}$) and also estimate the individual pulsar and companion masses. PSR J1748$-$2446ar is an eclipsing redback (minimum $M_{\\rm c} \\sim 0.34\\, \\rm M_{\\odot}$) system whose properties confirm it to be the counterpart to a previously published source identified in radio and X-ray imaging. With these discoveries, the total number of confirmed pulsars in Terzan 5 is 49, the highest for any globular cluster so far. These discoveries further enhance the rich set of pulsars known in Terzan 5 and provide scope for a deeper understanding of binary stellar evolution, cluster dynamics and ensemble population studies.","sentences":["We report the discovery of ten new pulsars in the globular cluster Terzan 5 as part of the Transients and Pulsars with MeerKAT (TRAPUM) Large Survey Project.","We observed Terzan 5 at L-band (856--1712 MHz) with the MeerKAT radio telescope for four hours on two epochs, and performed acceleration searches of 45 out of 288 tied-array beams covering the core of the cluster.","We obtained phase-connected timing solutions for nine discoveries, covering nearly two decades of archival observations from the Green Bank Telescope for all but one.","Highlights include PSR J1748$-$2446ao which is an eccentric ($e = 0.32$) wide-orbit (orbital period $P_{\\rm b} = 57.55$ d) system.","We were able to measure the rate of advance of periastron ($\\dot{\\omega}$) for this system allowing us to determine a total mass of $3.17 \\pm \\, 0.02\\, \\rm M_{\\odot}$. With a minimum companion mass ($M_{\\rm c}$) of $\\sim 0.8\\, \\rm M_{\\odot}$, PSR J1748$-$2446ao is a candidate double neutron star (DNS) system.","If confirmed to be a DNS, it would be the fastest spinning pulsar ($P = 2.27$ ms) and the longest orbital period measured for any known DNS system.","PSR J1748$-$2446ap has the second highest eccentricity for any recycled pulsar ($e \\sim 0.905$) and for this system we can measure the total mass ($1.997 \\pm 0.006\\, \\rm M_{\\odot}$) and also estimate the individual pulsar and companion masses.","PSR J1748$-$2446ar is an eclipsing redback (minimum $M_{\\rm c} \\sim 0.34\\, \\rm M_{\\odot}$) system whose properties confirm it to be the counterpart to a previously published source identified in radio and X-ray imaging.","With these discoveries, the total number of confirmed pulsars in Terzan 5 is 49, the highest for any globular cluster so far.","These discoveries further enhance the rich set of pulsars known in Terzan 5 and provide scope for a deeper understanding of binary stellar evolution, cluster dynamics and ensemble population studies."],"url":"http://arxiv.org/abs/2403.17799v1","category":"astro-ph.HE"}
{"created":"2024-03-26 15:27:42","title":"Fermihedral: On the Optimal Compilation for Fermion-to-Qubit Encoding","abstract":"This paper introduces Fermihedral, a compiler framework focusing on discovering the optimal Fermion-to-qubit encoding for targeted Fermionic Hamiltonians. Fermion-to-qubit encoding is a crucial step in harnessing quantum computing for efficient simulation of Fermionic quantum systems. Utilizing Pauli algebra, Fermihedral redefines complex constraints and objectives of Fermion-to-qubit encoding into a Boolean Satisfiability problem which can then be solved with high-performance solvers. To accommodate larger-scale scenarios, this paper proposed two new strategies that yield approximate optimal solutions mitigating the overhead from the exponentially large number of clauses. Evaluation across diverse Fermionic systems highlights the superiority of Fermihedral, showcasing substantial reductions in implementation costs, gate counts, and circuit depth in the compiled circuits. Real-system experiments on IonQ's device affirm its effectiveness, notably enhancing simulation accuracy.","sentences":["This paper introduces Fermihedral, a compiler framework focusing on discovering the optimal Fermion-to-qubit encoding for targeted Fermionic Hamiltonians.","Fermion-to-qubit encoding is a crucial step in harnessing quantum computing for efficient simulation of Fermionic quantum systems.","Utilizing Pauli algebra, Fermihedral redefines complex constraints and objectives of Fermion-to-qubit encoding into a Boolean Satisfiability problem which can then be solved with high-performance solvers.","To accommodate larger-scale scenarios, this paper proposed two new strategies that yield approximate optimal solutions mitigating the overhead from the exponentially large number of clauses.","Evaluation across diverse Fermionic systems highlights the superiority of Fermihedral, showcasing substantial reductions in implementation costs, gate counts, and circuit depth in the compiled circuits.","Real-system experiments on IonQ's device affirm its effectiveness, notably enhancing simulation accuracy."],"url":"http://arxiv.org/abs/2403.17794v1","category":"quant-ph"}
{"created":"2024-03-26 15:27:24","title":"Neural Exponential Stabilization of Control-affine Nonlinear Systems","abstract":"This paper proposes a novel learning-based approach for achieving exponential stabilization of nonlinear control-affine systems. We leverage the Control Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction Metrics (NCMs) and Neural Network (NN) controllers. First, we transform the infinite-dimensional semi-definite program (SDP) for CCM computation into a tractable inequality feasibility problem using element-wise bounds of matrix-valued functions. The terms in the inequality can be efficiently computed by our novel algorithms. Second, we propose a free parametrization of NCMs guaranteeing positive definiteness and the satisfaction of a partial differential equation, regardless of trainable parameters. Third, this parametrization and the inequality condition enable the design of contractivity-enforcing regularizers, which can be incorporated while designing the NN controller for exponential stabilization of the underlying nonlinear systems. Furthermore, when the training loss goes to zero, we provide formal guarantees on verification of the NCM and the exponentional stabilization under the NN controller. Finally, we validate our method through benchmark experiments on set-point stabilization and increasing the region of attraction of a locally pre-stabilized closed-loop system.","sentences":["This paper proposes a novel learning-based approach for achieving exponential stabilization of nonlinear control-affine systems.","We leverage the Control Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction Metrics (NCMs) and Neural Network (NN) controllers.","First, we transform the infinite-dimensional semi-definite program (SDP) for CCM computation into a tractable inequality feasibility problem using element-wise bounds of matrix-valued functions.","The terms in the inequality can be efficiently computed by our novel algorithms.","Second, we propose a free parametrization of NCMs guaranteeing positive definiteness and the satisfaction of a partial differential equation, regardless of trainable parameters.","Third, this parametrization and the inequality condition enable the design of contractivity-enforcing regularizers, which can be incorporated while designing the NN controller for exponential stabilization of the underlying nonlinear systems.","Furthermore, when the training loss goes to zero, we provide formal guarantees on verification of the NCM and the exponentional stabilization under the NN controller.","Finally, we validate our method through benchmark experiments on set-point stabilization and increasing the region of attraction of a locally pre-stabilized closed-loop system."],"url":"http://arxiv.org/abs/2403.17793v1","category":"eess.SY"}
{"created":"2024-03-26 15:25:05","title":"The Bernoulli Property for Geodesic Flow on Flat Surfaces","abstract":"Let $S$ be a compact surface of genus $\\geq 2$ equipped with a metric that is flat everywhere except at finitely many cone points with angles greater than $2\\pi$. We examine the geodesic flow on $S$ and prove local product structure for a wide class of equilibrium states. Using this, we establish the Bernoulli property for these systems.","sentences":["Let $S$ be a compact surface of genus $\\geq 2$ equipped with a metric that is flat everywhere except at finitely many cone points with angles greater than $2\\pi$. We examine the geodesic flow on $S$ and prove local product structure for a wide class of equilibrium states.","Using this, we establish the Bernoulli property for these systems."],"url":"http://arxiv.org/abs/2403.17791v1","category":"math.DS"}
{"created":"2024-03-26 15:20:56","title":"System Calibration of a Field Phenotyping Robot with Multiple High-Precision Profile Laser Scanners","abstract":"The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications. This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation. The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud. Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration. The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization. Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration. Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan.","sentences":["The creation of precise and high-resolution crop point clouds in agricultural fields has become a key challenge for high-throughput phenotyping applications.","This work implements a novel calibration method to calibrate the laser scanning system of an agricultural field robot consisting of two industrial-grade laser scanners used for high-precise 3D crop point cloud creation.","The calibration method optimizes the transformation between the scanner origins and the robot pose by minimizing 3D point omnivariances within the point cloud.","Moreover, we present a novel factor graph-based pose estimation method that fuses total station prism measurements with IMU and GNSS heading information for high-precise pose determination during calibration.","The root-mean-square error of the distances to a georeferenced ground truth point cloud results in 0.8 cm after parameter optimization.","Furthermore, our results show the importance of a reference point cloud in the calibration method needed to estimate the vertical translation of the calibration.","Challenges arise due to non-static parameters while the robot moves, indicated by systematic deviations to a ground truth terrestrial laser scan."],"url":"http://arxiv.org/abs/2403.17788v1","category":"cs.RO"}
{"created":"2024-03-26 15:17:55","title":"Neural Distributed Controllers with Port-Hamiltonian Structures","abstract":"Controlling large-scale cyber-physical systems necessitates optimal distributed policies, relying solely on local real-time data and limited communication with neighboring agents. However, finding optimal controllers remains challenging, even in seemingly simple scenarios. Parameterizing these policies using Neural Networks (NNs) can deliver good performance, but their sensitivity to small input changes can destabilize the closed-loop system. This paper addresses this issue for a network of nonlinear dissipative systems. Specifically, we leverage well-established port-Hamiltonian structures to characterize deep distributed control policies with closed-loop stability guarantees and a finite $\\mathcal{L}_2$ gain, regardless of specific NN parameters. This eliminates the need to constrain the parameters during optimization and enables training with standard methods like stochastic gradient descent. A numerical study on the consensus control of Kuramoto oscillators demonstrates the effectiveness of the proposed controllers.","sentences":["Controlling large-scale cyber-physical systems necessitates optimal distributed policies, relying solely on local real-time data and limited communication with neighboring agents.","However, finding optimal controllers remains challenging, even in seemingly simple scenarios.","Parameterizing these policies using Neural Networks (NNs) can deliver good performance, but their sensitivity to small input changes can destabilize the closed-loop system.","This paper addresses this issue for a network of nonlinear dissipative systems.","Specifically, we leverage well-established port-Hamiltonian structures to characterize deep distributed control policies with closed-loop stability guarantees and a finite $\\mathcal{L}_2$ gain, regardless of specific NN parameters.","This eliminates the need to constrain the parameters during optimization and enables training with standard methods like stochastic gradient descent.","A numerical study on the consensus control of Kuramoto oscillators demonstrates the effectiveness of the proposed controllers."],"url":"http://arxiv.org/abs/2403.17785v1","category":"eess.SY"}
{"created":"2024-03-26 15:14:25","title":"Facet formation in slow three-dimensional fracture","abstract":"Cracks develop various surface patterns as they propagate in three-dimensional (3D) materials. Facet formation in nominally tensile (mode-I) fracture emerge in the slow, non-inertial regime and oftentimes takes the form of surface steps. We show that the same phase-field framework that recently shed basic light on dynamic (inertial) tensile fracture in 3D, also gives rise to crack surface steps. Step formation is shown to be an intrinsically nonlinear phenomenon that involves two essential physical ingredients: finite-strength quenched disorder and a small, mesoscopic anti-plane shear (mode-III) loading component (on top of the dominant tensile, mode-I loading component). We quantify the interplay between disorder (both its strength and spatial correlation length) and mesoscopic mode I+III mixity in controlling step formation. Finally, we show that surface steps grow out of the small-scale, background surface roughness and are composed of two overlapping crack segments connected by a bridging crack, in agreement with experiments.","sentences":["Cracks develop various surface patterns as they propagate in three-dimensional (3D) materials.","Facet formation in nominally tensile (mode-I) fracture emerge in the slow, non-inertial regime and oftentimes takes the form of surface steps.","We show that the same phase-field framework that recently shed basic light on dynamic (inertial) tensile fracture in 3D, also gives rise to crack surface steps.","Step formation is shown to be an intrinsically nonlinear phenomenon that involves two essential physical ingredients: finite-strength quenched disorder and a small, mesoscopic anti-plane shear (mode-III) loading component (on top of the dominant tensile, mode-I loading component).","We quantify the interplay between disorder (both its strength and spatial correlation length) and mesoscopic mode I+III mixity in controlling step formation.","Finally, we show that surface steps grow out of the small-scale, background surface roughness and are composed of two overlapping crack segments connected by a bridging crack, in agreement with experiments."],"url":"http://arxiv.org/abs/2403.17781v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 15:07:27","title":"LiDAR-Based Crop Row Detection Algorithm for Over-Canopy Autonomous Navigation in Agriculture Fields","abstract":"Autonomous navigation is crucial for various robotics applications in agriculture. However, many existing methods depend on RTK-GPS systems, which are expensive and susceptible to poor signal coverage. This paper introduces a state-of-the-art LiDAR-based navigation system that can achieve over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the interrow spacing. Our crop row detection algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, weed presence, and discontinuities within the crop rows. Without utilizing the global localization of the robot, our navigation system can perform autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, providing a crop-agnostic approach to navigate the whole row-crop field. This navigation system has undergone tests in various simulated agricultural fields, achieving an average of $2.98cm$ autonomous driving accuracy without human intervention on the custom Amiga robot. In addition, the qualitative results of our crop row detection algorithm from the actual soybean fields validate our LiDAR-based crop row detection algorithm's potential for practical agricultural applications.","sentences":["Autonomous navigation is crucial for various robotics applications in agriculture.","However, many existing methods depend on RTK-GPS systems, which are expensive and susceptible to poor signal coverage.","This paper introduces a state-of-the-art LiDAR-based navigation system that can achieve over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the interrow spacing.","Our crop row detection algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, weed presence, and discontinuities within the crop rows.","Without utilizing the global localization of the robot, our navigation system can perform autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, providing a crop-agnostic approach to navigate the whole row-crop field.","This navigation system has undergone tests in various simulated agricultural fields, achieving an average of $2.98cm$ autonomous driving accuracy without human intervention on the custom Amiga robot.","In addition, the qualitative results of our crop row detection algorithm from the actual soybean fields validate our LiDAR-based crop row detection algorithm's potential for practical agricultural applications."],"url":"http://arxiv.org/abs/2403.17774v1","category":"cs.RO"}
{"created":"2024-03-26 15:00:01","title":"Giant planet formation in the solar system","abstract":"The formation history of Jupiter has been of interest due to its ability to shape the solar system's history. Yet little attention has been paid to the formation and growth of Saturn and the other giant planets. Here, we explore the implications of the simplest disc and pebble accretion model with steady-state accretion on the formation of giant planets in the solar system through N-body simulations. We conducted a statistical survey of different disc parameters and initial conditions of the protoplanetary disc to establish which combination best reproduces the present outer solar system. We examined the effect of the initial planetesimal disc mass, the number of planetesimals and their size-frequency distribution slope, pebble accretion prescription, and sticking efficiency on the likelihood of forming gas giants and their orbital distribution. The results reveal that the accretion sticking efficiency is the most sensitive parameter for controlling the final masses and number of giant planets. We have been unable to replicate the formation of all three types of giant planets in the solar system in a single simulation. The probability distribution of the final location of the giant planets is approximately constant in $\\log r$, suggesting there is a slight preference for formation closer to the Sun but no preference for more massive planets to form closer. The eccentricity distribution has a higher mean for more massive planets, indicating that systems with more massive planets are more violent. The formation timescales of the cores of the gas giants are distinct, suggesting that they formed sequentially.","sentences":["The formation history of Jupiter has been of interest due to its ability to shape the solar system's history.","Yet little attention has been paid to the formation and growth of Saturn and the other giant planets.","Here, we explore the implications of the simplest disc and pebble accretion model with steady-state accretion on the formation of giant planets in the solar system through N-body simulations.","We conducted a statistical survey of different disc parameters and initial conditions of the protoplanetary disc to establish which combination best reproduces the present outer solar system.","We examined the effect of the initial planetesimal disc mass, the number of planetesimals and their size-frequency distribution slope, pebble accretion prescription, and sticking efficiency on the likelihood of forming gas giants and their orbital distribution.","The results reveal that the accretion sticking efficiency is the most sensitive parameter for controlling the final masses and number of giant planets.","We have been unable to replicate the formation of all three types of giant planets in the solar system in a single simulation.","The probability distribution of the final location of the giant planets is approximately constant in $\\log r$, suggesting there is a slight preference for formation closer to the Sun but no preference for more massive planets to form closer.","The eccentricity distribution has a higher mean for more massive planets, indicating that systems with more massive planets are more violent.","The formation timescales of the cores of the gas giants are distinct, suggesting that they formed sequentially."],"url":"http://arxiv.org/abs/2403.17771v1","category":"astro-ph.EP"}
{"created":"2024-03-26 14:53:24","title":"MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash Representations","abstract":"We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing multiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM effectively tracks camera positions and incrementally builds a scalable multi-map representation for both small and large indoor environments. It dynamically allocates sub-maps for newly observed local regions, enabling constraint-free mapping without prior scene information. Unlike traditional grid-based methods, we use three orthogonal axis-aligned planes for hash-encoding scene properties, significantly reducing hash collisions and the number of trainable parameters. This hybrid approach not only speeds up convergence but also enhances the fidelity of surface reconstruction. Furthermore, our optimization strategy concurrently optimizes all sub-maps intersecting with the current camera frustum, ensuring global consistency. Extensive testing on both real-world and synthetic datasets has shown that MUTE-SLAM delivers state-of-the-art surface reconstruction quality and competitive tracking performance across diverse indoor settings. The code will be made public upon acceptance of the paper.","sentences":["We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing multiple tri-plane hash-encodings for efficient scene representation.","MUTE-SLAM effectively tracks camera positions and incrementally builds a scalable multi-map representation for both small and large indoor environments.","It dynamically allocates sub-maps for newly observed local regions, enabling constraint-free mapping without prior scene information.","Unlike traditional grid-based methods, we use three orthogonal axis-aligned planes for hash-encoding scene properties, significantly reducing hash collisions and the number of trainable parameters.","This hybrid approach not only speeds up convergence but also enhances the fidelity of surface reconstruction.","Furthermore, our optimization strategy concurrently optimizes all sub-maps intersecting with the current camera frustum, ensuring global consistency.","Extensive testing on both real-world and synthetic datasets has shown that MUTE-SLAM delivers state-of-the-art surface reconstruction quality and competitive tracking performance across diverse indoor settings.","The code will be made public upon acceptance of the paper."],"url":"http://arxiv.org/abs/2403.17765v1","category":"cs.CV"}
{"created":"2024-03-26 14:45:17","title":"Hidden-charm pentaquark states $qqqc\\bar{c}$ $(q = u,d)$ in the chiral SU(3) quark model","abstract":"In this work, we systematically calculate the spectrum of hidden-charm pentaquark states $qqqc\\bar{c}$ $(q = u,d)$ in the chiral SU(3) quark model, which has been quite successful in reproducing consistently the energies of octet and decuplet baryon ground states, the binding energy of deuteron, and the nucleon-nucleon ($NN$) scattering phase shifts and mixing parameters for partial waves with total angular momentum up to $J=6$. The Hamiltonian contains the one-gluon-exchange (OGE) potential, the Goldstone-boson-exchange (GBE) potential, the confinement potential, and the kinetic energy of the system. We solve the Schr\\\"odinger equation by use of the variational method. It is found that the masses of all the experimentally observed $P_c(4312)$, $P_c(4380)$, $P_c(4440)$, and $P_c(4457)$ states are much overestimated, indicating that these states are not compact pentaquark states in the chiral SU(3) quark model. All other $qqqc\\bar{c}$ $(q = u,d)$ states are found to lie much above the corresponding baryon-meson thresholds, and thus are not suggested as stable pentaquark states due to their fall-apart decays. A detailed comparison of the results with those obtained in the OGE model and the chromomagnetic interaction (CMI) model is further given.","sentences":["In this work, we systematically calculate the spectrum of hidden-charm pentaquark states $qqqc\\bar{c}$ $(q = u,d)$ in the chiral SU(3) quark model, which has been quite successful in reproducing consistently the energies of octet and decuplet baryon ground states, the binding energy of deuteron, and the nucleon-nucleon ($NN$) scattering phase shifts and mixing parameters for partial waves with total angular momentum up to $J=6$. The Hamiltonian contains the one-gluon-exchange (OGE) potential, the Goldstone-boson-exchange (GBE) potential, the confinement potential, and the kinetic energy of the system.","We solve the Schr\\\"odinger equation by use of the variational method.","It is found that the masses of all the experimentally observed $P_c(4312)$, $P_c(4380)$, $P_c(4440)$, and $P_c(4457)$ states are much overestimated, indicating that these states are not compact pentaquark states in the chiral SU(3) quark model.","All other $qqqc\\bar{c}$ $(q = u,d)$ states are found to lie much above the corresponding baryon-meson thresholds, and thus are not suggested as stable pentaquark states due to their fall-apart decays.","A detailed comparison of the results with those obtained in the OGE model and the chromomagnetic interaction (CMI) model is further given."],"url":"http://arxiv.org/abs/2403.17756v1","category":"hep-ph"}
{"created":"2024-03-26 14:43:57","title":"CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model","abstract":"Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management. Current Spatio-Temporal Transformer models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions. We introduce the Criss-Crossed Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA), Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified Temporal Self-attention (ReTSA). These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method. Extensive tests on six real-world datasets highlight CCDSReFormer's superior performance. An ablation study also confirms the significant impact of each component on the model's predictive accuracy, showcasing our model's ability to forecast traffic flow effectively.","sentences":["Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management.","Current Spatio-Temporal Transformer models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions.","We introduce the Criss-Crossed Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA), Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified Temporal Self-attention (ReTSA).","These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method.","Extensive tests on six real-world datasets highlight CCDSReFormer's superior performance.","An ablation study also confirms the significant impact of each component on the model's predictive accuracy, showcasing our model's ability to forecast traffic flow effectively."],"url":"http://arxiv.org/abs/2403.17753v1","category":"cs.LG"}
{"created":"2024-03-26 14:36:22","title":"Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients","abstract":"Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental results on two real-world datasets demonstrate that RAREMed provides accurate drug sets for both rare and common disease patients, thereby mitigating unfairness in medication recommendation systems.","sentences":["Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information.","However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions.","In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases.","RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes.","Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes.","Experimental results on two real-world datasets demonstrate that RAREMed provides accurate drug sets for both rare and common disease patients, thereby mitigating unfairness in medication recommendation systems."],"url":"http://arxiv.org/abs/2403.17745v1","category":"cs.LG"}
{"created":"2024-03-26 14:26:40","title":"Dirac Dispersions and Fermi Surface Nesting in LaAgSb$_{2}$","abstract":"LaCuSb$_{2}$ is a superconductor with a transition temperature of about $T_\\text{c} = 0.9$K and is a potential platform where Dirac fermions can be experimentally observed. In this paper, we report systematic high-resolution studies of its electronic structure using the angle-resolved photoemission spectroscopy (ARPES) technique supported by the DFT calculation. The Fermi surface consists of four branches, of which the two inner ones are more 3-dimensional and the theoretical calculations reproduce well the experiment. We observe several linear dispersions forming Dirac-like structures. The nodal lines are present in the system along ${\\text{M}}$-${\\text{A}}$ and ${\\text{X}}$-${\\text{R}}$ and Dirac crossings along ${\\text{X}}$-${\\text{R}}$ are observed by ARPES. Finally, the nesting between external Fermi surface pockets, which corresponds to charge density wave (CDW) modulation vector is enhanced in LaCuSb$_{2}$ as compared to LaAgSb$_{2}$, while CDW appears in the latter system.","sentences":["LaCuSb$_{2}$ is a superconductor with a transition temperature of about $T_\\text{c} = 0.9$K","and is a potential platform where Dirac fermions can be experimentally observed.","In this paper, we report systematic high-resolution studies of its electronic structure using the angle-resolved photoemission spectroscopy (ARPES) technique supported by the DFT calculation.","The Fermi surface consists of four branches, of which the two inner ones are more 3-dimensional and the theoretical calculations reproduce well the experiment.","We observe several linear dispersions forming Dirac-like structures.","The nodal lines are present in the system along ${\\text{M}}$-${\\text{A}}$ and ${\\text{X}}$-${\\text{R}}$ and Dirac crossings along ${\\text{X}}$-${\\text{R}}$ are observed by ARPES.","Finally, the nesting between external Fermi surface pockets, which corresponds to charge density wave (CDW) modulation vector is enhanced in LaCuSb$_{2}$ as compared to LaAgSb$_{2}$, while CDW appears in the latter system."],"url":"http://arxiv.org/abs/2403.17737v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 14:25:43","title":"A Family of Monomial Gr\u00f6bner Degenerations of Determinantal Ideals with Minimal Cellular Resolutions","abstract":"We explore a family of monomial ideals derived as Gr\\\"obner degenerations of determinantal ideals. These ideals, previously examined as block diagonal matching field ideals within the realm of toric degenerations of Grassmannians, are identified as monomial initial ideals of determinantal ideals. We establish their linear quotient property and calculate their Betti numbers, demonstrating that their minimal free resolution is supported on a regular CW complex, equivalently, they have a cellular resolution. On the combinatorial front, we examine a range of weight vectors that yield diverse monomial initial ideals for determinantal ideals. We show that despite variations in weight vectors, all these ideals share identical Betti numbers. However, altering a weight vector results in distinct cellular complexes supporting their minimal free resolutions.","sentences":["We explore a family of monomial ideals derived as Gr\\\"obner degenerations of determinantal ideals.","These ideals, previously examined as block diagonal matching field ideals within the realm of toric degenerations of Grassmannians, are identified as monomial initial ideals of determinantal ideals.","We establish their linear quotient property and calculate their Betti numbers, demonstrating that their minimal free resolution is supported on a regular CW complex, equivalently, they have a cellular resolution.","On the combinatorial front, we examine a range of weight vectors that yield diverse monomial initial ideals for determinantal ideals.","We show that despite variations in weight vectors, all these ideals share identical Betti numbers.","However, altering a weight vector results in distinct cellular complexes supporting their minimal free resolutions."],"url":"http://arxiv.org/abs/2403.17736v1","category":"math.AC"}
{"created":"2024-03-26 14:19:22","title":"On Structural Non-commutativity in Affine Feedback of SISO Nonlinear Systems","abstract":"The affine feedback connection of SISO nonlinear systems modeled by Chen--Fliess series is shown to be a group action on the plant which is isomorphic to the semi-direct product of shuffle and additive group of non-commutative formal power series. The additive and multiplicative feedback loops in an affine feedback connection are thus proven to be structurally non-commutative. A flip in the order of these loops results in a net additive feedback loop.","sentences":["The affine feedback connection of SISO nonlinear systems modeled by Chen--Fliess series is shown to be a group action on the plant which is isomorphic to the semi-direct product of shuffle and additive group of non-commutative formal power series.","The additive and multiplicative feedback loops in an affine feedback connection are thus proven to be structurally non-commutative.","A flip in the order of these loops results in a net additive feedback loop."],"url":"http://arxiv.org/abs/2403.17730v1","category":"math.OC"}
{"created":"2024-03-26 13:59:42","title":"Distance-Based Hierarchical Cutting of Complex Networks with Non-Preferential and Preferential Choice of Seeds","abstract":"Graphs and complex networks can be successively separated into connected components associated to respective seed nodes, therefore establishing a respective hierarchical organization. In the present work, we study the properties of the hierarchical structure implied by distance-based cutting of Erd\\H{o}s-R\\'enyi, Barab\\'asi-Albert, and a specific geometric network. Two main situations are considered regarding the choice of the seeds: non-preferential and preferential to the respective node degree. Among the obtained findings, we have the tendency of geometrical networks yielding more balanced pairs of connected components along the network progressive separation, presenting little chaining effects, followed by the Erd\\H{o}s-R\\'enyi and Barab\\'asi-Albert types of networks. The choice of seeds preferential to the node degree tended to enhance the balance of the connected components in the case of the geometrical networks.","sentences":["Graphs and complex networks can be successively separated into connected components associated to respective seed nodes, therefore establishing a respective hierarchical organization.","In the present work, we study the properties of the hierarchical structure implied by distance-based cutting of Erd\\H{o}s-R\\'enyi, Barab\\'asi-Albert, and a specific geometric network.","Two main situations are considered regarding the choice of the seeds: non-preferential and preferential to the respective node degree.","Among the obtained findings, we have the tendency of geometrical networks yielding more balanced pairs of connected components along the network progressive separation, presenting little chaining effects, followed by the Erd\\H{o}s-R\\'enyi and Barab\\'asi-Albert types of networks.","The choice of seeds preferential to the node degree tended to enhance the balance of the connected components in the case of the geometrical networks."],"url":"http://arxiv.org/abs/2403.17713v1","category":"physics.soc-ph"}
{"created":"2024-03-26 13:58:21","title":"Using quantum computers in control: interval matrix properties","abstract":"Quantum computing provides a powerful framework for tackling computational problems that are classically intractable. The goal of this paper is to explore the use of quantum computers for solving relevant problems in systems and control theory. In the recent literature, different quantum algorithms have been developed to tackle binary optimization, which plays an important role in various control-theoretic problems. As a prototypical example, we consider the verification of interval matrix properties such as non-singularity and stability on a quantum computer. We present a quantum algorithm solving these problems and we study its performance in simulation. Our results demonstrate that quantum computers provide a promising tool for control whose applicability to further computationally complex problems remains to be explored.","sentences":["Quantum computing provides a powerful framework for tackling computational problems that are classically intractable.","The goal of this paper is to explore the use of quantum computers for solving relevant problems in systems and control theory.","In the recent literature, different quantum algorithms have been developed to tackle binary optimization, which plays an important role in various control-theoretic problems.","As a prototypical example, we consider the verification of interval matrix properties such as non-singularity and stability on a quantum computer.","We present a quantum algorithm solving these problems and we study its performance in simulation.","Our results demonstrate that quantum computers provide a promising tool for control whose applicability to further computationally complex problems remains to be explored."],"url":"http://arxiv.org/abs/2403.17711v1","category":"eess.SY"}
{"created":"2024-03-26 13:40:18","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation","abstract":"Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets, demonstrating the superior segmentation performance of our proposed TM-UNet. Additionally, compared to the previous VM-UNet, our model achieves a one-third reduction in parameters.","sentences":["Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain.","Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity.","Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision.","However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction.","Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet.","The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions.","We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets, demonstrating the superior segmentation performance of our proposed TM-UNet.","Additionally, compared to the previous VM-UNet, our model achieves a one-third reduction in parameters."],"url":"http://arxiv.org/abs/2403.17701v1","category":"eess.IV"}
{"created":"2024-03-26 13:39:07","title":"Dynamical Zeta functions for differentiable parabolic maps of the interval","abstract":"This paper explores the domain of meromorphic extension for the dynamical zeta function associated to a class of one-dimensional differentiable parabolic maps featuring an indifferent fixed point. We establish the connection between this domain and the spectrum of the weighted transfer operators of the induced map. Furthermore, we discuss scenarios where meromorphic extensions occur beyond the confines of the natural disc of convergence of the dynamical zeta function.","sentences":["This paper explores the domain of meromorphic extension for the dynamical zeta function associated to a class of one-dimensional differentiable parabolic maps featuring an indifferent fixed point.","We establish the connection between this domain and the spectrum of the weighted transfer operators of the induced map.","Furthermore, we discuss scenarios where meromorphic extensions occur beyond the confines of the natural disc of convergence of the dynamical zeta function."],"url":"http://arxiv.org/abs/2403.17700v1","category":"math.DS"}
{"created":"2024-03-26 13:32:21","title":"Looking For Timing Variations in the Transits of 16 Exoplanets","abstract":"We update the ephemerides of 16 transiting exoplanets using our ground-based observations, new TESS data, and previously published observations including those of amateur astronomers. All these light curves were modeled by making use of a set of quantitative criteria with the EXOFAST code to obtain mid-transit times. We searched for statistically significant secular and/or periodic trends in the mid-transit times. We found that the timing data are well modeled by a linear ephemeris for all systems except for XO-2 b, for which we detect an orbital decay with the rate of -12.95 $\\pm$ 1.85 ms/yr that can be confirmed with future observations. We also detect a hint of potential periodic variations in the TTV data of HAT-P-13 b which also requires confirmation with further precise observations.","sentences":["We update the ephemerides of 16 transiting exoplanets using our ground-based observations, new TESS data, and previously published observations including those of amateur astronomers.","All these light curves were modeled by making use of a set of quantitative criteria with the EXOFAST code to obtain mid-transit times.","We searched for statistically significant secular and/or periodic trends in the mid-transit times.","We found that the timing data are well modeled by a linear ephemeris for all systems except for XO-2 b, for which we detect an orbital decay with the rate of -12.95 $\\pm$ 1.85 ms/yr that can be confirmed with future observations.","We also detect a hint of potential periodic variations in the TTV data of HAT-P-13 b which also requires confirmation with further precise observations."],"url":"http://arxiv.org/abs/2403.17690v1","category":"astro-ph.EP"}
{"created":"2024-03-26 13:08:56","title":"Shape Optimization of Geometrically Nonlinear Modal Coupling Coefficients: An Application to MEMS Gyroscopes","abstract":"Micro- and nanoelectromechanical system (MEMS and NEMS) resonators can exhibit rich nonlinear dynamics as they are often operated at large amplitudes with high quality factors and possess a high mode density with a variety of nonlinear modal couplings. Their impact is strongly influenced by internal resonance conditions and by the strength of the modal coupling coefficients. On one hand, strong nonlinear couplings are of academic interest and promise novel device concepts. On the other hand, however, they have the potential to disturb the linear system behavior on which industrial devices such as gyroscopes and micro mirrors are based on. In either case, being able to optimize the coupling coefficients by design is certainly beneficial. A main source of nonlinear modal couplings are geometric nonlinearities. In this work, we apply node-based shape optimization to tune the geometrically nonlinear 3-wave coupling coefficients of a MEMS gyroscope. We demonstrate that individual coupling coefficients can be tuned over several orders of magnitude by shape optimization, while satisfying typical constraints on manufacturability and operability of the devices. The optimized designs contain unintuitive geometrical features far away from any solution an experienced human MEMS or NEMS designer could have thought of. Thus, this work demonstrates the power of shape optimization for tailoring the complex nonlinear dynamic properties of MEMS and NEMS resonators.","sentences":["Micro- and nanoelectromechanical system (MEMS and NEMS) resonators can exhibit rich nonlinear dynamics as they are often operated at large amplitudes with high quality factors and possess a high mode density with a variety of nonlinear modal couplings.","Their impact is strongly influenced by internal resonance conditions and by the strength of the modal coupling coefficients.","On one hand, strong nonlinear couplings are of academic interest and promise novel device concepts.","On the other hand, however, they have the potential to disturb the linear system behavior on which industrial devices such as gyroscopes and micro mirrors are based on.","In either case, being able to optimize the coupling coefficients by design is certainly beneficial.","A main source of nonlinear modal couplings are geometric nonlinearities.","In this work, we apply node-based shape optimization to tune the geometrically nonlinear 3-wave coupling coefficients of a MEMS gyroscope.","We demonstrate that individual coupling coefficients can be tuned over several orders of magnitude by shape optimization, while satisfying typical constraints on manufacturability and operability of the devices.","The optimized designs contain unintuitive geometrical features far away from any solution an experienced human MEMS or NEMS designer could have thought of.","Thus, this work demonstrates the power of shape optimization for tailoring the complex nonlinear dynamic properties of MEMS and NEMS resonators."],"url":"http://arxiv.org/abs/2403.17679v1","category":"cs.CE"}
{"created":"2024-03-26 13:05:49","title":"Hierarchical Light Transformer Ensembles for Multimodal Trajectory Forecasting","abstract":"Accurate trajectory forecasting is crucial for the performance of various systems, such as advanced driver-assistance systems and self-driving vehicles. These forecasts allow to anticipate events leading to collisions and, therefore, to mitigate them. Deep Neural Networks have excelled in motion forecasting, but issues like overconfidence and uncertainty quantification persist. Deep Ensembles address these concerns, yet applying them to multimodal distributions remains challenging. In this paper, we propose a novel approach named Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently training an ensemble of Transformer architectures using a novel hierarchical loss function. HLT-Ens leverages grouped fully connected layers, inspired by grouped convolution techniques, to capture multimodal distributions, effectively. Through extensive experimentation, we demonstrate that HLT-Ens achieves state-of-the-art performance levels, offering a promising avenue for improving trajectory forecasting techniques.","sentences":["Accurate trajectory forecasting is crucial for the performance of various systems, such as advanced driver-assistance systems and self-driving vehicles.","These forecasts allow to anticipate events leading to collisions and, therefore, to mitigate them.","Deep Neural Networks have excelled in motion forecasting, but issues like overconfidence and uncertainty quantification persist.","Deep Ensembles address these concerns, yet applying them to multimodal distributions remains challenging.","In this paper, we propose a novel approach named Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently training an ensemble of Transformer architectures using a novel hierarchical loss function.","HLT-Ens leverages grouped fully connected layers, inspired by grouped convolution techniques, to capture multimodal distributions, effectively.","Through extensive experimentation, we demonstrate that HLT-Ens achieves state-of-the-art performance levels, offering a promising avenue for improving trajectory forecasting techniques."],"url":"http://arxiv.org/abs/2403.17678v1","category":"cs.CV"}
{"created":"2024-03-26 13:02:48","title":"Chattering Phenomena in Time-Optimal Control for High-Order Chain-of-Integrators Systems with Full State Constraints","abstract":"Time-optimal control for high-order chain-of-integrators systems with full state constraints and arbitrary given terminal states remains an open and challenging problem in optimal control theory domain. However, optimal control's behaviors in high-order problems lack of precision characterization, even where the existence of chattering phenomena remain unknown and overlooked. This paper establishes a theoretical framework of chattering phenomena in the problem, focusing on the uniqueness of state constraints inducing chattering, the upper bound of switching times in an unconstrained arc during chattering, and the convergence of states and costates to the chattering limit point. For the first time, this paper proves the existence of chattering phenomena in the problems. The chattering optimal control for 4th order problems with velocity constraints is precisely solved, providing an approach to plan strictly time-optimal snap-limited trajectories, while other cases of order $n\\leq4$ are proved to not allow chattering. The conclusions correct the longstanding misconception in the industry regarding the time-optimality of S-shaped trajectories with minimal switching times.","sentences":["Time-optimal control for high-order chain-of-integrators systems with full state constraints and arbitrary given terminal states remains an open and challenging problem in optimal control theory domain.","However, optimal control's behaviors in high-order problems lack of precision characterization, even where the existence of chattering phenomena remain unknown and overlooked.","This paper establishes a theoretical framework of chattering phenomena in the problem, focusing on the uniqueness of state constraints inducing chattering, the upper bound of switching times in an unconstrained arc during chattering, and the convergence of states and costates to the chattering limit point.","For the first time, this paper proves the existence of chattering phenomena in the problems.","The chattering optimal control for 4th order problems with velocity constraints is precisely solved, providing an approach to plan strictly time-optimal snap-limited trajectories, while other cases of order $n\\leq4$ are proved to not allow chattering.","The conclusions correct the longstanding misconception in the industry regarding the time-optimality of S-shaped trajectories with minimal switching times."],"url":"http://arxiv.org/abs/2403.17675v1","category":"math.OC"}
{"created":"2024-03-26 13:02:38","title":"Predicting Perceived Gloss: Do Weak Labels Suffice?","abstract":"Estimating perceptual attributes of materials directly from images is a challenging task due to their complex, not fully-understood interactions with external factors, such as geometry and lighting. Supervised deep learning models have recently been shown to outperform traditional approaches, but rely on large datasets of human-annotated images for accurate perception predictions. Obtaining reliable annotations is a costly endeavor, aggravated by the limited ability of these models to generalise to different aspects of appearance. In this work, we show how a much smaller set of human annotations (\"strong labels\") can be effectively augmented with automatically derived \"weak labels\" in the context of learning a low-dimensional image-computable gloss metric. We evaluate three alternative weak labels for predicting human gloss perception from limited annotated data. Incorporating weak labels enhances our gloss prediction beyond the current state of the art. Moreover, it enables a substantial reduction in human annotation costs without sacrificing accuracy, whether working with rendered images or real photographs.","sentences":["Estimating perceptual attributes of materials directly from images is a challenging task due to their complex, not fully-understood interactions with external factors, such as geometry and lighting.","Supervised deep learning models have recently been shown to outperform traditional approaches, but rely on large datasets of human-annotated images for accurate perception predictions.","Obtaining reliable annotations is a costly endeavor, aggravated by the limited ability of these models to generalise to different aspects of appearance.","In this work, we show how a much smaller set of human annotations (\"strong labels\") can be effectively augmented with automatically derived \"weak labels\" in the context of learning a low-dimensional image-computable gloss metric.","We evaluate three alternative weak labels for predicting human gloss perception from limited annotated data.","Incorporating weak labels enhances our gloss prediction beyond the current state of the art.","Moreover, it enables a substantial reduction in human annotation costs without sacrificing accuracy, whether working with rendered images or real photographs."],"url":"http://arxiv.org/abs/2403.17672v1","category":"cs.GR"}
{"created":"2024-03-26 12:31:58","title":"Exploring Dynamic Transformer for Efficient Object Tracking","abstract":"The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources. Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision. In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic transformer framework for efficient tracking. Real-world tracking scenarios exhibit diverse levels of complexity. We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones. DyTrack automatically learns to configure proper reasoning routes for various inputs, gaining better utilization of the available computational budget. Thus, it can achieve higher performance with the same running speed. We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model. Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors. Furthermore, a target-aware self-distillation strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model. Extensive experiments on multiple benchmarks demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model. For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps.","sentences":["The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources.","Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision.","In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic transformer framework for efficient tracking.","Real-world tracking scenarios exhibit diverse levels of complexity.","We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones.","DyTrack automatically learns to configure proper reasoning routes for various inputs, gaining better utilization of the available computational budget.","Thus, it can achieve higher performance with the same running speed.","We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model.","Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors.","Furthermore, a target-aware self-distillation strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model.","Extensive experiments on multiple benchmarks demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model.","For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps."],"url":"http://arxiv.org/abs/2403.17651v1","category":"cs.CV"}
{"created":"2024-03-26 12:31:33","title":"Microscale Morphology Driven Thermal Transport in Fiber Reinforced Polymer Composites","abstract":"Fiber-reinforced polymer composite (FRPC) materials are used extensively in various industries, such as aerospace, automobiles, and electronics packaging, due to their remarkable specific strength and desirable properties, such as enhanced durability and corrosion resistance. The evolution of thermal properties in FRPCs is crucial for advancing thermal management systems, optimizing material performance, and enhancing energy efficiency across these diverse sectors. Despite significant research efforts to develop new materials with improved thermal properties and reduced thermal degradation, there is a lack of understanding of the thermal transport phenomena considering the influence of microscale reinforcement morphology in these composites. In the current study, we performed experimental investigations complemented by computations to determine the thermal transport properties and associated phenomena in epoxy and carbon fiber-reinforced epoxy composites. The experimental findings were utilized as input data for numerical analysis to examine the impact of fiber morphology and volume fraction in thermal transport phenomena. Our results revealed that composites incorporating non-circular fibers manifested higher thermal conductivity than traditional circular fibers in the transverse direction. This can be attributed to increased interconnected heat flow pathways facilitated by the increased surface area of non-circular fibers with the same cross-sectional areas, resulting in efficient heat transfer.","sentences":["Fiber-reinforced polymer composite (FRPC) materials are used extensively in various industries, such as aerospace, automobiles, and electronics packaging, due to their remarkable specific strength and desirable properties, such as enhanced durability and corrosion resistance.","The evolution of thermal properties in FRPCs is crucial for advancing thermal management systems, optimizing material performance, and enhancing energy efficiency across these diverse sectors.","Despite significant research efforts to develop new materials with improved thermal properties and reduced thermal degradation, there is a lack of understanding of the thermal transport phenomena considering the influence of microscale reinforcement morphology in these composites.","In the current study, we performed experimental investigations complemented by computations to determine the thermal transport properties and associated phenomena in epoxy and carbon fiber-reinforced epoxy composites.","The experimental findings were utilized as input data for numerical analysis to examine the impact of fiber morphology and volume fraction in thermal transport phenomena.","Our results revealed that composites incorporating non-circular fibers manifested higher thermal conductivity than traditional circular fibers in the transverse direction.","This can be attributed to increased interconnected heat flow pathways facilitated by the increased surface area of non-circular fibers with the same cross-sectional areas, resulting in efficient heat transfer."],"url":"http://arxiv.org/abs/2403.17650v1","category":"physics.app-ph"}
{"created":"2024-03-26 12:27:32","title":"DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition","abstract":"End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task. A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach. DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities. More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities.","sentences":["End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks.","A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance.","However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially.","In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription.","To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to adapt swiftly to domain-specific entities for the NEC task.","A series of experiments conducted on the AISHELL-1 and Homophone datasets confirm the effectiveness of our modeling approach.","DANCER outperforms a strong baseline, the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate (CER) reduction of about 7% relatively on AISHELL-1 for named entities.","More notably, when tested on Homophone that contain named entities of high phonetic confusion, DANCER offers a more pronounced CER reduction of 46% relatively over PED-NEC for named entities."],"url":"http://arxiv.org/abs/2403.17645v1","category":"cs.CL"}
{"created":"2024-03-26 12:08:58","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems","abstract":"Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations. This adaptive approach selectively masks input tokens, transforming the recommendation task into an inference challenge based on varying token subsets, thereby enhancing the agent's ability to infer across diverse trajectory lengths. Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency. Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method.","sentences":["Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services.","Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework.","Recent advancements in offline RLRS provide a solution for how to address these two challenges.","However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs.","Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences.","In this study, we introduce a new offline RLRS method to deal with the above problems.","We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurations.","This adaptive approach selectively masks input tokens, transforming the recommendation task into an inference challenge based on varying token subsets, thereby enhancing the agent's ability to infer across diverse trajectory lengths.","Furthermore, we incorporate a multi-scale segmented retention mechanism that facilitates efficient modeling of long sequences, significantly enhancing computational efficiency.","Our experimental analysis, conducted on both online simulator and offline datasets, clearly demonstrates the advantages of our proposed method."],"url":"http://arxiv.org/abs/2403.17634v1","category":"cs.IR"}
{"created":"2024-03-26 12:04:09","title":"Waveform Design for Joint Communication and SAR Imaging Under Random Signaling","abstract":"Conventional synthetic aperture radar (SAR) imaging systems typically employ deterministic signal designs, which lack the capability to convey communication information and are thus not suitable for integrated sensing and communication (ISAC) scenarios. In this letter, we propose a joint communication and SAR imaging (JCASAR) system based on orthogonal frequency-division multiplexing (OFDM) signal with cyclic prefix (CP), which is capable of reconstructing the target profile while serving a communication user. In contrast to traditional matched filters, we propose a least squares (LS) estimator for range profiling. Then the SAR image is obtained followed by range cell migration correction (RCMC) and azimuth processing. By minimizing the mean squared error (MSE) of the proposed LS estimator, we investigate the optimal waveform design for SAR imaging, and JCASAR under random signaling, where power allocation strategies are conceived for Gaussian-distributed ISAC signals, in an effort to strike a flexible performance tradeoff between the communication and SAR imaging tasks. Numerical results are provided to validate the effectiveness of the proposed ISAC waveform design for JCASAR systems.","sentences":["Conventional synthetic aperture radar (SAR) imaging systems typically employ deterministic signal designs, which lack the capability to convey communication information and are thus not suitable for integrated sensing and communication (ISAC) scenarios.","In this letter, we propose a joint communication and SAR imaging (JCASAR) system based on orthogonal frequency-division multiplexing (OFDM) signal with cyclic prefix (CP), which is capable of reconstructing the target profile while serving a communication user.","In contrast to traditional matched filters, we propose a least squares (LS) estimator for range profiling.","Then the SAR image is obtained followed by range cell migration correction (RCMC) and azimuth processing.","By minimizing the mean squared error (MSE) of the proposed LS estimator, we investigate the optimal waveform design for SAR imaging, and JCASAR under random signaling, where power allocation strategies are conceived for Gaussian-distributed ISAC signals, in an effort to strike a flexible performance tradeoff between the communication and SAR imaging tasks.","Numerical results are provided to validate the effectiveness of the proposed ISAC waveform design for JCASAR systems."],"url":"http://arxiv.org/abs/2403.17627v1","category":"eess.SP"}
{"created":"2024-03-26 11:58:29","title":"Syzygy Theoretic Approach to Horrocks-type Criteria for Vector Bundles","abstract":"This paper studies a variant of Horrocks criteria for vector bundles mainly through a syzygy theoretic approach. In this spirit we begin with describing various proofs of the splitting criteria for ACM and Buchsbaum bundles, giving new sights of the structure theorem. Our main result gives a structure theorem of quasi-Buchsbaum bundles on ${\\mathbb P}^n$, which characterizes the null-correlation bundle. Also, the quasi-Buchsbaum bundles on ${\\mathbb P}^3$ with simple cohomologies are classified in terms of standard system of parameters.","sentences":["This paper studies a variant of Horrocks criteria for vector bundles mainly through a syzygy theoretic approach.","In this spirit we begin with describing various proofs of the splitting criteria for ACM and Buchsbaum bundles, giving new sights of the structure theorem.","Our main result gives a structure theorem of quasi-Buchsbaum bundles on ${\\mathbb P}^n$, which characterizes the null-correlation bundle.","Also, the quasi-Buchsbaum bundles on ${\\mathbb P}^3$ with simple cohomologies are classified in terms of standard system of parameters."],"url":"http://arxiv.org/abs/2403.17625v1","category":"math.AG"}
{"created":"2024-03-26 11:49:54","title":"Technical Report: Incorporating Blogs in Pollux","abstract":"This technical report describes the incorporation of political blogs into Pollux, the Specialised Information Service (FID) for Political Science in Germany. Considering the widespread use of political blogs in political science research, we decided to include them in the Pollux search system to enhance the available information infrastructure. We describe the crawling and analyzing of the blogs and the pipeline that integrates them into the Pollux system. To demonstrate the content of the incorporated blogs, we also provide a visualization of the topics covered by the blog posts during the first three months following integration.","sentences":["This technical report describes the incorporation of political blogs into Pollux, the Specialised Information Service (FID) for Political Science in Germany.","Considering the widespread use of political blogs in political science research, we decided to include them in the Pollux search system to enhance the available information infrastructure.","We describe the crawling and analyzing of the blogs and the pipeline that integrates them into the Pollux system.","To demonstrate the content of the incorporated blogs, we also provide a visualization of the topics covered by the blog posts during the first three months following integration."],"url":"http://arxiv.org/abs/2403.17618v1","category":"cs.DL"}
{"created":"2024-03-26 11:30:25","title":"Massless fermions in black string spacetime","abstract":"In this paper we investigate the behaviour of massless fermions in the black string spacetime by computing the eigenvalues and eigenfunctions of the Weyl equations. These solutions allowed us to study the behaviour of such massless fermions in terms of the cosmological constant, the black string's mass and the radial distance of the particle from the black string. The solutions, written in terms of Parabolic Cylinder functions and Laguerre polynomials, were obtained for a particle far from the black string and around the horizon event. For the particle around the event horizon, for all configuration of parameters, the energy eigenvalues are complex-valued, indicating QNM similarly to the case of spherical black holes. For the particle far from the black string, the energies derived from the Weyl equation set up conditions on the parameters in order to keep the energy as a real valued parameter.","sentences":["In this paper we investigate the behaviour of massless fermions in the black string spacetime by computing the eigenvalues and eigenfunctions of the Weyl equations.","These solutions allowed us to study the behaviour of such massless fermions in terms of the cosmological constant, the black string's mass and the radial distance of the particle from the black string.","The solutions, written in terms of Parabolic Cylinder functions and Laguerre polynomials, were obtained for a particle far from the black string and around the horizon event.","For the particle around the event horizon, for all configuration of parameters, the energy eigenvalues are complex-valued, indicating QNM similarly to the case of spherical black holes.","For the particle far from the black string, the energies derived from the Weyl equation set up conditions on the parameters in order to keep the energy as a real valued parameter."],"url":"http://arxiv.org/abs/2403.17604v1","category":"hep-th"}
{"created":"2024-03-26 11:28:31","title":"END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential Recommendation","abstract":"In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing. However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model. Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests. To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count. Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise. Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal. Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation.","sentences":["In recommendation systems, users frequently engage in multiple types of behaviors, such as clicking, adding to a cart, and purchasing.","However, with diversified behavior data, user behavior sequences will become very long in the short term, which brings challenges to the efficiency of the sequence recommendation model.","Meanwhile, some behavior data will also bring inevitable noise to the modeling of user interests.","To address the aforementioned issues, firstly, we develop the Efficient Behavior Sequence Miner (EBM) that efficiently captures intricate patterns in user behavior while maintaining low time complexity and parameter count.","Secondly, we design hard and soft denoising modules for different noise types and fully explore the relationship between behaviors and noise.","Finally, we introduce a contrastive loss function along with a guided training strategy to compare the valid information in the data with the noisy signal, and seamlessly integrate the two denoising processes to achieve a high degree of decoupling of the noisy signal.","Sufficient experiments on real-world datasets demonstrate the effectiveness and efficiency of our approach in dealing with multi-behavior sequential recommendation."],"url":"http://arxiv.org/abs/2403.17603v1","category":"cs.IR"}
{"created":"2024-03-26 11:12:30","title":"On the exterior product of H\u00f6lder differential forms","abstract":"We introduce a complex of cochains, $\\alpha$-fractional charges ($0 < \\alpha \\leq 1$), whose regularity is between that of De Pauw-Moonens-Pfeffer's charges and that of Whitney's flat cochains. We show that $\\alpha$-H\\\"older differential forms and their exterior derivative can be realized as $\\alpha$-fractional charges, and that it is possible to define the exterior product between an $\\alpha$-fractional and a $\\beta$-fractional charge, under the condition that $\\alpha + \\beta > 1$. This construction extends the Young integral in arbitrary dimension and codimension.","sentences":["We introduce a complex of cochains, $\\alpha$-fractional charges ($0 <","\\alpha \\leq 1$), whose regularity is between that of De Pauw-Moonens-Pfeffer's charges and that of Whitney's flat cochains.","We show that $\\alpha$-H\\\"older differential forms and their exterior derivative can be realized as $\\alpha$-fractional charges, and that it is possible to define the exterior product between an $\\alpha$-fractional and a $\\beta$-fractional charge, under the condition that $\\alpha","+ \\beta > 1$.","This construction extends the Young integral in arbitrary dimension and codimension."],"url":"http://arxiv.org/abs/2403.17600v1","category":"math.DG"}
{"created":"2024-03-26 11:09:27","title":"Ultrafast Adaptive Primary Frequency Tuning and Secondary Frequency Identification for S/S WPT system","abstract":"Magnetic resonance wireless power transfer (WPT) technology is increasingly being adopted across diverse applications. However, its effectiveness can be significantly compromised by parameter shifts within the resonance network, owing to its high system quality factor. Such shifts are inherent and challenging to mitigate during the manufacturing process. In response, this article introduces a rapid frequency tuning approach. Leveraging switch-controlled capacitors (SCC) to adjust the resonance network and the primary side's operating frequency, alongside a current zero-crossing detection (ZCD) circuit for voltage-current phase determination, this method circumvents the need for intricate knowledge of WPT system parameters. Moreover, it obviates the necessity for inter-side communication for real-time identification of the secondary side resonance frequency. The swift response of SCC and two-step perturb-and-observe algorithm mitigate output disturbances, thereby expediting the frequency tuning process. Experimental validation on a 200W Series-Series compensated WPT (SS-WPT) system demonstrates that the proposed method achieves frequency recognition accuracy within 0.7kHz in less than 1ms, increasing system efficiency up to 9%.","sentences":["Magnetic resonance wireless power transfer (WPT) technology is increasingly being adopted across diverse applications.","However, its effectiveness can be significantly compromised by parameter shifts within the resonance network, owing to its high system quality factor.","Such shifts are inherent and challenging to mitigate during the manufacturing process.","In response, this article introduces a rapid frequency tuning approach.","Leveraging switch-controlled capacitors (SCC) to adjust the resonance network and the primary side's operating frequency, alongside a current zero-crossing detection (ZCD) circuit for voltage-current phase determination, this method circumvents the need for intricate knowledge of WPT system parameters.","Moreover, it obviates the necessity for inter-side communication for real-time identification of the secondary side resonance frequency.","The swift response of SCC and two-step perturb-and-observe algorithm mitigate output disturbances, thereby expediting the frequency tuning process.","Experimental validation on a 200W Series-Series compensated WPT (SS-WPT) system demonstrates that the proposed method achieves frequency recognition accuracy within 0.7kHz in less than 1ms, increasing system efficiency up to 9%."],"url":"http://arxiv.org/abs/2403.17598v1","category":"eess.SY"}
{"created":"2024-03-26 11:03:40","title":"Nuclear spin-spin interactions in CdTe probed by zero and ultra-low-field optically detected NMR","abstract":"Nuclear magnetic resonance (NMR) is particularly relevant for studies of internuclear spin coupling at zero and ultra-low fields (ZULF), where spin-spin interactions dominate over Zeeman ones. Here we report on ZULF NMR in CdTe. In this semiconductor all magnetic isotopes have spin $I = 1/2$, so that internuclear interactions are never overshadowed by quadrupole effects. Our experiments rely on warm-up spectroscopy, a technique that combines optical pumping, additional cooling via adiabatic demagnetisation, and detection of the oscillating magnetic field-induced warm-up of the nuclear spin system via Hanle effect. We show that NMR spectra exhibit a rich fine structure, consistent with the low abundance of magnetic isotopes in CdTe, their zero quadrupole moments, as well as direct and indirect interactions between them. A model assuming that the electromagnetic radiation is absorbed by nuclear spin clusters composed of up to 5 magnetic isotopes allows us to reproduce the shape of a major part of the measured spectra.","sentences":["Nuclear magnetic resonance (NMR) is particularly relevant for studies of internuclear spin coupling at zero and ultra-low fields (ZULF), where spin-spin interactions dominate over Zeeman ones.","Here we report on ZULF NMR in CdTe.","In this semiconductor all magnetic isotopes have spin $I = 1/2$, so that internuclear interactions are never overshadowed by quadrupole effects.","Our experiments rely on warm-up spectroscopy, a technique that combines optical pumping, additional cooling via adiabatic demagnetisation, and detection of the oscillating magnetic field-induced warm-up of the nuclear spin system via Hanle effect.","We show that NMR spectra exhibit a rich fine structure, consistent with the low abundance of magnetic isotopes in CdTe, their zero quadrupole moments, as well as direct and indirect interactions between them.","A model assuming that the electromagnetic radiation is absorbed by nuclear spin clusters composed of up to 5 magnetic isotopes allows us to reproduce the shape of a major part of the measured spectra."],"url":"http://arxiv.org/abs/2403.17593v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-26 11:01:26","title":"Mass Concentration of Two-Spinless Fermi Systems with Attractive Interactions","abstract":"We study the two-spinless mass-critical Fermi systems with attractive interactions and trapping potentials. We prove that ground states of the system exist, if and only if the strength $a$ of attractive interactions satisfies $0<a<a_2^*$, where $0<a_2^*<+\\infty$ is the best constant of a dual finite-rank Lieb-Thirring inequality. By the blow-up analysis of many-fermion systems, we show that ground states of the system concentrate at the flattest minimum points of the trapping potential $V(x)$ as $a\\nearrow a_2^*$.","sentences":["We study the two-spinless mass-critical Fermi systems with attractive interactions and trapping potentials.","We prove that ground states of the system exist, if and only if the strength $a$ of attractive interactions satisfies $0<a<a_2^*$, where $0<a_2^*<+\\infty$ is the best constant of a dual finite-rank Lieb-Thirring inequality.","By the blow-up analysis of many-fermion systems, we show that ground states of the system concentrate at the flattest minimum points of the trapping potential $V(x)$ as $a\\nearrow a_2^*$."],"url":"http://arxiv.org/abs/2403.17591v1","category":"math-ph"}
{"created":"2024-03-26 10:48:38","title":"Critical Behavior of Non-Hermitian Kondo effect in Pseudogap System","abstract":"The combination of non-Hermitian physics and strong correlation can yield numerous novel and intriguing effects. A previous study on the non-Hermitian Kondo model in ultra-cold atoms reports the reversion of the renormalization group flow. In this work, We investigate the non-Hermitian Kondo effect in system with special form of density of sates $\\rho (\\omega) \\sim |\\omega|^{r}(r>0)$, which is called pseudogap system. We find that when $r<1/2$, our conclusion from perturbative renormalization group theory aligns well with previous studies on the traditional pseudogap Kondo problem. In the case of $r$ being equal to $1/2$, a fixed point with reverse property appears in the renormalization group flow. When $r$ is lager than $1/2$, an unstable fixed point appears on the complex plane of the parameter space. Additionally, we validate the conclusions around renormalization group for the $r<1/2$ interval using the large $N$ expansion method.","sentences":["The combination of non-Hermitian physics and strong correlation can yield numerous novel and intriguing effects.","A previous study on the non-Hermitian Kondo model in ultra-cold atoms reports the reversion of the renormalization group flow.","In this work, We investigate the non-Hermitian Kondo effect in system with special form of density of sates $\\rho (\\omega) \\sim |\\omega|^{r}(r>0)$, which is called pseudogap system.","We find that when $r<1/2$, our conclusion from perturbative renormalization group theory aligns well with previous studies on the traditional pseudogap Kondo problem.","In the case of $r$ being equal to $1/2$, a fixed point with reverse property appears in the renormalization group flow.","When $r$ is lager than $1/2$, an unstable fixed point appears on the complex plane of the parameter space.","Additionally, we validate the conclusions around renormalization group for the $r<1/2$ interval using the large $N$ expansion method."],"url":"http://arxiv.org/abs/2403.17586v1","category":"cond-mat.str-el"}
{"created":"2024-03-26 10:47:57","title":"A new construction of modified equations for variational integrators","abstract":"The construction of modified equations is an important step in the backward error analysis of symplectic integrator for Hamiltonian systems. In the context of partial differential equations, the standard construction leads to modified equations with increasingly high frequencies which increase the regularity requirements on the analysis. In this paper, we consider the next order modified equations for the implicit midpoint rule applied to the semilinear wave equation to give a proof-of-concept of a new construction which works directly with the variational principle. We show that a carefully chosen change of coordinates yields a modified system which inherits its analytical properties from the original wave equation. Our method systematically exploits additional degrees of freedom by modifying the symplectic structure and the Hamiltonian together.","sentences":["The construction of modified equations is an important step in the backward error analysis of symplectic integrator for Hamiltonian systems.","In the context of partial differential equations, the standard construction leads to modified equations with increasingly high frequencies which increase the regularity requirements on the analysis.","In this paper, we consider the next order modified equations for the implicit midpoint rule applied to the semilinear wave equation to give a proof-of-concept of a new construction which works directly with the variational principle.","We show that a carefully chosen change of coordinates yields a modified system which inherits its analytical properties from the original wave equation.","Our method systematically exploits additional degrees of freedom by modifying the symplectic structure and the Hamiltonian together."],"url":"http://arxiv.org/abs/2403.17585v1","category":"math.NA"}
{"created":"2024-03-26 10:47:40","title":"Gravito-capillary pinning of pendant droplets under wet uneven surfaces","abstract":"Pendant drops spontaneously appear on the underside of wet surfaces through the Rayleigh-Taylor instability. These droplets have no contact line, they are connected to a thin liquid film with which they exchange liquid and are thus mobile: any perturbation will set them in motion. Here, using experiments, numerical simulations, and theory I show that pendant drops sliding under a slightly tilted wet substrate can pin on topographic defects, despite their lack of contact line. Instead, this pinning force has a gravito-capillary origin: liquid has to moves up or down and the interface has to deforms for the drop the pass the defect. I propose a semi-analytical model for arbitrary substrate topographies that matches the pinning force observed experimentally and numerically, without any fitting parameter. I finally demonstrate how to harness this pinning force to guide pendant drops on complex paths.","sentences":["Pendant drops spontaneously appear on the underside of wet surfaces through the Rayleigh-Taylor instability.","These droplets have no contact line, they are connected to a thin liquid film with which they exchange liquid and are thus mobile: any perturbation will set them in motion.","Here, using experiments, numerical simulations, and theory I show that pendant drops sliding under a slightly tilted wet substrate can pin on topographic defects, despite their lack of contact line.","Instead, this pinning force has a gravito-capillary origin: liquid has to moves up or down and the interface has to deforms for the drop the pass the defect.","I propose a semi-analytical model for arbitrary substrate topographies that matches the pinning force observed experimentally and numerically, without any fitting parameter.","I finally demonstrate how to harness this pinning force to guide pendant drops on complex paths."],"url":"http://arxiv.org/abs/2403.17584v1","category":"physics.flu-dyn"}
{"created":"2024-03-26 10:37:54","title":"Flat band fine-tuning and its photonic applications","abstract":"Flat bands - single-particle energy bands - in tight-binding networks have attracted attention due to the presence of macroscopic degeneracies and their extreme sensitivity to perturbations. This makes them natural candidates for emerging exotic phases and unconventional orders. The challenging part however is to construct flat band networks, whose existence relies on symmetries and fine-tuning. In this review we consider the recently proposed systematic ways to construct flat band networks based on symmetries or fine-tuning. We then discuss how the fine-tuning constructions can be further extended, adapted or exploited in presence of perturbations, both single-particle and many-body. This strategy has lead to the discovery of non-perturbative metal-insulator transitions, fractal phases, nonlinear and quantum caging and many-body nonergodic quantum models. We discuss what implications these results may have for the design of fine-tuned nanophotonic systems including photonic crystals, nanocavities, and metasurfaces.","sentences":["Flat bands - single-particle energy bands - in tight-binding networks have attracted attention due to the presence of macroscopic degeneracies and their extreme sensitivity to perturbations.","This makes them natural candidates for emerging exotic phases and unconventional orders.","The challenging part however is to construct flat band networks, whose existence relies on symmetries and fine-tuning.","In this review we consider the recently proposed systematic ways to construct flat band networks based on symmetries or fine-tuning.","We then discuss how the fine-tuning constructions can be further extended, adapted or exploited in presence of perturbations, both single-particle and many-body.","This strategy has lead to the discovery of non-perturbative metal-insulator transitions, fractal phases, nonlinear and quantum caging and many-body nonergodic quantum models.","We discuss what implications these results may have for the design of fine-tuned nanophotonic systems including photonic crystals, nanocavities, and metasurfaces."],"url":"http://arxiv.org/abs/2403.17578v1","category":"physics.optics"}
{"created":"2024-03-26 10:36:08","title":"Channel-Adaptive Pilot Design for FDD-MIMO Systems Utilizing Gaussian Mixture Models","abstract":"In this work, we propose to utilize Gaussian mixture models (GMMs) to design pilots for downlink (DL) channel estimation in frequency division duplex (FDD) systems. The GMM captures prior information during training that is leveraged to design a codebook of pilot matrices in an initial offline phase. Once shared with the mobile terminal (MT), the GMM is utilized to determine a feedback index at the MT in the online phase. This index selects a pilot matrix from a codebook, eliminating the need for online pilot optimization. The GMM is further used for DL channel estimation at the MT via observation-dependent linear minimum mean square error (LMMSE) filters, parametrized by the GMM. The analytic representation of the GMM allows adaptation to any signal-to-noise ratio (SNR) level and pilot configuration without re-training. With extensive simulations, we demonstrate the superior performance of the proposed GMM-based pilot scheme compared to state-of-the-art approaches.","sentences":["In this work, we propose to utilize Gaussian mixture models (GMMs) to design pilots for downlink (DL) channel estimation in frequency division duplex (FDD) systems.","The GMM captures prior information during training that is leveraged to design a codebook of pilot matrices in an initial offline phase.","Once shared with the mobile terminal (MT), the GMM is utilized to determine a feedback index at the MT in the online phase.","This index selects a pilot matrix from a codebook, eliminating the need for online pilot optimization.","The GMM is further used for DL channel estimation at the MT via observation-dependent linear minimum mean square error (LMMSE) filters, parametrized by the GMM.","The analytic representation of the GMM allows adaptation to any signal-to-noise ratio (SNR) level and pilot configuration without re-training.","With extensive simulations, we demonstrate the superior performance of the proposed GMM-based pilot scheme compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.17577v1","category":"eess.SP"}
{"created":"2024-03-26 10:30:59","title":"MR sequence design using digital twins of non-idealized hardware","abstract":"MRI systems are traditionally engineered to produce close to idealized performance, enabling a simplified pulse sequence design philosophy. An example of this is control of eddy currents produced by gradient fields; usually these are compensated by pre-emphasizing demanded waveforms. This process typically happens invisibly to the pulse designer, allowing them to assume the achieved gradient waveform will be as desired. Whilst convenient, this imposes stricter limits on the sequence design than the hardware can handle (for example, pre-emphasis adds an additional overhead to amplifiers). This strategy can be undesirable particularly for lower performance or resource-limited hardware. Instead we explore the use of a 'digital twin' (i.e. an end-to-end model of the scanner system) to optimize control inputs, resulting in sequences that inherently compensate for known imperfections. We explore digital twin optimization specifically for gradient system imperfections as an exemplar. This is first explored in simulations using a simple exponential eddy current model, then experimentally using an empirical gradient impulse response function on a 7T MRI system. When unconstrained, digital twin optimization reproduces classic pre-emphasis. When strict hardware constraints are imposed (simulating lower performance hardware), it identifies novel sequences for scenarios where classic pre-emphasis would be unachievable. Experimentally, the optimization approach was demonstrated to substantially reduce ghosting effects in echo planar images on a 7T system. Digital twin optimization may allow more efficient use of hardware by taking a whole system approach. Ultimately this could enable the use of cheaper hardware without loss of performance.","sentences":["MRI systems are traditionally engineered to produce close to idealized performance, enabling a simplified pulse sequence design philosophy.","An example of this is control of eddy currents produced by gradient fields; usually these are compensated by pre-emphasizing demanded waveforms.","This process typically happens invisibly to the pulse designer, allowing them to assume the achieved gradient waveform will be as desired.","Whilst convenient, this imposes stricter limits on the sequence design than the hardware can handle (for example, pre-emphasis adds an additional overhead to amplifiers).","This strategy can be undesirable particularly for lower performance or resource-limited hardware.","Instead we explore the use of a 'digital twin' (i.e. an end-to-end model of the scanner system) to optimize control inputs, resulting in sequences that inherently compensate for known imperfections.","We explore digital twin optimization specifically for gradient system imperfections as an exemplar.","This is first explored in simulations using a simple exponential eddy current model, then experimentally using an empirical gradient impulse response function on a 7T MRI system.","When unconstrained, digital twin optimization reproduces classic pre-emphasis.","When strict hardware constraints are imposed (simulating lower performance hardware), it identifies novel sequences for scenarios where classic pre-emphasis would be unachievable.","Experimentally, the optimization approach was demonstrated to substantially reduce ghosting effects in echo planar images on a 7T system.","Digital twin optimization may allow more efficient use of hardware by taking a whole system approach.","Ultimately this could enable the use of cheaper hardware without loss of performance."],"url":"http://arxiv.org/abs/2403.17575v1","category":"physics.med-ph"}
{"created":"2024-03-26 10:28:41","title":"SPES: Towards Optimizing Performance-Resource Trade-Off for Serverless Functions","abstract":"As an emerging cloud computing deployment paradigm, serverless computing is gaining traction due to its efficiency and ability to harness on-demand cloud resources. However, a significant hurdle remains in the form of the cold start problem, causing latency when launching new function instances from scratch. Existing solutions tend to use over-simplistic strategies for function pre-loading/unloading without full invocation pattern exploitation, rendering unsatisfactory optimization of the trade-off between cold start latency and resource waste. To bridge this gap, we propose SPES, the first differentiated scheduler for runtime cold start mitigation by optimizing serverless function provision. Our insight is that the common architecture of serverless systems prompts the con- centration of certain invocation patterns, leading to predictable invocation behaviors. This allows us to categorize functions and pre-load/unload proper function instances with finer-grained strategies based on accurate invocation prediction. Experiments demonstrate the success of SPES in optimizing serverless function provision on both sides: reducing the 75th-percentile cold start rates by 49.77% and the wasted memory time by 56.43%, compared to the state-of-the-art. By mitigating the cold start issue, SPES is a promising advancement in facilitating cloud services deployed on serverless architectures.","sentences":["As an emerging cloud computing deployment paradigm, serverless computing is gaining traction due to its efficiency and ability to harness on-demand cloud resources.","However, a significant hurdle remains in the form of the cold start problem, causing latency when launching new function instances from scratch.","Existing solutions tend to use over-simplistic strategies for function pre-loading/unloading without full invocation pattern exploitation, rendering unsatisfactory optimization of the trade-off between cold start latency and resource waste.","To bridge this gap, we propose SPES, the first differentiated scheduler for runtime cold start mitigation by optimizing serverless function provision.","Our insight is that the common architecture of serverless systems prompts the con- centration of certain invocation patterns, leading to predictable invocation behaviors.","This allows us to categorize functions and pre-load/unload proper function instances with finer-grained strategies based on accurate invocation prediction.","Experiments demonstrate the success of SPES in optimizing serverless function provision on both sides: reducing the 75th-percentile cold start rates by 49.77% and the wasted memory time by 56.43%, compared to the state-of-the-art.","By mitigating the cold start issue, SPES is a promising advancement in facilitating cloud services deployed on serverless architectures."],"url":"http://arxiv.org/abs/2403.17574v1","category":"cs.SE"}
{"created":"2024-03-26 10:21:07","title":"A dispersive study of final-state interactions in $K\\to\u03c0\u03c0\u03c0$ amplitudes","abstract":"A system of dispersive representations of the Omn\\`es-Khuri-Treiman-Sawyer-Wali type for the final-state interactions in the amplitudes of the $K\\to\\pi\\pi\\pi$ weak transitions is constructed, under the assumptions that CP and isospin symmetries are conserved. Both the $\\Delta I = 1/2$ and $\\Delta I = 3/2$ transition channels are considered. The set of single-variable functions involved in these representations is identified, and the polynomial ambiguities in their definitions are discussed. Numerical solutions for the system of coupled integral relations satisfied by these functions are provided, and the determination of the subtraction constants in terms of the experimental information on the amplitudes in the decay region is addressed.","sentences":["A system of dispersive representations of the Omn\\`es-Khuri-Treiman-Sawyer-Wali type for the final-state interactions in the amplitudes of the $K\\to\\pi\\pi\\pi$ weak transitions is constructed, under the assumptions that CP and isospin symmetries are conserved.","Both the $\\Delta I = 1/2$ and $\\Delta I = 3/2$ transition channels are considered.","The set of single-variable functions involved in these representations is identified, and the polynomial ambiguities in their definitions are discussed.","Numerical solutions for the system of coupled integral relations satisfied by these functions are provided, and the determination of the subtraction constants in terms of the experimental information on the amplitudes in the decay region is addressed."],"url":"http://arxiv.org/abs/2403.17570v1","category":"hep-ph"}
{"created":"2024-03-26 10:20:44","title":"Points defects produced by irradiation: influence of boundary conditions on their biased elimination","abstract":"Most of the sinks able to absorb the point defects created under irradiation exhibit a stronger elastic interaction with the interstitial defect I than with the vacancy defect V. This bias of their elastic interaction (EB) is supposed to automatically determine the concomitant absorption bias (AB) leading to an elimination flux of I always larger than the flux of V. We show in this contribution that the link between the EB and the AB is not as rigid as thought previously. On a model system we can produce situations where this rule is in trouble: a sink which is primarily designed to attract more I than V can however absorb more V than I if it is located in the vicinity of another competing sink with prevalent elastic properties. As a consequence, the imbalance between the I and V fluxes absorbed by a given sink cannot be deduced univocally from its elastic interaction with the two defects because it is heavily influenced by the boundary conditions which are adopted to make the quantitative evaluation.","sentences":["Most of the sinks able to absorb the point defects created under irradiation exhibit a stronger elastic interaction with the interstitial defect I than with the vacancy defect V. This bias of their elastic interaction (EB) is supposed to automatically determine the concomitant absorption bias (AB) leading to an elimination flux of I always larger than the flux of V. We show in this contribution that the link between the EB and the AB is not as rigid as thought previously.","On a model system we can produce situations where this rule is in trouble: a sink which is primarily designed to attract more I than V can however absorb more V than I if it is located in the vicinity of another competing sink with prevalent elastic properties.","As a consequence, the imbalance between the I and V fluxes absorbed by a given sink cannot be deduced univocally from its elastic interaction with the two defects because it is heavily influenced by the boundary conditions which are adopted to make the quantitative evaluation."],"url":"http://arxiv.org/abs/2403.17569v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 10:20:15","title":"Equality of magnetization and edge current for interacting lattice fermions at positive temperature","abstract":"We prove that the bulk magnetization is equal to the edge current in the thermodynamic limit for a large class of models of lattice fermions with finite-range interactions satisfying local indistinguishability of the Gibbs state, a condition known to hold for sufficiently high temperatures. Our result implies that edge currents in such systems are determined by bulk properties and are therefore stable against large perturbations near the boundaries. Moreover, the equality persists also after taking the derivative with respect to the chemical potential. We show that this form of bulk-edge correspondence is essentially a consequence of homogeneity in the bulk and locality of the Gibbs state. An important intermediate result is a new version of Bloch's theorem for two-dimensional systems, stating that persistent currents vanish in the bulk.","sentences":["We prove that the bulk magnetization is equal to the edge current in the thermodynamic limit for a large class of models of lattice fermions with finite-range interactions satisfying local indistinguishability of the Gibbs state, a condition known to hold for sufficiently high temperatures.","Our result implies that edge currents in such systems are determined by bulk properties and are therefore stable against large perturbations near the boundaries.","Moreover, the equality persists also after taking the derivative with respect to the chemical potential.","We show that this form of bulk-edge correspondence is essentially a consequence of homogeneity in the bulk and locality of the Gibbs state.","An important intermediate result is a new version of Bloch's theorem for two-dimensional systems, stating that persistent currents vanish in the bulk."],"url":"http://arxiv.org/abs/2403.17566v1","category":"math-ph"}
{"created":"2024-03-26 10:19:04","title":"Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control via Spectral Method Model","abstract":"In this work, we present a model-based optimal boundary control design for an aerial robotic system composed of a quadrotor carrying a flexible cable. The whole system is modeled by partial differential equations (PDEs) combined with boundary conditions described by ordinary differential equations (ODEs). The proper orthogonal decomposition (POD) method is adopted to project the original infinite-dimensional system on a subspace spanned by orthogonal basis functions. Based on the reduced order model, nonlinear model predictive control (NMPC) is implemented online to realize shape trajectory tracking of the flexible cable in an optimal predictive fashion. The proposed reduced modeling and optimal control paradigms are numerically verified against an accurate high-dimensional FDM-based model in different scenarios and the controller's superior performance is shown compared to an optimally tuned PID controller.","sentences":["In this work, we present a model-based optimal boundary control design for an aerial robotic system composed of a quadrotor carrying a flexible cable.","The whole system is modeled by partial differential equations (PDEs) combined with boundary conditions described by ordinary differential equations (ODEs).","The proper orthogonal decomposition (POD) method is adopted to project the original infinite-dimensional system on a subspace spanned by orthogonal basis functions.","Based on the reduced order model, nonlinear model predictive control (NMPC) is implemented online to realize shape trajectory tracking of the flexible cable in an optimal predictive fashion.","The proposed reduced modeling and optimal control paradigms are numerically verified against an accurate high-dimensional FDM-based model in different scenarios and the controller's superior performance is shown compared to an optimally tuned PID controller."],"url":"http://arxiv.org/abs/2403.17565v1","category":"cs.RO"}
{"created":"2024-03-26 10:11:47","title":"Higher order differential subordinations for certain starlike functions","abstract":"In this paper, we employ a novel second and third-order differential subordination technique to establish the sufficient conditions for functions to belong to the classes $\\mathcal{S}^*_s$ and $\\mathcal{S}^*_{\\rho}$, where $\\mathcal{S}^*_s$ is the set of all normalized analytic functions $f$ satisfying $ zf'(z)/f(z)\\prec 1+\\sin z$ and $\\mathcal{S}^*_{\\rho}$ is the set of all normalized analytic functions $f$ satisfying $ zf'(z)/f(z)\\prec 1+\\sinh^{-1} z$.","sentences":["In this paper, we employ a novel second and third-order differential subordination technique to establish the sufficient conditions for functions to belong to the classes $\\mathcal{S}^*_s$ and $\\mathcal{S}^*_{\\rho}$, where $\\mathcal{S}^*_s$ is the set of all normalized analytic functions $f$ satisfying $ zf'(z)/f(z)\\prec 1+\\sin z$ and $\\mathcal{S}^*_{\\rho}$ is the set of all normalized analytic functions $f$ satisfying $ zf'(z)/f(z)\\prec 1+\\sinh^{-1} z$."],"url":"http://arxiv.org/abs/2403.17563v1","category":"math.CV"}
{"created":"2024-03-26 10:03:36","title":"Particle approximation for a conditional McKean--Vlasov stochastic differential equation","abstract":"In this paper, we construct a type of interacting particle systems to approximate a class of stochastic different equations whose coefficients depend on the conditional probability distributions of the processes given partial observations. After proving the well-posedness and regularity of the particle systems, we establish a quantitative convergence result for the empirical measures of the particle systems in the Wasserstein space, as the number of particles increases. Moreover, we discuss an Euler--Maruyama scheme of the particle system and validate its strong convergence. A numerical experiment is conducted to illustrate our results.","sentences":["In this paper, we construct a type of interacting particle systems to approximate a class of stochastic different equations whose coefficients depend on the conditional probability distributions of the processes given partial observations.","After proving the well-posedness and regularity of the particle systems, we establish a quantitative convergence result for the empirical measures of the particle systems in the Wasserstein space, as the number of particles increases.","Moreover, we discuss an Euler--Maruyama scheme of the particle system and validate its strong convergence.","A numerical experiment is conducted to illustrate our results."],"url":"http://arxiv.org/abs/2403.17555v1","category":"math.PR"}
{"created":"2024-03-26 10:01:13","title":"Robust Stability for Multiagent Systems with Spatio-Temporally Correlated Packet Loss","abstract":"A problem with considering correlations in the analysis of multiagent system with stochastic packet loss is that they induce dependencies between agents that are otherwise decoupled, preventing the application of decomposition methods required for efficient evaluation. To circumvent that issue, this paper is proposing an approach based on analysing sets of networks with independent communication links, only considering the correlations in an implicit fashion. Combining ideas from the robust stabilization of Markov jump linear systems with recently proposed techniques for analysing packet loss in multiagent systems, we obtain a linear matrix inequality based stability condition which is independent of the number of agents. The main result is that the set of stabilized probability distributions has non-empty interior such that small correlations cannot lead to instability, even though only distributions of independent links were analysed. Moreover, two examples are provided to demonstrate the applicability of the results to practically relevant scenarios.","sentences":["A problem with considering correlations in the analysis of multiagent system with stochastic packet loss is that they induce dependencies between agents that are otherwise decoupled, preventing the application of decomposition methods required for efficient evaluation.","To circumvent that issue, this paper is proposing an approach based on analysing sets of networks with independent communication links, only considering the correlations in an implicit fashion.","Combining ideas from the robust stabilization of Markov jump linear systems with recently proposed techniques for analysing packet loss in multiagent systems, we obtain a linear matrix inequality based stability condition which is independent of the number of agents.","The main result is that the set of stabilized probability distributions has non-empty interior such that small correlations cannot lead to instability, even though only distributions of independent links were analysed.","Moreover, two examples are provided to demonstrate the applicability of the results to practically relevant scenarios."],"url":"http://arxiv.org/abs/2403.17554v1","category":"math.OC"}
{"created":"2024-03-26 09:58:27","title":"Time-Optimal Flight with Safety Constraints and Data-driven Dynamics","abstract":"Time-optimal quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling. Model Predictive Contouring Control (MPCC) has emerged as a leading model-based approach for time optimization problems such as drone racing. However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately. This paper introduces three key components that enhance the MPCC approach for drone racing. First and foremost, we provide safety guarantees in the form of a constraint and terminal set. The safety set is designed as a spatial constraint which prevents gate collisions while allowing for time-optimization only in the cost function. Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real world data. Third, we use Trust Region Bayesian Optimization (TuRBO), a state of the art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPC controller given a sparse reward based on lap time minimization. The proposed approach achieves similar lap times to the best state-of-the-art RL and outperforms the best time-optimal controller while satisfying constraints. In both simulation and real-world, our approach consistently prevents gate crashes with 100\\% success rate, while pushing the quadrotor to its physical limit reaching speeds of more than 80km/h.","sentences":["Time-optimal quadrotor flight is an extremely challenging problem due to the limited control authority encountered at the limit of handling.","Model Predictive Contouring Control (MPCC) has emerged as a leading model-based approach for time optimization problems such as drone racing.","However, the standard MPCC formulation used in quadrotor racing introduces the notion of the gates directly in the cost function, creating a multi-objective optimization that continuously trades off between maximizing progress and tracking the path accurately.","This paper introduces three key components that enhance the MPCC approach for drone racing.","First and foremost, we provide safety guarantees in the form of a constraint and terminal set.","The safety set is designed as a spatial constraint which prevents gate collisions while allowing for time-optimization only in the cost function.","Second, we augment the existing first principles dynamics with a residual term that captures complex aerodynamic effects and thrust forces learned directly from real world data.","Third, we use Trust Region Bayesian Optimization (TuRBO), a state of the art global Bayesian Optimization algorithm, to tune the hyperparameters of the MPC controller given a sparse reward based on lap time minimization.","The proposed approach achieves similar lap times to the best state-of-the-art RL and outperforms the best time-optimal controller while satisfying constraints.","In both simulation and real-world, our approach consistently prevents gate crashes with 100\\% success rate, while pushing the quadrotor to its physical limit reaching speeds of more than 80km/h."],"url":"http://arxiv.org/abs/2403.17551v1","category":"cs.RO"}
{"created":"2024-03-26 09:58:06","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","abstract":"Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach will be made publicly available.","sentences":["Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors.","Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes.","To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes.","However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements.","Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space.","To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping.","Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks.","The code of our approach will be made publicly available."],"url":"http://arxiv.org/abs/2403.17550v1","category":"cs.CV"}
{"created":"2024-03-26 09:54:14","title":"Mpemba effect on non-equilibrium active Markov chains","abstract":"We study the Mpemba effect on a non-equilibrium Markov chain that mimics the run and tumble motion of an active particle in a discrete energy landscape. The broken detailed balance, rendered by the activity, gives rise to a unique anomalous relaxation in the system which is distinctly different than the typical equilibrium systems. We observe that the activity can both suppress or induce the Mpemba effect. Furthermore, we report an oscillatory Mpemba effect where the relaxation trajectories, emanating from the hot and cold initial conditions, cross each other multiple times and this occurs due to the emergence of complex eigenvalues in the relaxation spectrum due to the activity. Our work reveals a possible pathway for studying the Mpemba effect in active living systems where broken detailed balance is crucial to achieve many biological functions.","sentences":["We study the Mpemba effect on a non-equilibrium Markov chain that mimics the run and tumble motion of an active particle in a discrete energy landscape.","The broken detailed balance, rendered by the activity, gives rise to a unique anomalous relaxation in the system which is distinctly different than the typical equilibrium systems.","We observe that the activity can both suppress or induce the Mpemba effect.","Furthermore, we report an oscillatory Mpemba effect where the relaxation trajectories, emanating from the hot and cold initial conditions, cross each other multiple times and this occurs due to the emergence of complex eigenvalues in the relaxation spectrum due to the activity.","Our work reveals a possible pathway for studying the Mpemba effect in active living systems where broken detailed balance is crucial to achieve many biological functions."],"url":"http://arxiv.org/abs/2403.17547v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-26 09:49:35","title":"A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions","abstract":"Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved.","sentences":["Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information.","This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms.","Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information.","In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information.","We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks.","Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved."],"url":"http://arxiv.org/abs/2403.17545v1","category":"cs.CL"}
{"created":"2024-03-26 09:47:21","title":"Different intermediate water cluster with distinct nucleation dynamics among mono layer ice nucleation","abstract":"Recent first-principle calculations unveiled a distinctive dynamic behavior in water molecule rotation during the melting process of highly confined water, indicating a notable time-scale separation in diffusion. In this short paper, we conducted molecular dynamics (MD) simulations to explore the rotation dynamics during the mono-layer ice nucleation process to investigate the possible intermediate states characterized by the differences in rotation of water molecules. Our study reveals two types of ice clusters with similar ice geometric structure but possess distinctly different rotational behaviors. In terms of molecular rotation, one type cluster is ice like (ILC) and can be regarded as small ice nuclei while the other is supercooled liquid water like (SCC). We found distinct nucleation pathways, thermodynamic properties, and phase transition dynamics to associate with these intermediate clusters, which yielded an unexpectedly complex picture of mono-layer ice nucleation.","sentences":["Recent first-principle calculations unveiled a distinctive dynamic behavior in water molecule rotation during the melting process of highly confined water, indicating a notable time-scale separation in diffusion.","In this short paper, we conducted molecular dynamics (MD) simulations to explore the rotation dynamics during the mono-layer ice nucleation process to investigate the possible intermediate states characterized by the differences in rotation of water molecules.","Our study reveals two types of ice clusters with similar ice geometric structure but possess distinctly different rotational behaviors.","In terms of molecular rotation, one type cluster is ice like (ILC) and can be regarded as small ice nuclei while the other is supercooled liquid water like (SCC).","We found distinct nucleation pathways, thermodynamic properties, and phase transition dynamics to associate with these intermediate clusters, which yielded an unexpectedly complex picture of mono-layer ice nucleation."],"url":"http://arxiv.org/abs/2403.17544v1","category":"physics.chem-ph"}
{"created":"2024-03-26 09:46:56","title":"Determination of nuclear matter radii by means of microscopic optical potentials: the case of $^{78}$Kr","abstract":"In this work we use microscopic Nucleon-Nucleus Optical Potentials (OP) to analyze elastic scattering data for the differential cross section of the $^{78}$Kr (p,p) $^{78}$Kr reaction, with the goal of extracting the matter radius and estimating the neutron skin, quantities that are both needed to determine the slope parameter $L$ of the nuclear symmetry energy. Our analysis is performed with the factorized version of the microscopic OP obtained in a previous series of papers within the Watson multiple scattering theory at the first order of the spectator expansion, which is based on the underlying nucleon-nucleon dynamics and is free from phenomenological inputs. Differently from our previous applications, the proton and neutron densities are described with a two-parameter Fermi (2pF) distribution, which makes the extraction of the matter radius easier and allows us to make a meaningful comparison with the original analysis, that was performed with the Glauber model. With standard minimization techniques we performed data analysis and extracted the matter radius and the neutron skin. Our analysis produces a matter radius of $R_m^{{\\rm (rms)}} = 4.12$ fm, in good agreement with previous matter radii extracted from $^{76}$Kr and $^{80}$Kr, and a neutron skin of $\\Delta R_{np} \\simeq - 0.1$ fm, compatible with a previous analysis. Our factorized microscopic OP, supplied with 2pF densities, is a valuable tool to perform the analysis of the experimental differential cross section and extract information such as matter radius and neutron skin. Without any free parameters it provides a reasonably good description of the experimental differential cross section for scattering angles up to $\\approx$ 40 degrees. Compared to the Glauber model our OP can be applied to a wider range of scattering angles and allows one to probe the nuclear systems in a more internal region.","sentences":["In this work we use microscopic Nucleon-Nucleus Optical Potentials (OP) to analyze elastic scattering data for the differential cross section of the $^{78}$Kr (p,p) $^{78}$Kr reaction, with the goal of extracting the matter radius and estimating the neutron skin, quantities that are both needed to determine the slope parameter $L$ of the nuclear symmetry energy.","Our analysis is performed with the factorized version of the microscopic OP obtained in a previous series of papers within the Watson multiple scattering theory at the first order of the spectator expansion, which is based on the underlying nucleon-nucleon dynamics and is free from phenomenological inputs.","Differently from our previous applications, the proton and neutron densities are described with a two-parameter Fermi (2pF) distribution, which makes the extraction of the matter radius easier and allows us to make a meaningful comparison with the original analysis, that was performed with the Glauber model.","With standard minimization techniques we performed data analysis and extracted the matter radius and the neutron skin.","Our analysis produces a matter radius of $R_m^{{\\rm (rms)}} = 4.12$ fm, in good agreement with previous matter radii extracted from $^{76}$Kr and $^{80}$Kr, and a neutron skin of $\\Delta R_{np} \\simeq - 0.1$ fm, compatible with a previous analysis.","Our factorized microscopic OP, supplied with 2pF densities, is a valuable tool to perform the analysis of the experimental differential cross section and extract information such as matter radius and neutron skin.","Without any free parameters it provides a reasonably good description of the experimental differential cross section for scattering angles up to $\\approx$ 40 degrees.","Compared to the Glauber model our OP can be applied to a wider range of scattering angles and allows one to probe the nuclear systems in a more internal region."],"url":"http://arxiv.org/abs/2403.17543v1","category":"nucl-th"}
{"created":"2024-03-26 09:30:16","title":"Minimum-Delay Opportunity Charging Scheduling for Electric Buses","abstract":"Transit agencies that operate battery-electric buses must carefully manage fast-charging infrastructure to extend daily bus range without degrading on-time performance. To support this need, we propose a mixed-integer linear programming model to schedule opportunity charging that minimizes the amount of departure delay in all trips served by electric buses. Our novel approach directly tracks queuing at chargers in order to set and propagate departure delays. Allowing but minimizing delays makes it possible to optimize performance when delays due to traffic conditions and charging needs are inevitable, in contrast with existing methods that require charging to occur during scheduled layover time. To solve the model, we develop two algorithms based on decomposition. The first is an exact solution method based on Combinatorial Benders (CB) decomposition, which avoids directly enumerating the model's logic-based \"big M\" constraints and their inevitable computational challenges. The second, inspired by the CB approach but more efficient, is a polynomial-time heuristic based on linear programming that we call 3S. Computational experiments on both a simple notional transit network and the real bus system of King County, Washington, USA demonstrate the performance of both methods. The 3S method appears particularly promising for creating good charging schedules quickly at real-world scale.","sentences":["Transit agencies that operate battery-electric buses must carefully manage fast-charging infrastructure to extend daily bus range without degrading on-time performance.","To support this need, we propose a mixed-integer linear programming model to schedule opportunity charging that minimizes the amount of departure delay in all trips served by electric buses.","Our novel approach directly tracks queuing at chargers in order to set and propagate departure delays.","Allowing but minimizing delays makes it possible to optimize performance when delays due to traffic conditions and charging needs are inevitable, in contrast with existing methods that require charging to occur during scheduled layover time.","To solve the model, we develop two algorithms based on decomposition.","The first is an exact solution method based on Combinatorial Benders (CB) decomposition, which avoids directly enumerating the model's logic-based \"big M\" constraints and their inevitable computational challenges.","The second, inspired by the CB approach but more efficient, is a polynomial-time heuristic based on linear programming that we call 3S. Computational experiments on both a simple notional transit network and the real bus system of King County, Washington, USA demonstrate the performance of both methods.","The 3S method appears particularly promising for creating good charging schedules quickly at real-world scale."],"url":"http://arxiv.org/abs/2403.17527v1","category":"math.OC"}
{"created":"2024-03-26 09:13:06","title":"Random-coupled Neural Network","abstract":"Improving the efficiency of current neural networks and modeling them in biological neural systems have become popular research directions in recent years. Pulse-coupled neural network (PCNN) is a well applicated model for imitating the computation characteristics of the human brain in computer vision and neural network fields. However, differences between the PCNN and biological neural systems remain: limited neural connection, high computational cost, and lack of stochastic property. In this study, random-coupled neural network (RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic computing via a random inactivation process. This process randomly closes some neural connections in the RCNN model, realized by the random inactivation weight matrix of link input. This releases the computational burden of PCNN, making it affordable to achieve vast neural connections. Furthermore, the image and video processing mechanisms of RCNN are researched. It encodes constant stimuli as periodic spike trains and periodic stimuli as chaotic spike trains, the same as biological neural information encoding characteristics. Finally, the RCNN is applicated to image segmentation, fusion, and pulse shape discrimination subtasks. It is demonstrated to be robust, efficient, and highly anti-noised, with outstanding performance in all applications mentioned above.","sentences":["Improving the efficiency of current neural networks and modeling them in biological neural systems have become popular research directions in recent years.","Pulse-coupled neural network (PCNN) is a well applicated model for imitating the computation characteristics of the human brain in computer vision and neural network fields.","However, differences between the PCNN and biological neural systems remain: limited neural connection, high computational cost, and lack of stochastic property.","In this study, random-coupled neural network (RCNN) is proposed.","It overcomes these difficulties in PCNN's neuromorphic computing via a random inactivation process.","This process randomly closes some neural connections in the RCNN model, realized by the random inactivation weight matrix of link input.","This releases the computational burden of PCNN, making it affordable to achieve vast neural connections.","Furthermore, the image and video processing mechanisms of RCNN are researched.","It encodes constant stimuli as periodic spike trains and periodic stimuli as chaotic spike trains, the same as biological neural information encoding characteristics.","Finally, the RCNN is applicated to image segmentation, fusion, and pulse shape discrimination subtasks.","It is demonstrated to be robust, efficient, and highly anti-noised, with outstanding performance in all applications mentioned above."],"url":"http://arxiv.org/abs/2403.17512v1","category":"cs.CV"}
{"created":"2024-03-26 09:05:16","title":"Computing conservative probabilities of rare events with surrogates","abstract":"This article provides a critical review of the main methods used to produce conservative estimators of probabilities of rare events, or critical failures, for reliability and certification studies in the broadest sense. These probabilities must theoretically be calculated from simulations of (certified) numerical models, but which typically suffer from prohibitive computational costs. This occurs frequently, for instance, for complex and critical industrial systems. We focus therefore in adapting the common use of surrogates to replace these numerical models, the aim being to offer a high level of confidence in the results. We suggest avenues of research to improve the guarantees currently reachable.","sentences":["This article provides a critical review of the main methods used to produce conservative estimators of probabilities of rare events, or critical failures, for reliability and certification studies in the broadest sense.","These probabilities must theoretically be calculated from simulations of (certified) numerical models, but which typically suffer from prohibitive computational costs.","This occurs frequently, for instance, for complex and critical industrial systems.","We focus therefore in adapting the common use of surrogates to replace these numerical models, the aim being to offer a high level of confidence in the results.","We suggest avenues of research to improve the guarantees currently reachable."],"url":"http://arxiv.org/abs/2403.17505v1","category":"math.ST"}
{"created":"2024-03-26 08:49:19","title":"Learning Equivalence Relations on Polish Spaces","abstract":"We investigate natural variations of behaviourally correct learning and explanatory learning -- two learning paradigms studied in algorithmic learning theory -- that allow us to ``learn'' equivalence relations on Polish spaces. We give a characterization of the learnable equivalence relations in terms of their Borel complexity and show that the behaviorally correct and explanatory learnable equivalence relations coincide both in uniform and non-uniform versions of learnability and provide a characterization of the learnable equivalence relations in terms of their Borel complexity. We also show that the set of uniformly learnable equivalence relations is $\\pmb\\Pi^1_1$-complete in the codes and study the learnability of several equivalence relations arising naturally in logic as a case study.","sentences":["We investigate natural variations of behaviourally correct learning and explanatory learning -- two learning paradigms studied in algorithmic learning theory -- that allow us to ``learn'' equivalence relations on Polish spaces.","We give a characterization of the learnable equivalence relations in terms of their Borel complexity and show that the behaviorally correct and explanatory learnable equivalence relations coincide both in uniform and non-uniform versions of learnability and provide a characterization of the learnable equivalence relations in terms of their Borel complexity.","We also show that the set of uniformly learnable equivalence relations is $\\pmb\\Pi^1_1$-complete in the codes and study the learnability of several equivalence relations arising naturally in logic as a case study."],"url":"http://arxiv.org/abs/2403.17493v1","category":"math.LO"}
{"created":"2024-03-26 08:30:20","title":"From Computing to Quantum Mechanics: Accessible and Hands-On Quantum Computing Education for High School Students","abstract":"This paper outlines an alternative approach to teaching quantum computing at the high school level, tailored for students with limited prior knowledge in advanced mathematics and physics. This approach diverges from traditional methods by building upon foundational concepts in classical computing before gradually introducing quantum mechanics, thereby simplifying the entry into this complex field. The course was initially implemented in a program for gifted high school students under the Hong Kong Education Bureau and received encouraging feedback, indicating its potential effectiveness for a broader student audience. A key element of this approach is the practical application through portable NMR quantum computers, which provides students with hands-on experience. The paper describes the structure of the course, including the organization of the lectures, the integration of the hardware of the portable nuclear magnetic resonance (NMR) quantum computers, the Gemini/Triangulum series, and detailed lecture notes in an appendix. The initial success in the specialized program and ongoing discussions to expand the course to regular high schools in Hong Kong and Shenzhen suggest the viability of this approach for wider educational application. By focusing on accessibility and student engagement, this approach presents a valuable perspective on introducing quantum computing concepts at the high school level, aiming to enhance student understanding and interest in the field.","sentences":["This paper outlines an alternative approach to teaching quantum computing at the high school level, tailored for students with limited prior knowledge in advanced mathematics and physics.","This approach diverges from traditional methods by building upon foundational concepts in classical computing before gradually introducing quantum mechanics, thereby simplifying the entry into this complex field.","The course was initially implemented in a program for gifted high school students under the Hong Kong Education Bureau and received encouraging feedback, indicating its potential effectiveness for a broader student audience.","A key element of this approach is the practical application through portable NMR quantum computers, which provides students with hands-on experience.","The paper describes the structure of the course, including the organization of the lectures, the integration of the hardware of the portable nuclear magnetic resonance (NMR) quantum computers, the Gemini/Triangulum series, and detailed lecture notes in an appendix.","The initial success in the specialized program and ongoing discussions to expand the course to regular high schools in Hong Kong and Shenzhen suggest the viability of this approach for wider educational application.","By focusing on accessibility and student engagement, this approach presents a valuable perspective on introducing quantum computing concepts at the high school level, aiming to enhance student understanding and interest in the field."],"url":"http://arxiv.org/abs/2403.17485v1","category":"physics.ed-ph"}
{"created":"2024-03-26 08:26:16","title":"Theory of tunneling spectroscopy in $p$-wave altermagnet-superconductor hybrid structures","abstract":"We theoretically study the tunneling conductance of a junction consisting of a two-dimensional $p$-wave altermagnet (AM) and a superconductor (SC) for various pairing states. The zero bias conductance peaks arising from the dispersionless surface Andreev bound states (SABSs) in $d_{xy}$-wave and $p_{x}$-wave superconductor junctions are robust against varying the altermagnetic spin-splitting strength $\\alpha _{y}$. Moreover, for chiral $p$- or chiral $d$-wave SCs, zero bias conductance shows a non-monotonic behavior as a function of $\\alpha_{y}$ indicating the existence of the dispersive SABSs. Our obtained results of tunneling spectroscopy based on a $p$-wave AM serve as an effective way for the identification of the pairing states of unconventional superconductors. It is noted that our used Hamiltonian of AM is also available for persistent spin helix systems.","sentences":["We theoretically study the tunneling conductance of a junction consisting of a two-dimensional $p$-wave altermagnet (AM) and a superconductor (SC) for various pairing states.","The zero bias conductance peaks arising from the dispersionless surface Andreev bound states (SABSs) in $d_{xy}$-wave and $p_{x}$-wave superconductor junctions are robust against varying the altermagnetic spin-splitting strength $\\alpha _{y}$.","Moreover, for chiral $p$- or chiral $d$-wave SCs, zero bias conductance shows a non-monotonic behavior as a function of $\\alpha_{y}$ indicating the existence of the dispersive SABSs.","Our obtained results of tunneling spectroscopy based on a $p$-wave AM serve as an effective way for the identification of the pairing states of unconventional superconductors.","It is noted that our used Hamiltonian of AM is also available for persistent spin helix systems."],"url":"http://arxiv.org/abs/2403.17482v1","category":"cond-mat.supr-con"}
{"created":"2024-03-26 08:23:37","title":"A Type of Nonlinear Fr\u00e9chet Regressions","abstract":"The existing Fr\\'echet regression is actually defined within a linear framework, since the weight function in the Fr\\'echet objective function is linearly defined, and the resulting Fr\\'echet regression function is identified to be a linear model when the random object belongs to a Hilbert space. Even for nonparametric and semiparametric Fr\\'echet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fr\\'echet regressions are (locally) linear as well. We in this paper introduce a type of nonlinear Fr\\'echet regressions. Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space. Particularly, its generalized linear form can return to the standard linear Fr\\'echet regression through a special choice of the weight function. Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable. The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fr\\'echet regressions are established systemically. The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors.","sentences":["The existing Fr\\'echet regression is actually defined within a linear framework, since the weight function in the Fr\\'echet objective function is linearly defined, and the resulting Fr\\'echet regression function is identified to be a linear model when the random object belongs to a Hilbert space.","Even for nonparametric and semiparametric Fr\\'echet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fr\\'echet regressions are (locally) linear as well.","We in this paper introduce a type of nonlinear Fr\\'echet regressions.","Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space.","Particularly, its generalized linear form can return to the standard linear Fr\\'echet regression through a special choice of the weight function.","Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable.","The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fr\\'echet regressions are established systemically.","The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors."],"url":"http://arxiv.org/abs/2403.17481v1","category":"stat.ME"}
{"created":"2024-03-26 08:09:14","title":"Complexity Equals (Almost) Anything","abstract":"Recent investigations [arXiv:2111.02429][arXiv:2210.09647][arXiv:2304.05453] have introduced an infinite class of novel gravitational observables in Asymptotically anti-de Sitter (AdS) space that reside on codimension-one or -zero regions of the bulk spacetime. These observables encompass well-established holographic complexity measures such as the maximum volume of the extremal hypersurfaces and the action or spacetime volume of the Wheeler-DeWitt (WDW) patch. Furthermore, this family of observables exhibits two universal properties when applied to the thermofield double (TFD) state: they exhibit linear growth at late times and faithfully reproduce the switchback effect. This implies that any observable from this class has the potential to serve as a gravitational dual for the circuit complexity of boundary states.","sentences":["Recent investigations [arXiv:2111.02429][arXiv:2210.09647][arXiv:2304.05453] have introduced an infinite class of novel gravitational observables in Asymptotically anti-de Sitter (AdS) space that reside on codimension-one or -zero regions of the bulk spacetime.","These observables encompass well-established holographic complexity measures such as the maximum volume of the extremal hypersurfaces and the action or spacetime volume of the Wheeler-DeWitt (WDW) patch.","Furthermore, this family of observables exhibits two universal properties when applied to the thermofield double (TFD) state: they exhibit linear growth at late times and faithfully reproduce the switchback effect.","This implies that any observable from this class has the potential to serve as a gravitational dual for the circuit complexity of boundary states."],"url":"http://arxiv.org/abs/2403.17475v1","category":"hep-th"}
{"created":"2024-03-26 07:59:31","title":"Long run convergence of discrete-time interacting particle systems of the McKean-Vlasov type","abstract":"We consider a discrete time system of n coupled random vectors, a.k.a. interacting particles. The dynamics involves a vanishing step size, some random centered perturbations, and a mean vector field which induces the coupling between the particles. We study the doubly asymptotic regime where both the number of iterations and the number n of particles tend to infinity, without any constraint on the relative rates of convergence of these two parameters. We establish that the empirical measure of the interpolated trajectories of the particles converges in probability, in an ergodic sense, to the set of recurrent Mc-Kean-Vlasov distributions. A first application example is the granular media equation, where the particles are shown to converge to a critical point of the Helmholtz energy. A second example is the convergence of stochastic gradient descent to the global minimizer of the risk, in a wide two-layer neural networks using random features.","sentences":["We consider a discrete time system of n coupled random vectors, a.k.a. interacting particles.","The dynamics involves a vanishing step size, some random centered perturbations, and a mean vector field which induces the coupling between the particles.","We study the doubly asymptotic regime where both the number of iterations and the number n of particles tend to infinity, without any constraint on the relative rates of convergence of these two parameters.","We establish that the empirical measure of the interpolated trajectories of the particles converges in probability, in an ergodic sense, to the set of recurrent Mc-Kean-Vlasov distributions.","A first application example is the granular media equation, where the particles are shown to converge to a critical point of the Helmholtz energy.","A second example is the convergence of stochastic gradient descent to the global minimizer of the risk, in a wide two-layer neural networks using random features."],"url":"http://arxiv.org/abs/2403.17472v1","category":"math.PR"}
{"created":"2024-03-26 07:57:56","title":"Investigations on Physics-Informed Neural Networks for Aerodynamics","abstract":"Physics-Informed Neural Networks (PINNs) have recently emerged as a novel approach to simulate complex physical systems on the basis of both data observations and physical models. In this work, we investigate the use of PINNs for various applications in aerodynamics and we explain how to leverage their specific formulation to perform some tasks effectively. In particular, we demonstrate the ability of PINNs to construct parametric surrogate models, to achieve multiphysic couplings and to infer turbulence characteristics via data assimilation. The robustness and accuracy of the PINNs approach are analysed, then current issues and challenges are discussed.","sentences":["Physics-Informed Neural Networks (PINNs) have recently emerged as a novel approach to simulate complex physical systems on the basis of both data observations and physical models.","In this work, we investigate the use of PINNs for various applications in aerodynamics and we explain how to leverage their specific formulation to perform some tasks effectively.","In particular, we demonstrate the ability of PINNs to construct parametric surrogate models, to achieve multiphysic couplings and to infer turbulence characteristics via data assimilation.","The robustness and accuracy of the PINNs approach are analysed, then current issues and challenges are discussed."],"url":"http://arxiv.org/abs/2403.17470v1","category":"math.AP"}
{"created":"2024-03-26 07:55:40","title":"Green HPC: An analysis of the domain based on Top500","abstract":"The demand in computing power has never stopped growing over the years. Today, the performance of the most powerful systems exceeds the exascale and the number of petascale systems continues to grow. Unfortunately, this growth also goes hand in hand with ever-increasing energy costs, which in turn means a significant carbon footprint. In view of the environmental crisis, this paper intents to look at the often hidden issue of energy consumption of HPC systems. As it is not easy to access the data of the constructors, we then consider the Top500 as the tip of the iceberg to identify the trends of the whole domain.The objective of this work is to analyze Top500 and Green500 data from several perspectives in order to identify the dynamic of the domain regarding its environmental impact. The contributions are to take stock of the empirical laws governing the evolution of HPC computing systems both from the performance and energy perspectives, to analyze the most relevant data for developing the performance and energy efficiency of large-scale computing systems, to put these analyses into perspective with effects and impacts (lifespan of the HPC systems) and finally to derive a predictive model for the weight of HPC sector within the horizon 2030.","sentences":["The demand in computing power has never stopped growing over the years.","Today, the performance of the most powerful systems exceeds the exascale and the number of petascale systems continues to grow.","Unfortunately, this growth also goes hand in hand with ever-increasing energy costs, which in turn means a significant carbon footprint.","In view of the environmental crisis, this paper intents to look at the often hidden issue of energy consumption of HPC systems.","As it is not easy to access the data of the constructors, we then consider the Top500 as the tip of the iceberg to identify the trends of the whole domain.","The objective of this work is to analyze Top500 and Green500 data from several perspectives in order to identify the dynamic of the domain regarding its environmental impact.","The contributions are to take stock of the empirical laws governing the evolution of HPC computing systems both from the performance and energy perspectives, to analyze the most relevant data for developing the performance and energy efficiency of large-scale computing systems, to put these analyses into perspective with effects and impacts (lifespan of the HPC systems) and finally to derive a predictive model for the weight of HPC sector within the horizon 2030."],"url":"http://arxiv.org/abs/2403.17466v1","category":"cs.CY"}
{"created":"2024-03-26 07:46:27","title":"Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice","abstract":"Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.","sentences":["Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements.","Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset.","For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset.","So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one.","We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection."],"url":"http://arxiv.org/abs/2403.17458v1","category":"cs.CR"}
{"created":"2024-03-26 07:31:21","title":"Global regularity for a physically nonlinear version of the relaxed micromorphic model on Lipschitz domains","abstract":"In this paper, we investigate the global higher regularity properties of weak solutions for a linear elliptic system coupled with a nonlinear Maxwell-type system defined on Lipschitz domains. The regularity result is established using a modified finite difference approach. These adjusted finite differences involve inner variations in conjunction with a Piola-type transformation to preserve the curl-structure within the matrix Maxwell system. The proposed method is further applied to the linear relaxed micromorphic model.   As a result, for a physically nonlinear version of the relaxed micromorphic model, we demonstrate that for arbitrary $\\epsilon > 0$, the displacement vector $u$ belongs to $H^{\\frac{3}{2}-\\epsilon}(\\Omega)$, and the microdistortion tensor $P$ belongs to $H^{\\frac{1}{2}-\\epsilon}(\\Omega)$ while $\\Curl P$ belongs to $H^{\\frac{1}{2}-\\epsilon}(\\Omega)$.","sentences":["In this paper, we investigate the global higher regularity properties of weak solutions for a linear elliptic system coupled with a nonlinear Maxwell-type system defined on Lipschitz domains.","The regularity result is established using a modified finite difference approach.","These adjusted finite differences involve inner variations in conjunction with a Piola-type transformation to preserve the curl-structure within the matrix Maxwell system.","The proposed method is further applied to the linear relaxed micromorphic model.   ","As a result, for a physically nonlinear version of the relaxed micromorphic model, we demonstrate that for arbitrary $\\epsilon > 0$, the displacement vector $u$ belongs to $H^{\\frac{3}{2}-\\epsilon}(\\Omega)$, and the microdistortion tensor $P$ belongs to $H^{\\frac{1}{2}-\\epsilon}(\\Omega)$ while $\\Curl P$ belongs to $H^{\\frac{1}{2}-\\epsilon}(\\Omega)$."],"url":"http://arxiv.org/abs/2403.17451v1","category":"math.AP"}
{"created":"2024-03-26 07:26:00","title":"Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks","abstract":"Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, model compression has become an important research focus. Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classification networks across different data sets, our proposed Chain of Compression can significantly compress the computation cost by 100-1000 times with ignorable accuracy loss compared with the baseline model.","sentences":["Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance.","To release this burden, model compression has become an important research focus.","Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks.","Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined.","To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network.","Validated on the image-based regression and classification networks across different data sets, our proposed Chain of Compression can significantly compress the computation cost by 100-1000 times with ignorable accuracy loss compared with the baseline model."],"url":"http://arxiv.org/abs/2403.17447v1","category":"cs.LG"}
{"created":"2024-03-26 07:24:28","title":"Cost-benefit analysis of ecosystem modelling to support fisheries management","abstract":"Mathematical and statistical models underlie many of the world's most important fisheries management decisions. Since the 19th century, difficulty calibrating and fitting such models has been used to justify the selection of simple, stationary, single-species models to aid tactical fisheries management decisions. Whereas these justifications are reasonable, it is imperative that we quantify the value of different levels of model complexity for supporting fisheries management, especially given a changing climate, where old methodologies may no longer perform as well as in the past. Here we argue that cost-benefit analysis is an ideal lens to assess the value of model complexity in fisheries management. While some studies have reported the benefits of model complexity in fisheries, modeling costs are rarely considered. In the absence of cost data in the literature, we report, as a starting point, relative costs of single-species stock assessment and marine ecosystem models from two Australian organizations. We found that costs varied by two orders of magnitude, and that ecosystem model costs increased with model complexity. Using these costs, we walk through a hypothetical example of cost-benefit analysis. The demonstration is intended to catalyze the reporting of modeling costs and benefits.","sentences":["Mathematical and statistical models underlie many of the world's most important fisheries management decisions.","Since the 19th century, difficulty calibrating and fitting such models has been used to justify the selection of simple, stationary, single-species models to aid tactical fisheries management decisions.","Whereas these justifications are reasonable, it is imperative that we quantify the value of different levels of model complexity for supporting fisheries management, especially given a changing climate, where old methodologies may no longer perform as well as in the past.","Here we argue that cost-benefit analysis is an ideal lens to assess the value of model complexity in fisheries management.","While some studies have reported the benefits of model complexity in fisheries, modeling costs are rarely considered.","In the absence of cost data in the literature, we report, as a starting point, relative costs of single-species stock assessment and marine ecosystem models from two Australian organizations.","We found that costs varied by two orders of magnitude, and that ecosystem model costs increased with model complexity.","Using these costs, we walk through a hypothetical example of cost-benefit analysis.","The demonstration is intended to catalyze the reporting of modeling costs and benefits."],"url":"http://arxiv.org/abs/2403.17446v1","category":"q-bio.PE"}
{"created":"2024-03-26 07:19:26","title":"Touch the Core: Exploring Task Dependence Among Hybrid Targets for Recommendation","abstract":"As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms. These core conversions are usually continuous targets, such as \\textit{watch time}, \\textit{revenue}, and so on, whose predictions can be enhanced by previous discrete conversion actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets. However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion. Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks. In this paper, we study the MTL problem with hybrid targets for the first time and propose the model named Hybrid Targets Learning Network (HTLNet) to explore task dependence and enhance optimization. Specifically, we introduce label embedding for each task to explicitly transfer the label information among these tasks, which can effectively explore logical task dependence. We also further design the gradient adjustment regime between the final regression task and other classification tasks to enhance the optimization. Extensive experiments on two offline public datasets and one real-world industrial dataset are conducted to validate the effectiveness of HTLNet. Moreover, online A/B tests on the financial recommender system also show our model has superior improvement.","sentences":["As user behaviors become complicated on business platforms, online recommendations focus more on how to touch the core conversions, which are highly related to the interests of platforms.","These core conversions are usually continuous targets, such as \\textit{watch time}, \\textit{revenue}, and so on, whose predictions can be enhanced by previous discrete conversion actions.","Therefore, multi-task learning (MTL) can be adopted as the paradigm to learn these hybrid targets.","However, existing works mainly emphasize investigating the sequential dependence among discrete conversion actions, which neglects the complexity of dependence between discrete conversions and the final continuous conversion.","Moreover, simultaneously optimizing hybrid tasks with stronger task dependence will suffer from volatile issues where the core regression task might have a larger influence on other tasks.","In this paper, we study the MTL problem with hybrid targets for the first time and propose the model named Hybrid Targets Learning Network (HTLNet) to explore task dependence and enhance optimization.","Specifically, we introduce label embedding for each task to explicitly transfer the label information among these tasks, which can effectively explore logical task dependence.","We also further design the gradient adjustment regime between the final regression task and other classification tasks to enhance the optimization.","Extensive experiments on two offline public datasets and one real-world industrial dataset are conducted to validate the effectiveness of HTLNet.","Moreover, online A/B tests on the financial recommender system also show our model has superior improvement."],"url":"http://arxiv.org/abs/2403.17442v1","category":"cs.IR"}
{"created":"2024-03-26 07:13:04","title":"On a classification of axiom A diffeomorphisms with codimension one basic sets and isolated saddles","abstract":"Let $M^n$, $n\\geq 3$, be a closed orientable $n$-manifold and $\\mathbb{D}_k(M^n;a,b,c)$ the set of axiom A diffeomorp\\-hisms $f: M^n\\to M^n$ satisfying the following conditions: (1) $f$ has $k\\geq 1$ nontrivial basic sets each is either an orientable codimension one expanding attractor or an orientable codimension one contracting repeller, and other trivial basic sets which are $a$ sinks, $b$ sources, $c$ saddles; (2) the invariant manifolds of isolated saddles are intersected transversally. We classify the diffeomorphisms from $\\mathbb{D}_k(M^n;a,b,c)$ up to the global conjugacy on non-wandering sets for the following subsets $\\mathbb{S}_k(M^n;a,b,c), \\mathbb{P}_k(M^n;0,0,1), \\mathbb{M}_k(M^n;0,0,1)$ of $\\mathbb{D}_k(M^n;a,b,c)$ where $\\mathbb{S}_k(M^n;a,b,c)$ satisfies to the following conditions:   ($1_{\\mathbb{S}}$) every nontrivial basic set of any $f\\in\\mathbb{S}_k(M^n;a,b,c)$ is uniquely bunched, and there is at least one nontrivial attractor and at least one nontrivial repeller, i.e. $k\\geq 2$;   ($2_{\\mathbb{S}}$) $c\\geq 1$ and all isolated saddles have the same Morse index belonging to $\\{1,n-1\\}$.   The subset $\\mathbb{P}_k(M^n;0,0,1)\\subset\\mathbb{D}_k(M^n;0,0,1)$ satisfies to the following conditions:   ($1_{\\mathbb{P}}$) any boundary point of $f\\in\\mathbb{P}_k(M^n;0,0,1)$ is fixed;   ($2_{\\mathbb{P}}$) a unique isolated saddle has Morse index different from $\\{1,n-1\\}$.   The subset $\\mathbb{M}_k(M^n;0,0,1)\\subset\\mathbb{D}_k(M^n;0,0,1)$ satisfies to the following conditions:   ($1_{\\mathbb{M}}$) any boundary point of $f\\in\\mathbb{M}_k(M^n;0,0,1)$ is fixed;   ($2_{\\mathbb{M}}$) a unique isolated saddle has Morse index belonging to $\\{1,n-1\\}$.   The classification is based on a description of topological structure of supporting manifolds $M^n$.","sentences":["Let $M^n$, $n\\geq 3$, be a closed orientable $n$-manifold and $\\mathbb{D}_k(M^n;a,b,c)$ the set of axiom A diffeomorp\\-hisms $f: M^n\\to M^n$ satisfying the following conditions: (1) $f$ has $k\\geq 1$ nontrivial basic sets each is either an orientable codimension one expanding attractor or an orientable codimension one contracting repeller, and other trivial basic sets which are $a$ sinks, $b$ sources, $c$ saddles; (2) the invariant manifolds of isolated saddles are intersected transversally.","We classify the diffeomorphisms from $\\mathbb{D}_k(M^n;a,b,c)$ up to the global conjugacy on non-wandering sets for the following subsets $\\mathbb{S}_k(M^n;a,b,c), \\mathbb{P}_k(M^n;0,0,1), \\mathbb{M}_k(M^n;0,0,1)$ of $\\mathbb{D}_k(M^n;a,b,c)$ where $\\mathbb{S}_k(M^n;a,b,c)$ satisfies to the following conditions:   ($1_{\\mathbb{S}}$) every nontrivial basic set of any $f\\in\\mathbb{S}_k(M^n;a,b,c)$ is uniquely bunched, and there is at least one nontrivial attractor and at least one nontrivial repeller, i.e. $k\\geq 2$;   ($2_{\\mathbb{S}}$) $c\\geq 1$ and all isolated saddles have the same Morse index belonging to $\\{1,n-1\\}$.   The subset $\\mathbb{P}_k(M^n;0,0,1)\\subset\\mathbb{D}_k(M^n;0,0,1)$ satisfies to the following conditions:   ($1_{\\mathbb{P}}$) any boundary point of $f\\in\\mathbb{P}_k(M^n;0,0,1)$ is fixed;   ($2_{\\mathbb{P}}$) a unique isolated saddle has Morse index different from $\\{1,n-1\\}$.   The subset $\\mathbb{M}_k(M^n;0,0,1)\\subset\\mathbb{D}_k(M^n;0,0,1)$ satisfies to the following conditions:   ($1_{\\mathbb{M}}$) any boundary point of $f\\in\\mathbb{M}_k(M^n;0,0,1)$ is fixed;   ($2_{\\mathbb{M}}$) a unique isolated saddle has Morse index belonging to $\\{1,n-1\\}$.   The classification is based on a description of topological structure of supporting manifolds $M^n$."],"url":"http://arxiv.org/abs/2403.17439v1","category":"math.DS"}
{"created":"2024-03-26 06:59:49","title":"Numerical analysis of a FE/SAV scheme for a Caginalp phase field model with mechanical effects in stereolithography","abstract":"In this work we propose a phase field model based on a Caginalp system with mechanical effects to study the underlying physical and chemical processes behind stereolithography, which is an additive manufacturing (3D printing) technique that builds objects in a layer-by-layer fashion by using an ultraviolet laser to solidify liquid polymer resins. Existence of weak solutions is established by demonstrating the convergence of a numerical scheme based on a first order scalar auxiliary variable temporal discretization and a finite element spatial discretization. We further establish uniqueness and regularity of solutions, as well as optimal error estimates for the Caginalp system that are supported by numerical simulations. We also present some qualitative two-dimensional simulations of the stereolithography processes captured by the model.","sentences":["In this work we propose a phase field model based on a Caginalp system with mechanical effects to study the underlying physical and chemical processes behind stereolithography, which is an additive manufacturing (3D printing) technique that builds objects in a layer-by-layer fashion by using an ultraviolet laser to solidify liquid polymer resins.","Existence of weak solutions is established by demonstrating the convergence of a numerical scheme based on a first order scalar auxiliary variable temporal discretization and a finite element spatial discretization.","We further establish uniqueness and regularity of solutions, as well as optimal error estimates for the Caginalp system that are supported by numerical simulations.","We also present some qualitative two-dimensional simulations of the stereolithography processes captured by the model."],"url":"http://arxiv.org/abs/2403.17434v1","category":"math.NA"}
{"created":"2024-03-26 06:57:50","title":"Integrating Mamba Sequence Model and Hierarchical Upsampling Network for Accurate Semantic Segmentation of Multiple Sclerosis Legion","abstract":"Integrating components from convolutional neural networks and state space models in medical image segmentation presents a compelling approach to enhance accuracy and efficiency. We introduce Mamba HUNet, a novel architecture tailored for robust and efficient segmentation tasks. Leveraging strengths from Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet), Mamba HUNet combines convolutional neural networks local feature extraction power with state space models long range dependency modeling capabilities. We first converted HUNet into a lighter version, maintaining performance parity and then integrated this lighter HUNet into Mamba HUNet, further enhancing its efficiency. The architecture partitions input grayscale images into patches, transforming them into 1D sequences for processing efficiency akin to Vision Transformers and Mamba models. Through Visual State Space blocks and patch merging layers, hierarchical features are extracted while preserving spatial information. Experimental results on publicly available Magnetic Resonance Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate Mamba HUNet's effectiveness across diverse segmentation tasks. The model's robustness and flexibility underscore its potential in handling complex anatomical structures. These findings establish Mamba HUNet as a promising solution in advancing medical image segmentation, with implications for improving clinical decision making processes.","sentences":["Integrating components from convolutional neural networks and state space models in medical image segmentation presents a compelling approach to enhance accuracy and efficiency.","We introduce Mamba HUNet, a novel architecture tailored for robust and efficient segmentation tasks.","Leveraging strengths from Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet), Mamba HUNet combines convolutional neural networks local feature extraction power with state space models long range dependency modeling capabilities.","We first converted HUNet into a lighter version, maintaining performance parity and then integrated this lighter HUNet into Mamba HUNet, further enhancing its efficiency.","The architecture partitions input grayscale images into patches, transforming them into 1D sequences for processing efficiency akin to Vision Transformers and Mamba models.","Through Visual State Space blocks and patch merging layers, hierarchical features are extracted while preserving spatial information.","Experimental results on publicly available Magnetic Resonance Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate Mamba HUNet's effectiveness across diverse segmentation tasks.","The model's robustness and flexibility underscore its potential in handling complex anatomical structures.","These findings establish Mamba HUNet as a promising solution in advancing medical image segmentation, with implications for improving clinical decision making processes."],"url":"http://arxiv.org/abs/2403.17432v1","category":"eess.IV"}
{"created":"2024-03-26 06:55:13","title":"Java Classes with \"-Er\" and \"-Utils\" Suffixes Have Higher Complexity","abstract":"In object-oriented programming languages, a belief exists that classes with -Er/-Or and -Utils suffixes are \"code smells\" because they take over a lot of functional responsibility, turning out to be bulky and complicated, and therefore making it more difficult to maintain the code. In order to validate this intuition, we analyzed complexity and cohesion of 13,861 Java classes from 212 unique open-source GitHub repositories. We found out that average values of Cyclomatic Complexity and Cognitive Complexity metrics are at least 2.5 times higher when suffixes are present.","sentences":["In object-oriented programming languages, a belief exists that classes with -Er/-Or and -Utils suffixes are \"code smells\" because they take over a lot of functional responsibility, turning out to be bulky and complicated, and therefore making it more difficult to maintain the code.","In order to validate this intuition, we analyzed complexity and cohesion of 13,861 Java classes from 212 unique open-source GitHub repositories.","We found out that average values of Cyclomatic Complexity and Cognitive Complexity metrics are at least 2.5 times higher when suffixes are present."],"url":"http://arxiv.org/abs/2403.17430v1","category":"cs.PL"}
{"created":"2024-03-26 06:42:23","title":"Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model","abstract":"In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario included as two features is not accurate enough. In this paper, we propose the Masked Multi-domain Network (MMN) to solve this problem. To achieve the accuracy requirement, we model domain-specific parameters and propose a dynamically weighted loss to account for the loss scale imbalance issue within each mini-batch. To achieve the scalability requirement, we propose a parameter sharing and composition strategy to reduce model parameters from a product space to a sum space. To achieve the convenience requirement, we propose an auto-masking strategy which can take mixed data from all the domains as input. It avoids the overhead caused by data partitioning, individual processing and separate storage. Both offline and online experimental results validate the superiority of MMN for multi-type and multi-scenario CVR prediction. MMN is now the serving model for real-time CVR prediction in UC Toutiao.","sentences":["In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR).","This results in the multi-type and multi-scenario CVR prediction problem.","A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario.","2) Scalability: the model parameter size should be affordable.","3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage.","Existing approaches cannot simultaneously satisfy these requirements.","For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient.","Building a unified model trained on all the data with conversion type and display scenario included as two features is not accurate enough.","In this paper, we propose the Masked Multi-domain Network (MMN) to solve this problem.","To achieve the accuracy requirement, we model domain-specific parameters and propose a dynamically weighted loss to account for the loss scale imbalance issue within each mini-batch.","To achieve the scalability requirement, we propose a parameter sharing and composition strategy to reduce model parameters from a product space to a sum space.","To achieve the convenience requirement, we propose an auto-masking strategy which can take mixed data from all the domains as input.","It avoids the overhead caused by data partitioning, individual processing and separate storage.","Both offline and online experimental results validate the superiority of MMN for multi-type and multi-scenario CVR prediction.","MMN is now the serving model for real-time CVR prediction in UC Toutiao."],"url":"http://arxiv.org/abs/2403.17425v1","category":"cs.IR"}
{"created":"2024-03-26 06:42:09","title":"Coupling-Constant Averaged Exchange-Correlation Hole for He, Li, Be, N, Ne Atoms from CCSD","abstract":"Accurate approximation of the exchange-correlation (XC) energy in density functional theory (DFT) calculations is essential for reliably modelling electronic systems. Many such approximations are developed from models of the XC hole; accurate reference XC holes for real electronic systems are crucial for evaluating the accuracy of these models however the availability of reliable reference data is limited to a few systems. In this study, we employ the Lieb optimization with a coupled cluster singles and doubles (CCSD) reference to construct accurate coupling-constant averaged XC holes, resolved into individual exchange and correlation components, for five spherically symmetric atoms: He, Li, Be, N, and Ne. Alongside providing a new set of reference data for the construction and evaluation of model XC holes, we compare our data against the exchange and correlation hole models of the established LDA and PBE density functional approximations. Our analysis confirms the established rationalization for the limitations of LDA and the improvement observed with PBE in terms of the hole depth and its long-range decay, demonstrated in real-space for the series of spherically-symmetric atoms.","sentences":["Accurate approximation of the exchange-correlation (XC) energy in density functional theory (DFT) calculations is essential for reliably modelling electronic systems.","Many such approximations are developed from models of the XC hole; accurate reference XC holes for real electronic systems are crucial for evaluating the accuracy of these models however the availability of reliable reference data is limited to a few systems.","In this study, we employ the Lieb optimization with a coupled cluster singles and doubles (CCSD) reference to construct accurate coupling-constant averaged XC holes, resolved into individual exchange and correlation components, for five spherically symmetric atoms: He, Li, Be, N, and Ne.","Alongside providing a new set of reference data for the construction and evaluation of model XC holes, we compare our data against the exchange and correlation hole models of the established LDA and PBE density functional approximations.","Our analysis confirms the established rationalization for the limitations of LDA and the improvement observed with PBE in terms of the hole depth and its long-range decay, demonstrated in real-space for the series of spherically-symmetric atoms."],"url":"http://arxiv.org/abs/2403.17424v1","category":"physics.chem-ph"}
{"created":"2024-03-26 06:35:55","title":"InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion","abstract":"We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy.","sentences":["We present InterHandGen, a novel framework that learns the generative prior of two-hand interaction.","Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object.","Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup.","Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature.","Thus, we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution.","In particular, we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout.","For sampling, we combine anti-penetration and classifier-free guidance to enable plausible generation.","Furthermore, we establish the rigorous evaluation protocol of two-hand synthesis, where our method significantly outperforms baseline generative models in terms of plausibility and diversity.","We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images, achieving new state-of-the-art accuracy."],"url":"http://arxiv.org/abs/2403.17422v1","category":"cs.CV"}
{"created":"2024-03-26 06:14:19","title":"AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering for Recommendations","abstract":"Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms. However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations. While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance. Up to now, the over-correlation issue remains unexplored in RS. Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge. To this end, this paper aims to address the aforementioned gap by undertaking a comprehensive study of the over-correlation issue in graph collaborative filtering models. Firstly, we present empirical evidence to demonstrate the widespread prevalence of over-correlation in these models. Subsequently, we dive into a theoretical analysis which establishes a pivotal connection between the over-correlation and over-smoothing issues. Leveraging these insights, we introduce the Adaptive Feature De-correlation Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies correlation penalties to the feature dimensions of the representation matrix, effectively alleviating both over-correlation and over-smoothing issues. The efficacy of the proposed framework is corroborated through extensive experiments conducted with four representative graph collaborative filtering models across four publicly available datasets.","sentences":["Collaborative filtering methods based on graph neural networks (GNNs) have witnessed significant success in recommender systems (RS), capitalizing on their ability to capture collaborative signals within intricate user-item relationships via message-passing mechanisms.","However, these GNN-based RS inadvertently introduce excess linear correlation between user and item embeddings, contradicting the goal of providing personalized recommendations.","While existing research predominantly ascribes this flaw to the over-smoothing problem, this paper underscores the critical, often overlooked role of the over-correlation issue in diminishing the effectiveness of GNN representations and subsequent recommendation performance.","Up to now, the over-correlation issue remains unexplored in RS.","Meanwhile, how to mitigate the impact of over-correlation while preserving collaborative filtering signals is a significant challenge.","To this end, this paper aims to address the aforementioned gap by undertaking a comprehensive study of the over-correlation issue in graph collaborative filtering models.","Firstly, we present empirical evidence to demonstrate the widespread prevalence of over-correlation in these models.","Subsequently, we dive into a theoretical analysis which establishes a pivotal connection between the over-correlation and over-smoothing issues.","Leveraging these insights, we introduce the Adaptive Feature De-correlation Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies correlation penalties to the feature dimensions of the representation matrix, effectively alleviating both over-correlation and over-smoothing issues.","The efficacy of the proposed framework is corroborated through extensive experiments conducted with four representative graph collaborative filtering models across four publicly available datasets."],"url":"http://arxiv.org/abs/2403.17416v1","category":"cs.IR"}
{"created":"2024-03-26 06:12:21","title":"LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction","abstract":"Over-correction is a critical problem in Chinese grammatical error correction (CGEC) task. Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the GEC system. However, these methods still require the output of several GEC systems and inevitably lead to reduced error recall. In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of GEC system outputs without a model ensemble. Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text. In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner. Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged. Besides, we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT).","sentences":["Over-correction is a critical problem in Chinese grammatical error correction (CGEC) task.","Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the GEC system.","However, these methods still require the output of several GEC systems and inevitably lead to reduced error recall.","In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of GEC system outputs without a model ensemble.","Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text.","In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner.","Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged.","Besides, we find that LM-Combiner still has a good rewriting performance even with small parameters and few training data, and thus can cost-effectively mitigate the over-correction of black-box GEC systems (e.g., ChatGPT)."],"url":"http://arxiv.org/abs/2403.17413v1","category":"cs.CL"}
{"created":"2024-03-26 17:47:00","title":"Beyond chromatic threshold via $(p,q)$-theorem, and sharp blow-up phenomenon","abstract":"We establish a novel connection between the well-known chromatic threshold problem in extremal combinatorics and the celebrated $(p,q)$-theorem in discrete geometry. In particular, for a graph $G$ with bounded clique number and a natural density condition, we prove a $(p,q)$-theorem for an abstract convexity space associated with $G$. Our result strengthens those of Thomassen and Nikiforov on the chromatic threshold of cliques. Our $(p,q)$-theorem can also be viewed as a $\\chi$-boundedness result for (what we call) ultra maximal $K_r$-free graphs.   We further show that the graphs under study are blow-ups of constant size graphs, improving a result of Oberkampf and Schacht on homomorphism threshold of cliques. Our result unravels the cause underpinning such a blow-up phenomenon, differentiating the chromatic and homomorphism threshold problems for cliques. Our result implies that for the homomorphism threshold problem, rather than the minimum degree condition usually considered in the literature, the decisive factor is a clique density condition on co-neighborhoods of vertices. More precisely, we show that if an $n$-vertex $K_{r}$-free graph $G$ satisfies that the common neighborhood of every pair of non-adjacent vertices induces a subgraph with $K_{r-2}$-density at least $\\varepsilon>0$, then $G$ must be a blow-up of some $K_r$-free graph $F$ on at most $2^{O(\\frac{r}{\\varepsilon}\\log\\frac{1}{\\varepsilon})}$ vertices. Furthermore, this single exponential bound is optimal. We construct examples with no $K_r$-free homomorphic image of size smaller than $2^{\\Omega_r(\\frac{1}{\\varepsilon})}$.","sentences":["We establish a novel connection between the well-known chromatic threshold problem in extremal combinatorics and the celebrated $(p,q)$-theorem in discrete geometry.","In particular, for a graph $G$ with bounded clique number and a natural density condition, we prove a $(p,q)$-theorem for an abstract convexity space associated with $G$. Our result strengthens those of Thomassen and Nikiforov on the chromatic threshold of cliques.","Our $(p,q)$-theorem can also be viewed as a $\\chi$-boundedness result for (what we call) ultra maximal $K_r$-free graphs.   ","We further show that the graphs under study are blow-ups of constant size graphs, improving a result of Oberkampf and Schacht on homomorphism threshold of cliques.","Our result unravels the cause underpinning such a blow-up phenomenon, differentiating the chromatic and homomorphism threshold problems for cliques.","Our result implies that for the homomorphism threshold problem, rather than the minimum degree condition usually considered in the literature, the decisive factor is a clique density condition on co-neighborhoods of vertices.","More precisely, we show that if an $n$-vertex $K_{r}$-free graph $G$ satisfies that the common neighborhood of every pair of non-adjacent vertices induces a subgraph with $K_{r-2}$-density at least $\\varepsilon>0$, then $G$ must be a blow-up of some $K_r$-free graph $F$ on at most $2^{O(\\frac{r}{\\varepsilon}\\log\\frac{1}{\\varepsilon})}$ vertices.","Furthermore, this single exponential bound is optimal.","We construct examples with no $K_r$-free homomorphic image of size smaller than $2^{\\Omega_r(\\frac{1}{\\varepsilon})}$."],"url":"http://arxiv.org/abs/2403.17910v1","category":"math.CO"}
{"created":"2024-03-26 17:12:50","title":"On the properties of distance covariance for categorical data: Robustness, sure screening, and approximate null distributions","abstract":"Pearson's Chi-squared test, though widely used for detecting association between categorical variables, exhibits low statistical power in large sparse contingency tables. To address this limitation, two novel permutation tests have been recently developed: the distance covariance permutation test and the U-statistic permutation test. Both leverage the distance covariance functional but employ different estimators. In this work, we explore key statistical properties of the distance covariance for categorical variables. Firstly, we show that unlike Chi-squared, the distance covariance functional is B-robust for any number of categories (fixed or diverging). Second, we establish the strong consistency of distance covariance screening under mild conditions, and simulations confirm its advantage over Chi-squared screening, especially for large sparse tables. Finally, we derive an approximate null distribution for a bias-corrected distance correlation estimate, demonstrating its effectiveness through simulations.","sentences":["Pearson's Chi-squared test, though widely used for detecting association between categorical variables, exhibits low statistical power in large sparse contingency tables.","To address this limitation, two novel permutation tests have been recently developed: the distance covariance permutation test and the U-statistic permutation test.","Both leverage the distance covariance functional but employ different estimators.","In this work, we explore key statistical properties of the distance covariance for categorical variables.","Firstly, we show that unlike Chi-squared, the distance covariance functional is B-robust for any number of categories (fixed or diverging).","Second, we establish the strong consistency of distance covariance screening under mild conditions, and simulations confirm its advantage over Chi-squared screening, especially for large sparse tables.","Finally, we derive an approximate null distribution for a bias-corrected distance correlation estimate, demonstrating its effectiveness through simulations."],"url":"http://arxiv.org/abs/2403.17882v1","category":"stat.ME"}
{"created":"2024-03-26 17:06:56","title":"MIND Your Language: A Multilingual Dataset for Cross-lingual News Recommendation","abstract":"Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers. Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular. Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages. Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes. Using xMIND, we systematically benchmark several state-of-the-art content-based neural news recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns. Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption. Our findings thus warrant a broader research effort in multilingual and cross-lingual news recommendation. The xMIND dataset is available at https://github.com/andreeaiana/xMIND.","sentences":["Digital news platforms use news recommenders as the main instrument to cater to the individual information needs of readers.","Despite an increasingly language-diverse online community, in which many Internet users consume news in multiple languages, the majority of news recommendation focuses on major, resource-rich languages, and English in particular.","Moreover, nearly all news recommendation efforts assume monolingual news consumption, whereas more and more users tend to consume information in at least two languages.","Accordingly, the existing body of work on news recommendation suffers from a lack of publicly available multilingual benchmarks that would catalyze development of news recommenders effective in multilingual settings and for low-resource languages.","Aiming to fill this gap, we introduce xMIND, an open, multilingual news recommendation dataset derived from the English MIND dataset using machine translation, covering a set of 14 linguistically and geographically diverse languages, with digital footprints of varying sizes.","Using xMIND, we systematically benchmark several state-of-the-art content-based neural news recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer scenarios, considering both monolingual and bilingual news consumption patterns.","Our findings reveal that (i) current NNRs, even when based on a multilingual language model, suffer from substantial performance losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT training has limited benefits, particularly when combined with a bilingual news consumption.","Our findings thus warrant a broader research effort in multilingual and cross-lingual news recommendation.","The xMIND dataset is available at https://github.com/andreeaiana/xMIND."],"url":"http://arxiv.org/abs/2403.17876v1","category":"cs.IR"}
{"created":"2024-03-26 16:06:10","title":"Motives","abstract":"Making a survey of recent constructions of universal cohomologies we suggest a new framework for a theory of motives in algebraic geometry.","sentences":["Making a survey of recent constructions of universal cohomologies we suggest a new framework for a theory of motives in algebraic geometry."],"url":"http://arxiv.org/abs/2403.17825v1","category":"math.AG"}
{"created":"2024-03-26 15:18:59","title":"Query Refinement for Diverse Top-$k$ Selection","abstract":"Database queries are often used to select and rank items as decision support for many applications. As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes. In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query. We show the hardness of this problem and propose a Mixed Integer Linear Programming (MILP) based solution. We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios. We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations.","sentences":["Database queries are often used to select and rank items as decision support for many applications.","As automated decision-making tools become more prevalent, there is a growing recognition of the need to diversify their outcomes.","In this paper, we define and study the problem of modifying the selection conditions of an ORDER BY query so that the result of the modified query closely fits some user-defined notion of diversity while simultaneously maintaining the intent of the original query.","We show the hardness of this problem and propose a Mixed Integer Linear Programming (MILP) based solution.","We further present optimizations designed to enhance the scalability and applicability of the solution in real-life scenarios.","We investigate the performance characteristics of our algorithm and show its efficiency and the usefulness of our optimizations."],"url":"http://arxiv.org/abs/2403.17786v1","category":"cs.DB"}
{"created":"2024-03-26 15:13:16","title":"CaseLink: Inductive Graph Learning for Legal Case Retrieval","abstract":"In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case. This relevance is referred to as the case-to-case reference relation. To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners. Existing legal case retrieval models mainly work by comparing the text representations of individual cases. Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance. In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship. Due to the inductive manner in the task of legal case retrieval, using case reference as input is not applicable for testing. Thus, in this paper, a CaseLink model based on inductive graph learning is proposed to utilise the intrinsic case connectivity for legal case retrieval, a novel Global Case Graph is incorporated to represent both the case semantic relationship and the case legal charge relationship. A novel contrastive objective with a regularisation on the degree of case nodes is proposed to leverage the information carried by the case reference relationship to optimise the model. Extensive experiments have been conducted on two benchmark datasets, which demonstrate the state-of-the-art performance of CaseLink. The code has been released on https://github.com/yanran-tang/CaseLink.","sentences":["In case law, the precedents are the relevant cases that are used to support the decisions made by the judges and the opinions of lawyers towards a given case.","This relevance is referred to as the case-to-case reference relation.","To efficiently find relevant cases from a large case pool, retrieval tools are widely used by legal practitioners.","Existing legal case retrieval models mainly work by comparing the text representations of individual cases.","Although they obtain a decent retrieval accuracy, the intrinsic case connectivity relationships among cases have not been well exploited for case encoding, therefore limiting the further improvement of retrieval performance.","In a case pool, there are three types of case connectivity relationships: the case reference relationship, the case semantic relationship, and the case legal charge relationship.","Due to the inductive manner in the task of legal case retrieval, using case reference as input is not applicable for testing.","Thus, in this paper, a CaseLink model based on inductive graph learning is proposed to utilise the intrinsic case connectivity for legal case retrieval, a novel Global Case Graph is incorporated to represent both the case semantic relationship and the case legal charge relationship.","A novel contrastive objective with a regularisation on the degree of case nodes is proposed to leverage the information carried by the case reference relationship to optimise the model.","Extensive experiments have been conducted on two benchmark datasets, which demonstrate the state-of-the-art performance of CaseLink.","The code has been released on https://github.com/yanran-tang/CaseLink."],"url":"http://arxiv.org/abs/2403.17780v1","category":"cs.IR"}
{"created":"2024-03-26 15:07:58","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","abstract":"Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round. Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension. Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in federated learning.","sentences":["Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates.","Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified.","In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update.","We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg.","By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg.","Our numerical results unveil that, contrary to prevailing claims, SecAgg offers weak privacy against membership inference attacks even in a single training round.","Indeed, it is difficult to hide a local update by adding other independent local updates when the updates are of high dimension.","Our findings underscore the imperative for additional privacy-enhancing mechanisms, such as noise injection, in federated learning."],"url":"http://arxiv.org/abs/2403.17775v1","category":"cs.LG"}
{"created":"2024-03-26 15:03:51","title":"Synthesis of super-heavy elements in the outer crust of a magnetar","abstract":"A theoretical understanding of a possible mechanism for synthesizing super-heavy elements in the outer crust of magnetars is presented. We demonstrate that such a mechanism can be present whenever the baryon density in the outer crust of a neutron star reaches values around $10^{-2}$ fm$^{-3}$. This scenario could be realized in magnetars with hypothetical large magnetic fields, $B \\geq 10^{18}$ G. Under such conditions, the Coulomb lattice, formed by ionized nuclei, enables a mechanism that synthesizes super-heavy elements.","sentences":["A theoretical understanding of a possible mechanism for synthesizing super-heavy elements in the outer crust of magnetars is presented.","We demonstrate that such a mechanism can be present whenever the baryon density in the outer crust of a neutron star reaches values around $10^{-2}$ fm$^{-3}$. This scenario could be realized in magnetars with hypothetical large magnetic fields, $B \\geq 10^{18}$ G. Under such conditions, the Coulomb lattice, formed by ionized nuclei, enables a mechanism that synthesizes super-heavy elements."],"url":"http://arxiv.org/abs/2403.17773v1","category":"nucl-th"}
{"created":"2024-03-26 14:51:12","title":"Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons","abstract":"In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.","sentences":["In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias.","We then create further challenging sub-tasks in an effort to explain this failure.","From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features.","This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads."],"url":"http://arxiv.org/abs/2403.17760v1","category":"cs.CL"}
{"created":"2024-03-26 14:50:44","title":"A quaternion-based obtention of the angle between adjacent sides in the evolution of regular polygons of $M$ sides under the vortex filament equation","abstract":"In this paper, using a quaternion formalism, we give a rigorous proof for the expression of the angle between adjacent sides in the skew polygons appearing at rational times in the evolution of regular polygons of $M$ sides under the vortex filament equation. More precisely, after transforming the rotation matrices that characterize those skew polygons into quaternions, we show that the products of those matrices can be expressed in terms of products of quaternions in a compact form, which enables us to reduce the problem of determining the angle into another one of proving that some trigonometric sums are purely imaginary.","sentences":["In this paper, using a quaternion formalism, we give a rigorous proof for the expression of the angle between adjacent sides in the skew polygons appearing at rational times in the evolution of regular polygons of $M$ sides under the vortex filament equation.","More precisely, after transforming the rotation matrices that characterize those skew polygons into quaternions, we show that the products of those matrices can be expressed in terms of products of quaternions in a compact form, which enables us to reduce the problem of determining the angle into another one of proving that some trigonometric sums are purely imaginary."],"url":"http://arxiv.org/abs/2403.17758v1","category":"math.NT"}
{"created":"2024-03-26 14:49:22","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","abstract":"Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.","sentences":["Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars.","Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable.","Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images.","Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce.","We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics.","This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites."],"url":"http://arxiv.org/abs/2403.17757v1","category":"cs.CV"}
{"created":"2024-03-26 14:13:44","title":"Deep Learning for Segmentation of Cracks in High-Resolution Images of Steel Bridges","abstract":"Automating the current bridge visual inspection practices using drones and image processing techniques is a prominent way to make these inspections more effective, robust, and less expensive. In this paper, we investigate the development of a novel deep-learning method for the detection of fatigue cracks in high-resolution images of steel bridges. First, we present a novel and challenging dataset comprising of images of cracks in steel bridges. Secondly, we integrate the ConvNext neural network with a previous state- of-the-art encoder-decoder network for crack segmentation. We study and report, the effects of the use of background patches on the network performance when applied to high-resolution images of cracks in steel bridges. Finally, we introduce a loss function that allows the use of more background patches for the training process, which yields a significant reduction in false positive rates.","sentences":["Automating the current bridge visual inspection practices using drones and image processing techniques is a prominent way to make these inspections more effective, robust, and less expensive.","In this paper, we investigate the development of a novel deep-learning method for the detection of fatigue cracks in high-resolution images of steel bridges.","First, we present a novel and challenging dataset comprising of images of cracks in steel bridges.","Secondly, we integrate the ConvNext neural network with a previous state- of-the-art encoder-decoder network for crack segmentation.","We study and report, the effects of the use of background patches on the network performance when applied to high-resolution images of cracks in steel bridges.","Finally, we introduce a loss function that allows the use of more background patches for the training process, which yields a significant reduction in false positive rates."],"url":"http://arxiv.org/abs/2403.17725v1","category":"cs.CV"}
{"created":"2024-03-26 14:03:19","title":"Low-temperature benchmarking of qubit control wires by primary electron thermometry","abstract":"Low-frequency qubit control wires require non-trivial thermal anchoring and low-pass filtering. The resulting electron temperature serves as a quality benchmark for these signal lines. In this technical note, we make use of a primary electron thermometry technique, using a Coulomb blockade thermometer, to establish the electron temperature in the millikelvin regime. The experimental four-probe measurement setup, the data analysis, and the measurement limitations are discussed in detail. We verify the results by also using another electron thermometry technique, based on a superconductor-insulator-normal metal junction. Our comparison of signal lines with QDevil's QFilter to unfiltered signal lines demonstrates that the filter significantly reduces both the rms noise and electron temperature, which is measured to be 22 $\\pm$ 1 mK.","sentences":["Low-frequency qubit control wires require non-trivial thermal anchoring and low-pass filtering.","The resulting electron temperature serves as a quality benchmark for these signal lines.","In this technical note, we make use of a primary electron thermometry technique, using a Coulomb blockade thermometer, to establish the electron temperature in the millikelvin regime.","The experimental four-probe measurement setup, the data analysis, and the measurement limitations are discussed in detail.","We verify the results by also using another electron thermometry technique, based on a superconductor-insulator-normal metal junction.","Our comparison of signal lines with QDevil's QFilter to unfiltered signal lines demonstrates that the filter significantly reduces both the rms noise and electron temperature, which is measured to be 22 $\\pm$ 1 mK."],"url":"http://arxiv.org/abs/2403.17720v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-26 13:59:51","title":"Evaluating Authoring Tools with the Explorable Authoring Requirements","abstract":"Explorables with interactive, multimodal content, openly available on the web, are a promising medium for education. Yet authoring such explorables requires web development expertise, excluding most educators and students from the authoring and remixing process. Some tools are available to reduce this barrier of entry and others are in development, making a method to evaluate these new tools necessary. On the basis of the software quality model ISO 25010, empirical results, and domain modeling, we derive the Explorable Authoring Requirements (EAR) as a requirements catalogue explorable authoring tools should implement. We then outline a future research design to operationalize EAR.","sentences":["Explorables with interactive, multimodal content, openly available on the web, are a promising medium for education.","Yet authoring such explorables requires web development expertise, excluding most educators and students from the authoring and remixing process.","Some tools are available to reduce this barrier of entry and others are in development, making a method to evaluate these new tools necessary.","On the basis of the software quality model ISO 25010, empirical results, and domain modeling, we derive the Explorable Authoring Requirements (EAR) as a requirements catalogue explorable authoring tools should implement.","We then outline a future research design to operationalize EAR."],"url":"http://arxiv.org/abs/2403.17714v1","category":"cs.HC"}
{"created":"2024-03-26 13:56:34","title":"Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection","abstract":"Visual Relationship Detection (VRD) has seen significant advancements with Transformer-based architectures recently. However, we identify two key limitations in a conventional label assignment for training Transformer-based VRD models, which is a process of mapping a ground-truth (GT) to a prediction. Under the conventional assignment, an unspecialized query is trained since a query is expected to detect every relation, which makes it difficult for a query to specialize in specific relations. Furthermore, a query is also insufficiently trained since a GT is assigned only to a single prediction, therefore near-correct or even correct predictions are suppressed by being assigned no relation as a GT. To address these issues, we propose Groupwise Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise Query Specialization trains a specialized query by dividing queries and relations into disjoint groups and directing a query in a specific query group solely toward relations in the corresponding relation group. Quality-Aware Multi-Assignment further facilitates the training by assigning a GT to multiple predictions that are significantly close to a GT in terms of a subject, an object, and the relation in between. Experimental results and analyses show that SpeaQ effectively trains specialized queries, which better utilize the capacity of a model, resulting in consistent performance gains with zero additional inference cost across multiple VRD models and benchmarks. Code is available at https://github.com/mlvlab/SpeaQ.","sentences":["Visual Relationship Detection (VRD) has seen significant advancements with Transformer-based architectures recently.","However, we identify two key limitations in a conventional label assignment for training Transformer-based VRD models, which is a process of mapping a ground-truth (GT) to a prediction.","Under the conventional assignment, an unspecialized query is trained since a query is expected to detect every relation, which makes it difficult for a query to specialize in specific relations.","Furthermore, a query is also insufficiently trained since a GT is assigned only to a single prediction, therefore near-correct or even correct predictions are suppressed by being assigned no relation as a GT.","To address these issues, we propose Groupwise Query Specialization and Quality-Aware Multi-Assignment (SpeaQ).","Groupwise Query Specialization trains a specialized query by dividing queries and relations into disjoint groups and directing a query in a specific query group solely toward relations in the corresponding relation group.","Quality-Aware Multi-Assignment further facilitates the training by assigning a GT to multiple predictions that are significantly close to a GT in terms of a subject, an object, and the relation in between.","Experimental results and analyses show that SpeaQ effectively trains specialized queries, which better utilize the capacity of a model, resulting in consistent performance gains with zero additional inference cost across multiple VRD models and benchmarks.","Code is available at https://github.com/mlvlab/SpeaQ."],"url":"http://arxiv.org/abs/2403.17709v1","category":"cs.CV"}
{"created":"2024-03-26 13:40:52","title":"The Solution for the CVPR 2023 1st foundation model challenge-Track2","abstract":"In this paper, we propose a solution for cross-modal transportation retrieval. Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification to mine the knowledge implied by attribute labels. More importantly, We use the strategy of Inclusion Relation Matching to make the image-text pairs with inclusion relation have similar representation in the feature space. For the vehicle retrieval task, we use BLIP as the base model. Since aligning the color attributes of vehicles is challenging, we introduce attribute-based object detection techniques to add color patch blocks to vehicle images for color data augmentation. This serves as strong prior information, helping the model perform the image-text alignment. At the same time, we incorporate labeled attributes into the image-text alignment loss to learn fine-grained alignment and prevent similar images and texts from being incorrectly separated. Our approach ranked first in the final B-board test with a score of 70.9.","sentences":["In this paper, we propose a solution for cross-modal transportation retrieval.","Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy.","In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification to mine the knowledge implied by attribute labels.","More importantly, We use the strategy of Inclusion Relation Matching to make the image-text pairs with inclusion relation have similar representation in the feature space.","For the vehicle retrieval task, we use BLIP as the base model.","Since aligning the color attributes of vehicles is challenging, we introduce attribute-based object detection techniques to add color patch blocks to vehicle images for color data augmentation.","This serves as strong prior information, helping the model perform the image-text alignment.","At the same time, we incorporate labeled attributes into the image-text alignment loss to learn fine-grained alignment and prevent similar images and texts from being incorrectly separated.","Our approach ranked first in the final B-board test with a score of 70.9."],"url":"http://arxiv.org/abs/2403.17702v1","category":"cs.CV"}
{"created":"2024-03-26 13:35:29","title":"Tutte polynomials of matroids as universal valuative invariants","abstract":"We provide a full classification of all families of matroids that are closed under duality and minors, and for which the Tutte polynomial is a universal valuative invariant. There are four inclusion-wise maximal families, two of which are the class of elementary split matroids and the class of graphic Schubert matroids. As a consequence of our framework, we derive new properties of Tutte polynomials of arbitrary matroids. Among other results, we show that the Tutte polynomial of every matroid can be expressed uniquely as an integral combination of Tutte polynomials of graphic Schubert matroids.","sentences":["We provide a full classification of all families of matroids that are closed under duality and minors, and for which the Tutte polynomial is a universal valuative invariant.","There are four inclusion-wise maximal families, two of which are the class of elementary split matroids and the class of graphic Schubert matroids.","As a consequence of our framework, we derive new properties of Tutte polynomials of arbitrary matroids.","Among other results, we show that the Tutte polynomial of every matroid can be expressed uniquely as an integral combination of Tutte polynomials of graphic Schubert matroids."],"url":"http://arxiv.org/abs/2403.17696v1","category":"math.CO"}
{"created":"2024-03-26 13:31:33","title":"Mean temperature and concentration profiles in turbulent internal flows","abstract":"We derive explicit formulas for the mean profiles of temperature or concentration of a diffusing substance (modeled as passive scalars) in forced turbulent convection, as a function of the Reynolds and Prandtl numbers. The derivation leverages on the observed universality of the inner-layer thermal eddy diffusivity with respect to Reynolds and Prandtl number variations and across different flows, and on universality of the passive scalar defect in the core flow. Matching of the inner- and outer-layer expression yields a smooth compound passive scalar profile. We find excellent agreement of the analytical profile with data from direct numerical simulations of pipe and channel flows under various forcing conditions, and over a wide range of Reynolds and Prandtl numbers.","sentences":["We derive explicit formulas for the mean profiles of temperature or concentration of a diffusing substance (modeled as passive scalars) in forced turbulent convection, as a function of the Reynolds and Prandtl numbers.","The derivation leverages on the observed universality of the inner-layer thermal eddy diffusivity with respect to Reynolds and Prandtl number variations and across different flows, and on universality of the passive scalar defect in the core flow.","Matching of the inner- and outer-layer expression yields a smooth compound passive scalar profile.","We find excellent agreement of the analytical profile with data from direct numerical simulations of pipe and channel flows under various forcing conditions, and over a wide range of Reynolds and Prandtl numbers."],"url":"http://arxiv.org/abs/2403.17689v1","category":"physics.flu-dyn"}
{"created":"2024-03-26 13:02:43","title":"How Private is DP-SGD?","abstract":"We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.","sentences":["We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ.","While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis.","On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available.","This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version.","Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD."],"url":"http://arxiv.org/abs/2403.17673v1","category":"cs.LG"}
{"created":"2024-03-26 12:59:24","title":"Radiative Acceleration and X-ray Spectrum of Outflowing Pure Electron-Positron Pair Fireball in Magnetar Bursts","abstract":"One X-ray short burst accompanied by a Galactic fast radio burst has been detected so far, and its X-ray cut-off energy was significantly higher than that of other X-ray short bursts. Such X-ray bursts are thought to be emitted from a fireball in the magnetosphere of magnetars. If a fireball is formed around a magnetic pole, it expands and is accelerated under its radiation pressure, later producing photon emission and plasma outflow, which can be responsible for radio bursts. We numerically study the radiative acceleration of this outflowing fireball, which consists of electron-positron pairs and radiation, and obtain the spectrum of the escaped X-ray photons. We consistently take into account cyclotron resonant scattering, which enhances the scattering cross section resulting in a strong radiative force and high optical depth. Our results show that similar spectra to the observed X-ray spectrum in the Galactic fast radio burst are realized in outflowing fireballs and that the plasma outflow is simultaneously accelerated to a high Lorentz factor owing to cyclotron resonant scattering.","sentences":["One X-ray short burst accompanied by a Galactic fast radio burst has been detected so far, and its X-ray cut-off energy was significantly higher than that of other X-ray short bursts.","Such X-ray bursts are thought to be emitted from a fireball in the magnetosphere of magnetars.","If a fireball is formed around a magnetic pole, it expands and is accelerated under its radiation pressure, later producing photon emission and plasma outflow, which can be responsible for radio bursts.","We numerically study the radiative acceleration of this outflowing fireball, which consists of electron-positron pairs and radiation, and obtain the spectrum of the escaped X-ray photons.","We consistently take into account cyclotron resonant scattering, which enhances the scattering cross section resulting in a strong radiative force and high optical depth.","Our results show that similar spectra to the observed X-ray spectrum in the Galactic fast radio burst are realized in outflowing fireballs and that the plasma outflow is simultaneously accelerated to a high Lorentz factor owing to cyclotron resonant scattering."],"url":"http://arxiv.org/abs/2403.17668v1","category":"astro-ph.HE"}
{"created":"2024-03-26 12:41:42","title":"Generalising the maximum independent set algorithm via Boolean networks","abstract":"A simple greedy algorithm to find a maximal independent set (MIS) in a graph starts with the empty set and visits every vertex, adding it to the set if and only if none of its neighbours are already in the set. In this paper, we consider the generalisation of this MIS algorithm by letting it start with any set of vertices and we prove the hardness of many decision problems related to this generalisation. Our results are based on two main strategies. Firstly, we view the MIS algorithm as a sequential update of a Boolean network, which we refer to as the MIS network, according to a permutation of the vertex set. The set of fixed points of the MIS network corresponds to the set of MIS of the graph. Our generalisation then consists in starting from any configuration and following a sequential update given by a word of vertices. Secondly, we introduce the concept of a colony of a graph, that is a set of vertices that is dominated by an independent set. Deciding whether a set of vertices is a colony is NP-complete; decision problems related to the MIS algorithm will be reduced from the Colony problem. We first show that deciding whether a configuration can reach all maximal independent sets is coNP-complete. Second, we consider so-called fixing words, that allow to reach a MIS for any initial configuration, and fixing permutations, which we call permises; deciding whether a permutation is fixing is coNP-complete. Third, we show that deciding whether a graph has a permis is coNP-hard. Finally, we generalise the MIS algorithm to digraphs. The algorithm then uses the so-called kernel network, whose fixed points are the kernels of the digraph. Deciding whether the kernel network of a given digraph is fixable is coNP-hard, even for digraphs that have a kernel. Alternatively, we introduce two fixable Boolean networks whose sets of fixed points contain all kernels.","sentences":["A simple greedy algorithm to find a maximal independent set (MIS) in a graph starts with the empty set and visits every vertex, adding it to the set if and only if none of its neighbours are already in the set.","In this paper, we consider the generalisation of this MIS algorithm by letting it start with any set of vertices and we prove the hardness of many decision problems related to this generalisation.","Our results are based on two main strategies.","Firstly, we view the MIS algorithm as a sequential update of a Boolean network, which we refer to as the MIS network, according to a permutation of the vertex set.","The set of fixed points of the MIS network corresponds to the set of MIS of the graph.","Our generalisation then consists in starting from any configuration and following a sequential update given by a word of vertices.","Secondly, we introduce the concept of a colony of a graph, that is a set of vertices that is dominated by an independent set.","Deciding whether a set of vertices is a colony is NP-complete; decision problems related to the MIS algorithm will be reduced from the Colony problem.","We first show that deciding whether a configuration can reach all maximal independent sets is coNP-complete.","Second, we consider so-called fixing words, that allow to reach a MIS for any initial configuration, and fixing permutations, which we call permises; deciding whether a permutation is fixing is coNP-complete.","Third, we show that deciding whether a graph has a permis is coNP-hard.","Finally, we generalise the MIS algorithm to digraphs.","The algorithm then uses the so-called kernel network, whose fixed points are the kernels of the digraph.","Deciding whether the kernel network of a given digraph is fixable is coNP-hard, even for digraphs that have a kernel.","Alternatively, we introduce two fixable Boolean networks whose sets of fixed points contain all kernels."],"url":"http://arxiv.org/abs/2403.17658v1","category":"cs.DS"}
{"created":"2024-03-26 12:37:06","title":"Improving the Spatial Correlation Characteristics of Antenna Arrays using Linear Operators and Wide-band Modelling","abstract":"The analysis of wireless communication channels at the mmWave, sub-THz and THz bands gives rise to difficulties in the construction of antenna arrays due to the small maximum inter-element spacing constraints at these frequencies. Arrays with uniform spacing greater than half the wavelength for a certain carrier frequency exhibit aliasing side-lobes in the angular domain, prohibiting non-ambiguous estimates of a propagating wave-front's angle of arrival.   In this paper, we present how wide-band modelling of the array response is useful in mitigating this spatial aliasing effect. This approach aims to reduce the grating lobes by exploiting the angle- and frequency-dependent phase-shifts observed in the response of the array to a planar wave-front travelling across it.   Furthermore, we propose a method by which the spatial correlation characteristics of an array operating at 33 GHz carrier frequency with an instantaneous bandwidth of 1 GHz can be improved such that the angular-domain side-lobes are reduced by 5-10 dB. This method, applicable to arbitrary antenna array manifolds, makes use of a linear operator that is applied to the base-band samples of the channel transfer function measured in space and frequency domains. By means of synthetically simulated arrays, we show that when operating with a bandwidth of 1 GHz, the use of a derived linear operator applied to the array output results in the spatial correlation characteristics approaching those of the array operating at a bandwidth of 12 GHz. Hence, non-ambiguous angle estimates can be obtained in the field without the use of expensive high-bandwidth RF front-end components.","sentences":["The analysis of wireless communication channels at the mmWave, sub-THz and THz bands gives rise to difficulties in the construction of antenna arrays due to the small maximum inter-element spacing constraints at these frequencies.","Arrays with uniform spacing greater than half the wavelength for a certain carrier frequency exhibit aliasing side-lobes in the angular domain, prohibiting non-ambiguous estimates of a propagating wave-front's angle of arrival.   ","In this paper, we present how wide-band modelling of the array response is useful in mitigating this spatial aliasing effect.","This approach aims to reduce the grating lobes by exploiting the angle- and frequency-dependent phase-shifts observed in the response of the array to a planar wave-front travelling across it.   ","Furthermore, we propose a method by which the spatial correlation characteristics of an array operating at 33 GHz carrier frequency with an instantaneous bandwidth of 1 GHz can be improved such that the angular-domain side-lobes are reduced by 5-10 dB. This method, applicable to arbitrary antenna array manifolds, makes use of a linear operator that is applied to the base-band samples of the channel transfer function measured in space and frequency domains.","By means of synthetically simulated arrays, we show that when operating with a bandwidth of 1 GHz, the use of a derived linear operator applied to the array output results in the spatial correlation characteristics approaching those of the array operating at a bandwidth of 12 GHz.","Hence, non-ambiguous angle estimates can be obtained in the field without the use of expensive high-bandwidth RF front-end components."],"url":"http://arxiv.org/abs/2403.17654v1","category":"eess.SP"}
{"created":"2024-03-26 12:31:27","title":"Towards a Dutch hybrid quantum/HPC infrastructure","abstract":"Quantum Inspire has taken important steps to enable quantum applications by developing a setting that allows the execution of hybrid algorithms. Currently, the setting uses a classical server (HPC node) co-located with the quantum computer for the high frequency coupling needed by hybrid algorithms. A fast task manager (dispatcher) has been developed to orchestrate the interaction between the server and the quantum computer. Although successful, the setting imposes a specific hybrid job-structure. This is most likely always going to be the case and we are currently discussing how to make sure this does not hamper the uptake of the setting. Furthermore, first steps have been taken towards the integration with the Dutch National High-Performance Computing (HPC) Center, hosted by SURF. As a first approach we have setup a setting consisting of two SLURM clusters, one in the HPC (C1) and the second (C2) co-located with Quantum Inspire API. Jobs are submitted from C1 to C2. Quantum Inspire can then schedule with C2 the jobs to the quantum computer. With this setting, we enable control from both SURF and Quantum Inspire on the jobs being executed. By using C1 for the jobs submission we remove the accounting burden from Quantum Inspire. By having C2 co-located with Quantum Inspire API, we make the setting more resilient towards network failures. This setting can be extended for other HPC centers to submit jobs to Quantum Inspire backends.","sentences":["Quantum Inspire has taken important steps to enable quantum applications by developing a setting that allows the execution of hybrid algorithms.","Currently, the setting uses a classical server (HPC node) co-located with the quantum computer for the high frequency coupling needed by hybrid algorithms.","A fast task manager (dispatcher) has been developed to orchestrate the interaction between the server and the quantum computer.","Although successful, the setting imposes a specific hybrid job-structure.","This is most likely always going to be the case and we are currently discussing how to make sure this does not hamper the uptake of the setting.","Furthermore, first steps have been taken towards the integration with the Dutch National High-Performance Computing (HPC) Center, hosted by SURF.","As a first approach we have setup a setting consisting of two SLURM clusters, one in the HPC (C1) and the second (C2) co-located with Quantum Inspire API.","Jobs are submitted from C1 to C2.","Quantum Inspire can then schedule with C2 the jobs to the quantum computer.","With this setting, we enable control from both SURF and Quantum Inspire on the jobs being executed.","By using C1 for the jobs submission we remove the accounting burden from Quantum Inspire.","By having C2 co-located with Quantum Inspire API, we make the setting more resilient towards network failures.","This setting can be extended for other HPC centers to submit jobs to Quantum Inspire backends."],"url":"http://arxiv.org/abs/2403.17649v1","category":"quant-ph"}
{"created":"2024-03-26 12:29:56","title":"Healthcare Data Governance, Privacy, and Security - A Conceptual Framework","abstract":"The abundance of data has transformed the world in every aspect. It has become the core element in decision making, problem solving, and innovation in almost all areas of life, including business, science, healthcare, education, and many others. Despite all these advances, privacy and security remain critical concerns of the healthcare industry. It is important to note that healthcare data can also be a liability if it is not managed correctly. This data mismanagement can have severe consequences for patients and healthcare organisations, including patient safety, legal liability, damage to reputation, financial loss, and operational inefficiency. Healthcare organisations must comply with a range of regulations to protect patient data. We perform a classification of data governance elements or components in a manner that thoroughly assesses the healthcare data chain from a privacy and security standpoint. After deeply analysing the existing literature, we propose a conceptual privacy and security driven healthcare data governance framework.","sentences":["The abundance of data has transformed the world in every aspect.","It has become the core element in decision making, problem solving, and innovation in almost all areas of life, including business, science, healthcare, education, and many others.","Despite all these advances, privacy and security remain critical concerns of the healthcare industry.","It is important to note that healthcare data can also be a liability if it is not managed correctly.","This data mismanagement can have severe consequences for patients and healthcare organisations, including patient safety, legal liability, damage to reputation, financial loss, and operational inefficiency.","Healthcare organisations must comply with a range of regulations to protect patient data.","We perform a classification of data governance elements or components in a manner that thoroughly assesses the healthcare data chain from a privacy and security standpoint.","After deeply analysing the existing literature, we propose a conceptual privacy and security driven healthcare data governance framework."],"url":"http://arxiv.org/abs/2403.17648v1","category":"cs.CR"}
{"created":"2024-03-26 12:28:04","title":"Uncertainty-aware Distributional Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior performance.","sentences":["Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data.","A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity.","Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity.","In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity.","We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns.","Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior performance."],"url":"http://arxiv.org/abs/2403.17646v1","category":"cs.LG"}
{"created":"2024-03-26 12:05:52","title":"Average amplitude and phase detuning near driven 3rd integer resonance","abstract":"For resonant slow extraction of accelerated particle beams from synchrotrons, a 3rd order resonance driven by sextupole magnets is commonly used. This note reviews the non-linear particle dynamics under these conditions as described by the Kobayashi Hamiltonian, focussing on the amplitude and phase dependent detuning. Using the Kobayashi Hamiltonian as the invariant, an expression for the action and the average detuning as a function of the Hamiltonian is derived and compared to numerically obtained results.","sentences":["For resonant slow extraction of accelerated particle beams from synchrotrons, a 3rd order resonance driven by sextupole magnets is commonly used.","This note reviews the non-linear particle dynamics under these conditions as described by the Kobayashi Hamiltonian, focussing on the amplitude and phase dependent detuning.","Using the Kobayashi Hamiltonian as the invariant, an expression for the action and the average detuning as a function of the Hamiltonian is derived and compared to numerically obtained results."],"url":"http://arxiv.org/abs/2403.17629v1","category":"physics.acc-ph"}
{"created":"2024-03-26 11:35:41","title":"On the Pettis Integral Approach to Large Population Games","abstract":"The analysis of large population economies with incomplete information often entails the integration of a continuum of random variables. We showcase the usefulness of the integral notion \\`a la Pettis (1938) to study such models. We present several results on Pettis integrals, including convenient sufficient conditions for Pettis integrability and Fubini-like exchangeability formulae, illustrated through a running example. Building on these foundations, we conduct a unified analysis of Bayesian games with arbitrarily many heterogeneous agents. We provide a sufficient condition on payoff structures, under which the equilibrium uniqueness is guaranteed across all signal structures. Our condition is parsimonious, as it turns out necessary when strategic interactions are undirected. We further identify the moment restrictions, imposed on the equilibrium action-state joint distribution, which have crucial implications for information designer's problem of persuading a population of strategically interacting agents. To attain these results, we introduce and develop novel mathematical tools, built on the theory of integral kernels and reproducing kernel Hilbert spaces in functional analysis.","sentences":["The analysis of large population economies with incomplete information often entails the integration of a continuum of random variables.","We showcase the usefulness of the integral notion \\`a la Pettis (1938) to study such models.","We present several results on Pettis integrals, including convenient sufficient conditions for Pettis integrability and Fubini-like exchangeability formulae, illustrated through a running example.","Building on these foundations, we conduct a unified analysis of Bayesian games with arbitrarily many heterogeneous agents.","We provide a sufficient condition on payoff structures, under which the equilibrium uniqueness is guaranteed across all signal structures.","Our condition is parsimonious, as it turns out necessary when strategic interactions are undirected.","We further identify the moment restrictions, imposed on the equilibrium action-state joint distribution, which have crucial implications for information designer's problem of persuading a population of strategically interacting agents.","To attain these results, we introduce and develop novel mathematical tools, built on the theory of integral kernels and reproducing kernel Hilbert spaces in functional analysis."],"url":"http://arxiv.org/abs/2403.17605v1","category":"econ.TH"}
{"created":"2024-03-26 11:08:48","title":"An Exact Solution for Allocating Car Parking Spaces on Campus","abstract":"All over the world, especially in the university environment, planning managers and traffic engineers are constantly faced with the problem of inadequate allocation of car parking spaces to demanded users. Users could either prefer reserved parking spaces to unreserved parking spaces or vice versa. This makes the campus parking manager to be faced with two basic problem which are: the problem of allocating the actual number of available reserved spaces to users without any conflict over the same parking space, and the problem of determining the number of parking permit to be issued for parking lot with unreserved spaces. Hence, an optimal or available solution to the problem is required. This paper investigates a model for allocating car parking spaces, adds a constraint to address the reserved parking policy in a university environment and solves the parking allocation problem using an exact solution method. The result obtained gives the value of the objective function and the optimal allocation of users to each parking lot.","sentences":["All over the world, especially in the university environment, planning managers and traffic engineers are constantly faced with the problem of inadequate allocation of car parking spaces to demanded users.","Users could either prefer reserved parking spaces to unreserved parking spaces or vice versa.","This makes the campus parking manager to be faced with two basic problem which are: the problem of allocating the actual number of available reserved spaces to users without any conflict over the same parking space, and the problem of determining the number of parking permit to be issued for parking lot with unreserved spaces.","Hence, an optimal or available solution to the problem is required.","This paper investigates a model for allocating car parking spaces, adds a constraint to address the reserved parking policy in a university environment and solves the parking allocation problem using an exact solution method.","The result obtained gives the value of the objective function and the optimal allocation of users to each parking lot."],"url":"http://arxiv.org/abs/2403.17597v1","category":"cs.CE"}
{"created":"2024-03-26 11:06:08","title":"High order weak approximation of Stochastic Differential Equations for bounded and measurable test functions","abstract":"We present a method for approximating solutions of Stochastic Differential Equations (SDEs) with arbitrary rates. This approximation is derived for bounded and measurable test functions. Specifically, we demonstrate that, leveraging the standard weak approximation properties of numerical schemes for smooth test functions (such as first-order weak convergence for the Euler scheme) we can achieve convergence for simply bounded and measurable test functions at any desired rate by constructing a tailored approximation for the semigroup of the SDE. This is achieved by evaluating the scheme (e.g., Euler) on a random time grid. To establish convergence, we exploit the regularization properties of the scheme, which hold under a weak uniform H\\\"ormander condition.","sentences":["We present a method for approximating solutions of Stochastic Differential Equations (SDEs) with arbitrary rates.","This approximation is derived for bounded and measurable test functions.","Specifically, we demonstrate that, leveraging the standard weak approximation properties of numerical schemes for smooth test functions (such as first-order weak convergence for the Euler scheme) we can achieve convergence for simply bounded and measurable test functions at any desired rate by constructing a tailored approximation for the semigroup of the SDE.","This is achieved by evaluating the scheme (e.g., Euler) on a random time grid.","To establish convergence, we exploit the regularization properties of the scheme, which hold under a weak uniform H\\\"ormander condition."],"url":"http://arxiv.org/abs/2403.17596v1","category":"math.PR"}
{"created":"2024-03-26 10:58:04","title":"Multiplicative generalised polynomial sequences","abstract":"We fully classify completely multiplicative sequences which are given by generalised polynomial formulae, and obtain a similar result for (not necessarily completely) multiplicative sequences under the additional restriction that the sequence is not zero almost everywhere.","sentences":["We fully classify completely multiplicative sequences which are given by generalised polynomial formulae, and obtain a similar result for (not necessarily completely) multiplicative sequences under the additional restriction that the sequence is not zero almost everywhere."],"url":"http://arxiv.org/abs/2403.17590v1","category":"math.NT"}
{"created":"2024-03-26 10:41:48","title":"Measuring Dependence between Events","abstract":"Measuring dependence between two events, or equivalently between two binary random variables, amounts to expressing the dependence structure inherent in a $2\\times 2$ contingency table in a real number between $-1$ and $1$. Countless such dependence measures exist, but there is little theoretical guidance on how they compare and on their advantages and shortcomings. Thus, practitioners might be overwhelmed by the problem of choosing a suitable measure. We provide a set of natural desirable properties that a proper dependence measure should fulfill. We show that Yule's Q and the little-known Cole coefficient are proper, while the most widely-used measures, the phi coefficient and all contingency coefficients, are improper. They have a severe attainability problem, that is, even under perfect dependence they can be very far away from $-1$ and $1$, and often differ substantially from the proper measures in that they understate strength of dependence. The structural reason is that these are measures for equality of events rather than of dependence. We derive the (in some instances non-standard) limiting distributions of the measures and illustrate how asymptotically valid confidence intervals can be constructed. In a case study on drug consumption we demonstrate how misleading conclusions may arise from the use of improper dependence measures.","sentences":["Measuring dependence between two events, or equivalently between two binary random variables, amounts to expressing the dependence structure inherent in a $2\\times 2$ contingency table in a real number between $-1$ and $1$. Countless such dependence measures exist, but there is little theoretical guidance on how they compare and on their advantages and shortcomings.","Thus, practitioners might be overwhelmed by the problem of choosing a suitable measure.","We provide a set of natural desirable properties that a proper dependence measure should fulfill.","We show that Yule's Q and the little-known Cole coefficient are proper, while the most widely-used measures, the phi coefficient and all contingency coefficients, are improper.","They have a severe attainability problem, that is, even under perfect dependence they can be very far away from $-1$ and $1$, and often differ substantially from the proper measures in that they understate strength of dependence.","The structural reason is that these are measures for equality of events rather than of dependence.","We derive the (in some instances non-standard) limiting distributions of the measures and illustrate how asymptotically valid confidence intervals can be constructed.","In a case study on drug consumption we demonstrate how misleading conclusions may arise from the use of improper dependence measures."],"url":"http://arxiv.org/abs/2403.17580v1","category":"stat.ME"}
{"created":"2024-03-26 09:39:53","title":"Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks","abstract":"Descriptive grammars are highly valuable, but writing them is time-consuming and difficult. Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data. As for formal grammars, they can be challenging to interpret. In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar. More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules. For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study. We associate statistical information to each rule, and we compare the ranking of the model's results to those of other quantitative and statistical measures. Our method captures both well-known and less well-known significant grammar rules in Spanish, French, and Wolof.","sentences":["Descriptive grammars are highly valuable, but writing them is time-consuming and difficult.","Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data.","As for formal grammars, they can be challenging to interpret.","In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar.","More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules.","For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study.","We associate statistical information to each rule, and we compare the ranking of the model's results to those of other quantitative and statistical measures.","Our method captures both well-known and less well-known significant grammar rules in Spanish, French, and Wolof."],"url":"http://arxiv.org/abs/2403.17534v1","category":"cs.CL"}
{"created":"2024-03-26 09:22:32","title":"Determination of the dynamic Young's modulus of quantum materials in piezoactuator-driven uniaxial pressure cells using a low-frequency a.c. method","abstract":"We report on a new technique for measuring the dynamic Young's modulus, $E$, of quantum materials at low temperatures as a function of static tuning strain, $\\epsilon$, in piezoactuator-driven pressure cells. In addition to a static tuning of stress and strain, we apply a small-amplitude, finite-frequency a.c. (1 Hz$ \\lesssim \\omega \\lesssim $1000 Hz) uniaxial stress, $\\sigma_{ac}$, to the sample and measure the resulting a.c. strain, $\\epsilon_{ac}$, using a capacitive sensor to obtain the associated modulus $E$. We demonstrate the performance of the new technique through proof-of-principle experiments on the unconventional superconductor Sr$_2$RuO$_4$, which is known for its rich temperature-strain phase diagram. In particular, we show that the magnitude of $E$, measured using this a.c. technique at low frequencies, exhibits a pronounced nonlinear elasticity, which is in very good agreement with previous Young's modulus measurements on Sr$_2$RuO$_4$ under [100] strain using a d.c. method (Noad et al., Science 382, 447-450 (2023)). By combining the new a.c. Young's modulus measurements with a.c. elastocaloric measurements in a single measurement, we demonstrate that these a.c. techniques are powerful in detecting small anomalies in the elastic properties of quantum materials. Finally, using the case of Sr$_2$RuO$_4$ as an example, we demonstrate how the imaginary component of the modulus can provide additional information about the nature of ordered phases.","sentences":["We report on a new technique for measuring the dynamic Young's modulus, $E$, of quantum materials at low temperatures as a function of static tuning strain, $\\epsilon$, in piezoactuator-driven pressure cells.","In addition to a static tuning of stress and strain, we apply a small-amplitude, finite-frequency a.c.","(1 Hz$ \\lesssim \\omega \\lesssim $1000 Hz) uniaxial stress, $\\sigma_{ac}$, to the sample and measure the resulting a.c. strain, $\\epsilon_{ac}$, using a capacitive sensor to obtain the associated modulus $E$.","We demonstrate the performance of the new technique through proof-of-principle experiments on the unconventional superconductor Sr$_2$RuO$_4$, which is known for its rich temperature-strain phase diagram.","In particular, we show that the magnitude of $E$, measured using this a.c. technique at low frequencies, exhibits a pronounced nonlinear elasticity, which is in very good agreement with previous Young's modulus measurements on Sr$_2$RuO$_4$ under [100] strain using a d.c. method (Noad et al., Science 382, 447-450 (2023)).","By combining the new a.c.","Young's modulus measurements with a.c. elastocaloric measurements in a single measurement, we demonstrate that these a.c. techniques are powerful in detecting small anomalies in the elastic properties of quantum materials.","Finally, using the case of Sr$_2$RuO$_4$ as an example, we demonstrate how the imaginary component of the modulus can provide additional information about the nature of ordered phases."],"url":"http://arxiv.org/abs/2403.17519v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 09:16:21","title":"Speaker Distance Estimation in Enclosures from Single-Channel Audio","abstract":"Distance estimation from audio plays a crucial role in various applications, such as acoustic scene analysis, sound source localization, and room modeling. Most studies predominantly center on employing a classification approach, where distances are discretized into distinct categories, enabling smoother model training and achieving higher accuracy but imposing restrictions on the precision of the obtained sound source position. Towards this direction, in this paper we propose a novel approach for continuous distance estimation from audio signals using a convolutional recurrent neural network with an attention module. The attention mechanism enables the model to focus on relevant temporal and spectral features, enhancing its ability to capture fine-grained distance-related information. To evaluate the effectiveness of our proposed method, we conduct extensive experiments using audio recordings in controlled environments with three levels of realism (synthetic room impulse response, measured response with convolved speech, and real recordings) on four datasets (our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23). Experimental results show that the model achieves an absolute error of 0.11 meters in a noiseless synthetic scenario. Moreover, the results showed an absolute error of about 1.30 meters in the hybrid scenario. The algorithm's performance in the real scenario, where unpredictable environmental factors and noise are prevalent, yields an absolute error of approximately 0.50 meters. For reproducible research purposes we make model, code, and synthetic datasets available at https://github.com/michaelneri/audio-distance-estimation.","sentences":["Distance estimation from audio plays a crucial role in various applications, such as acoustic scene analysis, sound source localization, and room modeling.","Most studies predominantly center on employing a classification approach, where distances are discretized into distinct categories, enabling smoother model training and achieving higher accuracy but imposing restrictions on the precision of the obtained sound source position.","Towards this direction, in this paper we propose a novel approach for continuous distance estimation from audio signals using a convolutional recurrent neural network with an attention module.","The attention mechanism enables the model to focus on relevant temporal and spectral features, enhancing its ability to capture fine-grained distance-related information.","To evaluate the effectiveness of our proposed method, we conduct extensive experiments using audio recordings in controlled environments with three levels of realism (synthetic room impulse response, measured response with convolved speech, and real recordings) on four datasets (our synthetic dataset, QMULTIMIT, VoiceHome-2, and STARSS23).","Experimental results show that the model achieves an absolute error of 0.11 meters in a noiseless synthetic scenario.","Moreover, the results showed an absolute error of about 1.30 meters in the hybrid scenario.","The algorithm's performance in the real scenario, where unpredictable environmental factors and noise are prevalent, yields an absolute error of approximately 0.50 meters.","For reproducible research purposes we make model, code, and synthetic datasets available at https://github.com/michaelneri/audio-distance-estimation."],"url":"http://arxiv.org/abs/2403.17514v1","category":"eess.AS"}
{"created":"2024-03-26 09:04:18","title":"DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning","abstract":"Class-incremental learning (CIL) under an exemplar-free constraint has presented a significant challenge. Existing methods adhering to this constraint are prone to catastrophic forgetting, far more so than replay-based techniques that retain access to past samples. In this paper, to solve the exemplar-free CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The DS-AL contains a main stream offering an analytical (i.e., closed-form) linear solution, and a compensation stream improving the inherent under-fitting limitation due to adopting linear mapping. The main stream redefines the CIL problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an equivalence between the CIL and its joint-learning counterpart. The compensation stream is governed by a Dual-Activation Compensation (DAC) module. This module re-activates the embedding with a different activation function from the main stream one, and seeks fitting compensation by projecting the embedding to the null space of the main stream's linear mapping. Empirical results demonstrate that the DS-AL, despite being an exemplar-free technique, delivers performance comparable with or better than that of replay-based methods across various datasets, including CIFAR-100, ImageNet-100 and ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to execute CIL in a phase-invariant manner. This is evidenced by a never-before-seen 500-phase CIL ImageNet task, which performs on a level identical to a 5-phase one. Our codes are available at https://github.com/ZHUANGHP/Analytic-continual-learning.","sentences":["Class-incremental learning (CIL) under an exemplar-free constraint has presented a significant challenge.","Existing methods adhering to this constraint are prone to catastrophic forgetting, far more so than replay-based techniques that retain access to past samples.","In this paper, to solve the exemplar-free CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach.","The DS-AL contains a main stream offering an analytical (i.e., closed-form) linear solution, and a compensation stream improving the inherent under-fitting limitation due to adopting linear mapping.","The main stream redefines the CIL problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an equivalence between the CIL and its joint-learning counterpart.","The compensation stream is governed by a Dual-Activation Compensation (DAC) module.","This module re-activates the embedding with a different activation function from the main stream one, and seeks fitting compensation by projecting the embedding to the null space of the main stream's linear mapping.","Empirical results demonstrate that the DS-AL, despite being an exemplar-free technique, delivers performance comparable with or better than that of replay-based methods across various datasets, including CIFAR-100, ImageNet-100 and ImageNet-Full.","Additionally, the C-RLS' equivalent property allows the DS-AL to execute CIL in a phase-invariant manner.","This is evidenced by a never-before-seen 500-phase CIL ImageNet task, which performs on a level identical to a 5-phase one.","Our codes are available at https://github.com/ZHUANGHP/Analytic-continual-learning."],"url":"http://arxiv.org/abs/2403.17503v1","category":"cs.LG"}
{"created":"2024-03-26 08:47:25","title":"A new method to access heavy meson lightcone distribution amplitudes from first-principle","abstract":"We present a method to compute lightcone distribution amplitudes (LCDAs) of heavy meson within heavy quark effective theory (HQET). Our method utilizes quasi distribution amplitudes (quasi-DAs) with a large momentum component $P^z$. We point out that by sequentially integrating out $P^z$ and $m_H$, one can disentangle different dynamical scales. Integrating out $P^z$ allows to connect quasi-DAs to QCD LCDAs, and then integrating out $m_H$ enables to relate QCD LCDAs to HQET LCDAs. To verify this proposal, we make use of lattice QCD simulation on a lattice ensemble with spacing $a = 0.05187$\\,fm. The preliminary findings for HQET LCDAs qualitatively align with phenomenological models. Using a recent model for HQET LCDAs, we also fit the first inverse moment $\\lambda_B^{-1}$ and the result is consistent with the experimentally constrain from $B \\to \\gamma \\ell\\nu_\\ell$. This agreement demonstrates the promise of our method in providing first-principle predictions for heavy meson LCDAs.","sentences":["We present a method to compute lightcone distribution amplitudes (LCDAs) of heavy meson within heavy quark effective theory (HQET).","Our method utilizes quasi distribution amplitudes (quasi-DAs) with a large momentum component $P^z$. We point out that by sequentially integrating out $P^z$ and $m_H$, one can disentangle different dynamical scales.","Integrating out $P^z$ allows to connect quasi-DAs to QCD LCDAs, and then integrating out $m_H$ enables to relate QCD LCDAs to HQET LCDAs.","To verify this proposal, we make use of lattice QCD simulation on a lattice ensemble with spacing $a = 0.05187$\\,fm.","The preliminary findings for HQET LCDAs qualitatively align with phenomenological models.","Using a recent model for HQET LCDAs, we also fit the first inverse moment $\\lambda_B^{-1}$ and the result is consistent with the experimentally constrain from $B \\to \\gamma \\ell\\nu_\\ell$. This agreement demonstrates the promise of our method in providing first-principle predictions for heavy meson LCDAs."],"url":"http://arxiv.org/abs/2403.17492v1","category":"hep-ph"}
{"created":"2024-03-26 08:22:09","title":"Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost","abstract":"An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time. Each job can be processed by at most one fixed speed server at any time. Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one. Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived.","sentences":["An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time.","Each job can be processed by at most one fixed speed server at any time.","Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one.","Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived."],"url":"http://arxiv.org/abs/2403.17480v1","category":"cs.DS"}
{"created":"2024-03-26 07:57:04","title":"Geometric planted matchings beyond the Gaussian model","abstract":"We consider the problem of recovering an unknown matching between a set of $n$ randomly placed points in $\\mathbb{R}^d$ and random perturbations of these points. This can be seen as a model for particle tracking and more generally, entity resolution. We use matchings in random geometric graphs to derive minimax lower bounds for this problem that hold under great generality. Using these results we show that for a broad class of distributions, the order of the number of mistakes made by an estimator that minimizes the sum of squared Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to $n^{o(1)}$ factors when $d = o(\\log n)$. In the high-dimensional regime we consider a setup where both initial positions and perturbations have independent sub-Gaussian coordinates. In this setup we give sufficient conditions under which the same estimator makes no mistakes with high probability. We prove an analogous result for an adapted version of this estimator that incorporates information on the covariance matrix of the perturbations.","sentences":["We consider the problem of recovering an unknown matching between a set of $n$ randomly placed points in $\\mathbb{R}^d$ and random perturbations of these points.","This can be seen as a model for particle tracking and more generally, entity resolution.","We use matchings in random geometric graphs to derive minimax lower bounds for this problem that hold under great generality.","Using these results we show that for a broad class of distributions, the order of the number of mistakes made by an estimator that minimizes the sum of squared Euclidean distances is minimax optimal when $d$ is fixed and is optimal up to $n^{o(1)}$ factors when $d = o(\\log n)$. In the high-dimensional regime we consider a setup where both initial positions and perturbations have independent sub-Gaussian coordinates.","In this setup we give sufficient conditions under which the same estimator makes no mistakes with high probability.","We prove an analogous result for an adapted version of this estimator that incorporates information on the covariance matrix of the perturbations."],"url":"http://arxiv.org/abs/2403.17469v1","category":"math.ST"}
{"created":"2024-03-26 07:54:23","title":"Weak solutions to Kolmogorov-Fokker-Planck equations: Regularity, existence and uniqueness","abstract":"In this article, we establish embeddings \\`a la Lions and transfer of regularity \\`a la Bouchut for a large scale of kinetic spaces. We use them to identify a notion of weak solutions to Kolmogorov-Fokker-Planck equations with (local or integral) diffusion and rough (measurable) coefficients under minimal requirements. We prove their existence and uniqueness for a large class of source terms, first in full space for the time, position and velocity variables and then for the kinetic Cauchy problem on infinite and finite time intervals.","sentences":["In this article, we establish embeddings \\`a la Lions and transfer of regularity \\`a la Bouchut for a large scale of kinetic spaces.","We use them to identify a notion of weak solutions to Kolmogorov-Fokker-Planck equations with (local or integral) diffusion and rough (measurable) coefficients under minimal requirements.","We prove their existence and uniqueness for a large class of source terms, first in full space for the time, position and velocity variables and then for the kinetic Cauchy problem on infinite and finite time intervals."],"url":"http://arxiv.org/abs/2403.17464v1","category":"math.AP"}
{"created":"2024-03-26 07:39:48","title":"Quadratic speed-ups in quantum kernelized binary classification","abstract":"Classification is at the core of data-driven prediction and decision-making, representing a fundamental task in supervised machine learning. Recently, several quantum machine learning algorithms that use quantum kernels as a measure of similarities between data have emerged to perform binary classification on datasets encoded as quantum states. The potential advantages of quantum kernels arise from the ability of quantum computers to construct kernels that are more effective than their classical counterparts in capturing patterns in data or computing kernels more efficiently. However, existing quantum kernel-based classification algorithms do not harness the capability of having data samples in quantum superposition for additional enhancements. In this work, we demonstrate how such capability can be leveraged in quantum kernelized binary classifiers (QKCs) through Quantum Amplitude Estimation (QAE) for quadratic speed-up. Additionally, we propose new quantum circuits for the QKCs in which the number of qubits is reduced by one, and the circuit depth is reduced linearly with respect to the number of sample data. We verify the quadratic speed-up over previous methods through numerical simulations on the Iris dataset.","sentences":["Classification is at the core of data-driven prediction and decision-making, representing a fundamental task in supervised machine learning.","Recently, several quantum machine learning algorithms that use quantum kernels as a measure of similarities between data have emerged to perform binary classification on datasets encoded as quantum states.","The potential advantages of quantum kernels arise from the ability of quantum computers to construct kernels that are more effective than their classical counterparts in capturing patterns in data or computing kernels more efficiently.","However, existing quantum kernel-based classification algorithms do not harness the capability of having data samples in quantum superposition for additional enhancements.","In this work, we demonstrate how such capability can be leveraged in quantum kernelized binary classifiers (QKCs) through Quantum Amplitude Estimation (QAE) for quadratic speed-up.","Additionally, we propose new quantum circuits for the QKCs in which the number of qubits is reduced by one, and the circuit depth is reduced linearly with respect to the number of sample data.","We verify the quadratic speed-up over previous methods through numerical simulations on the Iris dataset."],"url":"http://arxiv.org/abs/2403.17453v1","category":"quant-ph"}
{"created":"2024-03-26 07:31:15","title":"An inexact proximal MM method for a class of nonconvex composite image reconstruction models","abstract":"This paper concerns a class of composite image reconstruction models for impluse noise removal, which is rather general and covers existing convex and nonconvex models proposed for reconstructing images with impluse noise. For this nonconvex and nonsmooth optimization problem, we propose a proximal majorization-minimization (MM) algorithm with an implementable inexactness criterion by seeking in each step an inexact minimizer of a strongly convex majorization of the objective function, and establish the convergence of the iterate sequence under the KL assumption on the constructed potential function. This inexact proximal MM method is applied to handle gray image deblurring and color image inpainting problems, for which the associated potential function satisfy the required KL assumption. Numerical comparisons with two state-of-art solvers for image deblurring and inpainting tasks validate the efficiency of the proposed algorithm and models.","sentences":["This paper concerns a class of composite image reconstruction models for impluse noise removal, which is rather general and covers existing convex and nonconvex models proposed for reconstructing images with impluse noise.","For this nonconvex and nonsmooth optimization problem, we propose a proximal majorization-minimization (MM) algorithm with an implementable inexactness criterion by seeking in each step an inexact minimizer of a strongly convex majorization of the objective function, and establish the convergence of the iterate sequence under the KL assumption on the constructed potential function.","This inexact proximal MM method is applied to handle gray image deblurring and color image inpainting problems, for which the associated potential function satisfy the required KL assumption.","Numerical comparisons with two state-of-art solvers for image deblurring and inpainting tasks validate the efficiency of the proposed algorithm and models."],"url":"http://arxiv.org/abs/2403.17450v1","category":"math.OC"}
{"created":"2024-03-26 07:27:33","title":"Inclusive and diffractive dijet photoproduction in ultraperipheral Pb-Pb collisions at the LHC","abstract":"In this contribution, we summarize NLO pQCD predictions for inclusive and diffractive dijet photoproduction in Pb-Pb UPCs at the LHC. We demonstrate that the theory describes well the preliminary ATLAS data on the inclusive cross section, which probes nuclear parton distributions (PDFs) down to $x_A \\approx 0.005$ and which can reduce current uncertainties of the small-$x$ nuclear gluon distribution by approximately a factor of 2. Employing predictions of the leading twist approach to nuclear shadowing for nuclear diffractive PDFs, we calculate the cross section of diffractive dijet photoproduction and show that its $x_{\\gamma}$ dependence is sensitive to the effect of nuclear shadowing and the mechanism of QCD factorization breaking in hard diffraction. We also find that due to large leading twist nuclear shadowing and restricted kinematics, the diffractive contribution to the inclusive cross section of dijet photoproduction does not exceed $5-10$%, which helps with an ambiguous interpretation of the ATLAS data.","sentences":["In this contribution, we summarize NLO pQCD predictions for inclusive and diffractive dijet photoproduction in Pb-Pb UPCs at the LHC.","We demonstrate that the theory describes well the preliminary ATLAS data on the inclusive cross section, which probes nuclear parton distributions (PDFs) down to $x_A \\approx 0.005$ and which can reduce current uncertainties of the small-$x$ nuclear gluon distribution by approximately a factor of 2.","Employing predictions of the leading twist approach to nuclear shadowing for nuclear diffractive PDFs, we calculate the cross section of diffractive dijet photoproduction and show that its $x_{\\gamma}$ dependence is sensitive to the effect of nuclear shadowing and the mechanism of QCD factorization breaking in hard diffraction.","We also find that due to large leading twist nuclear shadowing and restricted kinematics, the diffractive contribution to the inclusive cross section of dijet photoproduction does not exceed $5-10$%, which helps with an ambiguous interpretation of the ATLAS data."],"url":"http://arxiv.org/abs/2403.17449v1","category":"hep-ph"}
{"created":"2024-03-26 06:50:07","title":"Propagation of initial uncertainties to Arthurs-Kelly inequality","abstract":"When the interaction Hamiltonian is $\\widehat{H}_I = \\kappa \\left(\\hat{x}_3 \\hat{p}_1 + \\hat{p}_3 \\hat{p_2} \\right)$ for large $\\kappa$ and the initial quantum state is $\\Psi_{in} (x_1, x_2, x_3) = \\prod_{j=1}^3 \\phi_j (x_j)$, the joint measurement of complementary variables $x_3$ and $p_3$ at $t = 1 / \\kappa$ induces the uncertainty $\\Delta_{\\hat{x}_1} (t = 1 / \\kappa) \\Delta_{ \\hat{x}_2} (t = 1 / \\kappa) \\geq \\frac{1}{2} (\\Delta_{\\hat{x}_1} \\Delta_{\\hat{p}_1} + \\Delta_{\\hat{x}_2} \\Delta_{\\hat{p}_2} ) + \\Delta_{\\hat{x}_3} \\Delta_{\\hat{p}_3}$ , where the standard deviations in the right hand are the deviations at $t=0$. This is a generalized version of the well-known Arthurs-Keller inequality $\\Delta_{\\hat{x}_1} (t = 1 / \\kappa) \\Delta_{ \\hat{x}_2} (t = 1 / \\kappa) \\geq 1$ arising when all $\\phi_j (x_j)$ are the minimal uncertainty Gaussian states. If the initial probe state is entangled, it is shown that the generalized Arthurs-Kelly inequality can be violated. We show the violation explicitly by introducing a special example.","sentences":["When the interaction Hamiltonian is $\\widehat{H}_I = \\kappa \\left(\\hat{x}_3 \\hat{p}_1 + \\hat{p}_3 \\hat{p_2} \\right)$ for large $\\kappa$ and the initial quantum state is $\\Psi_{in} (x_1, x_2, x_3) = \\prod_{j=1}^3 \\phi_j (x_j)$, the joint measurement of complementary variables $x_3$ and $p_3$ at $t = 1 / \\kappa$ induces the uncertainty $\\Delta_{\\hat{x}_1} (t = 1 / \\kappa) \\Delta_{ \\hat{x}_2} (t = 1 / \\kappa) \\geq \\frac{1}{2} (\\Delta_{\\hat{x}_1} \\Delta_{\\hat{p}_1} + \\Delta_{\\hat{x}_2} \\Delta_{\\hat{p}_2} )","+ \\Delta_{\\hat{x}_3} \\Delta_{\\hat{p}_3}$ , where the standard deviations in the right hand are the deviations at $t=0$. This is a generalized version of the well-known Arthurs-Keller inequality $\\Delta_{\\hat{x}_1} (t = 1 / \\kappa) \\Delta_{ \\hat{x}_2} (t = 1 / \\kappa) \\geq 1$ arising when all $\\phi_j (x_j)$ are the minimal uncertainty Gaussian states.","If the initial probe state is entangled, it is shown that the generalized Arthurs-Kelly inequality can be violated.","We show the violation explicitly by introducing a special example."],"url":"http://arxiv.org/abs/2403.17429v1","category":"quant-ph"}
{"created":"2024-03-26 06:40:03","title":"Test-time Adaptation Meets Image Enhancement: Improving Accuracy via Uncertainty-aware Logit Switching","abstract":"Deep neural networks have achieved remarkable success in a variety of computer vision applications. However, there is a problem of degrading accuracy when the data distribution shifts between training and testing. As a solution of this problem, Test-time Adaptation~(TTA) has been well studied because of its practicality. Although TTA methods increase accuracy under distribution shift by updating the model at test time, using high-uncertainty predictions is known to degrade accuracy. Since the input image is the root of the distribution shift, we incorporate a new perspective on enhancing the input image into TTA methods to reduce the prediction's uncertainty. We hypothesize that enhancing the input image reduces prediction's uncertainty and increase the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the classification model is combined with the image enhancement model that transforms input images into recognition-friendly ones, and these models are updated by existing TTA methods. Furthermore, we found that the prediction from the enhanced image does not always have lower uncertainty than the prediction from the original image. Thus, we propose logit switching, which compares the uncertainty measure of these predictions and outputs the lower one. In our experiments, we evaluate TECA with various TTA methods and show that TECA reduces prediction's uncertainty and increases accuracy of TTA methods despite having no hyperparameters and little parameter overhead.","sentences":["Deep neural networks have achieved remarkable success in a variety of computer vision applications.","However, there is a problem of degrading accuracy when the data distribution shifts between training and testing.","As a solution of this problem, Test-time Adaptation~(TTA) has been well studied because of its practicality.","Although TTA methods increase accuracy under distribution shift by updating the model at test time, using high-uncertainty predictions is known to degrade accuracy.","Since the input image is the root of the distribution shift, we incorporate a new perspective on enhancing the input image into TTA methods to reduce the prediction's uncertainty.","We hypothesize that enhancing the input image reduces prediction's uncertainty and increase the accuracy of TTA methods.","On the basis of our hypothesis, we propose a novel method: Test-time Enhancer and Classifier Adaptation~(TECA).","In TECA, the classification model is combined with the image enhancement model that transforms input images into recognition-friendly ones, and these models are updated by existing TTA methods.","Furthermore, we found that the prediction from the enhanced image does not always have lower uncertainty than the prediction from the original image.","Thus, we propose logit switching, which compares the uncertainty measure of these predictions and outputs the lower one.","In our experiments, we evaluate TECA with various TTA methods and show that TECA reduces prediction's uncertainty and increases accuracy of TTA methods despite having no hyperparameters and little parameter overhead."],"url":"http://arxiv.org/abs/2403.17423v1","category":"cs.CV"}
{"created":"2024-03-26 06:27:50","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge","abstract":"The goal of the multi-sound source localization task is to localize sound sources from the mixture individually. While recent multi-sound source localization methods have shown improved performance, they face challenges due to their reliance on prior information about the number of objects to be separated. In this paper, to overcome this limitation, we present a novel multi-sound source localization method that can perform localization without prior knowledge of the number of sound sources. To achieve this goal, we propose an iterative object identification (IOI) module, which can recognize sound-making objects in an iterative manner. After finding the regions of sound-making objects, we devise object similarity-aware clustering (OSC) loss to guide the IOI module to effectively combine regions of the same object but also distinguish between different objects and backgrounds. It enables our method to perform accurate localization of sound-making objects without any prior knowledge. Extensive experimental results on the MUSIC and VGGSound benchmarks show the significant performance improvements of the proposed method over the existing methods for both single and multi-source. Our code is available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL","sentences":["The goal of the multi-sound source localization task is to localize sound sources from the mixture individually.","While recent multi-sound source localization methods have shown improved performance, they face challenges due to their reliance on prior information about the number of objects to be separated.","In this paper, to overcome this limitation, we present a novel multi-sound source localization method that can perform localization without prior knowledge of the number of sound sources.","To achieve this goal, we propose an iterative object identification (IOI) module, which can recognize sound-making objects in an iterative manner.","After finding the regions of sound-making objects, we devise object similarity-aware clustering (OSC) loss to guide the IOI module to effectively combine regions of the same object but also distinguish between different objects and backgrounds.","It enables our method to perform accurate localization of sound-making objects without any prior knowledge.","Extensive experimental results on the MUSIC and VGGSound benchmarks show the significant performance improvements of the proposed method over the existing methods for both single and multi-source.","Our code is available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL"],"url":"http://arxiv.org/abs/2403.17420v1","category":"cs.MM"}
{"created":"2024-03-26 06:04:50","title":"Neural Clustering based Visual Representation Learning","abstract":"We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with clustering (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural clustering framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution. Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives. Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks. The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability. Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC. We expect this work will provoke a rethink of the current de facto grid-style paradigm.","sentences":["We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis.","Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions.","Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution.","In this work, we propose feature extraction with clustering (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural clustering framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution.","Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives.","Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks.","The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability.","Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC.","We expect this work will provoke a rethink of the current de facto grid-style paradigm."],"url":"http://arxiv.org/abs/2403.17409v1","category":"cs.CV"}
{"created":"2024-03-26 05:46:10","title":"Vortex nucleations in spinor Bose condensates under localized synthetic magnetic fields","abstract":"Gauge fields are ubiquitous in modern quantum physics. In superfluids, quantized vortices can be induced by gauge fields. Here we demonstrate the first experimental observation of vortex nucleations in spinor Bose-Einstein Condensates under radially-localized synthetic magnetic fields. The associated gauge potentials $\\vec{A}$ are azimuthal and created by light-induced spin-orbital-angular-momentum coupling, generating circulating azimuthal velocity fields $\\propto \\vec{p}-\\vec{A}$ even when the canonical momentum $\\vec{p}= 0$. A sufficiently large azimuthal velocity peaked near the condensate center results in a dynamically unstable localized excitation that initiates vortex nucleations. This excitation appears as a spontaneously-formed vortex-antivortex pair near the cloud center. Following the initially developed instability, the dynamics is governed by the asymmetry and dissipation, where the atomic orbital angular momentum evolves and can reach the value of the ground state. Our system exhibits dynamical and Landau instabilities and agrees reasonably with time-dependent Gross-Pitaevskii simulations.","sentences":["Gauge fields are ubiquitous in modern quantum physics.","In superfluids, quantized vortices can be induced by gauge fields.","Here we demonstrate the first experimental observation of vortex nucleations in spinor Bose-Einstein Condensates under radially-localized synthetic magnetic fields.","The associated gauge potentials $\\vec{A}$ are azimuthal and created by light-induced spin-orbital-angular-momentum coupling, generating circulating azimuthal velocity fields $\\propto \\vec{p}-\\vec{A}$ even when the canonical momentum $\\vec{p}= 0$.","A sufficiently large azimuthal velocity peaked near the condensate center results in a dynamically unstable localized excitation that initiates vortex nucleations.","This excitation appears as a spontaneously-formed vortex-antivortex pair near the cloud center.","Following the initially developed instability, the dynamics is governed by the asymmetry and dissipation, where the atomic orbital angular momentum evolves and can reach the value of the ground state.","Our system exhibits dynamical and Landau instabilities and agrees reasonably with time-dependent Gross-Pitaevskii simulations."],"url":"http://arxiv.org/abs/2403.17403v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-26 05:41:39","title":"Infrastructure-less Localization from Indoor Environmental Sounds Based on Spectral Decomposition and Spatial Likelihood Model","abstract":"Human and/or asset tracking using an attached sensor units helps understand their activities. Most common indoor localization methods for human tracking technologies require expensive infrastructures, deployment and maintenance. To overcome this problem, environmental sounds have been used for infrastructure-free localization. While they achieve room-level classification, they suffer from two problems: low signal-to-noise-ratio (SNR) condition and non-uniqueness of sound over the coverage area. A microphone localization method was proposed using supervised spectral decomposition and spatial likelihood to solve these problems. The proposed method was evaluated with actual recordings in an experimental room with a size of 12 x 30 m. The results showed that the proposed method with supervised NMF was robust under low-SNR condition compared to a simple feature (mel frequency cepstrum coefficient: MFCC). Additionally, the proposed method could be easily integrated with prior distribution, which is available from other Bayesian localizations. The proposed method can be used to evaluate the spatial likelihood from environmental sounds.","sentences":["Human and/or asset tracking using an attached sensor units helps understand their activities.","Most common indoor localization methods for human tracking technologies require expensive infrastructures, deployment and maintenance.","To overcome this problem, environmental sounds have been used for infrastructure-free localization.","While they achieve room-level classification, they suffer from two problems: low signal-to-noise-ratio (SNR) condition and non-uniqueness of sound over the coverage area.","A microphone localization method was proposed using supervised spectral decomposition and spatial likelihood to solve these problems.","The proposed method was evaluated with actual recordings in an experimental room with a size of 12 x 30 m. The results showed that the proposed method with supervised NMF was robust under low-SNR condition compared to a simple feature (mel frequency cepstrum coefficient: MFCC).","Additionally, the proposed method could be easily integrated with prior distribution, which is available from other Bayesian localizations.","The proposed method can be used to evaluate the spatial likelihood from environmental sounds."],"url":"http://arxiv.org/abs/2403.17402v1","category":"eess.AS"}
{"created":"2024-03-26 05:26:14","title":"Generic dimensional and dynamical properties of invariant measures of full-shift systems over countable alphabets","abstract":"In this work, we are interested in characterizing typical (generic) dimensional properties of invariant measures associated with the full-shift system, $T$, in a product space whose alphabet is a countable set. More specifically, we show that the set of invariant measures with infinite packing dimension equal to infinity is a dense $G_\\delta$ subset of $\\mathcal{M}(T)$, the space of $T$-invariant measures endowed with the weak topology, where the alphabet $M$ is a countable Polish metric space. We also show that the set of invariant measures with upper $q$-generalized fractal dimension (with $q>1$) equal to infinity is a dense $G_\\delta$ subset of $\\mathcal{M}(T)$, where the alphabet $M$ is a countable compact metric space. This improves the results obtained by Carvalho and Condori in \\cite{AS} and \\cite{AS2}, respectively. Furthermore, we discuss the dynamical consequences of such results, regarding the upper recurrence rates and upper quantitative waiting time indicator for typical orbits, and how the fractal dimensions of invariant measures and such dynamical quantities behave under an $\\alpha$-H\\\"older conjugation.","sentences":["In this work, we are interested in characterizing typical (generic) dimensional properties of invariant measures associated with the full-shift system, $T$, in a product space whose alphabet is a countable set.","More specifically, we show that the set of invariant measures with infinite packing dimension equal to infinity is a dense $G_\\delta$ subset of $\\mathcal{M}(T)$, the space of $T$-invariant measures endowed with the weak topology, where the alphabet $M$ is a countable Polish metric space.","We also show that the set of invariant measures with upper $q$-generalized fractal dimension (with $q>1$) equal to infinity is a dense $G_\\delta$ subset of $\\mathcal{M}(T)$, where the alphabet $M$ is a countable compact metric space.","This improves the results obtained by Carvalho and Condori in \\cite{AS} and \\cite{AS2}, respectively.","Furthermore, we discuss the dynamical consequences of such results, regarding the upper recurrence rates and upper quantitative waiting time indicator for typical orbits, and how the fractal dimensions of invariant measures and such dynamical quantities behave under an $\\alpha$-H\\\"older conjugation."],"url":"http://arxiv.org/abs/2403.17398v1","category":"math.DS"}
{"created":"2024-03-26 05:25:49","title":"On Abhyankar-Sathaye Conjecture for a family of linear hypersurfaces in $\\A_{k}^4$","abstract":"Let $k$ be a field. In this paper we study the Abhyankar-Sathaye Epimorphism Conjecture for certain hyperplanes in $\\A_k^4$ defined by a polynomial of the form $a(X)Y-F(X,Z,T)$. When $k=\\bC$, Kaliman, V\\'{e}n\\'{e}reau and Zaidenberg have proved that such hyperplanes are rectifiable in $\\A^4_{\\bC}$. We extend their result over any field of characteristic zero and when $k$ is a field of arbitrary characteristic we prove the Abhyankar-Sathaye Conjecture for such hyperplanes under some conditions on the roots of $a(X)$.","sentences":["Let $k$ be a field.","In this paper we study the Abhyankar-Sathaye Epimorphism Conjecture for certain hyperplanes in $\\A_k^4$ defined by a polynomial of the form $a(X)Y-F(X,Z,T)$.","When $k=\\bC$, Kaliman, V\\'{e}n\\'{e}reau and Zaidenberg have proved that such hyperplanes are rectifiable in $\\A^4_{\\bC}$. We extend their result over any field of characteristic zero and when $k$ is a field of arbitrary characteristic we prove the Abhyankar-Sathaye Conjecture for such hyperplanes under some conditions on the roots of $a(X)$."],"url":"http://arxiv.org/abs/2403.17397v1","category":"math.AG"}
{"created":"2024-03-26 05:19:15","title":"SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter","abstract":"SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework, which designed specifically for point cloud data. Leveraging the characteristics of non-coincidence and weak correlation of target objects in point cloud, we adopt a strategy of retaining only the truth-determining pseudo labels and trimming the other fuzzy labels with points, instead of pursuing a balance between the quantity and quality of pseudo labels. Besides, we notice that changing the filter will make the model meet different distributed targets, which is beneficial to break the training bottleneck. Two mechanism are introduced to achieve above ideas: strict threshold and filter switching. The experiments are conducted to analyze the effectiveness of above approaches and their impact on the overall performance of the system. Evaluating on the KITTI dataset, SSF3D exhibits superior performance compared to the current state-of-the-art methods. The code will be released here.","sentences":["SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework, which designed specifically for point cloud data.","Leveraging the characteristics of non-coincidence and weak correlation of target objects in point cloud, we adopt a strategy of retaining only the truth-determining pseudo labels and trimming the other fuzzy labels with points, instead of pursuing a balance between the quantity and quality of pseudo labels.","Besides, we notice that changing the filter will make the model meet different distributed targets, which is beneficial to break the training bottleneck.","Two mechanism are introduced to achieve above ideas: strict threshold and filter switching.","The experiments are conducted to analyze the effectiveness of above approaches and their impact on the overall performance of the system.","Evaluating on the KITTI dataset, SSF3D exhibits superior performance compared to the current state-of-the-art methods.","The code will be released here."],"url":"http://arxiv.org/abs/2403.17390v1","category":"cs.CV"}
{"created":"2024-03-26 05:14:50","title":"Quantum-Enhanced Simulation-Based Optimization for Newsvendor Problems","abstract":"Simulation-based optimization is a widely used method to solve stochastic optimization problems. This method aims to identify an optimal solution by maximizing the expected value of the objective function. However, due to its computational complexity, the function cannot be accurately evaluated directly, hence it is estimated through simulation. Exploiting the enhanced efficiency of Quantum Amplitude Estimation (QAE) compared to classical Monte Carlo simulation, it frequently outpaces classical simulation-based optimization, resulting in notable performance enhancements in various scenarios. In this work, we make use of a quantum-enhanced algorithm for simulation-based optimization and apply it to solve a variant of the classical Newsvendor problem which is known to be NP-hard. Such problems provide the building block for supply chain management, particularly in inventory management and procurement optimization under risks and uncertainty","sentences":["Simulation-based optimization is a widely used method to solve stochastic optimization problems.","This method aims to identify an optimal solution by maximizing the expected value of the objective function.","However, due to its computational complexity, the function cannot be accurately evaluated directly, hence it is estimated through simulation.","Exploiting the enhanced efficiency of Quantum Amplitude Estimation (QAE) compared to classical Monte Carlo simulation, it frequently outpaces classical simulation-based optimization, resulting in notable performance enhancements in various scenarios.","In this work, we make use of a quantum-enhanced algorithm for simulation-based optimization and apply it to solve a variant of the classical Newsvendor problem which is known to be NP-hard.","Such problems provide the building block for supply chain management, particularly in inventory management and procurement optimization under risks and uncertainty"],"url":"http://arxiv.org/abs/2403.17389v1","category":"quant-ph"}
{"created":"2024-03-26 05:01:53","title":"Characterizing Dependency Update Practice of NPM, PyPI and Cargo Packages","abstract":"Keeping dependencies up-to-date prevents software supply chain attacks through outdated and vulnerable dependencies. Developers may use packages' dependency update practice as one of the selection criteria for choosing a package as a dependency. However, the lack of metrics characterizing packages' dependency update practice makes this assessment difficult. To measure the up-to-date characteristics of packages, we focus on the dependency management aspect and propose two update metrics: Time-Out-Of-Date (TOOD) and Post-Fix-Exposure-Time (PFET), to measure the updatedness of dependencies and updatedness of vulnerable dependencies, respectively. We design an algorithm to stabilize the dependency relationships in different time intervals and compute the proposed metrics for each package. Using our proposed metrics, we conduct a large-scale empirical study of update metrics with 2.9M packages, 66.8M package versions, and 26.8M unique package-dependency relations in NPM, PyPI, and Cargo, ranging from the year 2004 to 2023. We analyze the characteristics of the proposed metrics for capturing packages' dependency update practice in the three ecosystems. Given that the TOOD metric generates a greater volume of data than the PFET metric, we further explore the numerical relationship between these metrics to assess their potential as substitutes for vulnerability counts metrics. We find that PyPI packages update dependencies faster than NPM and Cargo. Conversely, Cargo packages update their vulnerable dependencies faster than NPM and PyPI. We also find that the general purpose update metric, TOOD, can be a proxy for the security-focused update metric, PFET.","sentences":["Keeping dependencies up-to-date prevents software supply chain attacks through outdated and vulnerable dependencies.","Developers may use packages' dependency update practice as one of the selection criteria for choosing a package as a dependency.","However, the lack of metrics characterizing packages' dependency update practice makes this assessment difficult.","To measure the up-to-date characteristics of packages, we focus on the dependency management aspect and propose two update metrics: Time-Out-Of-Date (TOOD) and Post-Fix-Exposure-Time (PFET), to measure the updatedness of dependencies and updatedness of vulnerable dependencies, respectively.","We design an algorithm to stabilize the dependency relationships in different time intervals and compute the proposed metrics for each package.","Using our proposed metrics, we conduct a large-scale empirical study of update metrics with 2.9M packages, 66.8M package versions, and 26.8M unique package-dependency relations in NPM, PyPI, and Cargo, ranging from the year 2004 to 2023.","We analyze the characteristics of the proposed metrics for capturing packages' dependency update practice in the three ecosystems.","Given that the TOOD metric generates a greater volume of data than the PFET metric, we further explore the numerical relationship between these metrics to assess their potential as substitutes for vulnerability counts metrics.","We find that PyPI packages update dependencies faster than NPM and Cargo.","Conversely, Cargo packages update their vulnerable dependencies faster than NPM and PyPI.","We also find that the general purpose update metric, TOOD, can be a proxy for the security-focused update metric, PFET."],"url":"http://arxiv.org/abs/2403.17382v1","category":"cs.SE"}
{"created":"2024-03-26 04:09:08","title":"CoDA: Instructive Chain-of-Domain Adaptation with Severity-Aware Visual Prompt Tuning","abstract":"Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains. When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes. To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels. Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism. CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations. Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance. SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images. Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture. CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes. Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively. Our code is available at https://github.com/Cuzyoung/CoDA","sentences":["Unsupervised Domain Adaptation (UDA) aims to adapt models from labeled source domains to unlabeled target domains.","When adapting to adverse scenes, existing UDA methods fail to perform well due to the lack of instructions, leading their models to overlook discrepancies within all adverse scenes.","To tackle this, we propose CoDA which instructs models to distinguish, focus, and learn from these discrepancies at scene and image levels.","Specifically, CoDA consists of a Chain-of-Domain (CoD) strategy and a Severity-Aware Visual Prompt Tuning (SAVPT) mechanism.","CoD focuses on scene-level instructions to divide all adverse scenes into easy and hard scenes, guiding models to adapt from source to easy domains with easy scene images, and then to hard domains with hard scene images, thereby laying a solid foundation for whole adaptations.","Building upon this foundation, we employ SAVPT to dive into more detailed image-level instructions to boost performance.","SAVPT features a novel metric Severity that divides all adverse scene images into low-severity and high-severity images.","Then Severity directs visual prompts and adapters, instructing models to concentrate on unified severity features instead of scene-specific features, without adding complexity to the model architecture.","CoDA achieves SOTA performances on widely-used benchmarks under all adverse scenes.","Notably, CoDA outperforms the existing ones by 4.6%, and 10.3% mIoU on the Foggy Driving, and Foggy Zurich benchmarks, respectively.","Our code is available at https://github.com/Cuzyoung/CoDA"],"url":"http://arxiv.org/abs/2403.17369v1","category":"cs.CV"}
{"created":"2024-03-26 03:29:42","title":"Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network","abstract":"Under circumstances of heterophily, where nodes with different labels tend to be connected based on semantic meanings, Graph Neural Networks (GNNs) often exhibit suboptimal performance. Current studies on graph heterophily mainly focus on aggregation calibration or neighbor extension and address the heterophily issue by utilizing node features or structural information to improve GNN representations. In this paper, we propose and demonstrate that the valuable semantic information inherent in heterophily can be utilized effectively in graph learning by investigating the distribution of neighbors for each individual node within the graph. The theoretical analysis is carried out to demonstrate the efficacy of the idea in enhancing graph learning. Based on this analysis, we propose HiGNN, an innovative approach that constructs an additional new graph structure, that integrates heterophilous information by leveraging node distribution to enhance connectivity between nodes that share similar semantic characteristics. We conduct empirical assessments on node classification tasks using both homophilous and heterophilous benchmark datasets and compare HiGNN to popular GNN baselines and SoTA methods, confirming the effectiveness in improving graph representations. In addition, by incorporating heterophilous information, we demonstrate a notable enhancement in existing GNN-based approaches, and the homophily degree across real-world datasets, thus affirming the efficacy of our approach.","sentences":["Under circumstances of heterophily, where nodes with different labels tend to be connected based on semantic meanings, Graph Neural Networks (GNNs) often exhibit suboptimal performance.","Current studies on graph heterophily mainly focus on aggregation calibration or neighbor extension and address the heterophily issue by utilizing node features or structural information to improve GNN representations.","In this paper, we propose and demonstrate that the valuable semantic information inherent in heterophily can be utilized effectively in graph learning by investigating the distribution of neighbors for each individual node within the graph.","The theoretical analysis is carried out to demonstrate the efficacy of the idea in enhancing graph learning.","Based on this analysis, we propose HiGNN, an innovative approach that constructs an additional new graph structure, that integrates heterophilous information by leveraging node distribution to enhance connectivity between nodes that share similar semantic characteristics.","We conduct empirical assessments on node classification tasks using both homophilous and heterophilous benchmark datasets and compare HiGNN to popular GNN baselines and SoTA methods, confirming the effectiveness in improving graph representations.","In addition, by incorporating heterophilous information, we demonstrate a notable enhancement in existing GNN-based approaches, and the homophily degree across real-world datasets, thus affirming the efficacy of our approach."],"url":"http://arxiv.org/abs/2403.17351v1","category":"cs.LG"}
{"created":"2024-03-26 02:34:48","title":"OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation","abstract":"Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques. In particular, we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate multi-modal information along the tour. Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.","sentences":["Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes.","Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision.","Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques.","In particular, we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals.","Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training.","To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate multi-modal information along the tour.","Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action.","In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework.","We demonstrate the superiority of OVER-NAV in extensive experiments."],"url":"http://arxiv.org/abs/2403.17334v1","category":"cs.CV"}
{"created":"2024-03-26 02:28:49","title":"Staircase Localization for Autonomous Exploration in Urban Environments","abstract":"A staircase localization method is proposed for robots to explore urban environments autonomously. The proposed method employs a modular design in the form of a cascade pipeline consisting of three modules of stair detection, line segment detection, and stair localization modules. The stair detection module utilizes an object detection algorithm based on deep learning to generate a region of interest (ROI). From the ROI, line segment features are extracted using a deep line segment detection algorithm. The extracted line segments are used to localize a staircase in terms of position, orientation, and stair direction. The stair detection and localization are performed only with a single RGB-D camera. Each component of the proposed pipeline does not need to be designed particularly for staircases, which makes it easy to maintain the whole pipeline and replace each component with state-of-the-art deep learning detection techniques. The results of real-world experiments show that the proposed method can perform accurate stair detection and localization during autonomous exploration for various structured and unstructured upstairs and downstairs with shadows, dirt, and occlusions by artificial and natural objects.","sentences":["A staircase localization method is proposed for robots to explore urban environments autonomously.","The proposed method employs a modular design in the form of a cascade pipeline consisting of three modules of stair detection, line segment detection, and stair localization modules.","The stair detection module utilizes an object detection algorithm based on deep learning to generate a region of interest (ROI).","From the ROI, line segment features are extracted using a deep line segment detection algorithm.","The extracted line segments are used to localize a staircase in terms of position, orientation, and stair direction.","The stair detection and localization are performed only with a single RGB-D camera.","Each component of the proposed pipeline does not need to be designed particularly for staircases, which makes it easy to maintain the whole pipeline and replace each component with state-of-the-art deep learning detection techniques.","The results of real-world experiments show that the proposed method can perform accurate stair detection and localization during autonomous exploration for various structured and unstructured upstairs and downstairs with shadows, dirt, and occlusions by artificial and natural objects."],"url":"http://arxiv.org/abs/2403.17330v1","category":"cs.CV"}
{"created":"2024-03-26 02:02:35","title":"Leveraging Symmetry in RL-based Legged Locomotion Control","abstract":"Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information of the robot's kinematics and dynamics morphology. The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal. This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware. To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance/invariance constraints. In this paper, we investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant/invariant, and leveraging data augmentation to approximate equivariant/invariant actor-critics. We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline. We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation. In addition, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot in real-world experiments.","sentences":["Model-free reinforcement learning is a promising approach for autonomously solving challenging robotics control problems, but faces exploration difficulty without information of the robot's kinematics and dynamics morphology.","The under-exploration of multiple modalities with symmetric states leads to behaviors that are often unnatural and sub-optimal.","This issue becomes particularly pronounced in the context of robotic systems with morphological symmetries, such as legged robots for which the resulting asymmetric and aperiodic behaviors compromise performance, robustness, and transferability to real hardware.","To mitigate this challenge, we can leverage symmetry to guide and improve the exploration in policy learning via equivariance/invariance constraints.","In this paper, we investigate the efficacy of two approaches to incorporate symmetry: modifying the network architectures to be strictly equivariant/invariant, and leveraging data augmentation to approximate equivariant/invariant actor-critics.","We implement the methods on challenging loco-manipulation and bipedal locomotion tasks and compare with an unconstrained baseline.","We find that the strictly equivariant policy consistently outperforms other methods in sample efficiency and task performance in simulation.","In addition, symmetry-incorporated approaches exhibit better gait quality, higher robustness and can be deployed zero-shot in real-world experiments."],"url":"http://arxiv.org/abs/2403.17320v1","category":"cs.RO"}
{"created":"2024-03-26 01:57:11","title":"Statistical analysis and method to propagate the impact of measurement uncertainty on dynamic mode decomposition","abstract":"We apply random matrix theory to study the impact of measurement uncertainty on dynamic mode decomposition. Specifically, when the measurements follow a normal probability density function, we show how the moments of that density propagate through the dynamic mode decomposition. While we focus on the first and second moments, the analytical expressions we derive are general and can be extended to higher-order moments. Further, the proposed numerical method to propagate uncertainty is agnostic of specific dynamic mode decomposition formulations. Of particular relevance, the estimated second moments provide confidence bounds that may be used as a metric of trustworthiness, that is, how much one can rely on a finite-dimensional linear operator to represent an underlying dynamical system. We perform numerical experiments on two canonical systems and verify the estimated confidence levels by comparing the moments to those obtained from Monte Carlo simulations.","sentences":["We apply random matrix theory to study the impact of measurement uncertainty on dynamic mode decomposition.","Specifically, when the measurements follow a normal probability density function, we show how the moments of that density propagate through the dynamic mode decomposition.","While we focus on the first and second moments, the analytical expressions we derive are general and can be extended to higher-order moments.","Further, the proposed numerical method to propagate uncertainty is agnostic of specific dynamic mode decomposition formulations.","Of particular relevance, the estimated second moments provide confidence bounds that may be used as a metric of trustworthiness, that is, how much one can rely on a finite-dimensional linear operator to represent an underlying dynamical system.","We perform numerical experiments on two canonical systems and verify the estimated confidence levels by comparing the moments to those obtained from Monte Carlo simulations."],"url":"http://arxiv.org/abs/2403.17318v1","category":"stat.ME"}
{"created":"2024-03-26 01:15:50","title":"The Brenier-Schr\u00f6dinger problem with respect to Feller semimartingales and non-local Hamilton-Jacobi-Bellman equations","abstract":"Motivated by a problem from incompressible fluid mechanics of Brenier (JAMS 1989), and its recent entropic relaxation by Arnaudo, Cruizero, L\\'eonard & Zambrini (AIHP PS 2020), we study a problem of entropic minimization on the path space when the reference measure is a generic Feller semimartingale. We show that, under some regularity condition, our problem connects naturally with a, possibly non-local, version of the Hamilton-Jacobi-Bellman equation. Additionally, we study existence of minimizers when the reference measure in a Ornstein-Uhlenbeck process.","sentences":["Motivated by a problem from incompressible fluid mechanics of Brenier (JAMS 1989), and its recent entropic relaxation by Arnaudo, Cruizero, L\\'eonard & Zambrini (AIHP PS 2020), we study a problem of entropic minimization on the path space when the reference measure is a generic Feller semimartingale.","We show that, under some regularity condition, our problem connects naturally with a, possibly non-local, version of the Hamilton-Jacobi-Bellman equation.","Additionally, we study existence of minimizers when the reference measure in a Ornstein-Uhlenbeck process."],"url":"http://arxiv.org/abs/2403.17305v1","category":"math.PR"}
{"created":"2024-03-26 01:15:31","title":"Alfv\u00e9n Waves at Mars","abstract":"The solar wind upstream of Mars's bow shock can be described in terms of Alfv\\'enic turbulence, with an incompressible energy cascade rate of $10^{-17}$ J m$^{-3}$ s$^{-1}$ at magnetohydrodynamics (MHD) scales. The solar wind has more Alfv\\'en waves propagating outwards from the Sun (than inwards) and a median Alfv\\'en ratio of $\\sim0.33$. Newly ionized planetary protons associated with the extended hydrogen corona generate waves at the local proton cyclotron frequency. These 'proton cyclotron waves' (PCW) mostly correspond to fast magnetosonic waves, although the ion cyclotron (Alfv\\'enic) wave mode is possible for large Interplanetary Magnetic Field cone angles. PCW do not show significant effects on the solar wind energy cascade rates at MHD scales but could affect smaller scales. The magnetosheath displays high amplitude wave activity, with high occurrence rate of Alfv\\'en waves. Turbulence appears not fully developed in the magnetosheath, suggesting fluctuations do not have enough time to interact in this small-size region. Some studies suggest PCW affect turbulence in the magnetosheath. Overall, wave activity is reduced inside the magnetic pile-up region and the Martian ionosphere. However, under certain conditions, upstream waves can reach the upper ionosphere. So far, there have not been conclusive observations of Alfv\\'en waves in the ionosphere or along crustal magnetic fields, which could be due to the lack of adequate observations.","sentences":["The solar wind upstream of Mars's bow shock can be described in terms of Alfv\\'enic turbulence, with an incompressible energy cascade rate of $10^{-17}$ J m$^{-3}$ s$^{-1}$ at magnetohydrodynamics (MHD) scales.","The solar wind has more Alfv\\'en waves propagating outwards from the Sun (than inwards) and a median Alfv\\'en ratio of $\\sim0.33$. Newly ionized planetary protons associated with the extended hydrogen corona generate waves at the local proton cyclotron frequency.","These 'proton cyclotron waves' (PCW) mostly correspond to fast magnetosonic waves, although the ion cyclotron (Alfv\\'enic) wave mode is possible for large Interplanetary Magnetic Field cone angles.","PCW do not show significant effects on the solar wind energy cascade rates at MHD scales but could affect smaller scales.","The magnetosheath displays high amplitude wave activity, with high occurrence rate of Alfv\\'en waves.","Turbulence appears not fully developed in the magnetosheath, suggesting fluctuations do not have enough time to interact in this small-size region.","Some studies suggest PCW affect turbulence in the magnetosheath.","Overall, wave activity is reduced inside the magnetic pile-up region and the Martian ionosphere.","However, under certain conditions, upstream waves can reach the upper ionosphere.","So far, there have not been conclusive observations of Alfv\\'en waves in the ionosphere or along crustal magnetic fields, which could be due to the lack of adequate observations."],"url":"http://arxiv.org/abs/2403.17304v1","category":"astro-ph.SR"}
{"created":"2024-03-26 01:06:47","title":"Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving","abstract":"Deep learning-based monocular depth estimation (MDE), extensively applied in autonomous driving, is known to be vulnerable to adversarial attacks. Previous physical attacks against MDE models rely on 2D adversarial patches, so they only affect a small, localized region in the MDE map but fail under various viewpoints. To address these limitations, we propose 3D Depth Fool (3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models. 3D$^2$Fool is specifically optimized to generate 3D adversarial textures agnostic to model types of vehicles and to have improved robustness in bad weather conditions, such as rain and fog. Experimental results validate the superior performance of our 3D$^2$Fool across various scenarios, including vehicles, MDE models, weather conditions, and viewpoints. Real-world experiments with printed 3D textures on physical vehicle models further demonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters.","sentences":["Deep learning-based monocular depth estimation (MDE), extensively applied in autonomous driving, is known to be vulnerable to adversarial attacks.","Previous physical attacks against MDE models rely on 2D adversarial patches, so they only affect a small, localized region in the MDE map but fail under various viewpoints.","To address these limitations, we propose 3D Depth Fool (3D$^2$Fool), the first 3D texture-based adversarial attack against MDE models.","3D$^2$Fool is specifically optimized to generate 3D adversarial textures agnostic to model types of vehicles and to have improved robustness in bad weather conditions, such as rain and fog.","Experimental results validate the superior performance of our 3D$^2$Fool across various scenarios, including vehicles, MDE models, weather conditions, and viewpoints.","Real-world experiments with printed 3D textures on physical vehicle models further demonstrate that our 3D$^2$Fool can cause an MDE error of over 10 meters."],"url":"http://arxiv.org/abs/2403.17301v1","category":"cs.CV"}
{"created":"2024-03-26 00:55:45","title":"Utilizing (Al, Ga)2O3/Ga2O3 superlattices to measure cation vacancy diffusion and vacancy-concentration-dependent diffusion of Al, Sn, and Fe in \\b{eta} -Ga2O3","abstract":"Diffusion of native defects such as vacancies and their interactions with impurities are fundamental in semiconductor crystal growth, device processing, and long-term aging of equilibration and transient diffusion of vacancies are rarely investigated. We used aluminum-gallium oxide/gallium oxide superlattices (SLs) to detect and analyze transient diffusion of cation vacancies during annealing in O2 at 1000-1100 C. Using a novel finite difference scheme for the diffusion equation with time- and space-varying diffusion constant, we extract diffusion constants for Al, Fe, and cation vacancies under the given conditions, including the vacancy concentration dependence for Al. indicate that vacancies present in the substrate transiently diffuse through the SLs, interacting with Sn as it also diffuses. In the case of SLs grown on Sn-doped beta-gallium oxide substrates, gradients observed in the extent of Al diffusion indicate that vacancies present in the substrate transiently diffuse through the SLs, interacting with Sn as it also diffuses. In the case of SLs grown on (010) Fe-doped substrates, the Al diffusion is uniform through the SLs, indicating a depth-uniform concentration of vacancies. We find no evidence in either case for the introduction of gallium vacancies from the free surface at rates sufficient to affect Al diffusion down to ppm concentrations, which has important bearing on the validity of typically-made assumptions of vacancy equilibration. Additionally, we show that unintentional impurities in Sn-doped gallium oxide such as Fe, Ni, Mn, Cu, and Li also diffuse towards the surface and accumulate. Many of these likely have fast interstitial diffusion modes capable of destabilizing devices over time, thus highlighting the importance of controlling unintentional impurities in beta-gallium oxide wafers.","sentences":["Diffusion of native defects such as vacancies and their interactions with impurities are fundamental in semiconductor crystal growth, device processing, and long-term aging of equilibration and transient diffusion of vacancies are rarely investigated.","We used aluminum-gallium oxide/gallium oxide superlattices (SLs) to detect and analyze transient diffusion of cation vacancies during annealing in O2 at 1000-1100 C. Using a novel finite difference scheme for the diffusion equation with time- and space-varying diffusion constant, we extract diffusion constants for Al, Fe, and cation vacancies under the given conditions, including the vacancy concentration dependence for Al. indicate that vacancies present in the substrate transiently diffuse through the SLs, interacting with Sn as it also diffuses.","In the case of SLs grown on Sn-doped beta-gallium oxide substrates, gradients observed in the extent of Al diffusion indicate that vacancies present in the substrate transiently diffuse through the SLs, interacting with Sn as it also diffuses.","In the case of SLs grown on (010) Fe-doped substrates, the Al diffusion is uniform through the SLs, indicating a depth-uniform concentration of vacancies.","We find no evidence in either case for the introduction of gallium vacancies from the free surface at rates sufficient to affect Al diffusion down to ppm concentrations, which has important bearing on the validity of typically-made assumptions of vacancy equilibration.","Additionally, we show that unintentional impurities in Sn-doped gallium oxide such as Fe, Ni, Mn, Cu, and Li also diffuse towards the surface and accumulate.","Many of these likely have fast interstitial diffusion modes capable of destabilizing devices over time, thus highlighting the importance of controlling unintentional impurities in beta-gallium oxide wafers."],"url":"http://arxiv.org/abs/2403.17298v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 00:25:32","title":"An Analysis of Switchback Designs in Reinforcement Learning","abstract":"This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time. Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators. We propose a novel \"weak signal analysis\" framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in Markov decision process environments. Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis. Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator. (ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent. (iii) In cases where the majority of errors are negative correlated, the alternating-day design becomes the optimal choice. These insights are crucial, offering guidelines for practitioners on designing experiments in A/B testing. Our analysis accommodates a variety of policy value estimators, including model-based estimators, least squares temporal difference learning estimators, and double reinforcement learning estimators, thereby offering a comprehensive understanding of optimal design strategies for policy evaluation in reinforcement learning.","sentences":["This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time.","Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators.","We propose a novel \"weak signal analysis\" framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in Markov decision process environments.","Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis.","Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator.","(ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent.","(iii)","In cases where the majority of errors are negative correlated, the alternating-day design becomes the optimal choice.","These insights are crucial, offering guidelines for practitioners on designing experiments in A/B testing.","Our analysis accommodates a variety of policy value estimators, including model-based estimators, least squares temporal difference learning estimators, and double reinforcement learning estimators, thereby offering a comprehensive understanding of optimal design strategies for policy evaluation in reinforcement learning."],"url":"http://arxiv.org/abs/2403.17285v1","category":"stat.ML"}
{"created":"2024-03-26 00:25:01","title":"Common Ground Tracking in Multimodal Dialogue","abstract":"Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on ``dialogue state tracking'' (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is ``common ground tracking'' (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and ``questions under discussion'' (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep neural model to predict moves toward construction of common ground. Model outputs cascade into a set of formal closure rules derived from situated evidence and belief axioms and update operations. We empirically assess the contribution of each feature type toward successful construction of common ground relative to ground truth, establishing a benchmark in this novel, challenging task.","sentences":["Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on ``dialogue state tracking'' (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history.","Less studied but just as important to dialogue modeling, however, is ``common ground tracking'' (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true.","In this paper we present a method for automatically identifying the current set of shared beliefs and ``questions under discussion'' (QUDs) of a group with a shared goal.","We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep neural model to predict moves toward construction of common ground.","Model outputs cascade into a set of formal closure rules derived from situated evidence and belief axioms and update operations.","We empirically assess the contribution of each feature type toward successful construction of common ground relative to ground truth, establishing a benchmark in this novel, challenging task."],"url":"http://arxiv.org/abs/2403.17284v1","category":"cs.CL"}
{"created":"2024-03-25 23:47:55","title":"Relational Network Verification","abstract":"Relational network verification is a new approach to validating network changes. In contrast to traditional network verification, which analyzes specifications for a single network snapshot, relational network verification analyzes specifications concerning two network snapshots (e.g., pre- and post-change snapshots) and captures their similarities and differences. Relational change specifications are compact and precise because they specify the flows or paths that change between snapshots and then simply mandate that other behaviors of the network \"stay the same\", without enumerating them. To achieve similar guarantees, single-snapshot specifications need to enumerate all flow and path behaviors that are not expected to change, so we can check that nothing has accidentally changed. Thus, precise single-snapshot specifications are proportional to network size, which makes them impractical to generate for many real-world networks.   To demonstrate the value of relational reasoning, we develop a high-level relational specification language and a tool called Rela to validate network changes. Rela first compiles input specifications and network snapshot representations to finite state transducers. It then checks compliance using decision procedures for automaton equivalence. Our experiments using data on complex changes to a global backbone (with over 10^3 routers) find that Rela specifications need fewer than 10 terms for 93% of them and it validates 80% of them within 20 minutes.","sentences":["Relational network verification is a new approach to validating network changes.","In contrast to traditional network verification, which analyzes specifications for a single network snapshot, relational network verification analyzes specifications concerning two network snapshots (e.g., pre- and post-change snapshots) and captures their similarities and differences.","Relational change specifications are compact and precise because they specify the flows or paths that change between snapshots and then simply mandate that other behaviors of the network \"stay the same\", without enumerating them.","To achieve similar guarantees, single-snapshot specifications need to enumerate all flow and path behaviors that are not expected to change, so we can check that nothing has accidentally changed.","Thus, precise single-snapshot specifications are proportional to network size, which makes them impractical to generate for many real-world networks.   ","To demonstrate the value of relational reasoning, we develop a high-level relational specification language and a tool called Rela to validate network changes.","Rela first compiles input specifications and network snapshot representations to finite state transducers.","It then checks compliance using decision procedures for automaton equivalence.","Our experiments using data on complex changes to a global backbone (with over 10^3 routers) find that Rela specifications need fewer than 10 terms for 93% of them and it validates 80% of them within 20 minutes."],"url":"http://arxiv.org/abs/2403.17277v1","category":"cs.NI"}
{"created":"2024-03-25 23:46:58","title":"Teaming-up radio and submm-FIR observations to probe dusty star-forming galaxies","abstract":"In this paper, we investigate the benefits of teaming up data from the radio to the far- 1 infrared (FIR) regime for the characterization of Dusty Star-Forming Galaxies (DSFGs). These galaxies 2 are thought to be the star-forming progenitors of local massive quiescent galaxies, and play a pivotal 3 role in the reconstruction of the cosmic star formation rate density up to high redshift. Due to their 4 dust-enshrouded nature, DSFGs are often invisible in the near-infrared/Optical/UV bands. Therefore, 5 they necessitate observations at longer wavelengths, primarily the FIR, where dust emission occurs, 6 and radio, which is not affected by dust absorption. Combining data from these two spectral windows 7 makes it possible to characterize even the dustiest objects, enabling the retrieval of information about 8 their age, dust temperature, and star-formation status, and facilitates the differentiation between 9 various galaxy populations that evolve throughout cosmic history. Despite the detection of faint radio 10 sources being a challenging task, this study demonstrates that an effective strategy to build statistically 11 relevant samples of DSFGs would be reaching deep sensitivities in the radio band, even restricted to 12 smaller areas, and then combining these radio observations with FIR/submm data. Additionally, 13 the paper quantifies the improvement in the Spectral Energy Distribution (SED) reconstruction of 14 DSFGs by incorporating ALMA band measurements, in particular, in its upgraded status thanks to 15 the anticipated Wideband Sensitivity Upgrade.","sentences":["In this paper, we investigate the benefits of teaming up data from the radio to the far- 1 infrared (FIR) regime for the characterization of Dusty Star-Forming Galaxies (DSFGs).","These galaxies 2 are thought to be the star-forming progenitors of local massive quiescent galaxies, and play a pivotal 3 role in the reconstruction of the cosmic star formation rate density up to high redshift.","Due to their 4 dust-enshrouded nature, DSFGs are often invisible in the near-infrared/Optical/UV bands.","Therefore, 5 they necessitate observations at longer wavelengths, primarily the FIR, where dust emission occurs, 6 and radio, which is not affected by dust absorption.","Combining data from these two spectral windows 7 makes it possible to characterize even the dustiest objects, enabling the retrieval of information about 8 their age, dust temperature, and star-formation status, and facilitates the differentiation between 9 various galaxy populations that evolve throughout cosmic history.","Despite the detection of faint radio 10 sources being a challenging task, this study demonstrates that an effective strategy to build statistically 11 relevant samples of DSFGs would be reaching deep sensitivities in the radio band, even restricted to 12 smaller areas, and then combining these radio observations with FIR/submm data.","Additionally, 13 the paper quantifies the improvement in the Spectral Energy Distribution (SED) reconstruction of 14 DSFGs by incorporating ALMA band measurements, in particular, in its upgraded status thanks to 15 the anticipated Wideband Sensitivity Upgrade."],"url":"http://arxiv.org/abs/2403.17276v1","category":"astro-ph.GA"}
{"created":"2024-03-25 23:35:04","title":"Exact block encoding of imaginary time evolution with universal quantum neural networks","abstract":"We develop a constructive approach to generate quantum neural networks capable of representing the exact thermal states of all many-body qubit Hamiltonians. The Trotter expansion of the imaginary-time propagator is implemented through an exact block encoding by means of a unitary, restricted Boltzmann machine architecture. Marginalization over the hidden-layer neurons (auxiliary qubits) creates the non-unitary action on the visible layer. Then, we introduce a unitary deep Boltzmann machine architecture, in which the hidden-layer qubits are allowed to couple laterally to other hidden qubits. We prove that this wave function ansatz is closed under the action of the imaginary-time propagator and, more generally, can represent the action of a universal set of quantum gate operations. We provide analytic expressions for the coefficients for both architectures, thus enabling exact network representations of thermal states without stochastic optimization of the network parameters. In the limit of large imaginary time, the ansatz yields the ground state of the system. The number of qubits grows linearly with the system size and total imaginary time for a fixed interaction order. Both networks can be readily implemented on quantum hardware via mid-circuit measurements of auxiliary qubits. If only one auxiliary qubit is measured and reset, the circuit depth scales linearly with imaginary time and system size, while the width is constant. Alternatively, one can employ a number of auxiliary qubits linearly proportional to the system size, and circuit depth grows linearly with imaginary time only.","sentences":["We develop a constructive approach to generate quantum neural networks capable of representing the exact thermal states of all many-body qubit Hamiltonians.","The Trotter expansion of the imaginary-time propagator is implemented through an exact block encoding by means of a unitary, restricted Boltzmann machine architecture.","Marginalization over the hidden-layer neurons (auxiliary qubits) creates the non-unitary action on the visible layer.","Then, we introduce a unitary deep Boltzmann machine architecture, in which the hidden-layer qubits are allowed to couple laterally to other hidden qubits.","We prove that this wave function ansatz is closed under the action of the imaginary-time propagator and, more generally, can represent the action of a universal set of quantum gate operations.","We provide analytic expressions for the coefficients for both architectures, thus enabling exact network representations of thermal states without stochastic optimization of the network parameters.","In the limit of large imaginary time, the ansatz yields the ground state of the system.","The number of qubits grows linearly with the system size and total imaginary time for a fixed interaction order.","Both networks can be readily implemented on quantum hardware via mid-circuit measurements of auxiliary qubits.","If only one auxiliary qubit is measured and reset, the circuit depth scales linearly with imaginary time and system size, while the width is constant.","Alternatively, one can employ a number of auxiliary qubits linearly proportional to the system size, and circuit depth grows linearly with imaginary time only."],"url":"http://arxiv.org/abs/2403.17273v1","category":"quant-ph"}
{"created":"2024-03-25 23:25:36","title":"Robust integration of fast flavor conversions in classical neutrino transport","abstract":"The quantum kinetic evolution of neutrinos in dense environments, such as the core-collapse supernovae or the neutron star mergers, can result in fast flavor conversion (FFC), presenting a significant challenge to achieving robust astrophysical modeling of these systems. Recent works that directly simulate the quantum kinetic transport of neutrinos in localized domains have suggested that the asymptotic outcome of FFCs can be modeled by simple analytical prescriptions. In this Letter, we incorporate the analytical prescriptions into global simulations that solve the classical neutrino transport equation including collisions and advection under spherical symmetry. We demonstrate excellent agreement between results obtained using this approach and those directly from the corresponding global quantum kinetic simulations. In particular, this effective method can also precisely capture the collisional feedback effect for cases where the FFC happens inside the neutrinosphere. Our work highlights that a robust integration of FFCs in classical neutrino transport used in astrophysical simulation can be feasible.","sentences":["The quantum kinetic evolution of neutrinos in dense environments, such as the core-collapse supernovae or the neutron star mergers, can result in fast flavor conversion (FFC), presenting a significant challenge to achieving robust astrophysical modeling of these systems.","Recent works that directly simulate the quantum kinetic transport of neutrinos in localized domains have suggested that the asymptotic outcome of FFCs can be modeled by simple analytical prescriptions.","In this Letter, we incorporate the analytical prescriptions into global simulations that solve the classical neutrino transport equation including collisions and advection under spherical symmetry.","We demonstrate excellent agreement between results obtained using this approach and those directly from the corresponding global quantum kinetic simulations.","In particular, this effective method can also precisely capture the collisional feedback effect for cases where the FFC happens inside the neutrinosphere.","Our work highlights that a robust integration of FFCs in classical neutrino transport used in astrophysical simulation can be feasible."],"url":"http://arxiv.org/abs/2403.17269v1","category":"astro-ph.HE"}
{"created":"2024-03-25 23:03:51","title":"Decoding the visual attention of pathologists to reveal their level of expertise","abstract":"We present a method for classifying the expertise of a pathologist based on how they allocated their attention during a cancer reading. We engage this decoding task by developing a novel method for predicting the attention of pathologists as they read whole-slide Images (WSIs) of prostate and make cancer grade classifications. Our ground truth measure of a pathologists' attention is the x, y and z (magnification) movement of their viewport as they navigated through WSIs during readings, and to date we have the attention behavior of 43 pathologists reading 123 WSIs. These data revealed that specialists have higher agreement in both their attention and cancer grades compared to general pathologists and residents, suggesting that sufficient information may exist in their attention behavior to classify their expertise level. To attempt this, we trained a transformer-based model to predict the visual attention heatmaps of resident, general, and specialist (GU) pathologists during Gleason grading. Based solely on a pathologist's attention during a reading, our model was able to predict their level of expertise with 75.3%, 56.1%, and 77.2% accuracy, respectively, better than chance and baseline models. Our model therefore enables a pathologist's expertise level to be easily and objectively evaluated, important for pathology training and competency assessment. Tools developed from our model could also be used to help pathology trainees learn how to read WSIs like an expert.","sentences":["We present a method for classifying the expertise of a pathologist based on how they allocated their attention during a cancer reading.","We engage this decoding task by developing a novel method for predicting the attention of pathologists as they read whole-slide Images (WSIs) of prostate and make cancer grade classifications.","Our ground truth measure of a pathologists' attention is the x, y and z (magnification) movement of their viewport as they navigated through WSIs during readings, and to date we have the attention behavior of 43 pathologists reading 123 WSIs.","These data revealed that specialists have higher agreement in both their attention and cancer grades compared to general pathologists and residents, suggesting that sufficient information may exist in their attention behavior to classify their expertise level.","To attempt this, we trained a transformer-based model to predict the visual attention heatmaps of resident, general, and specialist (GU) pathologists during Gleason grading.","Based solely on a pathologist's attention during a reading, our model was able to predict their level of expertise with 75.3%, 56.1%, and 77.2% accuracy, respectively, better than chance and baseline models.","Our model therefore enables a pathologist's expertise level to be easily and objectively evaluated, important for pathology training and competency assessment.","Tools developed from our model could also be used to help pathology trainees learn how to read WSIs like an expert."],"url":"http://arxiv.org/abs/2403.17255v1","category":"eess.IV"}
{"created":"2024-03-25 22:44:28","title":"Review Ecosystems to access Educational XR Experiences: a Scoping Review","abstract":"Educators, developers, and other stakeholders face challenges when creating, adapting, and utilizing virtual and augmented reality (XR) experiences for teaching curriculum topics. User created reviews of these applications provide important information about their relevance and effectiveness in supporting achievement of educational outcomes. To make these reviews accessible, relevant, and useful, they must be readily available and presented in a format that supports decision-making by educators. This paper identifies best practices for developing a new review ecosystem by analyzing existing approaches to providing reviews of interactive experiences. It focuses on the form and format of these reviews, as well as the mechanisms for sharing information about experiences and identifying which ones are most effective. The paper also examines the incentives that drive review creation and maintenance, ensuring that new experiences receive attention from reviewers and that relevant information is updated when necessary. The strategies and opportunities for developing an educational XR (eduXR) review ecosystem include methods for measuring properties such as quality metrics, engaging a broad range of stakeholders in the review process, and structuring the system as a closed loop managed by feedback and incentive structures to ensure stability and productivity. Computing educators are well-positioned to lead the development of these review ecosystems, which can relate XR experiences to the potential opportunities for teaching and learning that they offer.","sentences":["Educators, developers, and other stakeholders face challenges when creating, adapting, and utilizing virtual and augmented reality (XR) experiences for teaching curriculum topics.","User created reviews of these applications provide important information about their relevance and effectiveness in supporting achievement of educational outcomes.","To make these reviews accessible, relevant, and useful, they must be readily available and presented in a format that supports decision-making by educators.","This paper identifies best practices for developing a new review ecosystem by analyzing existing approaches to providing reviews of interactive experiences.","It focuses on the form and format of these reviews, as well as the mechanisms for sharing information about experiences and identifying which ones are most effective.","The paper also examines the incentives that drive review creation and maintenance, ensuring that new experiences receive attention from reviewers and that relevant information is updated when necessary.","The strategies and opportunities for developing an educational XR (eduXR) review ecosystem include methods for measuring properties such as quality metrics, engaging a broad range of stakeholders in the review process, and structuring the system as a closed loop managed by feedback and incentive structures to ensure stability and productivity.","Computing educators are well-positioned to lead the development of these review ecosystems, which can relate XR experiences to the potential opportunities for teaching and learning that they offer."],"url":"http://arxiv.org/abs/2403.17243v1","category":"cs.CY"}
{"created":"2024-03-25 22:42:26","title":"Tightness of the matrix Moment-SOS hierarchy","abstract":"This paper studies the matrix Moment-SOS hierarchy for solving polynomial matrix optimization. Our first result is to show the tightness (i.e., the finite convergence) of this hierarchy, if the nondegeneracy condition, strict complementarity condition and second order sufficient condition hold at every minimizer, under the usual archimedeanness assumption. A useful criterion for detecting tightness is the flat truncation. Our second result is to show that every minimizer of the moment relaxation must have a flat truncation when the relaxation order is big enough, under the above mentioned optimality assumptions. These results give connections between nonlinear semidefinite optimization theory and Moment-SOS methods for solving polynomial matrix optimization.","sentences":["This paper studies the matrix Moment-SOS hierarchy for solving polynomial matrix optimization.","Our first result is to show the tightness (i.e., the finite convergence) of this hierarchy, if the nondegeneracy condition, strict complementarity condition and second order sufficient condition hold at every minimizer, under the usual archimedeanness assumption.","A useful criterion for detecting tightness is the flat truncation.","Our second result is to show that every minimizer of the moment relaxation must have a flat truncation when the relaxation order is big enough, under the above mentioned optimality assumptions.","These results give connections between nonlinear semidefinite optimization theory and Moment-SOS methods for solving polynomial matrix optimization."],"url":"http://arxiv.org/abs/2403.17241v1","category":"math.OC"}
{"created":"2024-03-25 22:25:03","title":"A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme with A Mobile-Robot System Study","abstract":"This paper develops an adaptive state tracking control scheme for discrete-time systems, using the least-squares algorithm, as the new solution to the long-standing discrete-time adaptive state tracking control problem to which the Lyapunov method (well-developed for the continuous-time adaptive state tracking problem) is not applicable. The new adaptive state tracking scheme is based on a recently-developed new discrete-time error model which has been used for gradient algorithm based state tracking control schemes, and uses the least-squares algorithm for parameter adaptation. The new least-squares algorithm is derived to minimize an accumulative estimation error, to ensure certain optimality for parameter estimation. The system stability and output tracking properties are studied. Technical results are presented in terms of plant-model matching, error model, adaptive law, optimality formulation, and stability and tracking analysis. The developed adaptive control scheme is applied to a discrete-time multiple mobile robot system to meet an adaptive state tracking objective. In addition, a collision avoidance mechanism is proposed to prevent collisions in the whole tracking process. Simulation results are presented, which verify the desired system state tracking properties under the developed least-squares algorithm based adaptive control scheme.","sentences":["This paper develops an adaptive state tracking control scheme for discrete-time systems, using the least-squares algorithm, as the new solution to the long-standing discrete-time adaptive state tracking control problem to which the Lyapunov method (well-developed for the continuous-time adaptive state tracking problem) is not applicable.","The new adaptive state tracking scheme is based on a recently-developed new discrete-time error model which has been used for gradient algorithm based state tracking control schemes, and uses the least-squares algorithm for parameter adaptation.","The new least-squares algorithm is derived to minimize an accumulative estimation error, to ensure certain optimality for parameter estimation.","The system stability and output tracking properties are studied.","Technical results are presented in terms of plant-model matching, error model, adaptive law, optimality formulation, and stability and tracking analysis.","The developed adaptive control scheme is applied to a discrete-time multiple mobile robot system to meet an adaptive state tracking objective.","In addition, a collision avoidance mechanism is proposed to prevent collisions in the whole tracking process.","Simulation results are presented, which verify the desired system state tracking properties under the developed least-squares algorithm based adaptive control scheme."],"url":"http://arxiv.org/abs/2403.17235v1","category":"eess.SY"}
{"created":"2024-03-25 22:20:45","title":"Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process","abstract":"We present an active learning algorithm for learning dynamics that leverages side information by explicitly incorporating prior domain knowledge into the sampling process. Our proposed algorithm guides the exploration toward regions that demonstrate high empirical discrepancy between the observed data and an imperfect prior model of the dynamics derived from side information. Through numerical experiments, we demonstrate that this strategy explores regions of high discrepancy and accelerates learning while simultaneously reducing model uncertainty. We rigorously prove that our active learning algorithm yields a consistent estimate of the underlying dynamics by providing an explicit rate of convergence for the maximum predictive variance. We demonstrate the efficacy of our approach on an under-actuated pendulum system and on the half-cheetah MuJoCo environment.","sentences":["We present an active learning algorithm for learning dynamics that leverages side information by explicitly incorporating prior domain knowledge into the sampling process.","Our proposed algorithm guides the exploration toward regions that demonstrate high empirical discrepancy between the observed data and an imperfect prior model of the dynamics derived from side information.","Through numerical experiments, we demonstrate that this strategy explores regions of high discrepancy and accelerates learning while simultaneously reducing model uncertainty.","We rigorously prove that our active learning algorithm yields a consistent estimate of the underlying dynamics by providing an explicit rate of convergence for the maximum predictive variance.","We demonstrate the efficacy of our approach on an under-actuated pendulum system and on the half-cheetah MuJoCo environment."],"url":"http://arxiv.org/abs/2403.17233v1","category":"eess.SY"}
{"created":"2024-03-25 21:48:42","title":"Are Made and Missed Different? An analysis of Field Goal Attempts of Professional Basketball Players via Depth Based Testing Procedure","abstract":"In this paper, we develop a novel depth-based testing procedure on spatial point processes to examine the difference in made and missed field goal attempts for NBA players. Specifically, our testing procedure can statistically detect the differences between made and missed field goal attempts for NBA players. We first obtain the depths of two processes under the polar coordinate system. A two-dimensional Kolmogorov-Smirnov test is then performed to test the difference between the depths of the two processes. Throughout extensive simulation studies, we show our testing procedure with good frequentist properties under both null hypothesis and alternative hypothesis. A comparison against the competing methods shows that our proposed procedure has better testing reliability and testing power. Application to the shot chart data of 191 NBA players in the 2017-2018 regular season offers interesting insights about these players' made and missed shot patterns.","sentences":["In this paper, we develop a novel depth-based testing procedure on spatial point processes to examine the difference in made and missed field goal attempts for NBA players.","Specifically, our testing procedure can statistically detect the differences between made and missed field goal attempts for NBA players.","We first obtain the depths of two processes under the polar coordinate system.","A two-dimensional Kolmogorov-Smirnov test is then performed to test the difference between the depths of the two processes.","Throughout extensive simulation studies, we show our testing procedure with good frequentist properties under both null hypothesis and alternative hypothesis.","A comparison against the competing methods shows that our proposed procedure has better testing reliability and testing power.","Application to the shot chart data of 191 NBA players in the 2017-2018 regular season offers interesting insights about these players' made and missed shot patterns."],"url":"http://arxiv.org/abs/2403.17221v1","category":"stat.AP"}
{"created":"2024-03-25 21:48:36","title":"Making Sentence Embeddings Robust to User-Generated Content","abstract":"NLP models have been known to perform poorly on user-generated content (UGC), mainly because it presents a lot of lexical variations and deviates from the standard texts on which most of these models were trained. In this work, we focus on the robustness of LASER, a sentence embedding model, to UGC data. We evaluate this robustness by LASER's ability to represent non-standard sentences and their standard counterparts close to each other in the embedding space. Inspired by previous works extending LASER to other languages and modalities, we propose RoLASER, a robust English encoder trained using a teacher-student approach to reduce the distances between the representations of standard and UGC sentences. We show that with training only on standard and synthetic UGC-like data, RoLASER significantly improves LASER's robustness to both natural and artificial UGC data by achieving up to 2x and 11x better scores. We also perform a fine-grained analysis on artificial UGC data and find that our model greatly outperforms LASER on its most challenging UGC phenomena such as keyboard typos and social media abbreviations. Evaluation on downstream tasks shows that RoLASER performs comparably to or better than LASER on standard data, while consistently outperforming it on UGC data.","sentences":["NLP models have been known to perform poorly on user-generated content (UGC), mainly because it presents a lot of lexical variations and deviates from the standard texts on which most of these models were trained.","In this work, we focus on the robustness of LASER, a sentence embedding model, to UGC data.","We evaluate this robustness by LASER's ability to represent non-standard sentences and their standard counterparts close to each other in the embedding space.","Inspired by previous works extending LASER to other languages and modalities, we propose RoLASER, a robust English encoder trained using a teacher-student approach to reduce the distances between the representations of standard and UGC sentences.","We show that with training only on standard and synthetic UGC-like data, RoLASER significantly improves LASER's robustness to both natural and artificial UGC data by achieving up to 2x and 11x better scores.","We also perform a fine-grained analysis on artificial UGC data and find that our model greatly outperforms LASER on its most challenging UGC phenomena such as keyboard typos and social media abbreviations.","Evaluation on downstream tasks shows that RoLASER performs comparably to or better than LASER on standard data, while consistently outperforming it on UGC data."],"url":"http://arxiv.org/abs/2403.17220v1","category":"cs.CL"}
{"created":"2024-03-25 21:47:36","title":"A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection","abstract":"Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.   In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of-thought, and proposed three of our own prompting methods. Our results show that while our prompting methods improved the models' performance, LLMs generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that 57% of LLM responses contained errors, and the models frequently predicted incorrect locations of buggy code and misidentified bug types. LLMs only correctly localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted correctly by 70-100% of human participants. These findings suggest that despite their potential for other tasks, LLMs may fail to properly comprehend critical code structures and security-related concepts. Our data and code are available at https://figshare.com/s/78fe02e56e09ec49300b.","sentences":["Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks.","Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems.","Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities.","Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.   ","In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection.","We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of-thought, and proposed three of our own prompting methods.","Our results show that while our prompting methods improved the models' performance, LLMs generally struggled with vulnerability detection.","They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average.","By comprehensively analyzing and categorizing 287 instances of model reasoning, we found that 57% of LLM responses contained errors, and the models frequently predicted incorrect locations of buggy code and misidentified bug types.","LLMs only correctly localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted correctly by 70-100% of human participants.","These findings suggest that despite their potential for other tasks, LLMs may fail to properly comprehend critical code structures and security-related concepts.","Our data and code are available at https://figshare.com/s/78fe02e56e09ec49300b."],"url":"http://arxiv.org/abs/2403.17218v1","category":"cs.SE"}
{"created":"2024-03-25 21:40:44","title":"AnimateMe: 4D Facial Expressions via Diffusion Models","abstract":"The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging. Recent advances in diffusion models have notably enhanced the capabilities of generative models in 2D animation. In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation. By integrating the strengths of diffusion processes and geometric deep learning, we employ Graph Neural Networks (GNNs) as denoising diffusion models in a novel approach, formulating the diffusion process directly on the mesh space and enabling the generation of 3D facial expressions. This facilitates the generation of facial deformations through a mesh-diffusion-based model. Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method. Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions. Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database.","sentences":["The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging.","Recent advances in diffusion models have notably enhanced the capabilities of generative models in 2D animation.","In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation.","By integrating the strengths of diffusion processes and geometric deep learning, we employ Graph Neural Networks (GNNs) as denoising diffusion models in a novel approach, formulating the diffusion process directly on the mesh space and enabling the generation of 3D facial expressions.","This facilitates the generation of facial deformations through a mesh-diffusion-based model.","Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method.","Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions.","Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database."],"url":"http://arxiv.org/abs/2403.17213v1","category":"cs.CV"}
{"created":"2024-03-25 21:38:23","title":"Sharp total variation rates of convergence for fluctuations of linear statistics of $\u03b2$-ensembles","abstract":"In this article, we revisit the question of fluctuations of linear statistics of beta ensembles in the single cut and non-critical regime for general potentials $V$ under mild regularity and growth assumptions. Our main objective is to establish sharp quantitative Central Limit Theorems (CLT) for strong distances, such as the total variation distance, which to the best of our knowledge, is new for general potentials, even qualitatively. Namely, setting $\\mu_V$ the equilibrium measure, for a test function $\\xi \\in \\mathscr{C}^{14}$, we establish the convergence in total variation of $X_n=\\sum_{i=1}^n \\xi(\\lambda_i)-n\\langle \\xi,\\mu_V\\rangle$ to an explicit Gaussian variable at the sharp speed $1/n$. Under the same assumptions, we also establish multivariate CLTs for vectors of linear statistics in $p-$Wasserstein distances for any $p\\ge 1$, with the optimal rate $1/n$, a result which already in dimension one sharpens the speed of convergence established in the recent contribution [26] as well as the required regularity on the test functions. A second objective of this paper, in a more qualitative direction, is to establish the so-called super-convergence of linear statistics, that is to say the convergence of all derivatives of the densities of $X_n$ uniformly on $\\mathbb{R}$, provided that $\\xi\\in\\mathscr{C}^\\infty(\\mathbb{R})$ and is not too degenerated in some sense.","sentences":["In this article, we revisit the question of fluctuations of linear statistics of beta ensembles in the single cut and non-critical regime for general potentials $V$ under mild regularity and growth assumptions.","Our main objective is to establish sharp quantitative Central Limit Theorems (CLT) for strong distances, such as the total variation distance, which to the best of our knowledge, is new for general potentials, even qualitatively.","Namely, setting $\\mu_V$ the equilibrium measure, for a test function $\\xi \\in \\mathscr{C}^{14}$, we establish the convergence in total variation of $X_n=\\sum_{i=1}^n \\xi(\\lambda_i)-n\\langle \\xi,\\mu_V\\rangle$ to an explicit Gaussian variable at the sharp speed $1/n$. Under the same assumptions, we also establish multivariate CLTs for vectors of linear statistics in $p-$Wasserstein distances for any $p\\ge 1$, with the optimal rate $1/n$, a result which already in dimension one sharpens the speed of convergence established in the recent contribution [26] as well as the required regularity on the test functions.","A second objective of this paper, in a more qualitative direction, is to establish the so-called super-convergence of linear statistics, that is to say the convergence of all derivatives of the densities of $X_n$ uniformly on $\\mathbb{R}$, provided that $\\xi\\in\\mathscr{C}^\\infty(\\mathbb{R})$ and is not too degenerated in some sense."],"url":"http://arxiv.org/abs/2403.17211v1","category":"math.PR"}
{"created":"2024-03-25 21:37:25","title":"Constant-selection evolutionary dynamics on weighted networks","abstract":"The population structure often impacts evolutionary dynamics. In constant-selection evolutionary dynamics between two types, amplifiers of selection are networks that promote the fitter mutant to take over the entire population, and suppressors of selection do the opposite. It has been shown that most undirected and unweighted networks are amplifiers of selection under a common updating rule and initial condition. Here, we extensively investigate how edge weights influence selection on undirected networks. We show that random edge weights make small networks less amplifying than the corresponding unweighted networks in a majority of cases and also make them suppressors of selection (i.e., less amplifying than the complete graph, or equivalently, the Moran process) in many cases. Qualitatively, the same result holds true for larger empirical networks. These results suggest that amplifiers of selection are not as common for weighted networks as for unweighted counterparts.","sentences":["The population structure often impacts evolutionary dynamics.","In constant-selection evolutionary dynamics between two types, amplifiers of selection are networks that promote the fitter mutant to take over the entire population, and suppressors of selection do the opposite.","It has been shown that most undirected and unweighted networks are amplifiers of selection under a common updating rule and initial condition.","Here, we extensively investigate how edge weights influence selection on undirected networks.","We show that random edge weights make small networks less amplifying than the corresponding unweighted networks in a majority of cases and also make them suppressors of selection (i.e., less amplifying than the complete graph, or equivalently, the Moran process) in many cases.","Qualitatively, the same result holds true for larger empirical networks.","These results suggest that amplifiers of selection are not as common for weighted networks as for unweighted counterparts."],"url":"http://arxiv.org/abs/2403.17208v1","category":"physics.soc-ph"}
{"created":"2024-03-25 21:35:59","title":"Unified Differentiable Learning of the Electric Enthalpy and Dielectric Properties with Exact Physical Constraints","abstract":"The modern theory of polarization and electric enthalpy functionals have paved the way to first-principles studies of polarization in crystalline, disordered and liquid materials. However, calculating the polarization in large-scale ab-initio molecular dynamics remains prohibitive due to the high computational cost involved. In this work, we introduce a machine-learning formulation that simultaneously learns to predict electric enthalpy, atomic forces, polarization, Born charges, and polarizability for a given atomic configuration. These quantities are derived through differentiation of the electric enthalpy with respect to atomic positions and electric field within a local equivariant representation of the atomic environment. Our new method exactly constrains the polarization to be a conservative vector field, obeys the acoustic sum rule for Born charges, and ensures the conservation of electric enthalpy in machine-learning molecular dynamics. Importantly, the formulation is general and does not necessarily require equivariance. The locality of the model enables the efficient calculation of vibrational and dielectric properties of materials over large length-scale and long time-scale molecular dynamics at quantum-mechanical accuracy. We apply our approach to determine the infrared spectrum and frequency-dependent dielectric constant of $\\alpha$-SiO$_2$ using molecular dynamics, finding excellent agreement with results obtained from density functional perturbation theory. Additionally, we show that our model accurately captures the electric-field contributions to the electric enthalpy, forces, and polarization in reference to density functional theory calculations. This demonstrates the capability of our formulation to perform large-scale molecular dynamics under electric fields, thus paving the way to efficient and accurate studies of dielectric response of complex materials.","sentences":["The modern theory of polarization and electric enthalpy functionals have paved the way to first-principles studies of polarization in crystalline, disordered and liquid materials.","However, calculating the polarization in large-scale ab-initio molecular dynamics remains prohibitive due to the high computational cost involved.","In this work, we introduce a machine-learning formulation that simultaneously learns to predict electric enthalpy, atomic forces, polarization, Born charges, and polarizability for a given atomic configuration.","These quantities are derived through differentiation of the electric enthalpy with respect to atomic positions and electric field within a local equivariant representation of the atomic environment.","Our new method exactly constrains the polarization to be a conservative vector field, obeys the acoustic sum rule for Born charges, and ensures the conservation of electric enthalpy in machine-learning molecular dynamics.","Importantly, the formulation is general and does not necessarily require equivariance.","The locality of the model enables the efficient calculation of vibrational and dielectric properties of materials over large length-scale and long time-scale molecular dynamics at quantum-mechanical accuracy.","We apply our approach to determine the infrared spectrum and frequency-dependent dielectric constant of $\\alpha$-SiO$_2$ using molecular dynamics, finding excellent agreement with results obtained from density functional perturbation theory.","Additionally, we show that our model accurately captures the electric-field contributions to the electric enthalpy, forces, and polarization in reference to density functional theory calculations.","This demonstrates the capability of our formulation to perform large-scale molecular dynamics under electric fields, thus paving the way to efficient and accurate studies of dielectric response of complex materials."],"url":"http://arxiv.org/abs/2403.17207v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-25 21:17:14","title":"GPT-4 Understands Discourse at Least as Well as Humans Do","abstract":"We test whether a leading AI system GPT-4 understands discourse as well as humans do, using a standardized test of discourse comprehension. Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story. The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details). GPT-4 performs slightly, but not statistically significantly, better than humans given the very high level of human performance. Both GPT-4 and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding.","sentences":["We test whether a leading AI system GPT-4 understands discourse as well as humans do, using a standardized test of discourse comprehension.","Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story.","The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details).","GPT-4 performs slightly, but not statistically significantly, better than humans given the very high level of human performance.","Both GPT-4 and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding."],"url":"http://arxiv.org/abs/2403.17196v1","category":"cs.CL"}
{"created":"2024-03-25 21:16:52","title":"Hot electron dynamics in a semiconductor nanowire under intense THz excitation","abstract":"We report THz-pump / mid-infrared probe near-field studies on Si-doped GaAs-InGaAs core-shell nanowires utilizing THz radiation from the free-electron laser FELBE. Upon THz excitation of free carriers, we observe a red shift of the plasma resonance in both amplitude and phase spectra, which we attribute to the heating up of electrons in the conduction band. The simulation of heated electron distributions anticipates a significant electron population in both L- and X-valleys. The two-temperature model is utilized for a quantitative analysis of the dynamics of the electron gas temperature under THz pumping at various power levels.","sentences":["We report THz-pump / mid-infrared probe near-field studies on Si-doped GaAs-InGaAs core-shell nanowires utilizing THz radiation from the free-electron laser FELBE.","Upon THz excitation of free carriers, we observe a red shift of the plasma resonance in both amplitude and phase spectra, which we attribute to the heating up of electrons in the conduction band.","The simulation of heated electron distributions anticipates a significant electron population in both L- and X-valleys.","The two-temperature model is utilized for a quantitative analysis of the dynamics of the electron gas temperature under THz pumping at various power levels."],"url":"http://arxiv.org/abs/2403.17195v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-25 21:08:26","title":"Strategies to Improve Real-World Applicability of Laparoscopic Anatomy Segmentation Models","abstract":"Accurate identification and localization of anatomical structures of varying size and appearance in laparoscopic imaging are necessary to leverage the potential of computer vision techniques for surgical decision support. Segmentation performance of such models is traditionally reported using metrics of overlap such as IoU. However, imbalanced and unrealistic representation of classes in the training data and suboptimal selection of reported metrics have the potential to skew nominal segmentation performance and thereby ultimately limit clinical translation. In this work, we systematically analyze the impact of class characteristics (i.e., organ size differences), training and test data composition (i.e., representation of positive and negative examples), and modeling parameters (i.e., foreground-to-background class weight) on eight segmentation metrics: accuracy, precision, recall, IoU, F1 score, specificity, Hausdorff Distance, and Average Symmetric Surface Distance. Based on our findings, we propose two simple yet effective strategies to improve real-world applicability of image segmentation models in laparoscopic surgical data: (1) inclusion of negative examples in the training process and (2) adaptation of foreground-background weights in segmentation models to maximize model performance with respect to specific metrics of interest, depending on the clinical use case.","sentences":["Accurate identification and localization of anatomical structures of varying size and appearance in laparoscopic imaging are necessary to leverage the potential of computer vision techniques for surgical decision support.","Segmentation performance of such models is traditionally reported using metrics of overlap such as IoU. However, imbalanced and unrealistic representation of classes in the training data and suboptimal selection of reported metrics have the potential to skew nominal segmentation performance and thereby ultimately limit clinical translation.","In this work, we systematically analyze the impact of class characteristics (i.e., organ size differences), training and test data composition (i.e., representation of positive and negative examples), and modeling parameters (i.e., foreground-to-background class weight) on eight segmentation metrics: accuracy, precision, recall, IoU, F1 score, specificity, Hausdorff Distance, and Average Symmetric Surface Distance.","Based on our findings, we propose two simple yet effective strategies to improve real-world applicability of image segmentation models in laparoscopic surgical data: (1) inclusion of negative examples in the training process and (2) adaptation of foreground-background weights in segmentation models to maximize model performance with respect to specific metrics of interest, depending on the clinical use case."],"url":"http://arxiv.org/abs/2403.17192v1","category":"cs.CV"}
{"created":"2024-03-25 21:02:51","title":"High-dimensional continuification control of large-scale multi-agent systems under limited sensing and perturbations","abstract":"This paper investigates the robustness of a novel high-dimensional continuification control method for complex multi-agent systems. We begin by formulating a partial differential equation describing the spatio-temporal density dynamics of swarming agents. A stable control action for the density is then derived and validated under nominal conditions. Subsequently, we discretize this macroscopic strategy into actionable velocity inputs for the system's agents. Our analysis demonstrates the robustness of the approach beyond idealized assumptions of unlimited sensing and absence of perturbations.","sentences":["This paper investigates the robustness of a novel high-dimensional continuification control method for complex multi-agent systems.","We begin by formulating a partial differential equation describing the spatio-temporal density dynamics of swarming agents.","A stable control action for the density is then derived and validated under nominal conditions.","Subsequently, we discretize this macroscopic strategy into actionable velocity inputs for the system's agents.","Our analysis demonstrates the robustness of the approach beyond idealized assumptions of unlimited sensing and absence of perturbations."],"url":"http://arxiv.org/abs/2403.17191v1","category":"eess.SY"}
{"created":"2024-03-25 20:57:32","title":"Alternatives to classical option pricing","abstract":"We develop two alternate approaches to arbitrage-free, market-complete, option pricing. The first approach requires no riskless asset. We develop the general framework for this approach and illustrate it with two specific examples. The second approach does use a riskless asset. However, by ensuring equality between real-world and risk-neutral price-change probabilities, the second approach enables the computation of risk-neutral option prices utilizing expectations under the natural world probability P. This produces the same option prices as the classical approach in which prices are computed under the risk neutral measure Q. The second approach and the two specific examples of the first approach require the introduction of new, marketable asset types, specifically perpetual derivatives of a stock, and a stock whose cumulative return (rather than price) is deflated.","sentences":["We develop two alternate approaches to arbitrage-free, market-complete, option pricing.","The first approach requires no riskless asset.","We develop the general framework for this approach and illustrate it with two specific examples.","The second approach does use a riskless asset.","However, by ensuring equality between real-world and risk-neutral price-change probabilities, the second approach enables the computation of risk-neutral option prices utilizing expectations under the natural world probability","P.","This produces the same option prices as the classical approach in which prices are computed under the risk neutral measure Q.","The second approach and the two specific examples of the first approach require the introduction of new, marketable asset types, specifically perpetual derivatives of a stock, and a stock whose cumulative return (rather than price) is deflated."],"url":"http://arxiv.org/abs/2403.17187v1","category":"q-fin.PR"}
{"created":"2024-03-25 20:33:03","title":"Monodromy groups of indecomposable coverings of bounded genus","abstract":"For each nonnegative integer $g$, we classify the ramification types and monodromy groups of indecomposable coverings of complex curves $f: X\\to Y$ where $X$ has genus $g$, under the hypothesis that $n:=\\deg(f)$ is sufficiently large and the monodromy group is not $A_n$ or $S_n$. This proves a conjecture of Guralnick and several conjectures of Guralnick and Shareshian.","sentences":["For each nonnegative integer $g$, we classify the ramification types and monodromy groups of indecomposable coverings of complex curves $f: X\\to Y$ where $X$ has genus $g$, under the hypothesis that $n:=\\deg(f)$ is sufficiently large and the monodromy group is not $A_n$ or $S_n$. This proves a conjecture of Guralnick and several conjectures of Guralnick and Shareshian."],"url":"http://arxiv.org/abs/2403.17167v1","category":"math.AG"}
{"created":"2024-03-25 20:23:11","title":"Design Insights for Industrial CO2 Capture, Transport, and Storage Systems","abstract":"We present design methods and insights for CO2 capture, transport, and storage systems for clusters of industrial facilities, with a case-study focus on the state of Louisiana. Our analytical framework includes: (1) evaluating the scale and concentration of capturable CO2 emissions at individual facilities for the purpose of estimating the cost of CO2 capture retrofits, (2) a screening method to identify potential CO2 storage sites and estimate their storage capacities, injectivities, and costs; and (3) an approach for cost-minimized design of pipeline infrastructure connecting CO2 capture plants with storage sites that considers land use patterns, existing rights-of-way, demographics, and a variety of social and environmental justice factors. In applying our framework to Louisiana, we estimate up to 50 million tCO2/y of industrial emissions (out of today's total emissions of 130 MtCO2/y) can be captured at under 100 USD/tCO2, and up to 100 MtCO2/y at under 120 USD/tCO2. We identified 98 potential storage sites with estimated aggregate total injectivity between 330 and 730 MtCO2/yr and storage costs ranging from 8 to 17 USD/tCO2. We find dramatic reductions in the aggregate pipeline length and CO2 transport cost per tonne when groups of capture plants share pipeline infrastructure rather than build dedicated single-user pipelines. Smaller facilities (emitting less than 1 MtCO2/y), which account for a quarter of Louisiana's industrial emissions, see the largest transport cost benefits from sharing of infrastructure. Pipeline routes designed to avoid disadvantaged communities (social and environmental justice) so as not to reinforce historical practices of disenfranchisement involve only modestly higher pipeline lengths and costs.","sentences":["We present design methods and insights for CO2 capture, transport, and storage systems for clusters of industrial facilities, with a case-study focus on the state of Louisiana.","Our analytical framework includes: (1) evaluating the scale and concentration of capturable CO2 emissions at individual facilities for the purpose of estimating the cost of CO2 capture retrofits, (2) a screening method to identify potential CO2 storage sites and estimate their storage capacities, injectivities, and costs; and (3) an approach for cost-minimized design of pipeline infrastructure connecting CO2 capture plants with storage sites that considers land use patterns, existing rights-of-way, demographics, and a variety of social and environmental justice factors.","In applying our framework to Louisiana, we estimate up to 50 million tCO2/y of industrial emissions (out of today's total emissions of 130 MtCO2/y) can be captured at under 100 USD/tCO2, and up to 100 MtCO2/y at under 120 USD/tCO2.","We identified 98 potential storage sites with estimated aggregate total injectivity between 330 and 730 MtCO2/yr and storage costs ranging from 8 to 17 USD/tCO2.","We find dramatic reductions in the aggregate pipeline length and CO2 transport cost per tonne when groups of capture plants share pipeline infrastructure rather than build dedicated single-user pipelines.","Smaller facilities (emitting less than 1 MtCO2/y), which account for a quarter of Louisiana's industrial emissions, see the largest transport cost benefits from sharing of infrastructure.","Pipeline routes designed to avoid disadvantaged communities (social and environmental justice) so as not to reinforce historical practices of disenfranchisement involve only modestly higher pipeline lengths and costs."],"url":"http://arxiv.org/abs/2403.17162v1","category":"econ.GN"}
{"created":"2024-03-25 20:13:58","title":"The Mass Density of Merging Binary Black Holes Over Cosmic Time","abstract":"The connection between the binary black hole (BBH) mergers observed by LIGO-Virgo-KAGRA (LVK) and their stellar progenitors remains uncertain. Specifically, the fraction $\\epsilon$ of stellar mass that ends up in BBH mergers and the delay time $\\tau$ between star formation and BBH merger carry information about the astrophysical processes that give rise to merging BBHs. We model the BBH merger rate in terms of the cosmic star formation history, coupled with a metallicity-dependent efficiency $\\epsilon$ and a distribution of delay times $\\tau$, and infer these parameters with data from the Third Gravitational-Wave Transient Catalog (GWTC-3). We find that the progenitors to merging BBHs preferentially form in low metallicity environments with a low metallicity efficiency of $\\log_{10}\\epsilon_{<Z_t}=-3.99^{+0.68}_{-0.87}$ and a high metallicity efficiency of $\\log_{10}\\epsilon_{<Z_t}=-4.60^{+0.30}_{-0.34}$ at the 90% credible level. The data also prefer short delay times. For a power-law distribution $p(\\tau)\\propto \\tau^\\alpha$, we find $\\tau_\\text{min}<1.9 $ Gyr and $\\alpha<-1.32$ at 90% credibility. Our model allows us to extrapolate the mass density in BBHs out to high redshifts. We cumulatively integrate our modelled density rate over cosmic time to get the total mass density of merging stellar mass BBHs as a function of redshift. Today, stellar-mass BBH mergers make up only $\\sim 0.01\\%$ of the total stellar mass density created by high-mass ($>10\\,M_\\odot$) progenitors. However, because massive stars are so short-lived, there may be more mass in merging BBHs than in living massive stars as early as $\\sim 2.5$ Gyr ago. We also compare to the mass in supermassive BHs, finding that the mass densities were comparable $\\sim 12.5$ Gyr ago, but the mass density in SMBHs quickly increased to $\\sim 75$ times the mass density in merging stellar mass BBHs by $z\\sim 1$.","sentences":["The connection between the binary black hole (BBH) mergers observed by LIGO-Virgo-KAGRA (LVK) and their stellar progenitors remains uncertain.","Specifically, the fraction $\\epsilon$ of stellar mass that ends up in BBH mergers and the delay time $\\tau$ between star formation and BBH merger carry information about the astrophysical processes that give rise to merging BBHs.","We model the BBH merger rate in terms of the cosmic star formation history, coupled with a metallicity-dependent efficiency $\\epsilon$ and a distribution of delay times $\\tau$, and infer these parameters with data from the Third Gravitational-Wave Transient Catalog (GWTC-3).","We find that the progenitors to merging BBHs preferentially form in low metallicity environments with a low metallicity efficiency of $\\log_{10}\\epsilon_{<Z_t}=-3.99^{+0.68}_{-0.87}$ and a high metallicity efficiency of $\\log_{10}\\epsilon_{<Z_t}=-4.60^{+0.30}_{-0.34}$ at the 90% credible level.","The data also prefer short delay times.","For a power-law distribution $p(\\tau)\\propto \\tau^\\alpha$, we find $\\tau_\\text{min}<1.9 $ Gyr and $\\alpha<-1.32$ at 90% credibility.","Our model allows us to extrapolate the mass density in BBHs out to high redshifts.","We cumulatively integrate our modelled density rate over cosmic time to get the total mass density of merging stellar mass BBHs as a function of redshift.","Today, stellar-mass BBH mergers make up only $\\sim 0.01\\%$ of the total stellar mass density created by high-mass ($>10\\,M_\\odot$) progenitors.","However, because massive stars are so short-lived, there may be more mass in merging BBHs than in living massive stars as early as $\\sim 2.5$ Gyr ago.","We also compare to the mass in supermassive BHs, finding that the mass densities were comparable $\\sim 12.5$ Gyr ago, but the mass density in SMBHs quickly increased to $\\sim 75$ times the mass density in merging stellar mass BBHs by $z\\sim 1$."],"url":"http://arxiv.org/abs/2403.17156v1","category":"astro-ph.HE"}
{"created":"2024-03-26 17:20:04","title":"The Unreasonable Ineffectiveness of the Deeper Layers","abstract":"We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.","sentences":["We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed.","To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning.","In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU.","From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand.","From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge."],"url":"http://arxiv.org/abs/2403.17887v1","category":"cs.CL"}
{"created":"2024-03-26 15:15:15","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","abstract":"We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions. GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling. Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint. The sampled latent texture maps are then decoded into a final texture map. During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures. Finally, we apply reference-based inpainting and img2img on denser views for texture refinement. Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods. Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively.","sentences":["We present GenesisTex, a novel method for synthesizing textures for 3D geometries from text descriptions.","GenesisTex adapts the pretrained image diffusion model to texture space by texture space sampling.","Specifically, we maintain a latent texture map for each viewpoint, which is updated with predicted noise on the rendering of the corresponding viewpoint.","The sampled latent texture maps are then decoded into a final texture map.","During the sampling process, we focus on both global and local consistency across multiple viewpoints: global consistency is achieved through the integration of style consistency mechanisms within the noise prediction network, and low-level consistency is achieved by dynamically aligning latent textures.","Finally, we apply reference-based inpainting and img2img on denser views for texture refinement.","Our approach overcomes the limitations of slow optimization in distillation-based methods and instability in inpainting-based methods.","Experiments on meshes from various sources demonstrate that our method surpasses the baseline methods quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2403.17782v1","category":"cs.CV"}
{"created":"2024-03-26 15:09:55","title":"Deconvolution from two order statistics","abstract":"Economic data are often contaminated by measurement errors and truncated by ranking. This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements. The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity. Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models. We adapt an existing simulated sieve estimator and illustrate its performance in finite samples.","sentences":["Economic data are often contaminated by measurement errors and truncated by ranking.","This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements.","The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity.","Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models.","We adapt an existing simulated sieve estimator and illustrate its performance in finite samples."],"url":"http://arxiv.org/abs/2403.17777v1","category":"econ.EM"}
{"created":"2024-03-26 14:52:27","title":"Can patient-specific acquisition protocol improve performance on defect detection task in myocardial perfusion SPECT?","abstract":"Myocardial perfusion imaging using single-photon emission computed tomography (SPECT), or myocardial perfusion SPECT (MPS) is a widely used clinical imaging modality for the diagnosis of coronary artery disease. Current clinical protocols for acquiring and reconstructing MPS images are similar for most patients. However, for patients with outlier anatomical characteristics, such as large breasts, images acquired using conventional protocols are often sub-optimal in quality, leading to degraded diagnostic accuracy. Solutions to improve image quality for these patients outside of increased dose or total acquisition time remain challenging. Thus, there is an important need for new methodologies to improve image quality for such patients. One approach to improving this performance is adapting the image acquisition protocol specific to each patient. For this study, we first designed and implemented a personalized patient-specific protocol-optimization strategy, which we term precision SPECT (PRESPECT). This strategy integrates ideal observer theory with the constraints of tomographic reconstruction to optimize the acquisition time for each projection view, such that MPS defect detection performance is maximized. We performed a clinically realistic simulation study on patients with outlier anatomies on the task of detecting perfusion defects on various realizations of low-dose scans by an anthropomorphic channelized Hotelling observer. Our results show that using PRESPECT led to improved performance on the defect detection task for the considered patients. These results provide evidence that personalization of MPS acquisition protocol has the potential to improve defect detection performance, motivating further research to design optimal patient-specific acquisition and reconstruction protocols for MPS, as well as developing similar approaches for other medical imaging modalities.","sentences":["Myocardial perfusion imaging using single-photon emission computed tomography (SPECT), or myocardial perfusion SPECT (MPS) is a widely used clinical imaging modality for the diagnosis of coronary artery disease.","Current clinical protocols for acquiring and reconstructing MPS images are similar for most patients.","However, for patients with outlier anatomical characteristics, such as large breasts, images acquired using conventional protocols are often sub-optimal in quality, leading to degraded diagnostic accuracy.","Solutions to improve image quality for these patients outside of increased dose or total acquisition time remain challenging.","Thus, there is an important need for new methodologies to improve image quality for such patients.","One approach to improving this performance is adapting the image acquisition protocol specific to each patient.","For this study, we first designed and implemented a personalized patient-specific protocol-optimization strategy, which we term precision SPECT (PRESPECT).","This strategy integrates ideal observer theory with the constraints of tomographic reconstruction to optimize the acquisition time for each projection view, such that MPS defect detection performance is maximized.","We performed a clinically realistic simulation study on patients with outlier anatomies on the task of detecting perfusion defects on various realizations of low-dose scans by an anthropomorphic channelized Hotelling observer.","Our results show that using PRESPECT led to improved performance on the defect detection task for the considered patients.","These results provide evidence that personalization of MPS acquisition protocol has the potential to improve defect detection performance, motivating further research to design optimal patient-specific acquisition and reconstruction protocols for MPS, as well as developing similar approaches for other medical imaging modalities."],"url":"http://arxiv.org/abs/2403.17764v1","category":"physics.med-ph"}
{"created":"2024-03-26 14:17:01","title":"Masked Autoencoders are PDE Learners","abstract":"Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.","sentences":["Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability.","PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations.","As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs.","Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks.","In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations.","We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale."],"url":"http://arxiv.org/abs/2403.17728v1","category":"cs.LG"}
{"created":"2024-03-26 10:10:56","title":"Deep functional multiple index models with an application to SER","abstract":"Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities. We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model. Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework. Simulations for this new model show good performances. This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods.","sentences":["Speech Emotion Recognition (SER) plays a crucial role in advancing human-computer interaction and speech processing capabilities.","We introduce a novel deep-learning architecture designed specifically for the functional data model known as the multiple-index functional model.","Our key innovation lies in integrating adaptive basis layers and an automated data transformation search within the deep learning framework.","Simulations for this new model show good performances.","This allows us to extract features tailored for chunk-level SER, based on Mel Frequency Cepstral Coefficients (MFCCs).","We demonstrate the effectiveness of our approach on the benchmark IEMOCAP database, achieving good performance compared to existing methods."],"url":"http://arxiv.org/abs/2403.17562v1","category":"cs.SD"}
{"created":"2024-03-26 09:39:21","title":"BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat","abstract":"Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time. Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome. For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym. This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat. Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain. Other environments are open source but use less accurate simulation models. Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat domain. This article describes the building blocks of the environment and some use cases.","sentences":["Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time.","Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome.","For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym.","This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat.","Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain.","Other environments are open source but use less accurate simulation models.","Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat domain.","This article describes the building blocks of the environment and some use cases."],"url":"http://arxiv.org/abs/2403.17533v1","category":"cs.LG"}
{"created":"2024-03-26 09:31:55","title":"Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications","abstract":"Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase. Our model is available at https://huggingface.co/pkshatech/m-ST5.","sentences":["Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods.","However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored.","In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model.","By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters.","We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach.","Furthermore, we also have confirmed a positive correlation between the size of the model and its performance.","It was particularly noteworthy that languages with fewer resources or those with less linguistic similarity to English benefited more from the parameter increase.","Our model is available at https://huggingface.co/pkshatech/m-ST5."],"url":"http://arxiv.org/abs/2403.17528v1","category":"cs.CL"}
{"created":"2024-03-26 08:58:28","title":"Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies","abstract":"In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players' assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions.","sentences":["In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other).","In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations.","The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players' assumed efforts during the interaction.","We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions.","And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly.","However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions."],"url":"http://arxiv.org/abs/2403.17497v1","category":"cs.CL"}
{"created":"2024-03-26 08:32:39","title":"KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning","abstract":"Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods' overall performance. In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective. Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative. Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach.","sentences":["Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results.","However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods' overall performance.","In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective.","Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative.","Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.17486v1","category":"cs.CL"}
{"created":"2024-03-26 07:26:27","title":"Adaptive Line-Of-Sight guidance law based on vector fields path following for underactuated unmanned surface vehicle","abstract":"The focus of this paper is to develop a methodology that enables an unmanned surface vehicle (USV) to efficiently track a planned path. The introduction of a vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate trajectory tracking and minimizing the overshoot response time during USV tracking of curved paths improves the overall line-of-sight (LOS) guidance method. These improvements contribute to faster convergence to the desired path, reduce oscillations, and can mitigate the effects of persistent external disturbances. It is shown that the proposed guidance law exhibits k-exponential stability when converging to the desired path consisting of straight and curved lines. The results in the paper show that the proposed method effectively improves the accuracy of the USV tracking the desired path while ensuring the safety of the USV work.","sentences":["The focus of this paper is to develop a methodology that enables an unmanned surface vehicle (USV) to efficiently track a planned path.","The introduction of a vector field-based adaptive line-of-sight guidance law (VFALOS) for accurate trajectory tracking and minimizing the overshoot response time during USV tracking of curved paths improves the overall line-of-sight (LOS) guidance method.","These improvements contribute to faster convergence to the desired path, reduce oscillations, and can mitigate the effects of persistent external disturbances.","It is shown that the proposed guidance law exhibits k-exponential stability when converging to the desired path consisting of straight and curved lines.","The results in the paper show that the proposed method effectively improves the accuracy of the USV tracking the desired path while ensuring the safety of the USV work."],"url":"http://arxiv.org/abs/2403.17448v1","category":"cs.RO"}
{"created":"2024-03-26 07:19:06","title":"Adaptive LiDAR-Radar Fusion for Outdoor Odometry Across Dense Smoke Conditions","abstract":"Robust odometry estimation in perceptually degraded environments represents a key challenge in the field of robotics. In this paper, we propose a LiDAR-radar fusion method for robust odometry for adverse environment with LiDAR degeneracy. By comparing the LiDAR point cloud with the radar static point cloud obtained through preprocessing module, it is possible to identify instances of LiDAR degeneracy to overcome perceptual limits. We demonstrate the effectiveness of our method in challenging conditions such as dense smoke, showcasing its ability to reliably estimate odometry and identify/remove dynamic points prone to LiDAR degeneracy.","sentences":["Robust odometry estimation in perceptually degraded environments represents a key challenge in the field of robotics.","In this paper, we propose a LiDAR-radar fusion method for robust odometry for adverse environment with LiDAR degeneracy.","By comparing the LiDAR point cloud with the radar static point cloud obtained through preprocessing module, it is possible to identify instances of LiDAR degeneracy to overcome perceptual limits.","We demonstrate the effectiveness of our method in challenging conditions such as dense smoke, showcasing its ability to reliably estimate odometry and identify/remove dynamic points prone to LiDAR degeneracy."],"url":"http://arxiv.org/abs/2403.17441v1","category":"cs.RO"}
{"created":"2024-03-26 05:12:11","title":"A framework to identify supercritical and subcritical Turing bifurcations: Case study of a system sustaining cubic and quadratic autocatalysis","abstract":"In this work, we focus on an autocatalytic reaction-diffusion model and carry out multiple scale weakly nonlinear analysis. A cubic and a quadratic autocatalytic reaction system is analysed. We develop a framework to identify the critical surfaces in parameter space across which the nature of the Turing bifurcation changes from supercritical to subcritical. These are verified by direct numerical simulations of the system. Using weakly nonlinear analysis, we derive equations up to the fifth order that governs the amplitude of the spatial patterns. The limit point of the bifurcating solution is captured accurately by extending the analysis to the fifth order for the case of subcritical bifurcation. The numerical solutions are in good agreement with the predictions of the weakly nonlinear analysis for supercritical bifurcations. We show that when multiple steady states arise Turing patterns can coexist with another spatially uniform steady states. Furthermore, we show that our framework can be extended to get different patterns like squares and hexagons in a two-dimensional domain. We show that the shape of Turing patterns is influenced by the domain size. This shows that the geometry can influence the kind of patterns formed in natural systems. This study will aid the experimentalist identify operating conditions where Turing patterns can be obtained.","sentences":["In this work, we focus on an autocatalytic reaction-diffusion model and carry out multiple scale weakly nonlinear analysis.","A cubic and a quadratic autocatalytic reaction system is analysed.","We develop a framework to identify the critical surfaces in parameter space across which the nature of the Turing bifurcation changes from supercritical to subcritical.","These are verified by direct numerical simulations of the system.","Using weakly nonlinear analysis, we derive equations up to the fifth order that governs the amplitude of the spatial patterns.","The limit point of the bifurcating solution is captured accurately by extending the analysis to the fifth order for the case of subcritical bifurcation.","The numerical solutions are in good agreement with the predictions of the weakly nonlinear analysis for supercritical bifurcations.","We show that when multiple steady states arise Turing patterns can coexist with another spatially uniform steady states.","Furthermore, we show that our framework can be extended to get different patterns like squares and hexagons in a two-dimensional domain.","We show that the shape of Turing patterns is influenced by the domain size.","This shows that the geometry can influence the kind of patterns formed in natural systems.","This study will aid the experimentalist identify operating conditions where Turing patterns can be obtained."],"url":"http://arxiv.org/abs/2403.17386v1","category":"nlin.AO"}
{"created":"2024-03-26 03:51:01","title":"Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models","abstract":"We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.","sentences":["We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA).","Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information.","Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions.","Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources.","We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers.","Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods."],"url":"http://arxiv.org/abs/2403.17359v1","category":"cs.CL"}
{"created":"2024-03-26 03:31:24","title":"On the Heating of the Slow Solar-Wind by Imbalanced Alfv\u00e9n-Wave Turbulence from 0.06 au to 1 au: Parker Solar Probe and Solar Orbiter observations","abstract":"In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\\emph{PSP}) and Solar Orbiter (\\emph{SO}) missions to investigate the radial evolution of the heating of Alfv\\'enic slow wind (ASW) by imbalanced Alfv\\'en-Wave (AW) turbulent fluctuations from 0.06 au to 1 au. in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\\delta B/B\\leq 0.25$, plasma compressibility $C_n=\\delta n/n\\leq 0.25$ and normalized cross-helicity $\\sigma_c\\geq 0.65$). First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate. Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\\rm CH09}$ (with $Q_{\\rm CH09}\\simeq 1.55 Q_W$) derived by~\\cite{chandran09} based on the model of reflection-driven turbulence. Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams.","sentences":["In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\\emph{PSP}) and Solar Orbiter (\\emph{SO}) missions to investigate the radial evolution of the heating of Alfv\\'enic slow wind (ASW) by imbalanced Alfv\\'en-Wave (AW) turbulent fluctuations from 0.06 au to 1 au.","in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\\delta B/B\\leq 0.25$, plasma compressibility $C_n=\\delta n/n\\leq 0.25$ and normalized cross-helicity $\\sigma_c\\geq 0.65$).","First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate.","Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\\rm CH09}$ (with $Q_{\\rm CH09}\\simeq 1.55 Q_W$) derived by~\\cite{chandran09} based on the model of reflection-driven turbulence.","Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams."],"url":"http://arxiv.org/abs/2403.17352v1","category":"physics.space-ph"}
{"created":"2024-03-26 03:26:40","title":"On three dimensional flows of viscoelastic fluids of Giesekus type","abstract":"Viscoelastic rate-type fluids are popular models of choice in many applications involving flows of fluid-like materials with complex micro-structure. A well-developed mathematical theory for the most of these classical fluid models is however missing. The main purpose of this study is to provide a complete proof of long-time and large-data existence of weak solutions to unsteady internal three-dimensional flows of Giesekus fluids subject to a no-slip boundary condition. As a new auxiliary tool, we provide the identification of certain biting limits in the parabolic setting, presented here within the framework of evolutionary Stokes problems. We also generalize the long-time and large-data existence result to higher dimensions, to viscoelastic models with multiple relaxation mechanisms and to viscoelastic models with different type of dissipation.","sentences":["Viscoelastic rate-type fluids are popular models of choice in many applications involving flows of fluid-like materials with complex micro-structure.","A well-developed mathematical theory for the most of these classical fluid models is however missing.","The main purpose of this study is to provide a complete proof of long-time and large-data existence of weak solutions to unsteady internal three-dimensional flows of Giesekus fluids subject to a no-slip boundary condition.","As a new auxiliary tool, we provide the identification of certain biting limits in the parabolic setting, presented here within the framework of evolutionary Stokes problems.","We also generalize the long-time and large-data existence result to higher dimensions, to viscoelastic models with multiple relaxation mechanisms and to viscoelastic models with different type of dissipation."],"url":"http://arxiv.org/abs/2403.17348v1","category":"math.AP"}
{"created":"2024-03-26 03:08:00","title":"How many bits does your quantum estimation return?","abstract":"We give two upper bounds to the mutual information in arbitrary quantum estimation strategies. The first is based on some simple Fourier properties of the estimation apparatus. The second is derived using the first but, interestingly, depends only on the Fisher information of the parameter, so it is valid even beyond quantum estimation. We illustrate the usefulness of these bounds by characterizing the quantum phase estimation algorithm in the presence of noise. In addition, for the noiseless case, we extend the analysis beyond applying the bound and we discuss the optimal entangled and adaptive strategies, clarifying inaccuracies appearing on this topic in the literature.","sentences":["We give two upper bounds to the mutual information in arbitrary quantum estimation strategies.","The first is based on some simple Fourier properties of the estimation apparatus.","The second is derived using the first but, interestingly, depends only on the Fisher information of the parameter, so it is valid even beyond quantum estimation.","We illustrate the usefulness of these bounds by characterizing the quantum phase estimation algorithm in the presence of noise.","In addition, for the noiseless case, we extend the analysis beyond applying the bound and we discuss the optimal entangled and adaptive strategies, clarifying inaccuracies appearing on this topic in the literature."],"url":"http://arxiv.org/abs/2403.17345v1","category":"quant-ph"}
{"created":"2024-03-26 00:36:03","title":"Linear Numerical Schemes for a $\\textbf{Q}$-Tensor System for Nematic Liquid Crystals","abstract":"In this work, we present three linear numerical schemes to model nematic liquid crystals using the Landau-de Gennes $\\textbf{Q}$-tensor theory. The first scheme is based on using a truncation procedure of the energy, which allows for an unconditionally energy stable first order accurate decoupled scheme. The second scheme uses a modified second order accurate optimal dissipation algorithm, which gives a second order accurate coupled scheme. Finally, the third scheme uses a new idea to decouple the unknowns from the second scheme which allows us to obtain accurate dynamics while improving computational efficiency. We present several numerical experiments to offer a comparative study of the accuracy, efficiency and the ability of the numerical schemes to represent realistic dynamics.","sentences":["In this work, we present three linear numerical schemes to model nematic liquid crystals using the Landau-de Gennes $\\textbf{Q}$-tensor theory.","The first scheme is based on using a truncation procedure of the energy, which allows for an unconditionally energy stable first order accurate decoupled scheme.","The second scheme uses a modified second order accurate optimal dissipation algorithm, which gives a second order accurate coupled scheme.","Finally, the third scheme uses a new idea to decouple the unknowns from the second scheme which allows us to obtain accurate dynamics while improving computational efficiency.","We present several numerical experiments to offer a comparative study of the accuracy, efficiency and the ability of the numerical schemes to represent realistic dynamics."],"url":"http://arxiv.org/abs/2403.17289v1","category":"math.NA"}
{"created":"2024-03-25 23:04:09","title":"Latency-Aware Generative Semantic Communications with Pre-Trained Diffusion Models","abstract":"Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this paper, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.","sentences":["Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual prompts and conditioning signals to guide the generation process.","This enables semantic communications at extremely low data rates in future wireless networks.","In this paper, we develop a latency-aware semantic communications framework with pre-trained generative models.","The transmitter performs multi-modal semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent.","For the prompt, we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel.","Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints.","At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics.","Simulation results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications."],"url":"http://arxiv.org/abs/2403.17256v1","category":"cs.IT"}
{"created":"2024-03-25 20:44:01","title":"Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study","abstract":"Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep models that were recently proposed and evaluated their performance for stroke segmentation: a pure Transformer-based architecture (DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention mechanisms in their design, an advanced hybrid model that incorporates CNNs with Transformers (FCT), and the well- known self-adaptive nnUNet framework with its configuration based on given data. We examined their performance on two publicly available datasets, and found that the nnUNet achieved the best results with the simplest design among all. Revealing the robustness issue of Transformers to such variabilities serves as a potential reason for their weaker performance. Furthermore, nnUNet's success underscores the significant impact of preprocessing and postprocessing techniques in enhancing segmentation results, surpassing the focus solely on architectural designs","sentences":["Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage.","Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors.","Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets.","With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers.","The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered.","In this study, we selected four types of deep models that were recently proposed and evaluated their performance for stroke segmentation: a pure Transformer-based architecture (DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention mechanisms in their design, an advanced hybrid model that incorporates CNNs with Transformers (FCT), and the well- known self-adaptive nnUNet framework with its configuration based on given data.","We examined their performance on two publicly available datasets, and found that the nnUNet achieved the best results with the simplest design among all.","Revealing the robustness issue of Transformers to such variabilities serves as a potential reason for their weaker performance.","Furthermore, nnUNet's success underscores the significant impact of preprocessing and postprocessing techniques in enhancing segmentation results, surpassing the focus solely on architectural designs"],"url":"http://arxiv.org/abs/2403.17177v1","category":"eess.IV"}
{"created":"2024-03-25 19:40:26","title":"Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language","abstract":"Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as well. Furthermore, we experiment with multilingual and cross-lingual experiments that could benefit many low-resource languages.","sentences":["Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects.","There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships.","However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English.","This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German.","Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset.","We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision.","We train several state-of-the-art machine learning models on the automatically created dataset and release them as well.","Furthermore, we experiment with multilingual and cross-lingual experiments that could benefit many low-resource languages."],"url":"http://arxiv.org/abs/2403.17143v1","category":"cs.CL"}
{"created":"2024-03-25 19:39:17","title":"Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control","abstract":"Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning. A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the weights uniformly over a sphere and the biases uniformly over an interval. We show how the result can be used to get approximations of required accuracy in a model reference adaptive control application.","sentences":["Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning.","A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained.","While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees.","Thus, the approximation properties required for control with neural networks are assumed, rather than proved.","In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons.","It suffices to generate the weights uniformly over a sphere and the biases uniformly over an interval.","We show how the result can be used to get approximations of required accuracy in a model reference adaptive control application."],"url":"http://arxiv.org/abs/2403.17142v1","category":"math.OC"}
{"created":"2024-03-25 19:23:44","title":"An Equilibrium Analysis of the Arad-Rubinstein Game","abstract":"Colonel Blotto games with discrete strategy spaces effectively illus- trate the intricate nature of multidimensional strategic reasoning. This paper studies the equilibrium set of such games where, in line with prior experimental work, the tie-breaking rule is allowed to be flexible. We begin by pointing out that equilibrium constructions known from the literature extend to our class of games. However, we also note that, irrespective of the tie-breaking rule, the equilibrium set is excessively large. Specifically, any pure strategy that allocates at most twice the fair share to each battlefield is used with positive probability in some equilibrium. Furthermore, refinements based on the elimination of weakly dominated strategies prove ineffective. To derive specific predictions amid this multiplicity, we compute strategies resulting from long-run adaptive learning.","sentences":["Colonel Blotto games with discrete strategy spaces effectively illus-","trate the intricate nature of multidimensional strategic reasoning.","This paper studies the equilibrium set of such games where, in line with prior experimental work, the tie-breaking rule is allowed to be flexible.","We begin by pointing out that equilibrium constructions known from the literature extend to our class of games.","However, we also note that, irrespective of the tie-breaking rule, the equilibrium set is excessively large.","Specifically, any pure strategy that allocates at most twice the fair share to each battlefield is used with positive probability in some equilibrium.","Furthermore, refinements based on the elimination of weakly dominated strategies prove ineffective.","To derive specific predictions amid this multiplicity, we compute strategies resulting from long-run adaptive learning."],"url":"http://arxiv.org/abs/2403.17139v1","category":"cs.GT"}
{"created":"2024-03-25 19:18:25","title":"Adaptive Step Duration for Precise Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds","abstract":"This paper introduces a novel multi-step preview foot placement planning algorithm designed to enhance the robustness of bipedal robotic walking across challenging terrains with restricted footholds. Traditional one-step preview planning struggles to maintain stability when stepping areas are severely limited, such as with random stepping stones. In this work, we developed a discrete-time Model Predictive Control (MPC) based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of bipedal locomotion. This approach adaptively changes the step duration for optimal foot placement under constraints, thereby ensuring the robot's operational viability over multiple future steps and significantly improving its ability to navigate through environments with tight constraints on possible footholds. The effectiveness of this planning algorithm is demonstrated through simulations that include a variety of complex stepping-stone configurations and external perturbations. These tests underscore the algorithm's improved performance for navigating foothold-restricted environments, even with the presence of external disturbances.","sentences":["This paper introduces a novel multi-step preview foot placement planning algorithm designed to enhance the robustness of bipedal robotic walking across challenging terrains with restricted footholds.","Traditional one-step preview planning struggles to maintain stability when stepping areas are severely limited, such as with random stepping stones.","In this work, we developed a discrete-time Model Predictive Control (MPC) based on the step-to-step discrete evolution of the Divergent Component of Motion (DCM) of bipedal locomotion.","This approach adaptively changes the step duration for optimal foot placement under constraints, thereby ensuring the robot's operational viability over multiple future steps and significantly improving its ability to navigate through environments with tight constraints on possible footholds.","The effectiveness of this planning algorithm is demonstrated through simulations that include a variety of complex stepping-stone configurations and external perturbations.","These tests underscore the algorithm's improved performance for navigating foothold-restricted environments, even with the presence of external disturbances."],"url":"http://arxiv.org/abs/2403.17136v1","category":"cs.RO"}
{"created":"2024-03-25 17:59:41","title":"Invertible Diffusion Models for Compressed Sensing","abstract":"While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference.","sentences":["While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment.","Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS.","To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method.","IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and finetunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning.","To enable such memory-intensive end-to-end finetuning, we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks.","As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory.","In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction.","Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR.","Compared to the recent diffusion model-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference."],"url":"http://arxiv.org/abs/2403.17006v1","category":"cs.CV"}
{"created":"2024-03-25 17:59:26","title":"Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution","abstract":"Diffusion models are just at a tipping point for image super-resolution task. Nevertheless, it is not trivial to capitalize on diffusion models for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames. In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution. SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction. Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA delves into feature interaction within a 3D local window (tubelet) through self-attention, and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment. Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach.","sentences":["Diffusion models are just at a tipping point for image super-resolution task.","Nevertheless, it is not trivial to capitalize on diffusion models for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames.","In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution.","SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction.","Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE.","SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis.","TFA delves into feature interaction within a 3D local window (tubelet) through self-attention, and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment.","Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.17000v1","category":"cs.CV"}
{"created":"2024-03-25 17:54:21","title":"Complex-Valued Signal Recovery using the Bayesian LASSO","abstract":"Recovering complex-valued image recovery from noisy indirect data is important in applications such as ultrasound imaging and synthetic aperture radar. While there are many effective algorithms to recover point estimates of the magnitude, fewer are designed to recover the phase. Quantifying uncertainty in the estimate can also provide valuable information for real-time decision making. This investigation therefore proposes a new Bayesian inference method that recovers point estimates while also quantifying the uncertainty for complex-valued signals or images given noisy and indirect observation data. Our method is motivated by the Bayesian LASSO approach for real-valued sparse signals, and here we demonstrate that the Bayesian LASSO can be effectively adapted to recover complex-valued images whose magnitude is sparse in some (e.g.~the gradient) domain. Numerical examples demonstrate our algorithm's robustness to noise as well as its computational efficiency.","sentences":["Recovering complex-valued image recovery from noisy indirect data is important in applications such as ultrasound imaging and synthetic aperture radar.","While there are many effective algorithms to recover point estimates of the magnitude, fewer are designed to recover the phase.","Quantifying uncertainty in the estimate can also provide valuable information for real-time decision making.","This investigation therefore proposes a new Bayesian inference method that recovers point estimates while also quantifying the uncertainty for complex-valued signals or images given noisy and indirect observation data.","Our method is motivated by the Bayesian LASSO approach for real-valued sparse signals, and here we demonstrate that the Bayesian LASSO can be effectively adapted to recover complex-valued images whose magnitude is sparse in some (e.g.~the gradient) domain.","Numerical examples demonstrate our algorithm's robustness to noise as well as its computational efficiency."],"url":"http://arxiv.org/abs/2403.16992v1","category":"math.NA"}
{"created":"2024-03-25 17:48:06","title":"Dynamic Relative Representations for Goal-Oriented Semantic Communications","abstract":"In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating mismatches among devices, while optimizing energy consumption, delay, and effectiveness.","sentences":["In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions.","However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding.","In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data.","This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment.","We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications.","Numerical results demonstrate our methodology's effectiveness in mitigating mismatches among devices, while optimizing energy consumption, delay, and effectiveness."],"url":"http://arxiv.org/abs/2403.16986v1","category":"cs.NI"}
{"created":"2024-03-25 17:46:51","title":"Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video Streaming","abstract":"Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an increasingly popular approach in both live and video-on-demand (VoD) applications. However, designing a scalable and adaptable framework that reduces servers energy consumption and supports low latency and high quality services, particularly for live video streaming scenarios, is still challenging for Over-The-Top (OTT) service providers. To address such challenges, this paper introduces a new hybrid P2P-CDN framework that leverages new networking and computing paradigms, i.e., Network Function Virtualization (NFV) and edge computing for live video streaming. The proposed framework introduces a multi-layer architecture and a tree of possible actions therein (an action tree), taking into account all available resources from peers, edge, and CDN servers to efficiently distribute video fetching and transcoding tasks across a hybrid P2P-CDN network, consequently enhancing the users latency and video quality. We also discuss our testbed designed to validate the framework and compare it with baseline methods. The experimental results indicate that the proposed framework improves user Quality of Experience (QoE), reduces client serving latency, and improves edge server energy consumption compared to baseline approaches.","sentences":["Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an increasingly popular approach in both live and video-on-demand (VoD) applications.","However, designing a scalable and adaptable framework that reduces servers energy consumption and supports low latency and high quality services, particularly for live video streaming scenarios, is still challenging for Over-The-Top (OTT) service providers.","To address such challenges, this paper introduces a new hybrid P2P-CDN framework that leverages new networking and computing paradigms, i.e., Network Function Virtualization (NFV) and edge computing for live video streaming.","The proposed framework introduces a multi-layer architecture and a tree of possible actions therein (an action tree), taking into account all available resources from peers, edge, and CDN servers to efficiently distribute video fetching and transcoding tasks across a hybrid P2P-CDN network, consequently enhancing the users latency and video quality.","We also discuss our testbed designed to validate the framework and compare it with baseline methods.","The experimental results indicate that the proposed framework improves user Quality of Experience (QoE), reduces client serving latency, and improves edge server energy consumption compared to baseline approaches."],"url":"http://arxiv.org/abs/2403.16985v1","category":"cs.MM"}
{"created":"2024-03-25 17:43:55","title":"Robust Filter Design for Graph Signals","abstract":"Our goal in this paper is the robust design of filters acting on signals observed over graphs subject to small perturbations of their edges. The focus is on developing a method to identify spectral and polynomial graph filters that can adapt to the perturbations in the underlying graph structure while ensuring the filters adhere to the desired spectral mask. To address this, we propose a novel approach that leverages approximate closed-form expressions for the perturbed eigendecomposition of the Laplacian matrix associated with the nominal topology. Furthermore, when dealing with noisy input signals for graph filters, we propose a strategy for designing FIR filters that jointly minimize the approximation error with respect to the ideal filter and the estimation error of the output, ensuring robustness against both graph perturbations and noise. Numerical results validate the effectiveness of our proposed strategies, highlighting their capability to efficiently manage perturbations and noise.","sentences":["Our goal in this paper is the robust design of filters acting on signals observed over graphs subject to small perturbations of their edges.","The focus is on developing a method to identify spectral and polynomial graph filters that can adapt to the perturbations in the underlying graph structure while ensuring the filters adhere to the desired spectral mask.","To address this, we propose a novel approach that leverages approximate closed-form expressions for the perturbed eigendecomposition of the Laplacian matrix associated with the nominal topology.","Furthermore, when dealing with noisy input signals for graph filters, we propose a strategy for designing FIR filters that jointly minimize the approximation error with respect to the ideal filter and the estimation error of the output, ensuring robustness against both graph perturbations and noise.","Numerical results validate the effectiveness of our proposed strategies, highlighting their capability to efficiently manage perturbations and noise."],"url":"http://arxiv.org/abs/2403.16983v1","category":"cs.DM"}
{"created":"2024-03-25 17:12:43","title":"Network-Assisted Delivery of Adaptive Video Streaming Services through CDN, SDN, and MEC","abstract":"Multimedia applications, mainly video streaming services, are currently the dominant source of network load worldwide. In recent Video-on-Demand (VoD) and live video streaming services, traditional streaming delivery techniques have been replaced by adaptive solutions based on the HTTP protocol. Current trends toward high-resolution (e.g., 8K) and/or low-latency VoD and live video streaming pose new challenges to end-to-end (E2E) bandwidth demand and have stringent delay requirements. To do this, video providers typically rely on Content Delivery Networks (CDNs) to ensure that they provide scalable video streaming services. To support future streaming scenarios involving millions of users, it is necessary to increase the CDNs' efficiency. It is widely agreed that these requirements may be satisfied by adopting emerging networking techniques to present Network-Assisted Video Streaming (NAVS) methods. Motivated by this, this thesis goes one step beyond traditional pure client-based HAS algorithms by incorporating (an) in-network component(s) with a broader view of the network to present completely transparent NAVS solutions for HAS clients.","sentences":["Multimedia applications, mainly video streaming services, are currently the dominant source of network load worldwide.","In recent Video-on-Demand (VoD) and live video streaming services, traditional streaming delivery techniques have been replaced by adaptive solutions based on the HTTP protocol.","Current trends toward high-resolution (e.g., 8K) and/or low-latency VoD and live video streaming pose new challenges to end-to-end (E2E) bandwidth demand and have stringent delay requirements.","To do this, video providers typically rely on Content Delivery Networks (CDNs) to ensure that they provide scalable video streaming services.","To support future streaming scenarios involving millions of users, it is necessary to increase the CDNs' efficiency.","It is widely agreed that these requirements may be satisfied by adopting emerging networking techniques to present Network-Assisted Video Streaming (NAVS) methods.","Motivated by this, this thesis goes one step beyond traditional pure client-based HAS algorithms by incorporating (an) in-network component(s) with a broader view of the network to present completely transparent NAVS solutions for HAS clients."],"url":"http://arxiv.org/abs/2403.16951v1","category":"cs.MM"}
{"created":"2024-03-25 17:10:39","title":"Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling","abstract":"Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions. However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge. In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders. The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately capture nuanced user preferences on actions. Moreover, the LE allows to generate positive actions that augment the limited offline training data. We propose a LE Augmentation (LEA) method to further improve recommendation performance by optimising jointly the supervised component and the RL policy, using the augmented actions and historical user signals. We use LEA, the state and reward models in conjunction with state-of-the-art RL recommenders and report experimental results on two publicly available datasets.","sentences":["Reinforcement Learning (RL)-based recommender systems have demonstrated promising performance in meeting user expectations by learning to make accurate next-item recommendations from historical user-item interactions.","However, existing offline RL-based sequential recommendation methods face the challenge of obtaining effective user feedback from the environment.","Effectively modeling the user state and shaping an appropriate reward for recommendation remains a challenge.","In this paper, we leverage language understanding capabilities and adapt large language models (LLMs) as an environment (LE) to enhance RL-based recommenders.","The LE is learned from a subset of user-item interaction data, thus reducing the need for large training data, and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately capture nuanced user preferences on actions.","Moreover, the LE allows to generate positive actions that augment the limited offline training data.","We propose a LE Augmentation (LEA) method to further improve recommendation performance by optimising jointly the supervised component and the RL policy, using the augmented actions and historical user signals.","We use LEA, the state and reward models in conjunction with state-of-the-art RL recommenders and report experimental results on two publicly available datasets."],"url":"http://arxiv.org/abs/2403.16948v1","category":"cs.IR"}
{"created":"2024-03-25 16:47:36","title":"Stochastic Active Discretizations for Accelerating Temporal Uncertainty Management of Gas Pipeline Loads","abstract":"We propose a predictor-corrector adaptive method for the simulation of hyperbolic partial differential equations (PDEs) on networks under general uncertainty in parameters, initial conditions, or boundary conditions. The approach is based on the stochastic finite volume (SFV) framework that circumvents sampling schemes or simulation ensembles while also preserving fundamental properties, in particular hyperbolicity of the resulting systems and conservation of the discrete solutions. The initial boundary value problem (IBVP) on a set of network-connected one-dimensional domains that represent a pipeline is represented using active discretization of the physical and stochastic spaces, and we evaluate the propagation of uncertainty through network nodes by solving a junction Riemann problem. The adaptivity of our method in refining discretization based on error metrics enables computationally tractable evaluation of intertemporal uncertainty in order to support decisions about timing and quantity of pipeline operations to maximize delivery under transient and uncertain conditions. We illustrate our computational method using simulations for a representative network.","sentences":["We propose a predictor-corrector adaptive method for the simulation of hyperbolic partial differential equations (PDEs) on networks under general uncertainty in parameters, initial conditions, or boundary conditions.","The approach is based on the stochastic finite volume (SFV) framework that circumvents sampling schemes or simulation ensembles while also preserving fundamental properties, in particular hyperbolicity of the resulting systems and conservation of the discrete solutions.","The initial boundary value problem (IBVP) on a set of network-connected one-dimensional domains that represent a pipeline is represented using active discretization of the physical and stochastic spaces, and we evaluate the propagation of uncertainty through network nodes by solving a junction Riemann problem.","The adaptivity of our method in refining discretization based on error metrics enables computationally tractable evaluation of intertemporal uncertainty in order to support decisions about timing and quantity of pipeline operations to maximize delivery under transient and uncertain conditions.","We illustrate our computational method using simulations for a representative network."],"url":"http://arxiv.org/abs/2403.16929v1","category":"math.NA"}
{"created":"2024-03-25 16:31:56","title":"Solving the unique continuation problem for Schr\u00f6dinger equations with low regularity solutions using a stabilized finite element method","abstract":"In this paper, we consider the unique continuation problem for the Schr\\\"odinger equations. We prove a H\\\"older type conditional stability estimate and build up a parameterized stabilized finite element scheme adaptive to the \\textit{a priori} knowledge of the solution, achieving error estimates in interior domains with convergence up to continuous stability. The approximability of the scheme to solutions with only $H^1$-regularity is studied and the convergence rate for solutions with regularity higher than $H^1$ is also shown. Comparisons in terms of different parameterization for different regularities will be illustrated with respect to the convergence and condition numbers of the linear systems. Finally, numerical experiments will be given to illustrate the theory.","sentences":["In this paper, we consider the unique continuation problem for the Schr\\\"odinger equations.","We prove a H\\\"older type conditional stability estimate and build up a parameterized stabilized finite element scheme adaptive to the \\textit{a priori} knowledge of the solution, achieving error estimates in interior domains with convergence up to continuous stability.","The approximability of the scheme to solutions with only $H^1$-regularity is studied and the convergence rate for solutions with regularity higher than $H^1$ is also shown.","Comparisons in terms of different parameterization for different regularities will be illustrated with respect to the convergence and condition numbers of the linear systems.","Finally, numerical experiments will be given to illustrate the theory."],"url":"http://arxiv.org/abs/2403.16914v1","category":"math.NA"}
{"created":"2024-03-25 16:31:55","title":"New Intent Discovery with Attracting and Dispersing Prototype","abstract":"New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data. The task is addressed as a feature-clustering problem and recent studies augment instance representation. However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances. Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories. Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness. To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective. Experimental results evaluated on three challenging benchmarks (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even large language model) by a large margin (average +5.5% improvement).","sentences":["New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and large-scale unlabeled data.","The task is addressed as a feature-clustering problem and recent studies augment instance representation.","However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances.","Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories.","Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness.","To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective.","Experimental results evaluated on three challenging benchmarks (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even large language model) by a large margin (average +5.5% improvement)."],"url":"http://arxiv.org/abs/2403.16913v1","category":"cs.CL"}
{"created":"2024-03-25 16:12:19","title":"Hyperpixels: Pixel Filter Arrays of Multivariate Optical Elements for Optimized Spectral Imaging","abstract":"We introduce the concept of `hyperpixels' in which each element of a pixel filter array (suitable for CMOS image sensor integration) has a spectral transmission tailored to a target spectral component expected in application-specific scenes. These are analogous to arrays of multivariate optical elements that could be used for sensing specific analytes. Spectral tailoring is achieved by engineering the heights of multiple sub-pixel Fabry-Perot resonators that cover each pixel area. We first present a design approach for hyperpixels, based on a matched filter concept and, as an exemplar, design a set of 4 hyperpixels tailored to optimally discriminate between 4 spectral reflectance targets. Next, we fabricate repeating 2x2 pixel filter arrays of these designs, alongside repeating 2x2 arrays of an optimal bandpass filters, perform both spectral and imaging characterization. Experimentally measured hyperpixel transmission spectra show a 2.4x reduction in unmixing matrix condition number (p=0.031) compared to the optimal band-pass set. Imaging experiments using the filter arrays with a monochrome sensor achieve a 3.47x reduction in unmixing matrix condition number (p=0.020) compared to the optimal band-pass set. This demonstrates the utility of the hyperpixel approach and shows its superiority even over the optimal bandpass case. We expect that with further improvements in design and fabrication processes increased performance may be obtained. Because the hyperpixels are straightforward to customize, fabricate and can be placed atop monochrome sensors, this approach is highly versatile and could be adapted to a wide range of real-time imaging applications which are limited by low SNR including micro-endoscopy, capsule endoscopy, industrial inspection and machine vision.","sentences":["We introduce the concept of `hyperpixels' in which each element of a pixel filter array (suitable for CMOS image sensor integration) has a spectral transmission tailored to a target spectral component expected in application-specific scenes.","These are analogous to arrays of multivariate optical elements that could be used for sensing specific analytes.","Spectral tailoring is achieved by engineering the heights of multiple sub-pixel Fabry-Perot resonators that cover each pixel area.","We first present a design approach for hyperpixels, based on a matched filter concept and, as an exemplar, design a set of 4 hyperpixels tailored to optimally discriminate between 4 spectral reflectance targets.","Next, we fabricate repeating 2x2 pixel filter arrays of these designs, alongside repeating 2x2 arrays of an optimal bandpass filters, perform both spectral and imaging characterization.","Experimentally measured hyperpixel transmission spectra show a 2.4x reduction in unmixing matrix condition number (p=0.031) compared to the optimal band-pass set.","Imaging experiments using the filter arrays with a monochrome sensor achieve a 3.47x reduction in unmixing matrix condition number (p=0.020) compared to the optimal band-pass set.","This demonstrates the utility of the hyperpixel approach and shows its superiority even over the optimal bandpass case.","We expect that with further improvements in design and fabrication processes increased performance may be obtained.","Because the hyperpixels are straightforward to customize, fabricate and can be placed atop monochrome sensors, this approach is highly versatile and could be adapted to a wide range of real-time imaging applications which are limited by low SNR including micro-endoscopy, capsule endoscopy, industrial inspection and machine vision."],"url":"http://arxiv.org/abs/2403.16901v1","category":"physics.optics"}
{"created":"2024-03-25 15:53:32","title":"Discrete Latent Graph Generative Modeling with Diffusion Bridges","abstract":"Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLAD the first latent space graph generative model with competitive performance. Our source code is published at: \\url{https://github.com/v18nguye/GLAD}.","sentences":["Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance.","We present GLAD a latent space graph generative model.","Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity.","We learn the prior of our discrete latent space by adapting diffusion bridges to its structure.","By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space.","We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLAD the first latent space graph generative model with competitive performance.","Our source code is published at: \\url{https://github.com/v18nguye/GLAD}."],"url":"http://arxiv.org/abs/2403.16883v1","category":"cs.LG"}
{"created":"2024-03-25 14:48:00","title":"Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A User-Centric Learning Approach","abstract":"Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets) are an emerging indoor wireless communication paradigm, which combines the advantages of the capacious optical spectra of LiFi and ubiquitous coverage of WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource management for such hybrid networks. The existing LB methods are mostly network-centric, relying on a central unit to make a solution for the users all at once. Consequently, the solution needs to be updated for all users at the same pace, regardless of their moving status. This would affect the network performance in two aspects: i) when the update frequency is low, it would compromise the connectivity of fast-moving users; ii) when the update frequency is high, it would cause unnecessary handovers as well as hefty feedback costs for slow-moving users. Motivated by this, we investigate user-centric LB which allows users to update their solutions at different paces. The research is developed upon our previous work on adaptive target-condition neural network (ATCNN), which can conduct LB for individual users in quasi-static channels. In this paper, a deep neural network (DNN) model is designed to enable an adaptive update interval for each individual user. This new model is termed as mobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a user-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is proposed to handle resource management and mobility management simultaneously. Results show that at the same level of average update interval, MS-ATCNN can achieve a network throughput up to 215\\% higher than conventional LB methods such as game theory, especially for a larger number of users. In addition, MS-ATCNN costs an ultra low runtime at the level of 100s $\\mu$s, which is two to three orders of magnitude lower than game theory.","sentences":["Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets) are an emerging indoor wireless communication paradigm, which combines the advantages of the capacious optical spectra of LiFi and ubiquitous coverage of WiFi.","Meanwhile, load balancing (LB) becomes a key challenge in resource management for such hybrid networks.","The existing LB methods are mostly network-centric, relying on a central unit to make a solution for the users all at once.","Consequently, the solution needs to be updated for all users at the same pace, regardless of their moving status.","This would affect the network performance in two aspects: i) when the update frequency is low, it would compromise the connectivity of fast-moving users; ii) when the update frequency is high, it would cause unnecessary handovers as well as hefty feedback costs for slow-moving users.","Motivated by this, we investigate user-centric LB which allows users to update their solutions at different paces.","The research is developed upon our previous work on adaptive target-condition neural network (ATCNN), which can conduct LB for individual users in quasi-static channels.","In this paper, a deep neural network (DNN) model is designed to enable an adaptive update interval for each individual user.","This new model is termed as mobility-supporting neural network (MSNN).","Associating MSNN with ATCNN, a user-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is proposed to handle resource management and mobility management simultaneously.","Results show that at the same level of average update interval, MS-ATCNN can achieve a network throughput up to 215\\% higher than conventional LB methods such as game theory, especially for a larger number of users.","In addition, MS-ATCNN costs an ultra low runtime at the level of 100s $\\mu$s, which is two to three orders of magnitude lower than game theory."],"url":"http://arxiv.org/abs/2403.16823v1","category":"eess.SY"}
{"created":"2024-03-25 14:32:28","title":"An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems","abstract":"The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL-RL demonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency. The project's code can be found on our GitHub repository.","sentences":["The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment.","For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption.","Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice.","We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization.","In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall.","The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort.","Our results show that LLMs are capable of simulating complex population movements within large open spaces.","Besides, AitL-RL demonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications.","Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency.","The project's code can be found on our GitHub repository."],"url":"http://arxiv.org/abs/2403.16809v1","category":"eess.SY"}
{"created":"2024-03-25 14:17:38","title":"Cluster-Based Normalization Layer for Neural Networks","abstract":"Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions. This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration. For SCB-Norm, a supervised variant, the novel mechanism involves introducing predefined data partitioning, termed clusters, to normalize activations based on the assigned cluster. This cluster-driven approach creates a space that conforms to a Gaussian mixture model. On the other hand, UCB-Norm, an unsupervised counterpart, dynamically clusters neuron activations during training, adapting to task-specific challenges without relying on predefined data partitions (clusters). This dual approach ensures flexibility in addressing diverse learning scenarios. CB-Norm innovatively uses a one-step normalization approach, where parameters of each mixture component (cluster in activation space) serve as weights for deep neural networks. This adaptive clustering process tackles both clustering and resolution of deep neural network tasks concurrently during training, signifying a notable advancement in the field.","sentences":["Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity.","While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability.","Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.","This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach.","CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.","For SCB-Norm, a supervised variant, the novel mechanism involves introducing predefined data partitioning, termed clusters, to normalize activations based on the assigned cluster.","This cluster-driven approach creates a space that conforms to a Gaussian mixture model.","On the other hand, UCB-Norm, an unsupervised counterpart, dynamically clusters neuron activations during training, adapting to task-specific challenges without relying on predefined data partitions (clusters).","This dual approach ensures flexibility in addressing diverse learning scenarios.","CB-Norm innovatively uses a one-step normalization approach, where parameters of each mixture component (cluster in activation space) serve as weights for deep neural networks.","This adaptive clustering process tackles both clustering and resolution of deep neural network tasks concurrently during training, signifying a notable advancement in the field."],"url":"http://arxiv.org/abs/2403.16798v1","category":"cs.LG"}
{"created":"2024-03-25 14:13:09","title":"CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation","abstract":"Curb detection is an important function in intelligent driving and can be used to determine drivable areas of the road. However, curbs are difficult to detect due to the complex road environment. This paper introduces CurbNet, a novel framework for curb detection, leveraging point cloud segmentation. Addressing the dearth of comprehensive curb datasets and the absence of 3D annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames, which represents the largest and most categorically diverse collection of curb point clouds currently available. Recognizing that curbs are primarily characterized by height variations, our approach harnesses spatially-rich 3D point clouds for training. To tackle the challenges presented by the uneven distribution of curb features on the xy-plane and their reliance on z-axis high-frequency features, we introduce the multi-scale and channel attention (MSCA) module, a bespoke solution designed to optimize detection performance. Moreover, we propose an adaptive weighted loss function group, specifically formulated to counteract the imbalance in the distribution of curb point clouds relative to other categories. Our extensive experimentation on 2 major datasets has yielded results that surpass existing benchmarks set by leading curb detection and point cloud segmentation models. By integrating multi-clustering and curve fitting techniques in our post-processing stage, we have substantially reduced noise in curb detection, thereby enhancing precision to 0.8744. Notably, CurbNet has achieved an exceptional average metrics of over 0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark. Furthermore, corroborative real-world experiments and dataset analyzes mutually validate each other, solidifying CurbNet's superior detection proficiency and its robust generalizability.","sentences":["Curb detection is an important function in intelligent driving and can be used to determine drivable areas of the road.","However, curbs are difficult to detect due to the complex road environment.","This paper introduces CurbNet, a novel framework for curb detection, leveraging point cloud segmentation.","Addressing the dearth of comprehensive curb datasets and the absence of 3D annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames, which represents the largest and most categorically diverse collection of curb point clouds currently available.","Recognizing that curbs are primarily characterized by height variations, our approach harnesses spatially-rich 3D point clouds for training.","To tackle the challenges presented by the uneven distribution of curb features on the xy-plane and their reliance on z-axis high-frequency features, we introduce the multi-scale and channel attention (MSCA) module, a bespoke solution designed to optimize detection performance.","Moreover, we propose an adaptive weighted loss function group, specifically formulated to counteract the imbalance in the distribution of curb point clouds relative to other categories.","Our extensive experimentation on 2 major datasets has yielded results that surpass existing benchmarks set by leading curb detection and point cloud segmentation models.","By integrating multi-clustering and curve fitting techniques in our post-processing stage, we have substantially reduced noise in curb detection, thereby enhancing precision to 0.8744.","Notably, CurbNet has achieved an exceptional average metrics of over 0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.","Furthermore, corroborative real-world experiments and dataset analyzes mutually validate each other, solidifying CurbNet's superior detection proficiency and its robust generalizability."],"url":"http://arxiv.org/abs/2403.16794v1","category":"cs.CV"}
{"created":"2024-03-25 14:06:18","title":"Nonlinear dynamics as a ground-state solution on quantum computers","abstract":"For the solution of time-dependent nonlinear differential equations, we present variational quantum algorithms (VQAs) that encode both space and time in qubit registers. The spacetime encoding enables us to obtain the entire time evolution from a single ground-state computation. We describe a general procedure to construct efficient quantum circuits for the cost function evaluation required by VQAs. To mitigate the barren plateau problem during the optimization, we propose an adaptive multigrid strategy. The approach is illustrated for the nonlinear Burgers equation. We classically optimize quantum circuits to represent the desired ground-state solutions, run them on IBM Q System One and Quantinuum System Model H1, and demonstrate that current quantum computers are capable of accurately reproducing the exact results.","sentences":["For the solution of time-dependent nonlinear differential equations, we present variational quantum algorithms (VQAs) that encode both space and time in qubit registers.","The spacetime encoding enables us to obtain the entire time evolution from a single ground-state computation.","We describe a general procedure to construct efficient quantum circuits for the cost function evaluation required by VQAs.","To mitigate the barren plateau problem during the optimization, we propose an adaptive multigrid strategy.","The approach is illustrated for the nonlinear Burgers equation.","We classically optimize quantum circuits to represent the desired ground-state solutions, run them on IBM Q System One and Quantinuum System Model H1, and demonstrate that current quantum computers are capable of accurately reproducing the exact results."],"url":"http://arxiv.org/abs/2403.16791v1","category":"quant-ph"}
{"created":"2024-03-25 14:02:33","title":"HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation","abstract":"Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras. Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training. However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors. This drawback is also called confirmation bias in pseudo-labeling. In this paper, we propose a novel hybrid pseudo-labeling framework for unsupervised event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, we first employ a plain unsupervised domain adaptation framework as our baseline, which can generate a set of pseudo labels through self-training. Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images. A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality. Moreover, we propose a soft prototypical alignment module to further improve the consistency of target domain features. Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several supervised methods.","sentences":["Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras.","Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training.","However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors.","This drawback is also called confirmation bias in pseudo-labeling.","In this paper, we propose a novel hybrid pseudo-labeling framework for unsupervised event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels.","In particular, we first employ a plain unsupervised domain adaptation framework as our baseline, which can generate a set of pseudo labels through self-training.","Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images.","A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality.","Moreover, we propose a soft prototypical alignment module to further improve the consistency of target domain features.","Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several supervised methods."],"url":"http://arxiv.org/abs/2403.16788v1","category":"cs.CV"}
{"created":"2024-03-25 14:02:22","title":"Local search and trajectory metaheuristics for the flexible job shop scheduling problem with sequencing flexibility and position-based learning effect","abstract":"The flexible job shop scheduling problem with sequencing flexibility and position-based learning effect is considered in the present work. In [K. A. G. Araujo, E. G. Birgin, and D. P. Ronconi, Technical Report MCDO02022024, 2024], models, constructive heuristics, and benchmark instances for the same problem were introduced. In the present work, we are concerned with the development of effective and efficient methods for its resolution. For this purpose, a local search method and four trajectory metaheuristics are considered. In the local search, we show that the classical strategy of only reallocating operations that are part of the critical path can miss better quality neighbors, as opposed to what happens in the case where there is no learning effect. Consequently, we analyze an alternative type of neighborhood reduction that eliminates only neighbors that are not better than the current solution. In addition, we also suggest a neighborhood cut and experimentally verify that this significantly reduces the neighborhood size, bringing efficiency, with minimal loss in effectiveness. Extensive numerical experiments with the local search and the metaheuristics are carried on. The experiments show that tabu search, built on the reduced neighborhood, when applied to large-sized instances, stands out in relation to other the other three metaheuristics, namely, iterated local search, greedy randomized adaptive search procedure, and simulating annealing. Experiments with classical instances without sequencing flexibility show that the introduced methods also stand out in relation to methods from the literature. All the methods introduced, as well as the instances and solutions found, are freely available. As a whole, we build a test suite that can be used in future work.","sentences":["The flexible job shop scheduling problem with sequencing flexibility and position-based learning effect is considered in the present work.","In [K. A. G. Araujo, E. G. Birgin, and D. P. Ronconi, Technical Report MCDO02022024, 2024], models, constructive heuristics, and benchmark instances for the same problem were introduced.","In the present work, we are concerned with the development of effective and efficient methods for its resolution.","For this purpose, a local search method and four trajectory metaheuristics are considered.","In the local search, we show that the classical strategy of only reallocating operations that are part of the critical path can miss better quality neighbors, as opposed to what happens in the case where there is no learning effect.","Consequently, we analyze an alternative type of neighborhood reduction that eliminates only neighbors that are not better than the current solution.","In addition, we also suggest a neighborhood cut and experimentally verify that this significantly reduces the neighborhood size, bringing efficiency, with minimal loss in effectiveness.","Extensive numerical experiments with the local search and the metaheuristics are carried on.","The experiments show that tabu search, built on the reduced neighborhood, when applied to large-sized instances, stands out in relation to other the other three metaheuristics, namely, iterated local search, greedy randomized adaptive search procedure, and simulating annealing.","Experiments with classical instances without sequencing flexibility show that the introduced methods also stand out in relation to methods from the literature.","All the methods introduced, as well as the instances and solutions found, are freely available.","As a whole, we build a test suite that can be used in future work."],"url":"http://arxiv.org/abs/2403.16787v1","category":"math.OC"}
{"created":"2024-03-25 13:52:36","title":"Stochastic Inertial Dynamics Via Time Scaling and Averaging","abstract":"Our work is part of the close link between continuous-time dissipative dynamical systems and optimization algorithms, and more precisely here, in the stochastic setting. We aim to study stochastic convex minimization problems through the lens of stochastic inertial differential inclusions that are driven by the subgradient of a convex objective function. This will provide a general mathematical framework for analyzing the convergence properties of stochastic second-order inertial continuous-time dynamics involving vanishing viscous damping and measurable stochastic subgradient selections. Our chief goal in this paper is to develop a systematic and unified way that transfers the properties recently studied for first-order stochastic differential equations to second-order ones involving even subgradients in lieu of gradients. This program will rely on two tenets: time scaling and averaging, following an approach recently developed in the literature by one of the co-authors in the deterministic case.   Under a mild integrability assumption involving the diffusion term and the viscous damping, our first main result shows that almost surely, there is weak convergence of the trajectory towards a minimizer of the objective function and fast convergence of the values and gradients. We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex, strongly convex, and (local) Polyak-Lojasiewicz case. Finally, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution.","sentences":["Our work is part of the close link between continuous-time dissipative dynamical systems and optimization algorithms, and more precisely here, in the stochastic setting.","We aim to study stochastic convex minimization problems through the lens of stochastic inertial differential inclusions that are driven by the subgradient of a convex objective function.","This will provide a general mathematical framework for analyzing the convergence properties of stochastic second-order inertial continuous-time dynamics involving vanishing viscous damping and measurable stochastic subgradient selections.","Our chief goal in this paper is to develop a systematic and unified way that transfers the properties recently studied for first-order stochastic differential equations to second-order ones involving even subgradients in lieu of gradients.","This program will rely on two tenets: time scaling and averaging, following an approach recently developed in the literature by one of the co-authors in the deterministic case.   ","Under a mild integrability assumption involving the diffusion term and the viscous damping, our first main result shows that almost surely, there is weak convergence of the trajectory towards a minimizer of the objective function and fast convergence of the values and gradients.","We also provide a comprehensive complexity analysis by establishing several new pointwise and ergodic convergence rates in expectation for the convex, strongly convex, and (local) Polyak-Lojasiewicz case.","Finally, using Tikhonov regularization with a properly tuned vanishing parameter, we can obtain almost sure strong convergence of the trajectory towards the minimum norm solution."],"url":"http://arxiv.org/abs/2403.16775v1","category":"math.OC"}
{"created":"2024-03-25 13:50:11","title":"Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation","abstract":"The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.","sentences":["The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance.","This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise.","A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation.","In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation.","First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs.","Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words.","Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish to English translation.","Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods."],"url":"http://arxiv.org/abs/2403.16771v1","category":"cs.CL"}
{"created":"2024-03-25 13:28:23","title":"Modeling the secular evolution of embedded protoplanetary discs","abstract":"Context: Protoplanetary discs are known to form around nascent stars from their parent molecular cloud as a result of angular momentum conservation. As they progressively evolve and dissipate, they also form planets. While a lot of modeling efforts have been dedicated to their formation, the question of their secular evolution, from the so-called class 0 embedded phase to the class II phase where discs are believed to be isolated, remains poorly understood. Aims: We aim to explore the evolution between the embedded stages and the class II stage. We focus on the magnetic field evolution and the long-term interaction between the disc and the envelope. Methods: We use the GPU-accelerated code \\textsc{Idefix} to perform a 3D, barotropic, non-ideal magnetohydrodynamic (MHD) secular core collapse simulation that covers the system evolution from the collapse of the pre-stellar core until 100 kyr after the first hydrostatic core formation and the disc settling while ensuring sufficient vertical and azimuthal resolutions (down to $10^{-2}$ au) to properly resolve the disc internal dynamics and non-axisymmetric perturbations. Results: The disc evolution leads to a power-law gas surface density in Keplerian rotation that extends up to a few 10 au. The magnetic flux trapped in the disc during the initial collapse decreases from 100 mG at disc formation down to 1 mG by the end of the simulation. After the formation of the first hydrostatic core, the system evolves in three phases. A first phase with a small ($\\sim 10$ au), unstable, strongly accreting ($\\sim10^{-5}$ $\\mathrm{M_\\odot \\, yr^{-1}}$) disc that loses magnetic flux over the first 15 kyr, a second phase where the magnetic flux is advected with a smooth, expanding disc fed by the angular momentum of the infalling material...","sentences":["Context: Protoplanetary discs are known to form around nascent stars from their parent molecular cloud as a result of angular momentum conservation.","As they progressively evolve and dissipate, they also form planets.","While a lot of modeling efforts have been dedicated to their formation, the question of their secular evolution, from the so-called class 0 embedded phase to the class II phase where discs are believed to be isolated, remains poorly understood.","Aims:","We aim to explore the evolution between the embedded stages and the class II stage.","We focus on the magnetic field evolution and the long-term interaction between the disc and the envelope.","Methods: We use the GPU-accelerated code \\textsc{Idefix} to perform a 3D, barotropic, non-ideal magnetohydrodynamic (MHD) secular core collapse simulation that covers the system evolution from the collapse of the pre-stellar core until 100 kyr after the first hydrostatic core formation and the disc settling while ensuring sufficient vertical and azimuthal resolutions (down to $10^{-2}$ au) to properly resolve the disc internal dynamics and non-axisymmetric perturbations.","Results:","The disc evolution leads to a power-law gas surface density in Keplerian rotation that extends up to a few 10 au.","The magnetic flux trapped in the disc during the initial collapse decreases from 100 mG at disc formation down to 1 mG by the end of the simulation.","After the formation of the first hydrostatic core, the system evolves in three phases.","A first phase with a small ($\\sim 10$ au), unstable, strongly accreting ($\\sim10^{-5}$ $\\mathrm{M_\\odot \\, yr^{-1}}$) disc that loses magnetic flux over the first 15 kyr, a second phase where the magnetic flux is advected with a smooth, expanding disc fed by the angular momentum of the infalling material..."],"url":"http://arxiv.org/abs/2403.16753v1","category":"astro-ph.SR"}
{"created":"2024-03-25 12:07:24","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network","abstract":"Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively.","sentences":["Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks.","The existing methods for MAV detection assume that the training set and testing set have the same distribution.","As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy.","In this paper, we study the problem of cross-domain MAV detection.","The contributions of this paper are threefold.","1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images.","Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles.","A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset.","2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure.","To reduce the challenging pseudo-label noises, two novel modules are designed in this network.","The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties.","The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises.","3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones.","In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively."],"url":"http://arxiv.org/abs/2403.16669v1","category":"cs.RO"}
{"created":"2024-03-25 12:00:57","title":"Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $\u03b1$","abstract":"The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields. However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis. In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts. Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy.","sentences":["The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields.","However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis.","In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts.","Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy."],"url":"http://arxiv.org/abs/2403.16665v2","category":"cs.DS"}
{"created":"2024-03-25 11:57:30","title":"Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments","abstract":"This paper focuses on the acquisition of mapless navigation skills within unknown environments. We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism. Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge. Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments. Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models. Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios. Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments.","sentences":["This paper focuses on the acquisition of mapless navigation skills within unknown environments.","We introduce the Skill Q-Network (SQN), a novel reinforcement learning method featuring an adaptive skill ensemble mechanism.","Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge.","Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments.","Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models.","Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios.","Remarkably, our adaptive skill ensemble method enables zero-shot transfer to out-of-distribution domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments."],"url":"http://arxiv.org/abs/2403.16664v1","category":"cs.RO"}
{"created":"2024-03-25 11:40:14","title":"A short proof of the Dvoretzky--Kiefer--Wolfowitz--Massart inequality","abstract":"The Dvoretzky--Kiefer--Wolfowitz--Massart inequality gives a sub-Gaussian tail bound on the supremum norm distance between the empirical distribution function of a random sample and its population counterpart. We provide a short proof of a result that improves the existing bound in two respects. First, our one-sided bound holds without any restrictions on the failure probability, thereby verifying a conjecture of Birnbaum and McCarty (1958). Second, it is local in the sense that it holds uniformly over sub-intervals of the real line with an error rate that adapts to the behaviour of the population distribution function on the interval.","sentences":["The Dvoretzky--Kiefer--Wolfowitz--Massart inequality gives a sub-Gaussian tail bound on the supremum norm distance between the empirical distribution function of a random sample and its population counterpart.","We provide a short proof of a result that improves the existing bound in two respects.","First, our one-sided bound holds without any restrictions on the failure probability, thereby verifying a conjecture of Birnbaum and McCarty (1958).","Second, it is local in the sense that it holds uniformly over sub-intervals of the real line with an error rate that adapts to the behaviour of the population distribution function on the interval."],"url":"http://arxiv.org/abs/2403.16651v1","category":"math.PR"}
{"created":"2024-03-25 11:24:02","title":"V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster","abstract":"The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle's perception capability through message communication among neighboring traffic agents. Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units. However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication. To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information. The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling. Building upon this representation, we propose a novel framework V2X-PC for collaborative perception. This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers. As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object. To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without finetuning. Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps.","sentences":["The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle's perception capability through message communication among neighboring traffic agents.","Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units.","However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication.","To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information.","The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling.","Building upon this representation, we propose a novel framework V2X-PC for collaborative perception.","This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers.","As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object.","To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without finetuning.","Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps."],"url":"http://arxiv.org/abs/2403.16635v1","category":"cs.CV"}
{"created":"2024-03-25 11:20:23","title":"A comparative analysis of embedding models for patent similarity","abstract":"This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task. To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity. Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed.","sentences":["This paper makes two contributions to the field of text-based patent similarity.","First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static word embeddings (such as word2vec and doc2vec models) and contextual word embeddings (such as transformers based models), on the task of patent similarity calculation.","Second, it compares specifically the performance of Sentence Transformers (SBERT) architectures with different training phases on the patent similarity task.","To assess the models' performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners.","Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models.","Our results point out that, first, Patent SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer architecture proposed in this research, outperforms the current state-of-the-art in patent similarity.","Second, they show that, in some cases, large static models performances are still comparable to contextual ones when trained on extensive data; thus, we believe that the superiority in the performance of contextual embeddings may not be related to the actual architecture but rather to the way the training phase is performed."],"url":"http://arxiv.org/abs/2403.16630v1","category":"cs.CL"}
{"created":"2024-03-25 11:09:02","title":"Improving the Optimization in Model Predictive Controllers: Scheduling Large Groups of Electric Vehicles","abstract":"In parking lots with large groups of electric vehicles (EVs), charging has to happen in a coordinated manner, among others, due to the high load per vehicle and the limited capacity of the electricity grid. To achieve such coordination, model predictive control can be applied, thereby repeatedly solving an optimization problem. Due to its repetitive nature and its dependency on the time granularity, optimization has to be (computationally) efficient. The work presented here focuses on that optimization subroutine, its computational efficiency and how to speed up the optimization for large groups of EVs. In particular, we adapt FOCS, an algorithm that can solve the underlying optimization problem, to better suit the repetitive set-up of model predictive control by adding a pre-mature stop feature. Based on real-world data, we empirically show that the added feature speeds up the median computation time for 1-minute granularity by up to 44%. Furthermore, since FOCS is an algorithm that uses maximum flow methods as a subroutine, the impact of choosing various maximum flow methods on the runtime is investigated. Finally, we compare FOCS to a commercially available solver, concluding that FOCS outperforms the state-of-the-art when making a full-day schedule for large groups of EVs.","sentences":["In parking lots with large groups of electric vehicles (EVs), charging has to happen in a coordinated manner, among others, due to the high load per vehicle and the limited capacity of the electricity grid.","To achieve such coordination, model predictive control can be applied, thereby repeatedly solving an optimization problem.","Due to its repetitive nature and its dependency on the time granularity, optimization has to be (computationally) efficient.","The work presented here focuses on that optimization subroutine, its computational efficiency and how to speed up the optimization for large groups of EVs.","In particular, we adapt FOCS, an algorithm that can solve the underlying optimization problem, to better suit the repetitive set-up of model predictive control by adding a pre-mature stop feature.","Based on real-world data, we empirically show that the added feature speeds up the median computation time for 1-minute granularity by up to 44%.","Furthermore, since FOCS is an algorithm that uses maximum flow methods as a subroutine, the impact of choosing various maximum flow methods on the runtime is investigated.","Finally, we compare FOCS to a commercially available solver, concluding that FOCS outperforms the state-of-the-art when making a full-day schedule for large groups of EVs."],"url":"http://arxiv.org/abs/2403.16622v1","category":"math.OC"}
{"created":"2024-03-25 10:38:55","title":"Vector Ising Spin Annealer for Minimizing Ising Hamiltonians","abstract":"We introduce the Vector Ising Spin Annealer (VISA), a framework in gain-based computing that harnesses light-matter interactions to solve complex optimization problems encoded in spin Hamiltonians. Traditional driven-dissipative systems often select excited states due to limitations in spin movement. VISA transcends these constraints by enabling spins to operate in a three-dimensional space, offering a robust solution to minimize Ising Hamiltonians effectively. Our comparative analysis reveals VISA's superior performance over conventional single-dimension spin optimizers, demonstrating its ability to bridge substantial energy barriers in complex landscapes. Through detailed studies on cyclic and random graphs, we show VISA's proficiency in dynamically evolving the energy landscape with time-dependent gain and penalty annealing, illustrating its potential to redefine optimization in physical systems.","sentences":["We introduce the Vector Ising Spin Annealer (VISA), a framework in gain-based computing that harnesses light-matter interactions to solve complex optimization problems encoded in spin Hamiltonians.","Traditional driven-dissipative systems often select excited states due to limitations in spin movement.","VISA transcends these constraints by enabling spins to operate in a three-dimensional space, offering a robust solution to minimize Ising Hamiltonians effectively.","Our comparative analysis reveals VISA's superior performance over conventional single-dimension spin optimizers, demonstrating its ability to bridge substantial energy barriers in complex landscapes.","Through detailed studies on cyclic and random graphs, we show VISA's proficiency in dynamically evolving the energy landscape with time-dependent gain and penalty annealing, illustrating its potential to redefine optimization in physical systems."],"url":"http://arxiv.org/abs/2403.16608v1","category":"quant-ph"}
{"created":"2024-03-25 10:20:50","title":"Research Challenges for Adaptive Architecture: Empowering Occupants of Multi-Occupancy Buildings","abstract":"This positional paper outlines our vision of 'adaptive architecture', which involves the integration of robotic technology to physically change an architectural space in supporting the changing needs of its occupants, in response to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data & Technology\" call on \"How do new technologies enable and empower the inhabitants of multi-occupancy buildings?\". Specifically, while adaptive architecture holds promise for enhancing occupant satisfaction, comfort, and overall health and well-being, there remains a range of research challenges of (1) how it can effectively support individual occupants, while (2) mediating the conflicting needs of collocated others, and (3) integrating meaningfully into the sociocultural characteristics of their building community.","sentences":["This positional paper outlines our vision of 'adaptive architecture', which involves the integration of robotic technology to physically change an architectural space in supporting the changing needs of its occupants, in response to the CHI'24 workshop \"HabiTech - Inhabiting Buildings, Data & Technology\" call on \"How do new technologies enable and empower the inhabitants of multi-occupancy buildings?\".","Specifically, while adaptive architecture holds promise for enhancing occupant satisfaction, comfort, and overall health and well-being, there remains a range of research challenges of (1) how it can effectively support individual occupants, while (2) mediating the conflicting needs of collocated others, and (3) integrating meaningfully into the sociocultural characteristics of their building community."],"url":"http://arxiv.org/abs/2403.16600v1","category":"cs.RO"}
{"created":"2024-03-25 10:16:51","title":"The Adaptive Workplace: Orchestrating Architectural Services around the Wellbeing of Individual Occupants","abstract":"As the academic consortia members of the EU Horizon project SONATA (\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the workshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\" by proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to adapt a workplace to the ever-changing needs of individual occupants, instead of that occupants are expected to adapt to their workplace.","sentences":["As the academic consortia members of the EU Horizon project SONATA (\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the workshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\" by proposing the \"Adaptive Workplace\" concept.","In essence, our vision aims to adapt a workplace to the ever-changing needs of individual occupants, instead of that occupants are expected to adapt to their workplace."],"url":"http://arxiv.org/abs/2403.16595v1","category":"cs.HC"}
{"created":"2024-03-25 07:38:40","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification","abstract":"Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs. This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune. Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods.","sentences":["Following the significant achievements of large language models (LLMs), researchers have employed in-context learning for text classification tasks.","However, these studies focused on monolingual, single-turn classification tasks.","In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in chatbot interactions.","Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts.","LARA tackles these issues by combining a fine-tuned smaller model with a retrieval-augmented mechanism, integrated within the architecture of LLMs.","This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context.","Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of LLMs without extensive retraining and fine-tune.","Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods."],"url":"http://arxiv.org/abs/2403.16504v1","category":"cs.CL"}
{"created":"2024-03-25 07:29:18","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","abstract":"As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality prompt tuning approaches. Significantly, PathoTune facilitates the direct adaptation of natural visual foundation models to pathological tasks, drastically outperforming pathological foundation models with simple linear probing. The code will be available upon acceptance.","sentences":["As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving.","Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored.","For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap.","To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning.","The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features.","Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality prompt tuning approaches.","Significantly, PathoTune facilitates the direct adaptation of natural visual foundation models to pathological tasks, drastically outperforming pathological foundation models with simple linear probing.","The code will be available upon acceptance."],"url":"http://arxiv.org/abs/2403.16497v1","category":"cs.CV"}
{"created":"2024-03-25 07:20:59","title":"Sutured Heegaard Floer and embedded contact homologies are isomorphic","abstract":"We prove the equivalence of the sutured versions of Heegaard Floer homology, monopole Floer homology, and embedded contact homology. As applications we show that the knot versions of Heegaard Floer homology and embedded contact homology are equivalent and that product sutured 3-manifolds are characterized by the fact that they carry an adapted Reeb vector field without periodic orbits.","sentences":["We prove the equivalence of the sutured versions of Heegaard Floer homology, monopole Floer homology, and embedded contact homology.","As applications we show that the knot versions of Heegaard Floer homology and embedded contact homology are equivalent and that product sutured 3-manifolds are characterized by the fact that they carry an adapted Reeb vector field without periodic orbits."],"url":"http://arxiv.org/abs/2403.16492v1","category":"math.SG"}
{"created":"2024-03-25 03:43:36","title":"Implementing and Evaluating E2LSH on Storage","abstract":"Locality sensitive hashing (LSH) is one of the widely-used approaches to approximate nearest neighbor search (ANNS) in high-dimensional spaces. The first work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be solved efficiently at a sublinear query time in the database size with theoretically-guaranteed accuracy, although it required a large hash index size. Since then, several LSH variants having much smaller index sizes have been proposed. Their query time is linear or superlinear, but they have been shown to run effectively faster because they require fewer I/Os when the index is stored on hard disk drives and because they also permit in-memory execution with modern DRAM capacity.   In this paper, we show that E2LSH is regaining the advantage in query speed with the advent of modern flash storage devices such as solid-state drives (SSDs). We evaluate E2LSH on a modern single-node computing environment and analyze its computational cost and I/O cost, from which we derive storage performance requirements for its external memory execution. Our analysis indicates that E2LSH on a single consumer-grade SSD can run faster than the state-of-the-art small-index methods executed in-memory. It also indicates that E2LSH with emerging high-performance storage devices and interfaces can approach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to external memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical large datasets of up to one billion objects using different combinations of modern storage devices and interfaces. We demonstrate that our E2LSHoS implementation runs much faster than small-index methods and can approach in-memory E2LSH speeds, and also that its query time scales sublinearly with the database size beyond the index size limit of in-memory E2LSH.","sentences":["Locality sensitive hashing (LSH) is one of the widely-used approaches to approximate nearest neighbor search (ANNS) in high-dimensional spaces.","The first work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be solved efficiently at a sublinear query time in the database size with theoretically-guaranteed accuracy, although it required a large hash index size.","Since then, several LSH variants having much smaller index sizes have been proposed.","Their query time is linear or superlinear, but they have been shown to run effectively faster because they require fewer I/","Os when the index is stored on hard disk drives and because they also permit in-memory execution with modern DRAM capacity.   ","In this paper, we show that E2LSH is regaining the advantage in query speed with the advent of modern flash storage devices such as solid-state drives (SSDs).","We evaluate E2LSH on a modern single-node computing environment and analyze its computational cost and I/O cost, from which we derive storage performance requirements for its external memory execution.","Our analysis indicates that E2LSH on a single consumer-grade SSD can run faster than the state-of-the-art small-index methods executed in-memory.","It also indicates that E2LSH with emerging high-performance storage devices and interfaces can approach in-memory E2LSH speeds.","We implement a simple adaptation of E2LSH to external memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical large datasets of up to one billion objects using different combinations of modern storage devices and interfaces.","We demonstrate that our E2LSHoS implementation runs much faster than small-index methods and can approach in-memory E2LSH speeds, and also that its query time scales sublinearly with the database size beyond the index size limit of in-memory E2LSH."],"url":"http://arxiv.org/abs/2403.16404v1","category":"cs.PF"}
{"created":"2024-03-25 02:47:29","title":"Real-time Adaptation for Condition Monitoring Signal Prediction using Label-aware Neural Processes","abstract":"Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units. Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings. For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates. However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals. On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task. In this paper, we propose a neural process-based approach that addresses this trade-off. It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for prediction. Once trained, the model can encode an arbitrary number of observations without requiring retraining, enabling on-the-spot real-time predictions along with quantified uncertainty and can be readily updated as more online data is gathered. Furthermore, our model is designed to incorporate qualitative information (i.e., labels) from individual units. This integration not only enhances individualized predictions for each unit but also enables joint inference for both signals and their associated labels. Numerical studies on both synthetic and real-world data in reliability engineering highlight the advantageous features of our model in real-time adaptation, enhanced signal prediction with uncertainty quantification, and joint prediction for labels and signals.","sentences":["Building a predictive model that rapidly adapts to real-time condition monitoring (CM) signals is critical for engineering systems/units.","Unfortunately, many current methods suffer from a trade-off between representation power and agility in online settings.","For instance, parametric methods that assume an underlying functional form for CM signals facilitate efficient online prediction updates.","However, this simplification leads to vulnerability to model specifications and an inability to capture complex signals.","On the other hand, approaches based on over-parameterized or non-parametric models can excel at explaining complex nonlinear signals, but real-time updates for such models pose a challenging task.","In this paper, we propose a neural process-based approach that addresses this trade-off.","It encodes available observations within a CM signal into a representation space and then reconstructs the signal's history and evolution for prediction.","Once trained, the model can encode an arbitrary number of observations without requiring retraining, enabling on-the-spot real-time predictions along with quantified uncertainty and can be readily updated as more online data is gathered.","Furthermore, our model is designed to incorporate qualitative information (i.e., labels) from individual units.","This integration not only enhances individualized predictions for each unit but also enables joint inference for both signals and their associated labels.","Numerical studies on both synthetic and real-world data in reliability engineering highlight the advantageous features of our model in real-time adaptation, enhanced signal prediction with uncertainty quantification, and joint prediction for labels and signals."],"url":"http://arxiv.org/abs/2403.16377v1","category":"cs.LG"}
{"created":"2024-03-25 02:38:34","title":"ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving","abstract":"Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation. In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth. Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component. Encouraging results were obtained in the challenging benchmarks.","sentences":["Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving.","Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation.","However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction.","In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints.","The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation.","In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth.","Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component.","Encouraging results were obtained in the challenging benchmarks."],"url":"http://arxiv.org/abs/2403.16374v1","category":"cs.LG"}
{"created":"2024-03-25 00:41:46","title":"Percentile Optimization in Wireless Networks- Part II: Beamforming for Cell-Edge Throughput Maximization","abstract":"Part I of this two-part paper focused on the formulation of percentile problems, complexity analysis, and development of power control algorithms via the quadratic fractional transform (QFT) and logarithmic fractional transform (LFT) for sum-least-qth-percentile (SLqP) rate maximization problems. In this second part, we first tackle the significantly more challenging problems of optimizing SLqP rate via beamforming in a multiuser, multiple-input multiple-output (MU- MIMO) network to maximize cell-edge throughput. To this end, we first propose an adaptation of the QFT algorithm presented in Part I that enables optimization of the complex-valued multidimensional beamforming weights for the SLqP rate utility function. We also introduce a new class of problems which we term as sum-greatest-qth-percentile weighted mean squared error (SGqP-WMSE) minimization. We show that this class subsumes the well-known sum-weighted mean squared error (WMMSE) minimization and max-WMSE minimization problems. We demonstrate an equivalence between this class of problems and the SLqP rate maximization problems, and show that this correspondence can be exploited to obtain stationary-point solutions for the aforementioned beamforming problem. Next, we develop extensions for the QFT and LFT algorithms from Part I to optimize ergodic long-term average or ergodic SLqP utility. Finally, we also consider related problems which can be solved using the proposed techniques, including hybrid utility functions targeting optimization at specific subsets of users within cellular networks.","sentences":["Part I of this two-part paper focused on the formulation of percentile problems, complexity analysis, and development of power control algorithms via the quadratic fractional transform (QFT) and logarithmic fractional transform (LFT) for sum-least-qth-percentile (SLqP) rate maximization problems.","In this second part, we first tackle the significantly more challenging problems of optimizing SLqP rate via beamforming in a multiuser, multiple-input multiple-output (MU- MIMO) network to maximize cell-edge throughput.","To this end, we first propose an adaptation of the QFT algorithm presented in Part I that enables optimization of the complex-valued multidimensional beamforming weights for the SLqP rate utility function.","We also introduce a new class of problems which we term as sum-greatest-qth-percentile weighted mean squared error (SGqP-WMSE) minimization.","We show that this class subsumes the well-known sum-weighted mean squared error (WMMSE) minimization and max-WMSE minimization problems.","We demonstrate an equivalence between this class of problems and the SLqP rate maximization problems, and show that this correspondence can be exploited to obtain stationary-point solutions for the aforementioned beamforming problem.","Next, we develop extensions for the QFT and LFT algorithms from Part I to optimize ergodic long-term average or ergodic SLqP utility.","Finally, we also consider related problems which can be solved using the proposed techniques, including hybrid utility functions targeting optimization at specific subsets of users within cellular networks."],"url":"http://arxiv.org/abs/2403.16343v1","category":"cs.IT"}
{"created":"2024-03-24 22:12:40","title":"ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in Spent Nuclear Fuel Treatment Process","abstract":"This paper addresses the particularities in optimal control of the uranium extraction-scrubbing operation in the PUREX process. The control problem requires optimally stabilizing the system at a desired solvent saturation level, guaranteeing constraints, disturbance rejection, and adapting to set point variations. A qualified simulator named PAREX was developed by the French Alternative Energies and Atomic Energy Commission (CEA) to simulate liquid-liquid extraction operations in the PUREX process. However, since the mathematical model is complex and is described by a system of nonlinear, stiff, high-dimensional differential-algebraic equations (DAE), applying optimal control methods will lead to a large-scale nonlinear programming problem with a huge computational burden. The solution we propose in this work is to train a neural network to predict the process outputs using the measurement history. This neural network architecture, which employs the long short-term memory (LSTM), linear regression and logistic regression networks, allows reducing the number of state variables, thus reducing the complexity of the optimization problems in the control scheme. Furthermore, nonlinear model predictive control (NMPC) and moving horizon estimation (MHE) problems are developed and solved using the PSO (Particle Swarm Optimization) algorithm. Simulation results show that the proposed adaptive optimal control scheme satisfies the requirements of the control problem and provides promise for experimental testing.","sentences":["This paper addresses the particularities in optimal control of the uranium extraction-scrubbing operation in the PUREX process.","The control problem requires optimally stabilizing the system at a desired solvent saturation level, guaranteeing constraints, disturbance rejection, and adapting to set point variations.","A qualified simulator named PAREX was developed by the French Alternative Energies and Atomic Energy Commission (CEA) to simulate liquid-liquid extraction operations in the PUREX process.","However, since the mathematical model is complex and is described by a system of nonlinear, stiff, high-dimensional differential-algebraic equations (DAE), applying optimal control methods will lead to a large-scale nonlinear programming problem with a huge computational burden.","The solution we propose in this work is to train a neural network to predict the process outputs using the measurement history.","This neural network architecture, which employs the long short-term memory (LSTM), linear regression and logistic regression networks, allows reducing the number of state variables, thus reducing the complexity of the optimization problems in the control scheme.","Furthermore, nonlinear model predictive control (NMPC) and moving horizon estimation (MHE) problems are developed and solved using the PSO (Particle Swarm Optimization) algorithm.","Simulation results show that the proposed adaptive optimal control scheme satisfies the requirements of the control problem and provides promise for experimental testing."],"url":"http://arxiv.org/abs/2403.16307v1","category":"eess.SY"}
{"created":"2024-03-24 21:22:52","title":"Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on Dragonfly Network","abstract":"on adaptive routing to balance network traffic for optimum performance. Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion. In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy. Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away. This inaccuracy could lead to interconnect congestion. In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology. The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers. Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing. We implement the proposed Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms. Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction.","sentences":["on adaptive routing to balance network traffic for optimum performance.","Ideally, adaptive routing attempts to forward packets between minimal and non-minimal paths with the least congestion.","In practice, current adaptive routing algorithms estimate routing path congestion based on local information such as output queue occupancy.","Using local information to estimate global path congestion is inevitably inaccurate because a router has no precise knowledge of link states a few hops away.","This inaccuracy could lead to interconnect congestion.","In this study, we present Q-adaptive routing, a multi-agent reinforcement learning routing scheme for Dragonfly systems.","Q-adaptive routing enables routers to learn to route autonomously by leveraging advanced reinforcement learning technology.","The proposed Q-adaptive routing is highly scalable thanks to its fully distributed nature without using any shared information between routers.","Furthermore, a new two-level Q-table is designed for Q-adaptive to make it computational lightly and saves 50% of router memory usage compared with the previous Q-routing.","We implement the proposed Q-adaptive routing in SST/Merlin simulator.","Our evaluation results show that Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x average packet latency reduction compared with adaptive routing algorithms.","Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing under the ADV+1 adversarial traffic pattern with up to 3% system throughput improvement and 75% average packet latency reduction."],"url":"http://arxiv.org/abs/2403.16301v1","category":"cs.NI"}
{"created":"2024-03-24 20:31:42","title":"HemoSet: The First Blood Segmentation Dataset for Automation of Hemostasis Management","abstract":"Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly adapt to the visual interference that results from blood rapidly filling the surgical field. Introducing automation into the crucial surgical task of hemostasis management would offload mental and physical tasks from the surgeon and surgical assistants while simultaneously increasing the efficiency and safety of the operation. The first step in automation of hemostasis management is detection of blood in the surgical field. To propel the development of blood detection algorithms in surgeries, we present HemoSet, the first blood segmentation dataset based on bleeding during a live animal robotic surgery. Our dataset features vessel hemorrhage scenarios where turbulent flow leads to abnormal pooling geometries in surgical fields. These pools are formed in conditions endemic to surgical procedures -- uneven heterogeneous tissue, under glossy lighting conditions and rapid tool movement. We benchmark several state-of-the-art segmentation models and provide insight into the difficulties specific to blood detection. We intend for HemoSet to spur development of autonomous blood suction tools by providing a platform for training and refining blood segmentation models, addressing the precision needed for such robotics.","sentences":["Hemorrhaging occurs in surgeries of all types, forcing surgeons to quickly adapt to the visual interference that results from blood rapidly filling the surgical field.","Introducing automation into the crucial surgical task of hemostasis management would offload mental and physical tasks from the surgeon and surgical assistants while simultaneously increasing the efficiency and safety of the operation.","The first step in automation of hemostasis management is detection of blood in the surgical field.","To propel the development of blood detection algorithms in surgeries, we present HemoSet, the first blood segmentation dataset based on bleeding during a live animal robotic surgery.","Our dataset features vessel hemorrhage scenarios where turbulent flow leads to abnormal pooling geometries in surgical fields.","These pools are formed in conditions endemic to surgical procedures -- uneven heterogeneous tissue, under glossy lighting conditions and rapid tool movement.","We benchmark several state-of-the-art segmentation models and provide insight into the difficulties specific to blood detection.","We intend for HemoSet to spur development of autonomous blood suction tools by providing a platform for training and refining blood segmentation models, addressing the precision needed for such robotics."],"url":"http://arxiv.org/abs/2403.16286v1","category":"eess.IV"}
{"created":"2024-03-24 17:13:46","title":"Adversarially Masked Video Consistency for Unsupervised Domain Adaptation","abstract":"We study the problem of unsupervised domain adaptation for egocentric videos. We propose a transformer-based model to learn class-discriminative and domain-invariant feature representations. It consists of two novel designs. The first module is called Generative Adversarial Domain Alignment Network with the aim of learning domain-invariant representations. It simultaneously learns a mask generator and a domain-invariant encoder in an adversarial way. The domain-invariant encoder is trained to minimize the distance between the source and target domain. The masking generator, conversely, aims at producing challenging masks by maximizing the domain distance. The second is a Masked Consistency Learning module to learn class-discriminative representations. It enforces the prediction consistency between the masked target videos and their full forms. To better evaluate the effectiveness of domain adaptation methods, we construct a more challenging benchmark for egocentric videos, U-Ego4D. Our method achieves state-of-the-art performance on the Epic-Kitchen and the proposed U-Ego4D benchmark.","sentences":["We study the problem of unsupervised domain adaptation for egocentric videos.","We propose a transformer-based model to learn class-discriminative and domain-invariant feature representations.","It consists of two novel designs.","The first module is called Generative Adversarial Domain Alignment Network with the aim of learning domain-invariant representations.","It simultaneously learns a mask generator and a domain-invariant encoder in an adversarial way.","The domain-invariant encoder is trained to minimize the distance between the source and target domain.","The masking generator, conversely, aims at producing challenging masks by maximizing the domain distance.","The second is a Masked Consistency Learning module to learn class-discriminative representations.","It enforces the prediction consistency between the masked target videos and their full forms.","To better evaluate the effectiveness of domain adaptation methods, we construct a more challenging benchmark for egocentric videos, U-Ego4D.","Our method achieves state-of-the-art performance on the Epic-Kitchen and the proposed U-Ego4D benchmark."],"url":"http://arxiv.org/abs/2403.16242v1","category":"cs.CV"}
{"created":"2024-03-24 17:06:45","title":"Thermal Analysis for NVIDIA GTX480 Fermi GPU Architecture","abstract":"In this project, we design a four-layer (Silicon|TIM|Silicon|TIM), 3D floor plan for NVIDIA GTX480 Fermi GPU architecture and compare heat dissipation and power trends for matrix multiplication and Needleman-Wunsch kernels. First, cuda kernels for the two algorithms are written. These kernels are compiled and executed with the GPGPU Simulator to extract power logs for varying tensor sizes. These power logs are converted to ptrace files with an automation script written in Python. The 3D floor plan, along with the generated ptrace files are given to HotSpot, which generates thermal heat maps to show heat dissipation for various components of the Fermi architecture. These heat dissipation trends for both the kernels are observed for multiple tensor sizes to draw qualitative conclusions. The behavioral and execution patterns of both kernels are also observed with these varying heat dissipation trends. With this project, we observe that an increase in tensor size results in an increase of heat dissipation in components of the Fermi Architecture. However, the temperature of the chip remains saturated after a particular tensor size and remains constant thereafter. Heat dissipation is non-uniform with smaller tensor sizes, and becomes more uniform after a certain tensor size. This means, that after a particular tensor size, more cores of the architecture get activated in the computations, thereby resulting in an almost constant temperature. We also observe that Needleman Wunsch uses more data movement between DRAM and caches, thereby showing higher heat dissipation patterns in DRAMs when compared to Matrix multiplication for the same tensor size. Our observations are in accordance with the theoretical concepts behind the working of the two algorithms, thereby making our results consistent.","sentences":["In this project, we design a four-layer (Silicon|TIM|Silicon|TIM), 3D floor plan for NVIDIA GTX480 Fermi GPU architecture and compare heat dissipation and power trends for matrix multiplication and Needleman-Wunsch kernels.","First, cuda kernels for the two algorithms are written.","These kernels are compiled and executed with the GPGPU Simulator to extract power logs for varying tensor sizes.","These power logs are converted to ptrace files with an automation script written in Python.","The 3D floor plan, along with the generated ptrace files are given to HotSpot, which generates thermal heat maps to show heat dissipation for various components of the Fermi architecture.","These heat dissipation trends for both the kernels are observed for multiple tensor sizes to draw qualitative conclusions.","The behavioral and execution patterns of both kernels are also observed with these varying heat dissipation trends.","With this project, we observe that an increase in tensor size results in an increase of heat dissipation in components of the Fermi Architecture.","However, the temperature of the chip remains saturated after a particular tensor size and remains constant thereafter.","Heat dissipation is non-uniform with smaller tensor sizes, and becomes more uniform after a certain tensor size.","This means, that after a particular tensor size, more cores of the architecture get activated in the computations, thereby resulting in an almost constant temperature.","We also observe that Needleman Wunsch uses more data movement between DRAM and caches, thereby showing higher heat dissipation patterns in DRAMs when compared to Matrix multiplication for the same tensor size.","Our observations are in accordance with the theoretical concepts behind the working of the two algorithms, thereby making our results consistent."],"url":"http://arxiv.org/abs/2403.16239v1","category":"cs.AR"}
{"created":"2024-03-24 16:03:27","title":"Skull-to-Face: Anatomy-Guided 3D Facial Reconstruction and Editing","abstract":"Deducing the 3D face from a skull is an essential but challenging task in forensic science and archaeology. Existing methods for automated facial reconstruction yield inaccurate results, suffering from the non-determinative nature of the problem that a skull with a sparse set of tissue depth cannot fully determine the skinned face. Additionally, their texture-less results require further post-processing stages to achieve a photo-realistic appearance. This paper proposes an end-to-end 3D face reconstruction and exploration tool, providing textured 3D faces for reference. With the help of state-of-the-art text-to-image diffusion models and image-based facial reconstruction techniques, we generate an initial reference 3D face, whose biological profile aligns with the given skull. We then adapt these initial faces to meet the statistical expectations of extruded anatomical landmarks on the skull through an optimization process. The joint statistical distribution of tissue depths is learned on a small set of anatomical landmarks on the skull. To support further adjustment, we propose an efficient face adaptation tool to assist users in tuning tissue depths, either globally or at local regions, while observing plausible visual feedback. Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability.","sentences":["Deducing the 3D face from a skull is an essential but challenging task in forensic science and archaeology.","Existing methods for automated facial reconstruction yield inaccurate results, suffering from the non-determinative nature of the problem that a skull with a sparse set of tissue depth cannot fully determine the skinned face.","Additionally, their texture-less results require further post-processing stages to achieve a photo-realistic appearance.","This paper proposes an end-to-end 3D face reconstruction and exploration tool, providing textured 3D faces for reference.","With the help of state-of-the-art text-to-image diffusion models and image-based facial reconstruction techniques, we generate an initial reference 3D face, whose biological profile aligns with the given skull.","We then adapt these initial faces to meet the statistical expectations of extruded anatomical landmarks on the skull through an optimization process.","The joint statistical distribution of tissue depths is learned on a small set of anatomical landmarks on the skull.","To support further adjustment, we propose an efficient face adaptation tool to assist users in tuning tissue depths, either globally or at local regions, while observing plausible visual feedback.","Experiments conducted on a real skull-face dataset demonstrated the effectiveness of our proposed pipeline in terms of reconstruction accuracy, diversity, and stability."],"url":"http://arxiv.org/abs/2403.16207v1","category":"cs.CV"}
{"created":"2024-03-24 15:10:22","title":"Cross-domain Multi-modal Few-shot Object Detection via Rich Text","abstract":"Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features. However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient. We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift. Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD. Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model's language understanding capability. We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods.","sentences":["Cross-modal feature extraction and integration have led to steady performance improvements in few-shot learning tasks due to generating richer features.","However, existing multi-modal object detection (MM-OD) methods degrade when facing significant domain-shift and are sample insufficient.","We hypothesize that rich text information could more effectively help the model to build a knowledge relationship between the vision instance and its language description and can help mitigate domain shift.","Specifically, we study the Cross-Domain few-shot generalization of MM-OD (CDMM-FSOD) and propose a meta-learning based multi-modal few-shot object detection method that utilizes rich text semantic information as an auxiliary modality to achieve domain adaptation in the context of FSOD.","Our proposed network contains (i) a multi-modal feature aggregation module that aligns the vision and language support feature embeddings and (ii) a rich text semantic rectify module that utilizes bidirectional text feature generation to reinforce multi-modal feature alignment and thus to enhance the model's language understanding capability.","We evaluate our model on common standard cross-domain object detection datasets and demonstrate that our approach considerably outperforms existing FSOD methods."],"url":"http://arxiv.org/abs/2403.16188v1","category":"cs.CV"}
{"created":"2024-03-24 15:09:55","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models","abstract":"Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.","sentences":["Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models.","Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method.","However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks.","Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process.","First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank.","Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks.","We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters."],"url":"http://arxiv.org/abs/2403.16187v1","category":"cs.CL"}
{"created":"2024-03-24 14:02:25","title":"Towards Online Real-Time Memory-based Video Inpainting Transformers","abstract":"Video inpainting tasks have seen significant improvements in recent years with the rise of deep neural networks and, in particular, vision transformers. Although these models show promising reconstruction quality and temporal consistency, they are still unsuitable for live videos, one of the last steps to make them completely convincing and usable. The main limitations are that these state-of-the-art models inpaint using the whole video (offline processing) and show an insufficient frame rate. In our approach, we propose a framework to adapt existing inpainting transformers to these constraints by memorizing and refining redundant computations while maintaining a decent inpainting quality. Using this framework with some of the most recent inpainting models, we show great online results with a consistent throughput above 20 frames per second. The code and pretrained models will be made available upon acceptance.","sentences":["Video inpainting tasks have seen significant improvements in recent years with the rise of deep neural networks and, in particular, vision transformers.","Although these models show promising reconstruction quality and temporal consistency, they are still unsuitable for live videos, one of the last steps to make them completely convincing and usable.","The main limitations are that these state-of-the-art models inpaint using the whole video (offline processing) and show an insufficient frame rate.","In our approach, we propose a framework to adapt existing inpainting transformers to these constraints by memorizing and refining redundant computations while maintaining a decent inpainting quality.","Using this framework with some of the most recent inpainting models, we show great online results with a consistent throughput above 20 frames per second.","The code and pretrained models will be made available upon acceptance."],"url":"http://arxiv.org/abs/2403.16161v1","category":"cs.CV"}
{"created":"2024-03-24 13:44:32","title":"Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior Detection","abstract":"The online community has increasingly been inundated by a toxic wave of harmful comments. In response to this growing challenge, we introduce a two-stage ultra-low-cost multimodal harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates. We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images. Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or logistic regression, enabling the system to be trained rapidly and to perform inference at an ultra-low cost. By converting tweets into rich multimodal embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual information with near-perfect performance, achieving precision and recall rates above 99\\% but also demonstrates the ability to zero-shot harmful images without additional training, thanks to its multimodal embedding input. This capability empowers our system to identify unseen harmful images without requiring extensive and costly image datasets. Additionally, our system quickly adapts to new harmful content; if a new harmful content pattern is identified, we can fine-tune the classifier with the corresponding tweets' embeddings to promptly update the system. This makes it well suited to addressing the ever-evolving nature of online harmfulness, providing online communities with a robust, generalizable, and cost-effective tool to safeguard their communities.","sentences":["The online community has increasingly been inundated by a toxic wave of harmful comments.","In response to this growing challenge, we introduce a two-stage ultra-low-cost multimodal harmful behavior detection method designed to identify harmful comments and images with high precision and recall rates.","We first utilize the CLIP-ViT model to transform tweets and images into embeddings, effectively capturing the intricate interplay of semantic meaning and subtle contextual clues within texts and images.","Then in the second stage, the system feeds these embeddings into a conventional machine learning classifier like SVM or logistic regression, enabling the system to be trained rapidly and to perform inference at an ultra-low cost.","By converting tweets into rich multimodal embeddings through the CLIP-ViT model and utilizing them to train conventional machine learning classifiers, our system is not only capable of detecting harmful textual information with near-perfect performance, achieving precision and recall rates above 99\\% but also demonstrates the ability to zero-shot harmful images without additional training, thanks to its multimodal embedding input.","This capability empowers our system to identify unseen harmful images without requiring extensive and costly image datasets.","Additionally, our system quickly adapts to new harmful content; if a new harmful content pattern is identified, we can fine-tune the classifier with the corresponding tweets' embeddings to promptly update the system.","This makes it well suited to addressing the ever-evolving nature of online harmfulness, providing online communities with a robust, generalizable, and cost-effective tool to safeguard their communities."],"url":"http://arxiv.org/abs/2403.16151v1","category":"cs.MA"}
{"created":"2024-03-24 13:32:42","title":"Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach","abstract":"Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a recurrent neural network (RNN) architecture fed with time series data derived from geometrical parameters, as for example droplet diameter variation, our study shows the accuracy of our approach in predicting energy budgets, as for instance the kinetic, dissipation, and surface energy trends, across a range of Reynolds and Weber numbers in droplet dynamic problems. Finally, a two-phase sequential neural network using only geometric data, which is readily available in experimental settings, is employed to predict the energies and then use them to estimate static parameters, such as the Reynolds and Weber numbers. While our methodology has been primarily validated with simulation data, its adaptability to experimental datasets is a promising avenue for future exploration. We hope that our strategy can be useful for diverse applications, spanning from inkjet printing to combustion engines, where the prediction of energy budgets or dissipation energies is crucial.","sentences":["Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows.","The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs.","This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects.","Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision.","Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget.","The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics.","Using a recurrent neural network (RNN) architecture fed with time series data derived from geometrical parameters, as for example droplet diameter variation, our study shows the accuracy of our approach in predicting energy budgets, as for instance the kinetic, dissipation, and surface energy trends, across a range of Reynolds and Weber numbers in droplet dynamic problems.","Finally, a two-phase sequential neural network using only geometric data, which is readily available in experimental settings, is employed to predict the energies and then use them to estimate static parameters, such as the Reynolds and Weber numbers.","While our methodology has been primarily validated with simulation data, its adaptability to experimental datasets is a promising avenue for future exploration.","We hope that our strategy can be useful for diverse applications, spanning from inkjet printing to combustion engines, where the prediction of energy budgets or dissipation energies is crucial."],"url":"http://arxiv.org/abs/2403.16144v1","category":"physics.flu-dyn"}
{"created":"2024-03-24 13:10:09","title":"A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective","abstract":"Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/Pretext.","sentences":["Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models.","There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge.","In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc).","It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies.","Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/Pretext."],"url":"http://arxiv.org/abs/2403.16137v1","category":"cs.LG"}
{"created":"2024-03-24 13:09:46","title":"Data-Driven Sliding Mode Control for Partially Unknown Nonlinear Systems","abstract":"This paper introduces a new design method for data-driven control of nonlinear systems with partially unknown dynamics and unknown bounded disturbance. Since it is not possible to achieve exact nonlinearity cancellation in the presence of unknown disturbance, this paper adapts the idea of sliding mode control (SMC) to ensure system stability and robustness without assuming that the nonlinearity goes to zero faster than the state as in the existing methods. The SMC consists of a data-dependent robust controller ensuring the system state trajectory reach and remain on the sliding surface and a nominal controller solved from a data-dependent semidefinite program (SDP) ensuring robust stability of the state trajectory on the sliding surface. Numerical simulation results demonstrate effectiveness of the proposed data-driven SMC and its superior in terms of robust stability over the existing data-driven control that also uses approximate nonlinearity cancellation.","sentences":["This paper introduces a new design method for data-driven control of nonlinear systems with partially unknown dynamics and unknown bounded disturbance.","Since it is not possible to achieve exact nonlinearity cancellation in the presence of unknown disturbance, this paper adapts the idea of sliding mode control (SMC) to ensure system stability and robustness without assuming that the nonlinearity goes to zero faster than the state as in the existing methods.","The SMC consists of a data-dependent robust controller ensuring the system state trajectory reach and remain on the sliding surface and a nominal controller solved from a data-dependent semidefinite program (SDP) ensuring robust stability of the state trajectory on the sliding surface.","Numerical simulation results demonstrate effectiveness of the proposed data-driven SMC and its superior in terms of robust stability over the existing data-driven control that also uses approximate nonlinearity cancellation."],"url":"http://arxiv.org/abs/2403.16136v1","category":"eess.SY"}
{"created":"2024-03-24 13:03:27","title":"Runtime Monitoring and Fault Detection for Neural Network-Controlled Systems","abstract":"There is an emerging trend in applying deep learning methods to control complex nonlinear systems. This paper considers enhancing the runtime safety of nonlinear systems controlled by neural networks in the presence of disturbance and measurement noise. A robustly stable interval observer is designed to generate sound and precise lower and upper bounds for the neural network, nonlinear function, and system state. The obtained interval is utilised to monitor the real-time system safety and detect faults in the system outputs or actuators. An adaptive cruise control vehicular system is simulated to demonstrate effectiveness of the proposed design.","sentences":["There is an emerging trend in applying deep learning methods to control complex nonlinear systems.","This paper considers enhancing the runtime safety of nonlinear systems controlled by neural networks in the presence of disturbance and measurement noise.","A robustly stable interval observer is designed to generate sound and precise lower and upper bounds for the neural network, nonlinear function, and system state.","The obtained interval is utilised to monitor the real-time system safety and detect faults in the system outputs or actuators.","An adaptive cruise control vehicular system is simulated to demonstrate effectiveness of the proposed design."],"url":"http://arxiv.org/abs/2403.16132v1","category":"eess.SY"}
{"created":"2024-03-24 12:43:04","title":"A Codesign of Scheduling and Parallelization for Large Model Training in Heterogeneous Clusters","abstract":"Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters. However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space. The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism). The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling. This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster. Crius proposes a novel scheduling granularity called Cell. It represents a job with deterministic resources and pipeline stages. The exploration space of Cell is shrunk to the product of only data and tensor parallelism, thus exposing the potential for accurate and low-overhead performance estimation. Crius then accurately estimates Cells and efficiently schedules training jobs. When a Cell is selected as a scheduling choice, its represented job runs with the optimal parallelism plan explored. Experimental results show that Crius reduces job completion time by up to 48.9% and schedules large models with up to 1.49x cluster throughput improvement.","sentences":["Joint consideration of scheduling and adaptive parallelism offers great opportunities for improving the training efficiency of large models on heterogeneous GPU clusters.","However, integrating adaptive parallelism into a cluster scheduler expands the cluster scheduling space.","The new space is the product of the original scheduling space and the parallelism exploration space of adaptive parallelism (also a product of pipeline, data, and tensor parallelism).","The exponentially enlarged scheduling space and ever-changing optimal parallelism plan from adaptive parallelism together result in the contradiction between low-overhead and accurate performance data acquisition for efficient cluster scheduling.","This paper presents Crius, a training system for efficiently scheduling multiple large models with adaptive parallelism in a heterogeneous cluster.","Crius proposes a novel scheduling granularity called Cell.","It represents a job with deterministic resources and pipeline stages.","The exploration space of Cell is shrunk to the product of only data and tensor parallelism, thus exposing the potential for accurate and low-overhead performance estimation.","Crius then accurately estimates Cells and efficiently schedules training jobs.","When a Cell is selected as a scheduling choice, its represented job runs with the optimal parallelism plan explored.","Experimental results show that Crius reduces job completion time by up to 48.9% and schedules large models with up to 1.49x cluster throughput improvement."],"url":"http://arxiv.org/abs/2403.16125v1","category":"cs.DC"}
{"created":"2024-03-24 12:01:27","title":"ByteCard: Enhancing ByteDance's Data Warehouse with Learned Cardinality Estimation","abstract":"Cardinality estimation is a critical component and a longstanding challenge in modern data warehouses. ByteHouse, ByteDance's cloud-native engine for big data analysis in exabyte-scale environments, serves numerous internal decision-making business scenarios. With the increasing demand of ByteHouse, cardinality estimation becomes the bottleneck for efficiently processing queries. Specifically, the existing query optimizer of ByteHouse uses the traditional Selinger-like cardinality estimator, which can produce huge estimation errors, resulting in sub-optimal query plans. To improve cardinality estimation accuracy while maintaining a practical inference overhead, we develop ByteCard framework that enables efficient training/updating and integration of cardinality estimators. Furthermore, ByteCard adapts recent advances in cardinality estimation to build models that can balance accuracy and practicality (e.g., inference latency, model size, training/updating overhead). We observe significant query processing speed-up in ByteHouse after replacing the system's existing cardinality estimation with ByteCard's estimations for several optimization strategies. Evaluations on real-world datasets show the integration of ByteCard leads to an improvement of up to 30% in the 99th quantile of latency. At last, we share our valuable experience in engineering advanced cardinality estimators. We believe this experience can help other data warehouses integrate more accurate and sophisticated solutions on the critical path of query execution.","sentences":["Cardinality estimation is a critical component and a longstanding challenge in modern data warehouses.","ByteHouse, ByteDance's cloud-native engine for big data analysis in exabyte-scale environments, serves numerous internal decision-making business scenarios.","With the increasing demand of ByteHouse, cardinality estimation becomes the bottleneck for efficiently processing queries.","Specifically, the existing query optimizer of ByteHouse uses the traditional Selinger-like cardinality estimator, which can produce huge estimation errors, resulting in sub-optimal query plans.","To improve cardinality estimation accuracy while maintaining a practical inference overhead, we develop ByteCard framework that enables efficient training/updating and integration of cardinality estimators.","Furthermore, ByteCard adapts recent advances in cardinality estimation to build models that can balance accuracy and practicality (e.g., inference latency, model size, training/updating overhead).","We observe significant query processing speed-up in ByteHouse after replacing the system's existing cardinality estimation with ByteCard's estimations for several optimization strategies.","Evaluations on real-world datasets show the integration of ByteCard leads to an improvement of up to 30% in the 99th quantile of latency.","At last, we share our valuable experience in engineering advanced cardinality estimators.","We believe this experience can help other data warehouses integrate more accurate and sophisticated solutions on the critical path of query execution."],"url":"http://arxiv.org/abs/2403.16110v2","category":"cs.DB"}
{"created":"2024-03-24 08:26:54","title":"Markovian dynamics for a quantum/classical system and quantum trajectories","abstract":"Quantum trajectory techniques have been used in the theory of open systems as a starting point for numerical computations and to describe the monitoring of a quantum system in continuous time. Here we extend this technique and use it to develop a general approach to the dynamics of quantum/classical hybrid systems. By using two coupled stochastic differential equations, we can describe a classical component and a quantum one which have their own intrinsic dynamics and which interact with each other. A mathematically rigorous construction is given, under the restriction of having a Markovian joint dynamics and of involving only bounded operators on the Hilbert space of the quantum component. An important feature is that, if the interaction allows for a flow of information from the quantum component to the classical one, necessarily the dynamics is dissipative. We show also how this theory is connected to a suitable hybrid dynamical semigroup, which reduces to a quantum dynamical semigroup in the purely quantum case and includes Liouville and Kolmogorov-Fokker-Plank equations in the purely classical case. Moreover, this semigroup allows to compare the proposed stochastic dynamics with various other proposals based on hybrid master equations. Some simple example are constructed in order to show the variety of physical behaviours which can be described; in particular, a model presenting hidden entanglement is introduced.","sentences":["Quantum trajectory techniques have been used in the theory of open systems as a starting point for numerical computations and to describe the monitoring of a quantum system in continuous time.","Here we extend this technique and use it to develop a general approach to the dynamics of quantum/classical hybrid systems.","By using two coupled stochastic differential equations, we can describe a classical component and a quantum one which have their own intrinsic dynamics and which interact with each other.","A mathematically rigorous construction is given, under the restriction of having a Markovian joint dynamics and of involving only bounded operators on the Hilbert space of the quantum component.","An important feature is that, if the interaction allows for a flow of information from the quantum component to the classical one, necessarily the dynamics is dissipative.","We show also how this theory is connected to a suitable hybrid dynamical semigroup, which reduces to a quantum dynamical semigroup in the purely quantum case and includes Liouville and Kolmogorov-Fokker-Plank equations in the purely classical case.","Moreover, this semigroup allows to compare the proposed stochastic dynamics with various other proposals based on hybrid master equations.","Some simple example are constructed in order to show the variety of physical behaviours which can be described; in particular, a model presenting hidden entanglement is introduced."],"url":"http://arxiv.org/abs/2403.16065v1","category":"quant-ph"}
{"created":"2024-03-26 17:58:07","title":"Towards Explaining Hypercomplex Neural Networks","abstract":"Hypercomplex neural networks are gaining increasing interest in the deep learning community. The attention directed towards hypercomplex models originates from several aspects, spanning from purely theoretical and mathematical characteristics to the practical advantage of lightweight models over conventional networks, and their unique properties to capture both global and local relations. In particular, a branch of these architectures, parameterized hypercomplex neural networks (PHNNs), has also gained popularity due to their versatility across a multitude of application domains. Nonetheless, only few attempts have been made to explain or interpret their intricacies. In this paper, we propose inherently interpretable PHNNs and quaternion-like networks, thus without the need for any post-hoc method. To achieve this, we define a type of cosine-similarity transform within the parameterized hypercomplex domain. This PHB-cos transform induces weight alignment with relevant input features and allows to reduce the model into a single linear transform, rendering it directly interpretable. In this work, we start to draw insights into how this unique branch of neural models operates. We observe that hypercomplex networks exhibit a tendency to concentrate on the shape around the main object of interest, in addition to the shape of the object itself. We provide a thorough analysis, studying single neurons of different layers and comparing them against how real-valued networks learn. The code of the paper is available at https://github.com/ispamm/HxAI.","sentences":["Hypercomplex neural networks are gaining increasing interest in the deep learning community.","The attention directed towards hypercomplex models originates from several aspects, spanning from purely theoretical and mathematical characteristics to the practical advantage of lightweight models over conventional networks, and their unique properties to capture both global and local relations.","In particular, a branch of these architectures, parameterized hypercomplex neural networks (PHNNs), has also gained popularity due to their versatility across a multitude of application domains.","Nonetheless, only few attempts have been made to explain or interpret their intricacies.","In this paper, we propose inherently interpretable PHNNs and quaternion-like networks, thus without the need for any post-hoc method.","To achieve this, we define a type of cosine-similarity transform within the parameterized hypercomplex domain.","This PHB-cos transform induces weight alignment with relevant input features and allows to reduce the model into a single linear transform, rendering it directly interpretable.","In this work, we start to draw insights into how this unique branch of neural models operates.","We observe that hypercomplex networks exhibit a tendency to concentrate on the shape around the main object of interest, in addition to the shape of the object itself.","We provide a thorough analysis, studying single neurons of different layers and comparing them against how real-valued networks learn.","The code of the paper is available at https://github.com/ispamm/HxAI."],"url":"http://arxiv.org/abs/2403.17929v1","category":"cs.CV"}
{"created":"2024-03-26 17:45:06","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","abstract":"We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)\" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation, considering radial k-space sampling acquisition sequences. Our preliminary results suggest that R2D2 achieves: (i) suboptimal performance compared to its unrolled incarnation R2D2-Net, which is however non-scalable due to the necessary embedding of NUFFT-based data-consistency layers; (ii) superior reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based approximation for data consistency; (iii) superior reconstruction quality to PnP, while only requiring few iterations.","sentences":["We propose a new approach for non-Cartesian magnetic resonance image reconstruction.","While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale.","Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability.","To address this scalability challenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)\" approach recently introduced in astronomical imaging.","R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs.","The method can be interpreted as a learned version of the Matching Pursuit algorithm.","We demonstrate R2D2 in simulation, considering radial k-space sampling acquisition sequences.","Our preliminary results suggest that R2D2 achieves: (i) suboptimal performance compared to its unrolled incarnation R2D2-Net, which is however non-scalable due to the necessary embedding of NUFFT-based data-consistency layers; (ii) superior reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based approximation for data consistency; (iii) superior reconstruction quality to PnP, while only requiring few iterations."],"url":"http://arxiv.org/abs/2403.17905v1","category":"eess.IV"}
{"created":"2024-03-26 17:43:19","title":"Quasar Island - Three new $z\\sim6$ quasars, including a lensed candidate, identified with contrastive learning","abstract":"Of the hundreds of $z\\gtrsim6$ quasars discovered to date, only one is known to be gravitationally lensed, despite the high lensing optical depth expected at $z\\gtrsim6$. High-redshift quasars are typically identified in large-scale surveys by applying strict photometric selection criteria, in particular by imposing non-detections in bands blueward of the Lyman-$\\alpha$ line. Such procedures by design prohibit the discovery of lensed quasars, as the lensing foreground galaxy would contaminate the photometry of the quasar. We present a novel quasar selection methodology, applying contrastive learning (an unsupervised machine learning technique) to Dark Energy Survey imaging data. We describe the use of this technique to train a neural network which isolates an 'island' of 11 sources, of which 7 are known $z\\sim6$ quasars. Of the remaining four, three are newly discovered quasars (J0109-5424, $z=6.07$; J0122-4609, $z=5.99$; J0603-3923, $z=5.94$), as confirmed by follow-up Gemini-South/GMOS and archival NTT/EFOSC2 spectroscopy, implying a 91 per cent efficiency for our novel selection method; the final object on the island is a brown dwarf. In one case (J0109-5424), emission below the Lyman limit unambiguously indicates the presence of a foreground source, though high-resolution optical/near-infrared imaging is still needed to confirm the quasar's lensed (multiply-imaged) nature. Detection in the g band has led this quasar to escape selection by traditional colour cuts. Our findings demonstrate that machine learning techniques can thus play a key role in unveiling populations of quasars missed by traditional methods.","sentences":["Of the hundreds of $z\\gtrsim6$ quasars discovered to date, only one is known to be gravitationally lensed, despite the high lensing optical depth expected at $z\\gtrsim6$. High-redshift quasars are typically identified in large-scale surveys by applying strict photometric selection criteria, in particular by imposing non-detections in bands blueward of the Lyman-$\\alpha$ line.","Such procedures by design prohibit the discovery of lensed quasars, as the lensing foreground galaxy would contaminate the photometry of the quasar.","We present a novel quasar selection methodology, applying contrastive learning (an unsupervised machine learning technique) to Dark Energy Survey imaging data.","We describe the use of this technique to train a neural network which isolates an 'island' of 11 sources, of which 7 are known $z\\sim6$ quasars.","Of the remaining four, three are newly discovered quasars (J0109-5424, $z=6.07$; J0122-4609, $z=5.99$; J0603-3923, $z=5.94$), as confirmed by follow-up Gemini-South/GMOS and archival NTT/EFOSC2 spectroscopy, implying a 91 per cent efficiency for our novel selection method; the final object on the island is a brown dwarf.","In one case (J0109-5424), emission below the Lyman limit unambiguously indicates the presence of a foreground source, though high-resolution optical/near-infrared imaging is still needed to confirm the quasar's lensed (multiply-imaged) nature.","Detection in the g band has led this quasar to escape selection by traditional colour cuts.","Our findings demonstrate that machine learning techniques can thus play a key role in unveiling populations of quasars missed by traditional methods."],"url":"http://arxiv.org/abs/2403.17903v1","category":"astro-ph.GA"}
{"created":"2024-03-26 17:33:54","title":"Global asymptotics for $\u03b2$-Krawtchouk corners processes via multi-level loop equations","abstract":"We introduce a two-parameter family of probability distributions, indexed by $\\beta/2 = \\theta > 0$ and $K \\in \\mathbb{Z}_{\\geq 0}$, that are called $\\beta$-Krawtchouk corners processes. These measures are related to Jack symmetric functions, and can be thought of as integrable discretizations of $\\beta$-corners processes from random matrix theory, or alternatively as non-determinantal measures on lozenge tilings of infinite domains. We show that as $K$ tends to infinity the height function of these models concentrates around an explicit limit shape, and prove that its fluctuations are asymptotically described by a pull-back of the Gaussian free field, which agrees with the one for Wigner matrices. The main tools we use to establish our results are certain multi-level loop equations introduced in our earlier work arXiv:2108.07710.","sentences":["We introduce a two-parameter family of probability distributions, indexed by $\\beta/2 = \\theta > 0$ and $K \\in \\mathbb{Z}_{\\geq 0}$, that are called $\\beta$-Krawtchouk corners processes.","These measures are related to Jack symmetric functions, and can be thought of as integrable discretizations of $\\beta$-corners processes from random matrix theory, or alternatively as non-determinantal measures on lozenge tilings of infinite domains.","We show that as $K$ tends to infinity the height function of these models concentrates around an explicit limit shape, and prove that its fluctuations are asymptotically described by a pull-back of the Gaussian free field, which agrees with the one for Wigner matrices.","The main tools we use to establish our results are certain multi-level loop equations introduced in our earlier work arXiv:2108.07710."],"url":"http://arxiv.org/abs/2403.17895v1","category":"math.PR"}
{"created":"2024-03-26 17:21:24","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","abstract":"3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Our code will be made publicly available.","sentences":["3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed without baking.","However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians.","We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images.","Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks.","Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically.","To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization.","Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions.","We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering.","Our code will be made publicly available."],"url":"http://arxiv.org/abs/2403.17888v1","category":"cs.CV"}
{"created":"2024-03-26 17:11:51","title":"Low-Latency Neural Stereo Streaming","abstract":"The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime. While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance. This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming. Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding. Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs.","sentences":["The rise of new video modalities like virtual reality or autonomous driving has increased the demand for efficient multi-view video compression methods, both in terms of rate-distortion (R-D) performance and in terms of delay and runtime.","While most recent stereo video compression approaches have shown promising performance, they compress left and right views sequentially, leading to poor parallelization and runtime performance.","This work presents Low-Latency neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video coding method designed for fast and efficient low-latency stereo video streaming.","Instead of using a sequential cross-view motion compensation like existing methods, LLSS introduces a bidirectional feature shifting module to directly exploit mutual information among views and encode them effectively with a joint cross-view prior model for entropy coding.","Thanks to this design, LLSS processes left and right views in parallel, minimizing latency; all while substantially improving R-D performance compared to both existing neural and conventional codecs."],"url":"http://arxiv.org/abs/2403.17879v1","category":"cs.CV"}
{"created":"2024-03-26 16:00:31","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","abstract":"3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times. However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization. We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application. Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry. We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes. Our code will be released in https://github.com/maturk/dn-splatter.","sentences":["3D Gaussian splatting, a novel differentiable rendering technique, has achieved state-of-the-art novel view synthesis results with high rendering speeds and relatively low training times.","However, its performance on scenes commonly seen in indoor datasets is poor due to the lack of geometric constraints during optimization.","We extend 3D Gaussian splatting with depth and normal cues to tackle challenging indoor datasets and showcase techniques for efficient mesh extraction, an important downstream application.","Specifically, we regularize the optimization procedure with depth information, enforce local smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians supervised by normal cues to achieve better alignment with the true scene geometry.","We improve depth estimation and novel view synthesis results over baselines and show how this simple yet effective regularization technique can be used to directly extract meshes from the Gaussian representation yielding more physically accurate reconstructions on indoor scenes.","Our code will be released in https://github.com/maturk/dn-splatter."],"url":"http://arxiv.org/abs/2403.17822v1","category":"cs.CV"}
{"created":"2024-03-26 15:57:16","title":"Multiple solutions for quasilinear elliptic problems with concave and convex nonlinearities","abstract":"We prove the existence of multiple signed bounded solutions for a quasilinear elliptic equation with concave and convex nonlinearities. For this, we use a variational approach in an intersection Banach space indroduced by Candela and Palmieri, and a truncation technique given by Garcia Azorero and Peral.","sentences":["We prove the existence of multiple signed bounded solutions for a quasilinear elliptic equation with concave and convex nonlinearities.","For this, we use a variational approach in an intersection Banach space indroduced by Candela and Palmieri, and a truncation technique given by Garcia Azorero and Peral."],"url":"http://arxiv.org/abs/2403.17821v1","category":"math.AP"}
{"created":"2024-03-26 15:40:44","title":"Stabilization for degenerate equations with drift and small singular term","abstract":"We consider a degenerate/singular wave equation in one dimension, with drift and in presence of a leading operator which is not in divergence form. We impose a homogeneous Dirichlet boundary condition where the degeneracy occurs and a boundary damping at the other endpoint. We provide some conditions for the uniform exponential decay of solutions for the associated Cauchy problem.","sentences":["We consider a degenerate/singular wave equation in one dimension, with drift and in presence of a leading operator which is not in divergence form.","We impose a homogeneous Dirichlet boundary condition where the degeneracy occurs and a boundary damping at the other endpoint.","We provide some conditions for the uniform exponential decay of solutions for the associated Cauchy problem."],"url":"http://arxiv.org/abs/2403.17802v1","category":"math.AP"}
{"created":"2024-03-26 14:20:01","title":"Coupled Boundary and Volume Integral Equations for Electromagnetic Scattering","abstract":"We study frequency domain electromagnetic scattering at a bounded, penetrable, and inhomogeneous obstacle $ \\Omega \\subset \\mathbb{R}^3 $. From the Stratton-Chu integral representation, we derive a new representation formula when constant reference coefficients are given for the interior domain. The resulting integral representation contains the usual layer potentials, but also volume potentials on $\\Omega$. Then it is possible to follow a single-trace approach to obtain boundary integral equations perturbed by traces of compact volume integral operators with weakly singular kernels. The coupled boundary and volume integral equations are discretized with a Galerkin approach with usual Curl-conforming and Div-conforming finite elements on the boundary and in the volume. Compression techniques and special quadrature rules for singular integrands are required for an efficient and accurate method. Numerical experiments provide evidence that our new formulation enjoys promising properties.","sentences":["We study frequency domain electromagnetic scattering at a bounded, penetrable, and inhomogeneous obstacle $ \\Omega","\\subset","\\mathbb{R}^3 $.","From the Stratton-Chu integral representation, we derive a new representation formula when constant reference coefficients are given for the interior domain.","The resulting integral representation contains the usual layer potentials, but also volume potentials on $\\Omega$. Then it is possible to follow a single-trace approach to obtain boundary integral equations perturbed by traces of compact volume integral operators with weakly singular kernels.","The coupled boundary and volume integral equations are discretized with a Galerkin approach with usual Curl-conforming and Div-conforming finite elements on the boundary and in the volume.","Compression techniques and special quadrature rules for singular integrands are required for an efficient and accurate method.","Numerical experiments provide evidence that our new formulation enjoys promising properties."],"url":"http://arxiv.org/abs/2403.17731v1","category":"math.NA"}
{"created":"2024-03-26 14:11:19","title":"Regularity for nonlocal equations with local Neumann boundary conditions","abstract":"In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\\gamma}$ domains which satisfy local Neumann conditions on the boundary. Such solutions typically blow up at the boundary like $v \\asymp d^{s-1}$ and are sometimes called large solutions. In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem. The results of this article will be important in a forthcoming work on nonlocal free boundary problems.","sentences":["In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\\gamma}$ domains which satisfy local Neumann conditions on the boundary.","Such solutions typically blow up at the boundary like $v \\asymp d^{s-1}$ and are sometimes called large solutions.","In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem.","The results of this article will be important in a forthcoming work on nonlocal free boundary problems."],"url":"http://arxiv.org/abs/2403.17723v1","category":"math.AP"}
{"created":"2024-03-26 13:00:01","title":"A scaling limit of the 2D parabolic Anderson model with exclusion interaction","abstract":"We consider the (discrete) parabolic Anderson model $\\partial u(t,x)/\\partial t=\\Delta u(t,x) +\\xi_t(x) u(t,x)$, $t\\geq 0$, $x\\in \\mathbb{Z}^d$. Here, the $\\xi$-field is $\\mathbb{R}$-valued, acting as a dynamic random environment, and $\\Delta$ represents the discrete Laplacian. We focus on the case where $\\xi$ is given by a rescaled symmetric simple exclusion process which converges to an Ornstein--Uhlenbeck process. By scaling the Laplacian diffusively and considering the equation on a torus, we demonstrate that in dimension $d=2$, when a suitably renormalized version of the above equation is considered, the sequence of solutions converges in law. This resolves an open problem from~\\cite{EH23}, where a similar result was shown in the three-dimensional case. The novel contribution in the present work is the establishment of a gradient bound on the transition probability of a fixed but arbitrary number of labelled exclusion particles.","sentences":["We consider the (discrete) parabolic Anderson model $\\partial u(t,x)/\\partial t=\\Delta u(t,x) +\\xi_t(x) u(t,x)$, $t\\geq 0$, $x\\in \\mathbb{Z}^d$. Here, the $\\xi$-field is $\\mathbb{R}$-valued, acting as a dynamic random environment, and $\\Delta$ represents the discrete Laplacian.","We focus on the case where $\\xi$ is given by a rescaled symmetric simple exclusion process which converges to an Ornstein--Uhlenbeck process.","By scaling the Laplacian diffusively and considering the equation on a torus, we demonstrate that in dimension $d=2$, when a suitably renormalized version of the above equation is considered, the sequence of solutions converges in law.","This resolves an open problem from~\\cite{EH23}, where a similar result was shown in the three-dimensional case.","The novel contribution in the present work is the establishment of a gradient bound on the transition probability of a fixed but arbitrary number of labelled exclusion particles."],"url":"http://arxiv.org/abs/2403.17669v1","category":"math.PR"}
{"created":"2024-03-26 12:56:22","title":"Stable non-Hausdorff Riemannian foliations","abstract":"This paper aims to construct foliations $\\mathcal{F}$ that are stable, meaning that any other foliation close to $\\mathcal{F}$ is conjugate to $\\mathcal{F}$ by a diffeomorphism. Our main result yields examples of stable Riemannian foliations $\\mathcal{F}$ with dense leaves. To our knowledge, these are the first examples of stable Riemannian foliations that are not Hausdorff.","sentences":["This paper aims to construct foliations $\\mathcal{F}$ that are stable, meaning that any other foliation close to $\\mathcal{F}$ is conjugate to $\\mathcal{F}$ by a diffeomorphism.","Our main result yields examples of stable Riemannian foliations $\\mathcal{F}$ with dense leaves.","To our knowledge, these are the first examples of stable Riemannian foliations that are not Hausdorff."],"url":"http://arxiv.org/abs/2403.17666v1","category":"math.DG"}
{"created":"2024-03-26 10:43:38","title":"The power of relativistic jets: a comparative study","abstract":"We present the results of a comparison between different methods to estimate the power of relativistic jets from active galactic nuclei (AGN). We selected a sample of 32 objects (21 flat-spectrum radio quasars, 7 BL Lacertae Objects, 2 misaligned AGN, and 2 changing-look AGN) from the Very Large Baseline Array (VLBA) observations at 43 GHz of the Boston University blazar program. We then calculated the total, radiative, and kinetic jet power from both radio and high-energy gamma-ray observations, and compare the values. We found an excellent agreement between the radiative power calculated by using the Blandford and K\\\"onigl model with 37 or 43 GHz data, and the values derived from the high-energy $\\gamma-$ray luminosity. The agreement is still acceptable if 15 GHz data are used, although with a larger dispersion, but it improves if we use a constant fraction of the $\\gamma-$ray luminosity. We found a good agreement also for the kinetic power calculated with Blandford and K\\\"onigl model with 15 GHz data, and the value from the extended radio emission. We also propose some easy-to-use equations to estimate the jet power.","sentences":["We present the results of a comparison between different methods to estimate the power of relativistic jets from active galactic nuclei (AGN).","We selected a sample of 32 objects (21 flat-spectrum radio quasars, 7 BL Lacertae Objects, 2 misaligned AGN, and 2 changing-look AGN) from the Very Large Baseline Array (VLBA) observations at 43 GHz of the Boston University blazar program.","We then calculated the total, radiative, and kinetic jet power from both radio and high-energy gamma-ray observations, and compare the values.","We found an excellent agreement between the radiative power calculated by using the Blandford and K\\\"onigl model with 37 or 43 GHz data, and the values derived from the high-energy $\\gamma-$ray luminosity.","The agreement is still acceptable if 15 GHz data are used, although with a larger dispersion, but it improves if we use a constant fraction of the $\\gamma-$ray luminosity.","We found a good agreement also for the kinetic power calculated with Blandford and K\\\"onigl model with 15 GHz data, and the value from the extended radio emission.","We also propose some easy-to-use equations to estimate the jet power."],"url":"http://arxiv.org/abs/2403.17581v1","category":"astro-ph.HE"}
{"created":"2024-03-26 10:40:40","title":"Kurokawa-Mizumoto congruence and differential operators on automorphic forms","abstract":"We give the sufficient conditions for the vector-valued Kurokawa-Mizumoto congruence related to the Klingen-Eisenstein series to hold. And we give a reinterpretation for differential operators on the automorphic form by the representation theory.","sentences":["We give the sufficient conditions for the vector-valued Kurokawa-Mizumoto congruence related to the Klingen-Eisenstein series to hold.","And we give a reinterpretation for differential operators on the automorphic form by the representation theory."],"url":"http://arxiv.org/abs/2403.17579v1","category":"math.NT"}
{"created":"2024-03-26 10:26:35","title":"Functional differential equations driven by c\u00e0dl\u00e0g rough paths","abstract":"The existence of unique solutions is established for rough differential equations (RDEs) with path-dependent coefficients and driven by c\\`adl\\`ag rough paths. Moreover, it is shown that the associated solution map, also known as It\\^o-Lyons map, is locally Lipschitz continuous. These results are then applied to various classes of rough differential equations, such as controlled RDEs and RDEs with delay, as well as stochastic differential equations with delay. To that end, a joint rough path is constructed for a c\\`adl\\`ag martingale and its delayed version, that corresponds to stochastic It\\^o integration.","sentences":["The existence of unique solutions is established for rough differential equations (RDEs) with path-dependent coefficients and driven by c\\`adl\\`ag rough paths.","Moreover, it is shown that the associated solution map, also known as It\\^o-Lyons map, is locally Lipschitz continuous.","These results are then applied to various classes of rough differential equations, such as controlled RDEs and RDEs with delay, as well as stochastic differential equations with delay.","To that end, a joint rough path is constructed for a c\\`adl\\`ag martingale and its delayed version, that corresponds to stochastic It\\^o integration."],"url":"http://arxiv.org/abs/2403.17573v1","category":"math.PR"}
{"created":"2024-03-26 10:25:21","title":"Enhancing Privacy in Federated Learning through Local Training","abstract":"In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm by comparing it to alternative techniques, considering both theoretical analysis and numerical results from a classification task.","sentences":["In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation.","We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents.","The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy.","Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent.","Further, we investigate how employing local training can enhance privacy, addressing point (ii).","In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs.","We assess the effectiveness of the proposed algorithm by comparing it to alternative techniques, considering both theoretical analysis and numerical results from a classification task."],"url":"http://arxiv.org/abs/2403.17572v1","category":"cs.LG"}
{"created":"2024-03-26 09:42:28","title":"NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation","abstract":"Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely \"Heuristics-Guided Segmentation\" (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.","sentences":["Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction.","However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows.","In this work, we propose a novel paradigm, namely \"Heuristics-Guided Segmentation\" (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions.","Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles.","Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes.","Project page: https://cnhaox.github.io/NeRF-HuGS/."],"url":"http://arxiv.org/abs/2403.17537v1","category":"cs.CV"}
{"created":"2024-03-26 09:22:22","title":"Robust Full Waveform Inversion with deep Hessian deblurring","abstract":"Full Waveform Inversion (FWI) is a technique widely used in geophysics to obtain high-resolution subsurface velocity models from waveform seismic data. Due to its large computation cost, most flavors of FWI rely only on the computation of the gradient of the loss function to estimate the update direction, therefore ignoring the contribution of the Hessian. Depending on the level of computational resources one can afford, an approximate of the inverse of the Hessian can be calculated and used to speed up the convergence of FWI towards the global (or a plausible local) minimum. In this work, we propose to use an approximate Hessian computed from a linearization of the wave-equation as commonly done in Least-Squares Migration (LSM). More precisely, we rely on the link between a migrated image and a doubly migrated image (i.e., an image obtained by demigration-migration of the migrated image) to estimate the inverse of the Hessian. However, instead of using non-stationary compact filters to link the two images and approximate the Hessian, we propose to use a deep neural network to directly learn the mapping between the FWI gradient (output) and its Hessian (blurred) counterpart (input). By doing so, the network learns to act as an approximate inverse Hessian: as such, when the trained network is applied to the FWI gradient, an enhanced update direction is obtained, which is shown to be beneficial for the convergence of FWI. The weights of the trained (deblurring) network are then transferred to the next FWI iteration to expedite convergence. We demonstrate the effectiveness of the proposed approach on two synthetic datasets and a field dataset.","sentences":["Full Waveform Inversion (FWI) is a technique widely used in geophysics to obtain high-resolution subsurface velocity models from waveform seismic data.","Due to its large computation cost, most flavors of FWI rely only on the computation of the gradient of the loss function to estimate the update direction, therefore ignoring the contribution of the Hessian.","Depending on the level of computational resources one can afford, an approximate of the inverse of the Hessian can be calculated and used to speed up the convergence of FWI towards the global (or a plausible local) minimum.","In this work, we propose to use an approximate Hessian computed from a linearization of the wave-equation as commonly done in Least-Squares Migration (LSM).","More precisely, we rely on the link between a migrated image and a doubly migrated image (i.e., an image obtained by demigration-migration of the migrated image) to estimate the inverse of the Hessian.","However, instead of using non-stationary compact filters to link the two images and approximate the Hessian, we propose to use a deep neural network to directly learn the mapping between the FWI gradient (output) and its Hessian (blurred) counterpart (input).","By doing so, the network learns to act as an approximate inverse Hessian: as such, when the trained network is applied to the FWI gradient, an enhanced update direction is obtained, which is shown to be beneficial for the convergence of FWI.","The weights of the trained (deblurring) network are then transferred to the next FWI iteration to expedite convergence.","We demonstrate the effectiveness of the proposed approach on two synthetic datasets and a field dataset."],"url":"http://arxiv.org/abs/2403.17518v1","category":"physics.geo-ph"}
{"created":"2024-03-26 09:09:40","title":"EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields","abstract":"Machine learning force fields (MLFFs) have emerged as a promising approach to bridge the accuracy of quantum mechanical methods and the efficiency of classical force fields. However, the abundance of MLFF models and the challenge of accurately predicting atomic forces pose significant obstacles in their practical application. In this paper, we propose a novel ensemble learning framework, EL-MLFFs, which leverages the stacking method to integrate predictions from diverse MLFFs and enhance force prediction accuracy. By constructing a graph representation of molecular structures and employing a graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures atomic interactions and refines force predictions. We evaluate our approach on two distinct datasets: methane molecules and methanol adsorbed on a Cu(100) surface. The results demonstrate that EL-MLFFs significantly improves force prediction accuracy compared to individual MLFFs, with the ensemble of all eight models yielding the best performance. Moreover, our ablation study highlights the crucial roles of the residual network and graph attention layers in the model's architecture. The EL-MLFFs framework offers a promising solution to the challenges of model selection and force prediction accuracy in MLFFs, paving the way for more reliable and efficient molecular simulations.","sentences":["Machine learning force fields (MLFFs) have emerged as a promising approach to bridge the accuracy of quantum mechanical methods and the efficiency of classical force fields.","However, the abundance of MLFF models and the challenge of accurately predicting atomic forces pose significant obstacles in their practical application.","In this paper, we propose a novel ensemble learning framework, EL-MLFFs, which leverages the stacking method to integrate predictions from diverse MLFFs and enhance force prediction accuracy.","By constructing a graph representation of molecular structures and employing a graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures atomic interactions and refines force predictions.","We evaluate our approach on two distinct datasets: methane molecules and methanol adsorbed on a Cu(100) surface.","The results demonstrate that EL-MLFFs significantly improves force prediction accuracy compared to individual MLFFs, with the ensemble of all eight models yielding the best performance.","Moreover, our ablation study highlights the crucial roles of the residual network and graph attention layers in the model's architecture.","The EL-MLFFs framework offers a promising solution to the challenges of model selection and force prediction accuracy in MLFFs, paving the way for more reliable and efficient molecular simulations."],"url":"http://arxiv.org/abs/2403.17507v1","category":"cs.LG"}
{"created":"2024-03-26 08:53:25","title":"Dr.Hair: Reconstructing Scalp-Connected Hair Strands without Pre-training via Differentiable Rendering of Line Segments","abstract":"In the film and gaming industries, achieving a realistic hair appearance typically involves the use of strands originating from the scalp. However, reconstructing these strands from observed surface images of hair presents significant challenges. The difficulty in acquiring Ground Truth (GT) data has led state-of-the-art learning-based methods to rely on pre-training with manually prepared synthetic CG data. This process is not only labor-intensive and costly but also introduces complications due to the domain gap when compared to real-world data. In this study, we propose an optimization-based approach that eliminates the need for pre-training. Our method represents hair strands as line segments growing from the scalp and optimizes them using a novel differentiable rendering algorithm. To robustly optimize a substantial number of slender explicit geometries, we introduce 3D orientation estimation utilizing global optimization, strand initialization based on Laplace's equation, and reparameterization that leverages geometric connectivity and spatial proximity. Unlike existing optimization-based methods, our method is capable of reconstructing internal hair flow in an absolute direction. Our method exhibits robust and accurate inverse rendering, surpassing the quality of existing methods and significantly improving processing speed.","sentences":["In the film and gaming industries, achieving a realistic hair appearance typically involves the use of strands originating from the scalp.","However, reconstructing these strands from observed surface images of hair presents significant challenges.","The difficulty in acquiring Ground Truth (GT) data has led state-of-the-art learning-based methods to rely on pre-training with manually prepared synthetic CG data.","This process is not only labor-intensive and costly but also introduces complications due to the domain gap when compared to real-world data.","In this study, we propose an optimization-based approach that eliminates the need for pre-training.","Our method represents hair strands as line segments growing from the scalp and optimizes them using a novel differentiable rendering algorithm.","To robustly optimize a substantial number of slender explicit geometries, we introduce 3D orientation estimation utilizing global optimization, strand initialization based on Laplace's equation, and reparameterization that leverages geometric connectivity and spatial proximity.","Unlike existing optimization-based methods, our method is capable of reconstructing internal hair flow in an absolute direction.","Our method exhibits robust and accurate inverse rendering, surpassing the quality of existing methods and significantly improving processing speed."],"url":"http://arxiv.org/abs/2403.17496v1","category":"cs.CV"}
{"created":"2024-03-26 08:15:45","title":"C-minimal fields have the exchange property","abstract":"We show that C-minimal fields (i.e., C-minimal expansions of ACVF) have the exchange property, answering a question of Haskell and Macpherson. Additionally, we strengthen some theorems of Cubides Kovacsics and Delon on C-minimal fields. First, we show that definably complete C-minimal fields of characteristic 0 have generic differentiability. Second, we show that if the induced structure on the residue field is a pure ACF, then polynomial boundedness holds. In fact, polynomial boundedness can only fail if there are unexpected definable automorphisms of the multiplicative group of the residue field.","sentences":["We show that C-minimal fields (i.e., C-minimal expansions of ACVF) have the exchange property, answering a question of Haskell and Macpherson.","Additionally, we strengthen some theorems of Cubides Kovacsics and Delon on C-minimal fields.","First, we show that definably complete C-minimal fields of characteristic 0 have generic differentiability.","Second, we show that if the induced structure on the residue field is a pure ACF, then polynomial boundedness holds.","In fact, polynomial boundedness can only fail if there are unexpected definable automorphisms of the multiplicative group of the residue field."],"url":"http://arxiv.org/abs/2403.17478v1","category":"math.LO"}
{"created":"2024-03-26 07:56:35","title":"Fundamental solutions to Kolmogorov-Fokker-Planck equations with rough coefficients: existence, uniqueness, upper estimates","abstract":"We show the existence and uniqueness of fundamental solution operators to Kolmo\\-gorov-Fokker-Planck equations with rough (measurable) coefficients and local or integral diffusion on finite and infinite time strips. In the local case, that is to say when the diffusion operator is of differential type, we prove $\\L^2$ decay using Davies' method and the conservation property. We also prove that the existence of a generalized fundamental solution with the expected pointwise Gaussian upper bound is equivalent to Moser's $\\L^2-\\L^\\infty$ estimates for local weak solutions to the equation and its adjoint. When coefficients are real, this gives the existence and uniqueness of such a generalized fundamental solution and a new and natural way to obtain pointwise decay.","sentences":["We show the existence and uniqueness of fundamental solution operators to Kolmo\\-gorov-Fokker-Planck equations with rough (measurable) coefficients and local or integral diffusion on finite and infinite time strips.","In the local case, that is to say when the diffusion operator is of differential type, we prove $\\L^2$ decay using Davies' method and the conservation property.","We also prove that the existence of a generalized fundamental solution with the expected pointwise Gaussian upper bound is equivalent to Moser's $\\L^2-\\L^\\infty$ estimates for local weak solutions to the equation and its adjoint.","When coefficients are real, this gives the existence and uniqueness of such a generalized fundamental solution and a new and natural way to obtain pointwise decay."],"url":"http://arxiv.org/abs/2403.17468v1","category":"math.AP"}
{"created":"2024-03-26 07:53:28","title":"Localized Inverse Design in Conservation Laws and Hamilton-Jacobi Equations","abstract":"Consider the inverse design problem for a scalar conservation law, i.e., the problem of finding initial data evolving into a given profile at a given time. The solution we present below takes into account localizations both in the final interval where the profile is assigned and in the initial interval where the datum is sought, as well as additional a priori constraints on the datum's range provided by the model. These results are motivated and can be applied to data assimilation procedures in traffic modeling and accidents localization.","sentences":["Consider the inverse design problem for a scalar conservation law, i.e., the problem of finding initial data evolving into a given profile at a given time.","The solution we present below takes into account localizations both in the final interval where the profile is assigned and in the initial interval where the datum is sought, as well as additional a priori constraints on the datum's range provided by the model.","These results are motivated and can be applied to data assimilation procedures in traffic modeling and accidents localization."],"url":"http://arxiv.org/abs/2403.17463v1","category":"math.AP"}
{"created":"2024-03-26 07:53:21","title":"Rapid neutron star equation of state inference with Normalising Flows","abstract":"The first direct detection of gravitational waves from binary neutron stars on the 17th of August, 2017, (GW170817) heralded the arrival of a new messenger for probing neutron star astrophysics and provided the first constraints on neutron star equation of state from gravitational wave observations. Significant computational effort was expended to obtain these first results and therefore, as observations of binary neutron star coalescence become more routine in the coming observing runs, there is a need to improve the analysis speed and flexibility. Here, we present a rapid approach for inferring the neutron star equation of state based on Normalising Flows. As a demonstration, using the same input data, our approach, ASTREOS, produces results consistent with those presented by the LIGO-Virgo collaboration but requires < 1 sec to generate neutron star equation of state confidence intervals. Furthermore, ASTREOS allows for non-parametric equation of state inference. This rapid analysis will not only facilitate neutron star equation of state studies but can potentially enhance future alerts for electromagnetic follow-up observations of binary neutron star mergers.","sentences":["The first direct detection of gravitational waves from binary neutron stars on the 17th of August, 2017, (GW170817) heralded the arrival of a new messenger for probing neutron star astrophysics and provided the first constraints on neutron star equation of state from gravitational wave observations.","Significant computational effort was expended to obtain these first results and therefore, as observations of binary neutron star coalescence become more routine in the coming observing runs, there is a need to improve the analysis speed and flexibility.","Here, we present a rapid approach for inferring the neutron star equation of state based on Normalising Flows.","As a demonstration, using the same input data, our approach, ASTREOS, produces results consistent with those presented by the LIGO-Virgo collaboration but requires < 1 sec to generate neutron star equation of state confidence intervals.","Furthermore, ASTREOS allows for non-parametric equation of state inference.","This rapid analysis will not only facilitate neutron star equation of state studies but can potentially enhance future alerts for electromagnetic follow-up observations of binary neutron star mergers."],"url":"http://arxiv.org/abs/2403.17462v1","category":"gr-qc"}
{"created":"2024-03-26 07:51:46","title":"Sub-Dirac operators, spectral Einstein functionals and the noncommutative residue","abstract":"In this paper, we define the spectral Einstein functional associated with the sub-Dirac operator for manifolds with boundary. A proof of the Dabrowski-Sitarz-Zalecki type theorem for spectral Einstein functions associated with the sub-Dirac operator on four-dimensional manifolds with boundary is also given.","sentences":["In this paper, we define the spectral Einstein functional associated with the sub-Dirac operator for manifolds with boundary.","A proof of the Dabrowski-Sitarz-Zalecki type theorem for spectral Einstein functions associated with the sub-Dirac operator on four-dimensional manifolds with boundary is also given."],"url":"http://arxiv.org/abs/2403.17461v1","category":"math.DG"}
{"created":"2024-03-26 07:46:26","title":"Energy-momentum Tensor:Noether vs Hilbert","abstract":"We revisit the old problem of the energy-momentum tensor in general relativistic field theories. On the basis of the general covariance we derive a simple equation for the Hilbert and Noether energy-momentum tensors for the scalar and electromagnetic field theories. We see that the two definitions of energy-momentum tensors coincide and identify the Noether current if the space-time has the Killing vector. Relation to the Wald entropy is also briefly discussed.","sentences":["We revisit the old problem of the energy-momentum tensor in general relativistic field theories.","On the basis of the general covariance we derive a simple equation for the Hilbert and Noether energy-momentum tensors for the scalar and electromagnetic field theories.","We see that the two definitions of energy-momentum tensors coincide and identify the Noether current if the space-time has the Killing vector.","Relation to the Wald entropy is also briefly discussed."],"url":"http://arxiv.org/abs/2403.17457v1","category":"gr-qc"}
{"created":"2024-03-26 07:04:58","title":"Restrictions of holomorphic sections to products","abstract":"We associate quantum states with subsets of a product of two compact connected K\\\"ahler manifolds $M_1$ and $M_2$. To associate the quantum state with the subset, we use the map that restricts holomorphic sections of the quantum line bundle over the product of the two K\\\"ahler manifolds to the subset. We present a description of the kernel of this restriction map when the subset is a finite union of products. This in turn shows that the quantum states associated with the finite union of products are separable. Finally, for every pure state and certain mixed state, we construct subsets of $M_1\\times M_2$ such that the states associated with these subsets are the original states, to begin with.","sentences":["We associate quantum states with subsets of a product of two compact connected K\\\"ahler manifolds $M_1$ and $M_2$. To associate the quantum state with the subset, we use the map that restricts holomorphic sections of the quantum line bundle over the product of the two K\\\"ahler manifolds to the subset.","We present a description of the kernel of this restriction map when the subset is a finite union of products.","This in turn shows that the quantum states associated with the finite union of products are separable.","Finally, for every pure state and certain mixed state, we construct subsets of $M_1\\times M_2$ such that the states associated with these subsets are the original states, to begin with."],"url":"http://arxiv.org/abs/2403.17435v1","category":"math.DG"}
{"created":"2024-03-26 06:13:23","title":"Allard-Type Regularity for Varifolds with Prescribed Contact Angle","abstract":"Given a bounded $C^2$ domain in $\\mathbb{R}^{n+1}$ and an integral $n$-rectifiable varifold $V$ with bounded first variation and bounded generalized mean curvature. Given a $C^1$ function $\\theta$ defined on the boundary of the domain with range $(0,\\pi)$, we assume $V$ has prescribed contact angle $\\theta$ with $\\partial \\Omega$ and the tangent cone of $V$ at a point $X \\in \\partial \\Omega$ is a half-hyperplane of density one. Then we can show that the support of $V$ is a $C^{1,\\gamma}$ hypersurface with boundary near $X$ for some $\\gamma \\in (0,1)$.","sentences":["Given a bounded $C^2$ domain in $\\mathbb{R}^{n+1}$ and an integral $n$-rectifiable varifold $V$ with bounded first variation and bounded generalized mean curvature.","Given a $C^1$ function $\\theta$ defined on the boundary of the domain with range $(0,\\pi)$, we assume $V$ has prescribed contact angle $\\theta$ with $\\partial \\Omega$ and the tangent cone of $V$ at a point $X \\in \\partial \\Omega$ is a half-hyperplane of density one.","Then we can show that the support of $V$ is a $C^{1,\\gamma}$ hypersurface with boundary near $X$ for some $\\gamma \\in (0,1)$."],"url":"http://arxiv.org/abs/2403.17415v1","category":"math.DG"}
{"created":"2024-03-26 05:13:26","title":"Incoherent GRAPE (inGRAPE) for optimization of quantum systems with environmentally assisted control","abstract":"In this work, we review several results on development and application of incoherent version of GRAPE (Gradient Ascent Pulse Engineering) approach, inGRAPE, to optimization for open quantum systems driven by both coherent and incoherent controls. In the incoherent control approach, the environment serves as a control together with coherent field, and decoherence rates become generally time-dependent. For a qubit, explicit analytic expressions for evolution of the density matrix were obtained by solving a cubic equation via Cardano method. We discuss applications of incoherent GRAPE method to high fidelity gate generation for open one- and two-qubit systems and surprising properties of the underlying control landscapes, forming two groups - smooth single peak landscapes for Hadamard, C-NOT and C-Z gates, and more complicated with two peaks for T (or $\\pi/8$) gate. For a qutrit, a formulation of the environment-assisted incoherent control with time-dependent decoherence rates is provided.","sentences":["In this work, we review several results on development and application of incoherent version of GRAPE (Gradient Ascent Pulse Engineering) approach, inGRAPE, to optimization for open quantum systems driven by both coherent and incoherent controls.","In the incoherent control approach, the environment serves as a control together with coherent field, and decoherence rates become generally time-dependent.","For a qubit, explicit analytic expressions for evolution of the density matrix were obtained by solving a cubic equation via Cardano method.","We discuss applications of incoherent GRAPE method to high fidelity gate generation for open one-","and two-qubit systems and surprising properties of the underlying control landscapes, forming two groups - smooth single peak landscapes for Hadamard, C-NOT and C-Z gates, and more complicated with two peaks for T (or $\\pi/8$) gate.","For a qutrit, a formulation of the environment-assisted incoherent control with time-dependent decoherence rates is provided."],"url":"http://arxiv.org/abs/2403.17388v1","category":"quant-ph"}
{"created":"2024-03-26 04:53:15","title":"Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks","abstract":"This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.","sentences":["This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra.","The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture.","The parallel estimation architecture is a core module for direct wrapped phase prediction.","This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval.","To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function.","We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity.","We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies.","For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness.","Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech.","To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks."],"url":"http://arxiv.org/abs/2403.17378v1","category":"cs.SD"}
{"created":"2024-03-26 03:27:58","title":"An average intersection estimate for families of diffeomorphisms","abstract":"We show that for any sufficiently rich compact family $\\mathcal{H}$ of $C^1$ diffeomorphisms of a closed Riemannanian manifold $M$, the average geometric intersection number over $h \\in \\mathcal{H}$ between $h(V)$ and $W$, for $V, W$ any complementary dimensional submanifolds of $M$, is approximately (i.e. up to a uniform multiplicative error depending only on $\\mathcal{H}$) the product of their volumes. We also give a construction showing that such families always exist.","sentences":["We show that for any sufficiently rich compact family $\\mathcal{H}$ of $C^1$ diffeomorphisms of a closed Riemannanian manifold $M$, the average geometric intersection number over $h \\in \\mathcal{H}$ between $h(V)$ and $W$, for $V, W$ any complementary dimensional submanifolds of $M$, is approximately (i.e. up to a uniform multiplicative error depending only on $\\mathcal{H}$) the product of their volumes.","We also give a construction showing that such families always exist."],"url":"http://arxiv.org/abs/2403.17349v1","category":"math.DG"}
{"created":"2024-03-26 03:07:32","title":"Disambiguate Entity Matching through Relation Discovery with Large Language Models","abstract":"Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a \"match,\" especially when integrating with external databases. This ambiguity arises due to varying levels of detail and granularity among entities, complicating exact matches. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the \"relations\" between entities as crucial for resolving ambiguities in matching. By predefining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities.","sentences":["Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication.","Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT.","However, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a \"match,\" especially when integrating with external databases.","This ambiguity arises due to varying levels of detail and granularity among entities, complicating exact matches.","We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the \"relations\" between entities as crucial for resolving ambiguities in matching.","By predefining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities."],"url":"http://arxiv.org/abs/2403.17344v1","category":"cs.DB"}
{"created":"2024-03-26 02:47:12","title":"Cosmic-ray Acceleration in Core-Collapse Supernova Remnants with the Wind Termination Shock","abstract":"We investigate the attainable maximum energy of particles accelerated in the core-collapse supernova remnant (SNR) shock propagating in the free wind region with the Parker-spiral magnetic field, current sheet, and the wind termination shock (WTS) by using test particle simulations. This work focuses on Wolf-Rayet stars as progenitors. The magnetic field amplification in the free wind region (shock upstream region) is not considered in this work. Test particle simulations show that particles escaped from the core-collapse SNR reach and move along the WTS, and eventually return to the SNR shock from the poles or equator of the WTS. The particle attainable energy can be boosted by this cyclic motion between the SNR shock and WTS and can be larger than the particle energy that is limited by escape from the SNR shock. The particle energy limited by the cyclic motion between the SNR shock and WTS is about $10-100~{\\rm TeV}$. Thus, the core-collapse SNR without upstream magnetic field amplification can be the origin of the break around $10~{\\rm TeV}$ of the energy spectrum of observed cosmic ray protons and helium.","sentences":["We investigate the attainable maximum energy of particles accelerated in the core-collapse supernova remnant (SNR) shock propagating in the free wind region with the Parker-spiral magnetic field, current sheet, and the wind termination shock (WTS) by using test particle simulations.","This work focuses on Wolf-Rayet stars as progenitors.","The magnetic field amplification in the free wind region (shock upstream region) is not considered in this work.","Test particle simulations show that particles escaped from the core-collapse SNR reach and move along the WTS, and eventually return to the SNR shock from the poles or equator of the WTS.","The particle attainable energy can be boosted by this cyclic motion between the SNR shock and WTS and can be larger than the particle energy that is limited by escape from the SNR shock.","The particle energy limited by the cyclic motion between the SNR shock and WTS is about $10-100~{\\rm","TeV}$.","Thus, the core-collapse SNR without upstream magnetic field amplification can be the origin of the break around $10~{\\rm TeV}$ of the energy spectrum of observed cosmic ray protons and helium."],"url":"http://arxiv.org/abs/2403.17335v1","category":"astro-ph.HE"}
{"created":"2024-03-26 01:14:55","title":"Two Birds with One Stone: Differential Privacy by Low-power SRAM Memory","abstract":"The software-based implementation of differential privacy mechanisms has been shown to be neither friendly for lightweight devices nor secure against side-channel attacks. In this work, we aim to develop a hardware-based technique to achieve differential privacy by design. In contrary to the conventional software-based noise generation and injection process, our design realizes local differential privacy (LDP) by harnessing the inherent hardware noise into controlled LDP noise when data is stored in the memory. Specifically, the noise is tamed through a novel memory design and power downscaling technique, which leads to double-faceted gains in privacy and power efficiency. A well-round study that consists of theoretical design and analysis and chip implementation and experiments is presented. The results confirm that the developed technique is differentially private, saves 88.58% system power, speeds up software-based DP mechanisms by more than 10^6 times, while only incurring 2.46% chip overhead and 7.81% estimation errors in data recovery.","sentences":["The software-based implementation of differential privacy mechanisms has been shown to be neither friendly for lightweight devices nor secure against side-channel attacks.","In this work, we aim to develop a hardware-based technique to achieve differential privacy by design.","In contrary to the conventional software-based noise generation and injection process, our design realizes local differential privacy (LDP) by harnessing the inherent hardware noise into controlled LDP noise when data is stored in the memory.","Specifically, the noise is tamed through a novel memory design and power downscaling technique, which leads to double-faceted gains in privacy and power efficiency.","A well-round study that consists of theoretical design and analysis and chip implementation and experiments is presented.","The results confirm that the developed technique is differentially private, saves 88.58% system power, speeds up software-based DP mechanisms by more than 10^6 times, while only incurring 2.46% chip overhead and 7.81% estimation errors in data recovery."],"url":"http://arxiv.org/abs/2403.17303v1","category":"cs.CR"}
{"created":"2024-03-26 00:56:06","title":"Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs","abstract":"Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns. Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions.","sentences":["Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer.","By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations.","This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn.","2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers.","As sentence complexity increases, more layers are required for learning grammatical capabilities.","3) Morphological and semantics/syntax interface-related features are harder to capture than syntax.","4) For Transformer-based models, both embeddings and attentions capture grammatical features but show distinct patterns.","Different attention heads exhibit similar tendencies toward various linguistic phenomena, but with varied contributions."],"url":"http://arxiv.org/abs/2403.17299v1","category":"cs.CL"}
{"created":"2024-03-26 00:51:12","title":"Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation","abstract":"Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints. In this work, we design and implement new privacy-preserving machine learning protocols for logistic regression and neural network models. We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data. A significant source of inefficiency and inaccuracy in existing methods arises from using Yao's garbled circuits to compute non-linear activation functions. We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy.   Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning. Instead of claiming that the servers gain no knowledge during the computation, we contend that while some information is revealed about access patterns to lookup tables, it maintains epsilon-dX-privacy. Leveraging this relaxation significantly reduces the computational resources needed for training. We present new cryptographic protocols tailored to this relaxed security paradigm and define and analyze the leakage. Our evaluations show that our logistic regression protocol is up to 9x faster, and the neural network training is up to 688x faster than SecureML. Notably, our neural network achieves an accuracy of 96.6% on MNIST in 15 epochs, outperforming prior benchmarks that capped at 93.4% using the same architecture.","sentences":["Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints.","In this work, we design and implement new privacy-preserving machine learning protocols for logistic regression and neural network models.","We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data.","A significant source of inefficiency and inaccuracy in existing methods arises from using Yao's garbled circuits to compute non-linear activation functions.","We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy.   ","Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning.","Instead of claiming that the servers gain no knowledge during the computation, we contend that while some information is revealed about access patterns to lookup tables, it maintains epsilon-dX-privacy.","Leveraging this relaxation significantly reduces the computational resources needed for training.","We present new cryptographic protocols tailored to this relaxed security paradigm and define and analyze the leakage.","Our evaluations show that our logistic regression protocol is up to 9x faster, and the neural network training is up to 688x faster than SecureML.","Notably, our neural network achieves an accuracy of 96.6% on MNIST in 15 epochs, outperforming prior benchmarks that capped at 93.4% using the same architecture."],"url":"http://arxiv.org/abs/2403.17296v1","category":"cs.CR"}
{"created":"2024-03-26 00:41:54","title":"Tracing and segmentation of molecular patterns in 3-dimensional cryo-et/em density maps through algorithmic image processing and deep learning-based techniques","abstract":"Understanding the structures of biological macromolecules is highly important as they are closely associated with cellular functionalities. Comprehending the precise organization actin filaments is crucial because they form the dynamic cytoskeleton, which offers structural support to cells and connects the cell's interior with its surroundings. However, determining the precise organization of actin filaments is challenging due to the poor quality of cryo-electron tomography (cryo-ET) images, which suffer from low signal-to-noise (SNR) ratios and the presence of missing wedge, as well as diverse shape characteristics of actin filaments. To address these formidable challenges, the primary component of this dissertation focuses on developing sophisticated computational techniques for tracing actin filaments. In particular, three novel methodologies have been developed: i) BundleTrac, for tracing bundle-like actin filaments found in Stereocilium, ii) Spaghetti Tracer, for tracing filaments that move individually with loosely cohesive movements, and iii) Struwwel Tracer, for tracing randomly orientated actin filaments in the actin network. The second component of the dissertation introduces a convolutional neural network (CNN) based segmentation model to determine the location of protein secondary structures, such as helices and beta-sheets, in medium-resolution (5-10 Angstrom) 3-dimensional cryo-electron microscopy (cryo-EM) images. This methodology later evolved into a tool named DeepSSETracer. The final component of the dissertation presents a novel algorithm, cylindrical fit measure, to estimate image structure match at helix regions in medium-resolution cryo-EM images. Overall, my dissertation has made significant contributions to addressing critical research challenges in structural biology by introducing various computational methods and tools.","sentences":["Understanding the structures of biological macromolecules is highly important as they are closely associated with cellular functionalities.","Comprehending the precise organization actin filaments is crucial because they form the dynamic cytoskeleton, which offers structural support to cells and connects the cell's interior with its surroundings.","However, determining the precise organization of actin filaments is challenging due to the poor quality of cryo-electron tomography (cryo-ET) images, which suffer from low signal-to-noise (SNR) ratios and the presence of missing wedge, as well as diverse shape characteristics of actin filaments.","To address these formidable challenges, the primary component of this dissertation focuses on developing sophisticated computational techniques for tracing actin filaments.","In particular, three novel methodologies have been developed: i) BundleTrac, for tracing bundle-like actin filaments found in Stereocilium, ii) Spaghetti Tracer, for tracing filaments that move individually with loosely cohesive movements, and iii) Struwwel Tracer, for tracing randomly orientated actin filaments in the actin network.","The second component of the dissertation introduces a convolutional neural network (CNN) based segmentation model to determine the location of protein secondary structures, such as helices and beta-sheets, in medium-resolution (5-10 Angstrom) 3-dimensional cryo-electron microscopy (cryo-EM) images.","This methodology later evolved into a tool named DeepSSETracer.","The final component of the dissertation presents a novel algorithm, cylindrical fit measure, to estimate image structure match at helix regions in medium-resolution cryo-EM images.","Overall, my dissertation has made significant contributions to addressing critical research challenges in structural biology by introducing various computational methods and tools."],"url":"http://arxiv.org/abs/2403.17293v1","category":"eess.IV"}
{"created":"2024-03-25 23:48:19","title":"A unimodular Kaluza-Klein theory","abstract":"Unimodular gravity became an object of increasing interest in the late $80$-ties and was recently used in primordial Universe modeling with cosmological constant, in the context of the Brans-Dicke gravity including scalar field. In the present article we investigate the possibility of imposing the unimodular condition within the $5$-dimensional Kaluza-Klein theory including the scalar field. The variational principle is formulated in $5$ dimensions first, and dimensional reduction is applied to the resulting set of equations. A cosmological model based on these equations is then presented and discussed.","sentences":["Unimodular gravity became an object of increasing interest in the late $80$-ties and was recently used in primordial Universe modeling with cosmological constant, in the context of the Brans-Dicke gravity including scalar field.","In the present article we investigate the possibility of imposing the unimodular condition within the $5$-dimensional Kaluza-Klein theory including the scalar field.","The variational principle is formulated in $5$ dimensions first, and dimensional reduction is applied to the resulting set of equations.","A cosmological model based on these equations is then presented and discussed."],"url":"http://arxiv.org/abs/2403.17278v1","category":"gr-qc"}
{"created":"2024-03-25 23:11:31","title":"Tian's stabilization problem for toric Fanos","abstract":"In 1988, Tian posed the stabilization problem for equivariant global log canonical thresholds. We solve it in the case of toric Fano manifolds. This is the first general result on Tian's problem. A key new estimate involves expressing complex singularity exponents associated to orbits of a group action in terms of support and gauge functions from convex geometry. These techniques also yield a resolution of another conjecture of Tian from 2012 on more general thresholds associated to Grassmannians of plurianticanonical series.","sentences":["In 1988, Tian posed the stabilization problem for equivariant global log canonical thresholds.","We solve it in the case of toric Fano manifolds.","This is the first general result on Tian's problem.","A key new estimate involves expressing complex singularity exponents associated to orbits of a group action in terms of support and gauge functions from convex geometry.","These techniques also yield a resolution of another conjecture of Tian from 2012 on more general thresholds associated to Grassmannians of plurianticanonical series."],"url":"http://arxiv.org/abs/2403.17262v1","category":"math.AG"}
{"created":"2024-03-25 22:46:16","title":"SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution","abstract":"Singleton mentions, i.e.~entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective. However previous attempts to incorporate their detection in end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention spans in the OntoNotes benchmark. This paper addresses this limitation by combining predicted mentions from existing nested NER systems and features derived from OntoNotes syntax trees. With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons. We then propose a two-step neural mention and coreference resolution system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed singleton training yields results comparable to end-to-end systems for OntoNotes, while improving OOD stability (+1.1 avg. F1). We conduct error analysis for mention detection and delve into its impact on coreference clustering, revealing that precision improvements deliver more substantial benefits than increases in recall for resolving coreference chains.","sentences":["Singleton mentions, i.e.~entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective.","However previous attempts to incorporate their detection in end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention spans in the OntoNotes benchmark.","This paper addresses this limitation by combining predicted mentions from existing nested NER systems and features derived from OntoNotes syntax trees.","With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons.","We then propose a two-step neural mention and coreference resolution system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the out-of-domain (OOD) OntoGUM corpus.","Results indicate that reconstructed singleton training yields results comparable to end-to-end systems for OntoNotes, while improving OOD stability (+1.1 avg. F1).","We conduct error analysis for mention detection and delve into its impact on coreference clustering, revealing that precision improvements deliver more substantial benefits than increases in recall for resolving coreference chains."],"url":"http://arxiv.org/abs/2403.17245v1","category":"cs.CL"}
{"created":"2024-03-25 22:42:19","title":"The Role of $n$-gram Smoothing in the Age of Neural Networks","abstract":"For nearly three decades, language models derived from the $n$-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled $n$-gram models as the best performers, $n$-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens the role classical $n$-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-$\\lambda$ smoothing. Second, we derive a generalized framework for converting \\emph{any} $n$-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results find that our novel regularizers are comparable to and, indeed, sometimes outperform label smoothing on language modeling and machine translation.","sentences":["For nearly three decades, language models derived from the $n$-gram assumption held the state of the art on the task.","The key to their success lay in the application of various smoothing techniques that served to combat overfitting.","However, when neural language models toppled $n$-gram models as the best performers, $n$-gram smoothing techniques became less relevant.","Indeed, it would hardly be an understatement to suggest that the line of inquiry into $n$-gram smoothing techniques became dormant.","This paper re-opens the role classical $n$-gram smoothing techniques may play in the age of neural language models.","First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-$\\lambda$ smoothing.","Second, we derive a generalized framework for converting \\emph{any} $n$-gram smoothing technique into a regularizer compatible with neural language models.","Our empirical results find that our novel regularizers are comparable to and, indeed, sometimes outperform label smoothing on language modeling and machine translation."],"url":"http://arxiv.org/abs/2403.17240v1","category":"cs.CL"}
{"created":"2024-03-25 22:39:47","title":"Manufacturing Service Capability Prediction with Graph Neural Networks","abstract":"In the current landscape, the predominant methods for identifying manufacturing capabilities from manufacturers rely heavily on keyword matching and semantic matching. However, these methods often fall short by either overlooking valuable hidden information or misinterpreting critical data. Consequently, such approaches result in an incomplete identification of manufacturers' capabilities. This underscores the pressing need for data-driven solutions to enhance the accuracy and completeness of manufacturing capability identification. To address the need, this study proposes a Graph Neural Network-based method for manufacturing service capability identification over a knowledge graph. To enhance the identification performance, this work introduces a novel approach that involves aggregating information from the graph nodes' neighborhoods as well as oversampling the graph data, which can be effectively applied across a wide range of practical scenarios. Evaluations conducted on a Manufacturing Service Knowledge Graph and subsequent ablation studies demonstrate the efficacy and robustness of the proposed approach. This study not only contributes a innovative method for inferring manufacturing service capabilities but also significantly augments the quality of Manufacturing Service Knowledge Graphs.","sentences":["In the current landscape, the predominant methods for identifying manufacturing capabilities from manufacturers rely heavily on keyword matching and semantic matching.","However, these methods often fall short by either overlooking valuable hidden information or misinterpreting critical data.","Consequently, such approaches result in an incomplete identification of manufacturers' capabilities.","This underscores the pressing need for data-driven solutions to enhance the accuracy and completeness of manufacturing capability identification.","To address the need, this study proposes a Graph Neural Network-based method for manufacturing service capability identification over a knowledge graph.","To enhance the identification performance, this work introduces a novel approach that involves aggregating information from the graph nodes' neighborhoods as well as oversampling the graph data, which can be effectively applied across a wide range of practical scenarios.","Evaluations conducted on a Manufacturing Service Knowledge Graph and subsequent ablation studies demonstrate the efficacy and robustness of the proposed approach.","This study not only contributes a innovative method for inferring manufacturing service capabilities but also significantly augments the quality of Manufacturing Service Knowledge Graphs."],"url":"http://arxiv.org/abs/2403.17239v1","category":"cs.LG"}
{"created":"2024-03-25 22:26:09","title":"Neural Image Compression with Quantization Rectifier","abstract":"Neural image compression has been shown to outperform traditional image codecs in terms of rate-distortion performance. However, quantization introduces errors in the compression process, which can degrade the quality of the compressed image. Existing approaches address the train-test mismatch problem incurred during quantization, the random impact of quantization on the expressiveness of image features is still unsolved. This paper presents a novel quantization rectifier (QR) method for image compression that leverages image feature correlation to mitigate the impact of quantization. Our method designs a neural network architecture that predicts unquantized features from the quantized ones, preserving feature expressiveness for better image reconstruction quality. We develop a soft-to-predictive training technique to integrate QR into existing neural image codecs. In evaluation, we integrate QR into state-of-the-art neural image codecs and compare enhanced models and baselines on the widely-used Kodak benchmark. The results show consistent coding efficiency improvement by QR with a negligible increase in the running time.","sentences":["Neural image compression has been shown to outperform traditional image codecs in terms of rate-distortion performance.","However, quantization introduces errors in the compression process, which can degrade the quality of the compressed image.","Existing approaches address the train-test mismatch problem incurred during quantization, the random impact of quantization on the expressiveness of image features is still unsolved.","This paper presents a novel quantization rectifier (QR) method for image compression that leverages image feature correlation to mitigate the impact of quantization.","Our method designs a neural network architecture that predicts unquantized features from the quantized ones, preserving feature expressiveness for better image reconstruction quality.","We develop a soft-to-predictive training technique to integrate QR into existing neural image codecs.","In evaluation, we integrate QR into state-of-the-art neural image codecs and compare enhanced models and baselines on the widely-used Kodak benchmark.","The results show consistent coding efficiency improvement by QR with a negligible increase in the running time."],"url":"http://arxiv.org/abs/2403.17236v1","category":"cs.LG"}
{"created":"2024-03-25 21:31:08","title":"Kinetic theory of vacuum pair production in uniform electric fields revisited","abstract":"We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a uniform time-dependent electric field of arbitrary polarization. Taking into account the interaction with the external classical background in a nonperturbative manner, we quantize the electron-positron field and derive a system of ten quantum kinetic equations (QKEs) showing that the previously-used QKEs are incorrect once the external field rotates in space. We employ then the Wigner-function formalism of the field quantization and establish a direct connection between the Dirac-Heisenberg-Wigner (DHW) approach to investigating the vacuum pair-production process and the QKEs. We provide a self-contained description of the two theoretical frameworks rigorously proving their equivalence and present an exact one-to-one correspondence between the kinetic functions involved within the two techniques. Special focus is placed on the analysis of the spin effects in the final particle distributions.","sentences":["We investigate the phenomenon of electron-positron pair production from vacuum in the presence of a uniform time-dependent electric field of arbitrary polarization.","Taking into account the interaction with the external classical background in a nonperturbative manner, we quantize the electron-positron field and derive a system of ten quantum kinetic equations (QKEs) showing that the previously-used QKEs are incorrect once the external field rotates in space.","We employ then the Wigner-function formalism of the field quantization and establish a direct connection between the Dirac-Heisenberg-Wigner (DHW) approach to investigating the vacuum pair-production process and the QKEs.","We provide a self-contained description of the two theoretical frameworks rigorously proving their equivalence and present an exact one-to-one correspondence between the kinetic functions involved within the two techniques.","Special focus is placed on the analysis of the spin effects in the final particle distributions."],"url":"http://arxiv.org/abs/2403.17204v1","category":"hep-ph"}
{"created":"2024-03-25 21:27:38","title":"The generic temperature response of large biochemical networks","abstract":"Biological systems are remarkably susceptible to relatively small temperature changes. The most obvious example is fever, when a modest rise in body temperature of only few Kelvin has strong effects on our immune system and how it fights pathogens. Another very important example is climate change, when even smaller temperature changes lead to dramatic shifts in ecosystems. Although it is generally accepted that the main effect of an increase in temperature is the acceleration of biochemical reactions according to the Arrhenius equation, it is not clear how it effects large biochemical networks with complicated architectures. For developmental systems like fly and frog, it has been shown that the system response to temperature deviates in a characteristic manner from the linear Arrhenius plot of single reactions, but a rigorous explanation has not been given yet. Here we use a graph theoretical interpretation of the mean first passage times of a biochemical master equation to give a statistical description. We find that in the limit of large system size and if the network has a bias towards a target state, then the Arrhenius plot is generically quadratic, in excellent agreement with experimental data for developmental times in fly and frog.","sentences":["Biological systems are remarkably susceptible to relatively small temperature changes.","The most obvious example is fever, when a modest rise in body temperature of only few Kelvin has strong effects on our immune system and how it fights pathogens.","Another very important example is climate change, when even smaller temperature changes lead to dramatic shifts in ecosystems.","Although it is generally accepted that the main effect of an increase in temperature is the acceleration of biochemical reactions according to the Arrhenius equation, it is not clear how it effects large biochemical networks with complicated architectures.","For developmental systems like fly and frog, it has been shown that the system response to temperature deviates in a characteristic manner from the linear Arrhenius plot of single reactions, but a rigorous explanation has not been given yet.","Here we use a graph theoretical interpretation of the mean first passage times of a biochemical master equation to give a statistical description.","We find that in the limit of large system size and if the network has a bias towards a target state, then the Arrhenius plot is generically quadratic, in excellent agreement with experimental data for developmental times in fly and frog."],"url":"http://arxiv.org/abs/2403.17202v1","category":"physics.bio-ph"}
{"created":"2024-03-25 21:18:52","title":"Loop Quantum Gravity effects on electromagnetic properties of charged leptons","abstract":"The efforts in this contribution consist in reassessing a modified Dirac equation that incorporates a $\\gamma^0 \\gamma_5$-Lorentz-symmetry violating (LSV) term induced as a Loop Quantum Gravity (LQG) effect. Originally, this equation has been applied and considered as a good scenario for describing a number of investigations on the flight time of cosmic photons and neutrinos, which suggests that the speed of light in vacuum, in connection with the geometry that describes a granular space-time, takes an energy-dependent form, e.g., $v(E) =1 \\pm E/E_{\\tiny{\\textrm{LSV}}}$, with $E_{\\tiny{\\textrm{LSV}}} \\approx 6,5 \\times 10^{17}$ GeV for neutrinos. Once LQG provides a viable way to consistently understand this picture, we pursue an analysis of this effective Dirac equation to inspect some of its properties. These include: the derivation of the modified fermionic propagator, attainment of the Gordon decomposition of the vector current with minimal electromagnetic coupling to obtain information on the form factors, examination of the non-relativistic limit of the equation, evaluation of the spin- and velocity-dependent corrections to the Coulomb potential due to LQG effects, and the modified Hamiltonian in the low-relativistic regime. The study of the form factors may open up paths to set up bounds on the LQG parameters from the precision measurements of electromagnetic attributes of the charged leptons, such as their respective electric and magnetic dipole moments.","sentences":["The efforts in this contribution consist in reassessing a modified Dirac equation that incorporates a $\\gamma^0 \\gamma_5$-Lorentz-symmetry violating (LSV) term induced as a Loop Quantum Gravity (LQG) effect.","Originally, this equation has been applied and considered as a good scenario for describing a number of investigations on the flight time of cosmic photons and neutrinos, which suggests that the speed of light in vacuum, in connection with the geometry that describes a granular space-time, takes an energy-dependent form, e.g., $v(E) =1 \\pm E/E_{\\tiny{\\textrm{LSV}}}$, with $E_{\\tiny{\\textrm{LSV}}} \\approx 6,5 \\times 10^{17}$ GeV for neutrinos.","Once LQG provides a viable way to consistently understand this picture, we pursue an analysis of this effective Dirac equation to inspect some of its properties.","These include: the derivation of the modified fermionic propagator, attainment of the Gordon decomposition of the vector current with minimal electromagnetic coupling to obtain information on the form factors, examination of the non-relativistic limit of the equation, evaluation of the spin- and velocity-dependent corrections to the Coulomb potential due to LQG effects, and the modified Hamiltonian in the low-relativistic regime.","The study of the form factors may open up paths to set up bounds on the LQG parameters from the precision measurements of electromagnetic attributes of the charged leptons, such as their respective electric and magnetic dipole moments."],"url":"http://arxiv.org/abs/2403.17197v1","category":"hep-th"}
{"created":"2024-03-25 20:44:46","title":"Energy control in a quantum oscillator using coherent control and engineered environment","abstract":"We develop and analyze a new method for manipulation of energy in a quantum harmonic oscillator using coherent, e.g., electromagnetic, field and incoherent control. Coherent control is typically implemented by shaped laser pulse or tailored electromagnetic field. Incoherent control is implemented by engineered environment, whose mean number of excitations at the frequency of the oscillator is used as a control variable. An approach to coherent and incoherent controls design based on the speed gradient algorithms in general, finite and differential forms is proposed. It is proved that the differential form is able to completely manipulate the energy of the oscillator: an arbitrary energy can be achieved starting from any initial state of the oscillator. The key instrument which allows for complete energy manipulation in this case is the use of the engineered environment. A robustified speed-gradient control algorithm in differential form is also proposed. It is shown that the proposed robustified control algorithm ensures exponential stability of the closed loop system which is preserved for sampled-data control.","sentences":["We develop and analyze a new method for manipulation of energy in a quantum harmonic oscillator using coherent, e.g., electromagnetic, field and incoherent control.","Coherent control is typically implemented by shaped laser pulse or tailored electromagnetic field.","Incoherent control is implemented by engineered environment, whose mean number of excitations at the frequency of the oscillator is used as a control variable.","An approach to coherent and incoherent controls design based on the speed gradient algorithms in general, finite and differential forms is proposed.","It is proved that the differential form is able to completely manipulate the energy of the oscillator: an arbitrary energy can be achieved starting from any initial state of the oscillator.","The key instrument which allows for complete energy manipulation in this case is the use of the engineered environment.","A robustified speed-gradient control algorithm in differential form is also proposed.","It is shown that the proposed robustified control algorithm ensures exponential stability of the closed loop system which is preserved for sampled-data control."],"url":"http://arxiv.org/abs/2403.17178v1","category":"quant-ph"}
{"created":"2024-03-25 20:43:48","title":"Histogram Layers for Neural Engineered Features","abstract":"In the computer vision literature, many effective histogram-based features have been developed. These engineered features include local binary patterns and edge histogram descriptors among others and they have been shown to be informative features for a variety of computer vision tasks. In this paper, we explore whether these features can be learned through histogram layers embedded in a neural network and, therefore, be leveraged within deep learning frameworks. By using histogram features, local statistics of the feature maps from the convolution neural networks can be used to better represent the data. We present neural versions of local binary pattern and edge histogram descriptors that jointly improve the feature representation and perform image classification. Experiments are presented on benchmark and real-world datasets.","sentences":["In the computer vision literature, many effective histogram-based features have been developed.","These engineered features include local binary patterns and edge histogram descriptors among others and they have been shown to be informative features for a variety of computer vision tasks.","In this paper, we explore whether these features can be learned through histogram layers embedded in a neural network and, therefore, be leveraged within deep learning frameworks.","By using histogram features, local statistics of the feature maps from the convolution neural networks can be used to better represent the data.","We present neural versions of local binary pattern and edge histogram descriptors that jointly improve the feature representation and perform image classification.","Experiments are presented on benchmark and real-world datasets."],"url":"http://arxiv.org/abs/2403.17176v1","category":"cs.CV"}
{"created":"2024-03-25 20:38:55","title":"Degenerate Kirchhoff Problems with Nonlinear Neumann Boundary Condition","abstract":"In this paper we consider degenerate Kirchhoff-type equations of the form \\[-\\phi(\\Xi(u)) \\left(\\mathcal{A}(u)+|u|^{p-2}u\\right) = f(x,u)\\quad \\text{in } \\Omega,\\] \\[\\phantom{aaiaaaaaaaaa}\\phi (\\Xi(u)) \\mathcal{B}(u) \\cdot \\nu = g(x,u) \\quad \\text{on } \\partial\\Omega,\\] where $\\Omega\\subseteq \\mathbb{R}^N$, $N\\geq 2$, is a bounded domain with Lipschitz boundary $\\partial\\Omega$, $\\mathcal{A}$ denotes the double phase operator given by \\begin{align*} \\mathcal{A}(u)=\\operatorname{div} \\left(|\\nabla u|^{p-2}\\nabla u + \\mu(x) |\\nabla u|^{q-2}\\nabla u \\right)\\quad \\text{for }u\\in W^{1,\\mathcal{H}}(\\Omega), \\end{align*} $\\nu(x)$ is the outer unit normal of $\\Omega$ at $x \\in \\partial\\Omega$, \\[\\mathcal{B}(u)=|\\nabla u|^{p-2}\\nabla u + \\mu(x) |\\nabla u|^{q-2}\\nabla u,\\] \\[\\phantom{aaaiaaaa}\\Xi(u)= \\int_\\Omega \\left(\\frac{|\\nabla u|^p+|u|^p}{p}+\\mu(x) \\frac{|\\nabla u|^q}{q}\\right)\\,\\mathrm{d} x,\\] $1<p<N$, $p<q<p^*=\\frac{Np}{N-p}$, $0 \\leq \\mu(\\cdot)\\in L^\\infty(\\Omega)$, $\\phi(s) = a + b s^{\\zeta-1}$ for $s\\in\\mathbb{R}$ with $a \\geq 0$, $b>0$ and $\\zeta \\geq 1$, and $f\\colon\\Omega\\times\\mathbb{R}\\to\\mathbb{R}$, $g\\colon\\partial\\Omega\\times\\mathbb{R}\\to\\mathbb{R}$ are Carath\\'{e}odory functions that grow superlinearly and subcritically. We prove the existence of a nodal ground state solution to the problem above, based on variational methods and minimization of the associated energy functional $\\mathcal{E}\\colon W^{1,\\mathcal{H}}(\\Omega) \\to\\mathbb{R}$ over the constraint set \\[\\mathcal{C}=\\Big\\{u \\in W^{1,\\mathcal{H}}(\\Omega)\\colon u^{\\pm}\\neq 0,\\, \\left\\langle \\mathcal{E}'(u),u^+ \\right\\rangle= \\left\\langle \\mathcal{E}'(u),-u^- \\right\\rangle=0 \\Big\\},\\] whereby $\\mathcal{C}$ differs from the well-known nodal Nehari manifold due to the nonlocal character of the problem.","sentences":["In this paper we consider degenerate Kirchhoff-type equations of the form \\[-\\phi(\\Xi(u))","\\left(\\mathcal{A}(u)+|u|^{p-2}u\\right)","= f(x,u)\\quad \\text{in } \\Omega,\\] \\[\\phantom{aaiaaaaaaaaa}\\phi (\\Xi(u)) \\mathcal{B}(u) \\cdot \\nu = g(x,u) \\quad \\text{on } \\partial\\Omega,\\] where $\\Omega\\subseteq \\mathbb{R}^N$, $N\\geq 2$, is a bounded domain with Lipschitz boundary $\\partial\\Omega$, $\\mathcal{A}$ denotes the double phase operator given by \\begin{align*} \\mathcal{A}(u)=\\operatorname{div} \\left(|\\nabla u|^{p-2}\\nabla u + \\mu(x) |\\nabla u|^{q-2}\\nabla u \\right)\\quad \\text{for }u\\in W^{1,\\mathcal{H}}(\\Omega), \\end{align*} $\\nu(x)$ is the outer unit normal of $\\Omega$ at $x \\in \\partial\\Omega$, \\[\\mathcal{B}(u)=|\\nabla u|^{p-2}\\nabla u + \\mu(x) |\\nabla u|^{q-2}\\nabla u,\\] \\[\\phantom{aaaiaaaa}\\Xi(u)= \\int_\\Omega \\left(\\frac{|\\nabla u|^p+|u|^p}{p}+\\mu(x) \\frac{|\\nabla","u|^q}{q}\\right)\\,\\mathrm{d} x,\\] $1<p<N$, $p<q<p^*=\\frac{Np}{N-p}$, $0 \\leq \\mu(\\cdot)\\in L^\\infty(\\Omega)$, $\\phi(s) = a + b s^{\\zeta-1}$ for $s\\in\\mathbb{R}$ with $a \\geq 0$, $b>0$ and $\\zeta \\geq 1$, and $f\\colon\\Omega\\times\\mathbb{R}\\to\\mathbb{R}$, $g\\colon\\partial\\Omega\\times\\mathbb{R}\\to\\mathbb{R}$ are Carath\\'{e}odory functions that grow superlinearly and subcritically.","We prove the existence of a nodal ground state solution to the problem above, based on variational methods and minimization of the associated energy functional $\\mathcal{E}\\colon W^{1,\\mathcal{H}}(\\Omega) \\to\\mathbb{R}$ over the constraint set \\[\\mathcal{C}=\\Big\\{u \\in W^{1,\\mathcal{H}}(\\Omega)\\colon u^{\\pm}\\neq 0,\\, \\left\\langle \\mathcal{E}'(u),u^+ \\right\\rangle= \\left\\langle \\mathcal{E}'(u),-u^- \\right\\rangle=0 \\Big\\},\\] whereby $\\mathcal{C}$ differs from the well-known nodal Nehari manifold due to the nonlocal character of the problem."],"url":"http://arxiv.org/abs/2403.17172v1","category":"math.AP"}
{"created":"2024-03-25 20:36:25","title":"Approximations of Functions With Essential Singularities with Applications to Painlev\u00e9's First Transcendent","abstract":"In this work we develop an algorithmic procedure for associating a function defined on the Riemann surface of the $\\log$ to given asymptotic data from a function at an essential singularity. We do this by means of rational approximations (Pad\\'e approximants) used in tandem with Borel-\\'Ecalle summation. Our method is capable of handling situations where classical methods either do not work or converge very slowly eg. \\cite{DunLutz}. We provide a general outline of the procedure and then apply it to generating approximate tritronqu\\'ee solutions to Painlev\\'e's first equation ($\\text{P}_\\text{I}$). Our approximations (including $\\text{P}_\\text{I}$) are written as a finite linear combination of exponential integrals $\\text{Ei}^+$. Furthermore, from arXiv:2210.17502 we have explicit rational approximations for each $\\text{Ei}^+$ and thus for the approximation as a whole. In addition to rational approximations of $\\text{P}_\\text{I}$, we provide the first hundred or so poles of a tritronqu\\'ee solution with essentially arbitrary accuracy which is dependent upon the order of Pad\\'e used.","sentences":["In this work we develop an algorithmic procedure for associating a function defined on the Riemann surface of the $\\log$ to given asymptotic data from a function at an essential singularity.","We do this by means of rational approximations (Pad\\'e approximants) used in tandem with Borel-\\'Ecalle summation.","Our method is capable of handling situations where classical methods either do not work or converge very slowly eg. \\cite{DunLutz}.","We provide a general outline of the procedure and then apply it to generating approximate tritronqu\\'ee solutions to Painlev\\'e's first equation ($\\text{P}_\\text{I}$).","Our approximations (including $\\text{P}_\\text{I}$) are written as a finite linear combination of exponential integrals $\\text{Ei}^+$. Furthermore, from arXiv:2210.17502 we have explicit rational approximations for each $\\text{Ei}^+$ and thus for the approximation as a whole.","In addition to rational approximations of $\\text{P}_\\text{I}$, we provide the first hundred or so poles of a tritronqu\\'ee solution with essentially arbitrary accuracy which is dependent upon the order of Pad\\'e used."],"url":"http://arxiv.org/abs/2403.17170v1","category":"math.CV"}
{"created":"2024-03-25 20:24:30","title":"Applicability of mean-field theory for time-dependent open quantum systems with infinite-range interactions","abstract":"Understanding quantum many-body systems with long-range or infinite-range interactions is of relevance across a broad set of physical disciplines, including quantum optics, nuclear magnetic resonance and nuclear physics. From a theoretical viewpoint, these systems are appealing since they can be efficiently studied with numerics, and in the thermodynamic limit are expected to be governed by mean-field equations of motion. Over the past years the capabilities to experimentally create long-range interacting systems have dramatically improved permitting their control in space and time. This allows to induce and explore a plethora of nonequilibrium dynamical phases, including time-crystals and even chaotic regimes. However, establishing the emergence of these phases from numerical simulations turns out to be surprisingly challenging. This difficulty led to the assertion that mean-field theory may not be applicable to time-dependent infinite-range interacting systems. Here, we rigorously prove that mean-field theory in fact exactly captures their dynamics, in the thermodynamic limit. We further provide bounds for finite-size effects and their dependence on the evolution time.","sentences":["Understanding quantum many-body systems with long-range or infinite-range interactions is of relevance across a broad set of physical disciplines, including quantum optics, nuclear magnetic resonance and nuclear physics.","From a theoretical viewpoint, these systems are appealing since they can be efficiently studied with numerics, and in the thermodynamic limit are expected to be governed by mean-field equations of motion.","Over the past years the capabilities to experimentally create long-range interacting systems have dramatically improved permitting their control in space and time.","This allows to induce and explore a plethora of nonequilibrium dynamical phases, including time-crystals and even chaotic regimes.","However, establishing the emergence of these phases from numerical simulations turns out to be surprisingly challenging.","This difficulty led to the assertion that mean-field theory may not be applicable to time-dependent infinite-range interacting systems.","Here, we rigorously prove that mean-field theory in fact exactly captures their dynamics, in the thermodynamic limit.","We further provide bounds for finite-size effects and their dependence on the evolution time."],"url":"http://arxiv.org/abs/2403.17163v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-25 20:16:05","title":"Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG Direct Policy Optimization","abstract":"In this paper, we consider direct policy optimization for the linear-quadratic Gaussian (LQG) setting. Over the past few years, it has been recognized that the landscape of stabilizing output-feedback controllers of relevance to LQG has an intricate geometry, particularly as it pertains to the existence of spurious stationary points. In order to address such challenges, in this paper, we first adopt a Riemannian metric for the space of stabilizing full-order minimal output-feedback controllers. We then proceed to prove that the orbit of such controllers modulo coordinate transformation admits a Riemannian quotient manifold structure. This geometric structure is then used to develop a Riemannian gradient descent for the direct LQG policy optimization. We prove a local convergence guarantee with linear rate and show the proposed approach exhibits significantly faster and more robust numerical performance as compared with ordinary gradient descent for LQG. Subsequently, we provide reasons for this observed behavior; in particular, we argue that optimizing over the orbit space of controllers is the right theoretical and computational setup for direct LQG policy optimization.","sentences":["In this paper, we consider direct policy optimization for the linear-quadratic Gaussian (LQG) setting.","Over the past few years, it has been recognized that the landscape of stabilizing output-feedback controllers of relevance to LQG has an intricate geometry, particularly as it pertains to the existence of spurious stationary points.","In order to address such challenges, in this paper, we first adopt a Riemannian metric for the space of stabilizing full-order minimal output-feedback controllers.","We then proceed to prove that the orbit of such controllers modulo coordinate transformation admits a Riemannian quotient manifold structure.","This geometric structure is then used to develop a Riemannian gradient descent for the direct LQG policy optimization.","We prove a local convergence guarantee with linear rate and show the proposed approach exhibits significantly faster and more robust numerical performance as compared with ordinary gradient descent for LQG.","Subsequently, we provide reasons for this observed behavior; in particular, we argue that optimizing over the orbit space of controllers is the right theoretical and computational setup for direct LQG policy optimization."],"url":"http://arxiv.org/abs/2403.17157v1","category":"math.OC"}
{"created":"2024-03-25 19:56:12","title":"A Frobenius integrability theorem for plane fields generated by quasiconformal deformations","abstract":"We generalize the classical Frobenius integrability theorem to plane fields of class $C^Q$, a regularity class introduced by Reimann [Rei76] for vector fields in Euclidean spaces. A $C^Q$ vector field is uniquely integrable and its flow is a quasiconformal deformation. We show that an a.e. involutive $C^Q$ plane field (defined in a suitable way) in $\\R^n$ is integrable, with integral manifolds of class $C^1$.","sentences":["We generalize the classical Frobenius integrability theorem to plane fields of class $C^Q$, a regularity class introduced by Reimann [Rei76] for vector fields in Euclidean spaces.","A $C^Q$ vector field is uniquely integrable and its flow is a quasiconformal deformation.","We show that an a.e. involutive $C^Q$ plane field (defined in a suitable way) in $\\R^n$ is integrable, with integral manifolds of class $C^1$."],"url":"http://arxiv.org/abs/2403.17150v1","category":"math.DG"}
{"created":"2024-03-25 19:55:03","title":"Symplectic quantization and the Feynman propagator: a new real-time numerical approach to lattice field theory","abstract":"We present here the first numerical test of symplectic quantization, a new approach to quantum field theory. Symplectic quantization is characterized by the possibility to sample quantum fluctuations of relativistic fields by means of a deterministic dynamics generated by Hamilton-like equations, the latter evolving with respect to an additional time parameter $\\tau$. In the present work we study numerically the symplectic quantization dynamics for a real scalar field in 1+1 space-time dimensions and with $\\lambda \\phi^4$ non-linear interaction. We find that for $\\lambda \\ll 1$ the Fourier spectrum of the two-point correlation function obtained numerically reproduces qualitatively well the shape of the Feynman propagator. Within symplectic quantization the expectation over quantum fluctuations is computed as a dynamical average along the trajectories parametrized by the intrinsic time $\\tau$. As a numerical strategy to study quantum fluctuations of fields directly in Lorentzian space-time, we believe that symplectic quantization will be of key importance for the study of non-equilibrium relaxational dynamics in quantum field theory and the phenomenology of metastable bound states with very short life-time, something usually not accessible by numerical methods based on Euclidean field theory.","sentences":["We present here the first numerical test of symplectic quantization, a new approach to quantum field theory.","Symplectic quantization is characterized by the possibility to sample quantum fluctuations of relativistic fields by means of a deterministic dynamics generated by Hamilton-like equations, the latter evolving with respect to an additional time parameter $\\tau$. In the present work we study numerically the symplectic quantization dynamics for a real scalar field in 1+1 space-time dimensions and with $\\lambda \\phi^4$ non-linear interaction.","We find that for $\\lambda \\ll 1$ the Fourier spectrum of the two-point correlation function obtained numerically reproduces qualitatively well the shape of the Feynman propagator.","Within symplectic quantization the expectation over quantum fluctuations is computed as a dynamical average along the trajectories parametrized by the intrinsic time $\\tau$. As a numerical strategy to study quantum fluctuations of fields directly in Lorentzian space-time, we believe that symplectic quantization will be of key importance for the study of non-equilibrium relaxational dynamics in quantum field theory and the phenomenology of metastable bound states with very short life-time, something usually not accessible by numerical methods based on Euclidean field theory."],"url":"http://arxiv.org/abs/2403.17149v1","category":"hep-lat"}
{"created":"2024-03-25 19:22:47","title":"Superlattice induced electron percolation within a single Landau level","abstract":"We investigate the quantum Hall effect in a single Landau level in the presence of a square superlattice of $\\delta$-function potentials. The interplay between the superlattice spacing $a_s$ and the magnetic length $\\ell_B$ in clean system leads to three interesting characteristic regimes corresponding to $a_s \\lt \\ell_B$, $a_s \\gg \\ell_B$ and the intermediate one where $a_s \\sim \\ell_B$ . In the intermediate regime, the continuous magnetic translation symmetry breaks down to discrete lattice symmetry. In contrast, we show that in the other two regimes, the same is hardly broken in the topological band despite the presence of the superlattice. In the presence of weak disorder (white-noise) one typically expects a tiny fraction of extended states due to topological protection of the Landau level. Interestingly, we obtain a large fraction of extended states throughout the intermediate regime which maximizes at the special point $a_s = \\sqrt{2\\pi} \\ell_B$. We argue the superlattice induced percolation phenomenon requires both the breaking of the time reversal symmetry and the continuous magnetic translational symmetry. It could have a direct implication on the integer plateau transitions in both continuous quantum Hall systems and the lattice based anomalous quantum Hall effect.","sentences":["We investigate the quantum Hall effect in a single Landau level in the presence of a square superlattice of $\\delta$-function potentials.","The interplay between the superlattice spacing $a_s$ and the magnetic length $\\ell_B$ in clean system leads to three interesting characteristic regimes corresponding to $a_s \\lt \\ell_B$, $a_s \\gg \\ell_B$ and the intermediate one where $a_s \\sim \\ell_B$ .","In the intermediate regime, the continuous magnetic translation symmetry breaks down to discrete lattice symmetry.","In contrast, we show that in the other two regimes, the same is hardly broken in the topological band despite the presence of the superlattice.","In the presence of weak disorder (white-noise) one typically expects a tiny fraction of extended states due to topological protection of the Landau level.","Interestingly, we obtain a large fraction of extended states throughout the intermediate regime which maximizes at the special point $a_s = \\sqrt{2\\pi} \\ell_B$. We argue the superlattice induced percolation phenomenon requires both the breaking of the time reversal symmetry and the continuous magnetic translational symmetry.","It could have a direct implication on the integer plateau transitions in both continuous quantum Hall systems and the lattice based anomalous quantum Hall effect."],"url":"http://arxiv.org/abs/2403.17137v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-25 19:16:11","title":"Deep learning-based predictive modelling of transonic flow over an aerofoil","abstract":"Effectively predicting transonic unsteady flow over an aerofoil poses inherent challenges. In this study, we harness the power of deep neural network (DNN) models using the attention U-Net architecture. Through efficient training of these models, we achieve the capability to capture the complexities of transonic and unsteady flow dynamics at high resolution, even when faced with previously unseen conditions. We demonstrate that by leveraging the differentiability inherent in neural network representations, our approach provides a framework for assessing fundamental physical properties via global instability analysis. This integration bridges deep neural network models and traditional modal analysis, offering valuable insights into transonic flow dynamics and enhancing the interpretability of neural network models in flowfield diagnostics.","sentences":["Effectively predicting transonic unsteady flow over an aerofoil poses inherent challenges.","In this study, we harness the power of deep neural network (DNN) models using the attention U-Net architecture.","Through efficient training of these models, we achieve the capability to capture the complexities of transonic and unsteady flow dynamics at high resolution, even when faced with previously unseen conditions.","We demonstrate that by leveraging the differentiability inherent in neural network representations, our approach provides a framework for assessing fundamental physical properties via global instability analysis.","This integration bridges deep neural network models and traditional modal analysis, offering valuable insights into transonic flow dynamics and enhancing the interpretability of neural network models in flowfield diagnostics."],"url":"http://arxiv.org/abs/2403.17131v1","category":"physics.flu-dyn"}
{"created":"2024-03-25 19:04:53","title":"A high-order explicit Runge-Kutta approximation technique for the Shallow Water Equations","abstract":"We introduce a high-order space-time approximation of the Shallow Water Equations with sources that is invariant-domain preserving (IDP) and well-balanced with respect to rest states. The employed time-stepping technique is a novel explicit Runge-Kutta (ERK) approach which is an extension of the class of ERK-IDP methods introduced by Ern and Guermond (SIAM J. Sci. Comput. 44(5), A3366--A3392, 2022) for systems of non-linear conservation equations. The resulting method is then numerically illustrated through verification and validation.","sentences":["We introduce a high-order space-time approximation of the Shallow Water Equations with sources that is invariant-domain preserving (IDP) and well-balanced with respect to rest states.","The employed time-stepping technique is a novel explicit Runge-Kutta (ERK) approach which is an extension of the class of ERK-IDP methods introduced by Ern and Guermond (SIAM J. Sci.","Comput.","44(5), A3366--A3392, 2022) for systems of non-linear conservation equations.","The resulting method is then numerically illustrated through verification and validation."],"url":"http://arxiv.org/abs/2403.17123v1","category":"math.NA"}
{"created":"2024-03-25 18:54:34","title":"Tidal deformations of slowly spinning isolated horizons","abstract":"It is generally believed that tidal deformations of a black hole in an external field, as measured using its gravitational field multipoles, vanish. However, this does not mean that the black hole horizon is not deformed. Here we shall discuss the deformations of a black hole horizon in the presence of an external field using a characteristic initial value formulation. Unlike existing methods, the starting point here is the black hole horizon itself. The effect of, say, a binary companion responsible for the tidal deformation is encoded in the geometry of the spacetime in the vicinity of the horizon. The near horizon spacetime geometry, i.e. the metric, spin coefficients, and curvature components, are all obtained by integrating the Einstein field equations outwards starting from the horizon. This method yields a reformulation of black hole perturbation theory in a neighborhood of the horizon. By specializing the horizon geometry to be a perturbation of Kerr, this method can be used to calculate the metric for a tidally deformed Kerr black hole with arbitrary spin. As a first application, we apply this formulation here to a slowly spinning black hole and explicitly construct the spacetime metric in a neighborhood of the horizon. We propose natural definitions of the electric and magnetic surficial Love numbers based on the Weyl tensor component $\\Psi_2$. From our solution, we calculate the tidal perturbations of the black hole, and we extract both the field Love numbers and the surficial Love numbers which quantify the deformations of the horizon.","sentences":["It is generally believed that tidal deformations of a black hole in an external field, as measured using its gravitational field multipoles, vanish.","However, this does not mean that the black hole horizon is not deformed.","Here we shall discuss the deformations of a black hole horizon in the presence of an external field using a characteristic initial value formulation.","Unlike existing methods, the starting point here is the black hole horizon itself.","The effect of, say, a binary companion responsible for the tidal deformation is encoded in the geometry of the spacetime in the vicinity of the horizon.","The near horizon spacetime geometry, i.e. the metric, spin coefficients, and curvature components, are all obtained by integrating the Einstein field equations outwards starting from the horizon.","This method yields a reformulation of black hole perturbation theory in a neighborhood of the horizon.","By specializing the horizon geometry to be a perturbation of Kerr, this method can be used to calculate the metric for a tidally deformed Kerr black hole with arbitrary spin.","As a first application, we apply this formulation here to a slowly spinning black hole and explicitly construct the spacetime metric in a neighborhood of the horizon.","We propose natural definitions of the electric and magnetic surficial Love numbers based on the Weyl tensor component $\\Psi_2$. From our solution, we calculate the tidal perturbations of the black hole, and we extract both the field Love numbers and the surficial Love numbers which quantify the deformations of the horizon."],"url":"http://arxiv.org/abs/2403.17114v1","category":"gr-qc"}
{"created":"2024-03-25 18:52:26","title":"The Impact of Pradhan Mantri Ujjwala Yojana on Indian Households","abstract":"This study critically evaluates the impact of the Pradhan Mantri Ujjwala Yojana (PMUY) on LPG accessibility among poor households in India. Using Propensity Score Matching and Difference-in-Differences estimators and the National Family Health Survey (NFHS) dataset, the Average Treatment Effect on the interdedly Treated is a modest 2.1 percentage point increase in LPG consumption due to PMUY, with a parallel decrease in firewood consumption. Regional analysis reveals differential impacts, with significant progress in the North, West, and South but less pronounced effects in the East and North East. The study also underscores variance across social groups, with Schedule Caste households showing the most substantial benefits, while Scheduled Tribes households are hardly affected. Despite the PMUY's initial success in facilitating LPG access, sustaining its usage remains challenging. Policy should emphasise targeted interventions, income support, and address regional and community-specific disparities for the sustained usage of LPG.","sentences":["This study critically evaluates the impact of the Pradhan Mantri Ujjwala Yojana (PMUY) on LPG accessibility among poor households in India.","Using Propensity Score Matching and Difference-in-Differences estimators and the National Family Health Survey (NFHS) dataset, the Average Treatment Effect on the interdedly Treated is a modest 2.1 percentage point increase in LPG consumption due to PMUY, with a parallel decrease in firewood consumption.","Regional analysis reveals differential impacts, with significant progress in the North, West, and South but less pronounced effects in the East and North East.","The study also underscores variance across social groups, with Schedule Caste households showing the most substantial benefits, while Scheduled Tribes households are hardly affected.","Despite the PMUY's initial success in facilitating LPG access, sustaining its usage remains challenging.","Policy should emphasise targeted interventions, income support, and address regional and community-specific disparities for the sustained usage of LPG."],"url":"http://arxiv.org/abs/2403.17112v1","category":"econ.GN"}
{"created":"2024-03-25 18:28:48","title":"A Unified CPU-GPU Protocol for GNN Training","abstract":"Training a Graph Neural Network (GNN) model on large-scale graphs involves a high volume of data communication and compu- tations. While state-of-the-art CPUs and GPUs feature high computing power, the Standard GNN training protocol adopted in existing GNN frameworks cannot efficiently utilize the platform resources. To this end, we propose a novel Unified CPU-GPU protocol that can improve the resource utilization of GNN training on a CPU-GPU platform. The Unified CPU-GPU protocol instantiates multiple GNN training processes in parallel on both the CPU and the GPU. By allocating training processes on the CPU to perform GNN training collaboratively with the GPU, the proposed protocol improves the platform resource utilization and reduces the CPU-GPU data transfer overhead. Since the performance of a CPU and a GPU varies, we develop a novel load balancer that balances the workload dynamically between CPUs and GPUs during runtime. We evaluate our protocol using two representative GNN sampling algorithms, with two widely-used GNN models, on three datasets. Compared with the standard training protocol adopted in the state-of-the-art GNN frameworks, our protocol effectively improves resource utilization and overall training time. On a platform where the GPU moderately outperforms the CPU, our protocol speeds up GNN training by up to 1.41x. On a platform where the GPU significantly outperforms the CPU, our protocol speeds up GNN training by up to 1.26x. Our protocol is open-sourced and can be seamlessly integrated into state-of-the-art GNN frameworks and accelerate GNN training. Our protocol particularly benefits those with limited GPU access due to its high demand.","sentences":["Training a Graph Neural Network (GNN) model on large-scale graphs involves a high volume of data communication and compu- tations.","While state-of-the-art CPUs and GPUs feature high computing power, the Standard GNN training protocol adopted in existing GNN frameworks cannot efficiently utilize the platform resources.","To this end, we propose a novel Unified CPU-GPU protocol that can improve the resource utilization of GNN training on a CPU-GPU platform.","The Unified CPU-GPU protocol instantiates multiple GNN training processes in parallel on both the CPU and the GPU.","By allocating training processes on the CPU to perform GNN training collaboratively with the GPU, the proposed protocol improves the platform resource utilization and reduces the CPU-GPU data transfer overhead.","Since the performance of a CPU and a GPU varies, we develop a novel load balancer that balances the workload dynamically between CPUs and GPUs during runtime.","We evaluate our protocol using two representative GNN sampling algorithms, with two widely-used GNN models, on three datasets.","Compared with the standard training protocol adopted in the state-of-the-art GNN frameworks, our protocol effectively improves resource utilization and overall training time.","On a platform where the GPU moderately outperforms the CPU, our protocol speeds up GNN training by up to 1.41x.","On a platform where the GPU significantly outperforms the CPU, our protocol speeds up GNN training by up to 1.26x.","Our protocol is open-sourced and can be seamlessly integrated into state-of-the-art GNN frameworks and accelerate GNN training.","Our protocol particularly benefits those with limited GPU access due to its high demand."],"url":"http://arxiv.org/abs/2403.17092v1","category":"cs.DC"}
{"created":"2024-03-25 18:04:48","title":"On the solutions of the local Zamolodchikov tetrahedron equation","abstract":"We study the solutions of the local Zamolodhcikov tetrahedron equation in the form of correspondences derived by $3\\times 3$ matrices. We present all the associated generators of 4-simplex maps satisfying the local tetrahedron equation. Moreover, we demonstrate that, from some of our solutions, we can recover the 4-simplex extensions of Kashaev--Korepanov--Sergeev and Hirota type tetrahedron maps. Finally, we construct several novel 4-simplex maps.","sentences":["We study the solutions of the local Zamolodhcikov tetrahedron equation in the form of correspondences derived by $3\\times 3$ matrices.","We present all the associated generators of 4-simplex maps satisfying the local tetrahedron equation.","Moreover, we demonstrate that, from some of our solutions, we can recover the 4-simplex extensions of Kashaev--Korepanov--Sergeev and Hirota type tetrahedron maps.","Finally, we construct several novel 4-simplex maps."],"url":"http://arxiv.org/abs/2403.17070v1","category":"nlin.SI"}
{"created":"2024-03-25 18:02:28","title":"Chain rule symmetry for singular SPDEs","abstract":"We characterise the chain rule symmetry for the geometric stochastic heat equations in the full subcritical regime for Gaussian and non-Gaussian noises. We show that the renormalised counter-terms that give a solution invariant under changes of coordinates are generated by iterations of covariant derivatives. The result was known only for space-time white noises, with a very specific proof that so far could not be extended to the general case. The key idea of the present paper is to change the perspective on several levels and to use ideas coming from operad theory and homological algebra. Concretely, we introduce the operad of Christoffel trees that captures the counter-terms of the renormalised equation; our main new insight is to describe the space of invariant terms homologically, using a suitable perturbation of the differential of the operadic twisting of that operad. As a consequence, we obtain the correct renormalisation for the quasi-linear KPZ equation in the subcritical regime completing the programme started by Hairer and Gerencser. Previously, the main algebraic tool used in the study of singular SPDEs were Hopf algebras of decorated trees; our work shows that operad theory and homological algebra add new powerful tools with immediate applications to open problems that were out of reach by other methods.","sentences":["We characterise the chain rule symmetry for the geometric stochastic heat equations in the full subcritical regime for Gaussian and non-Gaussian noises.","We show that the renormalised counter-terms that give a solution invariant under changes of coordinates are generated by iterations of covariant derivatives.","The result was known only for space-time white noises, with a very specific proof that so far could not be extended to the general case.","The key idea of the present paper is to change the perspective on several levels and to use ideas coming from operad theory and homological algebra.","Concretely, we introduce the operad of Christoffel trees that captures the counter-terms of the renormalised equation; our main new insight is to describe the space of invariant terms homologically, using a suitable perturbation of the differential of the operadic twisting of that operad.","As a consequence, we obtain the correct renormalisation for the quasi-linear KPZ equation in the subcritical regime completing the programme started by Hairer and Gerencser.","Previously, the main algebraic tool used in the study of singular SPDEs were Hopf algebras of decorated trees; our work shows that operad theory and homological algebra add new powerful tools with immediate applications to open problems that were out of reach by other methods."],"url":"http://arxiv.org/abs/2403.17066v1","category":"math.PR"}
{"created":"2024-03-25 17:59:35","title":"SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer","abstract":"Diffusion Transformer (DiT) has emerged as the new trend of generative diffusion models on image generation. In view of extremely slow convergence in typical DiT, recent breakthroughs have been driven by mask strategy that significantly improves the training efficiency of DiT with additional intra-image contextual learning. Despite this progress, mask strategy still suffers from two inherent limitations: (a) training-inference discrepancy and (b) fuzzy relations between mask reconstruction & generative diffusion process, resulting in sub-optimal training of DiT. In this work, we address these limitations by novelly unleashing the self-supervised discrimination knowledge to boost DiT training. Technically, we frame our DiT in a teacher-student manner. The teacher-student discriminative pairs are built on the diffusion noises along the same Probability Flow Ordinary Differential Equation (PF-ODE). Instead of applying mask reconstruction loss over both DiT encoder and decoder, we decouple DiT encoder and decoder to separately tackle discriminative and generative objectives. In particular, by encoding discriminative pairs with student and teacher DiT encoders, a new discriminative loss is designed to encourage the inter-image alignment in the self-supervised embedding space. After that, student samples are fed into student DiT decoder to perform the typical generative diffusion task. Extensive experiments are conducted on ImageNet dataset, and our method achieves a competitive balance between training cost and generative capacity.","sentences":["Diffusion Transformer (DiT) has emerged as the new trend of generative diffusion models on image generation.","In view of extremely slow convergence in typical DiT, recent breakthroughs have been driven by mask strategy that significantly improves the training efficiency of DiT with additional intra-image contextual learning.","Despite this progress, mask strategy still suffers from two inherent limitations: (a) training-inference discrepancy and (b) fuzzy relations between mask reconstruction & generative diffusion process, resulting in sub-optimal training of DiT.","In this work, we address these limitations by novelly unleashing the self-supervised discrimination knowledge to boost DiT training.","Technically, we frame our DiT in a teacher-student manner.","The teacher-student discriminative pairs are built on the diffusion noises along the same Probability Flow Ordinary Differential Equation (PF-ODE).","Instead of applying mask reconstruction loss over both DiT encoder and decoder, we decouple DiT encoder and decoder to separately tackle discriminative and generative objectives.","In particular, by encoding discriminative pairs with student and teacher DiT encoders, a new discriminative loss is designed to encourage the inter-image alignment in the self-supervised embedding space.","After that, student samples are fed into student DiT decoder to perform the typical generative diffusion task.","Extensive experiments are conducted on ImageNet dataset, and our method achieves a competitive balance between training cost and generative capacity."],"url":"http://arxiv.org/abs/2403.17004v1","category":"cs.CV"}
{"created":"2024-03-25 17:59:32","title":"Addressing the Band Gap Problem with a Machine-Learned Exchange Functional","abstract":"The systematic underestimation of band gaps is one of the most fundamental challenges in semilocal density functional theory (DFT). In addition to hindering the application of DFT to predicting electronic properties, the band gap problem is intimately related to self-interaction and delocalization errors, which make the study of charge transfer mechanisms with DFT difficult. In this work, we present two key innovations to address the band gap problem. First, we design an approach for machine learning density functionals based on Gaussian processes to explicitly fit single-particle energy levels. Second, we introduce novel nonlocal features of the density matrix that are expressive enough to fit these single-particle levels. Combining these developments, we train a machine-learned functional for the exact exchange energy that predicts molecular energy gaps and reaction energies of a wide range of molecules in excellent agreement with reference hybrid DFT calculations. In addition, while being trained solely on molecular data, our model predicts reasonable formation energies of polarons in solids, showcasing its transferability and robustness. Our approach generalizes straightforwardly to full exchange-correlation functionals, thus paving the way to the design of novel state-of-the-art functionals for the prediction of electronic properties of molecules and materials.","sentences":["The systematic underestimation of band gaps is one of the most fundamental challenges in semilocal density functional theory (DFT).","In addition to hindering the application of DFT to predicting electronic properties, the band gap problem is intimately related to self-interaction and delocalization errors, which make the study of charge transfer mechanisms with DFT difficult.","In this work, we present two key innovations to address the band gap problem.","First, we design an approach for machine learning density functionals based on Gaussian processes to explicitly fit single-particle energy levels.","Second, we introduce novel nonlocal features of the density matrix that are expressive enough to fit these single-particle levels.","Combining these developments, we train a machine-learned functional for the exact exchange energy that predicts molecular energy gaps and reaction energies of a wide range of molecules in excellent agreement with reference hybrid DFT calculations.","In addition, while being trained solely on molecular data, our model predicts reasonable formation energies of polarons in solids, showcasing its transferability and robustness.","Our approach generalizes straightforwardly to full exchange-correlation functionals, thus paving the way to the design of novel state-of-the-art functionals for the prediction of electronic properties of molecules and materials."],"url":"http://arxiv.org/abs/2403.17002v2","category":"physics.chem-ph"}
{"created":"2024-03-25 17:59:31","title":"VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation","abstract":"Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at https://vp3d-cvpr24.github.io.","sentences":["Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models.","However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues.","In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation.","Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance.","Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt.","Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures.","It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation.","Our project page is available at https://vp3d-cvpr24.github.io."],"url":"http://arxiv.org/abs/2403.17001v1","category":"cs.CV"}
{"created":"2024-03-25 17:49:38","title":"Multiple normalized solutions to a system of nonlinear Schr\u00f6dinger equations","abstract":"We find a normalized solution $u=(u_1,\\ldots,u_K)$ to the system of $K$ coupled nonlinear Schr\\\"odinger equations \\begin{equation*}   \\left\\{ \\begin{array}{l}   -\\Delta u_i+ \\lambda_i u_i = \\sum_{j=1}^K\\beta_{i,j}u_i|u_i|^{p/2-2}|u_j|^{p/2} \\quad \\mathrm{in} \\, \\mathbb{R}^3,\\newline   u_i \\in H^1_{rad}(\\mathbb{R}^3),\\newline   \\int_{\\mathbb{R}^3} |u_i|^2 \\, dx = \\rho_i^2 \\quad \\text{for }i=1,\\ldots, K,   \\end{array} \\right. \\end{equation*} where $\\rho=(\\rho_1,\\ldots,\\rho_K)\\in(0,\\infty)^K$ is prescribed, $(\\lambda,u) \\in \\mathbb{R}^K\\times H^1(\\mathbb{R}^3)^K$ are the unknown and $4\\leq p<6$. In the case of two equations we show the existence of multiple solutions provided that the coupling is sufficiently large. We also show that for negative coupling there are no ground state solutions. The main novelty in our approach is that we use the Cwikel-Lieb-Rozenblum theorem in order to estimate the Morse index of a solution as well as a Liouville-type result in an exterior domain.","sentences":["We find a normalized solution $u=(u_1,\\ldots,u_K)$ to the system of $K$ coupled nonlinear Schr\\\"odinger equations \\begin{equation*}   \\left\\{ \\begin{array}{l}   -\\Delta","u_i+ \\lambda_i u_i = \\sum_{j=1}^K\\beta_{i,j}u_i|u_i|^{p/2-2}|u_j|^{p/2} \\quad \\mathrm{in} \\, \\mathbb{R}^3,\\newline   u_i","\\in H^1_{rad}(\\mathbb{R}^3),\\newline   \\int_{\\mathbb{R}^3} |u_i|^2 \\, dx = \\rho_i^2 \\quad \\text{for }i=1,\\ldots, K,   \\end{array} \\right.","\\end{equation*} where $\\rho=(\\rho_1,\\ldots,\\rho_K)\\in(0,\\infty)^K$ is prescribed, $(\\lambda,u) \\in \\mathbb{R}^K\\times H^1(\\mathbb{R}^3)^K$ are the unknown and $4\\leq p<6$.","In the case of two equations we show the existence of multiple solutions provided that the coupling is sufficiently large.","We also show that for negative coupling there are no ground state solutions.","The main novelty in our approach is that we use the Cwikel-Lieb-Rozenblum theorem in order to estimate the Morse index of a solution as well as a Liouville-type result in an exterior domain."],"url":"http://arxiv.org/abs/2403.16987v1","category":"math.AP"}
{"created":"2024-03-25 17:40:32","title":"Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy","abstract":"The use of fluorescent molecules to create long sequences of low-density, diffraction-limited images enables highly-precise molecule localization. However, this methodology requires lengthy imaging times, which limits the ability to view dynamic interactions of live cells on short time scales. Many techniques have been developed to reduce the number of frames needed for localization, from classic iterative optimization to deep neural networks. Particularly, deep algorithm unrolling utilizes both the structure of iterative sparse recovery algorithms and the performance gains of supervised deep learning. However, the robustness of this approach is highly dependant on having sufficient training data. In this paper we introduce deep unrolled self-supervised learning, which alleviates the need for such data by training a sequence-specific, model-based autoencoder that learns only from given measurements. Our proposed method exceeds the performance of its supervised counterparts, thus allowing for robust, dynamic imaging well below the diffraction limit without any labeled training samples. Furthermore, the suggested model-based autoencoder scheme can be utilized to enhance generalization in any sparse recovery framework, without the need for external training data.","sentences":["The use of fluorescent molecules to create long sequences of low-density, diffraction-limited images enables highly-precise molecule localization.","However, this methodology requires lengthy imaging times, which limits the ability to view dynamic interactions of live cells on short time scales.","Many techniques have been developed to reduce the number of frames needed for localization, from classic iterative optimization to deep neural networks.","Particularly, deep algorithm unrolling utilizes both the structure of iterative sparse recovery algorithms and the performance gains of supervised deep learning.","However, the robustness of this approach is highly dependant on having sufficient training data.","In this paper we introduce deep unrolled self-supervised learning, which alleviates the need for such data by training a sequence-specific, model-based autoencoder that learns only from given measurements.","Our proposed method exceeds the performance of its supervised counterparts, thus allowing for robust, dynamic imaging well below the diffraction limit without any labeled training samples.","Furthermore, the suggested model-based autoencoder scheme can be utilized to enhance generalization in any sparse recovery framework, without the need for external training data."],"url":"http://arxiv.org/abs/2403.16974v1","category":"eess.IV"}
{"created":"2024-03-25 17:22:11","title":"GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction","abstract":"Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry. Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry.","sentences":["Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics.","Two main requirements lie in rendering and reconstruction.","Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene geometry.","Learning of neural implicit surfaces is sparked from the success of neural rendering.","Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces.","The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes.","To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF).","The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision.","We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying geometry."],"url":"http://arxiv.org/abs/2403.16964v1","category":"cs.CV"}
{"created":"2024-03-25 17:21:48","title":"Constraining the history of reheating with the NANOGrav 15-year data","abstract":"Over the last few years, primordial black holes (PBHs) have emerged as a strong candidate for cold dark matter. A significant number of PBHs are produced when the strength of the primordial scalar power spectrum is enhanced on small scales (compared to the COBE normalized values on large scales). Such primordial spectra also inevitably lead to strong amplification of the scalar-induced, secondary gravitational waves (GWs) at higher frequencies. The recent detection of the stochastic gravitational wave background (SGWB) by the pulsar timing arrays (PTAs) has opened up the possibility of directly probing the very early universe. Different studies have shown that, when PBHs are assumed to have been formed during the epoch of radiation domination, the mechanism for the amplification of the scalar-induced GWs that is required to explain the PTA data can overproduce the PBHs over some ranges of masses. In this work, we assume a specific functional form for the primordial scalar power spectrum and examine the production of PBHs and the scalar-induced secondary GWs during the phase of reheating, which precedes the standard epoch of radiation domination. Specifically, we account for the uncertainties in the conditions for the formation of PBHs and ensure that the extent of PBHs produced remains within the observational bounds. We find that the scalar-induced SGWB generated during a phase of reheating with a steeper equation of state (than that of radiation) fit the NANOGrav 15-year data with a stronger Bayesian evidence than the astrophysical scenario involving GWs produced by merging supermassive binary black holes.","sentences":["Over the last few years, primordial black holes (PBHs) have emerged as a strong candidate for cold dark matter.","A significant number of PBHs are produced when the strength of the primordial scalar power spectrum is enhanced on small scales (compared to the COBE normalized values on large scales).","Such primordial spectra also inevitably lead to strong amplification of the scalar-induced, secondary gravitational waves (GWs) at higher frequencies.","The recent detection of the stochastic gravitational wave background (SGWB) by the pulsar timing arrays (PTAs) has opened up the possibility of directly probing the very early universe.","Different studies have shown that, when PBHs are assumed to have been formed during the epoch of radiation domination, the mechanism for the amplification of the scalar-induced GWs that is required to explain the PTA data can overproduce the PBHs over some ranges of masses.","In this work, we assume a specific functional form for the primordial scalar power spectrum and examine the production of PBHs and the scalar-induced secondary GWs during the phase of reheating, which precedes the standard epoch of radiation domination.","Specifically, we account for the uncertainties in the conditions for the formation of PBHs and ensure that the extent of PBHs produced remains within the observational bounds.","We find that the scalar-induced SGWB generated during a phase of reheating with a steeper equation of state (than that of radiation) fit the NANOGrav 15-year data with a stronger Bayesian evidence than the astrophysical scenario involving GWs produced by merging supermassive binary black holes."],"url":"http://arxiv.org/abs/2403.16963v1","category":"astro-ph.CO"}
{"created":"2024-03-25 17:21:21","title":"An $\u03b1$-potential game framework for $N$-player games","abstract":"This paper proposes and studies a general form of dynamic $N$-player non-cooperative games called $\\alpha$-potential games, where the change of a player's value function upon her unilateral deviation from her strategy is equal to the change of an $\\alpha$-potential function up to an error $\\alpha$. Analogous to the static potential game (which corresponds to $\\alpha=0$), the $\\alpha$-potential game framework is shown to reduce the challenging task of finding approximate Nash equilibria for a dynamic game to minimizing the $\\alpha$-potential function. Moreover, an analytical characterization of $\\alpha$-potential functions is established, with $\\alpha$ represented in terms of the magnitude of the asymmetry of value functions' second-order derivatives. For stochastic differential games in which the state dynamic is a controlled diffusion, $\\alpha$ is explicitly identified in terms of the number of players, the choice of admissible strategies, and the intensity of interactions, and the level of heterogeneity among players. This is achieved by introducing a suitable linear derivative of the value functions with respect to unilateral deviations of strategies and via analyzing the sensitivity processes of state dynamics with respect to controls.   For games with mean-field type interactions, $\\alpha$ is shown to decay to zero as the number of players goes to infinity, even with heterogeneity in state dynamics, cost functions, and admissible strategy classes. For distributed games, if a static potential function can be derived from the cost functions, then $\\alpha=0$. For crowd aversion games, $\\alpha$ is capable of capturing the subtle difference between the choice of admissible strategies.","sentences":["This paper proposes and studies a general form of dynamic $N$-player non-cooperative games called $\\alpha$-potential games, where the change of a player's value function upon her unilateral deviation from her strategy is equal to the change of an $\\alpha$-potential function up to an error $\\alpha$. Analogous to the static potential game (which corresponds to $\\alpha=0$), the $\\alpha$-potential game framework is shown to reduce the challenging task of finding approximate Nash equilibria for a dynamic game to minimizing the $\\alpha$-potential function.","Moreover, an analytical characterization of $\\alpha$-potential functions is established, with $\\alpha$ represented in terms of the magnitude of the asymmetry of value functions' second-order derivatives.","For stochastic differential games in which the state dynamic is a controlled diffusion, $\\alpha$ is explicitly identified in terms of the number of players, the choice of admissible strategies, and the intensity of interactions, and the level of heterogeneity among players.","This is achieved by introducing a suitable linear derivative of the value functions with respect to unilateral deviations of strategies and via analyzing the sensitivity processes of state dynamics with respect to controls.   ","For games with mean-field type interactions, $\\alpha$ is shown to decay to zero as the number of players goes to infinity, even with heterogeneity in state dynamics, cost functions, and admissible strategy classes.","For distributed games, if a static potential function can be derived from the cost functions, then $\\alpha=0$. For crowd aversion games, $\\alpha$ is capable of capturing the subtle difference between the choice of admissible strategies."],"url":"http://arxiv.org/abs/2403.16962v1","category":"math.OC"}
{"created":"2024-03-25 17:21:04","title":"Universe inflation and nonlinear electrodynamics","abstract":"WE analyse the universe inflation when the source of gravity is electromagnetic fields obeying nonlinear electrodynamics with two parameters and without singularities. The cosmology of the universe with stochastic magnetic fields is considered. The condition for the universe inflation is obtained. It is demonstrated that singularities of the energy density and pressure are absent as the scale factor approaches to zero. When the scale factor goes to infinity one has equation of state for ultra-relativistic case. The curvature invariants do not possess singularities. The evolution of universe is described showing that at large time the scale factor corresponds to the radiation era. The duration of universe inflation is analysed. We study the classical stability and causality by computing the speed of the sound. Cosmological parameters such as the spectral index $n_s$, the tensor-to-scalar ratio $r$ and the running of the spectral index $\\alpha_s$ are evaluated.","sentences":["WE analyse the universe inflation when the source of gravity is electromagnetic fields obeying nonlinear electrodynamics with two parameters and without singularities.","The cosmology of the universe with stochastic magnetic fields is considered.","The condition for the universe inflation is obtained.","It is demonstrated that singularities of the energy density and pressure are absent as the scale factor approaches to zero.","When the scale factor goes to infinity one has equation of state for ultra-relativistic case.","The curvature invariants do not possess singularities.","The evolution of universe is described showing that at large time the scale factor corresponds to the radiation era.","The duration of universe inflation is analysed.","We study the classical stability and causality by computing the speed of the sound.","Cosmological parameters such as the spectral index $n_s$, the tensor-to-scalar ratio $r$ and the running of the spectral index $\\alpha_s$ are evaluated."],"url":"http://arxiv.org/abs/2403.17043v1","category":"physics.gen-ph"}
{"created":"2024-03-25 17:19:43","title":"A Mathematical Description of the Quasi-Periodically Developed Heat Transfer Regime in Channels with Arrays of Periodic Solid Structures","abstract":"In this article, we present the governing equations for the temperature field upstream and downstream of the periodically developed flow region in channels with arrays of periodic solid structures. From the ansatz that the temperature field in this region is determined by exponentially decaying modes, just like the quasi-developed flow field, we arrive at an eigenvalue problem which governs the allowed temperature modes. This eigenvalue problem is virtually identical to the one for periodically developed heat transfer in isothermal solids, except that the conjugate heat transfer in the solid is effectively taken into account.","sentences":["In this article, we present the governing equations for the temperature field upstream and downstream of the periodically developed flow region in channels with arrays of periodic solid structures.","From the ansatz that the temperature field in this region is determined by exponentially decaying modes, just like the quasi-developed flow field, we arrive at an eigenvalue problem which governs the allowed temperature modes.","This eigenvalue problem is virtually identical to the one for periodically developed heat transfer in isothermal solids, except that the conjugate heat transfer in the solid is effectively taken into account."],"url":"http://arxiv.org/abs/2403.16960v1","category":"physics.flu-dyn"}
{"created":"2024-03-25 17:11:15","title":"Beyond-$\u039b$CDM Cosmologies with Bayesian Neural Networks II: Massive Neutrinos, Baryonic Feedback, and the Theoretical Error","abstract":"We study the capacity of Bayesian Neural Networks (BNNs) to detect new physics in the dark matter power spectrum. As in previous studies, the Bayesian Cosmological Network (BaCoN) classifies spectra into one of 5 classes: $\\Lambda$CDM, $f(R)$, $w$CDM, Dvali-Gabadaze-Porrati (DGP) gravity and a 'random' class, with this work extending it to include the effects of massive neutrinos and baryonic feedback. We further develop the treatment of theoretical errors in BaCoN-II, investigating several approaches and identifying the one that best allows the trained network to generalise to other power spectrum modelling prescriptions. In particular, we compare power spectra data produced by EuclidEmulator2, HMcode and halofit, all supplemented with the halo model reaction to model beyond-$\\Lambda$CDM physics. We investigate BNN-classifiers trained on these sets of spectra, adding in Stage-IV survey noise and various theoretical error models. Using our optimal theoretical error model, our fiducial classifier achieves a total classification accuracy of $\\sim$ 95% when it is trained on EuclidEmulator2-based spectra with modification parameters drawn from a Gaussian distribution centred around $\\Lambda$CDM ($f(R)$: $\\sigma_{fR0} = 10^{-5.5}$, DGP: $\\sigma_{r\\mathrm{c}} = 0.173$, $w$CDM: $\\sigma_{w0} = 0.097$, $\\sigma_{wa}=0.32$). This strengthens the promise of this method to glean the maximal amount of unbiased gravitational and cosmological information from forthcoming Stage-IV galaxy surveys.","sentences":["We study the capacity of Bayesian Neural Networks (BNNs) to detect new physics in the dark matter power spectrum.","As in previous studies, the Bayesian Cosmological Network (BaCoN) classifies spectra into one of 5 classes: $\\Lambda$CDM, $f(R)$, $w$CDM, Dvali-Gabadaze-Porrati (DGP) gravity and a 'random' class, with this work extending it to include the effects of massive neutrinos and baryonic feedback.","We further develop the treatment of theoretical errors in BaCoN-II, investigating several approaches and identifying the one that best allows the trained network to generalise to other power spectrum modelling prescriptions.","In particular, we compare power spectra data produced by EuclidEmulator2, HMcode and halofit, all supplemented with the halo model reaction to model beyond-$\\Lambda$CDM physics.","We investigate BNN-classifiers trained on these sets of spectra, adding in Stage-IV survey noise and various theoretical error models.","Using our optimal theoretical error model, our fiducial classifier achieves a total classification accuracy of $\\sim$ 95% when it is trained on EuclidEmulator2-based spectra with modification parameters drawn from a Gaussian distribution centred around $\\Lambda$CDM ($f(R)$: $\\sigma_{fR0} = 10^{-5.5}$, DGP: $\\sigma_{r\\mathrm{c}} = 0.173$, $w$CDM: $\\sigma_{w0} = 0.097$, $\\sigma_{wa}=0.32$).","This strengthens the promise of this method to glean the maximal amount of unbiased gravitational and cosmological information from forthcoming Stage-IV galaxy surveys."],"url":"http://arxiv.org/abs/2403.16949v1","category":"astro-ph.CO"}
{"created":"2024-03-25 16:47:03","title":"High-order transient multidimensional simulation of a thermo-electro-chemo-mechanical model for Lithium-ion batteries","abstract":"We build a transient multidimensional multiphysical model based on continuum theories, involving the coupled mechanical, thermal and electrochemical phenomena occurring simultaneously in the discharge or charge of lithium-ion batteries. The process delivers a system of coupled nonlinear partial differential equations. Besides initial and boundary conditions, we highlight the treatment of the electrode-electrolyte interface condition, which corresponds to a Butler-Volmer reaction kinetics equation. We present the derivation of the strong and weak forms of the model, as well as the discretization procedure in space and in time. The discretized model is computationally solved in two dimensions by means of a finite element method that employs $hp$ layered meshes, along with staggered second order semi-implicit time integration. The expected error estimate is of higher order than any other similar work, both in space and in time. A representative battery cell geometry, under distinct operating scenarios, is simulated. The numerical results show that the full model allows for important additional insights to be drawn than when caring only for the electrochemical coupling. Considering the multiphysics becomes more important as the applied current is increased, whether for discharge or for charge. Our full model provides battery design professionals with a valuable tool to optimize designs and advance the energy storage industry.","sentences":["We build a transient multidimensional multiphysical model based on continuum theories, involving the coupled mechanical, thermal and electrochemical phenomena occurring simultaneously in the discharge or charge of lithium-ion batteries.","The process delivers a system of coupled nonlinear partial differential equations.","Besides initial and boundary conditions, we highlight the treatment of the electrode-electrolyte interface condition, which corresponds to a Butler-Volmer reaction kinetics equation.","We present the derivation of the strong and weak forms of the model, as well as the discretization procedure in space and in time.","The discretized model is computationally solved in two dimensions by means of a finite element method that employs $hp$ layered meshes, along with staggered second order semi-implicit time integration.","The expected error estimate is of higher order than any other similar work, both in space and in time.","A representative battery cell geometry, under distinct operating scenarios, is simulated.","The numerical results show that the full model allows for important additional insights to be drawn than when caring only for the electrochemical coupling.","Considering the multiphysics becomes more important as the applied current is increased, whether for discharge or for charge.","Our full model provides battery design professionals with a valuable tool to optimize designs and advance the energy storage industry."],"url":"http://arxiv.org/abs/2403.16928v1","category":"math.NA"}
{"created":"2024-03-25 16:37:46","title":"$p$-adic rigidity of eigenforms of infinite slope","abstract":"We give a notion of $p$-adic families of Hecke eigenforms that allows for the slope of the forms be infinite at $p$. We prove that, contrary to the case of finite slope when every eigenform lives in a Hida or Coleman family, the only families of infinite slope are either twists of Hida or Coleman families with Dirichlet characters of $p$-power conductor, or non-ordinary families with complex multiplication. Our proof goes via a local study of deformations of potentially trianguline Galois representations, relying on work of Berger and Chenevier, and a global input coming from an analogue of a result of Balasubramanyam, Ghate and Vatsal on a Greenberg-type conjecture for families of Hilbert modular forms.","sentences":["We give a notion of $p$-adic families of Hecke eigenforms that allows for the slope of the forms be infinite at $p$. We prove that, contrary to the case of finite slope when every eigenform lives in a Hida or Coleman family, the only families of infinite slope are either twists of Hida or Coleman families with Dirichlet characters of $p$-power conductor, or non-ordinary families with complex multiplication.","Our proof goes via a local study of deformations of potentially trianguline Galois representations, relying on work of Berger and Chenevier, and a global input coming from an analogue of a result of Balasubramanyam, Ghate and Vatsal on a Greenberg-type conjecture for families of Hilbert modular forms."],"url":"http://arxiv.org/abs/2403.16918v1","category":"math.NT"}
{"created":"2024-03-25 16:37:17","title":"Existence and uniqueness for renormalized solutions to a general noncoercive nonlinear parabolic equation","abstract":"This paper introduces the concept of renormalized solution for a general class of non-coercive nonlinear parabolic problems, including both singularities and unbounded lower order terms. We prove existence and uniqueness of renormalized solutions for such class of problems.","sentences":["This paper introduces the concept of renormalized solution for a general class of non-coercive nonlinear parabolic problems, including both singularities and unbounded lower order terms.","We prove existence and uniqueness of renormalized solutions for such class of problems."],"url":"http://arxiv.org/abs/2403.16917v1","category":"math.AP"}
{"created":"2024-03-25 16:10:47","title":"State Space Models as Foundation Models: A Control Theoretic Overview","abstract":"In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.","sentences":["In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models.","This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks.","Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data.","The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems.","Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas.","This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments.","It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective.","Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences."],"url":"http://arxiv.org/abs/2403.16899v1","category":"eess.SY"}
{"created":"2024-03-26 17:52:23","title":"Leveraging Near-Field Lighting for Monocular Depth Estimation from Endoscopy Videos","abstract":"Monocular depth estimation in endoscopy videos can enable assistive and robotic surgery to obtain better coverage of the organ and detection of various health issues. Despite promising progress on mainstream, natural image depth estimation, techniques perform poorly on endoscopy images due to a lack of strong geometric features and challenging illumination effects. In this paper, we utilize the photometric cues, i.e., the light emitted from an endoscope and reflected by the surface, to improve monocular depth estimation. We first create two novel loss functions with supervised and self-supervised variants that utilize a per-pixel shading representation. We then propose a novel depth refinement network (PPSNet) that leverages the same per-pixel shading representation. Finally, we introduce teacher-student transfer learning to produce better depth maps from both synthetic data with supervision and clinical data with self-supervision. We achieve state-of-the-art results on the C3VD dataset while estimating high-quality depth maps from clinical data. Our code, pre-trained models, and supplementary materials can be found on our project page: https://ppsnet.github.io/","sentences":["Monocular depth estimation in endoscopy videos can enable assistive and robotic surgery to obtain better coverage of the organ and detection of various health issues.","Despite promising progress on mainstream, natural image depth estimation, techniques perform poorly on endoscopy images due to a lack of strong geometric features and challenging illumination effects.","In this paper, we utilize the photometric cues, i.e., the light emitted from an endoscope and reflected by the surface, to improve monocular depth estimation.","We first create two novel loss functions with supervised and self-supervised variants that utilize a per-pixel shading representation.","We then propose a novel depth refinement network (PPSNet) that leverages the same per-pixel shading representation.","Finally, we introduce teacher-student transfer learning to produce better depth maps from both synthetic data with supervision and clinical data with self-supervision.","We achieve state-of-the-art results on the C3VD dataset while estimating high-quality depth maps from clinical data.","Our code, pre-trained models, and supplementary materials can be found on our project page: https://ppsnet.github.io/"],"url":"http://arxiv.org/abs/2403.17915v1","category":"cs.CV"}
{"created":"2024-03-26 17:43:15","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models","abstract":"The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while maintaining a compact model size.","sentences":["The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms.","However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images.","On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension.","In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block.","SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size.","Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while maintaining a compact model size."],"url":"http://arxiv.org/abs/2403.17902v1","category":"eess.IV"}
{"created":"2024-03-26 17:17:10","title":"Empirical Analysis of EIP-3675: Miner Dynamics, Transaction Fees, and Transaction Time","abstract":"The Ethereum Improvement Proposal 3675 (EIP-3675) marks a significant shift, transitioning from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus mechanism. This transition resulted in a staggering 99.95% decrease in energy consumption. However, the transition prompts two critical questions: (1). How does EIP-3675 affect miners' dynamics? and (2). How do users determine priority fees, considering that paying too little may cause delays or non-inclusion, yet paying too much wastes money with little to no benefits? To address the first question, we present a comprehensive empirical study examining EIP-3675's effect on miner dynamics (i.e., miner participation, distribution, and the degree of randomness in miner selection). Our findings reveal that the transition has encouraged broader participation of miners in block append operation, resulting in a larger pool of unique miners ($\\approx50\\times$ PoW), and the change in miner distribution with the increased number of unique small category miners ($\\approx60\\times$ PoW). However, there is an unintended consequence: a reduction in the miner selection randomness, which signifies the negative impact of the transition to PoS-Ethereum on network decentralization. Regarding the second question, we employed regression-based machine learning models; the Gradient Boosting Regressor performed best in predicting priority fees, while the K-Neighbours Regressor was worst.","sentences":["The Ethereum Improvement Proposal 3675 (EIP-3675) marks a significant shift, transitioning from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus mechanism.","This transition resulted in a staggering 99.95% decrease in energy consumption.","However, the transition prompts two critical questions: (1).","How does EIP-3675 affect miners' dynamics?","and (2).","How do users determine priority fees, considering that paying too little may cause delays or non-inclusion, yet paying too much wastes money with little to no benefits?","To address the first question, we present a comprehensive empirical study examining EIP-3675's effect on miner dynamics (i.e., miner participation, distribution, and the degree of randomness in miner selection).","Our findings reveal that the transition has encouraged broader participation of miners in block append operation, resulting in a larger pool of unique miners ($\\approx50\\times$ PoW), and the change in miner distribution with the increased number of unique small category miners ($\\approx60\\times$ PoW).","However, there is an unintended consequence: a reduction in the miner selection randomness, which signifies the negative impact of the transition to PoS-Ethereum on network decentralization.","Regarding the second question, we employed regression-based machine learning models; the Gradient Boosting Regressor performed best in predicting priority fees, while the K-Neighbours Regressor was worst."],"url":"http://arxiv.org/abs/2403.17885v1","category":"cs.DB"}
{"created":"2024-03-26 17:16:04","title":"Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using Sentinel Data","abstract":"Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications. To advance the development of machine learning algorithms in this domain, our study introduces the \\textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection. This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each patch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI). The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance. Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection. The code and dataset are available online (https://zenodo.org/records/10881058).","sentences":["Utilizing satellite imagery for wildfire detection presents substantial potential for practical applications.","To advance the development of machine learning algorithms in this domain, our study introduces the \\textit{Sen2Fire} dataset--a challenging satellite remote sensing dataset tailored for wildfire detection.","This dataset is curated from Sentinel-2 multi-spectral data and Sentinel-5P aerosol product, comprising a total of 2466 image patches.","Each patch has a size of 512$\\times$512 pixels with 13 bands.","Given the distinctive sensitivities of various wavebands to wildfire responses, our research focuses on optimizing wildfire detection by evaluating different wavebands and employing a combination of spectral indices, such as normalized burn ratio (NBR) and normalized difference vegetation index (NDVI).","The results suggest that, in contrast to using all bands for wildfire detection, selecting specific band combinations yields superior performance.","Additionally, our study underscores the positive impact of integrating Sentinel-5 aerosol data for wildfire detection.","The code and dataset are available online (https://zenodo.org/records/10881058)."],"url":"http://arxiv.org/abs/2403.17884v1","category":"cs.CV"}
{"created":"2024-03-26 16:57:33","title":"To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning","abstract":"Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable. Remarkably, even the applicability of standard supervised pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training. Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them.","sentences":["Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis.","Unfortunately, its applicability in 3D data processing has been relatively limited.","While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios.","Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable.","Remarkably, even the applicability of standard supervised pre-training is poorly understood.","In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks.","We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks.","Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training.","Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them."],"url":"http://arxiv.org/abs/2403.17869v1","category":"cs.CV"}
{"created":"2024-03-26 16:33:12","title":"Mechanistic Design and Scaling of Hybrid Architectures","abstract":"The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.","sentences":["The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation.","We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws.","Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives.","We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters.","Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks.","The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes.","Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology."],"url":"http://arxiv.org/abs/2403.17844v1","category":"cs.LG"}
{"created":"2024-03-26 16:14:43","title":"GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning","abstract":"Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.","sentences":["Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency.","Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment.","To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions.","We also employ an Exploit-Explore mechanism to enhance performance.","Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in FEMINST test accuracy.","Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning."],"url":"http://arxiv.org/abs/2403.17833v1","category":"cs.LG"}
{"created":"2024-03-26 16:04:19","title":"Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders","abstract":"Self-supervised pre-training of image encoders is omnipresent in the literature, particularly following the introduction of Masked autoencoders (MAE). Current efforts attempt to learn object-centric representations from motion in videos. In particular, SiamMAE recently introduced a Siamese network, training a shared-weight encoder from two frames of a video with a high asymmetric masking ratio (95%). In this work, we propose CropMAE, an alternative approach to the Siamese pre-training introduced by SiamMAE. Our method specifically differs by exclusively considering pairs of cropped images sourced from the same image but cropped differently, deviating from the conventional pairs of frames extracted from a video. CropMAE therefore alleviates the need for video datasets, while maintaining competitive performances and drastically reducing pre-training time. Furthermore, we demonstrate that CropMAE learns similar object-centric representations without explicit motion, showing that current self-supervised learning methods do not learn objects from motion, but rather thanks to the Siamese architecture. Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling the reconstruction of images using only two visible patches. Our code is available at https://github.com/alexandre-eymael/CropMAE.","sentences":["Self-supervised pre-training of image encoders is omnipresent in the literature, particularly following the introduction of Masked autoencoders (MAE).","Current efforts attempt to learn object-centric representations from motion in videos.","In particular, SiamMAE recently introduced a Siamese network, training a shared-weight encoder from two frames of a video with a high asymmetric masking ratio (95%).","In this work, we propose CropMAE, an alternative approach to the Siamese pre-training introduced by SiamMAE.","Our method specifically differs by exclusively considering pairs of cropped images sourced from the same image but cropped differently, deviating from the conventional pairs of frames extracted from a video.","CropMAE therefore alleviates the need for video datasets, while maintaining competitive performances and drastically reducing pre-training time.","Furthermore, we demonstrate that CropMAE learns similar object-centric representations without explicit motion, showing that current self-supervised learning methods do not learn objects from motion, but rather thanks to the Siamese architecture.","Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling the reconstruction of images using only two visible patches.","Our code is available at https://github.com/alexandre-eymael/CropMAE."],"url":"http://arxiv.org/abs/2403.17823v1","category":"cs.CV"}
{"created":"2024-03-26 15:50:37","title":"Are Compressed Language Models Less Subgroup Robust?","abstract":"To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.","sentences":["To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models.","However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset.","In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models.","We show that worst-group performance does not depend on model size alone, but also on the compression method used.","Additionally, we find that model compression does not always worsen the performance on minority subgroups.","Altogether, our analysis serves to further research into the subgroup robustness of model compression."],"url":"http://arxiv.org/abs/2403.17811v1","category":"cs.LG"}
{"created":"2024-03-26 14:54:35","title":"Asymptotic Bayes risk of semi-supervised learning with uncertain labeling","abstract":"This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.","sentences":["This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels.","Our main aim is to compute the Bayes risk for this model.","We compare the behavior of the Bayes risk and the best known algorithm for this model.","This comparison eventually gives new insights over the algorithm."],"url":"http://arxiv.org/abs/2403.17767v1","category":"stat.ML"}
{"created":"2024-03-26 14:20:42","title":"Continual Few-shot Event Detection via Hierarchical Augmentation Networks","abstract":"Traditional continual event detection relies on abundant labeled data for training, which is often impractical to obtain in real-world applications. In this paper, we introduce continual few-shot event detection (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible. The CFED task is challenging as it involves memorizing previous event types and learning new event types with few-shot samples. To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Networks (HANet). To memorize previous event types with limited memory, we incorporate prototypical augmentation into the memory set. For the issue of learning new event types in few-shot scenarios, we propose a contrastive augmentation module for token representations. Despite comparing with previous state-of-the-art methods, we also conduct comparisons with ChatGPT. Experiment results demonstrate that our method significantly outperforms all of these methods in multiple continual few-shot event detection tasks.","sentences":["Traditional continual event detection relies on abundant labeled data for training, which is often impractical to obtain in real-world applications.","In this paper, we introduce continual few-shot event detection (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible.","The CFED task is challenging as it involves memorizing previous event types and learning new event types with few-shot samples.","To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Networks (HANet).","To memorize previous event types with limited memory, we incorporate prototypical augmentation into the memory set.","For the issue of learning new event types in few-shot scenarios, we propose a contrastive augmentation module for token representations.","Despite comparing with previous state-of-the-art methods, we also conduct comparisons with ChatGPT.","Experiment results demonstrate that our method significantly outperforms all of these methods in multiple continual few-shot event detection tasks."],"url":"http://arxiv.org/abs/2403.17733v1","category":"cs.CL"}
{"created":"2024-03-26 14:16:56","title":"FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts","abstract":"Quickly understanding lengthy lecture videos is essential for learners with limited time and interest in various topics to improve their learning efficiency. To this end, video summarization has been actively researched to enable users to view only important scenes from a video. However, these studies focus on either the visual or audio information of a video and extract important segments in the video. Therefore, there is a risk of missing important information when both the teacher's speech and visual information on the blackboard or slides are important, such as in a lecture video. To tackle this issue, we propose FastPerson, a video summarization approach that considers both the visual and auditory information in lecture videos. FastPerson creates summary videos by utilizing audio transcriptions along with on-screen images and text, minimizing the risk of overlooking crucial information for learners. Further, it provides a feature that allows learners to switch between the summary and original videos for each chapter of the video, enabling them to adjust the pace of learning based on their interests and level of understanding. We conducted an evaluation with 40 participants to assess the effectiveness of our method and confirmed that it reduced viewing time by 53\\% at the same level of comprehension as that when using traditional video playback methods.","sentences":["Quickly understanding lengthy lecture videos is essential for learners with limited time and interest in various topics to improve their learning efficiency.","To this end, video summarization has been actively researched to enable users to view only important scenes from a video.","However, these studies focus on either the visual or audio information of a video and extract important segments in the video.","Therefore, there is a risk of missing important information when both the teacher's speech and visual information on the blackboard or slides are important, such as in a lecture video.","To tackle this issue, we propose FastPerson, a video summarization approach that considers both the visual and auditory information in lecture videos.","FastPerson creates summary videos by utilizing audio transcriptions along with on-screen images and text, minimizing the risk of overlooking crucial information for learners.","Further, it provides a feature that allows learners to switch between the summary and original videos for each chapter of the video, enabling them to adjust the pace of learning based on their interests and level of understanding.","We conducted an evaluation with 40 participants to assess the effectiveness of our method and confirmed that it reduced viewing time by 53\\% at the same level of comprehension as that when using traditional video playback methods."],"url":"http://arxiv.org/abs/2403.17727v1","category":"cs.CV"}
{"created":"2024-03-26 12:17:46","title":"Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency","abstract":"We propose a voxel-based optimization framework, ReVoRF, for few-shot radiance fields that strategically address the unreliability in pseudo novel view synthesis. Our method pivots on the insight that relative depth relationships within neighboring regions are more reliable than the absolute color values in disoccluded areas. Consequently, we devise a bilateral geometric consistency loss that carefully navigates the trade-off between color fidelity and geometric accuracy in the context of depth consistency for uncertain regions. Moreover, we present a reliability-guided learning strategy to discern and utilize the variable quality across synthesized views, complemented by a reliability-aware voxel smoothing algorithm that smoothens the transition between reliable and unreliable data patches. Our approach allows for a more nuanced use of all available data, promoting enhanced learning from regions previously considered unsuitable for high-quality reconstruction. Extensive experiments across diverse datasets reveal that our approach attains significant gains in efficiency and accuracy, delivering rendering speeds of 3 FPS, 7 mins to train a $360^\\circ$ scene, and a 5\\% improvement in PSNR over existing few-shot methods. Code is available at https://github.com/HKCLynn/ReVoRF.","sentences":["We propose a voxel-based optimization framework, ReVoRF, for few-shot radiance fields that strategically address the unreliability in pseudo novel view synthesis.","Our method pivots on the insight that relative depth relationships within neighboring regions are more reliable than the absolute color values in disoccluded areas.","Consequently, we devise a bilateral geometric consistency loss that carefully navigates the trade-off between color fidelity and geometric accuracy in the context of depth consistency for uncertain regions.","Moreover, we present a reliability-guided learning strategy to discern and utilize the variable quality across synthesized views, complemented by a reliability-aware voxel smoothing algorithm that smoothens the transition between reliable and unreliable data patches.","Our approach allows for a more nuanced use of all available data, promoting enhanced learning from regions previously considered unsuitable for high-quality reconstruction.","Extensive experiments across diverse datasets reveal that our approach attains significant gains in efficiency and accuracy, delivering rendering speeds of 3 FPS, 7 mins to train a $360^\\circ$ scene, and a 5\\% improvement in PSNR over existing few-shot methods.","Code is available at https://github.com/HKCLynn/ReVoRF."],"url":"http://arxiv.org/abs/2403.17638v1","category":"cs.CV"}
{"created":"2024-03-26 11:48:37","title":"Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles from 3D Cell Painting Images","abstract":"Despite their black-box nature, deep learning models are extensively used in image-based drug discovery to extract feature vectors from single cells in microscopy images. To better understand how these networks perform representation learning, we employ visual explainability techniques (e.g., Grad-CAM). Our analyses reveal several mechanisms by which supervised models cheat, exploiting biologically irrelevant pixels when extracting morphological features from images, such as noise in the background. This raises doubts regarding the fidelity of learned single-cell representations and their relevance when investigating downstream biological questions. To address this misalignment between researcher expectations and machine behavior, we introduce Grad-CAMO, a novel single-cell interpretability score for supervised feature extractors. Grad-CAMO measures the proportion of a model's attention that is concentrated on the cell of interest versus the background. This metric can be assessed per-cell or averaged across a validation set, offering a tool to audit individual features vectors or guide the improved design of deep learning architectures. Importantly, Grad-CAMO seamlessly integrates into existing workflows, requiring no dataset or model modifications, and is compatible with both 2D and 3D Cell Painting data. Additional results are available at https://github.com/eigenvivek/Grad-CAMO.","sentences":["Despite their black-box nature, deep learning models are extensively used in image-based drug discovery to extract feature vectors from single cells in microscopy images.","To better understand how these networks perform representation learning, we employ visual explainability techniques (e.g., Grad-CAM).","Our analyses reveal several mechanisms by which supervised models cheat, exploiting biologically irrelevant pixels when extracting morphological features from images, such as noise in the background.","This raises doubts regarding the fidelity of learned single-cell representations and their relevance when investigating downstream biological questions.","To address this misalignment between researcher expectations and machine behavior, we introduce Grad-CAMO, a novel single-cell interpretability score for supervised feature extractors.","Grad-CAMO measures the proportion of a model's attention that is concentrated on the cell of interest versus the background.","This metric can be assessed per-cell or averaged across a validation set, offering a tool to audit individual features vectors or guide the improved design of deep learning architectures.","Importantly, Grad-CAMO seamlessly integrates into existing workflows, requiring no dataset or model modifications, and is compatible with both 2D and 3D Cell Painting data.","Additional results are available at https://github.com/eigenvivek/Grad-CAMO."],"url":"http://arxiv.org/abs/2403.17615v1","category":"eess.IV"}
{"created":"2024-03-26 10:01:01","title":"RuBia: A Russian Language Bias Detection Dataset","abstract":"Warning: this work contains upsetting or disturbing content.   Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers. Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To illustrate the dataset's purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases.","sentences":["Warning: this work contains upsetting or disturbing content.   ","Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data.","To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific.","In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia.","The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains.","Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it.","These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourcing workers.","Overall, there are nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia.","To illustrate the dataset's purpose, we conduct a diagnostic evaluation of state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs' predisposition to social biases."],"url":"http://arxiv.org/abs/2403.17553v1","category":"cs.CL"}
{"created":"2024-03-26 07:21:19","title":"Quantum accelerated cross regression algorithm for multiview feature extraction","abstract":"Multi-view Feature Extraction (MvFE) has wide applications in machine learning, image processing and other fields. When dealing with massive high-dimensional data, the performance of classical computer faces severe challenges due to MvFE involves expensive matrix calculation. To address this challenge, a quantum-accelerated cross-regression algorithm for MvFE is proposed. The main contributions are as follows:(1) a quantum version algorithm for MvFE is proposed for the first time, filling the gap of quantum computing in the field of MvFE;(2) a quantum algorithm is designed to construct the block-encoding of the target data matrix, so that the optimal Hamiltonian simulation technology based on the block-encoding framework can be used to efficiently realize the quantum simulation of the target data matrix. This approach reduces the dependence of the algorithm's on simulation errors to enhance algorithm performance;(3) compared with the classical counterpart algorithm, the proposed quantum algorithm has a polynomial acceleration in the number of data points, the dimension of data points and the number of view data.","sentences":["Multi-view Feature Extraction (MvFE) has wide applications in machine learning, image processing and other fields.","When dealing with massive high-dimensional data, the performance of classical computer faces severe challenges due to MvFE involves expensive matrix calculation.","To address this challenge, a quantum-accelerated cross-regression algorithm for MvFE is proposed.","The main contributions are as follows:(1) a quantum version algorithm for MvFE is proposed for the first time, filling the gap of quantum computing in the field of MvFE;(2)","a quantum algorithm is designed to construct the block-encoding of the target data matrix, so that the optimal Hamiltonian simulation technology based on the block-encoding framework can be used to efficiently realize the quantum simulation of the target data matrix.","This approach reduces the dependence of the algorithm's on simulation errors to enhance algorithm performance;(3) compared with the classical counterpart algorithm, the proposed quantum algorithm has a polynomial acceleration in the number of data points, the dimension of data points and the number of view data."],"url":"http://arxiv.org/abs/2403.17444v1","category":"quant-ph"}
{"created":"2024-03-26 06:16:11","title":"Short-term Classification of Strong Solar Energetic Particle Events using Multivariate Time Series Classifiers","abstract":"Solar energetic particle (SEP) events are one of the most crucial aspects of space weather that require continuous monitoring and forecasting. Their prediction depends on various factors including source eruptions. In the present work, we use the Geostationary Solar Energetic Particle (GSEP) data set covering solar cycles 22, 23, and 24. We develop a framework using time series-based machine learning (ML) models with the aim of developing robust short-term forecasts by classifying SEP events. For this purpose, we introduce an ensemble learning approach that merges the results from univariate time series of three proton channels (10 MeV, 50 MeV, and 100 MeV) and the long band X-ray flux channel from the Geostationary Operational Environmental Satellite (GOES) missions and analyze their performance. We consider three models, namely, time series forest (TSF), supervised time series forest (STSF) and bag of SFA symbols (BOSS). Our study also focuses on understanding and developing confidence in the predictive capabilities of our models. Therefore, we utilize multiple evaluation techniques and metrics. Based on that, we find STSF to perform well in all scenarios. The summary of metrics for the STSF model is as follows: AUC = 0.981; F1-score = 0.960; TSS = 0.919; HSS = 0.920; GSS = 0.852; and MCC = 0.920. The Brier score loss of the STSF model is 0.077. This work lays the foundation for building near-real-time (NRT) short-term SEP event predictions using robust ML methods.","sentences":["Solar energetic particle (SEP) events are one of the most crucial aspects of space weather that require continuous monitoring and forecasting.","Their prediction depends on various factors including source eruptions.","In the present work, we use the Geostationary Solar Energetic Particle (GSEP) data set covering solar cycles 22, 23, and 24.","We develop a framework using time series-based machine learning (ML) models with the aim of developing robust short-term forecasts by classifying SEP events.","For this purpose, we introduce an ensemble learning approach that merges the results from univariate time series of three proton channels (10 MeV, 50 MeV, and 100 MeV) and the long band X-ray flux channel from the Geostationary Operational Environmental Satellite (GOES) missions and analyze their performance.","We consider three models, namely, time series forest (TSF), supervised time series forest (STSF) and bag of SFA symbols (BOSS).","Our study also focuses on understanding and developing confidence in the predictive capabilities of our models.","Therefore, we utilize multiple evaluation techniques and metrics.","Based on that, we find STSF to perform well in all scenarios.","The summary of metrics for the STSF model is as follows: AUC = 0.981; F1-score = 0.960; TSS = 0.919; HSS = 0.920; GSS = 0.852; and MCC = 0.920.","The Brier score loss of the STSF model is 0.077.","This work lays the foundation for building near-real-time (NRT) short-term SEP event predictions using robust ML methods."],"url":"http://arxiv.org/abs/2403.17418v1","category":"astro-ph.SR"}
{"created":"2024-03-26 06:11:07","title":"PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models","abstract":"Prompt compression is an innovative method for efficiently condensing input prompts while preserving essential information. To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is a unified plug-and-play solution for compressing prompts in Large Language Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and metrics for comprehensive performance evaluation. PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces. In this paper, we outline the key components and functionalities of PCToolkit. We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, summarization, mathematical problem-solving, question answering, few-shot learning, synthetic tasks, code completion, boolean expressions, multiple choice questions, and lies recognition.","sentences":["Prompt compression is an innovative method for efficiently condensing input prompts while preserving essential information.","To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the Prompt Compression Toolkit (PCToolkit).","This toolkit is a unified plug-and-play solution for compressing prompts in Large Language Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and metrics for comprehensive performance evaluation.","PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces.","In this paper, we outline the key components and functionalities of PCToolkit.","We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, summarization, mathematical problem-solving, question answering, few-shot learning, synthetic tasks, code completion, boolean expressions, multiple choice questions, and lies recognition."],"url":"http://arxiv.org/abs/2403.17411v1","category":"cs.CL"}
{"created":"2024-03-26 05:48:02","title":"Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study","abstract":"Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, the total number of experts, the sparsity in expert selection, the complexity of the routing mechanism, and the complexity of individual experts. Our analysis sheds light on \\textit{how \\textbf{sparsity} contributes to the MoE's generalization}, offering insights from the perspective of classical learning theory.","sentences":["Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts).","This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data.","Conventional MoE mechanisms select all available experts, incurring substantial computational costs.","In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance.","Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive.","In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors.","Specifically, we investigate the impact of the number of data samples, the total number of experts, the sparsity in expert selection, the complexity of the routing mechanism, and the complexity of individual experts.","Our analysis sheds light on \\textit{how \\textbf{sparsity} contributes to the MoE's generalization}, offering insights from the perspective of classical learning theory."],"url":"http://arxiv.org/abs/2403.17404v1","category":"cs.LG"}
{"created":"2024-03-26 04:40:33","title":"Compensating for charge sharing by a deep-learning method: a preliminary experimental study","abstract":"Photon counting detectors (PCDs) bring valuable advantages to diagnostic computed tomography (CT), including lower noise and higher resolution than energy integrating detectors. However, there are still several nonideal factors preventing PCDs from meeting people's expectations, for example, charge sharing and pile up. In this paper, we did some preliminary work on charge sharing and conducted an experimental study using an XCounter PCD to compare the effects of no anti-coincidence, anti-coincidence by hardware and charge sharing compensation by a deep learning method. In our results, a smaller bias and standard deviation are obtained from deep learning method than directly from no-anti-coincidence mode of the detector. Our network also outperforms the anti-coincidence mode of the detector in the low energy bin and has smaller standard deviation in the high energy bin. The results validate that a deep learning method is suitable to compensate for charge sharing.","sentences":["Photon counting detectors (PCDs) bring valuable advantages to diagnostic computed tomography (CT), including lower noise and higher resolution than energy integrating detectors.","However, there are still several nonideal factors preventing PCDs from meeting people's expectations, for example, charge sharing and pile up.","In this paper, we did some preliminary work on charge sharing and conducted an experimental study using an XCounter PCD to compare the effects of no anti-coincidence, anti-coincidence by hardware and charge sharing compensation by a deep learning method.","In our results, a smaller bias and standard deviation are obtained from deep learning method than directly from no-anti-coincidence mode of the detector.","Our network also outperforms the anti-coincidence mode of the detector in the low energy bin and has smaller standard deviation in the high energy bin.","The results validate that a deep learning method is suitable to compensate for charge sharing."],"url":"http://arxiv.org/abs/2403.17375v1","category":"physics.med-ph"}
{"created":"2024-03-26 04:30:40","title":"Multi-Domain Recommendation to Attract Users via Domain Preference Modeling","abstract":"Recently, web platforms have been operating various service domains simultaneously. Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU), which recommends items from multiple ``unseen'' domains with which each user has not interacted yet, by using knowledge from the user's ``seen'' domains. In this paper, we point out two challenges of MDRAU task. First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains. Second, a user might have different preferences for each of the target unseen domains, which requires that recommendations reflect the user's preferences on domains as well as items. To tackle these challenges, we propose DRIP framework that models users' preferences at two levels (i.e., domain and item) and learns various seen-unseen domain mappings in a unified way with masked domain modeling. Our extensive experiments demonstrate the effectiveness of DRIP in MDRAU task and its ability to capture users' domain-level preferences.","sentences":["Recently, web platforms have been operating various service domains simultaneously.","Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU), which recommends items from multiple ``unseen'' domains with which each user has not interacted yet, by using knowledge from the user's ``seen'' domains.","In this paper, we point out two challenges of MDRAU task.","First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains.","Second, a user might have different preferences for each of the target unseen domains, which requires that recommendations reflect the user's preferences on domains as well as items.","To tackle these challenges, we propose DRIP framework that models users' preferences at two levels (i.e., domain and item) and learns various seen-unseen domain mappings in a unified way with masked domain modeling.","Our extensive experiments demonstrate the effectiveness of DRIP in MDRAU task and its ability to capture users' domain-level preferences."],"url":"http://arxiv.org/abs/2403.17374v1","category":"cs.IR"}
{"created":"2024-03-26 04:16:57","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential Recommenders","abstract":"Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions. While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs. However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models. Consequently, designing a simple and universal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential \\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable challenge. We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture. Along these dimensions, we dissect the model designs, and answer the following sub-questions: First, we explore how to construct MMSR from scratch, ensuring its performance either on par with or exceeds existing SR methods without complex techniques. Second, we examine if MMSR can benefit from existing multi-modal pre-training paradigms. Third, we assess MMSR's capability in tackling common challenges like cold start and domain transferring. Our experiment results across four real-world recommendation scenarios demonstrate the great potential ID-agnostic multi-modal sequential recommendation. Our framework can be found at: https://github.com/MMSR23/MMSR.","sentences":["Sequential Recommendation (SR) aims to predict future user-item interactions based on historical interactions.","While many SR approaches concentrate on user IDs and item IDs, the human perception of the world through multi-modal signals, like text and images, has inspired researchers to delve into constructing SR from multi-modal information without using IDs.","However, the complexity of multi-modal learning manifests in diverse feature extractors, fusion methods, and pre-trained models.","Consequently, designing a simple and universal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential \\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable challenge.","We systematically summarize the existing multi-modal related SR methods and distill the essence into four core components: visual encoder, text encoder, multimodal fusion module, and sequential architecture.","Along these dimensions, we dissect the model designs, and answer the following sub-questions: First, we explore how to construct MMSR from scratch, ensuring its performance either on par with or exceeds existing SR methods without complex techniques.","Second, we examine if MMSR can benefit from existing multi-modal pre-training paradigms.","Third, we assess MMSR's capability in tackling common challenges like cold start and domain transferring.","Our experiment results across four real-world recommendation scenarios demonstrate the great potential ID-agnostic multi-modal sequential recommendation.","Our framework can be found at: https://github.com/MMSR23/MMSR."],"url":"http://arxiv.org/abs/2403.17372v1","category":"cs.IR"}
{"created":"2024-03-26 04:02:09","title":"A Moreau Envelope Approach for LQR Meta-Policy Estimation","abstract":"We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in discrete-time linear time-invariant uncertain dynamical systems. We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a meta-policy efficiently adjustable to new realizations. Moreover, we design an algorithm to find an approximate first-order stationary point of the meta-LQR cost function. Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system. We also provide empirical evidence that our method has better sample complexity than Model-Agnostic Meta-Learning (MAML) approaches.","sentences":["We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in discrete-time linear time-invariant uncertain dynamical systems.","We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a meta-policy efficiently adjustable to new realizations.","Moreover, we design an algorithm to find an approximate first-order stationary point of the meta-LQR cost function.","Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system.","We also provide empirical evidence that our method has better sample complexity than Model-Agnostic Meta-Learning (MAML) approaches."],"url":"http://arxiv.org/abs/2403.17364v1","category":"math.OC"}
{"created":"2024-03-26 03:53:00","title":"Activity-Biometrics: Person Identification from Daily Activities","abstract":"In this work, we study a novel problem which focuses on person identification while performing daily activities. Learning biometric features from RGB videos is challenging due to spatio-temporal complexity and presence of appearance biases such as clothing color and background. We propose ABNet, a novel framework which leverages disentanglement of biometric and non-biometric features to perform effective person identification from daily activities. ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle non-biometric features with the help of biometric distortion. In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning. We perform comprehensive evaluation of the proposed approach across five different datasets which are derived from existing activity recognition benchmarks. Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activity-based biometrics across all five datasets. The code and dataset can be accessed at: \\url{https://github.com/sacrcv/Activity-Biometrics/}","sentences":["In this work, we study a novel problem which focuses on person identification while performing daily activities.","Learning biometric features from RGB videos is challenging due to spatio-temporal complexity and presence of appearance biases such as clothing color and background.","We propose ABNet, a novel framework which leverages disentanglement of biometric and non-biometric features to perform effective person identification from daily activities.","ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle non-biometric features with the help of biometric distortion.","In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning.","We perform comprehensive evaluation of the proposed approach across five different datasets which are derived from existing activity recognition benchmarks.","Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activity-based biometrics across all five datasets.","The code and dataset can be accessed at: \\url{https://github.com/sacrcv/Activity-Biometrics/}"],"url":"http://arxiv.org/abs/2403.17360v1","category":"cs.CV"}
{"created":"2024-03-26 03:32:45","title":"Multi-Objective Trajectory Planning with Dual-Encoder","abstract":"Time-jerk optimal trajectory planning is crucial in advancing robotic arms' performance in dynamic tasks. Traditional methods rely on solving complex nonlinear programming problems, bringing significant delays in generating optimized trajectories. In this paper, we propose a two-stage approach to accelerate time-jerk optimal trajectory planning. Firstly, we introduce a dual-encoder based transformer model to establish a good preliminary trajectory. This trajectory is subsequently refined through sequential quadratic programming to improve its optimality and robustness. Our approach outperforms the state-of-the-art by up to 79.72\\% in reducing trajectory planning time. Compared with existing methods, our method shrinks the optimality gap with the objective function value decreasing by up to 29.9\\%.","sentences":["Time-jerk optimal trajectory planning is crucial in advancing robotic arms' performance in dynamic tasks.","Traditional methods rely on solving complex nonlinear programming problems, bringing significant delays in generating optimized trajectories.","In this paper, we propose a two-stage approach to accelerate time-jerk optimal trajectory planning.","Firstly, we introduce a dual-encoder based transformer model to establish a good preliminary trajectory.","This trajectory is subsequently refined through sequential quadratic programming to improve its optimality and robustness.","Our approach outperforms the state-of-the-art by up to 79.72\\% in reducing trajectory planning time.","Compared with existing methods, our method shrinks the optimality gap with the objective function value decreasing by up to 29.9\\%."],"url":"http://arxiv.org/abs/2403.17353v1","category":"cs.RO"}
{"created":"2024-03-26 03:05:20","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","abstract":"In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we aim to open new avenues for employing LLMs in biomedical imaging and enriching the understanding of their potential in this specialized domain.","sentences":["In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data.","The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens.","This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs.","We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters.","More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in MedMNIST-2D and 3D.","Through this work, we aim to open new avenues for employing LLMs in biomedical imaging and enriching the understanding of their potential in this specialized domain."],"url":"http://arxiv.org/abs/2403.17343v1","category":"cs.CV"}
{"created":"2024-03-26 02:21:36","title":"Accuracy enhancement method for speech emotion recognition from spectrogram using temporal frequency correlation and positional information learning through knowledge transfer","abstract":"In this paper, we propose a method to improve the accuracy of speech emotion recognition (SER) by using vision transformer (ViT) to attend to the correlation of frequency (y-axis) with time (x-axis) in spectrogram and transferring positional information between ViT through knowledge transfer. The proposed method has the following originality i) We use vertically segmented patches of log-Mel spectrogram to analyze the correlation of frequencies over time. This type of patch allows us to correlate the most relevant frequencies for a particular emotion with the time they were uttered. ii) We propose the use of image coordinate encoding, an absolute positional encoding suitable for ViT. By normalizing the x, y coordinates of the image to -1 to 1 and concatenating them to the image, we can effectively provide valid absolute positional information for ViT. iii) Through feature map matching, the locality and location information of the teacher network is effectively transmitted to the student network. Teacher network is a ViT that contains locality of convolutional stem and absolute position information through image coordinate encoding, and student network is a structure that lacks positional encoding in the basic ViT structure. In feature map matching stage, we train through the mean absolute error (L1 loss) to minimize the difference between the feature maps of the two networks. To validate the proposed method, three emotion datasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into log-Mel spectrograms for comparison experiments. The experimental results show that the proposed method significantly outperforms the state-of-the-art methods in terms of weighted accuracy while requiring significantly fewer floating point operations (FLOPs). Overall, the proposed method offers an promising solution for SER by providing improved efficiency and performance.","sentences":["In this paper, we propose a method to improve the accuracy of speech emotion recognition (SER) by using vision transformer (ViT) to attend to the correlation of frequency (y-axis) with time (x-axis) in spectrogram and transferring positional information between ViT through knowledge transfer.","The proposed method has the following originality i)","We use vertically segmented patches of log-Mel spectrogram to analyze the correlation of frequencies over time.","This type of patch allows us to correlate the most relevant frequencies for a particular emotion with the time they were uttered.","ii)","We propose the use of image coordinate encoding, an absolute positional encoding suitable for ViT. By normalizing the x, y coordinates of the image to -1 to 1 and concatenating them to the image, we can effectively provide valid absolute positional information for ViT. iii)","Through feature map matching, the locality and location information of the teacher network is effectively transmitted to the student network.","Teacher network is a ViT that contains locality of convolutional stem and absolute position information through image coordinate encoding, and student network is a structure that lacks positional encoding in the basic ViT structure.","In feature map matching stage, we train through the mean absolute error (L1 loss) to minimize the difference between the feature maps of the two networks.","To validate the proposed method, three emotion datasets (SAVEE, EmoDB, and CREMA-D) consisting of speech were converted into log-Mel spectrograms for comparison experiments.","The experimental results show that the proposed method significantly outperforms the state-of-the-art methods in terms of weighted accuracy while requiring significantly fewer floating point operations (FLOPs).","Overall, the proposed method offers an promising solution for SER by providing improved efficiency and performance."],"url":"http://arxiv.org/abs/2403.17327v1","category":"cs.SD"}
{"created":"2024-03-26 01:52:59","title":"Project MOSLA: Recording Every Moment of Second Language Acquisition","abstract":"Second language acquisition (SLA) is a complex and dynamic process. Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process). In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom. The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and fine-tuned state-of-the-art speech models. Our experiments reveal linguistic insights into learners' proficiency development over time, as well as the potential for automatically detecting the areas of focus on the screen purely from the unannotated multimodal data. Our dataset is freely available for research purposes and can serve as a valuable resource for a wide range of applications, including but not limited to SLA, proficiency assessment, language and speech processing, pedagogy, and multimodal learning analytics.","sentences":["Second language acquisition (SLA) is a complex and dynamic process.","Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process).","In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom.","The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and fine-tuned state-of-the-art speech models.","Our experiments reveal linguistic insights into learners' proficiency development over time, as well as the potential for automatically detecting the areas of focus on the screen purely from the unannotated multimodal data.","Our dataset is freely available for research purposes and can serve as a valuable resource for a wide range of applications, including but not limited to SLA, proficiency assessment, language and speech processing, pedagogy, and multimodal learning analytics."],"url":"http://arxiv.org/abs/2403.17314v1","category":"cs.CL"}
{"created":"2024-03-26 01:29:17","title":"HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification","abstract":"Existing self-supervised methods in natural language processing (NLP), especially hierarchical text classification (HTC), mainly focus on self-supervised contrastive learning, extremely relying on human-designed augmentation rules to generate contrastive samples, which can potentially corrupt or distort the original information. In this paper, we tend to investigate the feasibility of a contrastive learning scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the contrastive samples and fused during the learning process. Specifically, we propose an information lossless contrastive learning strategy for HTC, namely \\textbf{H}ierarchy-aware \\textbf{I}nformation \\textbf{L}ossless contrastive \\textbf{L}earning (HILL), which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample. The structure encoder takes the document embedding as input, extracts the essential syntactic information inherent in the label hierarchy with the principle of structural entropy minimization, and injects the syntactic information into the text representation via hierarchical representation learning. Experiments on three common datasets are conducted to verify the superiority of HILL.","sentences":["Existing self-supervised methods in natural language processing (NLP), especially hierarchical text classification (HTC), mainly focus on self-supervised contrastive learning, extremely relying on human-designed augmentation rules to generate contrastive samples, which can potentially corrupt or distort the original information.","In this paper, we tend to investigate the feasibility of a contrastive learning scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the contrastive samples and fused during the learning process.","Specifically, we propose an information lossless contrastive learning strategy for HTC, namely \\textbf{H}ierarchy-aware \\textbf{I}nformation \\textbf{L}ossless contrastive \\textbf{L}earning (HILL), which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample.","The structure encoder takes the document embedding as input, extracts the essential syntactic information inherent in the label hierarchy with the principle of structural entropy minimization, and injects the syntactic information into the text representation via hierarchical representation learning.","Experiments on three common datasets are conducted to verify the superiority of HILL."],"url":"http://arxiv.org/abs/2403.17307v1","category":"cs.CL"}
{"created":"2024-03-26 00:33:49","title":"Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study","abstract":"Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD). (2) Recent algorithms present smaller standard deviation in accuracy across clients than FedAvg, indicating that the advanced algorithms' performances are stable. (3) However, algorithms such as FedDyn and SCAFFOLD are more prone to catastrophic failures without the support of additional techniques such as gradient clipping. We hope that our empirical study can help the community to build best practices in evaluating FL algorithms.","sentences":["Federated Learning (FL) emerged as a practical approach to training a model from decentralized data.","The proliferation of FL led to the development of numerous FL algorithms and mechanisms.","Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc.","To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame.","Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics.","A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD).","(2) Recent algorithms present smaller standard deviation in accuracy across clients than FedAvg, indicating that the advanced algorithms' performances are stable.","(3) However, algorithms such as FedDyn and SCAFFOLD are more prone to catastrophic failures without the support of additional techniques such as gradient clipping.","We hope that our empirical study can help the community to build best practices in evaluating FL algorithms."],"url":"http://arxiv.org/abs/2403.17287v1","category":"cs.LG"}
{"created":"2024-03-25 23:32:34","title":"Galaxy spin direction asymmetry in JWST deep fields","abstract":"The unprecedented imaging power of JWST provides new abilities to observe the shapes of objects in the early Universe in a way that has not been possible before. Recently, JWST acquired a deep field image inside the same field imaged in the past as the HST Ultra Deep Field. Computer-based quantitative analysis of spiral galaxies in that field shows that among 34 galaxies for which their rotation of direction can be determined by the shapes of the arms, 24 rotate clockwise, and just 10 rotate counterclockwise. The one-tailed binomial distribution probability to have asymmetry equal or stronger than the observed asymmetry by chance is $\\sim$0.012. While the analysis is limited by the small size of the data, the observed asymmetry is aligned with all relevant previous large-scale analyses from all premier digital sky surveys, all show a higher number of galaxies rotating clockwise in that part of the sky, and the magnitude of the asymmetry increases as the redshift gets higher. This paper also provides data and analysis to reproduce previous experiments suggesting that the distribution of galaxy rotation in the Universe is random, to show that the exact same data used in these studies in fact show non-random distribution, and in excellent agreement with the results shown here. These findings reinforce consideration of the possibility that the directions of rotation of spiral galaxies as observed from Earth are not necessarily randomly distributed. The explanation can be related to the large-scale structure of the Universe, but can also be related to a possible anomaly in the physics of galaxy rotation.","sentences":["The unprecedented imaging power of JWST provides new abilities to observe the shapes of objects in the early Universe in a way that has not been possible before.","Recently, JWST acquired a deep field image inside the same field imaged in the past as the HST Ultra Deep Field.","Computer-based quantitative analysis of spiral galaxies in that field shows that among 34 galaxies for which their rotation of direction can be determined by the shapes of the arms, 24 rotate clockwise, and just 10 rotate counterclockwise.","The one-tailed binomial distribution probability to have asymmetry equal or stronger than the observed asymmetry by chance is $\\sim$0.012.","While the analysis is limited by the small size of the data, the observed asymmetry is aligned with all relevant previous large-scale analyses from all premier digital sky surveys, all show a higher number of galaxies rotating clockwise in that part of the sky, and the magnitude of the asymmetry increases as the redshift gets higher.","This paper also provides data and analysis to reproduce previous experiments suggesting that the distribution of galaxy rotation in the Universe is random, to show that the exact same data used in these studies in fact show non-random distribution, and in excellent agreement with the results shown here.","These findings reinforce consideration of the possibility that the directions of rotation of spiral galaxies as observed from Earth are not necessarily randomly distributed.","The explanation can be related to the large-scale structure of the Universe, but can also be related to a possible anomaly in the physics of galaxy rotation."],"url":"http://arxiv.org/abs/2403.17271v1","category":"astro-ph.CO"}
{"created":"2024-03-25 23:07:31","title":"Diffusion-based Negative Sampling on Graphs for Link Prediction","abstract":"Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable ``hardness'' levels from the latent space. Our method, called Conditional Diffusion-based Multi-level Negative Sampling (DMNS), leverages the Markov chain property of diffusion models to generate negative nodes in multiple levels of variable hardness and reconcile them for effective graph link prediction. We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling. Extensive experiments on several benchmark datasets demonstrate the effectiveness of DMNS.","sentences":["Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal.","Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control.","Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space.","To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable ``hardness'' levels from the latent space.","Our method, called Conditional Diffusion-based Multi-level Negative Sampling (DMNS), leverages the Markov chain property of diffusion models to generate negative nodes in multiple levels of variable hardness and reconcile them for effective graph link prediction.","We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling.","Extensive experiments on several benchmark datasets demonstrate the effectiveness of DMNS."],"url":"http://arxiv.org/abs/2403.17259v1","category":"cs.LG"}
{"created":"2024-03-25 23:02:33","title":"A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning","abstract":"Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification.   Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language model (LLM)","sentences":["Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them.","The development of supervised models has been at the forefront of research in this area.","However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming.","Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type.","In this work, we address this notable challenge in current state-of-the-art ABSA research.","We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning.","The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies.","We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms.","Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification.   ","Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language model (LLM)"],"url":"http://arxiv.org/abs/2403.17254v1","category":"cs.CL"}
{"created":"2024-03-25 22:52:50","title":"Machine learning for moduli space of genus two curves and an application to post-quantum cryptography","abstract":"We use machine learning to study the locus ${\\mathcal L}_n$ of genus two curves with $(n, n)$-split Jacobian. More precisely we design a transformer model which given values for the Igusa invariants determines if the corresponding genus two curve is in the locus ${\\mathcal L}_n$, for $n=2, 3, 5, 7$. Such curves are important in isogeny based cryptography.   During this study we discover that there are no rational points ${\\mathfrak p} \\in {\\mathcal L}_n$ with weighted moduli height $\\leq 2$ in any of ${\\mathcal L}_2$, ${\\mathcal L}_3$, and ${\\mathcal L}_5$. This extends on previous work of the authors to use machine learning methods to study the moduli space of genus 2 algebraic curves.","sentences":["We use machine learning to study the locus ${\\mathcal L}_n$ of genus two curves with $(n, n)$-split Jacobian.","More precisely we design a transformer model which given values for the Igusa invariants determines if the corresponding genus two curve is in the locus ${\\mathcal L}_n$, for $n=2, 3, 5, 7$.","Such curves are important in isogeny based cryptography.   ","During this study we discover that there are no rational points ${\\mathfrak p} \\in {\\mathcal L}_n$ with weighted moduli height $\\leq 2$ in any of ${\\mathcal L}_2$, ${\\mathcal L}_3$, and ${\\mathcal L}_5$.","This extends on previous work of the authors to use machine learning methods to study the moduli space of genus 2 algebraic curves."],"url":"http://arxiv.org/abs/2403.17250v1","category":"math.AG"}
{"created":"2024-03-25 22:39:20","title":"Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks","abstract":"Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates. However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases. To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs) including both Large Language Models (LLMs) and Vision Language Models (VLMs). Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories. To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity. The metrics measure the temporal alignment and semantic fidelity of language descriptions between two sub-task decompositions, namely an FM sub-task decomposition prediction and a ground-truth sub-task decomposition. We present scores for temporal similarity and semantic similarity above 90%, compared to 30% of a randomized baseline, for multiple robotic environments, demonstrating the effectiveness of our proposed framework. Our results enable building diverse, large-scale, language-supervised datasets for improved robotic TAMP.","sentences":["Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates.","However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases.","To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs) including both Large Language Models (LLMs) and Vision Language Models (VLMs).","Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories.","To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity.","The metrics measure the temporal alignment and semantic fidelity of language descriptions between two sub-task decompositions, namely an FM sub-task decomposition prediction and a ground-truth sub-task decomposition.","We present scores for temporal similarity and semantic similarity above 90%, compared to 30% of a randomized baseline, for multiple robotic environments, demonstrating the effectiveness of our proposed framework.","Our results enable building diverse, large-scale, language-supervised datasets for improved robotic TAMP."],"url":"http://arxiv.org/abs/2403.17238v1","category":"cs.RO"}
{"created":"2024-03-26 17:59:58","title":"Efficient Video Object Segmentation via Modulated Cross-Attention Memory","abstract":"Recently, transformer-based approaches have shown promising results for semi-supervised video object segmentation. However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames. We propose a transformer-based approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion. The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length. Extensive experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017, demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos. Compared to the best existing transformer-based approach, our MAVOS increases the speed by 7.6x, while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets. Notably on the LVOS dataset, our MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU. Our code and models will be publicly available at: https://github.com/Amshaker/MAVOS.","sentences":["Recently, transformer-based approaches have shown promising results for semi-supervised video object segmentation.","However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames.","We propose a transformer-based approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion.","The proposed MCA effectively encodes both local and global features at various levels of granularity while efficiently maintaining consistent speed regardless of the video length.","Extensive experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017, demonstrate the effectiveness of our proposed contributions leading to real-time inference and markedly reduced memory demands without any degradation in segmentation accuracy on long videos.","Compared to the best existing transformer-based approach, our MAVOS increases the speed by 7.6x, while significantly reducing the GPU memory by 87% with comparable segmentation performance on short and long video datasets.","Notably on the LVOS dataset, our MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second (FPS) on a single V100 GPU.","Our code and models will be publicly available at: https://github.com/Amshaker/MAVOS."],"url":"http://arxiv.org/abs/2403.17937v1","category":"cs.CV"}
{"created":"2024-03-26 17:56:33","title":"Optimizing Vaccine Site Locations While Considering Travel Inconvenience and Public Health Outcomes","abstract":"During the COVID-19 pandemic, there were over three million infections in Los Angeles County (LAC). To facilitate distribution when vaccines first became available, LAC set up six mega-sites for dispensing a large number of vaccines to the public. To understand if another choice of mega-site location would have improved accessibility and health outcomes, and to provide insight into future vaccine allocation problems, we propose a multi-objective mixed integer linear programming model that balances travel convenience, infection reduction, and equitable distribution. We provide a tractable objective formulation that effectively proxies real-world public health goals of reducing infections while considering travel inconvenience and equitable distribution of resources. Compared with the solution empirically used in LAC in 2020, we recommend more dispersed mega-site locations that result in a 28% reduction in travel inconvenience and avert an additional 1,000 infections.","sentences":["During the COVID-19 pandemic, there were over three million infections in Los Angeles County (LAC).","To facilitate distribution when vaccines first became available, LAC set up six mega-sites for dispensing a large number of vaccines to the public.","To understand if another choice of mega-site location would have improved accessibility and health outcomes, and to provide insight into future vaccine allocation problems, we propose a multi-objective mixed integer linear programming model that balances travel convenience, infection reduction, and equitable distribution.","We provide a tractable objective formulation that effectively proxies real-world public health goals of reducing infections while considering travel inconvenience and equitable distribution of resources.","Compared with the solution empirically used in LAC in 2020, we recommend more dispersed mega-site locations that result in a 28% reduction in travel inconvenience and avert an additional 1,000 infections."],"url":"http://arxiv.org/abs/2403.17923v1","category":"math.OC"}
{"created":"2024-03-26 16:38:12","title":"Multi Agent Pathfinding for Noise Restricted Hybrid Fuel Unmanned Aerial Vehicles","abstract":"Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple agents from respective start to goal locations such that no paths conflict. We address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles which are subject to location-dependent noise restrictions. We solve this problem by searching a constraint tree for which the subproblem at each node is a set of shortest path problems subject to the noise and fuel constraints and conflict zone avoidance. A labeling algorithm is presented to solve this subproblem, including the conflict zones which are treated as dynamic obstacles. We present the experimental results of the algorithms for various graph sizes and number of agents.","sentences":["Multi Agent Path Finding (MAPF) seeks the optimal set of paths for multiple agents from respective start to goal locations such that no paths conflict.","We address the MAPF problem for a fleet of hybrid-fuel unmanned aerial vehicles which are subject to location-dependent noise restrictions.","We solve this problem by searching a constraint tree for which the subproblem at each node is a set of shortest path problems subject to the noise and fuel constraints and conflict zone avoidance.","A labeling algorithm is presented to solve this subproblem, including the conflict zones which are treated as dynamic obstacles.","We present the experimental results of the algorithms for various graph sizes and number of agents."],"url":"http://arxiv.org/abs/2403.17849v1","category":"math.OC"}
{"created":"2024-03-26 15:31:37","title":"A new double-pass type of the optical spring","abstract":"In detuned optical cavities, the radiation pressure force acting on the mirrors depends on their displacements. This is equivalent to the rigidity (the optical spring), inserted between the mirrors. This effect can be used for optimization of the mechanical susceptibility of probe mirrors in high-precision force sensors. However, in some cases, the use of detuned cavities or even just any high-finesse cavities could be problematic due to technological constraints.   We consider a new type of the optical spring that does not require the cavity (but can use a resonance tuned one to increase the optomechanical coupling). Instead, it uses the double interaction of the probing light with the mechanical object. We propose two possible implementation of this concept, suitable, respectively, for the atomic spin ensembles and for the laser gravitational-wave detectors.","sentences":["In detuned optical cavities, the radiation pressure force acting on the mirrors depends on their displacements.","This is equivalent to the rigidity (the optical spring), inserted between the mirrors.","This effect can be used for optimization of the mechanical susceptibility of probe mirrors in high-precision force sensors.","However, in some cases, the use of detuned cavities or even just any high-finesse cavities could be problematic due to technological constraints.   ","We consider a new type of the optical spring that does not require the cavity (but can use a resonance tuned one to increase the optomechanical coupling).","Instead, it uses the double interaction of the probing light with the mechanical object.","We propose two possible implementation of this concept, suitable, respectively, for the atomic spin ensembles and for the laser gravitational-wave detectors."],"url":"http://arxiv.org/abs/2403.17795v1","category":"quant-ph"}
{"created":"2024-03-26 13:38:12","title":"Simulation of hydrogen adsorption in hierarchical silicalite: Role of electrostatics and surface chemistry","abstract":"Adsorption in nanoporous materials is one strategy that can be used to store hydrogen at conditions of temperature and pressure that are economically viable. Adsorption capacity of nanoporous materials depends on surface area which can be enhanced by incorporating a hierarchical pore structure. We report grand canonical Monte Carlo (GCMC) simulation results on the adsorption of hydrogen in hierarchical models of silicalite that incorporate 4 nm wide mesopores in addition to the 0.5 nm wide micropores at 298 K, using different force fields to model hydrogen. Our results suggest that incorporating mesopores in silicalite can enhance adsorption by at least 20% if electrostatic interactions are not included and up to 100% otherwise. Incorporating electrostatic interactions results in higher adsorption by close to 100% at lower pressures for hierarchical silicalite whereas for unmodified silicalite, it is less significant at all pressures. Hydroxylating the mesopore surface in hierarchical silicalite results in an enhancement in adsorption at pressures below 1 atm and suppression by up to 20 % at higher pressures. Temperature dependence at selected pressures exhibits expected decrease in adsorption amounts at higher temperatures. These findings can be useful in the engineering, selection, and optimization of nanoporous materials for hydrogen storage.","sentences":["Adsorption in nanoporous materials is one strategy that can be used to store hydrogen at conditions of temperature and pressure that are economically viable.","Adsorption capacity of nanoporous materials depends on surface area which can be enhanced by incorporating a hierarchical pore structure.","We report grand canonical Monte Carlo (GCMC) simulation results on the adsorption of hydrogen in hierarchical models of silicalite that incorporate 4 nm wide mesopores in addition to the 0.5 nm wide micropores at 298 K, using different force fields to model hydrogen.","Our results suggest that incorporating mesopores in silicalite can enhance adsorption by at least 20% if electrostatic interactions are not included and up to 100% otherwise.","Incorporating electrostatic interactions results in higher adsorption by close to 100% at lower pressures for hierarchical silicalite whereas for unmodified silicalite, it is less significant at all pressures.","Hydroxylating the mesopore surface in hierarchical silicalite results in an enhancement in adsorption at pressures below 1 atm and suppression by up to 20 % at higher pressures.","Temperature dependence at selected pressures exhibits expected decrease in adsorption amounts at higher temperatures.","These findings can be useful in the engineering, selection, and optimization of nanoporous materials for hydrogen storage."],"url":"http://arxiv.org/abs/2403.17699v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 09:40:39","title":"Nonsmooth convex-concave saddle point problems with cardinality penalties","abstract":"In this paper, we focus on a class of convexly constrained nonsmooth convex-concave saddle point problems with cardinality penalties. Although such nonsmooth nonconvex-nonconcave and discontinuous min-max problems may not have a saddle point, we show that they have a local saddle point and a global minimax point, and some local saddle points have the lower bound properties. We define a class of strong local saddle points based on the lower bound properties for stability of variable selection. Moreover, we give a framework to construct continuous relaxations of the discontinuous min-max problems based on the convolution, such that they have the same saddle points with the original problem. We also establish the relations between the continuous relaxation problems and the original problems regarding local saddle points, global minimax points, local minimax points and stationary points. Finally, we illustrate our results with distributionally robust sparse convex regression, sparse robust bond portfolio construction and sparse convex-concave logistic regression saddle point problems.","sentences":["In this paper, we focus on a class of convexly constrained nonsmooth convex-concave saddle point problems with cardinality penalties.","Although such nonsmooth nonconvex-nonconcave and discontinuous min-max problems may not have a saddle point, we show that they have a local saddle point and a global minimax point, and some local saddle points have the lower bound properties.","We define a class of strong local saddle points based on the lower bound properties for stability of variable selection.","Moreover, we give a framework to construct continuous relaxations of the discontinuous min-max problems based on the convolution, such that they have the same saddle points with the original problem.","We also establish the relations between the continuous relaxation problems and the original problems regarding local saddle points, global minimax points, local minimax points and stationary points.","Finally, we illustrate our results with distributionally robust sparse convex regression, sparse robust bond portfolio construction and sparse convex-concave logistic regression saddle point problems."],"url":"http://arxiv.org/abs/2403.17535v1","category":"math.OC"}
{"created":"2024-03-26 09:11:58","title":"baskexact: An R package for analytical calculation of basket trial operating characteristics","abstract":"Basket trials are a new type of clinical trial in which a treatment is investigated in several subgroups. For the analysis of these trials, information is shared between the subgroups based on the observed data to increase the power. Many approaches for the analysis of basket trials have been suggested, but only a few have been implemented in open source software packages. The R package baskexact facilitates the evaluation of two basket trial designs which use empirical Bayes techniques for sharing information. With baskexact, operating characteristics for single-stage and two-stage designs can be calculated analytically and optimal tuning parameters can be selected.","sentences":["Basket trials are a new type of clinical trial in which a treatment is investigated in several subgroups.","For the analysis of these trials, information is shared between the subgroups based on the observed data to increase the power.","Many approaches for the analysis of basket trials have been suggested, but only a few have been implemented in open source software packages.","The R package baskexact facilitates the evaluation of two basket trial designs which use empirical Bayes techniques for sharing information.","With baskexact, operating characteristics for single-stage and two-stage designs can be calculated analytically and optimal tuning parameters can be selected."],"url":"http://arxiv.org/abs/2403.17510v1","category":"stat.CO"}
{"created":"2024-03-26 09:08:25","title":"Algorithmic unfolding for image reconstruction and localization problems in fluorescence microscopy","abstract":"We propose an unfolded accelerated projected-gradient descent procedure to estimate model and algorithmic parameters for image super-resolution and molecule localization problems in image microscopy. The variational lower-level constraint enforces sparsity of the solution and encodes different noise statistics (Gaussian, Poisson), while the upper-level cost assesses optimality w.r.t.~the task considered. In more detail, a standard $\\ell_2$ cost is considered for image reconstruction (e.g., deconvolution/super-resolution, semi-blind deconvolution) problems, while a smoothed $\\ell_1$ is employed to assess localization precision in some exemplary fluorescence microscopy problems exploiting single-molecule activation. Several numerical experiments are reported to validate the proposed approach on synthetic and realistic ISBI data.","sentences":["We propose an unfolded accelerated projected-gradient descent procedure to estimate model and algorithmic parameters for image super-resolution and molecule localization problems in image microscopy.","The variational lower-level constraint enforces sparsity of the solution and encodes different noise statistics (Gaussian, Poisson), while the upper-level cost assesses optimality w.r.t.~the task considered.","In more detail, a standard $\\ell_2$ cost is considered for image reconstruction (e.g., deconvolution/super-resolution, semi-blind deconvolution) problems, while a smoothed $\\ell_1$ is employed to assess localization precision in some exemplary fluorescence microscopy problems exploiting single-molecule activation.","Several numerical experiments are reported to validate the proposed approach on synthetic and realistic ISBI data."],"url":"http://arxiv.org/abs/2403.17506v1","category":"math.NA"}
{"created":"2024-03-26 05:26:51","title":"Compressed sensing enhanced by quantum approximate optimization algorithm","abstract":"We present a framework to deal with a range of large scale compressive sensing problems using a quantum subroutine. We apply a quantum approximate optimization algorithm (QAOA) to support detection in a sparse signal reconstruction algorithm: matching pursuit. The constrained optimization required in this algorithm is difficult to handle when the size of the problem is large and constraints are given by unstructured patterns. Our framework utilizes specially designed structured constraints that are easy to manipulate and reduce the optimization problem to the solution of an Ising model which can be found using Ising solvers. In this research, we test the performance of QAOA for this purpose on a simulator of quantum computer. We observe that our method can outperform reference classical methods. Our results explore a promising path of applying quantum computers in the compressive sensing field.","sentences":["We present a framework to deal with a range of large scale compressive sensing problems using a quantum subroutine.","We apply a quantum approximate optimization algorithm (QAOA) to support detection in a sparse signal reconstruction algorithm: matching pursuit.","The constrained optimization required in this algorithm is difficult to handle when the size of the problem is large and constraints are given by unstructured patterns.","Our framework utilizes specially designed structured constraints that are easy to manipulate and reduce the optimization problem to the solution of an Ising model which can be found using Ising solvers.","In this research, we test the performance of QAOA for this purpose on a simulator of quantum computer.","We observe that our method can outperform reference classical methods.","Our results explore a promising path of applying quantum computers in the compressive sensing field."],"url":"http://arxiv.org/abs/2403.17399v1","category":"quant-ph"}
{"created":"2024-03-26 05:12:18","title":"Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object Detection","abstract":"We delve into pseudo-labeling for semi-supervised monocular 3D object detection (SSM3OD) and discover two primary issues: a misalignment between the prediction quality of 3D and 2D attributes and the tendency of depth supervision derived from pseudo-labels to be noisy, leading to significant optimization conflicts with other reliable forms of supervision. We introduce a novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach features a Decoupled Pseudo-label Generation (DPG) module, designed to efficiently generate pseudo-labels by separately processing 2D and 3D attributes. This module incorporates a unique homography-based method for identifying dependable pseudo-labels in BEV space, specifically for 3D attributes. Additionally, we present a DepthGradient Projection (DGP) module to mitigate optimization conflicts caused by noisy depth supervision of pseudo-labels, effectively decoupling the depth gradient and removing conflicting gradients. This dual decoupling strategy-at both the pseudo-label generation and gradient levels-significantly improves the utilization of pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark demonstrate the superiority of our method over existing approaches.","sentences":["We delve into pseudo-labeling for semi-supervised monocular 3D object detection (SSM3OD) and discover two primary issues: a misalignment between the prediction quality of 3D and 2D attributes and the tendency of depth supervision derived from pseudo-labels to be noisy, leading to significant optimization conflicts with other reliable forms of supervision.","We introduce a novel decoupled pseudo-labeling (DPL) approach for SSM3OD.","Our approach features a Decoupled Pseudo-label Generation (DPG) module, designed to efficiently generate pseudo-labels by separately processing 2D and 3D attributes.","This module incorporates a unique homography-based method for identifying dependable pseudo-labels in BEV space, specifically for 3D attributes.","Additionally, we present a DepthGradient Projection (DGP) module to mitigate optimization conflicts caused by noisy depth supervision of pseudo-labels, effectively decoupling the depth gradient and removing conflicting gradients.","This dual decoupling strategy-at both the pseudo-label generation and gradient levels-significantly improves the utilization of pseudo-labels in SSM3OD.","Our comprehensive experiments on the KITTI benchmark demonstrate the superiority of our method over existing approaches."],"url":"http://arxiv.org/abs/2403.17387v1","category":"cs.CV"}
{"created":"2024-03-26 03:10:45","title":"TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos","abstract":"We propose TRAM, a two-stage method to reconstruct a human's global trajectory and motion from in-the-wild videos. TRAM robustifies SLAM to recover the camera motion in the presence of dynamic humans and uses the scene background to derive the motion scale. Using the recovered camera as a metric-scale reference frame, we introduce a video transformer model (VIMO) to regress the kinematic body motion of a human. By composing the two motions, we achieve accurate recovery of 3D humans in the world space, reducing global motion errors by 60% from prior work. https://yufu-wang.github.io/tram4d/","sentences":["We propose TRAM, a two-stage method to reconstruct a human's global trajectory and motion from in-the-wild videos.","TRAM robustifies SLAM to recover the camera motion in the presence of dynamic humans and uses the scene background to derive the motion scale.","Using the recovered camera as a metric-scale reference frame, we introduce a video transformer model (VIMO) to regress the kinematic body motion of a human.","By composing the two motions, we achieve accurate recovery of 3D humans in the world space, reducing global motion errors by 60% from prior work.","https://yufu-wang.github.io/tram4d/"],"url":"http://arxiv.org/abs/2403.17346v1","category":"cs.CV"}
{"created":"2024-03-26 02:48:52","title":"Destination-Constrained Linear Dynamical System Modeling in Set-Valued Frameworks","abstract":"Directional motion towards a specified destination is a common occurrence in physical processes and human societal activities. Utilizing this prior information can significantly improve the control and predictive performance of system models. This paper primarily focuses on reconstructing linear dynamic system models based on destination constraints in the set-valued framework. We treat destination constraints as inherent information in the state evolution process and employ convex optimization techniques to construct a coherent and robust state model. This refined model effectively captures the impact of destination constraints on the state evolution at each time step. Furthermore, we design an optimal weight matrix for the reconstructed model to ensure smoother and more natural trajectories of state evolution. We also analyze the theoretical guarantee of optimality for this weight matrix and the properties of the reconstructed model. Finally, simulation experiments verify that the reconstructed model has significant advantages over the unconstrained and unoptimized weighted models and constrains the evolution of state trajectories with different starting and ending points.","sentences":["Directional motion towards a specified destination is a common occurrence in physical processes and human societal activities.","Utilizing this prior information can significantly improve the control and predictive performance of system models.","This paper primarily focuses on reconstructing linear dynamic system models based on destination constraints in the set-valued framework.","We treat destination constraints as inherent information in the state evolution process and employ convex optimization techniques to construct a coherent and robust state model.","This refined model effectively captures the impact of destination constraints on the state evolution at each time step.","Furthermore, we design an optimal weight matrix for the reconstructed model to ensure smoother and more natural trajectories of state evolution.","We also analyze the theoretical guarantee of optimality for this weight matrix and the properties of the reconstructed model.","Finally, simulation experiments verify that the reconstructed model has significant advantages over the unconstrained and unoptimized weighted models and constrains the evolution of state trajectories with different starting and ending points."],"url":"http://arxiv.org/abs/2403.17337v1","category":"eess.SY"}
{"created":"2024-03-26 02:32:52","title":"Labeling subtypes in a Parkinson's Cohort using Multifeatures in MRI - Integrating Grey and White Matter Information","abstract":"Thresholding of networks has long posed a challenge in brain connectivity analysis. Weighted networks are typically binarized using threshold measures to facilitate network analysis. Previous studies on MRI-based brain networks have predominantly utilized density or sparsity-based thresholding techniques, optimized within specific ranges derived from network metrics such as path length, clustering coefficient, and small-world index. Thus, determination of a single threshold value for facilitating comparative analysis of networks remains elusive. To address this, our study introduces Mutual K-Nearest Neighbor (MKNN)-based thresholding for brain network analysis. Here, nearest neighbor selection is based on the highest correlation between features of brain regions. Construction of brain networks was accomplished by computing Pearson correlations between grey matter volume and white matter volume for each pair of brain regions. Structural MRI data from 180 Parkinsons patients and 70 controls from the NIMHANS, India were analyzed. Subtypes within Parkinsons disease were identified based on grey and white matter volume atrophy using source-based morphometric decomposition. The loading coefficients were correlated with clinical features to discern clinical relationship with the deciphered subtypes. Our data-mining approach revealed: Subtype A (N = 51, intermediate type), Subtype B (N = 57, mild-severe type with mild motor symptoms), and Subtype AB (N = 36, most-severe type with predominance in motor impairment). Subtype-specific weighted matrices were binarized using MKNN-based thresholding for brain network analysis. Permutation tests on network metrics of resulting bipartite graphs demonstrated significant group differences in betweenness centrality and participation coefficient. The identified hubs were specific to each subtype, with some hubs conserved across different subtypes.","sentences":["Thresholding of networks has long posed a challenge in brain connectivity analysis.","Weighted networks are typically binarized using threshold measures to facilitate network analysis.","Previous studies on MRI-based brain networks have predominantly utilized density or sparsity-based thresholding techniques, optimized within specific ranges derived from network metrics such as path length, clustering coefficient, and small-world index.","Thus, determination of a single threshold value for facilitating comparative analysis of networks remains elusive.","To address this, our study introduces Mutual K-Nearest Neighbor (MKNN)-based thresholding for brain network analysis.","Here, nearest neighbor selection is based on the highest correlation between features of brain regions.","Construction of brain networks was accomplished by computing Pearson correlations between grey matter volume and white matter volume for each pair of brain regions.","Structural MRI data from 180 Parkinsons patients and 70 controls from the NIMHANS, India were analyzed.","Subtypes within Parkinsons disease were identified based on grey and white matter volume atrophy using source-based morphometric decomposition.","The loading coefficients were correlated with clinical features to discern clinical relationship with the deciphered subtypes.","Our data-mining approach revealed: Subtype A (N = 51, intermediate type), Subtype B (N = 57, mild-severe type with mild motor symptoms), and Subtype AB (N = 36, most-severe type with predominance in motor impairment).","Subtype-specific weighted matrices were binarized using MKNN-based thresholding for brain network analysis.","Permutation tests on network metrics of resulting bipartite graphs demonstrated significant group differences in betweenness centrality and participation coefficient.","The identified hubs were specific to each subtype, with some hubs conserved across different subtypes."],"url":"http://arxiv.org/abs/2403.17332v1","category":"eess.IV"}
{"created":"2024-03-26 00:35:06","title":"Sparse-Graph-Enabled Formation Planning for Large-Scale Aerial Swarms","abstract":"The formation trajectory planning using complete graphs to model collaborative constraints becomes computationally intractable as the number of drones increases due to the curse of dimensionality. To tackle this issue, this paper presents a sparse graph construction method for formation planning to realize better efficiency-performance trade-off. Firstly, a sparsification mechanism for complete graphs is designed to ensure the global rigidity of sparsified graphs, which is a necessary condition for uniquely corresponding to a geometric shape. Secondly, a good sparse graph is constructed to preserve the main structural feature of complete graphs sufficiently. Since the graph-based formation constraint is described by Laplacian matrix, the sparse graph construction problem is equivalent to submatrix selection, which has combinatorial time complexity and needs a scoring metric. Via comparative simulations, the Max-Trace matrix-revealing metric shows the promising performance. The sparse graph is integrated into the formation planning. Simulation results with 72 drones in complex environments demonstrate that when preserving 30\\% connection edges, our method has comparative formation error and recovery performance w.r.t. complete graphs. Meanwhile, the planning efficiency is improved by approximate an order of magnitude. Benchmark comparisons and ablation studies are conducted to fully validate the merits of our method.","sentences":["The formation trajectory planning using complete graphs to model collaborative constraints becomes computationally intractable as the number of drones increases due to the curse of dimensionality.","To tackle this issue, this paper presents a sparse graph construction method for formation planning to realize better efficiency-performance trade-off.","Firstly, a sparsification mechanism for complete graphs is designed to ensure the global rigidity of sparsified graphs, which is a necessary condition for uniquely corresponding to a geometric shape.","Secondly, a good sparse graph is constructed to preserve the main structural feature of complete graphs sufficiently.","Since the graph-based formation constraint is described by Laplacian matrix, the sparse graph construction problem is equivalent to submatrix selection, which has combinatorial time complexity and needs a scoring metric.","Via comparative simulations, the Max-Trace matrix-revealing metric shows the promising performance.","The sparse graph is integrated into the formation planning.","Simulation results with 72 drones in complex environments demonstrate that when preserving 30\\% connection edges, our method has comparative formation error and recovery performance w.r.t.","complete graphs.","Meanwhile, the planning efficiency is improved by approximate an order of magnitude.","Benchmark comparisons and ablation studies are conducted to fully validate the merits of our method."],"url":"http://arxiv.org/abs/2403.17288v1","category":"cs.RO"}
{"created":"2024-03-25 23:34:22","title":"Optimal Operation of Reconfigurable Active Distribution Networks Aiming at Resiliency Improvement","abstract":"As natural disasters bring about power outage and financial losses, network resiliency is an important challenge for distribution network operators (DNOs). On the other side, power loss reduction during normal operating condition is a major concern of DNOs. In this paper, optimal scheduling of active distribution network (ADN) is addressed through simultaneous minimization of power loss in normal condition and load shedding in critical condition after natural disasters. A new formulation is developed for the network reconfiguration to optimize the system operation in both normal and emergency conditions in the presence of conventional and renewable-energy-based distributed generation (DG) as well as energy storage systems (ESSs). The line flow based (LFB) algorithm is used for the AC power flow calculations, and all the developed relations have been convexified to construct a mixed-integer quadratically-constrained programming (MIQCP) optimization model. The simulations have been implemented on the IEEE 33-bus system in GAMS, and the results are investigated.","sentences":["As natural disasters bring about power outage and financial losses, network resiliency is an important challenge for distribution network operators (DNOs).","On the other side, power loss reduction during normal operating condition is a major concern of DNOs.","In this paper, optimal scheduling of active distribution network (ADN) is addressed through simultaneous minimization of power loss in normal condition and load shedding in critical condition after natural disasters.","A new formulation is developed for the network reconfiguration to optimize the system operation in both normal and emergency conditions in the presence of conventional and renewable-energy-based distributed generation (DG) as well as energy storage systems (ESSs).","The line flow based (LFB) algorithm is used for the AC power flow calculations, and all the developed relations have been convexified to construct a mixed-integer quadratically-constrained programming (MIQCP) optimization model.","The simulations have been implemented on the IEEE 33-bus system in GAMS, and the results are investigated."],"url":"http://arxiv.org/abs/2403.17272v1","category":"eess.SY"}
{"created":"2024-03-25 22:51:27","title":"Impact-Aware Bimanual Catching of Large-Momentum Objects","abstract":"This paper investigates one of the most challenging tasks in dynamic manipulation -- catching large-momentum moving objects. Beyond the realm of quasi-static manipulation, dealing with highly dynamic objects can significantly improve the robot's capability of interacting with its surrounding environment. Yet, the inevitable motion mismatch between the fast moving object and the approaching robot will result in large impulsive forces, which lead to the unstable contacts and irreversible damage to both the object and the robot. To address the above problems, we propose an online optimization framework to: 1) estimate and predict the linear and angular motion of the object; 2) search and select the optimal contact locations across every surface of the object to mitigate impact through sequential quadratic programming (SQP); 3) simultaneously optimize the end-effector motion, stiffness, and contact force for both robots using multi-mode trajectory optimization (MMTO); and 4) realise the impact-aware catching motion on the compliant robotic system based on indirect force controller. We validate the impulse distribution, contact selection, and impact-aware MMTO algorithms in simulation and demonstrate the benefits of the proposed framework in real-world experiments including catching large-momentum moving objects with well-defined motion, constrained motion and free-flying motion.","sentences":["This paper investigates one of the most challenging tasks in dynamic manipulation -- catching large-momentum moving objects.","Beyond the realm of quasi-static manipulation, dealing with highly dynamic objects can significantly improve the robot's capability of interacting with its surrounding environment.","Yet, the inevitable motion mismatch between the fast moving object and the approaching robot will result in large impulsive forces, which lead to the unstable contacts and irreversible damage to both the object and the robot.","To address the above problems, we propose an online optimization framework to: 1) estimate and predict the linear and angular motion of the object; 2) search and select the optimal contact locations across every surface of the object to mitigate impact through sequential quadratic programming (SQP); 3) simultaneously optimize the end-effector motion, stiffness, and contact force for both robots using multi-mode trajectory optimization (MMTO); and 4) realise the impact-aware catching motion on the compliant robotic system based on indirect force controller.","We validate the impulse distribution, contact selection, and impact-aware MMTO algorithms in simulation and demonstrate the benefits of the proposed framework in real-world experiments including catching large-momentum moving objects with well-defined motion, constrained motion and free-flying motion."],"url":"http://arxiv.org/abs/2403.17249v1","category":"cs.RO"}
{"created":"2024-03-25 21:58:09","title":"WIN-PDQ: A Wiener-estimator-based projection-domain quantitative SPECT method that accounts for intra-regional uptake heterogeneity","abstract":"SPECT can enable the quantification of activity uptake in lesions and at-risk organs in {\\alpha}-particle-emitting radiopharmaceutical therapies ({\\alpha}-RPTs). But this quantification is challenged by the low photon counts, complicated isotope physics, and the image-degrading effects in {\\alpha}-RPT SPECT. Thus, strategies to optimize the SPECT system and protocol designs for the task of regional uptake quantification are needed. Objectively performing this task-based optimization requires a reliable (accurate and precise) regional uptake quantification method. Conventional reconstruction-based quantification (RBQ) methods have been observed to be erroneous for {\\alpha}-RPT SPECT. Projection-domain quantification methods, which estimate regional uptake directly from SPECT projections, have demonstrated potential in providing reliable regional uptake estimates, but these methods assume constant uptake within the regions, an assumption that may not hold. To address these challenges, we propose WIN-PDQ, a Wiener-estimator-based projection-domain quantitative SPECT method. The method accounts for the heterogeneity within the regions of interest while estimating mean uptake. An early-stage evaluation of the method was conducted using 3D Monte Carlo-simulated SPECT of anthropomorphic phantoms with radium-223 uptake and lumpy-model-based intra-regional uptake heterogeneity. In this evaluation with phantoms of varying mean regional uptake and intra-regional uptake heterogeneity, the WIN-PDQ method yielded ensemble unbiased estimates and significantly outperformed both reconstruction-based and previously proposed projection-domain quantification methods. In conclusion, based on these preliminary findings, the proposed method is showing potential for estimating mean regional uptake in {\\alpha}-RPTs and towards enabling the objective task-based optimization of SPECT system and protocol designs.","sentences":["SPECT can enable the quantification of activity uptake in lesions and at-risk organs in {\\alpha}-particle-emitting radiopharmaceutical therapies ({\\alpha}-RPTs).","But this quantification is challenged by the low photon counts, complicated isotope physics, and the image-degrading effects in {\\alpha}-RPT SPECT.","Thus, strategies to optimize the SPECT system and protocol designs for the task of regional uptake quantification are needed.","Objectively performing this task-based optimization requires a reliable (accurate and precise) regional uptake quantification method.","Conventional reconstruction-based quantification (RBQ) methods have been observed to be erroneous for {\\alpha}-RPT SPECT.","Projection-domain quantification methods, which estimate regional uptake directly from SPECT projections, have demonstrated potential in providing reliable regional uptake estimates, but these methods assume constant uptake within the regions, an assumption that may not hold.","To address these challenges, we propose WIN-PDQ, a Wiener-estimator-based projection-domain quantitative SPECT method.","The method accounts for the heterogeneity within the regions of interest while estimating mean uptake.","An early-stage evaluation of the method was conducted using 3D Monte Carlo-simulated SPECT of anthropomorphic phantoms with radium-223 uptake and lumpy-model-based intra-regional uptake heterogeneity.","In this evaluation with phantoms of varying mean regional uptake and intra-regional uptake heterogeneity, the WIN-PDQ method yielded ensemble unbiased estimates and significantly outperformed both reconstruction-based and previously proposed projection-domain quantification methods.","In conclusion, based on these preliminary findings, the proposed method is showing potential for estimating mean regional uptake in {\\alpha}-RPTs and towards enabling the objective task-based optimization of SPECT system and protocol designs."],"url":"http://arxiv.org/abs/2403.17226v1","category":"physics.med-ph"}
{"created":"2024-03-25 21:50:22","title":"Physics-compliant diagonal representation of beyond-diagonal RIS","abstract":"Physics-compliant models of RIS-parametrized channels assign a load-terminated port to each RIS element. For conventional diagonal RIS (D-RIS), each auxiliary port is terminated by its own independent and individually tunable load (i.e., independent of the other auxiliary ports). For beyond-diagonal RIS (BD-RIS), the auxiliary ports are terminated by a tunable load circuit which couples the auxiliary ports to each other. Here, we point out that a physics-compliant model of the load circuit of a BD-RIS takes the same form as a physics-compliant model of a D-RIS-parametrized radio environment: a multi-port network with a subset of ports terminated by individually tunable loads (independent of each other). Consequently, we recognize that a BD-RIS-parametrized radio environment can be understood as a multi-port cascade network (i.e., the cascade of radio environment with load circuit) terminated by individually tunable loads (independent of each other). Hence, the BD-RIS problem can be mapped into the original D-RIS problem by replacing the radio environment with the cascade of radio environment and load circuit. The insight that BD-RIS can be physics-compliantly analyzed with the conventional D-RIS formalism implies that (i) the same optimization protocols as for D-RIS can be used for the BD-RIS case, and (ii) it is unclear if existing comparisons between BD-RIS and D-RIS are fair because for a fixed number of RIS elements, a BD-RIS has usually more tunable lumped elements.","sentences":["Physics-compliant models of RIS-parametrized channels assign a load-terminated port to each RIS element.","For conventional diagonal RIS (D-RIS), each auxiliary port is terminated by its own independent and individually tunable load (i.e., independent of the other auxiliary ports).","For beyond-diagonal RIS (BD-RIS), the auxiliary ports are terminated by a tunable load circuit which couples the auxiliary ports to each other.","Here, we point out that a physics-compliant model of the load circuit of a BD-RIS takes the same form as a physics-compliant model of a D-RIS-parametrized radio environment: a multi-port network with a subset of ports terminated by individually tunable loads (independent of each other).","Consequently, we recognize that a BD-RIS-parametrized radio environment can be understood as a multi-port cascade network (i.e., the cascade of radio environment with load circuit) terminated by individually tunable loads (independent of each other).","Hence, the BD-RIS problem can be mapped into the original D-RIS problem by replacing the radio environment with the cascade of radio environment and load circuit.","The insight that BD-RIS can be physics-compliantly analyzed with the conventional D-RIS formalism implies that (i) the same optimization protocols as for D-RIS can be used for the BD-RIS case, and (ii) it is unclear if existing comparisons between BD-RIS and D-RIS are fair because for a fixed number of RIS elements, a BD-RIS has usually more tunable lumped elements."],"url":"http://arxiv.org/abs/2403.17222v1","category":"eess.SP"}
{"created":"2024-03-25 21:31:39","title":"Revisiting the Mapping of Quantum Circuits: Entering the Multi-Core Era","abstract":"Quantum computing represents a paradigm shift in computation, offering the potential to solve complex problems intractable for classical computers. Although current quantum processors already consist of a few hundred of qubits, their scalability remains a significant challenge. Modular quantum computing architectures have emerged as a promising approach to scale up quantum computing systems. This paper delves into the critical aspects of distributed multi-core quantum computing, focusing on quantum circuit mapping, a fundamental task to successfully execute quantum algorithms across cores while minimizing inter-core communications. We derive the theoretical bounds on the number of non-local communications needed for random quantum circuits and introduce the Hungarian Qubit Assignment (HQA) algorithm, a multi-core mapping algorithm designed to optimize qubit assignments to cores with the aim of reducing inter-core communications. Our exhaustive evaluation of HQA against state-of-the-art circuit mapping algorithms for modular architectures reveals a $4.9\\times$ and $1.6\\times$ improvement in terms of execution time and non-local communications, respectively, compared to the best performing algorithm. HQA emerges as a very promising scalable approach for mapping quantum circuits into multi-core architectures, positioning it as a valuable tool for harnessing the potential of quantum computing at scale.","sentences":["Quantum computing represents a paradigm shift in computation, offering the potential to solve complex problems intractable for classical computers.","Although current quantum processors already consist of a few hundred of qubits, their scalability remains a significant challenge.","Modular quantum computing architectures have emerged as a promising approach to scale up quantum computing systems.","This paper delves into the critical aspects of distributed multi-core quantum computing, focusing on quantum circuit mapping, a fundamental task to successfully execute quantum algorithms across cores while minimizing inter-core communications.","We derive the theoretical bounds on the number of non-local communications needed for random quantum circuits and introduce the Hungarian Qubit Assignment (HQA) algorithm, a multi-core mapping algorithm designed to optimize qubit assignments to cores with the aim of reducing inter-core communications.","Our exhaustive evaluation of HQA against state-of-the-art circuit mapping algorithms for modular architectures reveals a $4.9\\times$ and $1.6\\times$ improvement in terms of execution time and non-local communications, respectively, compared to the best performing algorithm.","HQA emerges as a very promising scalable approach for mapping quantum circuits into multi-core architectures, positioning it as a valuable tool for harnessing the potential of quantum computing at scale."],"url":"http://arxiv.org/abs/2403.17205v1","category":"quant-ph"}
{"created":"2024-03-25 20:53:17","title":"Robust Finite-time Stabilization of Linear Systems with Limited State Quantization","abstract":"This paper investigates the robust asymptotic stabilization of a linear time-invariant (LTI) system by a static feedback with a static state quantization. It is shown that the controllable LTI system can be stabilized to zero in a finite time by means of a nonlinear feedback with a quantizer having a limited (finite) number of values (quantization seeds) even when all parameters of the controller and the quantizer are time-invariant. The control design is based on generalized homogeneity. A homogeneous spherical quantizer is introduced. The static homogeneous feedback is shown to be local (or global) finite-time stabilizer for the linear system (dependently of the system matrix). The tuning rules for both the quantizer and the feedback law are obtained in the form of Linear Matrix Inequalities (LMIs). The closed-loop system is proven to be robust with respect to some bounded matched and vanishing mismatched perturbations. Theoretical results are supported by numerical simulations. \\","sentences":["This paper investigates the robust asymptotic stabilization of a linear time-invariant (LTI) system by a static feedback with a static state quantization.","It is shown that the controllable LTI system can be stabilized to zero in a finite time by means of a nonlinear feedback with a quantizer having a limited (finite) number of values (quantization seeds) even when all parameters of the controller and the quantizer are time-invariant.","The control design is based on generalized homogeneity.","A homogeneous spherical quantizer is introduced.","The static homogeneous feedback is shown to be local (or global) finite-time stabilizer for the linear system (dependently of the system matrix).","The tuning rules for both the quantizer and the feedback law are obtained in the form of Linear Matrix Inequalities (LMIs).","The closed-loop system is proven to be robust with respect to some bounded matched and vanishing mismatched perturbations.","Theoretical results are supported by numerical simulations.","\\"],"url":"http://arxiv.org/abs/2403.17184v1","category":"math.OC"}
{"created":"2024-03-25 20:37:54","title":"Generation of genuine multipartite entangled states via indistinguishability of identical particles","abstract":"Indistinguishability of identical particles is a resource for quantum information processing and has been utilized to generate entanglement from independent particles that spatially overlap only at the detection stage. Here we provide a general controllable scheme capable of generating, from a pure product state of $N$ qubits, a comprehensive class of multipartite entangled states, including W, Dicke, GHZ, and cluster states with both bosonic and fermionic statistics within the framework of spatially localized operations and classical communication (sLOCC). Using graph-based representations of the sLOCC framework, we translate the generation schemes of specific entangled states into colored, complex, and weighted digraphs, each corresponding to a given experimental setup. This graph-theoretical approach allows for precise targeting of particular multipartite states, exploration of diverse generation schemes, and optimization of generation efficiency. Our results demonstrate that the indistinguishability of identical graph nodes in quantum networks offers useful perspectives for photonic technology.","sentences":["Indistinguishability of identical particles is a resource for quantum information processing and has been utilized to generate entanglement from independent particles that spatially overlap only at the detection stage.","Here we provide a general controllable scheme capable of generating, from a pure product state of $N$ qubits, a comprehensive class of multipartite entangled states, including W, Dicke, GHZ, and cluster states with both bosonic and fermionic statistics within the framework of spatially localized operations and classical communication (sLOCC).","Using graph-based representations of the sLOCC framework, we translate the generation schemes of specific entangled states into colored, complex, and weighted digraphs, each corresponding to a given experimental setup.","This graph-theoretical approach allows for precise targeting of particular multipartite states, exploration of diverse generation schemes, and optimization of generation efficiency.","Our results demonstrate that the indistinguishability of identical graph nodes in quantum networks offers useful perspectives for photonic technology."],"url":"http://arxiv.org/abs/2403.17171v1","category":"quant-ph"}
{"created":"2024-03-25 20:18:12","title":"Multi-Contact Inertial Estimation and Localization in Legged Robots","abstract":"Optimal estimation is a promising tool for multi-contact inertial estimation and localization. To harness its advantages in robotics, it is crucial to solve these large and challenging optimization problems efficiently. To tackle this, we (i) develop a multiple-shooting solver that exploits both temporal and parametric structures through a parametrized Riccati recursion. Additionally, we (ii) propose an inertial local manifold that ensures its full physical consistency. It also enhances convergence compared to the singularity-free log-Cholesky approach. To handle its singularities, we (iii) introduce a nullspace approach in our optimal estimation solver. We (iv) finally develop the analytical derivatives of contact dynamics for both inertial parametrizations. Our framework can successfully solve estimation problems for complex maneuvers such as brachiation in humanoids. We demonstrate its numerical capabilities across various robotics tasks and its benefits in experimental trials with the Go1 robot.","sentences":["Optimal estimation is a promising tool for multi-contact inertial estimation and localization.","To harness its advantages in robotics, it is crucial to solve these large and challenging optimization problems efficiently.","To tackle this, we (i) develop a multiple-shooting solver that exploits both temporal and parametric structures through a parametrized Riccati recursion.","Additionally, we (ii) propose an inertial local manifold that ensures its full physical consistency.","It also enhances convergence compared to the singularity-free log-Cholesky approach.","To handle its singularities, we (iii) introduce a nullspace approach in our optimal estimation solver.","We (iv) finally develop the analytical derivatives of contact dynamics for both inertial parametrizations.","Our framework can successfully solve estimation problems for complex maneuvers such as brachiation in humanoids.","We demonstrate its numerical capabilities across various robotics tasks and its benefits in experimental trials with the Go1 robot."],"url":"http://arxiv.org/abs/2403.17161v1","category":"cs.RO"}
{"created":"2024-03-25 20:17:04","title":"CYGENT: A cybersecurity conversational agent with log summarization powered by GPT-3","abstract":"In response to the escalating cyber-attacks in the modern IT and IoT landscape, we developed CYGENT, a conversational agent framework powered by GPT-3.5 turbo model, designed to aid system administrators in ensuring optimal performance and uninterrupted resource availability. This study focuses on fine-tuning GPT-3 models for cybersecurity tasks, including conversational AI and generative AI tailored specifically for cybersecurity operations. CYGENT assists users by providing cybersecurity information, analyzing and summarizing uploaded log files, detecting specific events, and delivering essential instructions. The conversational agent was developed based on the GPT-3.5 turbo model. We fine-tuned and validated summarizer models (GPT3) using manually generated data points. Using this approach, we achieved a BERTscore of over 97%, indicating GPT-3's enhanced capability in summarizing log files into human-readable formats and providing necessary information to users. Furthermore, we conducted a comparative analysis of GPT-3 models with other Large Language Models (LLMs), including CodeT5-small, CodeT5-base, and CodeT5-base-multi-sum, with the objective of analyzing log analysis techniques. Our analysis consistently demonstrated that Davinci (GPT-3) model outperformed all other LLMs, showcasing higher performance. These findings are crucial for improving human comprehension of logs, particularly in light of the increasing numbers of IoT devices. Additionally, our research suggests that the CodeT5-base-multi-sum model exhibits comparable performance to Davinci to some extent in summarizing logs, indicating its potential as an offline model for this task.","sentences":["In response to the escalating cyber-attacks in the modern IT and IoT landscape, we developed CYGENT, a conversational agent framework powered by GPT-3.5 turbo model, designed to aid system administrators in ensuring optimal performance and uninterrupted resource availability.","This study focuses on fine-tuning GPT-3 models for cybersecurity tasks, including conversational AI and generative AI tailored specifically for cybersecurity operations.","CYGENT assists users by providing cybersecurity information, analyzing and summarizing uploaded log files, detecting specific events, and delivering essential instructions.","The conversational agent was developed based on the GPT-3.5 turbo model.","We fine-tuned and validated summarizer models (GPT3) using manually generated data points.","Using this approach, we achieved a BERTscore of over 97%, indicating GPT-3's enhanced capability in summarizing log files into human-readable formats and providing necessary information to users.","Furthermore, we conducted a comparative analysis of GPT-3 models with other Large Language Models (LLMs), including CodeT5-small, CodeT5-base, and CodeT5-base-multi-sum, with the objective of analyzing log analysis techniques.","Our analysis consistently demonstrated that Davinci (GPT-3) model outperformed all other LLMs, showcasing higher performance.","These findings are crucial for improving human comprehension of logs, particularly in light of the increasing numbers of IoT devices.","Additionally, our research suggests that the CodeT5-base-multi-sum model exhibits comparable performance to Davinci to some extent in summarizing logs, indicating its potential as an offline model for this task."],"url":"http://arxiv.org/abs/2403.17160v1","category":"cs.CR"}
{"created":"2024-03-25 20:09:46","title":"On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance","abstract":"Deciding what combination of operators to use across the Edge AI tiers to achieve specific latency and model performance requirements is an open question for MLOps engineers. This study aims to empirically assess the accuracy vs inference time trade-off of different black-box Edge AI deployment strategies, i.e., combinations of deployment operators and deployment tiers. In this paper, we conduct inference experiments involving 3 deployment operators (i.e., Partitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile, Edge, Cloud) and their combinations on four widely used Computer-Vision models to investigate the optimal strategies from the point of view of MLOps developers. Our findings suggest that Edge deployment using the hybrid Quantization + Early Exit operator could be preferred over non-hybrid operators (Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency is a concern at medium accuracy loss. However, when minimizing accuracy loss is a concern, MLOps engineers should prefer using only a Quantization operator on edge at a latency reduction or increase, respectively over the Early Exit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge) operators. In scenarios constrained by Mobile CPU/RAM resources, a preference for Partitioning across mobile and edge tiers is observed over mobile deployment. For models with smaller input data samples (such as FCN), a network-constrained cloud deployment can also be a better alternative than Mobile/Edge deployment and Partitioning strategies. For models with large input data samples (ResNet, ResNext, DUC), an edge tier having higher network/computational capabilities than Cloud/Mobile can be a more viable option than Partitioning and Mobile/Cloud deployment strategies.","sentences":["Deciding what combination of operators to use across the Edge AI tiers to achieve specific latency and model performance requirements is an open question for MLOps engineers.","This study aims to empirically assess the accuracy vs inference time trade-off of different black-box Edge AI deployment strategies, i.e., combinations of deployment operators and deployment tiers.","In this paper, we conduct inference experiments involving 3 deployment operators (i.e., Partitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile, Edge, Cloud) and their combinations on four widely used Computer-Vision models to investigate the optimal strategies from the point of view of MLOps developers.","Our findings suggest that Edge deployment using the hybrid Quantization + Early Exit operator could be preferred over non-hybrid operators (Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency is a concern at medium accuracy loss.","However, when minimizing accuracy loss is a concern, MLOps engineers should prefer using only a Quantization operator on edge at a latency reduction or increase, respectively over the Early Exit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge) operators.","In scenarios constrained by Mobile CPU/RAM resources, a preference for Partitioning across mobile and edge tiers is observed over mobile deployment.","For models with smaller input data samples (such as FCN), a network-constrained cloud deployment can also be a better alternative than Mobile/Edge deployment and Partitioning strategies.","For models with large input data samples (ResNet, ResNext, DUC), an edge tier having higher network/computational capabilities than Cloud/Mobile can be a more viable option than Partitioning and Mobile/Cloud deployment strategies."],"url":"http://arxiv.org/abs/2403.17154v1","category":"cs.SE"}
{"created":"2024-03-25 19:16:16","title":"A Personalized Predictive Model that Jointly Optimizes Discrimination and Calibration","abstract":"Precision medicine is accelerating rapidly in the field of health research. This includes fitting predictive models for individual patients based on patient similarity in an attempt to improve model performance. We propose an algorithm which fits a personalized predictive model (PPM) using an optimal size of a similar subpopulation that jointly optimizes model discrimination and calibration, as it is criticized that calibration is not assessed nearly as often as discrimination despite poorly calibrated models being potentially misleading. We define a mixture loss function that considers model discrimination and calibration, and allows for flexibility in emphasizing one performance measure over another. We empirically show that the relationship between the size of subpopulation and calibration is quadratic, which motivates the development of our jointly optimized model. We also investigate the effect of within-population patient weighting on performance and conclude that the size of subpopulation has a larger effect on the predictive performance of the PPM compared to the choice of weight function.","sentences":["Precision medicine is accelerating rapidly in the field of health research.","This includes fitting predictive models for individual patients based on patient similarity in an attempt to improve model performance.","We propose an algorithm which fits a personalized predictive model (PPM) using an optimal size of a similar subpopulation that jointly optimizes model discrimination and calibration, as it is criticized that calibration is not assessed nearly as often as discrimination despite poorly calibrated models being potentially misleading.","We define a mixture loss function that considers model discrimination and calibration, and allows for flexibility in emphasizing one performance measure over another.","We empirically show that the relationship between the size of subpopulation and calibration is quadratic, which motivates the development of our jointly optimized model.","We also investigate the effect of within-population patient weighting on performance and conclude that the size of subpopulation has a larger effect on the predictive performance of the PPM compared to the choice of weight function."],"url":"http://arxiv.org/abs/2403.17132v1","category":"stat.ME"}
{"created":"2024-03-25 19:04:04","title":"6D Movable Antenna Enhanced Wireless Network Via Discrete Position and Rotation Optimization","abstract":"Six-dimensional movable antenna (6DMA) is an effective approach to improve wireless network capacity by adjusting the 3D positions and 3D rotations of distributed antenna surfaces based on the users' spatial distribution and statistical channel information. Although continuously positioning/rotating 6DMA surfaces can achieve the greatest flexibility and thus the highest capacity improvement, it is difficult to implement due to the discrete movement constraints of practical stepper motors. Thus, in this paper, we consider a 6DMA-aided base station (BS) with only a finite number of possible discrete positions and rotations for the 6DMA surfaces. We aim to maximize the average network capacity for random numbers of users at random locations by jointly optimizing the 3D positions and 3D rotations of multiple 6DMA surfaces at the BS subject to discrete movement constraints. In particular, we consider the practical cases with and without statistical channel knowledge of the users, and propose corresponding offline and online optimization algorithms, by leveraging the Monte Carlo and conditional sample mean (CSM) methods, respectively. Simulation results verify the effectiveness of our proposed offline and online algorithms for discrete position/rotation optimization of 6DMA surfaces as compared to various benchmark schemes with fixed-position antennas (FPAs) and 6DMAs with limited movability. It is shown that 6DMA-BS can significantly enhance wireless network capacity, even under discrete position/rotation constraints, by exploiting the spatial distribution characteristics of the users.","sentences":["Six-dimensional movable antenna (6DMA) is an effective approach to improve wireless network capacity by adjusting the 3D positions and 3D rotations of distributed antenna surfaces based on the users' spatial distribution and statistical channel information.","Although continuously positioning/rotating 6DMA surfaces can achieve the greatest flexibility and thus the highest capacity improvement, it is difficult to implement due to the discrete movement constraints of practical stepper motors.","Thus, in this paper, we consider a 6DMA-aided base station (BS) with only a finite number of possible discrete positions and rotations for the 6DMA surfaces.","We aim to maximize the average network capacity for random numbers of users at random locations by jointly optimizing the 3D positions and 3D rotations of multiple 6DMA surfaces at the BS subject to discrete movement constraints.","In particular, we consider the practical cases with and without statistical channel knowledge of the users, and propose corresponding offline and online optimization algorithms, by leveraging the Monte Carlo and conditional sample mean (CSM) methods, respectively.","Simulation results verify the effectiveness of our proposed offline and online algorithms for discrete position/rotation optimization of 6DMA surfaces as compared to various benchmark schemes with fixed-position antennas (FPAs) and 6DMAs with limited movability.","It is shown that 6DMA-BS can significantly enhance wireless network capacity, even under discrete position/rotation constraints, by exploiting the spatial distribution characteristics of the users."],"url":"http://arxiv.org/abs/2403.17122v1","category":"cs.IT"}
{"created":"2024-03-25 19:03:58","title":"High-dimensional Factor Analysis for Network-linked Data","abstract":"Factor analysis is a widely used statistical tool in many scientific disciplines, such as psychology, economics, and sociology. As observations linked by networks become increasingly common, incorporating network structures into factor analysis remains an open problem. In this paper, we focus on high-dimensional factor analysis involving network-connected observations, and propose a generalized factor model with latent factors that account for both the network structure and the dependence structure among high-dimensional variables. These latent factors can be shared by the high-dimensional variables and the network, or exclusively applied to either of them. We develop a computationally efficient estimation procedure and establish asymptotic inferential theories. Notably, we show that by borrowing information from the network, the proposed estimator of the factor loading matrix achieves optimal asymptotic variance under much milder identifiability constraints than existing literature. Furthermore, we develop a hypothesis testing procedure to tackle the challenge of discerning the shared and individual latent factors' structure. The finite sample performance of the proposed method is demonstrated through simulation studies and a real-world dataset involving a statistician co-authorship network.","sentences":["Factor analysis is a widely used statistical tool in many scientific disciplines, such as psychology, economics, and sociology.","As observations linked by networks become increasingly common, incorporating network structures into factor analysis remains an open problem.","In this paper, we focus on high-dimensional factor analysis involving network-connected observations, and propose a generalized factor model with latent factors that account for both the network structure and the dependence structure among high-dimensional variables.","These latent factors can be shared by the high-dimensional variables and the network, or exclusively applied to either of them.","We develop a computationally efficient estimation procedure and establish asymptotic inferential theories.","Notably, we show that by borrowing information from the network, the proposed estimator of the factor loading matrix achieves optimal asymptotic variance under much milder identifiability constraints than existing literature.","Furthermore, we develop a hypothesis testing procedure to tackle the challenge of discerning the shared and individual latent factors' structure.","The finite sample performance of the proposed method is demonstrated through simulation studies and a real-world dataset involving a statistician co-authorship network."],"url":"http://arxiv.org/abs/2403.17121v1","category":"stat.ME"}
{"created":"2024-03-25 18:49:12","title":"Vision-Based Dexterous Motion Planning by Dynamic Movement Primitives with Human Hand Demonstration","abstract":"This paper proposes a vision-based framework for a 7-degree-of-freedom robotic manipulator, with the primary objective of facilitating its capacity to acquire information from human hand demonstrations for the execution of dexterous pick-and-place tasks. Most existing works only focus on the position demonstration without considering the orientations. In this paper, by employing a single depth camera, MediaPipe is applied to generate the three-dimensional coordinates of a human hand, thereby comprehensively recording the hand's motion, encompassing the trajectory of the wrist, orientation of the hand, and the grasp motion. A mean filter is applied during data pre-processing to smooth the raw data. The demonstration is designed to pick up an object at a specific angle, navigate around obstacles in its path and subsequently, deposit it within a sloped container. The robotic system demonstrates its learning capabilities, facilitated by the implementation of Dynamic Movement Primitives, enabling the assimilation of user actions into its trajectories with different start and end poi","sentences":["This paper proposes a vision-based framework for a 7-degree-of-freedom robotic manipulator, with the primary objective of facilitating its capacity to acquire information from human hand demonstrations for the execution of dexterous pick-and-place tasks.","Most existing works only focus on the position demonstration without considering the orientations.","In this paper, by employing a single depth camera, MediaPipe is applied to generate the three-dimensional coordinates of a human hand, thereby comprehensively recording the hand's motion, encompassing the trajectory of the wrist, orientation of the hand, and the grasp motion.","A mean filter is applied during data pre-processing to smooth the raw data.","The demonstration is designed to pick up an object at a specific angle, navigate around obstacles in its path and subsequently, deposit it within a sloped container.","The robotic system demonstrates its learning capabilities, facilitated by the implementation of Dynamic Movement Primitives, enabling the assimilation of user actions into its trajectories with different start and end poi"],"url":"http://arxiv.org/abs/2403.17111v1","category":"cs.RO"}
{"created":"2024-03-25 18:45:31","title":"Design Principles of Dynamic Resource Management for High-Performance Parallel Programming Models","abstract":"With Dynamic Resource Management (DRM) the resources assigned to a job can be changed dynamically during its execution. From the system's perspective, DRM opens a new level of flexibility in resource allocation and job scheduling and therefore has the potential to improve system efficiency metrics such as the utilization rate, job throughput, energy efficiency, and responsiveness. From the application perspective, users can tailor the resources they request to their needs offering potential optimizations in queuing time or charged costs. Despite these obvious advantages and many attempts over the last decade to establish DRM in HPC, it remains a concept discussed in academia rather than being successfully deployed on production systems. This stems from the fact that support for DRM requires changes in all the layers of the HPC system software stack including applications, programming models, process managers, and resource management software, as well as an extensive and holistic co-design process to establish new techniques and policies for scheduling and resource optimization. In this work, we therefore start with the assumption that resources are accessible by processes executed either on them (e.g., on CPU) or controlling them (e.g., GPU-offloading). Then, the overall DRM problem can be decomposed into dynamic process management (DPM) and dynamic resource mapping or allocation (DRA). The former determines which processes (or which change in processes) must be managed and the latter identifies the resources where they will be executed. The interfaces for such \\mbox{DPM/DPA} in these layers need to be standardized, which requires a careful design to be interoperable while providing high flexibility. Based on a survey of existing approaches we propose design principles, that form the basis of a holistic approach to DMR in HPC and provide a prototype implementation using MPI.","sentences":["With Dynamic Resource Management (DRM) the resources assigned to a job can be changed dynamically during its execution.","From the system's perspective, DRM opens a new level of flexibility in resource allocation and job scheduling and therefore has the potential to improve system efficiency metrics such as the utilization rate, job throughput, energy efficiency, and responsiveness.","From the application perspective, users can tailor the resources they request to their needs offering potential optimizations in queuing time or charged costs.","Despite these obvious advantages and many attempts over the last decade to establish DRM in HPC, it remains a concept discussed in academia rather than being successfully deployed on production systems.","This stems from the fact that support for DRM requires changes in all the layers of the HPC system software stack including applications, programming models, process managers, and resource management software, as well as an extensive and holistic co-design process to establish new techniques and policies for scheduling and resource optimization.","In this work, we therefore start with the assumption that resources are accessible by processes executed either on them (e.g., on CPU) or controlling them (e.g., GPU-offloading).","Then, the overall DRM problem can be decomposed into dynamic process management (DPM) and dynamic resource mapping or allocation (DRA).","The former determines which processes (or which change in processes) must be managed and the latter identifies the resources where they will be executed.","The interfaces for such \\mbox{DPM/DPA} in these layers need to be standardized, which requires a careful design to be interoperable while providing high flexibility.","Based on a survey of existing approaches we propose design principles, that form the basis of a holistic approach to DMR in HPC and provide a prototype implementation using MPI."],"url":"http://arxiv.org/abs/2403.17107v1","category":"cs.DC"}
{"created":"2024-03-25 18:38:47","title":"Practical Acceleration of the Condat-V\u0169 Algorithm","abstract":"The Condat-V\\~u algorithm is a widely used primal-dual method for optimizing composite objectives of three functions. Several algorithms for optimizing composite objectives of two functions are special cases of Condat-V\\~u, including proximal gradient descent (PGD). It is well-known that PGD exhibits suboptimal performance, and a simple adjustment to PGD can accelerate its convergence rate from $\\mathcal{O}(1/T)$ to $\\mathcal{O}(1/T^2)$ on convex objectives, and this accelerated rate is optimal. In this work, we show that a simple adjustment to the Condat-V\\~u algorithm allows it to recover accelerated PGD (APGD) as a special case, instead of PGD. We prove that this accelerated Condat--V\\~u algorithm achieves optimal convergence rates and significantly outperforms the traditional Condat-V\\~u algorithm in regimes where the Condat--V\\~u algorithm approximates the dynamics of PGD. We demonstrate the effectiveness of our approach in various applications in machine learning and computational imaging.","sentences":["The Condat-V\\~u algorithm is a widely used primal-dual method for optimizing composite objectives of three functions.","Several algorithms for optimizing composite objectives of two functions are special cases of Condat-V\\~u, including proximal gradient descent (PGD).","It is well-known that PGD exhibits suboptimal performance, and a simple adjustment to PGD can accelerate its convergence rate from $\\mathcal{O}(1/T)$ to $\\mathcal{O}(1/T^2)$ on convex objectives, and this accelerated rate is optimal.","In this work, we show that a simple adjustment to the Condat-V\\~u algorithm allows it to recover accelerated PGD (APGD) as a special case, instead of PGD.","We prove that this accelerated Condat--V\\~u algorithm achieves optimal convergence rates and significantly outperforms the traditional Condat-V\\~u algorithm in regimes where the Condat--V\\~u algorithm approximates the dynamics of PGD.","We demonstrate the effectiveness of our approach in various applications in machine learning and computational imaging."],"url":"http://arxiv.org/abs/2403.17100v1","category":"math.OC"}
{"created":"2024-03-25 18:16:29","title":"GGDMiner - Discovery of Graph Generating Dependencies for Graph Data Profiling","abstract":"With the increasing use of graph-structured data, there is also increasing interest in investigating graph data dependencies and their applications, e.g., in graph data profiling. Graph Generating Dependencies (GGDs) are a class of dependencies for property graphs that can express the relation between different graph patterns and constraints based on their attribute similarities. Rich syntax and semantics of GGDs make them a good candidate for graph data profiling. Nonetheless, GGDs are difficult to define manually, especially when there are no data experts available. In this paper, we propose GGDMiner, a framework for discovering approximate GGDs from graph data automatically, with the intention of profiling graph data through GGDs for the user. GGDMiner has three main steps: (1) pre-processing, (2) candidate generation, and, (3) GGD extraction. To optimize memory consumption and execution time, GGDMiner uses a factorized representation of each discovered graph pattern, called Answer Graph. Our results show that the discovered set of GGDs can give an overview about the input graph, both schema level information and also correlations between the graph patterns and attributes.","sentences":["With the increasing use of graph-structured data, there is also increasing interest in investigating graph data dependencies and their applications, e.g., in graph data profiling.","Graph Generating Dependencies (GGDs) are a class of dependencies for property graphs that can express the relation between different graph patterns and constraints based on their attribute similarities.","Rich syntax and semantics of GGDs make them a good candidate for graph data profiling.","Nonetheless, GGDs are difficult to define manually, especially when there are no data experts available.","In this paper, we propose GGDMiner, a framework for discovering approximate GGDs from graph data automatically, with the intention of profiling graph data through GGDs for the user.","GGDMiner has three main steps: (1) pre-processing, (2) candidate generation, and, (3) GGD extraction.","To optimize memory consumption and execution time, GGDMiner uses a factorized representation of each discovered graph pattern, called Answer Graph.","Our results show that the discovered set of GGDs can give an overview about the input graph, both schema level information and also correlations between the graph patterns and attributes."],"url":"http://arxiv.org/abs/2403.17082v1","category":"cs.DB"}
{"created":"2024-03-25 18:05:03","title":"Non-stationary Bandits with Habituation and Recover Dynamics and Knapsack Constraints","abstract":"Multi-armed bandit models have proven to be useful in modeling many real world problems in the areas of control and sequential decision making with partial information. However, in many scenarios, such as those prevalent in healthcare and operations management, the decision maker's expected reward will decrease if an action is selected too frequently while it may recover if they abstain from selecting this action. This scenario is further complicated when choosing a particular action also expends a random amount of a limited resource where the distribution is also initially unknown to the decision maker. In this paper we study a class of models that address this setting that we call reducing or gaining unknown efficacy bandits with stochastic knapsack constraints (ROGUEwK). We propose a combination upper confidence bound (UCB) and lower confidence bound (LCB) approximation algorithm for optimizing this model. Our algorithm chooses which action to play at each time point by solving a linear program (LP) with the UCB for the average rewards and LCB for the average costs as inputs. We show that the regret of our algorithm is sub-linear as a function of time and total constraint budget when compared to a dynamic oracle. We validate the performance of our algorithm against existing state of the art non-stationary and knapsack bandit approaches in a simulation study and show that our methods are able to on average achieve a 13% improvement in terms of total reward.","sentences":["Multi-armed bandit models have proven to be useful in modeling many real world problems in the areas of control and sequential decision making with partial information.","However, in many scenarios, such as those prevalent in healthcare and operations management, the decision maker's expected reward will decrease if an action is selected too frequently while it may recover if they abstain from selecting this action.","This scenario is further complicated when choosing a particular action also expends a random amount of a limited resource where the distribution is also initially unknown to the decision maker.","In this paper we study a class of models that address this setting that we call reducing or gaining unknown efficacy bandits with stochastic knapsack constraints (ROGUEwK).","We propose a combination upper confidence bound (UCB) and lower confidence bound (LCB) approximation algorithm for optimizing this model.","Our algorithm chooses which action to play at each time point by solving a linear program (LP) with the UCB for the average rewards and LCB for the average costs as inputs.","We show that the regret of our algorithm is sub-linear as a function of time and total constraint budget when compared to a dynamic oracle.","We validate the performance of our algorithm against existing state of the art non-stationary and knapsack bandit approaches in a simulation study and show that our methods are able to on average achieve a 13% improvement in terms of total reward."],"url":"http://arxiv.org/abs/2403.17073v1","category":"math.OC"}
{"created":"2024-03-25 18:03:50","title":"Trajectory Optimization with Global Yaw Parameterization for Field-of-View Constrained Autonomous Flight","abstract":"Trajectory generation for quadrotors with limited field-of-view sensors has numerous applications such as aerial exploration, coverage, inspection, videography, and target tracking. Most previous works simplify the task of optimizing yaw trajectories by either aligning the heading of the robot with its velocity, or potentially restricting the feasible space of candidate trajectories by using a limited yaw domain to circumvent angular singularities. In this paper, we propose a novel \\textit{global} yaw parameterization method for trajectory optimization that allows a 360-degree yaw variation as demanded by the underlying algorithm. This approach effectively bypasses inherent singularities by including supplementary quadratic constraints and transforming the final decision variables into the desired state representation. This method significantly reduces the needed control effort, and improves optimization feasibility. Furthermore, we apply the method to several examples of different applications that require jointly optimizing over both the yaw and position trajectories. Ultimately, we present a comprehensive numerical analysis and evaluation of our proposed method in both simulation and real-world experiments.","sentences":["Trajectory generation for quadrotors with limited field-of-view sensors has numerous applications such as aerial exploration, coverage, inspection, videography, and target tracking.","Most previous works simplify the task of optimizing yaw trajectories by either aligning the heading of the robot with its velocity, or potentially restricting the feasible space of candidate trajectories by using a limited yaw domain to circumvent angular singularities.","In this paper, we propose a novel \\textit{global} yaw parameterization method for trajectory optimization that allows a 360-degree yaw variation as demanded by the underlying algorithm.","This approach effectively bypasses inherent singularities by including supplementary quadratic constraints and transforming the final decision variables into the desired state representation.","This method significantly reduces the needed control effort, and improves optimization feasibility.","Furthermore, we apply the method to several examples of different applications that require jointly optimizing over both the yaw and position trajectories.","Ultimately, we present a comprehensive numerical analysis and evaluation of our proposed method in both simulation and real-world experiments."],"url":"http://arxiv.org/abs/2403.17067v1","category":"cs.RO"}
{"created":"2024-03-25 17:55:52","title":"Comp4D: LLM-Guided Compositional 4D Scene Generation","abstract":"Recent advancements in diffusion models for 2D and 3D content creation have sparked a surge of interest in generating 4D content. However, the scarcity of 3D scene datasets constrains current methodologies to primarily object-centric generation. To overcome this limitation, we present Comp4D, a novel framework for Compositional 4D Generation. Unlike conventional methods that generate a singular 4D representation of the entire scene, Comp4D innovatively constructs each 4D object within the scene separately. Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories. It then constructs the compositional 4D scene by accurately positioning these objects along their designated paths. To refine the scene, our method employs a compositional score distillation technique guided by the pre-defined trajectories, utilizing pre-trained diffusion models across text-to-image, text-to-video, and text-to-3D domains. Extensive experiments demonstrate our outstanding 4D content creation capability compared to prior arts, showcasing superior visual quality, motion fidelity, and enhanced object interactions.","sentences":["Recent advancements in diffusion models for 2D and 3D content creation have sparked a surge of interest in generating 4D content.","However, the scarcity of 3D scene datasets constrains current methodologies to primarily object-centric generation.","To overcome this limitation, we present Comp4D, a novel framework for Compositional 4D Generation.","Unlike conventional methods that generate a singular 4D representation of the entire scene, Comp4D innovatively constructs each 4D object within the scene separately.","Utilizing Large Language Models (LLMs), the framework begins by decomposing an input text prompt into distinct entities and maps out their trajectories.","It then constructs the compositional 4D scene by accurately positioning these objects along their designated paths.","To refine the scene, our method employs a compositional score distillation technique guided by the pre-defined trajectories, utilizing pre-trained diffusion models across text-to-image, text-to-video, and text-to-3D domains.","Extensive experiments demonstrate our outstanding 4D content creation capability compared to prior arts, showcasing superior visual quality, motion fidelity, and enhanced object interactions."],"url":"http://arxiv.org/abs/2403.16993v1","category":"cs.CV"}
{"created":"2024-03-25 17:43:50","title":"State-Augmented Linear Games with Antagonistic Error for High-Dimensional, Nonlinear Hamilton-Jacobi Reachability","abstract":"Hamilton-Jacobi Reachability (HJR) is a popular method for analyzing the liveness and safety of a dynamical system with bounded control and disturbance. The corresponding HJ value function offers a robust controller and characterizes the reachable sets, but is traditionally solved with Dynamic Programming (DP) and limited to systems of dimension less than six. Recently, the space-parallelizeable, generalized Hopf formula has been shown to also solve the HJ value with a nearly three-log increase in dimension limit, but is limited to linear systems. To extend this potential, we demonstrate how state-augmented (SA) spaces, which are well-known for their improved linearization accuracy, may be used to solve tighter, conservative approximations of the value function with any linear model in this SA space. Namely, we show that with a representation of the true dynamics in the SA space, a series of inequalities confirms that the value of a SA linear game with antagonistic error is a conservative envelope of the true value function. It follows that if the optimal controller for the HJ SA linear game with error may succeed, it will also succeed in the true system. Unlike previous methods, this result offers the ability to safely approximate reachable sets and their corresponding controllers with the Hopf formula in a non-convex manner. Finally, we demonstrate this in the slow manifold system for clarity, and in the controlled Van der Pol system with different lifting functions.","sentences":["Hamilton-Jacobi Reachability (HJR) is a popular method for analyzing the liveness and safety of a dynamical system with bounded control and disturbance.","The corresponding HJ value function offers a robust controller and characterizes the reachable sets, but is traditionally solved with Dynamic Programming (DP) and limited to systems of dimension less than six.","Recently, the space-parallelizeable, generalized Hopf formula has been shown to also solve the HJ value with a nearly three-log increase in dimension limit, but is limited to linear systems.","To extend this potential, we demonstrate how state-augmented (SA) spaces, which are well-known for their improved linearization accuracy, may be used to solve tighter, conservative approximations of the value function with any linear model in this SA space.","Namely, we show that with a representation of the true dynamics in the SA space, a series of inequalities confirms that the value of a SA linear game with antagonistic error is a conservative envelope of the true value function.","It follows that if the optimal controller for the HJ SA linear game with error may succeed, it will also succeed in the true system.","Unlike previous methods, this result offers the ability to safely approximate reachable sets and their corresponding controllers with the Hopf formula in a non-convex manner.","Finally, we demonstrate this in the slow manifold system for clarity, and in the controlled Van der Pol system with different lifting functions."],"url":"http://arxiv.org/abs/2403.16982v1","category":"eess.SY"}
{"created":"2024-03-25 17:41:52","title":"An Optimal Solution to Infinite Horizon Nonlinear Control Problems: Part II","abstract":"This paper considers the infinite horizon optimal control problem for nonlinear systems. Under the condition of nonlinear controllability of the system to any terminal set containing the origin and forward invariance of the terminal set, we establish a regularized solution approach consisting of a ``finite free final time\" optimal transfer problem to the terminal set which renders the set globally asymptotically stable. Further, we show that the approximations converge to the optimal infinite horizon cost as the size of the terminal set decreases to zero. We also perform the analysis for the discounted problem and show that the terminal set is asymptotically stable only for a subset of the state space and not globally. The theory is empirically evaluated on various nonholonomic robotic systems to show that the cost of our approximate problem converges and the transfer time into the terminal set is dependent on the initial state of the system, necessitating the free final time formulation.","sentences":["This paper considers the infinite horizon optimal control problem for nonlinear systems.","Under the condition of nonlinear controllability of the system to any terminal set containing the origin and forward invariance of the terminal set, we establish a regularized solution approach consisting of a ``finite free final time\" optimal transfer problem to the terminal set which renders the set globally asymptotically stable.","Further, we show that the approximations converge to the optimal infinite horizon cost as the size of the terminal set decreases to zero.","We also perform the analysis for the discounted problem and show that the terminal set is asymptotically stable only for a subset of the state space and not globally.","The theory is empirically evaluated on various nonholonomic robotic systems to show that the cost of our approximate problem converges and the transfer time into the terminal set is dependent on the initial state of the system, necessitating the free final time formulation."],"url":"http://arxiv.org/abs/2403.16979v1","category":"math.OC"}
{"created":"2024-03-25 17:40:35","title":"Unconditionally positivity-preserving approximations of the Ait-Sahalia type model Explicit Milstein-type schemes","abstract":"The present article aims to design and analyze efficient first-order strong schemes for a generalized A\\\"{i}t-Sahalia type model arising in mathematical finance and evolving in a positive domain $(0, \\infty)$, which possesses a diffusion term with superlinear growth and a highly nonlinear drift that blows up at the origin. Such a complicated structure of the model unavoidably causes essential difficulties in the construction and convergence analysis of time discretizations. By incorporating implicitness in the term $\\alpha_{-1} x^{-1}$ and a corrective mapping $\\Phi_h$ in the recursion, we develop a novel class of explicit and unconditionally positivity-preserving (i.e., for any step-size $h>0$) Milstein-type schemes for the underlying model. In both non-critical and general critical cases, we introduce a novel approach to analyze mean-square error bounds of the novel schemes, without relying on a priori high-order moment bounds of the numerical approximations. The expected order-one mean-square convergence is attained for the proposed scheme. The above theoretical guarantee can be used to justify the optimal complexity of the Multilevel Monte Carlo method. Numerical experiments are finally provided to verify the theoretical findings.","sentences":["The present article aims to design and analyze efficient first-order strong schemes for a generalized A\\\"{i}t-Sahalia type model arising in mathematical finance and evolving in a positive domain $(0, \\infty)$, which possesses a diffusion term with superlinear growth and a highly nonlinear drift that blows up at the origin.","Such a complicated structure of the model unavoidably causes essential difficulties in the construction and convergence analysis of time discretizations.","By incorporating implicitness in the term $\\alpha_{-1} x^{-1}$ and a corrective mapping $\\Phi_h$ in the recursion, we develop a novel class of explicit and unconditionally positivity-preserving (i.e., for any step-size $h>0$) Milstein-type schemes for the underlying model.","In both non-critical and general critical cases, we introduce a novel approach to analyze mean-square error bounds of the novel schemes, without relying on a priori high-order moment bounds of the numerical approximations.","The expected order-one mean-square convergence is attained for the proposed scheme.","The above theoretical guarantee can be used to justify the optimal complexity of the Multilevel Monte Carlo method.","Numerical experiments are finally provided to verify the theoretical findings."],"url":"http://arxiv.org/abs/2403.16975v1","category":"math.NA"}
{"created":"2024-03-25 17:25:44","title":"Unsupervised Feature Selection via Nonnegative Orthogonal Constrained Regularized Minimization","abstract":"Unsupervised feature selection has drawn wide attention in the era of big data since it is a primary technique for dimensionality reduction. However, many existing unsupervised feature selection models and solution methods were presented for the purpose of application, and lack of theoretical support, e.g., without convergence analysis. In this paper, we first establish a novel unsupervised feature selection model based on regularized minimization with nonnegative orthogonal constraints, which has advantages of embedding feature selection into the nonnegative spectral clustering and preventing overfitting. An effective inexact augmented Lagrangian multiplier method is proposed to solve our model, which adopts the proximal alternating minimization method to solve subproblem at each iteration. We show that the sequence generated by our method globally converges to a Karush-Kuhn-Tucker point of our model. Extensive numerical experiments on popular datasets demonstrate the stability and robustness of our method. Moreover, comparison results of algorithm performance show that our method outperforms some existing state-of-the-art methods.","sentences":["Unsupervised feature selection has drawn wide attention in the era of big data since it is a primary technique for dimensionality reduction.","However, many existing unsupervised feature selection models and solution methods were presented for the purpose of application, and lack of theoretical support, e.g., without convergence analysis.","In this paper, we first establish a novel unsupervised feature selection model based on regularized minimization with nonnegative orthogonal constraints, which has advantages of embedding feature selection into the nonnegative spectral clustering and preventing overfitting.","An effective inexact augmented Lagrangian multiplier method is proposed to solve our model, which adopts the proximal alternating minimization method to solve subproblem at each iteration.","We show that the sequence generated by our method globally converges to a Karush-Kuhn-Tucker point of our model.","Extensive numerical experiments on popular datasets demonstrate the stability and robustness of our method.","Moreover, comparison results of algorithm performance show that our method outperforms some existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.16966v1","category":"math.OC"}
{"created":"2024-03-25 17:23:23","title":"The first cut is the cheapest: optimizing Athena/X-IFU-like TES detectors resolution by filter truncation","abstract":"The X-ray Integral Field Unit (X-IFU) instrument on the future ESA mission Athena X-ray Observatory is a cryogenic micro-calorimeter array of Transition Edge Sensor (TES) detectors designed to provide spatially-resolved high-resolution spectroscopy. The onboard reconstruction software provides energy, spatial location and arrival time of incoming X-ray photons hitting the detector. A new processing algorithm based on a truncation of the classical optimal filter and called 0-padding, has been recently proposed aiming to reduce the computational cost without compromising energy resolution. Initial tests with simple synthetic data displayed promising results. This study explores the slightly better performance of the 0-padding filter and assess its final application to real data. The goal is to examine the larger sensitivity to instrumental conditions that was previously observed during the analysis of the simulations. This 0-padding technique is thoroughly tested using more realistic simulations and real data acquired from NASA and NIST laboratories employing X-IFU-like TES detectors. Different fitting methods are applied to the data, and a comparative analysis is performed to assess the energy resolution values obtained from these fittings. The 0-padding filter achieves energy resolutions as good as those obtained with standard filters, even with those of larger lengths, across different line complexes and instrumental conditions. This method proves to be useful for energy reconstruction of X-ray photons detected by the TES detectors provided proper corrections for baseline drift and jitter effects are applied. The finding is highly promising especially for onboard processing, offering efficiency in computational resources and facilitating the analysis of sources with higher count rates at high resolution.","sentences":["The X-ray Integral Field Unit (X-IFU) instrument on the future ESA mission Athena X-ray Observatory is a cryogenic micro-calorimeter array of Transition Edge Sensor (TES) detectors designed to provide spatially-resolved high-resolution spectroscopy.","The onboard reconstruction software provides energy, spatial location and arrival time of incoming X-ray photons hitting the detector.","A new processing algorithm based on a truncation of the classical optimal filter and called 0-padding, has been recently proposed aiming to reduce the computational cost without compromising energy resolution.","Initial tests with simple synthetic data displayed promising results.","This study explores the slightly better performance of the 0-padding filter and assess its final application to real data.","The goal is to examine the larger sensitivity to instrumental conditions that was previously observed during the analysis of the simulations.","This 0-padding technique is thoroughly tested using more realistic simulations and real data acquired from NASA and NIST laboratories employing X-IFU-like TES detectors.","Different fitting methods are applied to the data, and a comparative analysis is performed to assess the energy resolution values obtained from these fittings.","The 0-padding filter achieves energy resolutions as good as those obtained with standard filters, even with those of larger lengths, across different line complexes and instrumental conditions.","This method proves to be useful for energy reconstruction of X-ray photons detected by the TES detectors provided proper corrections for baseline drift and jitter effects are applied.","The finding is highly promising especially for onboard processing, offering efficiency in computational resources and facilitating the analysis of sources with higher count rates at high resolution."],"url":"http://arxiv.org/abs/2403.16965v1","category":"astro-ph.IM"}
{"created":"2024-03-25 17:16:27","title":"Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance","abstract":"Large-scale text-to-image diffusion models have achieved great success in synthesizing high-quality and diverse images given target text prompts. Despite the revolutionary image generation ability, current state-of-the-art models still struggle to deal with multi-concept generation accurately in many cases. This phenomenon is known as ``concept bleeding\" and displays as the unexpected overlapping or merging of various concepts. This paper presents a general approach for text-to-image diffusion models to address the mutual interference between different subjects and their attachments in complex scenes, pursuing better text-image consistency. The core idea is to isolate the synthesizing processes of different concepts. We propose to bind each attachment to corresponding subjects separately with split text prompts. Besides, we introduce a revision method to fix the concept bleeding problem in multi-subject synthesis. We first depend on pre-trained object detection and segmentation models to obtain the layouts of subjects. Then we isolate and resynthesize each subject individually with corresponding text prompts to avoid mutual interference. Overall, we achieve a training-free strategy, named Isolated Diffusion, to optimize multi-concept text-to-image synthesis. It is compatible with the latest Stable Diffusion XL (SDXL) and prior Stable Diffusion (SD) models. We compare our approach with alternative methods using a variety of multi-concept text prompts and demonstrate its effectiveness with clear advantages in text-image consistency and user study.","sentences":["Large-scale text-to-image diffusion models have achieved great success in synthesizing high-quality and diverse images given target text prompts.","Despite the revolutionary image generation ability, current state-of-the-art models still struggle to deal with multi-concept generation accurately in many cases.","This phenomenon is known as ``concept bleeding\" and displays as the unexpected overlapping or merging of various concepts.","This paper presents a general approach for text-to-image diffusion models to address the mutual interference between different subjects and their attachments in complex scenes, pursuing better text-image consistency.","The core idea is to isolate the synthesizing processes of different concepts.","We propose to bind each attachment to corresponding subjects separately with split text prompts.","Besides, we introduce a revision method to fix the concept bleeding problem in multi-subject synthesis.","We first depend on pre-trained object detection and segmentation models to obtain the layouts of subjects.","Then we isolate and resynthesize each subject individually with corresponding text prompts to avoid mutual interference.","Overall, we achieve a training-free strategy, named Isolated Diffusion, to optimize multi-concept text-to-image synthesis.","It is compatible with the latest Stable Diffusion XL (SDXL) and prior Stable Diffusion (SD) models.","We compare our approach with alternative methods using a variety of multi-concept text prompts and demonstrate its effectiveness with clear advantages in text-image consistency and user study."],"url":"http://arxiv.org/abs/2403.16954v1","category":"cs.CV"}
{"created":"2024-03-25 17:01:34","title":"Hyperspherical Classification with Dynamic Label-to-Prototype Assignment","abstract":"Aiming to enhance the utilization of metric space by the parametric softmax classifier, recent studies suggest replacing it with a non-parametric alternative. Although a non-parametric classifier may provide better metric space utilization, it introduces the challenge of capturing inter-class relationships. A shared characteristic among prior non-parametric classifiers is the static assignment of labels to prototypes during the training, ie, each prototype consistently represents a class throughout the training course. Orthogonal to previous works, we present a simple yet effective method to optimize the category assigned to each prototype (label-to-prototype assignment) during the training. To this aim, we formalize the problem as a two-step optimization objective over network parameters and label-to-prototype assignment mapping. We solve this optimization using a sequential combination of gradient descent and Bipartide matching. We demonstrate the benefits of the proposed approach by conducting experiments on balanced and long-tail classification problems using different backbone network architectures. In particular, our method outperforms its competitors by 1.22\\% accuracy on CIFAR-100, and 2.15\\% on ImageNet-200 using a metric space dimension half of the size of its competitors. Code: https://github.com/msed-Ebrahimi/DL2PA_CVPR24","sentences":["Aiming to enhance the utilization of metric space by the parametric softmax classifier, recent studies suggest replacing it with a non-parametric alternative.","Although a non-parametric classifier may provide better metric space utilization, it introduces the challenge of capturing inter-class relationships.","A shared characteristic among prior non-parametric classifiers is the static assignment of labels to prototypes during the training, ie, each prototype consistently represents a class throughout the training course.","Orthogonal to previous works, we present a simple yet effective method to optimize the category assigned to each prototype (label-to-prototype assignment) during the training.","To this aim, we formalize the problem as a two-step optimization objective over network parameters and label-to-prototype assignment mapping.","We solve this optimization using a sequential combination of gradient descent and Bipartide matching.","We demonstrate the benefits of the proposed approach by conducting experiments on balanced and long-tail classification problems using different backbone network architectures.","In particular, our method outperforms its competitors by 1.22\\% accuracy on CIFAR-100, and 2.15\\% on ImageNet-200 using a metric space dimension half of the size of its competitors.","Code: https://github.com/msed-Ebrahimi/DL2PA_CVPR24"],"url":"http://arxiv.org/abs/2403.16937v1","category":"cs.CV"}
{"created":"2024-03-25 17:00:00","title":"Minimum-cost paths for electric cars","abstract":"An electric car equipped with a battery of a finite capacity travels on a road network with an infrastructure of charging stations. Each charging station has a possibly different cost per unit of energy. Traversing a given road segment requires a specified amount of energy that may be positive, zero or negative. The car can only traverse a road segment if it has enough charge to do so (the charge cannot drop below zero), and it cannot charge its battery beyond its capacity.   To travel from one point to another the car needs to choose a \\emph{travel plan} consisting of a path in the network and a recharging schedule that specifies how much energy to charge at each charging station on the path, making sure of having enough energy to reach the next charging station or the destination. The cost of the plan is the total charging cost along the chosen path. We reduce the problem of computing plans between every two junctions of the network to two problems: Finding optimal energetic paths when no charging is allowed and finding standard shortest paths. When there are no negative cycles in the network, we obtain an $O(n^3)$-time algorithm for computing all-pairs travel plans, where~$n$ is the number of junctions in the network. We obtain slightly faster algorithms under some further assumptions. We also consider the case in which a bound is placed on the number of rechargings allowed.","sentences":["An electric car equipped with a battery of a finite capacity travels on a road network with an infrastructure of charging stations.","Each charging station has a possibly different cost per unit of energy.","Traversing a given road segment requires a specified amount of energy that may be positive, zero or negative.","The car can only traverse a road segment if it has enough charge to do so (the charge cannot drop below zero), and it cannot charge its battery beyond its capacity.   ","To travel from one point to another the car needs to choose a \\emph{travel plan} consisting of a path in the network and a recharging schedule that specifies how much energy to charge at each charging station on the path, making sure of having enough energy to reach the next charging station or the destination.","The cost of the plan is the total charging cost along the chosen path.","We reduce the problem of computing plans between every two junctions of the network to two problems: Finding optimal energetic paths when no charging is allowed and finding standard shortest paths.","When there are no negative cycles in the network, we obtain an $O(n^3)$-time algorithm for computing all-pairs travel plans, where~$n$ is the number of junctions in the network.","We obtain slightly faster algorithms under some further assumptions.","We also consider the case in which a bound is placed on the number of rechargings allowed."],"url":"http://arxiv.org/abs/2403.16936v1","category":"cs.DS"}
{"created":"2024-03-25 16:38:34","title":"On the optimality of MCMC integration for functions with unbounded second moment","abstract":"We study the Markov chain Monte Carlo (MCMC) estimator for numerical integration for functions that do not need to be square integrable w.r.t. the invariant distribution. For chains with a spectral gap we show that the absolute mean error for $L^p$ functions, with $p \\in (1,2)$, decreases like $n^{1/p -1}$, which is known to be the optimal rate. This improves currently known results where an additional parameter $\\delta>0$ appears and the convergence is of order $n^{(1+\\delta)/p-1}$.","sentences":["We study the Markov chain Monte Carlo (MCMC) estimator for numerical integration for functions that do not need to be square integrable w.r.t.","the invariant distribution.","For chains with a spectral gap we show that the absolute mean error for $L^p$ functions, with $p \\in (1,2)$, decreases like $n^{1/p -1}$, which is known to be the optimal rate.","This improves currently known results where an additional parameter $\\delta>0$ appears and the convergence is of order $n^{(1+\\delta)/p-1}$."],"url":"http://arxiv.org/abs/2403.16920v1","category":"math.NA"}
{"created":"2024-03-25 16:36:13","title":"SCOD: From Heuristics to Theory","abstract":"This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely on an ID data sample. Third, we introduce POSCOD, a simple method for learning a plugin estimate of the optimal SCOD strategy from both an ID data sample and an unlabeled mixture of ID and OOD data. Our empirical results confirm the theoretical findings and demonstrate that our proposed method, POSCOD, out performs existing OOD methods in effectively addressing the SCOD problem.","sentences":["This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or out-of-distribution samples - a recently proposed problem known as Selective Classification in the presence of Out-of-Distribution data (SCOD).","We make three key contributions to SCOD.","Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and out-of-distribution (OOD) data as input.","This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD.","Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely on an ID data sample.","Third, we introduce POSCOD, a simple method for learning a plugin estimate of the optimal SCOD strategy from both an ID data sample and an unlabeled mixture of ID and OOD data.","Our empirical results confirm the theoretical findings and demonstrate that our proposed method, POSCOD, out performs existing OOD methods in effectively addressing the SCOD problem."],"url":"http://arxiv.org/abs/2403.16916v1","category":"cs.LG"}
{"created":"2024-03-25 16:29:49","title":"Achieving Optical Refractive Index of 10-Plus by Colloidal Self-Assembly","abstract":"This study demonstrates the developments of self-assembled optical metasurfaces to overcome inherent limitations in polarization density (P) within natural materials, which hinder achieving high refractive indices (n) at optical frequencies. The Maxwellian macroscopic description establishes a link between P and n, revealing a static limit in natural materials, restricting n to approximately 4.0 at optical frequencies. Optical metasurfaces, utilizing metallic colloids on a deep-subwavelength scale, offer a solution by unnaturally enhancing n through electric dipolar (ED) resonances. Self-assembly enables the creation of nanometer-scale metallic gaps between metallic nanoparticles (NPs), paving the way for achieving exceptionally high n at optical frequencies. This study focuses on assembling polyhedral gold (Au) NPs into a closely packed monolayer by rationally designing the polymeric ligand to balance attractive and repulsive forces, in that polymeric brush-mediated self-assembly of the close-packed Au NP monolayer is robustly achieved over a large-area. The resulting monolayer of Au nanospheres (NSs), nanooctahedras (NOs), and nanocubes (NCs) exhibits high macroscopic integrity and crystallinity, sufficiently enough for pushing n to record-high regimes. The study underlies the significance of capacitive coupling in achieving an unnaturally high n and explores fine-tuning Au NC size to optimize this coupling. The achieved n of 10.12 at optical frequencies stands as a benchmark, highlighting the potential of polyhedral Au NPs in advancing optical metasurfaces.","sentences":["This study demonstrates the developments of self-assembled optical metasurfaces to overcome inherent limitations in polarization density (P) within natural materials, which hinder achieving high refractive indices (n) at optical frequencies.","The Maxwellian macroscopic description establishes a link between P and n, revealing a static limit in natural materials, restricting n to approximately 4.0 at optical frequencies.","Optical metasurfaces, utilizing metallic colloids on a deep-subwavelength scale, offer a solution by unnaturally enhancing n through electric dipolar (ED) resonances.","Self-assembly enables the creation of nanometer-scale metallic gaps between metallic nanoparticles (NPs), paving the way for achieving exceptionally high n at optical frequencies.","This study focuses on assembling polyhedral gold (Au) NPs into a closely packed monolayer by rationally designing the polymeric ligand to balance attractive and repulsive forces, in that polymeric brush-mediated self-assembly of the close-packed Au NP monolayer is robustly achieved over a large-area.","The resulting monolayer of Au nanospheres (NSs), nanooctahedras (NOs), and nanocubes (NCs) exhibits high macroscopic integrity and crystallinity, sufficiently enough for pushing n to record-high regimes.","The study underlies the significance of capacitive coupling in achieving an unnaturally high n and explores fine-tuning Au NC size to optimize this coupling.","The achieved n of 10.12 at optical frequencies stands as a benchmark, highlighting the potential of polyhedral Au NPs in advancing optical metasurfaces."],"url":"http://arxiv.org/abs/2403.16911v1","category":"physics.optics"}
{"created":"2024-03-25 16:11:29","title":"Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via Convex Optimization","abstract":"We propose an output feedback control-based motion planning technique for agents to enable them to converge to a specified polynomial trajectory while imposing a set of safety constraints on our controller to avoid collisions within the free configuration space (polygonal environment). To achieve this, we 1) decompose our polygonal environment into different overlapping cells 2) write out our polynomial trajectories as the output of a reference dynamical system with given initial conditions 3) formulate convergence and safety constraints as Linear Matrix Inequalities (LMIs) on our controller using Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) and 4) solve a semi-definite programming (SDP) problem with convergence and safety constraints imposed to synthesize a controller for each convex cell. Extensive simulations are included to test our motion planning method under different initial conditions and different reference trajectories. The synthesized controller is robust to changes in initial conditions and is always safe relative to the boundaries of the polygonal environment.","sentences":["We propose an output feedback control-based motion planning technique for agents to enable them to converge to a specified polynomial trajectory while imposing a set of safety constraints on our controller to avoid collisions within the free configuration space (polygonal environment).","To achieve this, we 1) decompose our polygonal environment into different overlapping cells 2) write out our polynomial trajectories as the output of a reference dynamical system with given initial conditions 3) formulate convergence and safety constraints as Linear Matrix Inequalities (LMIs) on our controller using Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) and 4) solve a semi-definite programming (SDP) problem with convergence and safety constraints imposed to synthesize a controller for each convex cell.","Extensive simulations are included to test our motion planning method under different initial conditions and different reference trajectories.","The synthesized controller is robust to changes in initial conditions and is always safe relative to the boundaries of the polygonal environment."],"url":"http://arxiv.org/abs/2403.16900v1","category":"eess.SY"}
{"created":"2024-03-25 15:58:26","title":"Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction","abstract":"In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of measurements collected from a known forward model describing certain sensing or imaging modality. Due to resource constraints, this task is often extremely ill-posed, which necessitates the adoption of expressive prior information to regularize the solution space. Score-based diffusion models, due to its impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate {\\em unconditional} score functions of an image prior distribution in conjunction with flexible choices of forward models.   This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in general nonlinear inverse problems. Motivated by the plug-and-play framework in the imaging community, we introduce a diffusion plug-and-play method (\\textsf{DPnP}) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising diffusion sampler based solely on the score functions of the image prior. The key insight is that denoising under white Gaussian noise can be solved {\\em rigorously} via both stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using the unconditional score functions. We establish both asymptotic and non-asymptotic performance guarantees of \\textsf{DPnP}, and provide numerical experiments to illustrate its promise in solving both linear and nonlinear image reconstruction tasks. To the best of our knowledge, \\textsf{DPnP} is the first provably-robust posterior sampling method for nonlinear inverse problems using unconditional diffusion priors.","sentences":["In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of measurements collected from a known forward model describing certain sensing or imaging modality.","Due to resource constraints, this task is often extremely ill-posed, which necessitates the adoption of expressive prior information to regularize the solution space.","Score-based diffusion models, due to its impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction.","In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate {\\em unconditional} score functions of an image prior distribution in conjunction with flexible choices of forward models.   ","This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in general nonlinear inverse problems.","Motivated by the plug-and-play framework in the imaging community, we introduce a diffusion plug-and-play method (\\textsf{DPnP}) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising diffusion sampler based solely on the score functions of the image prior.","The key insight is that denoising under white Gaussian noise can be solved {\\em rigorously} via both stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using the unconditional score functions.","We establish both asymptotic and non-asymptotic performance guarantees of \\textsf{DPnP}, and provide numerical experiments to illustrate its promise in solving both linear and nonlinear image reconstruction tasks.","To the best of our knowledge, \\textsf{DPnP} is the first provably-robust posterior sampling method for nonlinear inverse problems using unconditional diffusion priors."],"url":"http://arxiv.org/abs/2403.17042v1","category":"eess.IV"}
{"created":"2024-03-25 15:58:22","title":"A low-order locking-free multiscale finite element method for isotropic elasticity","abstract":"The multiscale hybrid-mixed (MHM) method consists of a multi-level strategy to approximate the solution of boundary value problems with heterogeneous coefficients. In this context, we propose a family of low-order finite elements for the linear elasticity problem which are free from Poisson locking. The finite elements rely on face degrees of freedom associated with multiscale bases obtained from local Neumann problems with piecewise polynomial interpolations on faces. We establish sufficient refinement levels on the fine-scale mesh such that the MHM method is well-posed, optimally convergent under local regularity conditions, and locking-free. Two-dimensional numerical tests assess theoretical results.","sentences":["The multiscale hybrid-mixed (MHM) method consists of a multi-level strategy to approximate the solution of boundary value problems with heterogeneous coefficients.","In this context, we propose a family of low-order finite elements for the linear elasticity problem which are free from Poisson locking.","The finite elements rely on face degrees of freedom associated with multiscale bases obtained from local Neumann problems with piecewise polynomial interpolations on faces.","We establish sufficient refinement levels on the fine-scale mesh such that the MHM method is well-posed, optimally convergent under local regularity conditions, and locking-free.","Two-dimensional numerical tests assess theoretical results."],"url":"http://arxiv.org/abs/2403.16890v1","category":"math.NA"}
{"created":"2024-03-25 15:56:24","title":"Movable-Antenna Position Optimization: A Graph-based Approach","abstract":"Fluid antennas (FAs) and movable antennas (MAs) have emerged as promising technologies in wireless communications, which offer the flexibility to improve channel conditions by adjusting transmit/receive antenna positions within a spatial region. In this letter, we focus on an MA-enhanced multiple-input single-output (MISO) communication system, aiming to optimize the positions of multiple transmit MAs to maximize the received signal power. Unlike the prior works on continuously searching for the optimal MA positions, we propose to sample the transmit region into discrete points, such that the continuous antenna position optimization problem is transformed to a discrete sampling point selection problem based on the point-wise channel information. However, such a point selection problem is combinatory and challenging to be optimally solved. To tackle this challenge, we ingeniously recast it as an equivalent fixed-hop shortest path problem in graph theory and propose a customized algorithm to solve it optimally in polynomial time. To further reduce the complexity, a linear-time sequential update algorithm is also proposed to obtain a high-quality suboptimal solution. Numerical results demonstrate that the proposed algorithms can yield considerable performance gains over the conventional fixed-position antennas with/without antenna selection.","sentences":["Fluid antennas (FAs) and movable antennas (MAs) have emerged as promising technologies in wireless communications, which offer the flexibility to improve channel conditions by adjusting transmit/receive antenna positions within a spatial region.","In this letter, we focus on an MA-enhanced multiple-input single-output (MISO) communication system, aiming to optimize the positions of multiple transmit MAs to maximize the received signal power.","Unlike the prior works on continuously searching for the optimal MA positions, we propose to sample the transmit region into discrete points, such that the continuous antenna position optimization problem is transformed to a discrete sampling point selection problem based on the point-wise channel information.","However, such a point selection problem is combinatory and challenging to be optimally solved.","To tackle this challenge, we ingeniously recast it as an equivalent fixed-hop shortest path problem in graph theory and propose a customized algorithm to solve it optimally in polynomial time.","To further reduce the complexity, a linear-time sequential update algorithm is also proposed to obtain a high-quality suboptimal solution.","Numerical results demonstrate that the proposed algorithms can yield considerable performance gains over the conventional fixed-position antennas with/without antenna selection."],"url":"http://arxiv.org/abs/2403.16886v1","category":"cs.IT"}
{"created":"2024-03-25 15:47:06","title":"DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques","abstract":"Maps provide robots with crucial environmental knowledge, thereby enabling them to perform interactive tasks effectively. Easily accessing accurate abstract-to-detailed geometric and semantic concepts from maps is crucial for robots to make informed and efficient decisions. To comprehensively model the environment and effectively manage the map data structure, we propose DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed Distance Field (TSDF) submaps and panoptic labels to hierarchically model the environment. The output map is able to maintain both voxel- and submap-level metric and semantic information. Two modules are presented to enhance the mapping efficiency and label consistency: (1) an inter-submaps label fusion strategy to eliminate duplicate points across submaps and (2) a conditional random field (CRF) based approach to enhance panoptic labels through object label comprehension and contextual information. We conducted experiments with two public datasets including indoor and outdoor scenarios. Our system performs comparably to state-of-the-art (SOTA) methods across geometry and label accuracy evaluation metrics. The experiment results highlight the effectiveness and scalability of our system, as it is capable of constructing precise geometry and maintaining consistent panoptic labels. Our code is publicly available at https://github.com/hutslib/DHP-Mapping.","sentences":["Maps provide robots with crucial environmental knowledge, thereby enabling them to perform interactive tasks effectively.","Easily accessing accurate abstract-to-detailed geometric and semantic concepts from maps is crucial for robots to make informed and efficient decisions.","To comprehensively model the environment and effectively manage the map data structure, we propose DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed Distance Field (TSDF) submaps and panoptic labels to hierarchically model the environment.","The output map is able to maintain both voxel- and submap-level metric and semantic information.","Two modules are presented to enhance the mapping efficiency and label consistency: (1) an inter-submaps label fusion strategy to eliminate duplicate points across submaps and (2) a conditional random field (CRF) based approach to enhance panoptic labels through object label comprehension and contextual information.","We conducted experiments with two public datasets including indoor and outdoor scenarios.","Our system performs comparably to state-of-the-art (SOTA) methods across geometry and label accuracy evaluation metrics.","The experiment results highlight the effectiveness and scalability of our system, as it is capable of constructing precise geometry and maintaining consistent panoptic labels.","Our code is publicly available at https://github.com/hutslib/DHP-Mapping."],"url":"http://arxiv.org/abs/2403.16880v1","category":"cs.RO"}
{"created":"2024-03-25 15:46:34","title":"Algebraic Constraints on Common Lines with Applications to Community Detection in Cryo-EM","abstract":"We revisit the topic of common lines between 2D class averages in single particle cryo-electron microscopy (cryo-EM). We derive a novel low-rank constraint on a certain $2n \\times n$ matrix storing properly-scaled basis vectors for the common lines between $n$ tomographic images of one molecular conformation. Using this constraint and others, we introduce an optimization algorithm to denoise the common lines of one conformation. As a main application, we develop a clustering algorithm to partition a set of noisy common lines into homogeneous communities, in the case of discrete heterogeneity in cryo-EM. We demonstrate the methods on synthetic and experimental datasets.","sentences":["We revisit the topic of common lines between 2D class averages in single particle cryo-electron microscopy (cryo-EM).","We derive a novel low-rank constraint on a certain $2n \\times n$ matrix storing properly-scaled basis vectors for the common lines between $n$ tomographic images of one molecular conformation.","Using this constraint and others, we introduce an optimization algorithm to denoise the common lines of one conformation.","As a main application, we develop a clustering algorithm to partition a set of noisy common lines into homogeneous communities, in the case of discrete heterogeneity in cryo-EM.","We demonstrate the methods on synthetic and experimental datasets."],"url":"http://arxiv.org/abs/2403.16879v1","category":"math.OC"}
{"created":"2024-03-25 15:39:45","title":"Optimality of spherical codes via exact semidefinite programming bounds","abstract":"We show that the spectral embeddings of all known triangle-free strongly regular graphs are optimal spherical codes (the new cases are $56$ points in $20$ dimensions, $50$ points in $21$ dimensions, and $77$ points in $21$ dimensions), as are certain mutually unbiased basis arrangements constructed using Kerdock codes in up to $1024$ dimensions (namely, $2^{4k} + 2^{2k+1}$ points in $2^{2k}$ dimensions for $2 \\le k \\le 5$). As a consequence of the latter, we obtain optimality of the Kerdock binary codes of block length $64$, $256$, and $1024$, as well as uniqueness for block length $64$. We also prove universal optimality for $288$ points on a sphere in $16$ dimensions. To prove these results, we use three-point semidefinite programming bounds, for which only a few sharp cases were known previously. To obtain rigorous results, we develop improved techniques for rounding approximate solutions of semidefinite programs to produce exact optimal solutions.","sentences":["We show that the spectral embeddings of all known triangle-free strongly regular graphs are optimal spherical codes (the new cases are $56$ points in $20$ dimensions, $50$ points in $21$ dimensions, and $77$ points in $21$ dimensions), as are certain mutually unbiased basis arrangements constructed using Kerdock codes in up to $1024$ dimensions (namely, $2^{4k} + 2^{2k+1}$ points in $2^{2k}$ dimensions for $2 \\le k \\le 5$).","As a consequence of the latter, we obtain optimality of the Kerdock binary codes of block length $64$, $256$, and $1024$, as well as uniqueness for block length $64$. We also prove universal optimality for $288$ points on a sphere in $16$ dimensions.","To prove these results, we use three-point semidefinite programming bounds, for which only a few sharp cases were known previously.","To obtain rigorous results, we develop improved techniques for rounding approximate solutions of semidefinite programs to produce exact optimal solutions."],"url":"http://arxiv.org/abs/2403.16874v1","category":"math.MG"}
{"created":"2024-03-25 15:37:43","title":"Conformal Off-Policy Prediction for Multi-Agent Systems","abstract":"Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more \"ego\" agents change their policies. Unlike the single-agent scenario, this setting introduces higher complexity as the distribution shifts affect predictions for all agents, not just the ego agents, and the prediction task involves full multi-dimensional trajectories, not just reward values. A key contribution of MA-COPP is to avoid enumeration or exhaustive search of the output space of agent trajectories, which is instead required by existing COPP methods to construct the prediction region. We achieve this by showing that an over-approximation of the true JPR can be constructed, without enumeration, from the maximum density ratio of the JPR trajectories. We evaluate the effectiveness of MA-COPP in multi-agent systems from the PettingZoo library and the F1TENTH autonomous racing environment, achieving nominal coverage in higher dimensions and various shift settings.","sentences":["Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe.","To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process.","Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards).","In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more \"ego\" agents change their policies.","Unlike the single-agent scenario, this setting introduces higher complexity as the distribution shifts affect predictions for all agents, not just the ego agents, and the prediction task involves full multi-dimensional trajectories, not just reward values.","A key contribution of MA-COPP is to avoid enumeration or exhaustive search of the output space of agent trajectories, which is instead required by existing COPP methods to construct the prediction region.","We achieve this by showing that an over-approximation of the true JPR can be constructed, without enumeration, from the maximum density ratio of the JPR trajectories.","We evaluate the effectiveness of MA-COPP in multi-agent systems from the PettingZoo library and the F1TENTH autonomous racing environment, achieving nominal coverage in higher dimensions and various shift settings."],"url":"http://arxiv.org/abs/2403.16871v1","category":"cs.MA"}
{"created":"2024-03-25 15:28:38","title":"Encoding of lexical tone in self-supervised models of spoken language","abstract":"Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.","sentences":["Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics.","The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood.","Tone is a suprasegmental feature that is present in more than half of the world's languages.","This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies.","We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages.","We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory."],"url":"http://arxiv.org/abs/2403.16865v1","category":"cs.CL"}
{"created":"2024-03-25 15:27:21","title":"Improved convergence rates for the Difference-of-Convex algorithm","abstract":"We consider a difference-of-convex formulation where one of the terms is allowed to be hypoconvex (or weakly convex). We first examine the precise behavior of a single iteration of the Difference-of-Convex algorithm (DCA), giving a tight characterization of the objective function decrease. This requires distinguishing between eight distinct parameter regimes.   Our proofs are inspired by the performance estimation framework, but are much simplified compared to similar previous work.   We then derive sublinear DCA convergence rates towards critical points, distinguishing between cases where at least one of the functions is smooth and where both functions are nonsmooth. We conjecture the tightness of these rates for four parameter regimes, based on strong numerical evidence obtained via performance estimation, as well as the leading constant in the asymptotic sublinear rate for two more regimes.","sentences":["We consider a difference-of-convex formulation where one of the terms is allowed to be hypoconvex (or weakly convex).","We first examine the precise behavior of a single iteration of the Difference-of-Convex algorithm (DCA), giving a tight characterization of the objective function decrease.","This requires distinguishing between eight distinct parameter regimes.   ","Our proofs are inspired by the performance estimation framework, but are much simplified compared to similar previous work.   ","We then derive sublinear DCA convergence rates towards critical points, distinguishing between cases where at least one of the functions is smooth and where both functions are nonsmooth.","We conjecture the tightness of these rates for four parameter regimes, based on strong numerical evidence obtained via performance estimation, as well as the leading constant in the asymptotic sublinear rate for two more regimes."],"url":"http://arxiv.org/abs/2403.16864v1","category":"math.OC"}
{"created":"2024-03-25 15:26:50","title":"SIP: Autotuning GPU Native Schedules via Stochastic Instruction Perturbation","abstract":"Large language models (LLMs) have become a significant workload since their appearance. However, they are also computationally expensive as they have billions of parameters and are trained with massive amounts of data. Thus, recent works have developed dedicated CUDA kernels for LLM training and inference instead of relying on compilergenerated ones, so that hardware resources are as fully utilized as possible. In this work, we explore the possibility of GPU native instruction optimization to further push the CUDA kernels to extreme performance. Contrary to prior works, we adopt an automatic optimization approach by defining a search space of possible GPU native instruction schedules, and then we apply stochastic search to perform optimization. Experiments show that SIP can further improve CUDA kernel throughput by automatically discovering better GPU native instruction schedules and the optimized schedules are tested by 10 million test samples.","sentences":["Large language models (LLMs) have become a significant workload since their appearance.","However, they are also computationally expensive as they have billions of parameters and are trained with massive amounts of data.","Thus, recent works have developed dedicated CUDA kernels for LLM training and inference instead of relying on compilergenerated ones, so that hardware resources are as fully utilized as possible.","In this work, we explore the possibility of GPU native instruction optimization to further push the CUDA kernels to extreme performance.","Contrary to prior works, we adopt an automatic optimization approach by defining a search space of possible GPU native instruction schedules, and then we apply stochastic search to perform optimization.","Experiments show that SIP can further improve CUDA kernel throughput by automatically discovering better GPU native instruction schedules and the optimized schedules are tested by 10 million test samples."],"url":"http://arxiv.org/abs/2403.16863v1","category":"cs.AR"}
{"created":"2024-03-25 15:26:32","title":"INPC: Implicit Neural Point Clouds for Radiance Field Rendering","abstract":"We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common benchmark datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.","sentences":["We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes.","In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid.","In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds.","Our method achieves state-of-the-art image quality on several common benchmark datasets.","Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance."],"url":"http://arxiv.org/abs/2403.16862v1","category":"cs.CV"}
{"created":"2024-03-25 15:23:14","title":"A Semi-Lagrangian Approach for Time and Energy Path Planning Optimization in Static Flow Fields","abstract":"Efficient path planning for autonomous mobile robots is a critical problem across numerous domains, where optimizing both time and energy consumption is paramount. This paper introduces a novel methodology that considers the dynamic influence of an environmental flow field and considers geometric constraints, including obstacles and forbidden zones, enriching the complexity of the planning problem. We formulate it as a multi-objective optimal control problem, propose a novel transformation called Harmonic Transformation, and apply a semi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is obtained considering two distinct approaches: a deterministic method and an evolutionary-based one, both of which are designed to make use of the proposed Harmonic Transformation. Through an extensive analysis of these approaches, we demonstrate their efficacy in finding optimized paths.","sentences":["Efficient path planning for autonomous mobile robots is a critical problem across numerous domains, where optimizing both time and energy consumption is paramount.","This paper introduces a novel methodology that considers the dynamic influence of an environmental flow field and considers geometric constraints, including obstacles and forbidden zones, enriching the complexity of the planning problem.","We formulate it as a multi-objective optimal control problem, propose a novel transformation called Harmonic Transformation, and apply a semi-Lagrangian scheme to solve it.","The set of Pareto efficient solutions is obtained considering two distinct approaches: a deterministic method and an evolutionary-based one, both of which are designed to make use of the proposed Harmonic Transformation.","Through an extensive analysis of these approaches, we demonstrate their efficacy in finding optimized paths."],"url":"http://arxiv.org/abs/2403.16859v1","category":"cs.RO"}
{"created":"2024-03-25 15:18:23","title":"Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints","abstract":"This paper studies semantic-aware communication for remote estimation of multiple Markov sources over a lossy and rate-constrained channel. Unlike most existing studies that treat all source states equally, we exploit the semantics of information and consider that the remote actuator has different tolerances for the estimation errors of different states. We aim to find an optimal scheduling policy that minimizes the long-term state-dependent costs of estimation errors under a transmission frequency constraint. We theoretically show the structure of the optimal policy by leveraging the average-cost Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic programming. By exploiting the optimal structural results, we develop a novel policy search algorithm, termed intersection search plus relative value iteration (Insec-RVI), that can find the optimal policy using only a few iterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on the Lyapunov optimization theorem. We also design an efficient average-cost Q-learning algorithm to estimate the optimal policy without knowing a priori the channel and source statistics. Numerical results show that continuous transmission is inefficient, and remarkably, our semantic-aware policies can attain the optimum by strategically utilizing fewer transmissions by exploiting the timing of the important information.","sentences":["This paper studies semantic-aware communication for remote estimation of multiple Markov sources over a lossy and rate-constrained channel.","Unlike most existing studies that treat all source states equally, we exploit the semantics of information and consider that the remote actuator has different tolerances for the estimation errors of different states.","We aim to find an optimal scheduling policy that minimizes the long-term state-dependent costs of estimation errors under a transmission frequency constraint.","We theoretically show the structure of the optimal policy by leveraging the average-cost Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic programming.","By exploiting the optimal structural results, we develop a novel policy search algorithm, termed intersection search plus relative value iteration (Insec-RVI), that can find the optimal policy using only a few iterations.","To avoid the ``curse of dimensionality'' of MDPs, we propose an online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on the Lyapunov optimization theorem.","We also design an efficient average-cost Q-learning algorithm to estimate the optimal policy without knowing a priori the channel and source statistics.","Numerical results show that continuous transmission is inefficient, and remarkably, our semantic-aware policies can attain the optimum by strategically utilizing fewer transmissions by exploiting the timing of the important information."],"url":"http://arxiv.org/abs/2403.16855v1","category":"eess.SY"}
{"created":"2024-03-26 17:59:23","title":"AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation","abstract":"Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh recovery) involves the human body, hand, and expression estimation. Most existing methods have tackled this task in a two-stage manner, first detecting the human body part with an off-the-shelf detection model and inferring the different human body parts individually. Despite the impressive results achieved, these methods suffer from 1) loss of valuable contextual information via cropping, 2) introducing distractions, and 3) lacking inter-association among different persons and body parts, inevitably causing performance degradation, especially for crowded scenes. To address these issues, we introduce a novel all-in-one-stage framework, AiOS, for multiple expressive human pose and shape recovery without an additional human detection step. Specifically, our method is built upon DETR, which treats multi-person whole-body mesh recovery task as a progressive set prediction problem with various sequential detection. We devise the decoder tokens and extend them to our task. Specifically, we first employ a human token to probe a human location in the image and encode global features for each instance, which provides a coarse location for the later transformer block. Then, we introduce a joint-related token to probe the human joint in the image and encoder a fine-grained local feature, which collaborates with the global feature to regress the whole-body mesh. This straightforward but effective model outperforms previous state-of-the-art methods by a 9% reduction in NMVE on AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a 3% reduction in PVE on EgoBody.","sentences":["Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh recovery) involves the human body, hand, and expression estimation.","Most existing methods have tackled this task in a two-stage manner, first detecting the human body part with an off-the-shelf detection model and inferring the different human body parts individually.","Despite the impressive results achieved, these methods suffer from 1) loss of valuable contextual information via cropping, 2) introducing distractions, and 3) lacking inter-association among different persons and body parts, inevitably causing performance degradation, especially for crowded scenes.","To address these issues, we introduce a novel all-in-one-stage framework, AiOS, for multiple expressive human pose and shape recovery without an additional human detection step.","Specifically, our method is built upon DETR, which treats multi-person whole-body mesh recovery task as a progressive set prediction problem with various sequential detection.","We devise the decoder tokens and extend them to our task.","Specifically, we first employ a human token to probe a human location in the image and encode global features for each instance, which provides a coarse location for the later transformer block.","Then, we introduce a joint-related token to probe the human joint in the image and encoder a fine-grained local feature, which collaborates with the global feature to regress the whole-body mesh.","This straightforward but effective model outperforms previous state-of-the-art methods by a 9% reduction in NMVE on AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a 3% reduction in PVE on EgoBody."],"url":"http://arxiv.org/abs/2403.17934v1","category":"cs.CV"}
{"created":"2024-03-26 16:52:26","title":"X-ray Properties of the Luminous Quasar PG 1634+706 at z = 1.337 from SRG and XMM-Newton Data","abstract":"In the fall of 2019, during the in-flight calibration phase of the SRG observatory, the onboard eROSITA and Mikhail Pavlinsky ART-XC telescopes carried out a series of observations of PG 1634+706 - one of the most luminous (an X-ray luminosity $\\sim 10^{46}$ erg/s) quasars in the Universe at $z<2$. Approximately at the same dates this quasar was also observed by the XMM-Newton observatory. Although the object had already been repeatedly studied in X-rays previously, its new observations allowed its energy spectrum to be measured more accurately in the wide range $1-30$ keV (in the quasar rest frame). Its spectrum can be described by a two-component model that consists of a power-law continuum with a slope $\\Gamma\\approx 1.9$ and a broadened iron emission line at an energy of about 6.4 keV. The X-ray variability of the quasar was also investigated. On time scales of the order of several hours (here and below, in the source rest frame) the X-ray luminosity does not exhibit a statistically significant variability. However, it changed noticeably from observation to observation in the fall of 2019, having increased approximately by a factor of 1.5 in 25 days. A comparison of the new SRG and XMM-Newton measurements with the previous measurements of other X-ray observatories has shown that in the entire 17-year history of observations of the quasar PG 1634+706 its X-ray luminosity has varied by no more than a factor of 2.5, while the variations on time scales of several weeks and several years are comparable in amplitude.","sentences":["In the fall of 2019, during the in-flight calibration phase of the SRG observatory, the onboard eROSITA and Mikhail Pavlinsky ART-XC telescopes carried out a series of observations of PG 1634+706 - one of the most luminous (an X-ray luminosity $\\sim 10^{46}$ erg/s) quasars in the Universe at $z<2$. Approximately at the same dates this quasar was also observed by the XMM-Newton observatory.","Although the object had already been repeatedly studied in X-rays previously, its new observations allowed its energy spectrum to be measured more accurately in the wide range $1-30$ keV (in the quasar rest frame).","Its spectrum can be described by a two-component model that consists of a power-law continuum with a slope $\\Gamma\\approx 1.9$ and a broadened iron emission line at an energy of about 6.4 keV. The X-ray variability of the quasar was also investigated.","On time scales of the order of several hours (here and below, in the source rest frame) the X-ray luminosity does not exhibit a statistically significant variability.","However, it changed noticeably from observation to observation in the fall of 2019, having increased approximately by a factor of 1.5 in 25 days.","A comparison of the new SRG and XMM-Newton measurements with the previous measurements of other X-ray observatories has shown that in the entire 17-year history of observations of the quasar PG 1634+706 its X-ray luminosity has varied by no more than a factor of 2.5, while the variations on time scales of several weeks and several years are comparable in amplitude."],"url":"http://arxiv.org/abs/2403.17865v1","category":"astro-ph.HE"}
{"created":"2024-03-26 16:44:49","title":"Emittance preservation in a plasma-wakefield accelerator","abstract":"Radio-frequency particle accelerators are engines of discovery, powering high-energy physics and photon science, but are also large and expensive due to their limited accelerating fields. Plasma-wakefield accelerators (PWFAs) provide orders-of-magnitude stronger fields in the charge-density wave behind a particle bunch travelling in a plasma, promising particle accelerators of greatly reduced size and cost. However, PWFAs can easily degrade the beam quality of the bunches they accelerate. Emittance, which determines how tightly beams can be focused, is a critical beam quality in for instance colliders and free-electron lasers, but is particularly prone to degradation. We demonstrate, for the first time, emittance preservation in a high-gradient and high-efficiency PWFA while simultaneously preserving charge and energy spread. This establishes that PWFAs can accelerate without degradation$\\unicode{x2014}$essential for energy boosters in photon science and multistage facilities for compact high-energy particle colliders.","sentences":["Radio-frequency particle accelerators are engines of discovery, powering high-energy physics and photon science, but are also large and expensive due to their limited accelerating fields.","Plasma-wakefield accelerators (PWFAs) provide orders-of-magnitude stronger fields in the charge-density wave behind a particle bunch travelling in a plasma, promising particle accelerators of greatly reduced size and cost.","However, PWFAs can easily degrade the beam quality of the bunches they accelerate.","Emittance, which determines how tightly beams can be focused, is a critical beam quality in for instance colliders and free-electron lasers, but is particularly prone to degradation.","We demonstrate, for the first time, emittance preservation in a high-gradient and high-efficiency PWFA while simultaneously preserving charge and energy spread.","This establishes that PWFAs can accelerate without degradation$\\unicode{x2014}$essential for energy boosters in photon science and multistage facilities for compact high-energy particle colliders."],"url":"http://arxiv.org/abs/2403.17855v1","category":"physics.acc-ph"}
{"created":"2024-03-26 16:27:51","title":"Negative electronic compressibility in charge islands in twisted bilayer graphene","abstract":"We report on the observation of negative electronic compressibility in twisted bilayer graphene for Fermi energies close to insulating states. To observe this negative compressibility, we take advantage of naturally occurring twist angle domains that emerge during the fabrication of the samples, leading to the formation of charge islands. We accurately measure their capacitance using Coulomb oscillations, from which we infer the compressibility of the electron gas. Notably, we not only observe the negative electronic compressibility near correlated insulating states at integer filling, but also prominently near the band insulating state at full filling, located at the edges of both the flat- and remote bands. Furthermore, the individual twist angle domains yield a well-defined carrier density, enabling us to quantify the strength of electronic interactions and verify the theoretical prediction that the inverse negative capacitance contribution is proportional to the average distance between the charge carriers. A detailed analysis of our findings suggests that Wigner crystallization is the most likely explanation for the observed negative electronic compressibility.","sentences":["We report on the observation of negative electronic compressibility in twisted bilayer graphene for Fermi energies close to insulating states.","To observe this negative compressibility, we take advantage of naturally occurring twist angle domains that emerge during the fabrication of the samples, leading to the formation of charge islands.","We accurately measure their capacitance using Coulomb oscillations, from which we infer the compressibility of the electron gas.","Notably, we not only observe the negative electronic compressibility near correlated insulating states at integer filling, but also prominently near the band insulating state at full filling, located at the edges of both the flat- and remote bands.","Furthermore, the individual twist angle domains yield a well-defined carrier density, enabling us to quantify the strength of electronic interactions and verify the theoretical prediction that the inverse negative capacitance contribution is proportional to the average distance between the charge carriers.","A detailed analysis of our findings suggests that Wigner crystallization is the most likely explanation for the observed negative electronic compressibility."],"url":"http://arxiv.org/abs/2403.17840v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-26 15:53:07","title":"Probing the shape of the quark-gluon plasma droplet via event-by-event QGP tomography","abstract":"This study investigates Quark-Gluon Plasma (QGP) in heavy-ion collisions through two avenues: high-$p_{\\perp}$ frameworks and hydrodynamic modeling. Using the T$_{\\text{R}}$ENTo model, we find that IP-Glasma mimicking $p=0$ value aligns well with high-$p_{\\perp}$ data, in agreement with Bayesian analysis of the low-$p_{\\perp}$ regime. While adjusting $p$ values may improve a fit to a particular high-$p_{\\perp}$ observable, it does not permit an earlier onset of transverse expansion.","sentences":["This study investigates Quark-Gluon Plasma (QGP) in heavy-ion collisions through two avenues: high-$p_{\\perp}$ frameworks and hydrodynamic modeling.","Using the T$_{\\text{R}}$ENTo model, we find that IP-Glasma mimicking $p=0$ value aligns well with high-$p_{\\perp}$ data, in agreement with Bayesian analysis of the low-$p_{\\perp}$ regime.","While adjusting $p$ values may improve a fit to a particular high-$p_{\\perp}$ observable, it does not permit an earlier onset of transverse expansion."],"url":"http://arxiv.org/abs/2403.17817v1","category":"hep-ph"}
{"created":"2024-03-26 15:52:51","title":"Ultrafast Heating Induced Suppression of $d$-band Dominance in the Electronic Excitation Spectrum of Cuprum","abstract":"The combination of isochoric heating of solids by free electron lasers (FEL) and in situ diagnostics by X-ray Thomson scattering (XRTS) allows for measurements of material properties at warm dense matter (WDM) conditions relevant for astrophysics, inertial confinement fusion, and material science. In the case of metals, the FEL beam pumps energy directly into electrons with the lattice structure of ions being nearly unaffected. This leads to a unique transient state that gives rise to a set of interesting physical effects, which can serve as a reliable testing platform for WDM theories. In this work, we present extensive linear-response time-dependent density functional theory (TDDFT) results for the electronic dynamic structure factor of isochorically heated copper with a face-centered cubic lattice. At ambient conditions, the plasmon is heavily damped due to the presence of $d$-band excitations, and its position is independent of the wavenumber. In contrast, the plasmon feature starts to dominate the excitation spectrum and has a Bohm-Gross type plasmon dispersion for temperatures $T \\geq 4~{\\rm eV}$, where the quasi-free electrons in the interstitial region are in the WDM regime. In addition, we analyze the thermal changes in the $d$-band excitations and outline the possibility to use future XRTS measurements of isochorically heated copper as a controlled testbed for WDM theories.","sentences":["The combination of isochoric heating of solids by free electron lasers (FEL) and in situ diagnostics by X-ray Thomson scattering (XRTS) allows for measurements of material properties at warm dense matter (WDM) conditions relevant for astrophysics, inertial confinement fusion, and material science.","In the case of metals, the FEL beam pumps energy directly into electrons with the lattice structure of ions being nearly unaffected.","This leads to a unique transient state that gives rise to a set of interesting physical effects, which can serve as a reliable testing platform for WDM theories.","In this work, we present extensive linear-response time-dependent density functional theory (TDDFT) results for the electronic dynamic structure factor of isochorically heated copper with a face-centered cubic lattice.","At ambient conditions, the plasmon is heavily damped due to the presence of $d$-band excitations, and its position is independent of the wavenumber.","In contrast, the plasmon feature starts to dominate the excitation spectrum and has a Bohm-Gross type plasmon dispersion for temperatures $T \\geq 4~{\\rm eV}$, where the quasi-free electrons in the interstitial region are in the WDM regime.","In addition, we analyze the thermal changes in the $d$-band excitations and outline the possibility to use future XRTS measurements of isochorically heated copper as a controlled testbed for WDM theories."],"url":"http://arxiv.org/abs/2403.17815v1","category":"physics.chem-ph"}
{"created":"2024-03-26 15:52:16","title":"Non-commutative factorizations and finite-dimensional representations of free algebras","abstract":"A very first step to develop non-commutative algebraic geometry is the arithmetic of polynomials in non-commuting variables over a commutative field, that is, the study of elements in free associative algebras. This investigation is presented as a natural extension of the classical theory in one variable by using Leavitt algebras, which are localizations of free algebras with respect to the Gabriel topology defined by an ideal of codimension 1. In particular to any polynomial in n non-commuting variables with non-zero constant term we associate a finite-dimensional module over the free algebra of rank n, which turns out to be simple if and only if the polynomial is irreducible. This approach leads to new insights in the study of the factorization of polynomials into irreducible ones and other related topics, such as an algorithm to divide polynomials or to compute the greatest common divisor between them, or a description of similar polynomials. The case of polynomials with zero constant term reduces to an open question whether the intersection of nonunital subalgebras of codimension 1 in a free associative algebra is trivial, that is, 0.","sentences":["A very first step to develop non-commutative algebraic geometry is the arithmetic of polynomials in non-commuting variables over a commutative field, that is, the study of elements in free associative algebras.","This investigation is presented as a natural extension of the classical theory in one variable by using Leavitt algebras, which are localizations of free algebras with respect to the Gabriel topology defined by an ideal of codimension 1.","In particular to any polynomial in n non-commuting variables with non-zero constant term we associate a finite-dimensional module over the free algebra of rank n, which turns out to be simple if and only if the polynomial is irreducible.","This approach leads to new insights in the study of the factorization of polynomials into irreducible ones and other related topics, such as an algorithm to divide polynomials or to compute the greatest common divisor between them, or a description of similar polynomials.","The case of polynomials with zero constant term reduces to an open question whether the intersection of nonunital subalgebras of codimension 1 in a free associative algebra is trivial, that is, 0."],"url":"http://arxiv.org/abs/2403.17813v1","category":"math.RA"}
{"created":"2024-03-26 15:25:21","title":"A Novel Temperature-based Model for SWIPT","abstract":"In this letter, a novel communication paradigm for simultaneous wireless information and power transfer (SWIPT) is proposed, which leverages the thermal characteristics of electromagnetic signals. In particular, the proposed scheme exploits the inherent thermal dynamics of electromagnetic signals, enabling the seamless integration of information decoding and energy harvesting (EH). As a consequence, in contrast to conventional SWIPT techniques, the proposed model eliminates the need to divide the received signal into orthogonal components. By exploiting the thermal correlation between consecutive time slots, the communication channel is converted to a virtual multiple-input multiple-output (MIMO) channel with memory. We evaluate the achievable rate of the proposed temperature-modulated channel for uniform and exponential input distributions and assess its performance in terms of harvested energy through a non-linear harvesting model. Our numerical results reveal that the exponential distribution outperforms the uniform distribution in rate and harvested energy at low input power levels, while the uniform distribution achieves a better EH performance at high input power levels.","sentences":["In this letter, a novel communication paradigm for simultaneous wireless information and power transfer (SWIPT) is proposed, which leverages the thermal characteristics of electromagnetic signals.","In particular, the proposed scheme exploits the inherent thermal dynamics of electromagnetic signals, enabling the seamless integration of information decoding and energy harvesting (EH).","As a consequence, in contrast to conventional SWIPT techniques, the proposed model eliminates the need to divide the received signal into orthogonal components.","By exploiting the thermal correlation between consecutive time slots, the communication channel is converted to a virtual multiple-input multiple-output (MIMO) channel with memory.","We evaluate the achievable rate of the proposed temperature-modulated channel for uniform and exponential input distributions and assess its performance in terms of harvested energy through a non-linear harvesting model.","Our numerical results reveal that the exponential distribution outperforms the uniform distribution in rate and harvested energy at low input power levels, while the uniform distribution achieves a better EH performance at high input power levels."],"url":"http://arxiv.org/abs/2403.17792v1","category":"cs.IT"}
{"created":"2024-03-26 14:52:25","title":"CPT and Lorentz symmetry tests with hydrogen using a novel in-beam hyperfine spectroscopy method applicable to antihydrogen experiments","abstract":"We present a Rabi-type measurement of two ground-state hydrogen hyperfine transitions performed in two opposite external magnetic field directions. This puts first constraints at the level of 2.3 10^-21 GeV on a set of coefficients of the Standard Model Extension, which were not measured by previous experiments. Moreover, we introduce a novel method, applicable to antihydrogen hyperfine spectroscopy in a beam, that determines the zero-field hyperfine transition frequency from the two transitions measured at the same magnetic field. Our value, nu_0 = 1.420 405 751 63(63) GHz, is in agreement with literature at a relative precision of 0.44 ppb. This is the highest precision achieved on hydrogen in a beam, improving over previous results by a factor of 6.","sentences":["We present a Rabi-type measurement of two ground-state hydrogen hyperfine transitions performed in two opposite external magnetic field directions.","This puts first constraints at the level of 2.3 10^-21","GeV on a set of coefficients of the Standard Model Extension, which were not measured by previous experiments.","Moreover, we introduce a novel method, applicable to antihydrogen hyperfine spectroscopy in a beam, that determines the zero-field hyperfine transition frequency from the two transitions measured at the same magnetic field.","Our value, nu_0 = 1.420 405 751 63(63) GHz, is in agreement with literature at a relative precision of 0.44 ppb.","This is the highest precision achieved on hydrogen in a beam, improving over previous results by a factor of 6."],"url":"http://arxiv.org/abs/2403.17763v1","category":"hep-ex"}
{"created":"2024-03-26 14:28:46","title":"The Unified Era: An understanding journey from observations to the Unified Model of Active Galactic Nuclei","abstract":"The Unified Model of Active Galactic Nuclei (UMAGN) is a comprehensive theoretical framework aimed at elucidating the diverse observations of AGN, encompassing quasars and Seyfert galaxies. The Model attributes the observed variations to different orientations of a surrounding matter disk around a supermassive black hole (SMBH), with the primary factor influencing observational diversity being the alignment of the AGN with the observer's line of sight. We present a comprehensive overview of the observational evidence, empirical and theoretical research, tracing key milestones that led to a unified perspective. We encapsulate the scientific journey culminating in the proposal of the UMAGN, including insights into the accretion disk, torus, and relativistic jet, and emphasize the properties of objects within UMAGN. Additionally, we underscore recent progress in multimessenger research involving electromagnetic waves, gravitational waves, astroparticles, and neutrinos, notably through collaborations such as the Event Horizon Telescope, LIGO/Virgo, IceCube, Pierre Auger, and KM3Net. We argue that these advancements present opportunities to enhance and refine UMAGN, contributing to a deeper understanding of AGNs and their implications for the formation and evolution of galaxies. The convergence of observational and theoretical research, coupled with emerging multimessenger techniques, paves the way for further strides in comprehending these enigmatic cosmic phenomena.","sentences":["The Unified Model of Active Galactic Nuclei (UMAGN) is a comprehensive theoretical framework aimed at elucidating the diverse observations of AGN, encompassing quasars and Seyfert galaxies.","The Model attributes the observed variations to different orientations of a surrounding matter disk around a supermassive black hole (SMBH), with the primary factor influencing observational diversity being the alignment of the AGN with the observer's line of sight.","We present a comprehensive overview of the observational evidence, empirical and theoretical research, tracing key milestones that led to a unified perspective.","We encapsulate the scientific journey culminating in the proposal of the UMAGN, including insights into the accretion disk, torus, and relativistic jet, and emphasize the properties of objects within UMAGN.","Additionally, we underscore recent progress in multimessenger research involving electromagnetic waves, gravitational waves, astroparticles, and neutrinos, notably through collaborations such as the Event Horizon Telescope, LIGO/Virgo, IceCube, Pierre Auger, and KM3Net.","We argue that these advancements present opportunities to enhance and refine UMAGN, contributing to a deeper understanding of AGNs and their implications for the formation and evolution of galaxies.","The convergence of observational and theoretical research, coupled with emerging multimessenger techniques, paves the way for further strides in comprehending these enigmatic cosmic phenomena."],"url":"http://arxiv.org/abs/2403.17739v1","category":"astro-ph.GA"}
{"created":"2024-03-26 14:27:06","title":"Renormalization of the next-to-leading-power $\u03b3\u03b3\\to h $ and $gg\\to h$ soft quark functions","abstract":"We calculate directly in position space the one-loop renormalization kernels of the soft operators $O_\\gamma$ and $O_g$ that appear in the soft-quark contributions to, respectively, the subleading-power $\\gamma\\gamma\\to h$ and $gg\\to h$ form factors mediated by the $b$-quark. We present an IR/rapidity divergence-free definition for $O_g$ and demonstrate that with a correspondent definition of the collinear function, a consistent factorization theorem is recovered. Using conformal symmetry techniques, we establish a relation between the evolution kernels of the leading-twist heavy-light light-ray operator, whose matrix element defines the $B$-meson light-cone distribution amplitude (LCDA), and $O_\\gamma$ to all orders in perturbation theory. Application of this relation allows us to bootstrap the kernel of $O_\\gamma$ to the two-loop level. We construct an ansatz for the kernel of $O_g$ at higher orders. We test this ansatz against the consistency requirement at two-loop and find they differ only by a particular constant.","sentences":["We calculate directly in position space the one-loop renormalization kernels of the soft operators $O_\\gamma$ and $O_g$ that appear in the soft-quark contributions to, respectively, the subleading-power $\\gamma\\gamma\\to h$ and $gg\\to h$ form factors mediated by the $b$-quark.","We present an IR/rapidity divergence-free definition for $O_g$ and demonstrate that with a correspondent definition of the collinear function, a consistent factorization theorem is recovered.","Using conformal symmetry techniques, we establish a relation between the evolution kernels of the leading-twist heavy-light light-ray operator, whose matrix element defines the $B$-meson light-cone distribution amplitude (LCDA), and $O_\\gamma$ to all orders in perturbation theory.","Application of this relation allows us to bootstrap the kernel of $O_\\gamma$ to the two-loop level.","We construct an ansatz for the kernel of $O_g$ at higher orders.","We test this ansatz against the consistency requirement at two-loop and find they differ only by a particular constant."],"url":"http://arxiv.org/abs/2403.17738v1","category":"hep-ph"}
{"created":"2024-03-26 14:06:23","title":"Nuclear matrix elements of neutrinoless double-beta decay in covariant density functional theory with different mechanisms","abstract":"Nuclear matrix elements (NMEs) for neutrinoless double-beta ($0\\nu\\beta\\beta$) decay in candidate nuclei play a crucial role in interpreting results from current experiments and in designing future ones. Accurate NME values serve as important nuclear inputs for constraining parameters in new physics, such as neutrino mass and the Wilson coefficients of lepton-number-violating (LNV) operators. In this study, we present a comprehensive calculation of NMEs for $0\\nu\\beta\\beta$ decay in $^{76}$Ge, $^{82}$Se, $^{100}$Mo, $^{130}$Te, and $^{136}$Xe, using nuclear wave functions obtained from multi-reference covariant density functional theory (MR-CDFT). We employ three types of transition potentials at the leading order in chiral effective field theory. Our results, along with recent data, are utilized to constrain the coefficients of LNV operators. We find that NMEs for the standard and short-range mechanisms are significantly larger than those for the non-standard long-range mechanism. The use of NMEs from various nuclear models does not notably change the parameter space intervals for the coefficients, although MR-CDFT yields the most stringent constraint. Furthermore, our NMEs can also be used to perform a more comprehensive analysis with multiple isotopes.","sentences":["Nuclear matrix elements (NMEs) for neutrinoless double-beta ($0\\nu\\beta\\beta$) decay in candidate nuclei play a crucial role in interpreting results from current experiments and in designing future ones.","Accurate NME values serve as important nuclear inputs for constraining parameters in new physics, such as neutrino mass and the Wilson coefficients of lepton-number-violating (LNV) operators.","In this study, we present a comprehensive calculation of NMEs for $0\\nu\\beta\\beta$ decay in $^{76}$Ge, $^{82}$Se, $^{100}$Mo, $^{130}$Te, and $^{136}$Xe, using nuclear wave functions obtained from multi-reference covariant density functional theory (MR-CDFT).","We employ three types of transition potentials at the leading order in chiral effective field theory.","Our results, along with recent data, are utilized to constrain the coefficients of LNV operators.","We find that NMEs for the standard and short-range mechanisms are significantly larger than those for the non-standard long-range mechanism.","The use of NMEs from various nuclear models does not notably change the parameter space intervals for the coefficients, although MR-CDFT yields the most stringent constraint.","Furthermore, our NMEs can also be used to perform a more comprehensive analysis with multiple isotopes."],"url":"http://arxiv.org/abs/2403.17722v1","category":"nucl-th"}
{"created":"2024-03-26 14:05:34","title":"Electron Diffusion in Microbunched Electron Cooling","abstract":"Coherent electron cooling is a novel method to cool dense hadron beams on timescales of a few hours. This method uses a copropagating beam of electrons to pick up the density fluctuations within the hadron beam in one straight section and then provide corrective energy kicks to the hadrons in a downstream straight, cooling the beam. Microbunched electron cooling is an extension of this idea which induces a microbunching instability in the electron beam as it travels between the two straights, amplifying the signal. However, initial noise in the electron bunch will also be amplified, providing random kicks to the hadrons downstream which tend to increase their emittance. In this paper, we develop an analytic estimate of the effect of the electron noise and benchmark it against simulations.","sentences":["Coherent electron cooling is a novel method to cool dense hadron beams on timescales of a few hours.","This method uses a copropagating beam of electrons to pick up the density fluctuations within the hadron beam in one straight section and then provide corrective energy kicks to the hadrons in a downstream straight, cooling the beam.","Microbunched electron cooling is an extension of this idea which induces a microbunching instability in the electron beam as it travels between the two straights, amplifying the signal.","However, initial noise in the electron bunch will also be amplified, providing random kicks to the hadrons downstream which tend to increase their emittance.","In this paper, we develop an analytic estimate of the effect of the electron noise and benchmark it against simulations."],"url":"http://arxiv.org/abs/2403.17721v1","category":"physics.acc-ph"}
{"created":"2024-03-26 14:00:22","title":"On the first eigenvalue and eigenfunction of the Laplacian with mixed boundary conditions","abstract":"We consider the eigenvalue problem for the Laplacian with mixed Dirichlet and Neumann boundary conditions. For a certain class of bounded, simply connected planar domains we prove monotonicity properties of the first eigenfunction. As a consequence, we establish a variant of the hot spots conjecture for mixed boundary conditions. Moreover, we obtain an inequality between the lowest eigenvalue of this mixed problem and the lowest eigenvalue of the corresponding dual problem where the Dirichlet and Neumann boundary conditions are interchanged. The proofs are based on a novel variational principle, which we establish.","sentences":["We consider the eigenvalue problem for the Laplacian with mixed Dirichlet and Neumann boundary conditions.","For a certain class of bounded, simply connected planar domains we prove monotonicity properties of the first eigenfunction.","As a consequence, we establish a variant of the hot spots conjecture for mixed boundary conditions.","Moreover, we obtain an inequality between the lowest eigenvalue of this mixed problem and the lowest eigenvalue of the corresponding dual problem where the Dirichlet and Neumann boundary conditions are interchanged.","The proofs are based on a novel variational principle, which we establish."],"url":"http://arxiv.org/abs/2403.17717v1","category":"math.SP"}
{"created":"2024-03-26 13:37:47","title":"Cosmology of axion dark matter","abstract":"I present an introduction and topical review on axions as a dark matter candidate. Emphasis is placed on issues surrounding the cosmology of axion dark matter that are relevant for present-day searches, including: early-Universe production mechanisms, predictions of the axion mass, bounds on axion properties derived from cosmological data, as well as the direct and indirect detection of relic axion populations.","sentences":["I present an introduction and topical review on axions as a dark matter candidate.","Emphasis is placed on issues surrounding the cosmology of axion dark matter that are relevant for present-day searches, including: early-Universe production mechanisms, predictions of the axion mass, bounds on axion properties derived from cosmological data, as well as the direct and indirect detection of relic axion populations."],"url":"http://arxiv.org/abs/2403.17697v1","category":"hep-ph"}
{"created":"2024-03-26 12:25:12","title":"Orbital variability of polarized X-ray radiation reflected from a companion star in X-ray binaries","abstract":"The reflection of X-ray radiation produced near a compact object from its stellar companion contributes to the orbital variability of polarization in X-ray binaries. The X-rays are reflected mainly via Thomson scattering resulting in a high polarization. The orbital variability of the polarization strongly depends on the inclination and the orbital parameters allowing us to constrain them. To explore this phenomenon, we present analytical single-scattering models for the polarized reflection. We find that while diluted by the direct emission, the reflection can produce a polarization degree of about 1$\\%$ in the case of a large reflection albedo. We fitted the orbital variations of the X-ray polarization observed by the Imaging X-ray Polarimetry Explorer from an accreting weakly magnetized neutron star `clocked burster' GS 1826$-$238 and found that the amplitude of the variations is too large to be primarily caused by the companion star. The polarized reflection is more significant if the compact object is obscured from the observer, and thus it should be more easily observable in certain high-inclination targets.","sentences":["The reflection of X-ray radiation produced near a compact object from its stellar companion contributes to the orbital variability of polarization in X-ray binaries.","The X-rays are reflected mainly via Thomson scattering resulting in a high polarization.","The orbital variability of the polarization strongly depends on the inclination and the orbital parameters allowing us to constrain them.","To explore this phenomenon, we present analytical single-scattering models for the polarized reflection.","We find that while diluted by the direct emission, the reflection can produce a polarization degree of about 1$\\%$ in the case of a large reflection albedo.","We fitted the orbital variations of the X-ray polarization observed by the Imaging X-ray Polarimetry Explorer from an accreting weakly magnetized neutron star `clocked burster' GS 1826$-$238 and found that the amplitude of the variations is too large to be primarily caused by the companion star.","The polarized reflection is more significant if the compact object is obscured from the observer, and thus it should be more easily observable in certain high-inclination targets."],"url":"http://arxiv.org/abs/2403.17644v1","category":"astro-ph.HE"}
{"created":"2024-03-26 12:21:51","title":"REFeREE: A REference-FREE Model-Based Metric for Text Simplification","abstract":"Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly. We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available. Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time.","sentences":["Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly.","We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum.","REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available.","Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time."],"url":"http://arxiv.org/abs/2403.17640v1","category":"cs.CL"}
{"created":"2024-03-26 11:57:33","title":"Ground- and excited-state properties of LiNb$_{1-x}$Ta$_x$O$_3$ solid solutions","abstract":"LiNb$_{1-x}$Ta$_x$O$_3$ solid solutions are investigated from first principles and by optical spectroscopy. The ground- and excited-state properties of the solid solutions are modelled within density functional theory as a function of the Ta concentration using special quasirandom structures spanning the entire composition range between LiNbO$_3$ and LiTaO$_3$. Deviations from a Vegard behavior are predicted for the lattice parameters, the heat capacity, the electronic bandgap, and consequently the absorption edge. The latter is measured for crystals of different composition by low temperature optical spectroscopy, qualitatively confirming the theoretical predictions. The LiNb$_{0.11}$Ta$_{0.89}$O$_3$ composition is found to be a highly unusual crystal with a permanent macroscopic electric polarization and nonetheless zero birefringence.","sentences":["LiNb$_{1-x}$Ta$_x$O$_3$ solid solutions are investigated from first principles and by optical spectroscopy.","The ground- and excited-state properties of the solid solutions are modelled within density functional theory as a function of the Ta concentration using special quasirandom structures spanning the entire composition range between LiNbO$_3$ and LiTaO$_3$. Deviations from a Vegard behavior are predicted for the lattice parameters, the heat capacity, the electronic bandgap, and consequently the absorption edge.","The latter is measured for crystals of different composition by low temperature optical spectroscopy, qualitatively confirming the theoretical predictions.","The LiNb$_{0.11}$Ta$_{0.89}$O$_3$ composition is found to be a highly unusual crystal with a permanent macroscopic electric polarization and nonetheless zero birefringence."],"url":"http://arxiv.org/abs/2403.17623v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 11:51:47","title":"Ferroelectric to paraelectric structural transition in LiTaO$_3$ and LiNbO$_3$","abstract":"The ferroelectric to paraelectric phase transition in LiTaO$_3$ and in pure as well as Mg doped LiNbO$_3$ is investigated theoretically by atomistic calculations in the framework of the density functional theory, as well as experimentally by calorimetry and electrical conductivity measurements. First principles models within the stochastic self-consistent harmonic approximation (SSCHA) allow to consider anharmonic effects and thus to obtain a realistic estimate of the Curie temperature $T_C$ of both ferroelectrics. \\textit{Ab initio} molecular dynamics (AIMD) calculations performed on large supercells confirm the Curie temperatures estimated with the SSCHA approach. Moreover, they also suggest that the structural phase transition is a continuous process beginning at temperatures well below $T_C$. According to AIMD, significant ionic displacements occurr already at temperatures of about 100\\,K and 300\\,K below $T_C$ in LiTaO$_3$ and LiNbO$_3$, respectively. To asses whether and how far the ionic displacements affect the materials properties, the AIMD results are compared with measurements of the electrical conductivity and of the heat capacity across the phase transition. Our first principles calculations moreover show that Mg ions, a frequently employed dopant, raise the Curie temperature in LiNbO$_3$.","sentences":["The ferroelectric to paraelectric phase transition in LiTaO$_3$ and in pure as well as Mg doped LiNbO$_3$ is investigated theoretically by atomistic calculations in the framework of the density functional theory, as well as experimentally by calorimetry and electrical conductivity measurements.","First principles models within the stochastic self-consistent harmonic approximation (SSCHA) allow to consider anharmonic effects and thus to obtain a realistic estimate of the Curie temperature $T_C$ of both ferroelectrics.","\\textit{Ab initio} molecular dynamics (AIMD) calculations performed on large supercells confirm the Curie temperatures estimated with the SSCHA approach.","Moreover, they also suggest that the structural phase transition is a continuous process beginning at temperatures well below $T_C$. According to AIMD, significant ionic displacements occurr already at temperatures of about 100\\,K and 300\\,K below $T_C$ in LiTaO$_3$ and LiNbO$_3$, respectively.","To asses whether and how far the ionic displacements affect the materials properties, the AIMD results are compared with measurements of the electrical conductivity and of the heat capacity across the phase transition.","Our first principles calculations moreover show that Mg ions, a frequently employed dopant, raise the Curie temperature in LiNbO$_3$."],"url":"http://arxiv.org/abs/2403.17620v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 11:09:58","title":"Coimagining the Future of Voice Assistants with Cultural Sensitivity","abstract":"Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the user experience (UX) is often limited, leading to underuse, disengagement, and abandonment. Co-designing interactions for VAs with potential end-users can be useful. Crowdsourcing this process online and anonymously may add value. However, most work has been done in the English-speaking West on dialogue data sets. We must be sensitive to cultural differences in language, social interactions, and attitudes towards technology. Our aims were to explore the value of co-designing VAs in the non-Western context of Japan and demonstrate the necessity of cultural sensitivity. We conducted an online elicitation study (N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the implications for coimagining interactions with future VAs, offer design guidelines for the Japanese and English-speaking US contexts, and suggest opportunities for cultural plurality in VA design and scholarship.","sentences":["Voice assistants (VAs) are becoming a feature of our everyday life.","Yet, the user experience (UX) is often limited, leading to underuse, disengagement, and abandonment.","Co-designing interactions for VAs with potential end-users can be useful.","Crowdsourcing this process online and anonymously may add value.","However, most work has been done in the English-speaking West on dialogue data sets.","We must be sensitive to cultural differences in language, social interactions, and attitudes towards technology.","Our aims were to explore the value of co-designing VAs in the non-Western context of Japan and demonstrate the necessity of cultural sensitivity.","We conducted an online elicitation study (N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined dialogues (N = 282) and activities (N = 73) with future VAs.","We discuss the implications for coimagining interactions with future VAs, offer design guidelines for the Japanese and English-speaking US contexts, and suggest opportunities for cultural plurality in VA design and scholarship."],"url":"http://arxiv.org/abs/2403.17599v1","category":"cs.HC"}
{"created":"2024-03-26 10:08:16","title":"Anomalous shift in Andreev reflection from side incidence","abstract":"Andreev reflection at a normal-superconductor interface may be accompanied with an anomalous spatial shift. The studies so far are limited to the top incidence configuration. Here, we investigate this effect in the side incidence configuration, with the interface parallel to the principal axis of superconductor. We find that the shift exhibits rich behaviors reflecting the character of pair potential. It has two contributions: one from the $k$-dependent phase of pair potential, and the other from the evanescent mode. For chiral $p$-wave pairing, the pairing phase contribution is proportional to the chirality of pairing and is independent of excitation energy, whereas the evanescent mode contribution is independent of chirality and is nonzero only for excitation energy below the gap. The two contributions also have opposite parity with respect to the incident angle. For $d_{x^{2}-y^{2}}$-wave pairing, only the evanescent mode contribution exists, and the shift exhibits suppressed zones in incident angles, manifesting the superconducting nodes. The dependence of the shift on other factors, such as the angle of incident plane and Fermi surface anisotropy, are discussed.","sentences":["Andreev reflection at a normal-superconductor interface may be accompanied with an anomalous spatial shift.","The studies so far are limited to the top incidence configuration.","Here, we investigate this effect in the side incidence configuration, with the interface parallel to the principal axis of superconductor.","We find that the shift exhibits rich behaviors reflecting the character of pair potential.","It has two contributions: one from the $k$-dependent phase of pair potential, and the other from the evanescent mode.","For chiral $p$-wave pairing, the pairing phase contribution is proportional to the chirality of pairing and is independent of excitation energy, whereas the evanescent mode contribution is independent of chirality and is nonzero only for excitation energy below the gap.","The two contributions also have opposite parity with respect to the incident angle.","For $d_{x^{2}-y^{2}}$-wave pairing, only the evanescent mode contribution exists, and the shift exhibits suppressed zones in incident angles, manifesting the superconducting nodes.","The dependence of the shift on other factors, such as the angle of incident plane and Fermi surface anisotropy, are discussed."],"url":"http://arxiv.org/abs/2403.17560v1","category":"cond-mat.supr-con"}
{"created":"2024-03-26 09:12:51","title":"Unification of Conformal Gravity and Internal Interactions","abstract":"Based on the observation that the dimension of the tangent space is not necessarily equal to the dimension of the corresponding curved manifold and on the known fact that gravitational theories can be formulated in a gauge theoretic way, we discuss how to describe all known interactions in a unified manner. This is achieved by enlarging the tangent group of the four-dimensional manifold to $SO(2,16)$, which permits the inclusion of both gauge groups, the one that describes gravity as a gauge theory as well as the $SO(10)$ describing the internal interactions. Moreover it permits the use of both Weyl and Majorana conditions imposed on the fermions, as to avoid the duplication of fermion multiplets of $SO(10)$ appearing in previous attempts. The gravity theory discussed in the present work is the Conformal Gravity which, after a spontaneous symmetry breaking, can lead either to Weyl Gravity or to the usual Einstein Gravity.","sentences":["Based on the observation that the dimension of the tangent space is not necessarily equal to the dimension of the corresponding curved manifold and on the known fact that gravitational theories can be formulated in a gauge theoretic way, we discuss how to describe all known interactions in a unified manner.","This is achieved by enlarging the tangent group of the four-dimensional manifold to $SO(2,16)$, which permits the inclusion of both gauge groups, the one that describes gravity as a gauge theory as well as the $SO(10)$ describing the internal interactions.","Moreover it permits the use of both Weyl and Majorana conditions imposed on the fermions, as to avoid the duplication of fermion multiplets of $SO(10)$ appearing in previous attempts.","The gravity theory discussed in the present work is the Conformal Gravity which, after a spontaneous symmetry breaking, can lead either to Weyl Gravity or to the usual Einstein Gravity."],"url":"http://arxiv.org/abs/2403.17511v1","category":"hep-th"}
{"created":"2024-03-26 09:05:00","title":"Stability evaluation of approximate Riemann solvers using the direct Lyapunov method","abstract":"The paper presents a new approach of stability evaluation of the approximate Riemann solvers based on the direct Lyapunov method. The present methodology offers a detailed understanding of the origins of numerical shock instability in the approximate Riemann solvers. The pressure perturbation feeding the density and transverse momentum perturbations is identified as the cause of the numerical shock instabilities in the complete approximate Riemann solvers, while the magnitude of the numerical shock instabilities are found to be proportional to the magnitude of the pressure perturbations. A shock-stable HLLEM scheme is proposed based on the insights obtained from this analysis about the origins of numerical shock instability in the approximate Riemann solvers. A set of numerical test cases are solved to show that the proposed scheme is free from numerical shock instability problems of the original HLLEM scheme at high Mach numbers.","sentences":["The paper presents a new approach of stability evaluation of the approximate Riemann solvers based on the direct Lyapunov method.","The present methodology offers a detailed understanding of the origins of numerical shock instability in the approximate Riemann solvers.","The pressure perturbation feeding the density and transverse momentum perturbations is identified as the cause of the numerical shock instabilities in the complete approximate Riemann solvers, while the magnitude of the numerical shock instabilities are found to be proportional to the magnitude of the pressure perturbations.","A shock-stable HLLEM scheme is proposed based on the insights obtained from this analysis about the origins of numerical shock instability in the approximate Riemann solvers.","A set of numerical test cases are solved to show that the proposed scheme is free from numerical shock instability problems of the original HLLEM scheme at high Mach numbers."],"url":"http://arxiv.org/abs/2403.17504v1","category":"math.NA"}
{"created":"2024-03-26 09:00:32","title":"Model-independent description of $B\\rightarrow D \u03c0\\ell \u03bd$ decays","abstract":"In this contribution we present a novel, model-independent description of semileptonic $B\\rightarrow D \\pi \\ell \\nu$ decays. In addition, we discuss recent developments in the understanding of coupled-channel $D \\pi$-$D \\eta$-$D_s K$ S-wave scattering and, for the first time, apply them to semileptonic decays. We not only obtain model-independent predictions for kinematic distributions in $B\\rightarrow D \\pi \\ell \\nu$ decays, but also rule out the hypothesis that the gap between the inclusive $B\\rightarrow X\\ell\\nu$ branching fraction and the sum over exclusive channels is made up predominantly by $B\\rightarrow D^{(\\ast)} \\eta \\ell \\nu$ decays.","sentences":["In this contribution we present a novel, model-independent description of semileptonic $B\\rightarrow D \\pi \\ell \\nu$ decays.","In addition, we discuss recent developments in the understanding of coupled-channel $D \\pi$-$D \\eta$-$D_s","K$ S-wave scattering and, for the first time, apply them to semileptonic decays.","We not only obtain model-independent predictions for kinematic distributions in $B\\rightarrow D \\pi \\ell \\nu$ decays, but also rule out the hypothesis that the gap between the inclusive $B\\rightarrow X\\ell\\nu$ branching fraction and the sum over exclusive channels is made up predominantly by $B\\rightarrow D^{(\\ast)} \\eta \\ell \\nu$ decays."],"url":"http://arxiv.org/abs/2403.17501v1","category":"hep-ph"}
{"created":"2024-03-26 07:41:08","title":"TeV scale Signatures of Lepton Flavour Violation","abstract":"The juxtaposition of the precision of lepton flavour measurements and the limited energy range of the Large Hadron Collider (LHC) to discover dynamical degrees of freedom linked to the generation of the observed lepton mass patterns naively suggests only a limited relevance of the LHC's high luminosity phase. This, potentially, extends to future colliders. We show that minimal non-standard modifications of widely considered scenarios such as the type-II seesaw model can, however, lead to a rich phenomenological interplay of muon precision measurements and electroweak resonance searches at present and future colliders, with testable implications for the HL-LHC phase.","sentences":["The juxtaposition of the precision of lepton flavour measurements and the limited energy range of the Large Hadron Collider (LHC) to discover dynamical degrees of freedom linked to the generation of the observed lepton mass patterns naively suggests only a limited relevance of the LHC's high luminosity phase.","This, potentially, extends to future colliders.","We show that minimal non-standard modifications of widely considered scenarios such as the type-II seesaw model can, however, lead to a rich phenomenological interplay of muon precision measurements and electroweak resonance searches at present and future colliders, with testable implications for the HL-LHC phase."],"url":"http://arxiv.org/abs/2403.17455v1","category":"hep-ph"}
{"created":"2024-03-26 05:54:00","title":"A Full Resolution of the 450 \u03bcm Extragalactic Background Light","abstract":"The extragalactic background light (EBL) is the cumulative radiation outside the Milky Way. The determination of its corresponding primary emitting sources as well as its total energy level across the entire electromagnetic spectrum has profound implications for both cosmology and galaxy formation. However, the detailed origin of the EBL at far-infrared wavelengths, particularly those close to the peak of the cosmic infrared background, remains unclear. Here we report the results of our ongoing SCUBA-2 450 $\\mu$m survey of 10 massive galaxy cluster fields. By exploiting the strong gravitational lensing offered by these clusters, we obtain significant counts down to an unprecedented depth of $\\sim$0.1 mJy at this wavelength, about ten times deeper than that reached by any other previous survey. The cumulative energy density based on the counts is 138.1$^{+23.9}_{-19.3}$ Jy degree$^{-2}$, or 0.45$^{+0.08}_{-0.06}$ MJy sr$^{-1}$. Comparing our measurements to those made by the COBE and Planck satellites, we find that at this flux density level, the 450 $\\mu$m EBL is entirely resolved by our SCUBA-2 observations. Thus, we find for the first time that discrete sources produce fully to the 450 $\\mu$m EBL, and that about half of it comes from sources with sub-mJy flux densities. Our deep number counts provide strong constraints on galaxy formation models.","sentences":["The extragalactic background light (EBL) is the cumulative radiation outside the Milky Way.","The determination of its corresponding primary emitting sources as well as its total energy level across the entire electromagnetic spectrum has profound implications for both cosmology and galaxy formation.","However, the detailed origin of the EBL at far-infrared wavelengths, particularly those close to the peak of the cosmic infrared background, remains unclear.","Here we report the results of our ongoing SCUBA-2 450","$\\mu$m survey of 10 massive galaxy cluster fields.","By exploiting the strong gravitational lensing offered by these clusters, we obtain significant counts down to an unprecedented depth of $\\sim$0.1 mJy at this wavelength, about ten times deeper than that reached by any other previous survey.","The cumulative energy density based on the counts is 138.1$^{+23.9}_{-19.3}$ Jy degree$^{-2}$, or 0.45$^{+0.08}_{-0.06}$ MJy sr$^{-1}$. Comparing our measurements to those made by the COBE and Planck satellites, we find that at this flux density level, the 450 $\\mu$m EBL is entirely resolved by our SCUBA-2 observations.","Thus, we find for the first time that discrete sources produce fully to the 450 $\\mu$m EBL, and that about half of it comes from sources with sub-mJy flux densities.","Our deep number counts provide strong constraints on galaxy formation models."],"url":"http://arxiv.org/abs/2403.17406v1","category":"astro-ph.CO"}
{"created":"2024-03-26 05:40:59","title":"New and old Saito-Kurokawa lifts classically via $L^2$ norms and bounds on their supnorms: level aspect","abstract":"In the first half of the paper, we lay down a classical approach to the study of Saito-Kurokawa (SK) lifts of (Hecke congruence) square-free level, including the allied new-oldform theory. Our treatment of this relies on a novel idea of computing ranks of certain matrices whose entries are $L^2$-norms of eigenforms. For computing the $L^2$ norms we work with the Hecke algebra of $\\mathrm{GSp}(2)$.   In the second half, we formulate precise conjectures on the $L^\\infty$ size of the space of SK lifts of square-free level, measured by the supremum of its Bergman kernel, and prove bounds towards them using the results from the first half. Here we rely on counting points on lattices, and on the geometric side of the Bergman kernels of spaces of Jacobi forms underlying the SK lifts. Along the way, we prove a non-trivial bound for the sup-norm of a Jacobi newform of square-free level and also discuss about their size on average.","sentences":["In the first half of the paper, we lay down a classical approach to the study of Saito-Kurokawa (SK) lifts of (Hecke congruence) square-free level, including the allied new-oldform theory.","Our treatment of this relies on a novel idea of computing ranks of certain matrices whose entries are $L^2$-norms of eigenforms.","For computing the $L^2$ norms we work with the Hecke algebra of $\\mathrm{GSp}(2)$.   In the second half, we formulate precise conjectures on the $L^\\infty$ size of the space of SK lifts of square-free level, measured by the supremum of its Bergman kernel, and prove bounds towards them using the results from the first half.","Here we rely on counting points on lattices, and on the geometric side of the Bergman kernels of spaces of Jacobi forms underlying the SK lifts.","Along the way, we prove a non-trivial bound for the sup-norm of a Jacobi newform of square-free level and also discuss about their size on average."],"url":"http://arxiv.org/abs/2403.17401v1","category":"math.NT"}
{"created":"2024-03-26 05:24:08","title":"Axial Anomalies of Maximally Supersymmetric Tensor Theories","abstract":"We revisit anomalies of $(4,0)$ and $(3,1)$ maximally supersymmetric tensor theories in $d=6$. A $(4,0)$ on-shell tensor multiplet descends to that of the $d=5$ maximal supergravity upon a dimensional reduction, hypothesized to offer a strong-coupled UV completion of the latter in the same sense of $(2,0)$ theories as the UV completion of $d=5$ $N=2$ pure Yang-Mills. The gravitational anomalies, found to be nonvanishing, had been computed, although its relevance in the absence of the $d=6$ metric is not obvious. We perform a comprehensive anomaly computation for (4,0) and (3,1) tensor supermultiplets, respectively, for $Sp(4)$ and $Sp(3)\\times Sp(1)$ $R$-symmetry anomalies and the mixed $R$-gravitational anomaly thereof, and find that anomalies involving $R$-symmetries cancel out identically. We close with questions on how to address the anomaly in this class of theories with no general covariance.","sentences":["We revisit anomalies of $(4,0)$ and $(3,1)$ maximally supersymmetric tensor theories in $d=6$. A $(4,0)$ on-shell tensor multiplet descends to that of the $d=5$ maximal supergravity upon a dimensional reduction, hypothesized to offer a strong-coupled UV completion of the latter in the same sense of $(2,0)$ theories as the UV completion of $d=5$ $N=2$ pure Yang-Mills.","The gravitational anomalies, found to be nonvanishing, had been computed, although its relevance in the absence of the $d=6$ metric is not obvious.","We perform a comprehensive anomaly computation for (4,0) and (3,1) tensor supermultiplets, respectively, for $Sp(4)$ and $Sp(3)\\times Sp(1)$ $R$-symmetry anomalies and the mixed $R$-gravitational anomaly thereof, and find that anomalies involving $R$-symmetries cancel out identically.","We close with questions on how to address the anomaly in this class of theories with no general covariance."],"url":"http://arxiv.org/abs/2403.17393v1","category":"hep-th"}
{"created":"2024-03-26 04:56:53","title":"Upper bound of higher-order Sobolev norms for Zkaharov system in one space dimension","abstract":"We study the Cauchy problem for the Zakharov system in one space dimension with the Diriclet boundary conditions. We establish the global well-posedness and the growth of higher-order Sobolev norms of solutions to the Zakharov system by using the modified energy method.","sentences":["We study the Cauchy problem for the Zakharov system in one space dimension with the Diriclet boundary conditions.","We establish the global well-posedness and the growth of higher-order Sobolev norms of solutions to the Zakharov system by using the modified energy method."],"url":"http://arxiv.org/abs/2403.17380v1","category":"math.AP"}
{"created":"2024-03-26 04:03:43","title":"Spatio-Temporal Correlation of Epileptic Seizures with The Electrocardiography Brain Perfusion Index","abstract":"The Electrocardiography Brain Perfusion index (EBPi) is a novel electrocardiography (ECG)-based metric that may function as a proxy for cerebral blood flow (CBF). We investigated the spatio-temporal correlation between EBPi and epileptic seizure events. EBPi was computed retrospectively from clinical EEG and ECG data captured previously from 30 epilepsy patients during seizures. Significant EBPi changes were compared temporally with clinically defined ground-truth seizure onset and offset times. We also assessed the spatial correlation between EBPi metrics and clinically defined ground-truth seizure locations. A significant increase in EBPi was detected 10.5 s [-6, 53] (median [95% confidence interval (CI)]) after ground-truth seizure onset, and a significant decrease in EBPi was detected 5 s [-42, 74] (median [95% CI]) after ground-truth seizure offset. EBPi demonstrated a positive predictive value of 61.5% [33.3, 75] (median [95% CI]) and a sensitivity of 57.1% [38.5, 66.7] (median [95% CI]) for the detection of ground truth seizure locations. EBPi signals exhibited a temporal sensitivity to seizure events and in some cases were correlated spatially to the seizure location. Therefore, EBPi, which has been linked to CBF, appears to contain spatio-temporal information related to seizure activity and might have useful application in augmenting EEG data captured during seizures.","sentences":["The Electrocardiography Brain Perfusion index (EBPi) is a novel electrocardiography (ECG)-based metric that may function as a proxy for cerebral blood flow (CBF).","We investigated the spatio-temporal correlation between EBPi and epileptic seizure events.","EBPi was computed retrospectively from clinical EEG and ECG data captured previously from 30 epilepsy patients during seizures.","Significant EBPi changes were compared temporally with clinically defined ground-truth seizure onset and offset times.","We also assessed the spatial correlation between EBPi metrics and clinically defined ground-truth seizure locations.","A significant increase in EBPi was detected 10.5 s","[-6, 53] (median [95% confidence interval (CI)]) after ground-truth seizure onset, and a significant decrease in EBPi was detected 5 s","[-42, 74] (median [95% CI]) after ground-truth seizure offset.","EBPi demonstrated a positive predictive value of 61.5% [33.3, 75] (median [95% CI]) and a sensitivity of 57.1% [38.5, 66.7] (median [95% CI]) for the detection of ground truth seizure locations.","EBPi signals exhibited a temporal sensitivity to seizure events and in some cases were correlated spatially to the seizure location.","Therefore, EBPi, which has been linked to CBF, appears to contain spatio-temporal information related to seizure activity and might have useful application in augmenting EEG data captured during seizures."],"url":"http://arxiv.org/abs/2403.17366v1","category":"physics.med-ph"}
{"created":"2024-03-26 03:34:20","title":"Large topological Hall effect arising from spin reorientation in kagome magnet Fe3Ge","abstract":"Materials systems with spin chirality can provide ultra-high-density, ultra-fast, and ultralow-power information carriers for digital transformation. These material systems include magnetic skyrmions, chiral domain walls, spin reorientation,and so on. The topological Hall effect (THE) has been identified as the most convenient and effective tool for detecting the presence of spin chirality in these systems. The research on the THE that may arise from spin reorientation and specifically in Fe3Ge with spin reorientation remains an unexplored area, so we study the THE in Fe3Ge Conduct systematic research. X-Ray Diffraction (XRD) results indicate that our Fe3Ge ribbon sample has a D019 structure. First-principles calculations and magnetic and electrical testing confirm spin reorientation in the Fe3Ge ribbon sample at 350 K.The Hall resistivity test results are consistent with our expectations, indicating the presence of the THE in the Fe3Ge ribbon sample. The topological Hall resistivity reaches a maximum value of 0.69 m{\\Omega} cm at 400 K. For the first time, a detailed experimental study of the THE in Fe3Ge with spin reorientation has been conducted, introducing a new member to the family of THE.","sentences":["Materials systems with spin chirality can provide ultra-high-density, ultra-fast, and ultralow-power information carriers for digital transformation.","These material systems include magnetic skyrmions, chiral domain walls, spin reorientation,and so on.","The topological Hall effect (THE) has been identified as the most convenient and effective tool for detecting the presence of spin chirality in these systems.","The research on the THE that may arise from spin reorientation and specifically in Fe3Ge with spin reorientation remains an unexplored area, so we study the THE in Fe3Ge Conduct systematic research.","X-Ray Diffraction (XRD) results indicate that our Fe3Ge ribbon sample has a D019 structure.","First-principles calculations and magnetic and electrical testing confirm spin reorientation in the Fe3Ge ribbon sample at 350 K.The Hall resistivity test results are consistent with our expectations, indicating the presence of the THE in the Fe3Ge ribbon sample.","The topological Hall resistivity reaches a maximum value of 0.69 m{\\Omega} cm at 400 K. For the first time, a detailed experimental study of the THE in Fe3Ge with spin reorientation has been conducted, introducing a new member to the family of THE."],"url":"http://arxiv.org/abs/2403.17354v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-26 02:11:40","title":"A possibly solar metallicity atmosphere escaping from HAT-P-32b revealed by H$\u03b1$ and He absorption","abstract":"This paper presents a hydrodynamic simulation that couples detailed non-local thermodynamic equilibrium (NLTE) calculations of the hydrogen and helium level populations to model the H$\\alpha$ and He 10830 transmission spectra of the hot Jupiter HAT-P-32b. A Monte Carlo simulation is applied to calculate the number of Ly$\\alpha$ resonance scatterings, which is the main process for populating H(2). In the examined parameter space, only the models with H/He $\\geq$ 99.5/0.5, $(0.5 \\sim 3.0)$ times the fiducial value of $F_{\\rm XUV}$, $\\beta_m = 0.16\\sim 0.3$, can explain the H$\\alpha$ and He 10830 lines simultaneously. We find a mass-loss rate of $\\sim (1.0\\sim 3.1) \\times 10^{13}$ g s$^{-1}$, consistent with previous studies. Moreover, we find that the stellar Ly$\\alpha$ flux should be as high as $4 \\times 10^{5}$ erg cm$^{-2}$ s$^{-1}$, indicating high stellar activity during the observation epoch of the two absorption lines. Despite the fact that the metallicity in the lower atmosphere of HAT-P-32b may be super-solar, our simulations tentatively suggest it is close to solar in the upper atmosphere. The difference in metallicity between the lower and upper atmospheres is essential for future atmospheric characterisations.","sentences":["This paper presents a hydrodynamic simulation that couples detailed non-local thermodynamic equilibrium (NLTE) calculations of the hydrogen and helium level populations to model the H$\\alpha$ and He 10830 transmission spectra of the hot Jupiter HAT-P-32b.","A Monte Carlo simulation is applied to calculate the number of Ly$\\alpha$ resonance scatterings, which is the main process for populating H(2).","In the examined parameter space, only the models with H/He $\\geq$ 99.5/0.5, $(0.5 \\sim 3.0)$ times the fiducial value of $F_{\\rm XUV}$, $\\beta_m = 0.16\\sim 0.3$, can explain the H$\\alpha$ and He 10830 lines simultaneously.","We find a mass-loss rate of $\\sim (1.0\\sim 3.1) \\times 10^{13}$ g s$^{-1}$, consistent with previous studies.","Moreover, we find that the stellar Ly$\\alpha$ flux should be as high as $4 \\times 10^{5}$ erg cm$^{-2}$","s$^{-1}$, indicating high stellar activity during the observation epoch of the two absorption lines.","Despite the fact that the metallicity in the lower atmosphere of HAT-P-32b may be super-solar, our simulations tentatively suggest it is close to solar in the upper atmosphere.","The difference in metallicity between the lower and upper atmospheres is essential for future atmospheric characterisations."],"url":"http://arxiv.org/abs/2403.17325v1","category":"astro-ph.EP"}
{"created":"2024-03-26 02:05:57","title":"A novel directly energy-preserving method for charged particle dynamics","abstract":"In this paper, we apply the coordinate increment discrete gradient (CIDG) method to solve the Lorentz force system which can be written as a non-canonical Hamiltonian system. Then we can obtain a new energy-preserving CIDG-I method for the system. The CIDG-I method can combine with its adjoint method CIDG-II which is also a energy-preserving method to form a new method, namely CIDG-C method. The CIDG-C method is symmetrical and can conserve the Hamiltonian energy directly and exactly. With comparison to the well-used Boris method, numerical experiments indicate that the CIDG-C method holds advantage over the Boris method in terms of energy-conserving.   The Fig. 5(b) in the original paper contains an error. We submit the correct Fig. 5(b) and an errata in this paper, which is described in remark4.1.","sentences":["In this paper, we apply the coordinate increment discrete gradient (CIDG) method to solve the Lorentz force system which can be written as a non-canonical Hamiltonian system.","Then we can obtain a new energy-preserving CIDG-I method for the system.","The CIDG-I method can combine with its adjoint method CIDG-II which is also a energy-preserving method to form a new method, namely CIDG-C method.","The CIDG-C method is symmetrical and can conserve the Hamiltonian energy directly and exactly.","With comparison to the well-used Boris method, numerical experiments indicate that the CIDG-C method holds advantage over the Boris method in terms of energy-conserving.   ","The Fig.","5(b) in the original paper contains an error.","We submit the correct Fig.","5(b) and an errata in this paper, which is described in remark4.1."],"url":"http://arxiv.org/abs/2403.17322v1","category":"math.NA"}
{"created":"2024-03-26 01:56:39","title":"Recent advances on the spherical metal oxides for sustainable degradation of antibiotics","abstract":"Due to the permanent harm to human health and ecosystem balance, antibiotic pollution in water has become an important direction of current environmental governance. Spherical metal oxides (SMOs) have been frequently utilized as effective heterogeneous photocatalysts for the efficient degradation of antibiotics due to the unique properties (e.g., strong light absorption ability, high separation efficiency of photo-generated electron hole pairs, and good catalytic activity). This review will firstly focus on summarizing the rational design and synthesis of SMOs with various tuned microstructures such as hollow, porous shell, yolk shell, core shell, and nanoflowers. These structures can expose more active sites, achieve a higher utilization rate of light, enhance the mass transfer efficiency and improve the effective diffusion of reactive oxygen species (ROS). Secondly, this review will mainly analyze the intrinsic relationship between the structure of SMOs and its photocatalytic property, the ability to generate ROS, and the degradation pathway for antibiotics. Moreover, the photocatalytic mechanisms and recent progress of different SMOs catalysts for degrading typical antibiotics are compared in detail. Finally, challenges and prospects of future direction in the development of SMOs for antibiotic degradation are reviewed. It is expected to provide a rational design of SMOs catalysts for efficient photocatalytic degradation of environmental pollutants.","sentences":["Due to the permanent harm to human health and ecosystem balance, antibiotic pollution in water has become an important direction of current environmental governance.","Spherical metal oxides (SMOs) have been frequently utilized as effective heterogeneous photocatalysts for the efficient degradation of antibiotics due to the unique properties (e.g., strong light absorption ability, high separation efficiency of photo-generated electron hole pairs, and good catalytic activity).","This review will firstly focus on summarizing the rational design and synthesis of SMOs with various tuned microstructures such as hollow, porous shell, yolk shell, core shell, and nanoflowers.","These structures can expose more active sites, achieve a higher utilization rate of light, enhance the mass transfer efficiency and improve the effective diffusion of reactive oxygen species (ROS).","Secondly, this review will mainly analyze the intrinsic relationship between the structure of SMOs and its photocatalytic property, the ability to generate ROS, and the degradation pathway for antibiotics.","Moreover, the photocatalytic mechanisms and recent progress of different SMOs catalysts for degrading typical antibiotics are compared in detail.","Finally, challenges and prospects of future direction in the development of SMOs for antibiotic degradation are reviewed.","It is expected to provide a rational design of SMOs catalysts for efficient photocatalytic degradation of environmental pollutants."],"url":"http://arxiv.org/abs/2403.17316v1","category":"physics.bio-ph"}
{"created":"2024-03-26 01:50:27","title":"The Physical Origin of the Mass-Size Relation and Its Scatter of Disk Galaxies","abstract":"Utilizing a kinematic decomposition of simulated galaxies, we focus on galaxies with tiny kinematically inferred stellar halos, indicative of weak external influences. We investigate the intricate interplay between internal (natural) and external (nurture) processes in shaping the scaling relationships of specific angular momentum ($j_\\star$), stellar mass ($M_\\star$), and size of disk galaxies within the IllustrisTNG simulation. The correlation among mass, size, and angular momentum of galaxies is examined by comparing simulations with observations and the theoretical predictions of the exponential hypothesis. Galaxies with tiny stellar halos exhibit a large scatter in the $j_\\star$-$M_\\star$ relation, which suggests that it is inherently present in their initial conditions. The analysis reveals that the disks of these galaxies adhere to the exponential hypothesis, resulting in a tight fiducial $j_\\star$-$M_\\star$-scale length (size) relation that is qualitatively consistent with observations. The inherent scatter in $j_\\star$ provides a robust explanation for the mass-size relation and its substantial variability. Notably, galaxies that are moderately influenced by external processes closely adhere to a scaling relation akin to that of galaxies with tiny stellar halos. This result underscores the dominant role of internal processes in shaping the overall $j_\\star$-$M_\\star$ and mass-size relation, with external effects playing a relatively minor role in disk galaxies. Furthermore, the correlation between galaxy size and the virial radius of the dark matter halo exists but fails to provide strong evidence of the connection between galaxies and their parent dark matter halos.","sentences":["Utilizing a kinematic decomposition of simulated galaxies, we focus on galaxies with tiny kinematically inferred stellar halos, indicative of weak external influences.","We investigate the intricate interplay between internal (natural) and external (nurture) processes in shaping the scaling relationships of specific angular momentum ($j_\\star$), stellar mass ($M_\\star$), and size of disk galaxies within the IllustrisTNG simulation.","The correlation among mass, size, and angular momentum of galaxies is examined by comparing simulations with observations and the theoretical predictions of the exponential hypothesis.","Galaxies with tiny stellar halos exhibit a large scatter in the $j_\\star$-$M_\\star$ relation, which suggests that it is inherently present in their initial conditions.","The analysis reveals that the disks of these galaxies adhere to the exponential hypothesis, resulting in a tight fiducial $j_\\star$-$M_\\star$-scale length (size) relation that is qualitatively consistent with observations.","The inherent scatter in $j_\\star$ provides a robust explanation for the mass-size relation and its substantial variability.","Notably, galaxies that are moderately influenced by external processes closely adhere to a scaling relation akin to that of galaxies with tiny stellar halos.","This result underscores the dominant role of internal processes in shaping the overall $j_\\star$-$M_\\star$ and mass-size relation, with external effects playing a relatively minor role in disk galaxies.","Furthermore, the correlation between galaxy size and the virial radius of the dark matter halo exists but fails to provide strong evidence of the connection between galaxies and their parent dark matter halos."],"url":"http://arxiv.org/abs/2403.17313v1","category":"astro-ph.GA"}
{"created":"2024-03-26 01:01:21","title":"Suzaku observation of an iron K-shell line in the spiral galaxy NGC 6946","abstract":"An emission line at ~6.7 keV is attributable to a He-like iron K-shell transition, which indicates existence of a thin thermal plasma with a temperature of several keV. Using Suzaku archival data, we searched for the iron K-line from the spiral galaxy NGC 6946, and found the iron K-line at 6.68+/-0.07 keV at the 3.1 sigma level in the central r<2.'5 region. The iron line luminosity from the central region was estimated to be (2.3+/-1.2)x10^{37} erg s^{-1} at a distance of 5.5 Mpc. The origin of the iron emission line is discussed.","sentences":["An emission line at ~6.7 keV is attributable to a He-like iron K-shell transition, which indicates existence of a thin thermal plasma with a temperature of several keV. Using Suzaku archival data, we searched for the iron K-line from the spiral galaxy NGC 6946, and found the iron K-line at 6.68+/-0.07 keV at the 3.1 sigma level in the central r<2.'5 region.","The iron line luminosity from the central region was estimated to be (2.3+/-1.2)x10^{37} erg s^{-1} at a distance of 5.5 Mpc.","The origin of the iron emission line is discussed."],"url":"http://arxiv.org/abs/2403.17300v1","category":"astro-ph.HE"}
{"created":"2024-03-26 00:44:14","title":"Maximum A Posteriori Ly-alpha Estimator (MAPLE): Band-power and covariance estimation of the 3D Ly-alpha forest power spectrum","abstract":"We present a novel maximum a posteriori estimator to jointly estimate band-powers and the covariance of the three-dimensional power spectrum (P3D) of Lyman-alpha forest flux fluctuations, called MAPLE. Our Wiener-filter based algorithm reconstructs a window-deconvolved P3D in the presence of complex survey geometries typical for Lyman-alpha surveys that are sparsely sampled transverse to and densely sampled along the line-of-sight. We demonstrate our method on idealized Gaussian random fields with two selection functions: (i) a sparse sampling of 30 background sources per square degree designed to emulate the currently observing the Dark Energy Spectroscopic Instrument (DESI); (ii) a dense sampling of 900 background sources per square degree emulating the upcoming Prime Focus Spectrograph Galaxy Evolution Survey. Our proof-of-principle shows promise, especially since the algorithm can be extended to marginalize jointly over nuisance parameters and contaminants, i.e.offsets introduced by continuum fitting. Our code is implemented in JAX and is publicly available on GitHub.","sentences":["We present a novel maximum a posteriori estimator to jointly estimate band-powers and the covariance of the three-dimensional power spectrum (P3D) of Lyman-alpha forest flux fluctuations, called MAPLE.","Our Wiener-filter based algorithm reconstructs a window-deconvolved P3D in the presence of complex survey geometries typical for Lyman-alpha surveys that are sparsely sampled transverse to and densely sampled along the line-of-sight.","We demonstrate our method on idealized Gaussian random fields with two selection functions: (i) a sparse sampling of 30 background sources per square degree designed to emulate the currently observing the Dark Energy Spectroscopic Instrument (DESI); (ii) a dense sampling of 900 background sources per square degree emulating the upcoming Prime Focus Spectrograph Galaxy Evolution Survey.","Our proof-of-principle shows promise, especially since the algorithm can be extended to marginalize jointly over nuisance parameters and contaminants, i.e.offsets introduced by continuum fitting.","Our code is implemented in JAX and is publicly available on GitHub."],"url":"http://arxiv.org/abs/2403.17294v1","category":"astro-ph.CO"}
{"created":"2024-03-26 00:07:53","title":"The flavor structures on magnetized orbifold models and 4D modular symmetric models","abstract":"We study quark and lepton flavor structures on magnetized $T^2/\\mathbb{Z}_2$ twisted orbifold model. There are 6,460 number of flavor models but most of them cannot lead to realistic flavor observables because of the difficulties on realizing mass hierarchies and small (large) mixing angles of quarks (leptons). We find that certain zero point patterns of zero-modes of fermions and Higgs fields give the flavor models being able to avoid these difficulties. We classify such flavor models and show numerical example.   Next we study four-dimensional (4D) modular symmetric quark flavor models without fine-tuning. Mass matrices become hierarchical as close to the modular symmetric points depending on its residual charges. Actually the residual $Z_N$ symmetries with $N\\geq 6$ can originate the large quark mass hierarchies. Also the products of residual symmetries such as $Z_3\\times Z_3\\times Z_3$ have such possibility. We study the quark flavor model with $\\Gamma_6$ symmetry and $A_4\\times A_4\\times A_4$ symmetry. We assume the vicinity of the cusp $\\tau=i\\infty$ where residual $Z_6$ and $Z_3\\times Z_3\\times Z_3$ symmetry remain. Consequently we find the models realizing the order of the quark mass ratios and the absolute values of the Cabibbo-Kobayashi-Maskawa (CKM) matrix elements without fine-tuning.   Finally we construct the Siegel modular forms. Zero-modes on $T^6$ at $\\vec{z}=0$ are the Siegel modular forms of weight 1/2 for the subgroup of $Sp(6,\\mathbb{Z})$. They have several moduli parameters and therefore have the possibility realizing the flavor structures including the CP phases. We study the Siegel modular forms transformed by $\\widetilde{\\Delta}(96)$ and show numerical example. We find one of moduli parameters $\\omega_1$ works on the large mass hierarchies and $\\omega_2$ works on the CP violation successfully in our model.","sentences":["We study quark and lepton flavor structures on magnetized $T^2/\\mathbb{Z}_2$ twisted orbifold model.","There are 6,460 number of flavor models but most of them cannot lead to realistic flavor observables because of the difficulties on realizing mass hierarchies and small (large) mixing angles of quarks (leptons).","We find that certain zero point patterns of zero-modes of fermions and Higgs fields give the flavor models being able to avoid these difficulties.","We classify such flavor models and show numerical example.   ","Next we study four-dimensional (4D) modular symmetric quark flavor models without fine-tuning.","Mass matrices become hierarchical as close to the modular symmetric points depending on its residual charges.","Actually the residual $Z_N$ symmetries with $N\\geq 6$ can originate the large quark mass hierarchies.","Also the products of residual symmetries such as $Z_3\\times Z_3\\times Z_3$ have such possibility.","We study the quark flavor model with $\\Gamma_6$ symmetry and $A_4\\times A_4\\times A_4$ symmetry.","We assume the vicinity of the cusp $\\tau=i\\infty$ where residual $Z_6$ and $Z_3\\times Z_3\\times Z_3$ symmetry remain.","Consequently we find the models realizing the order of the quark mass ratios and the absolute values of the Cabibbo-Kobayashi-Maskawa (CKM) matrix elements without fine-tuning.   ","Finally we construct the Siegel modular forms.","Zero-modes on $T^6$ at $\\vec{z}=0$ are the Siegel modular forms of weight 1/2 for the subgroup of $Sp(6,\\mathbb{Z})$. They have several moduli parameters and therefore have the possibility realizing the flavor structures including the CP phases.","We study the Siegel modular forms transformed by $\\widetilde{\\Delta}(96)$ and show numerical example.","We find one of moduli parameters $\\omega_1$ works on the large mass hierarchies and $\\omega_2$ works on the CP violation successfully in our model."],"url":"http://arxiv.org/abs/2403.17280v1","category":"hep-ph"}
{"created":"2024-03-25 23:51:07","title":"Stellar Spin Down in Post-Mass Transfer Binary Systems","abstract":"Motivated by measurements of the rotation speed of accretor stars in post-mass-transfer (post-MT) systems, we investigate how magnetic braking affects the spin-down of individual stars during binary evolution with the \\texttt{MESAbinary} module. Unlike the conventional assumption of tidal synchronization coupled with magnetic braking in binaries, we first calculate whether tides are strong enough to synchronize the orbit. Subsequently, this influences the spin-down of stars and the orbital separation. In this study, we apply four magnetic braking prescriptions to reduce the spin angular momentum of the two stars throughout the entire binary evolution simulation. Our findings reveal that despite magnetic braking causing continuous spin-down of the accretor, when the donor begins to transfer material onto the accretor, the accretor can rapidly spin up to its critical rotation rate. After MT, magnetic braking becomes more important in affecting the angular momentum evolution of the stars. Post-MT accretor stars thus serve as a valuable testbed for observing how the magnetic braking prescriptions operate in spinning down stars from their critical rotation, including the saturation regimes of the magnetic braking. The rotation rate of the accretor star, combined with its mass, could provide age information since the cessation of MT. By comparing the models against observation, the magnetic braking prescription by Garraffo et al. (2018b) is found to better align with the rotation data of post-MT accretors.","sentences":["Motivated by measurements of the rotation speed of accretor stars in post-mass-transfer (post-MT) systems, we investigate how magnetic braking affects the spin-down of individual stars during binary evolution with the \\texttt{MESAbinary} module.","Unlike the conventional assumption of tidal synchronization coupled with magnetic braking in binaries, we first calculate whether tides are strong enough to synchronize the orbit.","Subsequently, this influences the spin-down of stars and the orbital separation.","In this study, we apply four magnetic braking prescriptions to reduce the spin angular momentum of the two stars throughout the entire binary evolution simulation.","Our findings reveal that despite magnetic braking causing continuous spin-down of the accretor, when the donor begins to transfer material onto the accretor, the accretor can rapidly spin up to its critical rotation rate.","After MT, magnetic braking becomes more important in affecting the angular momentum evolution of the stars.","Post-MT accretor stars thus serve as a valuable testbed for observing how the magnetic braking prescriptions operate in spinning down stars from their critical rotation, including the saturation regimes of the magnetic braking.","The rotation rate of the accretor star, combined with its mass, could provide age information since the cessation of MT.","By comparing the models against observation, the magnetic braking prescription by Garraffo et al. (2018b) is found to better align with the rotation data of post-MT accretors."],"url":"http://arxiv.org/abs/2403.17279v1","category":"astro-ph.SR"}
{"created":"2024-03-25 23:28:57","title":"Human Stress Response and Perceived Safety during Encounters with Quadruped Robots","abstract":"Despite the rise of mobile robot deployments in home and work settings, perceived safety of users and bystanders is understudied in the human-robot interaction (HRI) literature. To address this, we present a study designed to identify elements of a human-robot encounter that correlate with observed stress response. Stress is a key component of perceived safety and is strongly associated with human physiological response. In this study a Boston Dynamics Spot and a Unitree Go1 navigate autonomously through a shared environment occupied by human participants wearing multimodal physiological sensors to track their electrocardiography (ECG) and electrodermal activity (EDA). The encounters are varied through several trials and participants self-rate their stress levels after each encounter. The study resulted in a multidimensional dataset archiving various objective and subjective aspects of a human-robot encounter, containing insights for understanding perceived safety in such encounters. To this end, acute stress responses were decoded from the human participants' ECG and EDA and compared across different human-robot encounter conditions. Statistical analysis of data indicate that on average (1) participants feel more stress during encounters compared to baselines, (2) participants feel more stress encountering multiple robots compared to a single robot and (3) participants stress increases during navigation behavior compared with search behavior.","sentences":["Despite the rise of mobile robot deployments in home and work settings, perceived safety of users and bystanders is understudied in the human-robot interaction (HRI) literature.","To address this, we present a study designed to identify elements of a human-robot encounter that correlate with observed stress response.","Stress is a key component of perceived safety and is strongly associated with human physiological response.","In this study a Boston Dynamics Spot and a Unitree Go1 navigate autonomously through a shared environment occupied by human participants wearing multimodal physiological sensors to track their electrocardiography (ECG) and electrodermal activity (EDA).","The encounters are varied through several trials and participants self-rate their stress levels after each encounter.","The study resulted in a multidimensional dataset archiving various objective and subjective aspects of a human-robot encounter, containing insights for understanding perceived safety in such encounters.","To this end, acute stress responses were decoded from the human participants' ECG and EDA and compared across different human-robot encounter conditions.","Statistical analysis of data indicate that on average (1) participants feel more stress during encounters compared to baselines, (2) participants feel more stress encountering multiple robots compared to a single robot and (3) participants stress increases during navigation behavior compared with search behavior."],"url":"http://arxiv.org/abs/2403.17270v1","category":"cs.RO"}
{"created":"2024-03-25 23:22:26","title":"Synthetic active liquid crystals powered by acoustic waves","abstract":"Active nematics are materials composed of mobile, elongated particles that can transform energy from the environment into a mechanical motion. Current experimental realizations of the active nematics are of biological origin and include cell layers, suspensions of elongated bacteria in liquid crystal, and combinations of bio-filaments with molecular motors. Here, we report the realization of a fully synthetic active nematic system comprised of a lyotropic chromonic liquid crystal energized by ultrasonic waves. This artificial active liquid crystal is free from biological degradation and variability, exhibits stable material properties, and enables precise and rapid activity control over a significantly extended range. We demonstrate that the energy of the acoustic field is converted into microscopic extensile stresses disrupting long-range nematic order and giving rise to an undulation instability, development of active turbulence, and proliferation of topological defects. We reveal the emergence of unconventional free-standing persistent vortices in the nematic director field at high activity levels. The results provide a foundation for the design of externally energized active nematic fluids with stable material properties and tunable topological defects dynamics crucial for the realization of reconfigurable microfluidic systems.","sentences":["Active nematics are materials composed of mobile, elongated particles that can transform energy from the environment into a mechanical motion.","Current experimental realizations of the active nematics are of biological origin and include cell layers, suspensions of elongated bacteria in liquid crystal, and combinations of bio-filaments with molecular motors.","Here, we report the realization of a fully synthetic active nematic system comprised of a lyotropic chromonic liquid crystal energized by ultrasonic waves.","This artificial active liquid crystal is free from biological degradation and variability, exhibits stable material properties, and enables precise and rapid activity control over a significantly extended range.","We demonstrate that the energy of the acoustic field is converted into microscopic extensile stresses disrupting long-range nematic order and giving rise to an undulation instability, development of active turbulence, and proliferation of topological defects.","We reveal the emergence of unconventional free-standing persistent vortices in the nematic director field at high activity levels.","The results provide a foundation for the design of externally energized active nematic fluids with stable material properties and tunable topological defects dynamics crucial for the realization of reconfigurable microfluidic systems."],"url":"http://arxiv.org/abs/2403.17268v1","category":"cond-mat.soft"}
{"created":"2024-03-25 23:22:18","title":"Black hole mass estimate in OJ 287 based on the bulk-motion comptonization model","abstract":"The multi-wavelength outburst activity in the BL Lacertae source OJ 287 has sparked a lot of controversy about whether the source contains one or two black holes (BHs) and what characteristics of this black hole binary would be. In this article we present the results of analysis of the X-ray flaring activity of OJ 287 using the data of Swift/XRT observations. We discovered that the energy spectra in all spectral states can be adequately fit with the XSPEC BMC model (the Comptonization one). As a result we found that the X-ray photon index of the BMC model, $\\Gamma$ correlates with the mass accretion rate, $\\dot M$. We found the photon index $\\Gamma$ to increase monotonically with accretion rate $\\dot M$ from $\\Gamma\\sim 2$ in the intermediate state (IS) to $\\Gamma\\sim2.5$ the high/soft state (HSS) with subsequent saturation at $\\Gamma\\sim$ 2.6 level at higher luminosities. This type of behavior of the spectral index is remarkably similar to the pattern observed in a number of established stellar-mass black hole candidates. Assuming the universality of the observed pattern of the correlation between the photon index and mass accretion rate, we estimate the BH mass in OJ 287 to be around $2\\times10^8$ solar masses, using the well studied BH binaries GX 339-4 and XTE J1859-226 to calibrate the model.","sentences":["The multi-wavelength outburst activity in the BL Lacertae source OJ 287 has sparked a lot of controversy about whether the source contains one or two black holes (BHs) and what characteristics of this black hole binary would be.","In this article we present the results of analysis of the X-ray flaring activity of OJ 287 using the data of Swift/XRT observations.","We discovered that the energy spectra in all spectral states can be adequately fit with the XSPEC BMC model (the Comptonization one).","As a result we found that the X-ray photon index of the BMC model, $\\Gamma$ correlates with the mass accretion rate, $\\dot M$. We found the photon index $\\Gamma$ to increase monotonically with accretion rate $\\dot M$ from $\\Gamma\\sim 2$ in the intermediate state (IS) to $\\Gamma\\sim2.5$ the high/soft state (HSS) with subsequent saturation at $\\Gamma\\sim$ 2.6 level at higher luminosities.","This type of behavior of the spectral index is remarkably similar to the pattern observed in a number of established stellar-mass black hole candidates.","Assuming the universality of the observed pattern of the correlation between the photon index and mass accretion rate, we estimate the BH mass in OJ 287 to be around $2\\times10^8$ solar masses, using the well studied BH binaries GX 339-4 and XTE J1859-226 to calibrate the model."],"url":"http://arxiv.org/abs/2403.17267v1","category":"astro-ph.HE"}
{"created":"2024-03-25 23:12:11","title":"Exact Results for the Giant Graviton four-point Correlator","abstract":"We study the four-point correlator $\\langle \\mathcal{O}_2 \\mathcal{O}_2 \\mathcal{D} \\mathcal{D} \\rangle$ in $\\mathcal{N}=4$ super Yang-Mills theory (SYM) with $SU(N)$ gauge group, where $\\mathcal{O}_2$ represents the superconformal primary operator with dimension two, while $\\mathcal{D}$ denotes a determinant operator of dimension $N$, which is holographically dual to a giant graviton D3-brane extending along $S^5$. We analyse the integrated correlator associated with this observable, obtained after integrating out the spacetime dependence over a supersymmetric invariant measure. Similarly to other classes of integrated correlators in $\\mathcal{N}=4$ SYM, this integrated correlator can be computed through supersymmetric localisation on the four-sphere. Employing matrix-model recursive techniques, we demonstrate that the integrated correlator can be reformulated as an infinite sum of protected three-point functions with known coefficients. This insight allows us to circumvent the complexity associated with the dimension-$N$ determinant operator, significantly streamlining the large-$N$ expansion of the integrated correlator. In the planar limit and beyond, we derive exact results for the integrated correlator valid for all values of the 't Hooft coupling, and investigate the resurgent properties of their strong coupling expansion. Additionally, in the large-$N$ expansion with fixed (complexified) Yang-Mills coupling, we deduce the $SL(2, \\mathbb{Z})$ completion of these results in terms of the non-holomorphic Eisenstein series. The proposed modular functions are confirmed by explicit instanton calculations from the matrix model, and agree with expectations from the holographic dual picture of known results in type IIB string theory.","sentences":["We study the four-point correlator $\\langle \\mathcal{O}_2 \\mathcal{O}_2 \\mathcal{D} \\mathcal{D} \\rangle$ in $\\mathcal{N}=4$ super Yang-Mills theory (SYM) with $SU(N)$ gauge group, where $\\mathcal{O}_2$ represents the superconformal primary operator with dimension two, while $\\mathcal{D}$ denotes a determinant operator of dimension $N$, which is holographically dual to a giant graviton D3-brane extending along $S^5$.","We analyse the integrated correlator associated with this observable, obtained after integrating out the spacetime dependence over a supersymmetric invariant measure.","Similarly to other classes of integrated correlators in $\\mathcal{N}=4$ SYM, this integrated correlator can be computed through supersymmetric localisation on the four-sphere.","Employing matrix-model recursive techniques, we demonstrate that the integrated correlator can be reformulated as an infinite sum of protected three-point functions with known coefficients.","This insight allows us to circumvent the complexity associated with the dimension-$N$ determinant operator, significantly streamlining the large-$N$ expansion of the integrated correlator.","In the planar limit and beyond, we derive exact results for the integrated correlator valid for all values of the 't Hooft coupling, and investigate the resurgent properties of their strong coupling expansion.","Additionally, in the large-$N$ expansion with fixed (complexified)","Yang-Mills coupling, we deduce the $SL(2, \\mathbb{Z})$ completion of these results in terms of the non-holomorphic Eisenstein series.","The proposed modular functions are confirmed by explicit instanton calculations from the matrix model, and agree with expectations from the holographic dual picture of known results in type IIB string theory."],"url":"http://arxiv.org/abs/2403.17263v1","category":"hep-th"}
{"created":"2024-03-25 23:05:46","title":"NS5-brane backgrounds and coset CFT partition functions","abstract":"Worldsheet string theory is solvable for a variety of backgrounds involving Neveu-Schwarz fivebranes, in terms of gauged nonlinear sigma models on group manifolds. We compute the worldsheet torus partition function of these models, and propose gauging of null isometries as a unifying principle and conceptual framework for this large family of string backgrounds. In the process, we explain how partition functions of asymmetrically gauged Wess-Zumino-Witten models can be computed from the path integral, and organize and systematize various results scattered throughout the literature.","sentences":["Worldsheet string theory is solvable for a variety of backgrounds involving Neveu-Schwarz fivebranes, in terms of gauged nonlinear sigma models on group manifolds.","We compute the worldsheet torus partition function of these models, and propose gauging of null isometries as a unifying principle and conceptual framework for this large family of string backgrounds.","In the process, we explain how partition functions of asymmetrically gauged Wess-Zumino-Witten models can be computed from the path integral, and organize and systematize various results scattered throughout the literature."],"url":"http://arxiv.org/abs/2403.17258v1","category":"hep-th"}
{"created":"2024-03-25 23:04:15","title":"Statistical Inference on Hierarchical Simultaneous Autoregressive Models with Missing Data","abstract":"Efficient estimation methods for simultaneous autoregressive (SAR) models with missing data in the response variable have been well-developed in the literature. It is common practice to introduce a measurement error into SAR models. The measurement error serves to distinguish the noise component from the spatial process. However, the previous literature has not considered adding a measurement error to the SAR models with missing data. The maximum likelihood estimation for such models with large datasets is challenging and computationally expensive. This paper proposes two efficient likelihood-based estimation methods: the marginal maximum likelihood (ML) and expectation-maximisation (EM) algorithms for estimating SAR models with both measurement errors and missing data in the response variable. The spatial error model (SEM) and the spatial autoregressive model (SAM), two popular SAR model types, are considered. The missing data mechanism is assumed to follow missing at random (MAR). While naive calculation approaches lead to computational complexities of $O(n^3)$, where n is the total number of observations, our computational approaches for both the marginal ML and EM algorithms are designed to reduce the computational complexity. The performance of the proposed methods is investigated empirically using simulated and real datasets.","sentences":["Efficient estimation methods for simultaneous autoregressive (SAR) models with missing data in the response variable have been well-developed in the literature.","It is common practice to introduce a measurement error into SAR models.","The measurement error serves to distinguish the noise component from the spatial process.","However, the previous literature has not considered adding a measurement error to the SAR models with missing data.","The maximum likelihood estimation for such models with large datasets is challenging and computationally expensive.","This paper proposes two efficient likelihood-based estimation methods: the marginal maximum likelihood (ML) and expectation-maximisation (EM) algorithms for estimating SAR models with both measurement errors and missing data in the response variable.","The spatial error model (SEM) and the spatial autoregressive model (SAM), two popular SAR model types, are considered.","The missing data mechanism is assumed to follow missing at random (MAR).","While naive calculation approaches lead to computational complexities of $O(n^3)$, where n is the total number of observations, our computational approaches for both the marginal ML and EM algorithms are designed to reduce the computational complexity.","The performance of the proposed methods is investigated empirically using simulated and real datasets."],"url":"http://arxiv.org/abs/2403.17257v1","category":"stat.ME"}
{"created":"2024-03-25 22:45:08","title":"Use of Euler's theorem in the elucidation of economic concepts in goods exchange","abstract":"Starting from a plausible assumption about the Total Revenue concept, a system of economic agents, that simulates the exchange of goods, is studied. Following a methodology equivalent to that used in the statistical-mechanical determination of the distribution of energies in a physical system, it is shown that the price of the exchanged goods arises naturally, as well as the well-known ``Law of demand\". It is also shown that by using Euler's Theorem of homogeneous functions, the conditions emerge for the appearance of wholesale and retail prices. We found also that the Total Revenue is the product of two factors: the configurational entropy and a \"free energy\" type one. We study numerically the case in which the exchanged good is money itself, in systems in which economic agents have a \"roof\" on the amount of money they can possess. It is shown numerically that the increase in the quantity of money in the hands of the agents necessarily leads to its depreciation, and only the expansion of the economy, understood here as raising the \"roof\", minimizes this loss of purchasing power. The concept of indexing is discussed.","sentences":["Starting from a plausible assumption about the Total Revenue concept, a system of economic agents, that simulates the exchange of goods, is studied.","Following a methodology equivalent to that used in the statistical-mechanical determination of the distribution of energies in a physical system, it is shown that the price of the exchanged goods arises naturally, as well as the well-known ``Law of demand\".","It is also shown that by using Euler's Theorem of homogeneous functions, the conditions emerge for the appearance of wholesale and retail prices.","We found also that the Total Revenue is the product of two factors: the configurational entropy and a \"free energy\" type one.","We study numerically the case in which the exchanged good is money itself, in systems in which economic agents have a \"roof\" on the amount of money they can possess.","It is shown numerically that the increase in the quantity of money in the hands of the agents necessarily leads to its depreciation, and only the expansion of the economy, understood here as raising the \"roof\", minimizes this loss of purchasing power.","The concept of indexing is discussed."],"url":"http://arxiv.org/abs/2403.17244v1","category":"physics.soc-ph"}
{"created":"2024-03-25 22:44:15","title":"The c-d conjecture","abstract":"We conjecture a relation between the local dimension $d$ of a local nearest-neighbor critical Hamiltonian in one spatial dimension and the maximum central charge, $c_{\\text{max}}$, that it can yield. Specifically, we propose that $c_{\\text{max}} \\leq d-1$, establishing a link between the short-distance lattice realization of a model and its emerging long-distance entanglement properties. This inequality can be viewed as a general form of a $c$-theorem establishing the reduction of effective degrees of freedom between the UV lattice and the IR conformal field theory. We support this conjecture with numerous examples.","sentences":["We conjecture a relation between the local dimension $d$ of a local nearest-neighbor critical Hamiltonian in one spatial dimension and the maximum central charge, $c_{\\text{max}}$, that it can yield.","Specifically, we propose that $c_{\\text{max}} \\leq d-1$, establishing a link between the short-distance lattice realization of a model and its emerging long-distance entanglement properties.","This inequality can be viewed as a general form of a $c$-theorem establishing the reduction of effective degrees of freedom between the UV lattice and the IR conformal field theory.","We support this conjecture with numerous examples."],"url":"http://arxiv.org/abs/2403.17242v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-25 22:03:32","title":"The role of the Look Elsewhere Effect in determining the significance of an oscillation disappearance search for a light sterile neutrino","abstract":"In the ongoing vibrant experimental quest to assess whether the numerous indications for a light sterile neutrino are only experimental fluctuations or the manifestations of a profound and real underlying effect, one aspect which has recently attracted a specific interest is the statistical treatment of the data. Especially in cases of supposed positive hints, the correct statistical assessment of their significance is of paramount importance, to avoid that potential overstatements lead to a wrong understanding of the real status of the experimental investigation in the field. In this work I show how latest crucial advancements in the statistical data processing for the interpretation of the output of a sterile search can be effectively put and understood in the context of the Look Elsewhere Effect phenomenon, developed and now of routine usage for results interpretation in other areas of HEP research.","sentences":["In the ongoing vibrant experimental quest to assess whether the numerous indications for a light sterile neutrino are only experimental fluctuations or the manifestations of a profound and real underlying effect, one aspect which has recently attracted a specific interest is the statistical treatment of the data.","Especially in cases of supposed positive hints, the correct statistical assessment of their significance is of paramount importance, to avoid that potential overstatements lead to a wrong understanding of the real status of the experimental investigation in the field.","In this work I show how latest crucial advancements in the statistical data processing for the interpretation of the output of a sterile search can be effectively put and understood in the context of the Look Elsewhere Effect phenomenon, developed and now of routine usage for results interpretation in other areas of HEP research."],"url":"http://arxiv.org/abs/2403.17228v1","category":"hep-ph"}
{"created":"2024-03-25 21:59:21","title":"Jones polynomials from matrix elements of tangles in a pseudounitary representation","abstract":"In these notes we review the calculation of Jones polynomials using a matrix representation of the braid group and Temperley-Lieb algebra. The pseudounitary representation that we consider allows constructing ``states'' from the group/algebra matrices and compute the knot invariants as matrix elements, rather than traces. In comparison with a more standard way of computing the invariants through traces, the matrix element method is more interesting and complete from the point of view of applications. As a byproduct of the discussion we prove a general formula for pretzel knots.","sentences":["In these notes we review the calculation of Jones polynomials using a matrix representation of the braid group and Temperley-Lieb algebra.","The pseudounitary representation that we consider allows constructing ``states'' from the group/algebra matrices and compute the knot invariants as matrix elements, rather than traces.","In comparison with a more standard way of computing the invariants through traces, the matrix element method is more interesting and complete from the point of view of applications.","As a byproduct of the discussion we prove a general formula for pretzel knots."],"url":"http://arxiv.org/abs/2403.17227v1","category":"hep-th"}
{"created":"2024-03-25 21:46:35","title":"Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis","abstract":"We consider the problem of finding plausible knowledge that is missing from a given ontology, as a generalisation of the well-studied taxonomy expansion task. One line of work treats this task as a Natural Language Inference (NLI) problem, thus relying on the knowledge captured by language models to identify the missing knowledge. Another line of work uses concept embeddings to identify what different concepts have in common, taking inspiration from cognitive models for category based induction. These two approaches are intuitively complementary, but their effectiveness has not yet been compared. In this paper, we introduce a benchmark for evaluating ontology completion methods and thoroughly analyse the strengths and weaknesses of both approaches. We find that both approaches are indeed complementary, with hybrid strategies achieving the best overall results. We also find that the task is highly challenging for Large Language Models, even after fine-tuning.","sentences":["We consider the problem of finding plausible knowledge that is missing from a given ontology, as a generalisation of the well-studied taxonomy expansion task.","One line of work treats this task as a Natural Language Inference (NLI) problem, thus relying on the knowledge captured by language models to identify the missing knowledge.","Another line of work uses concept embeddings to identify what different concepts have in common, taking inspiration from cognitive models for category based induction.","These two approaches are intuitively complementary, but their effectiveness has not yet been compared.","In this paper, we introduce a benchmark for evaluating ontology completion methods and thoroughly analyse the strengths and weaknesses of both approaches.","We find that both approaches are indeed complementary, with hybrid strategies achieving the best overall results.","We also find that the task is highly challenging for Large Language Models, even after fine-tuning."],"url":"http://arxiv.org/abs/2403.17216v1","category":"cs.CL"}
