{"created":"2024-02-13 18:59:51","title":"IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation","abstract":"Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.","sentences":["Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images.","They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts.","A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly.","In this paper, we further explore the design space of text-to-3D models.","We significantly improve multi-view generation by considering video instead of image generators.","Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views.","Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets."],"url":"http://arxiv.org/abs/2402.08682v1","category":"cs.CV"}
{"created":"2024-02-13 18:59:05","title":"Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance","abstract":"The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V.","sentences":["The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images.","To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs.","However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.","In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process.","Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations.","Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods.","Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V."],"url":"http://arxiv.org/abs/2402.08680v1","category":"cs.LG"}
{"created":"2024-02-13 18:58:48","title":"COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability","abstract":"Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence. Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at https://github.com/Yu-Fangxu/COLD-Attack.","sentences":["Jailbreaks on Large language models (LLMs) have recently received increasing attention.","For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks.","In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing.","Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence.","The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence.","Our extensive experiments on various LLMs (Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad applicability, strong controllability, high success rate, and attack transferability.","Our code is available at https://github.com/Yu-Fangxu/COLD-Attack."],"url":"http://arxiv.org/abs/2402.08679v1","category":"cs.LG"}
{"created":"2024-02-13 18:54:08","title":"Model Assessment and Selection under Temporal Distribution Shift","abstract":"We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.","sentences":["We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs.","To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model.","This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors.","We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates.","Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data."],"url":"http://arxiv.org/abs/2402.08672v1","category":"cs.LG"}
{"created":"2024-02-13 18:53:13","title":"Are Semi-Dense Detector-Free Methods Good at Matching Local Features?","abstract":"Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in this case SAM often surpasses SDF methods. Our findings highlight a strong correlation between the ability to establish accurate correspondences in textured regions and the accuracy of the resulting estimated pose/homography. Our code will be made available.","sentences":["Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods.","While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics.","Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention.","This paper is a first attempt to study this link.","We start with proposing a novel structured attention-based image matching architecture (SAM).","It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy.","We then propose to limit the computation of the matching accuracy to textured regions, and show that in this case SAM often surpasses SDF methods.","Our findings highlight a strong correlation between the ability to establish accurate correspondences in textured regions and the accuracy of the resulting estimated pose/homography.","Our code will be made available."],"url":"http://arxiv.org/abs/2402.08671v1","category":"cs.CV"}
{"created":"2024-02-13 18:51:18","title":"Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models","abstract":"The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics. However, the application of LVLMs in this field is still limited due to the following complexities: First, LVLMs lack user preference knowledge as they are trained from vast general datasets. Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences. To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation. We utilize user history as in-context user preferences to address the first challenge. Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items. We conduct comprehensive experiments across four datasets with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results indicate the efficacy of VST.","sentences":["The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics.","However, the application of LVLMs in this field is still limited due to the following complexities:","First, LVLMs lack user preference knowledge as they are trained from vast general datasets.","Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences.","To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation.","We utilize user history as in-context user preferences to address the first challenge.","Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items.","We conduct comprehensive experiments across four datasets with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b.","The numerical results indicate the efficacy of VST."],"url":"http://arxiv.org/abs/2402.08670v1","category":"cs.AI"}
{"created":"2024-02-13 18:39:36","title":"The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting","abstract":"We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message content, divided equally into JITAIs generated by 10 iterations with GPT-4, a baseline provided by 10 laypersons (LayPs), and a gold standard set by 10 healthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs generated by GPT-4 were superior to those by HCPs and LayPs over all assessed scales: i.e., appropriateness, engagement, effectiveness, and professionality. This study indicates that LLMs have significant potential for implementing JITAIs as a building block of personalized or \"precision\" health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability.","sentences":["We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health.","JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs.","However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity.","To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation.","Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs.","Subsequently, we generated a total of 450 proposed JITAI decisions and message content, divided equally into JITAIs generated by 10 iterations with GPT-4, a baseline provided by 10 laypersons (LayPs), and a gold standard set by 10 healthcare professionals (HCPs).","Ratings from 27 LayPs indicated that JITAIs generated by GPT-4 were superior to those by HCPs and LayPs over all assessed scales: i.e., appropriateness, engagement, effectiveness, and professionality.","This study indicates that LLMs have significant potential for implementing JITAIs as a building block of personalized or \"precision\" health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability."],"url":"http://arxiv.org/abs/2402.08658v1","category":"cs.HC"}
{"created":"2024-02-13 18:33:45","title":"SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds","abstract":"Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of GNNs, extending to applications within recommendation systems. Furthermore, we illustrate its utility in downstream tasks, notably in enhancing GNN stability and facilitating adversarial targeted attacks.","sentences":["Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance.","In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs.","This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability.","We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis.","Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of GNNs, extending to applications within recommendation systems.","Furthermore, we illustrate its utility in downstream tasks, notably in enhancing GNN stability and facilitating adversarial targeted attacks."],"url":"http://arxiv.org/abs/2402.08653v1","category":"cs.LG"}
{"created":"2024-02-13 18:27:53","title":"Generating Universal Adversarial Perturbations for Quantum Classifiers","abstract":"Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints. We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and adversarial samples.","sentences":["Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies.","Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks.","Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers.","In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers.","We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence.","We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks.","Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints.","We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and adversarial samples."],"url":"http://arxiv.org/abs/2402.08648v1","category":"cs.LG"}
{"created":"2024-02-13 18:24:23","title":"Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data","abstract":"Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.","sentences":["Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data.","We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation.","The theory gives new insights into reasoning towards human-like machine intelligence."],"url":"http://arxiv.org/abs/2402.08646v1","category":"cs.AI"}
{"created":"2024-02-13 18:24:08","title":"Tandem Transformers for Inference Efficient LLMs","abstract":"The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.","sentences":["The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially.","While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   ","We introduce a novel architecture, Tandem transformers, to address these issues.","This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously).","The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations.","On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance.","We further incorporate the tandem model within the speculative decoding (SPEED) framework where the large model validates tokens from the small model.","This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy."],"url":"http://arxiv.org/abs/2402.08644v1","category":"cs.AI"}
{"created":"2024-02-13 18:13:02","title":"Tuning the Spin Interaction in Non-planar Organic Diradicals Through Mechanical Manipulation","abstract":"Open-shell polycyclic aromatic hydrocarbons (PAHs) represent promising building blocks for carbon-based functional magnetic materials. Their magnetic properties stem from the presence of unpaired electrons localized in radical states of $\\pi$ character. Consequently, these materials are inclined to exhibit spin delocalization, form extended collective states, and respond to the flexibility of the molecular backbones. However, they are also highly reactive, requiring structural strategies to protect the radical states from reacting with the environment. Here, we demonstrate that the open-shell ground state of the diradical 2-OS survives on a Au(111) substrate as a global singlet formed by two unpaired electrons with anti-parallel spins coupled through a conformational dependent interaction. The 2-OS molecule is a protected derivative of the Chichibabin's diradical, featuring a non-planar geometry that destabilizes the closed-shell quinoidal structure. Using scanning tunneling microscopy (STM), we localized the two interacting spins at the molecular edges, and detected an excited triplet state a few millielectronvolts above the singlet ground state. Mean-field Hubbard simulations reveal that the exchange coupling between the two spins strongly depends on the torsional angles between the different molecular moieties, suggesting the possibility of influencing the molecule's magnetic state through structural changes. This was demonstrated here using the STM tip to manipulate the molecular conformation, while simultaneously detecting changes in the spin excitation spectrum. Our work suggests the potential of these PAHs for a new class of all-carbon spin-crossover materials.","sentences":["Open-shell polycyclic aromatic hydrocarbons (PAHs) represent promising building blocks for carbon-based functional magnetic materials.","Their magnetic properties stem from the presence of unpaired electrons localized in radical states of $\\pi$ character.","Consequently, these materials are inclined to exhibit spin delocalization, form extended collective states, and respond to the flexibility of the molecular backbones.","However, they are also highly reactive, requiring structural strategies to protect the radical states from reacting with the environment.","Here, we demonstrate that the open-shell ground state of the diradical 2-OS survives on a Au(111) substrate as a global singlet formed by two unpaired electrons with anti-parallel spins coupled through a conformational dependent interaction.","The 2-OS molecule is a protected derivative of the Chichibabin's diradical, featuring a non-planar geometry that destabilizes the closed-shell quinoidal structure.","Using scanning tunneling microscopy (STM), we localized the two interacting spins at the molecular edges, and detected an excited triplet state a few millielectronvolts above the singlet ground state.","Mean-field Hubbard simulations reveal that the exchange coupling between the two spins strongly depends on the torsional angles between the different molecular moieties, suggesting the possibility of influencing the molecule's magnetic state through structural changes.","This was demonstrated here using the STM tip to manipulate the molecular conformation, while simultaneously detecting changes in the spin excitation spectrum.","Our work suggests the potential of these PAHs for a new class of all-carbon spin-crossover materials."],"url":"http://arxiv.org/abs/2402.08641v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-13 18:09:38","title":"Forecasting high-impact research topics via machine learning on evolving knowledge graphs","abstract":"The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.","sentences":["The exponential growth in scientific publications poses a severe challenge for human researchers.","It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field.","While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived.","Here we show how to predict the impact of onsets of ideas that have never been published by researchers.","For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers.","It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers.","Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions.","We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas."],"url":"http://arxiv.org/abs/2402.08640v1","category":"cs.DL"}
{"created":"2024-02-13 18:04:53","title":"SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages","abstract":"Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs). While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. The scores are obtained using a comparative annotation framework. We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP. We further report experiments for each language and across the different languages.","sentences":["Exploring and quantifying semantic relatedness is central to representing language.","It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of Large Language Models (LLMs).","While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness.","In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu.","These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by a relatively limited availability of NLP resources.","Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences.","The scores are obtained using a comparative annotation framework.","We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP.","We further report experiments for each language and across the different languages."],"url":"http://arxiv.org/abs/2402.08638v2","category":"cs.CL"}
{"created":"2024-02-13 17:59:34","title":"Knowledge Editing on Black-box Large Language Models","abstract":"Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82\\%\\uparrow$).","sentences":["Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge.","Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available.","To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time.","To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses.","Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82\\%\\uparrow$)."],"url":"http://arxiv.org/abs/2402.08631v1","category":"cs.CL"}
{"created":"2024-02-13 17:56:47","title":"Nonlinear Graphon mean-field systems","abstract":"We address a system of weakly interacting particles where the heterogenous connections among the particles are described by a graph sequence and the number of particles grows to infinity. Our results extend the existing law of large numbers and propagation of chaos results to the case where the interaction between one particle and its neighbors is expressed as a nonlinear function of the local empirical measure. In the limit of the number of particles which tends to infinity, if the graph sequence converges to a graphon, then we show that the limit system is described by an infinite collection of processes and can be seen as a process in a suitable $L^2$ space constructed via a Fubini extension. The proof is built on decoupling techniques and careful estimates of the Wasserstein distance.","sentences":["We address a system of weakly interacting particles where the heterogenous connections among the particles are described by a graph sequence and the number of particles grows to infinity.","Our results extend the existing law of large numbers and propagation of chaos results to the case where the interaction between one particle and its neighbors is expressed as a nonlinear function of the local empirical measure.","In the limit of the number of particles which tends to infinity, if the graph sequence converges to a graphon, then we show that the limit system is described by an infinite collection of processes and can be seen as a process in a suitable $L^2$ space constructed via a Fubini extension.","The proof is built on decoupling techniques and careful estimates of the Wasserstein distance."],"url":"http://arxiv.org/abs/2402.08628v1","category":"math.PR"}
{"created":"2024-02-13 17:42:12","title":"Quasineutral multistability in an epidemiological-like model for defective-helper betacoronavirus infection in cell cultures","abstract":"It is well known that, during replication, RNA viruses spontaneously generate defective viral genomes (DVGs). DVGs are unable to complete an infectious cycle autonomously, and depend on coinfection with a helper wild-type virus (HV) for their replication and/or transmission. The study of the dynamics arising from a HV and its DVGs has been a longstanding question in virology. It has been shown that DVGs can modulate HV replication and, depending on the strength of interference, result in HV extinctions or self-sustained persistent fluctuations. Extensive experimental work has provided mechanistic explanations for DVG generation and compelling evidences of HV-DVGs virus coevolution. Some of these observations have been captured in mathematical models. Here, we develop and investigate an epidemiological-like mathematical model specifically designed to study the dynamics of betacoronavirus in cell cultures experiments. The dynamics of the model is governed by several degenerate normally hyperbolic invariant manifolds given by quasineutral planes - i.e. filled by equilibrium points. Three different quasineutral planes have been identified depending on parameters and involving: (i) persistence of HV and DVGs; (\\emph{ii}) persistence of non-infected cells and DVG-infected cells; and (iii) persistence of DVG-infected cells and DVGs. Sensitivity analyses indicate that model dynamics largely depend on the maximum burst size ($B$), and both the production rate ($\\beta$) and replicative advantage ($\\delta$) of DVGs. Finally, the model has been fitted to single-passage experimental data using artificial intelligence and key virological parameters have been estimated.","sentences":["It is well known that, during replication, RNA viruses spontaneously generate defective viral genomes (DVGs).","DVGs are unable to complete an infectious cycle autonomously, and depend on coinfection with a helper wild-type virus (HV) for their replication and/or transmission.","The study of the dynamics arising from a HV and its DVGs has been a longstanding question in virology.","It has been shown that DVGs can modulate HV replication and, depending on the strength of interference, result in HV extinctions or self-sustained persistent fluctuations.","Extensive experimental work has provided mechanistic explanations for DVG generation and compelling evidences of HV-DVGs virus coevolution.","Some of these observations have been captured in mathematical models.","Here, we develop and investigate an epidemiological-like mathematical model specifically designed to study the dynamics of betacoronavirus in cell cultures experiments.","The dynamics of the model is governed by several degenerate normally hyperbolic invariant manifolds given by quasineutral planes - i.e. filled by equilibrium points.","Three different quasineutral planes have been identified depending on parameters and involving: (i) persistence of HV and DVGs; (\\emph{ii}) persistence of non-infected cells and DVG-infected cells; and (iii) persistence of DVG-infected cells and DVGs.","Sensitivity analyses indicate that model dynamics largely depend on the maximum burst size ($B$), and both the production rate ($\\beta$) and replicative advantage ($\\delta$) of DVGs.","Finally, the model has been fitted to single-passage experimental data using artificial intelligence and key virological parameters have been estimated."],"url":"http://arxiv.org/abs/2402.08620v1","category":"math.DS"}
{"created":"2024-02-13 17:18:56","title":"Mixtures of Experts Unlock Parameter Scaling for Deep RL","abstract":"The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.","sentences":["The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size.","Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance.","In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes.","This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning."],"url":"http://arxiv.org/abs/2402.08609v1","category":"cs.LG"}
{"created":"2024-02-13 17:09:29","title":"Globally-Optimal Greedy Experiment Selection for Active Sequential Estimation","abstract":"Motivated by modern applications such as computerized adaptive testing, sequential rank aggregation, and heterogeneous data source selection, we study the problem of active sequential estimation, which involves adaptively selecting experiments for sequentially collected data. The goal is to design experiment selection rules for more accurate model estimation. Greedy information-based experiment selection methods, optimizing the information gain for one-step ahead, have been employed in practice thanks to their computational convenience, flexibility to context or task changes, and broad applicability. However, statistical analysis is restricted to one-dimensional cases due to the problem's combinatorial nature and the seemingly limited capacity of greedy algorithms, leaving the multidimensional problem open.   In this study, we close the gap for multidimensional problems. In particular, we propose adopting a class of greedy experiment selection methods and provide statistical analysis for the maximum likelihood estimator following these selection rules. This class encompasses both existing methods and introduces new methods with improved numerical efficiency. We prove that these methods produce consistent and asymptotically normal estimators. Additionally, within a decision theory framework, we establish that the proposed methods achieve asymptotic optimality when the risk measure aligns with the selection rule. We also conduct extensive numerical studies on both simulated and real data to illustrate the efficacy of the proposed methods.   From a technical perspective, we devise new analytical tools to address theoretical challenges. These analytical tools are of independent theoretical interest and may be reused in related problems involving stochastic approximation and sequential designs.","sentences":["Motivated by modern applications such as computerized adaptive testing, sequential rank aggregation, and heterogeneous data source selection, we study the problem of active sequential estimation, which involves adaptively selecting experiments for sequentially collected data.","The goal is to design experiment selection rules for more accurate model estimation.","Greedy information-based experiment selection methods, optimizing the information gain for one-step ahead, have been employed in practice thanks to their computational convenience, flexibility to context or task changes, and broad applicability.","However, statistical analysis is restricted to one-dimensional cases due to the problem's combinatorial nature and the seemingly limited capacity of greedy algorithms, leaving the multidimensional problem open.   ","In this study, we close the gap for multidimensional problems.","In particular, we propose adopting a class of greedy experiment selection methods and provide statistical analysis for the maximum likelihood estimator following these selection rules.","This class encompasses both existing methods and introduces new methods with improved numerical efficiency.","We prove that these methods produce consistent and asymptotically normal estimators.","Additionally, within a decision theory framework, we establish that the proposed methods achieve asymptotic optimality when the risk measure aligns with the selection rule.","We also conduct extensive numerical studies on both simulated and real data to illustrate the efficacy of the proposed methods.   ","From a technical perspective, we devise new analytical tools to address theoretical challenges.","These analytical tools are of independent theoretical interest and may be reused in related problems involving stochastic approximation and sequential designs."],"url":"http://arxiv.org/abs/2402.08602v1","category":"math.ST"}
{"created":"2024-02-13 16:53:48","title":"Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs","abstract":"In this paper, we present \"Graph Feature Preprocessor\", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Preprocessor and gradient-boosting-based machine learning models, is able to detect these illicit transactions with higher minority-class F1 scores than standard graph neural networks. In addition, the end-to-end throughput rate of our solution executed on a multicore CPU outperforms the graph neural network baselines executed on a powerful V100 GPU. Overall, the combination of high accuracy, a high throughput rate, and low latency of our solution demonstrates the practical value of our library in real-world applications. Graph Feature Preprocessor has been integrated into IBM mainframe software products, namely \"IBM Cloud Pak for Data on Z\" and \"AI Toolkit for IBM Z and LinuxONE\".","sentences":["In this paper, we present \"Graph Feature Preprocessor\", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time.","These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection.","We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models.","Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner.","We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets.","In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging.","Our solution, which combines our Graph Feature Preprocessor and gradient-boosting-based machine learning models, is able to detect these illicit transactions with higher minority-class F1 scores than standard graph neural networks.","In addition, the end-to-end throughput rate of our solution executed on a multicore CPU outperforms the graph neural network baselines executed on a powerful V100 GPU.","Overall, the combination of high accuracy, a high throughput rate, and low latency of our solution demonstrates the practical value of our library in real-world applications.","Graph Feature Preprocessor has been integrated into IBM mainframe software products, namely \"IBM Cloud Pak for Data on Z\" and \"AI Toolkit for IBM Z and LinuxONE\"."],"url":"http://arxiv.org/abs/2402.08593v1","category":"cs.LG"}
{"created":"2024-02-13 16:52:10","title":"Convolutional Neural Networks Towards Facial Skin Lesions Detection","abstract":"Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment. Manipulating patient images often necessitates professional image processing software. This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach. The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods. The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion. Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services. The dataset includes administrative information, clinical data, and facial and profile images. A total of 2300 patient images were extracted from this raw data. A software tool was developed to crop and label lesions, with input from two treatment experts. In the lesion preparation phase, the selected area was standardized to 50 * 50 pixels. Subsequently, a convolutional neural network model was employed for lesion labeling. The classification model demonstrated high accuracy, with a measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity. Internal validation involved performance indicators and cross-validation, while external validation compared the model's performance indicators with those of the transfer learning method using the Vgg16 deep network model. Compared to existing studies, the results of this research showcase the efficacy and desirability of the proposed model and methodology.","sentences":["Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment.","Manipulating patient images often necessitates professional image processing software.","This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach.","The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods.","The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion.","Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services.","The dataset includes administrative information, clinical data, and facial and profile images.","A total of 2300 patient images were extracted from this raw data.","A software tool was developed to crop and label lesions, with input from two treatment experts.","In the lesion preparation phase, the selected area was standardized to 50 * 50 pixels.","Subsequently, a convolutional neural network model was employed for lesion labeling.","The classification model demonstrated high accuracy, with a measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity.","Internal validation involved performance indicators and cross-validation, while external validation compared the model's performance indicators with those of the transfer learning method using the Vgg16 deep network model.","Compared to existing studies, the results of this research showcase the efficacy and desirability of the proposed model and methodology."],"url":"http://arxiv.org/abs/2402.08592v1","category":"eess.IV"}
{"created":"2024-02-13 16:46:11","title":"Machine Learning based Pointing Models for Radio/Sub-millimeter Telescopes","abstract":"Radio, sub-millimiter and millimeter ground-based telescopes are powerful instruments for studying the gas and dust-rich regions of the Universe that are invisible at optical wavelengths, but the pointing accuracy is crucial for obtaining high-quality data. Pointing errors are small deviations of the telescope's orientation from its desired direction. The telescopes use linear regression pointing models to correct for these errors, taking into account various factors such as weather conditions, telescope mechanical structure, and the target's position in the sky. However, residual pointing errors can still occur due to factors that are hard to model accurately, such as thermal and gravitational deformation and environmental conditions like humidity and wind. Here we present a proof-of-concept for reducing pointing error for the Atacama Pathfinder EXperiment (APEX) telescope in the high-altitude Atacama Desert in Chile based on machine learning. Using historic pointing data from 2022, we trained eXtreme Gradient Boosting (XGBoost) models that reduced the root-mean-square errors (RMSE) for azimuth and elevation (horizontal and vertical angle) pointing corrections by 4.3% and 9.5%, respectively, on hold-out test data. Our results will inform operations of current and future facilities such as the next-generation Atacama Large Aperture Submillimeter Telescope (AtLAST).","sentences":["Radio, sub-millimiter and millimeter ground-based telescopes are powerful instruments for studying the gas and dust-rich regions of the Universe that are invisible at optical wavelengths, but the pointing accuracy is crucial for obtaining high-quality data.","Pointing errors are small deviations of the telescope's orientation from its desired direction.","The telescopes use linear regression pointing models to correct for these errors, taking into account various factors such as weather conditions, telescope mechanical structure, and the target's position in the sky.","However, residual pointing errors can still occur due to factors that are hard to model accurately, such as thermal and gravitational deformation and environmental conditions like humidity and wind.","Here we present a proof-of-concept for reducing pointing error for the Atacama Pathfinder EXperiment (APEX) telescope in the high-altitude Atacama Desert in Chile based on machine learning.","Using historic pointing data from 2022, we trained eXtreme Gradient Boosting (XGBoost) models that reduced the root-mean-square errors (RMSE) for azimuth and elevation (horizontal and vertical angle) pointing corrections by 4.3% and 9.5%, respectively, on hold-out test data.","Our results will inform operations of current and future facilities such as the next-generation Atacama Large Aperture Submillimeter Telescope (AtLAST)."],"url":"http://arxiv.org/abs/2402.08589v1","category":"astro-ph.IM"}
{"created":"2024-02-13 16:36:21","title":"FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis","abstract":"Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision in the analysis of medical images. Further, FESS loss demonstrates superior performance in limited annotated data availability scenarios often present in the medical domain.","sentences":["Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research.","It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures.","Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions.","To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss.","The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images.","FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision in the analysis of medical images.","Further, FESS loss demonstrates superior performance in limited annotated data availability scenarios often present in the medical domain."],"url":"http://arxiv.org/abs/2402.08582v1","category":"cs.CV"}
{"created":"2024-02-13 16:30:30","title":"FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing","abstract":"Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To further reduce resource consumption, a channel-wise model pruning algorithm that shrinks the footprint of local models while accounting for both data and system heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous model aggregation algorithm is proposed to aggregate the heterogeneous predictors in FedLPS. We implemented the proposed FedLPS on a real FL platform and compared it with state-of-the-art (SOTA) FL frameworks. The experimental results on five popular datasets and two modern DNN models illustrate that the proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88% and reduces the computational resource consumption by 21.3%. Our code is available at:https://github.com/jyzgh/FedLPS.","sentences":["Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices.","By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy.","Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity.","However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios.","In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap.","FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders.","To further reduce resource consumption, a channel-wise model pruning algorithm that shrinks the footprint of local models while accounting for both data and system heterogeneity is employed in FedLPS.","Additionally, a novel heterogeneous model aggregation algorithm is proposed to aggregate the heterogeneous predictors in FedLPS.","We implemented the proposed FedLPS on a real FL platform and compared it with state-of-the-art (SOTA) FL frameworks.","The experimental results on five popular datasets and two modern DNN models illustrate that the proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88% and reduces the computational resource consumption by 21.3%.","Our code is available at:https://github.com/jyzgh/FedLPS."],"url":"http://arxiv.org/abs/2402.08578v1","category":"cs.LG"}
{"created":"2024-02-13 16:14:32","title":"Online Foundation Model Selection in Robotics","abstract":"Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, without additional training. The online learning algorithm aims to maximize a composite reward that includes model performance, execution time, and costs based on the context extracted from the data. It results in an improved trade-off between selecting open-source and closed-source models compared to non-contextual methods, as validated by our theoretical analysis. Experiments across language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open X-Embodiment demonstrate real-world applications of the solution. The results show that the solution significantly improves the task success rate by up to 14%.","sentences":["Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing.","The models are accessible in two ways: open-source or paid, closed-source options.","Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives.","We call it the model selection problem.","Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models.","Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets.","We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context.","The encoder distills vast data distributions into low-dimensional features, i.e., the context, without additional training.","The online learning algorithm aims to maximize a composite reward that includes model performance, execution time, and costs based on the context extracted from the data.","It results in an improved trade-off between selecting open-source and closed-source models compared to non-contextual methods, as validated by our theoretical analysis.","Experiments across language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open X-Embodiment demonstrate real-world applications of the solution.","The results show that the solution significantly improves the task success rate by up to 14%."],"url":"http://arxiv.org/abs/2402.08570v1","category":"cs.RO"}
{"created":"2024-02-13 16:05:51","title":"Artificial Intelligence for Literature Reviews: Opportunities and Challenges","abstract":"This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research.","sentences":["This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs).","A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic.","Numerous tools have been developed to assist and partially automate the SLR process.","The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews.","Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases.","We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features.","We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing.","Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research."],"url":"http://arxiv.org/abs/2402.08565v1","category":"cs.AI"}
{"created":"2024-02-13 16:04:21","title":"Higher Layers Need More LoRA Experts","abstract":"Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines. We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code is available at https://github.com/GCYZSL/MoLA.","sentences":["Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited.","Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods.","Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages.","Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy.","Does this statement also apply to parameter-efficient MoE?","In this paper, we introduce a novel parameter-efficient MoE method, \\textit{\\textbf{M}oE-L\\textbf{o}RA with \\textbf{L}ayer-wise Expert \\textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts.","We investigate several architectures with varying layer-wise expert configurations.","Experiments on six well-known NLP and commonsense QA benchmarks demonstrate that MoLA achieves equal or superior performance compared to all baselines.","We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total.","With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer.","This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications.","The code is available at https://github.com/GCYZSL/MoLA."],"url":"http://arxiv.org/abs/2402.08562v1","category":"cs.CL"}
{"created":"2024-02-13 15:59:19","title":"Exploring diversity perceptions in a community through a Q&A chatbot","abstract":"While diversity has become a debated issue in design, very little research exists on positive use-cases for diversity beyond scholarly criticism. The current work addresses this gap through the case of a diversity-aware chatbot, exploring what benefits a diversity-aware chatbot could bring to people and how do people interpret diversity when being presented with it. In this paper, we motivate a Q&A chatbot as a technology probe and deploy it in two student communities within a study. During the study, we collected contextual data on people's expectations and perceptions when presented with diversity during the study. Our key findings show that people seek out others with shared niche interests, or their search is driven by exploration and inspiration when presented with diversity. Although interacting with chatbots is limited, participants found the engagement novel and interesting to motivate future research.","sentences":["While diversity has become a debated issue in design, very little research exists on positive use-cases for diversity beyond scholarly criticism.","The current work addresses this gap through the case of a diversity-aware chatbot, exploring what benefits a diversity-aware chatbot could bring to people and how do people interpret diversity when being presented with it.","In this paper, we motivate a Q&A chatbot as a technology probe and deploy it in two student communities within a study.","During the study, we collected contextual data on people's expectations and perceptions when presented with diversity during the study.","Our key findings show that people seek out others with shared niche interests, or their search is driven by exploration and inspiration when presented with diversity.","Although interacting with chatbots is limited, participants found the engagement novel and interesting to motivate future research."],"url":"http://arxiv.org/abs/2402.08558v1","category":"cs.HC"}
{"created":"2024-02-13 15:53:09","title":"Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting","abstract":"We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).   We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average. We show this theorem using a connection with Blackwell approachability.   Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\\sqrt{T})$.","sentences":["We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake.","In each round, a new cake arrives, which is identical to the ones in previous rounds.","Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice.","We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice.","The simultaneous version was first considered by Aumann and Maschler (1995).   ","We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search.","This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   ","We analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable.","Specifically, the players can enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average.","We show this theorem using a connection with Blackwell approachability.   ","Finally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player.","We show that fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\\sqrt{T})$."],"url":"http://arxiv.org/abs/2402.08547v1","category":"cs.GT"}
{"created":"2024-02-13 15:43:30","title":"Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning","abstract":"This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and aims to explore early detection and disease progression in Alzheimer's disease (AD). We employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources. Through Spearman correlation coefficient analysis, we identify some features strongly correlated with AD diagnosis. We build and test three machine learning models using these features: random forest, XGBoost, and support vector machine (SVM). Among them, the XGBoost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of Alzheimer's disease, demonstrating its unique research value and practical significance.","sentences":["This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and aims to explore early detection and disease progression in Alzheimer's disease (AD).","We employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources.","Through Spearman correlation coefficient analysis, we identify some features strongly correlated with AD diagnosis.","We build and test three machine learning models using these features: random forest, XGBoost, and support vector machine (SVM).","Among them, the XGBoost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%.","Overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of Alzheimer's disease, demonstrating its unique research value and practical significance."],"url":"http://arxiv.org/abs/2402.08539v1","category":"cs.LG"}
{"created":"2024-02-13 15:40:38","title":"Multiplicity dependence of the freezeout parameters in high energy hadron-hadron collisions","abstract":"We examined the transverse momentum spectra of various identified particles, across different multiplicity classes in proton-proton collisions at a center-of-mass energy of $\\sqrt{s}$ = 7 TeV. Utilizing the Tsallis and Hagedorn models, parameters relevant to the bulk properties of nuclear matter were extracted. Both models exhibit good agreement with experimental data. In our analyses, we observed a consistent decrease in the effective temperature for the Tsallis model and the kinetic or thermal freeze-out temperature for the Hagedorn model, as we transition from higher multiplicity (class-I) to lower multiplicity (class-X). Additionally, the transverse flow velocity experiences a decline from class-I to class-X. The normalization constant which represents the multiplicity of produced particles is observed to decrease as we move towards higher multiplicity classes. While the effective and kinetic freeze-out temperatures, as well as the transverse flow velocity, show a mild dependency on multiplicity for lighter particles, this relationship becomes more pronounced for heavier particles. Various particle species are observed to undergo decoupling from the fireball at distinct temperatures: lighter particles exhibit lower temperatures, while heavier ones show higher temperatures, thereby supporting the concept of multiple freeze-out scenarios. Moreover, we identified a positive correlation between the kinetic freeze-out temperature and transverse flow velocity, a scenario where particles experience stronger collective motion at higher freeze-out temperature. The reason for this positive correlation is that as the multiplicity increases, more energy is transferred into the system. This heightened energy causes greater excitation and pressure within the system, leading to a quick expansion.","sentences":["We examined the transverse momentum spectra of various identified particles, across different multiplicity classes in proton-proton collisions at a center-of-mass energy of $\\sqrt{s}$ = 7 TeV. Utilizing the Tsallis and Hagedorn models, parameters relevant to the bulk properties of nuclear matter were extracted.","Both models exhibit good agreement with experimental data.","In our analyses, we observed a consistent decrease in the effective temperature for the Tsallis model and the kinetic or thermal freeze-out temperature for the Hagedorn model, as we transition from higher multiplicity (class-I) to lower multiplicity (class-X).","Additionally, the transverse flow velocity experiences a decline from class-I to class-X.","The normalization constant which represents the multiplicity of produced particles is observed to decrease as we move towards higher multiplicity classes.","While the effective and kinetic freeze-out temperatures, as well as the transverse flow velocity, show a mild dependency on multiplicity for lighter particles, this relationship becomes more pronounced for heavier particles.","Various particle species are observed to undergo decoupling from the fireball at distinct temperatures: lighter particles exhibit lower temperatures, while heavier ones show higher temperatures, thereby supporting the concept of multiple freeze-out scenarios.","Moreover, we identified a positive correlation between the kinetic freeze-out temperature and transverse flow velocity, a scenario where particles experience stronger collective motion at higher freeze-out temperature.","The reason for this positive correlation is that as the multiplicity increases, more energy is transferred into the system.","This heightened energy causes greater excitation and pressure within the system, leading to a quick expansion."],"url":"http://arxiv.org/abs/2402.08535v1","category":"hep-ph"}
{"created":"2024-02-13 15:35:24","title":"A Distributional Analogue to the Successor Representation","abstract":"This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible.","sentences":["This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process.","Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour.","We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning.","Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy.","Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state.","As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible."],"url":"http://arxiv.org/abs/2402.08530v1","category":"cs.LG"}
{"created":"2024-02-13 15:29:50","title":"Concept-1K: A Novel Benchmark for Instance Incremental Learning","abstract":"Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.","sentences":["Incremental learning (IL) is essential to realize the human-level intelligence in the neural network.","However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting.","To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps.","Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size.","Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance.","Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs.","The data, code and scripts are publicly available at https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning."],"url":"http://arxiv.org/abs/2402.08526v1","category":"cs.LG"}
{"created":"2024-02-13 15:10:30","title":"Counterfactual Influence in Markov Decision Processes","abstract":"Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs). Given an MDP path $\\tau$, this kind of inference allows us to derive counterfactual paths $\\tau'$ describing what-if versions of $\\tau$ obtained under different action sequences than those observed in $\\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones. Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions. We devise an algorithm to construct counterfactual models that automatically satisfy influence constraints. Leveraging such models, we derive counterfactual policies that are not just optimal for a given reward structure but also remain tailored to the observed path. Even though there is an unavoidable trade-off between policy optimality and strength of influence constraints, our experiments demonstrate that it is possible to derive (near-)optimal policies while remaining under the influence of the observation.","sentences":["Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs).","Given an MDP path $\\tau$, this kind of inference allows us to derive counterfactual paths $\\tau'$ describing what-if versions of $\\tau$ obtained under different action sequences than those observed in $\\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones.","Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now.","In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions.","We devise an algorithm to construct counterfactual models that automatically satisfy influence constraints.","Leveraging such models, we derive counterfactual policies that are not just optimal for a given reward structure but also remain tailored to the observed path.","Even though there is an unavoidable trade-off between policy optimality and strength of influence constraints, our experiments demonstrate that it is possible to derive (near-)optimal policies while remaining under the influence of the observation."],"url":"http://arxiv.org/abs/2402.08514v1","category":"cs.AI"}
{"created":"2024-02-13 15:05:54","title":"Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the Unknown","abstract":"Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast amount of applications. It strategically allocates computational resources to focus on promising segments of the search tree, making it a very attractive search algorithm in large search spaces. However, it often expends its limited resources on reevaluating previously explored regions when they remain the most promising path. Our proposed methodology, denoted as AmEx-MCTS, solves this problem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the decoupling of value updates, visit count updates, and the selected path during the tree search, thereby enabling the exclusion of already explored subtrees or leaves. This segregation preserves the utility of visit counts for both exploration-exploitation balancing and quality metrics within MCTS. The resultant augmentation facilitates in a considerably broader search using identical computational resources, preserving the essential characteristics of MCTS. The expanded coverage not only yields more precise estimations but also proves instrumental in larger and more complex problems. Our empirical evaluation demonstrates the superior performance of AmEx-MCTS, surpassing classical MCTS and related approaches by a substantial margin.","sentences":["Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast amount of applications.","It strategically allocates computational resources to focus on promising segments of the search tree, making it a very attractive search algorithm in large search spaces.","However, it often expends its limited resources on reevaluating previously explored regions when they remain the most promising path.","Our proposed methodology, denoted as AmEx-MCTS, solves this problem by introducing a novel MCTS formulation.","Central to AmEx-MCTS is the decoupling of value updates, visit count updates, and the selected path during the tree search, thereby enabling the exclusion of already explored subtrees or leaves.","This segregation preserves the utility of visit counts for both exploration-exploitation balancing and quality metrics within MCTS.","The resultant augmentation facilitates in a considerably broader search using identical computational resources, preserving the essential characteristics of MCTS.","The expanded coverage not only yields more precise estimations but also proves instrumental in larger and more complex problems.","Our empirical evaluation demonstrates the superior performance of AmEx-MCTS, surpassing classical MCTS and related approaches by a substantial margin."],"url":"http://arxiv.org/abs/2402.08511v1","category":"cs.AI"}
{"created":"2024-02-13 15:04:11","title":"From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)","abstract":"SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input graphs into new output graphs. It is now common to constrain graphs through SHACL shapes allowing users to understand which data they can expect and which not. However, it becomes challenging to understand what graph data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input graph may affect the output graph, but may no longer apply literally, and new shapes may be imposed by the query template. In this paper, we study the derivation of shape constraints that hold on all possible output graphs of a given SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input graphs adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of SHACL (Simple SHACL). We formally define the problem of deriving the most restrictive set of Simple SHACL shapes that constrain the results from evaluating a SCCQ over any input graph restricted by a given set of Simple SHACL shapes. We propose and implement an algorithm that statically analyses input SHACL shapes and CONSTRUCT queries and prove its soundness and complexity.","sentences":["SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input graphs into new output graphs.","It is now common to constrain graphs through SHACL shapes allowing users to understand which data they can expect and which not.","However, it becomes challenging to understand what graph data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input graph may affect the output graph, but may no longer apply literally, and new shapes may be imposed by the query template.","In this paper, we study the derivation of shape constraints that hold on all possible output graphs of a given SPARQL CONSTRUCT query.","We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input graphs adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown.","We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of SHACL (Simple SHACL).","We formally define the problem of deriving the most restrictive set of Simple SHACL shapes that constrain the results from evaluating a SCCQ over any input graph restricted by a given set of Simple SHACL shapes.","We propose and implement an algorithm that statically analyses input SHACL shapes and CONSTRUCT queries and prove its soundness and complexity."],"url":"http://arxiv.org/abs/2402.08509v1","category":"cs.DB"}
{"created":"2024-02-13 14:51:45","title":"A Systematic Review of Data-to-Text NLG","abstract":"This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.","sentences":["This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review.","We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures.","Our review provides a roadmap for future research in this rapidly evolving field."],"url":"http://arxiv.org/abs/2402.08496v1","category":"cs.CL"}
{"created":"2024-02-13 14:38:12","title":"The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale","abstract":"Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation. ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications. This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment. Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023. These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists. Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning. Consistency was evaluated through two rounds of testing. Results: In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists. Future research should focus on in-depth Fine-tuning.","sentences":["Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation.","ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications.","This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment.","Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023.","These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists.","Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning.","Consistency was evaluated through two rounds of testing.","Results:","In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%.","Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists.","Conclusion: While ChatGPT shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists.","Future research should focus on in-depth Fine-tuning."],"url":"http://arxiv.org/abs/2402.08492v1","category":"cs.AI"}
{"created":"2024-02-13 14:36:46","title":"Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming","abstract":"Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.","sentences":["Cellular reprogramming can be used for both the prevention and cure of different diseases.","However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs.","In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies.","For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode.","Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training.","Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models."],"url":"http://arxiv.org/abs/2402.08491v1","category":"cs.LG"}
{"created":"2024-02-13 14:10:15","title":"A precise bare simulation approach to the minimization of some distances. II. Further Foundations","abstract":"The constrained minimization (respectively maximization) of directed distances and of related generalized entropies is a fundamental task in information theory as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition. In our previous paper \"A precise bare simulation approach to the minimization of some distances. I. Foundations\", we obtained such kind of constrained optima by a new dimension-free precise bare (pure) simulation method, provided basically that (i) the underlying directed distance is of f-divergence type, and that (ii) this can be connected to a light-tailed probability distribution in a certain manner. In the present paper, we extend this approach such that constrained optimization problems of a very huge amount of directed distances and generalized entropies -- and beyond -- can be tackled by a newly developed dimension-free extended bare simulation method, for obtaining both optima as well as optimizers. Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit). For instance, we cover constrained optimizations of arbitrary f-divergences, Bregman distances, scaled Bregman distances and weighted Euclidean distances. The potential for wide-spread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences in various different research fields (which may also serve as an interdisciplinary interface).","sentences":["The constrained minimization (respectively maximization) of directed distances and of related generalized entropies is a fundamental task in information theory as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition.","In our previous paper \"A precise bare simulation approach to the minimization of some distances.","I. Foundations\", we obtained such kind of constrained optima by a new dimension-free precise bare (pure) simulation method, provided basically that (i) the underlying directed distance is of f-divergence type, and that (ii) this can be connected to a light-tailed probability distribution in a certain manner.","In the present paper, we extend this approach such that constrained optimization problems of a very huge amount of directed distances and generalized entropies -- and beyond -- can be tackled by a newly developed dimension-free extended bare simulation method, for obtaining both optima as well as optimizers.","Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit).","For instance, we cover constrained optimizations of arbitrary f-divergences, Bregman distances, scaled Bregman distances and weighted Euclidean distances.","The potential for wide-spread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences in various different research fields (which may also serve as an interdisciplinary interface)."],"url":"http://arxiv.org/abs/2402.08478v1","category":"cs.IT"}
{"created":"2024-02-13 14:07:49","title":"Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models","abstract":"Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that our results are applicable to other transformer models with continuous inputs. We also propose a robust way to detect the modified images.","sentences":["Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets.","However, these models are poorly understood due to their complexity and size.","While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets.","In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model.","Using the Imagenette dataset, we show that while the model achieves over 99\\% zero-shot classification performance, it fails systematic evaluations completely.","Using a linear approximation, we provide a framework to explain the striking differences.","We have also obtained similar results using a different model to support that our results are applicable to other transformer models with continuous inputs.","We also propose a robust way to detect the modified images."],"url":"http://arxiv.org/abs/2402.08473v1","category":"cs.CV"}
{"created":"2024-02-13 14:05:02","title":"Large Language Models for the Automated Analysis of Optimization Algorithms","abstract":"The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity. In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb. This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior. Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted. In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community. Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential of LLMs in this field.","sentences":["The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity.","In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb.","This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior.","Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted.","In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community.","Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential of LLMs in this field."],"url":"http://arxiv.org/abs/2402.08472v1","category":"cs.AI"}
{"created":"2024-02-13 14:00:59","title":"Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale","abstract":"We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term \"aging\" trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously. ST-GTrend imposes flatness and smoothness regularization to ensure the disentanglement between aging and fluctuation. To scale the analysis to large PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate the training and inference of ST-GTrend. We have evaluated ST-GTrend on three large-scale PV datasets, spanning a time period of 10 years. Our results show that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean Distances by 34.74% and 33.66% compared to the SOTA methods. Our results demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We further verify the generality and effectiveness of ST-GTrend for trend analysis using financial and economic datasets.","sentences":["We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks.","PV power stations have become an integral component to the global sustainable energy production landscape.","Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset.","One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters.","ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term \"aging\" trend from multiple fluctuation terms in the PV input data.","To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously.","ST-GTrend imposes flatness and smoothness regularization to ensure the disentanglement between aging and fluctuation.","To scale the analysis to large PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate the training and inference of ST-GTrend.","We have evaluated ST-GTrend on three large-scale PV datasets, spanning a time period of 10 years.","Our results show that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean Distances by 34.74% and 33.66% compared to the SOTA methods.","Our results demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times.","We further verify the generality and effectiveness of ST-GTrend for trend analysis using financial and economic datasets."],"url":"http://arxiv.org/abs/2402.08470v1","category":"cs.LG"}
{"created":"2024-02-13 13:54:47","title":"ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System","abstract":"Most of the intrusion detection datasets to research machine learning-based intrusion detection systems (IDSs) are devoted to cyber-only systems, and they typically collect data from one architectural layer. Additionally, often the attacks are generated in dedicated attack sessions, without reproducing the realistic alternation and overlap of normal and attack actions. We present a dataset for intrusion detection by performing penetration testing on an embedded cyber-physical system built over Robot Operating System 2 (ROS2). Features are monitored from three architectural layers: the Linux operating system, the network, and the ROS2 services. The dataset is structured as a time series and describes the expected behavior of the system and its response to ROS2-specific attacks: it repeatedly alternates periods of attack-free operation with periods when a specific attack is being performed. Noteworthy, this allows measuring the time to detect an attacker and the number of malicious activities performed before detection. Also, it allows training an intrusion detector to minimize both, by taking advantage of the numerous alternating periods of normal and attack operations.","sentences":["Most of the intrusion detection datasets to research machine learning-based intrusion detection systems (IDSs) are devoted to cyber-only systems, and they typically collect data from one architectural layer.","Additionally, often the attacks are generated in dedicated attack sessions, without reproducing the realistic alternation and overlap of normal and attack actions.","We present a dataset for intrusion detection by performing penetration testing on an embedded cyber-physical system built over Robot Operating System 2 (ROS2).","Features are monitored from three architectural layers: the Linux operating system, the network, and the ROS2 services.","The dataset is structured as a time series and describes the expected behavior of the system and its response to ROS2-specific attacks: it repeatedly alternates periods of attack-free operation with periods when a specific attack is being performed.","Noteworthy, this allows measuring the time to detect an attacker and the number of malicious activities performed before detection.","Also, it allows training an intrusion detector to minimize both, by taking advantage of the numerous alternating periods of normal and attack operations."],"url":"http://arxiv.org/abs/2402.08468v1","category":"cs.CR"}
{"created":"2024-02-13 13:48:54","title":"Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence","abstract":"Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connection between the emerging management-based regulatory frameworks governing AI and the need for human oversight during training. We broadly cover some of the technical components involved in human-guided training and then argue that the kinds of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training than on data-only training. We hope to foster a discussion between legal scholars and computer scientists involving how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks.","sentences":["Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation.","Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm.","These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed.","Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm.","If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability.","In this paper, we discuss the connection between the emerging management-based regulatory frameworks governing AI and the need for human oversight during training.","We broadly cover some of the technical components involved in human-guided training and then argue that the kinds of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training than on data-only training.","We hope to foster a discussion between legal scholars and computer scientists involving how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks."],"url":"http://arxiv.org/abs/2402.08466v1","category":"cs.AI"}
{"created":"2024-02-13 13:39:42","title":"The SRG/eROSITA All-Sky Survey: X-ray selection function models for the eRASS1 galaxy cluster cosmology","abstract":"Characterising galaxy cluster populations from catalog of sources selected in astronomical surveys requires knowledge of sample incompleteness, known as selection function. The first All-Sky Survey (eRASS1) by eROSITA onboard Spectrum Roentgen Gamma (SRG) has enabled the collection of large samples of galaxy clusters detected in the soft X-ray band over the Western Galactic hemisphere. The driving goal consists in constraining cosmological parameters, which puts stringent requirements on accuracy, flexibility and explainability of the selection function models. We use a large set of mock observations of the eRASS1 survey and we process simulated data identically to the real eRASS1 events. We match detected sources to simulated clusters and we associate detections to intrinsic cluster properties. We train a series of models to build selection functions depending only on observable surface brightness data. We develop a second series of models relying on global cluster characteristics such as X-ray luminosity, flux, and expected instrumental count-rate as well as on morphological properties. We validate our models using our simulations and we rank them according to selected performance metrics. We validate the models with datasets of clusters detected in X-rays and via the Sunyaev-Zeldovich effect. We present the complete Bayesian population modelling framework developed for this purpose. Our results reveal the surface brightness characteristics most relevant to cluster selection in the eRASS1 sample, in particular the ambiguous role of central surface brightness at the scale of the instrument resolution. We have produced a series of user-friendly selection function models and demonstrated their validity and their limitations. Our selection function for bright sources reproduces well the catalog matches with external datasets. (abridged)","sentences":["Characterising galaxy cluster populations from catalog of sources selected in astronomical surveys requires knowledge of sample incompleteness, known as selection function.","The first All-Sky Survey (eRASS1) by eROSITA onboard Spectrum Roentgen Gamma (SRG) has enabled the collection of large samples of galaxy clusters detected in the soft X-ray band over the Western Galactic hemisphere.","The driving goal consists in constraining cosmological parameters, which puts stringent requirements on accuracy, flexibility and explainability of the selection function models.","We use a large set of mock observations of the eRASS1 survey and we process simulated data identically to the real eRASS1 events.","We match detected sources to simulated clusters and we associate detections to intrinsic cluster properties.","We train a series of models to build selection functions depending only on observable surface brightness data.","We develop a second series of models relying on global cluster characteristics such as X-ray luminosity, flux, and expected instrumental count-rate as well as on morphological properties.","We validate our models using our simulations and we rank them according to selected performance metrics.","We validate the models with datasets of clusters detected in X-rays and via the Sunyaev-Zeldovich effect.","We present the complete Bayesian population modelling framework developed for this purpose.","Our results reveal the surface brightness characteristics most relevant to cluster selection in the eRASS1 sample, in particular the ambiguous role of central surface brightness at the scale of the instrument resolution.","We have produced a series of user-friendly selection function models and demonstrated their validity and their limitations.","Our selection function for bright sources reproduces well the catalog matches with external datasets.","(abridged)"],"url":"http://arxiv.org/abs/2402.08457v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:39:38","title":"The First SRG/eROSITA All-Sky Survey: Optical Identification and Properties of Galaxy Clusters and Groups in the Western Galactic Hemisphere","abstract":"The first SRG/eROSITA All-Sky Survey (eRASS1) provides the largest intracluster medium-selected galaxy cluster and group catalog covering the western galactic hemisphere. Compared to samples selected purely on X-ray extent, the sample purity can be enhanced by identifying cluster candidates using optical and near-infrared data from the DESI Legacy Imaging Surveys. Using the red-sequence-based cluster finder eROMaPPer, we measure individual photometric properties (redshift $z_\\lambda$, richness $\\lambda$, optical center, and BCG position) for 12,000 eRASS1 clusters over a sky area of 13,116 deg$^2$, augmented by 247 cases identified by matching the candidates with known clusters from the literature. The median redshift of the identified eRASS1 sample is $z=0.31$, with 10% of the clusters at $z>0.72$. The photometric redshifts have an accuracy of $\\delta z/(1+z)<0.005$ for $0.05<z<0.9$. Spectroscopic cluster properties (redshift $z_{\\rm spec}$ and velocity dispersion $\\sigma$) are measured a posteriori for a subsample of 3,210 and 1,499 eRASS1 clusters, respectively, using an extensive compilation of spectroscopic redshifts of galaxies from the literature. We infer that the primary eRASS1 sample has a purity of 86% and optical completeness >95% for $z>0.05$. For these and further quality assessments of the eRASS1 identified catalog, we apply our identification method to a collection of galaxy cluster catalogs in the literature, as well as blindly on the full Legacy Surveys covering 24,069 deg$^2$. Using a combination of these cluster samples, we investigate the velocity dispersion-richness relation, finding $\\log(\\lambda)=2.401\\times\\log(\\sigma)-5.074$ with an intrinsic scatter of $0.10\\pm0.01$ dex. Our main result is the identified eRASS1 cluster catalog with a high purity and a well-defined X-ray selection process, enabling precise cosmological analyses presented in companion papers.","sentences":["The first SRG/eROSITA All-Sky Survey (eRASS1) provides the largest intracluster medium-selected galaxy cluster and group catalog covering the western galactic hemisphere.","Compared to samples selected purely on X-ray extent, the sample purity can be enhanced by identifying cluster candidates using optical and near-infrared data from the DESI Legacy Imaging Surveys.","Using the red-sequence-based cluster finder eROMaPPer, we measure individual photometric properties (redshift $z_\\lambda$, richness $\\lambda$, optical center, and BCG position) for 12,000 eRASS1 clusters over a sky area of 13,116 deg$^2$, augmented by 247 cases identified by matching the candidates with known clusters from the literature.","The median redshift of the identified eRASS1 sample is $z=0.31$, with 10% of the clusters at $z>0.72$. The photometric redshifts have an accuracy of $\\delta z/(1+z)<0.005$ for $0.05<z<0.9$. Spectroscopic cluster properties (redshift $z_{\\rm spec}$ and velocity dispersion $\\sigma$) are measured a posteriori for a subsample of 3,210 and 1,499 eRASS1 clusters, respectively, using an extensive compilation of spectroscopic redshifts of galaxies from the literature.","We infer that the primary eRASS1 sample has a purity of 86% and optical completeness >95% for $z>0.05$. For these and further quality assessments of the eRASS1 identified catalog, we apply our identification method to a collection of galaxy cluster catalogs in the literature, as well as blindly on the full Legacy Surveys covering 24,069 deg$^2$. Using a combination of these cluster samples, we investigate the velocity dispersion-richness relation, finding $\\log(\\lambda)=2.401\\times\\log(\\sigma)-5.074$ with an intrinsic scatter of $0.10\\pm0.01$ dex.","Our main result is the identified eRASS1 cluster catalog with a high purity and a well-defined X-ray selection process, enabling precise cosmological analyses presented in companion papers."],"url":"http://arxiv.org/abs/2402.08453v1","category":"astro-ph.CO"}
{"created":"2024-02-13 13:36:19","title":"Unveiling interatomic distances influencing the reaction coordinates in alanine dipeptide isomerization: An explainable deep learning approach","abstract":"The present work shows that the free energy landscape associated with alanine dipeptide isomerization can be effectively represented by specific interatomic distances without explicit reference to dihedral angles. Conventionally, two stable states of alanine dipeptide in vacuum, i.e., C$7_{\\mathrm{eq}}$ ($\\beta$-sheet structure) and C$7_{\\mathrm{ax}}$ (left handed $\\alpha$-helix structure), have been primarily characterized using the main chain dihedral angles, $\\varphi$ (C-N-C$_\\alpha$-C) and $\\psi$ (N-C$_\\alpha$-C-N). However, our recent deep learning combined with \"Explainable AI\" (XAI) framework has shown that the transition state can be adequately captured by a free energy landscape using $\\varphi$ and $\\theta$ (O-C-N-C$_\\alpha$) [T. Kikutsuji, et al. J. Chem. Phys. 156, 154108 (2022)]. In perspective of extending these insights to other collective variables, a more detailed characterization of transition state is required. In this work, we employ the interatomic distances and bond angles as input variables for deep learning, rather than the conventional and more elaborate dihedral angles. Our approach utilizes deep learning to investigate whether changes in the main chain dihedral angle can be expressed in terms of interatomic distances and bond angles. Furthermore, by incorporating XAI into our predictive analysis, we quantified the importance of each input variable and succeeded in clarifying the specific interatomic distance that affects the transition state. The results indicate that constructing a free energy landscape based on using the identified interatomic distance can clearly distinguish between the two stable states and provide a comprehensive explanation for the energy barrier crossing.","sentences":["The present work shows that the free energy landscape associated with alanine dipeptide isomerization can be effectively represented by specific interatomic distances without explicit reference to dihedral angles.","Conventionally, two stable states of alanine dipeptide in vacuum, i.e., C$7_{\\mathrm{eq}}$ ($\\beta$-sheet structure) and C$7_{\\mathrm{ax}}$ (left handed $\\alpha$-helix structure), have been primarily characterized using the main chain dihedral angles, $\\varphi$ (C-N-C$_\\alpha$-C) and $\\psi$ (N-C$_\\alpha$-C-N).","However, our recent deep learning combined with \"Explainable AI\" (XAI) framework has shown that the transition state can be adequately captured by a free energy landscape using $\\varphi$ and $\\theta$ (O-C-N-C$_\\alpha$)","[T. Kikutsuji, et al. J. Chem.","Phys. 156, 154108 (2022)].","In perspective of extending these insights to other collective variables, a more detailed characterization of transition state is required.","In this work, we employ the interatomic distances and bond angles as input variables for deep learning, rather than the conventional and more elaborate dihedral angles.","Our approach utilizes deep learning to investigate whether changes in the main chain dihedral angle can be expressed in terms of interatomic distances and bond angles.","Furthermore, by incorporating XAI into our predictive analysis, we quantified the importance of each input variable and succeeded in clarifying the specific interatomic distance that affects the transition state.","The results indicate that constructing a free energy landscape based on using the identified interatomic distance can clearly distinguish between the two stable states and provide a comprehensive explanation for the energy barrier crossing."],"url":"http://arxiv.org/abs/2402.08448v1","category":"physics.chem-ph"}
{"created":"2024-02-13 13:30:34","title":"$1$-Bit SubTHz RIS with Planar Tightly Coupled Dipoles: Beam Shaping and Prototypes","abstract":"In this paper, a proof-of-concept study of a $1$-bit wideband reconfigurable intelligent surface (RIS) comprising planar tightly coupled dipoles (PTCD) is presented. The developed RIS operates at subTHz frequencies and a $3$-dB gain bandwidth of $27.4\\%$ with the center frequency at $102$ GHz is shown to be obtainable via full-wave electromagnetic simulations. The binary phase shift offered by each RIS unit element is enabled by changing the polarization of the reflected wave by $180^\\circ$. The proposed PTCD-based RIS has a planar configuration with one dielectric layer bonded to a ground plane, and hence, it can be fabricated by using cost-effective printed circuit board (PCB) technology. We analytically calculate the response of the entire designed RIS and showcase that a good agreement between that result and equivalent full-wave simulations is obtained. To efficiently compute the $1$-bit RIS response for different pointing directions, thus, designing a directive beam codebook, we devise a fast approximate beamforming optimization approach, which is compared with time-consuming full-wave simulations. Finally, to prove our concept, we present several passive prototypes with frozen beams for the proposed $1$-bit wideband RIS.","sentences":["In this paper, a proof-of-concept study of a $1$-bit wideband reconfigurable intelligent surface (RIS) comprising planar tightly coupled dipoles (PTCD) is presented.","The developed RIS operates at subTHz frequencies and a $3$-dB gain bandwidth of $27.4\\%$ with the center frequency at $102$ GHz is shown to be obtainable via full-wave electromagnetic simulations.","The binary phase shift offered by each RIS unit element is enabled by changing the polarization of the reflected wave by $180^\\circ$. The proposed PTCD-based RIS has a planar configuration with one dielectric layer bonded to a ground plane, and hence, it can be fabricated by using cost-effective printed circuit board (PCB) technology.","We analytically calculate the response of the entire designed RIS and showcase that a good agreement between that result and equivalent full-wave simulations is obtained.","To efficiently compute the $1$-bit RIS response for different pointing directions, thus, designing a directive beam codebook, we devise a fast approximate beamforming optimization approach, which is compared with time-consuming full-wave simulations.","Finally, to prove our concept, we present several passive prototypes with frozen beams for the proposed $1$-bit wideband RIS."],"url":"http://arxiv.org/abs/2402.08445v1","category":"eess.SP"}
{"created":"2024-02-13 12:58:53","title":"Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot","abstract":"Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering. In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result.","sentences":["Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers.","For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets.","Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code.","The task of designing good prompts is known as prompt engineering.","In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code.","We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods.","Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result."],"url":"http://arxiv.org/abs/2402.08430v1","category":"cs.SE"}
{"created":"2024-02-13 12:50:04","title":"Vehicle Behavior Prediction by Episodic-Memory Implanted NDT","abstract":"In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability. The code is available at https://github.com/JWFangit/eMem-NDT.","sentences":["In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents.","Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use.","In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev.","eMem-NDT).","The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions.","eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree.","Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes.","By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances).","We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability.","The code is available at https://github.com/JWFangit/eMem-NDT."],"url":"http://arxiv.org/abs/2402.08423v1","category":"cs.AI"}
{"created":"2024-02-13 12:49:22","title":"Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins","abstract":"Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme to the centralized training decentralized execution framework, allowing joint training of the agents' policies. The proposed MARL scheme, referred to as multi-agent conservative quantile regression (MA-CQR) addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages.","sentences":["Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks.","An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment.","This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment.","A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data.","In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data.","To further exploit the offline data, we adapt the proposed scheme to the centralized training decentralized execution framework, allowing joint training of the agents' policies.","The proposed MARL scheme, referred to as multi-agent conservative quantile regression (MA-CQR) addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages."],"url":"http://arxiv.org/abs/2402.08421v1","category":"cs.LG"}
{"created":"2024-02-13 12:49:13","title":"Vision-Based Hand Gesture Customization from a Single Demonstration","abstract":"Hand gesture recognition is becoming a more prevalent mode of human-computer interaction, especially as cameras proliferate across everyday devices. Despite continued progress in this field, gesture customization is often underexplored. Customization is crucial since it enables users to define and demonstrate gestures that are more natural, memorable, and accessible. However, customization requires efficient usage of user-provided data. We introduce a method that enables users to easily design bespoke gestures with a monocular camera from one demonstration. We employ transformers and meta-learning techniques to address few-shot learning challenges. Unlike prior work, our method supports any combination of one-handed, two-handed, static, and dynamic gestures, including different viewpoints. We evaluated our customization method through a user study with 20 gestures collected from 21 participants, achieving up to 97% average recognition accuracy from one demonstration. Our work provides a viable path for vision-based gesture customization, laying the foundation for future advancements in this domain.","sentences":["Hand gesture recognition is becoming a more prevalent mode of human-computer interaction, especially as cameras proliferate across everyday devices.","Despite continued progress in this field, gesture customization is often underexplored.","Customization is crucial since it enables users to define and demonstrate gestures that are more natural, memorable, and accessible.","However, customization requires efficient usage of user-provided data.","We introduce a method that enables users to easily design bespoke gestures with a monocular camera from one demonstration.","We employ transformers and meta-learning techniques to address few-shot learning challenges.","Unlike prior work, our method supports any combination of one-handed, two-handed, static, and dynamic gestures, including different viewpoints.","We evaluated our customization method through a user study with 20 gestures collected from 21 participants, achieving up to 97% average recognition accuracy from one demonstration.","Our work provides a viable path for vision-based gesture customization, laying the foundation for future advancements in this domain."],"url":"http://arxiv.org/abs/2402.08420v1","category":"cs.HC"}
{"created":"2024-02-13 12:21:06","title":"Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging","abstract":"Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast. However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI. This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment. Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features. Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on LF features. Beyond such advantages, this strategy can seamlessly be utilized by modulating the contrast of LF features in alignment with UHF guidance, even when employing arbitrary segmentation models. Exhaustive experiments demonstrated that the proposed method significantly outperformed all baseline models on both brain tissue and whole-brain segmentation tasks; further, it exhibited remarkable adaptability and scalability by successfully integrating diverse segmentation models and tasks. These improvements were not only quantifiable but also visible in the superlative visual quality of segmentation masks.","sentences":["Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast.","However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI.","This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment.","Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features.","Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on LF features.","Beyond such advantages, this strategy can seamlessly be utilized by modulating the contrast of LF features in alignment with UHF guidance, even when employing arbitrary segmentation models.","Exhaustive experiments demonstrated that the proposed method significantly outperformed all baseline models on both brain tissue and whole-brain segmentation tasks; further, it exhibited remarkable adaptability and scalability by successfully integrating diverse segmentation models and tasks.","These improvements were not only quantifiable but also visible in the superlative visual quality of segmentation masks."],"url":"http://arxiv.org/abs/2402.08409v1","category":"cs.CV"}
{"created":"2024-02-13 12:04:43","title":"LLMs and the Human Condition","abstract":"This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship with our world.","sentences":["This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action.","Taking seriously the idea of language as action the model is then applied to the conversational user interfaces.","Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what LLMs are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up.","When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense.","By understanding where ChatGPT's apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship with our world."],"url":"http://arxiv.org/abs/2402.08403v1","category":"cs.CL"}
{"created":"2024-02-13 12:02:37","title":"LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection","abstract":"In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives. Machine learning and deep learning approaches have been extensively employed for identifying fake news. However, a significant challenge in identifying fake news is the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algorithm, utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize news into two groups: interest (fake) and non-interest (real). Subsequently, we enhance the graph structure using structural augmentation techniques. Ultimately, we predict the final labels for all unlabeled data using a GNN that induces randomness within the local neighborhood of nodes through the aggregation function. We evaluate our proposed method on five common datasets and compare the results against a set of baseline models, including both OCL and binary labeled models. The results demonstrate that LOSS-GAT achieves a notable improvement, surpassing 10%, with the advantage of utilizing only a limited set of labeled fake news. Noteworthy, LOSS-GAT even outperforms binary labeled models.","sentences":["In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives.","Machine learning and deep learning approaches have been extensively employed for identifying fake news.","However, a significant challenge in identifying fake news is the limited availability of labeled news datasets.","Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge.","On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels.","In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT.","Initially, we employ a two-step label propagation algorithm, utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize news into two groups: interest (fake) and non-interest (real).","Subsequently, we enhance the graph structure using structural augmentation techniques.","Ultimately, we predict the final labels for all unlabeled data using a GNN that induces randomness within the local neighborhood of nodes through the aggregation function.","We evaluate our proposed method on five common datasets and compare the results against a set of baseline models, including both OCL and binary labeled models.","The results demonstrate that LOSS-GAT achieves a notable improvement, surpassing 10%, with the advantage of utilizing only a limited set of labeled fake news.","Noteworthy, LOSS-GAT even outperforms binary labeled models."],"url":"http://arxiv.org/abs/2402.08401v1","category":"cs.LG"}
{"created":"2024-02-13 11:25:20","title":"Selective Learning: Towards Robust Calibration with Dynamic Regularization","abstract":"Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At a high level, DReg aims to obtain a more reliable model capable of acknowledging what it knows and does not know. Specifically, DReg effectively fits the labels for in-distribution samples (samples that should be learned) while applying regularization dynamically to samples beyond model capabilities (e.g., outliers), thereby obtaining a robust calibrated model especially on the samples beyond model capabilities. Both theoretical and empirical analyses sufficiently demonstrate the superiority of DReg compared with previous methods.","sentences":["Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance.","This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing.","Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function.","The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence.","However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence).","Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off.","At a high level, DReg aims to obtain a more reliable model capable of acknowledging what it knows and does not know.","Specifically, DReg effectively fits the labels for in-distribution samples (samples that should be learned) while applying regularization dynamically to samples beyond model capabilities (e.g., outliers), thereby obtaining a robust calibrated model especially on the samples beyond model capabilities.","Both theoretical and empirical analyses sufficiently demonstrate the superiority of DReg compared with previous methods."],"url":"http://arxiv.org/abs/2402.08384v1","category":"cs.LG"}
