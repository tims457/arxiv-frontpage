{"created":"2024-05-08 17:59:11","title":"THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models","abstract":"Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \"Type I hallucinations\". Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term \"Type II hallucinations\". Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.","sentences":["Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem.","Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \"Type I hallucinations\".","Instead, they focus on hallucinations responding to very specific question formats -- typically a multiple-choice response regarding a particular object or attribute -- which we term \"Type II hallucinations\".","Additionally, such benchmarks often require external API calls to models which are subject to change.","In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of hallucinations are often anti-correlated.","To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs.","We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics.","By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type","I hallucinations are incomplete.","Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline."],"url":"http://arxiv.org/abs/2405.05256v1","category":"cs.CV"}
{"created":"2024-05-08 17:57:39","title":"Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge","abstract":"Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.","sentences":["Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts.","However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models.","This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied.","This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning.","Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course.","First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert.","We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator.","Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback.","We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings."],"url":"http://arxiv.org/abs/2405.05253v1","category":"cs.CL"}
{"created":"2024-05-08 17:56:47","title":"Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models","abstract":"Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images. However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models. Existing works mainly adopt a retraining process to enhance DM efficiency. This is computationally expensive and not very scalable. To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining. Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation. In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality. Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model. Project webpage: https://atedm.github.io.","sentences":["Diffusion Models (DMs) have exhibited superior performance in generating high-quality and diverse images.","However, this exceptional performance comes at the cost of expensive architectural design, particularly due to the attention module heavily used in leading models.","Existing works mainly adopt a retraining process to enhance DM efficiency.","This is computationally expensive and not very scalable.","To this end, we introduce the Attention-driven Training-free Efficient Diffusion Model (AT-EDM) framework that leverages attention maps to perform run-time pruning of redundant tokens, without the need for any retraining.","Specifically, for single-denoising-step pruning, we develop a novel ranking algorithm, Generalized Weighted Page Rank (G-WPR), to identify redundant tokens, and a similarity-based recovery method to restore tokens for the convolution operation.","In addition, we propose a Denoising-Steps-Aware Pruning (DSAP) approach to adjust the pruning budget across different denoising timesteps for better generation quality.","Extensive evaluations show that AT-EDM performs favorably against prior art in terms of efficiency (e.g., 38.8% FLOPs saving and up to 1.53x speed-up over Stable Diffusion XL) while maintaining nearly the same FID and CLIP scores as the full model.","Project webpage: https://atedm.github.io."],"url":"http://arxiv.org/abs/2405.05252v1","category":"cs.CV"}
{"created":"2024-05-08 17:51:53","title":"LLMs with Personalities in Multi-issue Negotiation Games","abstract":"Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be \"jail broken\" to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.","sentences":["Powered by large language models (LLMs), AI agents have become capable of many human tasks.","Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk.","Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation.","Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies.","Low conscientiousness is associated with high toxicity.","These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be \"jail broken\" to exploit agreeable opponents.","We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science."],"url":"http://arxiv.org/abs/2405.05248v2","category":"cs.CL"}
{"created":"2024-05-08 17:40:12","title":"SVDD Challenge 2024: A Singing Voice Deepfake Detection Challenge Evaluation Plan","abstract":"The rapid advancement of AI-generated singing voices, which now closely mimic natural human singing and align seamlessly with musical scores, has led to heightened concerns for artists and the music industry. Unlike spoken voice, singing voice presents unique challenges due to its musical nature and the presence of strong background music, making singing voice deepfake detection (SVDD) a specialized field requiring focused attention. To promote SVDD research, we recently proposed the \"SVDD Challenge,\" the very first research challenge focusing on SVDD for lab-controlled and in-the-wild bonafide and deepfake singing voice recordings. The challenge will be held in conjunction with the 2024 IEEE Spoken Language Technology Workshop (SLT 2024).","sentences":["The rapid advancement of AI-generated singing voices, which now closely mimic natural human singing and align seamlessly with musical scores, has led to heightened concerns for artists and the music industry.","Unlike spoken voice, singing voice presents unique challenges due to its musical nature and the presence of strong background music, making singing voice deepfake detection (SVDD) a specialized field requiring focused attention.","To promote SVDD research, we recently proposed the \"SVDD Challenge,\" the very first research challenge focusing on SVDD for lab-controlled and in-the-wild bonafide and deepfake singing voice recordings.","The challenge will be held in conjunction with the 2024 IEEE Spoken Language Technology Workshop (SLT 2024)."],"url":"http://arxiv.org/abs/2405.05244v1","category":"eess.AS"}
{"created":"2024-05-08 17:40:03","title":"Deep learning-based variational autoencoder for classification of quantum and classical states of light","abstract":"Advancements in optical quantum technologies have been enabled by the generation, manipulation, and characterization of light, with identification based on its photon statistics. However, characterizing light and its sources through single photon measurements often requires efficient detectors and longer measurement times to obtain high-quality photon statistics. Here we introduce a deep learning-based variational autoencoder (VAE) method for classifying single photon added coherent state (SPACS), single photon added thermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of light. Our semisupervised learning-based VAE efficiently maps the photon statistics features of light to a lower dimension, enabling quasi-instantaneous classification with low average photon counts. The proposed VAE method is robust and maintains classification accuracy in the presence of losses inherent in an experiment, such as finite collection efficiency, non-unity quantum efficiency, finite number of detectors, etc. Additionally, leveraging the transfer learning capabilities of VAE enables successful classification of data of any quality using a single trained model. We envision that such a deep learning methodology will enable better classification of quantum light and light sources even in the presence of poor detection quality.","sentences":["Advancements in optical quantum technologies have been enabled by the generation, manipulation, and characterization of light, with identification based on its photon statistics.","However, characterizing light and its sources through single photon measurements often requires efficient detectors and longer measurement times to obtain high-quality photon statistics.","Here we introduce a deep learning-based variational autoencoder (VAE) method for classifying single photon added coherent state (SPACS), single photon added thermal state (SPACS), mixed states between coherent/SPACS and thermal/SPATS of light.","Our semisupervised learning-based VAE efficiently maps the photon statistics features of light to a lower dimension, enabling quasi-instantaneous classification with low average photon counts.","The proposed VAE method is robust and maintains classification accuracy in the presence of losses inherent in an experiment, such as finite collection efficiency, non-unity quantum efficiency, finite number of detectors, etc.","Additionally, leveraging the transfer learning capabilities of VAE enables successful classification of data of any quality using a single trained model.","We envision that such a deep learning methodology will enable better classification of quantum light and light sources even in the presence of poor detection quality."],"url":"http://arxiv.org/abs/2405.05243v1","category":"quant-ph"}
{"created":"2024-05-08 17:37:57","title":"BenthicNet: A global compilation of seafloor images for deep learning applications","abstract":"Advances in underwater imaging enable the collection of extensive seafloor image datasets that are necessary for monitoring important benthic ecosystems. The ability to collect seafloor imagery has outpaced our capacity to analyze it, hindering expedient mobilization of this crucial environmental information. Recent machine learning approaches provide opportunities to increase the efficiency with which seafloor image datasets are analyzed, yet large and consistent datasets necessary to support development of such approaches are scarce. Here we present BenthicNet: a global compilation of seafloor imagery designed to support the training and evaluation of large-scale image recognition models. An initial set of over 11.4 million images was collected and curated to represent a diversity of seafloor environments using a representative subset of 1.3 million images. These are accompanied by 2.6 million annotations translated to the CATAMI scheme, which span 190,000 of the images. A large deep learning model was trained on this compilation and preliminary results suggest it has utility for automating large and small-scale image analysis tasks. The compilation and model are made openly available for use by the scientific community at https://doi.org/10.20383/103.0614.","sentences":["Advances in underwater imaging enable the collection of extensive seafloor image datasets that are necessary for monitoring important benthic ecosystems.","The ability to collect seafloor imagery has outpaced our capacity to analyze it, hindering expedient mobilization of this crucial environmental information.","Recent machine learning approaches provide opportunities to increase the efficiency with which seafloor image datasets are analyzed, yet large and consistent datasets necessary to support development of such approaches are scarce.","Here we present BenthicNet: a global compilation of seafloor imagery designed to support the training and evaluation of large-scale image recognition models.","An initial set of over 11.4 million images was collected and curated to represent a diversity of seafloor environments using a representative subset of 1.3 million images.","These are accompanied by 2.6 million annotations translated to the CATAMI scheme, which span 190,000 of the images.","A large deep learning model was trained on this compilation and preliminary results suggest it has utility for automating large and small-scale image analysis tasks.","The compilation and model are made openly available for use by the scientific community at https://doi.org/10.20383/103.0614."],"url":"http://arxiv.org/abs/2405.05241v1","category":"cs.CV"}
{"created":"2024-05-08 17:33:42","title":"EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning","abstract":"The diagnosis and treatment of chest diseases play a crucial role in maintaining human health. X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness. Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination. Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks. EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation. Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field. Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning. The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice. Our codes and models are available at: https://github.com/hustvl/EVA-X.","sentences":["The diagnosis and treatment of chest diseases play a crucial role in maintaining human health.","X-ray examination has become the most common clinical examination means due to its efficiency and cost-effectiveness.","Artificial intelligence analysis methods for chest X-ray images are limited by insufficient annotation data and varying levels of annotation, resulting in weak generalization ability and difficulty in clinical dissemination.","Here we present EVA-X, an innovative foundational model based on X-ray images with broad applicability to various chest disease detection tasks.","EVA-X is the first X-ray image based self-supervised learning method capable of capturing both semantic and geometric information from unlabeled images for universal X-ray image representation.","Through extensive experimentation, EVA-X has demonstrated exceptional performance in chest disease analysis and localization, becoming the first model capable of spanning over 20 different chest diseases and achieving leading results in over 11 different detection tasks in the medical field.","Additionally, EVA-X significantly reduces the burden of data annotation in the medical AI field, showcasing strong potential in the domain of few-shot learning.","The emergence of EVA-X will greatly propel the development and application of foundational medical models, bringing about revolutionary changes in future medical research and clinical practice.","Our codes and models are available at: https://github.com/hustvl/EVA-X."],"url":"http://arxiv.org/abs/2405.05237v1","category":"cs.CV"}
{"created":"2024-05-08 17:28:07","title":"RACH Traffic Prediction in Massive Machine Type Communications","abstract":"Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks. However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments. Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks. This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks. The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns. Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network. Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios. We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics. Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system.","sentences":["Traffic pattern prediction has emerged as a promising approach for efficiently managing and mitigating the impacts of event-driven bursty traffic in massive machine-type communication (mMTC) networks.","However, achieving accurate predictions of bursty traffic remains a non-trivial task due to the inherent randomness of events, and these challenges intensify within live network environments.","Consequently, there is a compelling imperative to design a lightweight and agile framework capable of assimilating continuously collected data from the network and accurately forecasting bursty traffic in mMTC networks.","This paper addresses these challenges by presenting a machine learning-based framework tailored for forecasting bursty traffic in multi-channel slotted ALOHA networks.","The proposed machine learning network comprises long-term short-term memory (LSTM) and a DenseNet with feed-forward neural network (FFNN) layers, where the residual connections enhance the training ability of the machine learning network in capturing complicated patterns.","Furthermore, we develop a new low-complexity online prediction algorithm that updates the states of the LSTM network by leveraging frequently collected data from the mMTC network.","Simulation results and complexity analysis demonstrate the superiority of our proposed algorithm in terms of both accuracy and complexity, making it well-suited for time-critical live scenarios.","We evaluate the performance of the proposed framework in a network with a single base station and thousands of devices organized into groups with distinct traffic-generating characteristics.","Comprehensive evaluations and simulations indicate that our proposed machine learning approach achieves a remarkable $52\\%$ higher accuracy in long-term predictions compared to traditional methods, without imposing additional processing load on the system."],"url":"http://arxiv.org/abs/2405.05235v1","category":"eess.SY"}
{"created":"2024-05-08 17:27:16","title":"How a table modulates the risk of airborne transmission between facing individuals","abstract":"Airborne transmission has been recognized as an important route of transmission for SARS-CoV-2, the virus responsible for the COVID-19 pandemic. While coughing and sneezing are spectacular sources of production of infected aerosols with far-reaching airflows, the prevalence of asymptomatic transmissions highlighted the importance of social activities. Gathering around a table, a common scenario of human interactions, not only fixes a typical distance between static interlocutors, but influences airborne transmission, by serving both as a flow diverter and a surface for droplet deposition. Here, we use high-fidelity large-eddy simulations to characterize short-range airborne transmission when two people face each other at a typical table. We show that compared to the natural distance travelled by free buoyant puffs and jets, the distance between the table and the emission constitutes a new length scale that modifies downward exhaled flows, common during nose breathing, speech, and laughter. When the table is close to the emitter, its main effect is to restrict the forward spread of emitted particles. However, if the distance between individuals is too short, particles reaching the recipient become more concentrated, rising transmission risks. Additionally, simulations of forceful exhalations, like laughter, demonstrate that the table acts as a filter that collects medium-sized particles that would have remained in the free jet otherwise, but can in that case be involved in the fomite transmission route. The table introduces a cut-off size for particles that depends on the inertia of the exhaled material, thereby modifying the size distribution of particles suspended in the air.","sentences":["Airborne transmission has been recognized as an important route of transmission for SARS-CoV-2, the virus responsible for the COVID-19 pandemic.","While coughing and sneezing are spectacular sources of production of infected aerosols with far-reaching airflows, the prevalence of asymptomatic transmissions highlighted the importance of social activities.","Gathering around a table, a common scenario of human interactions, not only fixes a typical distance between static interlocutors, but influences airborne transmission, by serving both as a flow diverter and a surface for droplet deposition.","Here, we use high-fidelity large-eddy simulations to characterize short-range airborne transmission when two people face each other at a typical table.","We show that compared to the natural distance travelled by free buoyant puffs and jets, the distance between the table and the emission constitutes a new length scale that modifies downward exhaled flows, common during nose breathing, speech, and laughter.","When the table is close to the emitter, its main effect is to restrict the forward spread of emitted particles.","However, if the distance between individuals is too short, particles reaching the recipient become more concentrated, rising transmission risks.","Additionally, simulations of forceful exhalations, like laughter, demonstrate that the table acts as a filter that collects medium-sized particles that would have remained in the free jet otherwise, but can in that case be involved in the fomite transmission route.","The table introduces a cut-off size for particles that depends on the inertia of the exhaled material, thereby modifying the size distribution of particles suspended in the air."],"url":"http://arxiv.org/abs/2405.05232v1","category":"physics.flu-dyn"}
{"created":"2024-05-08 17:24:24","title":"myAURA: Personalized health library for epilepsy management via knowledge graph sparsification and visualization","abstract":"Objective: We report the development of the patient-centered myAURA application and suite of methods designed to aid epilepsy patients, caregivers, and researchers in making decisions about care and self-management.   Materials and Methods: myAURA rests on the federation of an unprecedented collection of heterogeneous data resources relevant to epilepsy, such as biomedical databases, social media, and electronic health records. A generalizable, open-source methodology was developed to compute a multi-layer knowledge graph linking all this heterogeneous data via the terms of a human-centered biomedical dictionary.   Results: The power of the approach is first exemplified in the study of the drug-drug interaction phenomenon. Furthermore, we employ a novel network sparsification methodology using the metric backbone of weighted graphs, which reveals the most important edges for inference, recommendation, and visualization, such as pharmacology factors patients discuss on social media. The network sparsification approach also allows us to extract focused digital cohorts from social media whose discourse is more relevant to epilepsy or other biomedical problems. Finally, we present our patient-centered design and pilot-testing of myAURA, including its user interface, based on focus groups and other stakeholder input.   Discussion: The ability to search and explore myAURA's heterogeneous data sources via a sparsified multi-layer knowledge graph, as well as the combination of those layers in a single map, are useful features for integrating relevant information for epilepsy.   Conclusion: Our stakeholder-driven, scalable approach to integrate traditional and non-traditional data sources, enables biomedical discovery and data-powered patient self-management in epilepsy, and is generalizable to other chronic conditions.","sentences":["Objective: We report the development of the patient-centered myAURA application and suite of methods designed to aid epilepsy patients, caregivers, and researchers in making decisions about care and self-management.   ","Materials and Methods: myAURA rests on the federation of an unprecedented collection of heterogeneous data resources relevant to epilepsy, such as biomedical databases, social media, and electronic health records.","A generalizable, open-source methodology was developed to compute a multi-layer knowledge graph linking all this heterogeneous data via the terms of a human-centered biomedical dictionary.   ","Results:","The power of the approach is first exemplified in the study of the drug-drug interaction phenomenon.","Furthermore, we employ a novel network sparsification methodology using the metric backbone of weighted graphs, which reveals the most important edges for inference, recommendation, and visualization, such as pharmacology factors patients discuss on social media.","The network sparsification approach also allows us to extract focused digital cohorts from social media whose discourse is more relevant to epilepsy or other biomedical problems.","Finally, we present our patient-centered design and pilot-testing of myAURA, including its user interface, based on focus groups and other stakeholder input.   ","Discussion: The ability to search and explore myAURA's heterogeneous data sources via a sparsified multi-layer knowledge graph, as well as the combination of those layers in a single map, are useful features for integrating relevant information for epilepsy.   ","Conclusion: Our stakeholder-driven, scalable approach to integrate traditional and non-traditional data sources, enables biomedical discovery and data-powered patient self-management in epilepsy, and is generalizable to other chronic conditions."],"url":"http://arxiv.org/abs/2405.05229v1","category":"cs.IR"}
{"created":"2024-05-08 17:11:38","title":"Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers","abstract":"Large Language Models (LLMs) have profoundly changed the world. Their self-attention mechanism is the key to the success of transformers in LLMs. However, the quadratic computational cost $O(n^2)$ to the length $n$ input sequence is the notorious obstacle for further improvement and scalability in the longer context. In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices. We propose a $\\mathsf{conv}$ basis system, \"similar\" to the rank basis, and show that any lower triangular (attention) matrix can always be decomposed as a sum of $k$ structured convolution matrices in this basis system. We then design an algorithm to quickly decompose the attention matrix into $k$ convolution matrices. Thanks to Fast Fourier Transforms (FFT), the attention {\\it inference} can be computed in $O(knd \\log n)$ time, where $d$ is the hidden dimension. In practice, we have $ d \\ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma. Thus, when $kd = n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. Our approach can avoid explicitly computing the $n \\times n$ attention matrix, which may largely alleviate the quadratic computational complexity. Furthermore, our algorithm works on any input matrices. This work provides a new paradigm for accelerating attention computation in transformers to enable their application to longer contexts.","sentences":["Large Language Models (LLMs) have profoundly changed the world.","Their self-attention mechanism is the key to the success of transformers in LLMs.","However, the quadratic computational cost $O(n^2)$ to the length $n$ input sequence is the notorious obstacle for further improvement and scalability in the longer context.","In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices.","We propose a $\\mathsf{conv}$ basis system, \"similar\" to the rank basis, and show that any lower triangular (attention) matrix can always be decomposed as a sum of $k$ structured convolution matrices in this basis system.","We then design an algorithm to quickly decompose the attention matrix into $k$ convolution matrices.","Thanks to Fast Fourier Transforms (FFT), the attention {\\it inference} can be computed in $O(knd \\log n)$ time, where $d$ is the hidden dimension.","In practice, we have $ d \\ll n$, i.e., $d=3,072$ and $n=1,000,000$ for Gemma.","Thus, when $kd = n^{o(1)}$, our algorithm achieve almost linear time, i.e., $n^{1+o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well.","Our approach can avoid explicitly computing the $n \\times n$ attention matrix, which may largely alleviate the quadratic computational complexity.","Furthermore, our algorithm works on any input matrices.","This work provides a new paradigm for accelerating attention computation in transformers to enable their application to longer contexts."],"url":"http://arxiv.org/abs/2405.05219v1","category":"cs.LG"}
{"created":"2024-05-08 16:43:25","title":"Hybrid Quantum Graph Neural Network for Molecular Property Prediction","abstract":"To accelerate the process of materials design, materials science has increasingly used data driven techniques to extract information from collected data. Specially, machine learning (ML) algorithms, which span the ML discipline, have demonstrated ability to predict various properties of materials with the level of accuracy similar to explicit calculation of quantum mechanical theories, but with significantly reduced run time and computational resources. Within ML, graph neural networks have emerged as an important algorithm within the field of machine learning, since they are capable of predicting accurately a wide range of important physical, chemical and electronic properties due to their higher learning ability based on the graph representation of material and molecular descriptors through the aggregation of information embedded within the graph. In parallel with the development of state of the art classical machine learning applications, the fusion of quantum computing and machine learning have created a new paradigm where classical machine learning model can be augmented with quantum layers which are able to encode high dimensional data more efficiently. Leveraging the structure of existing algorithms, we developed a unique and novel gradient free hybrid quantum classical convoluted graph neural network (HyQCGNN) to predict formation energies of perovskite materials. The performance of our hybrid statistical model is competitive with the results obtained purely from a classical convoluted graph neural network, and other classical machine learning algorithms, such as XGBoost. Consequently, our study suggests a new pathway to explore how quantum feature encoding and parametric quantum circuits can yield drastic improvements of complex ML algorithm like graph neural network.","sentences":["To accelerate the process of materials design, materials science has increasingly used data driven techniques to extract information from collected data.","Specially, machine learning (ML) algorithms, which span the ML discipline, have demonstrated ability to predict various properties of materials with the level of accuracy similar to explicit calculation of quantum mechanical theories, but with significantly reduced run time and computational resources.","Within ML, graph neural networks have emerged as an important algorithm within the field of machine learning, since they are capable of predicting accurately a wide range of important physical, chemical and electronic properties due to their higher learning ability based on the graph representation of material and molecular descriptors through the aggregation of information embedded within the graph.","In parallel with the development of state of the art classical machine learning applications, the fusion of quantum computing and machine learning have created a new paradigm where classical machine learning model can be augmented with quantum layers which are able to encode high dimensional data more efficiently.","Leveraging the structure of existing algorithms, we developed a unique and novel gradient free hybrid quantum classical convoluted graph neural network (HyQCGNN) to predict formation energies of perovskite materials.","The performance of our hybrid statistical model is competitive with the results obtained purely from a classical convoluted graph neural network, and other classical machine learning algorithms, such as XGBoost.","Consequently, our study suggests a new pathway to explore how quantum feature encoding and parametric quantum circuits can yield drastic improvements of complex ML algorithm like graph neural network."],"url":"http://arxiv.org/abs/2405.05205v1","category":"quant-ph"}
{"created":"2024-05-08 16:40:15","title":"A multiple coupon collection process and its Markov embedding structure","abstract":"The embedding problem of Markov transition matrices into Markov semigroups is a classic problem that regained a lot of impetus and activities in recent years. We consider it here for the following generalisation of the well-known coupon collection process: from a finite set of distinct objects, a subset is drawn repeatedly according to some probability distribution, independently and with replacement, and each time united with the set of objects sampled so far. We derive and interpret properties and explicit conditions for the resulting discrete-time Markov chain to be representable within a semigroup or a flow of a continuous-time process of the same type.","sentences":["The embedding problem of Markov transition matrices into Markov semigroups is a classic problem that regained a lot of impetus and activities in recent years.","We consider it here for the following generalisation of the well-known coupon collection process: from a finite set of distinct objects, a subset is drawn repeatedly according to some probability distribution, independently and with replacement, and each time united with the set of objects sampled so far.","We derive and interpret properties and explicit conditions for the resulting discrete-time Markov chain to be representable within a semigroup or a flow of a continuous-time process of the same type."],"url":"http://arxiv.org/abs/2405.05203v1","category":"math.PR"}
{"created":"2024-05-08 16:25:42","title":"MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning","abstract":"We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.","sentences":["We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs).","Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability.","Additionally, relying solely on a single sample may result in the omission of true nodes and edges.","To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer.","To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM.","This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision.","Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts."],"url":"http://arxiv.org/abs/2405.05189v1","category":"cs.CL"}
{"created":"2024-05-08 16:10:46","title":"A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective","abstract":"3D occupancy perception technology aims to observe and understand dense 3D environments for autonomous vehicles. Owing to its comprehensive perception capability, this technology is emerging as a trend in autonomous driving perception systems, and is attracting significant attention from both industry and academia. Similar to traditional bird's-eye view (BEV) perception, 3D occupancy perception has the nature of multi-source input and the necessity for information fusion. However, the difference is that it captures vertical structures that are ignored by 2D BEV. In this survey, we review the most recent works on 3D occupancy perception, and provide in-depth analyses of methodologies with various input modalities. Specifically, we summarize general network pipelines, highlight information fusion techniques, and discuss effective network training. We evaluate and analyze the occupancy perception performance of the state-of-the-art on the most popular datasets. Furthermore, challenges and future research directions are discussed. We hope this report will inspire the community and encourage more research work on 3D occupancy perception. A comprehensive list of studies in this survey is available in an active repository that continuously collects the latest work: https://github.com/HuaiyuanXu/3D-Occupancy-Perception.","sentences":["3D occupancy perception technology aims to observe and understand dense 3D environments for autonomous vehicles.","Owing to its comprehensive perception capability, this technology is emerging as a trend in autonomous driving perception systems, and is attracting significant attention from both industry and academia.","Similar to traditional bird's-eye view (BEV) perception, 3D occupancy perception has the nature of multi-source input and the necessity for information fusion.","However, the difference is that it captures vertical structures that are ignored by 2D BEV.","In this survey, we review the most recent works on 3D occupancy perception, and provide in-depth analyses of methodologies with various input modalities.","Specifically, we summarize general network pipelines, highlight information fusion techniques, and discuss effective network training.","We evaluate and analyze the occupancy perception performance of the state-of-the-art on the most popular datasets.","Furthermore, challenges and future research directions are discussed.","We hope this report will inspire the community and encourage more research work on 3D occupancy perception.","A comprehensive list of studies in this survey is available in an active repository that continuously collects the latest work: https://github.com/HuaiyuanXu/3D-Occupancy-Perception."],"url":"http://arxiv.org/abs/2405.05173v1","category":"cs.CV"}
{"created":"2024-05-08 15:54:57","title":"ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with Probability Map Guided Multi-Format Feature Fusion","abstract":"Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks. However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied. This has been a long-standing hindrance to the improvement of pose estimation accuracy. To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method. ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body. Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %. The emphasis of our study is focusing on the position information that is not exploited before in radar singal. This provides direction to investigate other potential non-redundant information from mmWave rader.","sentences":["Millimetre wave (mmWave) radar is a non-intrusive privacy and relatively convenient and inexpensive device, which has been demonstrated to be applicable in place of RGB cameras in human indoor pose estimation tasks.","However, mmWave radar relies on the collection of reflected signals from the target, and the radar signals containing information is difficult to be fully applied.","This has been a long-standing hindrance to the improvement of pose estimation accuracy.","To address this major challenge, this paper introduces a probability map guided multi-format feature fusion model,","ProbRadarM3F.","This is a novel radar feature extraction framework using a traditional FFT method in parallel with a probability map based positional encoding method.","ProbRadarM3F fuses the traditional heatmap features and the positional features, then effectively achieves the estimation of 14 keypoints of the human body.","Experimental evaluation on the HuPR dataset proves the effectiveness of the model proposed in this paper, outperforming other methods experimented on this dataset with an AP of 69.9 %.","The emphasis of our study is focusing on the position information that is not exploited before in radar singal.","This provides direction to investigate other potential non-redundant information from mmWave rader."],"url":"http://arxiv.org/abs/2405.05164v1","category":"cs.CV"}
{"created":"2024-05-08 15:52:50","title":"Selective Classification Under Distribution Shifts","abstract":"In selective classification (SC), a classifier abstains from making predictions that are likely to be wrong to avoid excessive errors. To deploy imperfect classifiers -- imperfect either due to intrinsic statistical noise of data or for robustness issue of the classifier or beyond -- in high-stakes scenarios, SC appears to be an attractive and necessary path to follow. Despite decades of research in SC, most previous SC methods still focus on the ideal statistical setting only, i.e., the data distribution at deployment is the same as that of training, although practical data can come from the wild. To bridge this gap, in this paper, we propose an SC framework that takes into account distribution shifts, termed generalized selective classification, that covers label-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature. We focus on non-training-based confidence-score functions for generalized SC on deep learning (DL) classifiers and propose two novel margin-based score functions. Through extensive analysis and experiments, we show that our proposed score functions are more effective and reliable than the existing ones for generalized SC on a variety of classification tasks and DL classifiers.","sentences":["In selective classification (SC), a classifier abstains from making predictions that are likely to be wrong to avoid excessive errors.","To deploy imperfect classifiers -- imperfect either due to intrinsic statistical noise of data or for robustness issue of the classifier or beyond -- in high-stakes scenarios, SC appears to be an attractive and necessary path to follow.","Despite decades of research in SC, most previous SC methods still focus on the ideal statistical setting only, i.e., the data distribution at deployment is the same as that of training, although practical data can come from the wild.","To bridge this gap, in this paper, we propose an SC framework that takes into account distribution shifts, termed generalized selective classification, that covers label-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature.","We focus on non-training-based confidence-score functions for generalized SC on deep learning (DL) classifiers and propose two novel margin-based score functions.","Through extensive analysis and experiments, we show that our proposed score functions are more effective and reliable than the existing ones for generalized SC on a variety of classification tasks and DL classifiers."],"url":"http://arxiv.org/abs/2405.05160v1","category":"cs.LG"}
{"created":"2024-05-08 15:46:31","title":"The Potential and Implications of Generative AI on HCI Education","abstract":"Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines. As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI. In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module. We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions. Our insights are based on replies to a survey sent out to the students after completing the module. Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students' understanding of key concepts and ideas and highlighting knowledge gaps. We also discuss potential pitfalls that should be considered and the need to assess students' literacies and assumptions of GAIs as pedagogical tools. Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice. We end with a discussion of our findings in relation to the TPACK framework in HCI.","sentences":["Generative AI (GAI) is impacting teaching and learning directly or indirectly across a range of subjects and disciplines.","As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI.","In this paper, we report on the main pedagogical insights gained from the inclusion of generative AI into a 10 week undergraduate module.","We designed the module to encourage student experimentation with GAI models as part of the design brief requirement and planned practical sessions and discussions.","Our insights are based on replies to a survey sent out to the students after completing the module.","Our key findings, for HCI educators, report on the use of AI as a persona for developing project ideas and creating resources for design, and AI as a mirror for reflecting students' understanding of key concepts and ideas and highlighting knowledge gaps.","We also discuss potential pitfalls that should be considered and the need to assess students' literacies and assumptions of GAIs as pedagogical tools.","Finally, we put forward the case for educators to take the opportunities GAI presents as an educational tool and be experimental, creative, and courageous in their practice.","We end with a discussion of our findings in relation to the TPACK framework in HCI."],"url":"http://arxiv.org/abs/2405.05154v1","category":"cs.HC"}
{"created":"2024-05-08 15:40:42","title":"Boundary symmetry breaking of flocking systems","abstract":"We consider a flocking system confined transversally between two infinite reflecting parallel walls separated by a distance $L_\\perp$. Infinite or periodic boundary conditions are assumed longitudinally to the direction of collective motion, defining a ring geometry typical of experimental realizations with flocking active colloids. Such a confinement selects a flocking state with its mean direction aligned parallel to the wall, thus breaking explicitly the rotational symmetry locally by a boundary effect. Finite size scaling analysis and numerical simulations show that confinement induces an effective mass term ${M_c} \\sim L_\\perp^{-\\zeta}$ (with positive $\\zeta$ being the dynamical scaling exponent of the free theory) suppressing scale free correlations at small wave-numbers. However, due to the finite system size in the transversal direction, this effect can only be detected for large enough longitudinal system sizes (i.e. narrow ring geometries). Furthermore, in the longitudinal direction, density correlations are characterized by an anomalous effective mass term. The effective mass term also enhances the global scalar order parameter and suppresses fluctuations of the mean flocking direction. These results suggest an equivalence between transversal confinement and driving by an homogeneous external field, which breaks the rotational symmetry at the global level.","sentences":["We consider a flocking system confined transversally between two infinite reflecting parallel walls separated by a distance $L_\\perp$. Infinite or periodic boundary conditions are assumed longitudinally to the direction of collective motion, defining a ring geometry typical of experimental realizations with flocking active colloids.","Such a confinement selects a flocking state with its mean direction aligned parallel to the wall, thus breaking explicitly the rotational symmetry locally by a boundary effect.","Finite size scaling analysis and numerical simulations show that confinement induces an effective mass term ${M_c} \\sim L_\\perp^{-\\zeta}$ (with positive $\\zeta$ being the dynamical scaling exponent of the free theory) suppressing scale free correlations at small wave-numbers.","However, due to the finite system size in the transversal direction, this effect can only be detected for large enough longitudinal system sizes (i.e. narrow ring geometries).","Furthermore, in the longitudinal direction, density correlations are characterized by an anomalous effective mass term.","The effective mass term also enhances the global scalar order parameter and suppresses fluctuations of the mean flocking direction.","These results suggest an equivalence between transversal confinement and driving by an homogeneous external field, which breaks the rotational symmetry at the global level."],"url":"http://arxiv.org/abs/2405.05148v1","category":"cond-mat.soft"}
{"created":"2024-05-08 15:39:38","title":"Hybrid Convolutional Neural Networks with Reliability Guarantee","abstract":"Making AI safe and dependable requires the generation of dependable models and dependable execution of those models. We propose redundant execution as a well-known technique that can be used to ensure reliable execution of the AI model. This generic technique will extend the application scope of AI-accelerators that do not feature well-documented safety or dependability properties. Typical redundancy techniques incur at least double or triple the computational expense of the original. We adopt a co-design approach, integrating reliable model execution with non-reliable execution, focusing that additional computational expense only where it is strictly necessary. We describe the design, implementation and some preliminary results of a hybrid CNN.","sentences":["Making AI safe and dependable requires the generation of dependable models and dependable execution of those models.","We propose redundant execution as a well-known technique that can be used to ensure reliable execution of the AI model.","This generic technique will extend the application scope of AI-accelerators that do not feature well-documented safety or dependability properties.","Typical redundancy techniques incur at least double or triple the computational expense of the original.","We adopt a co-design approach, integrating reliable model execution with non-reliable execution, focusing that additional computational expense only where it is strictly necessary.","We describe the design, implementation and some preliminary results of a hybrid CNN."],"url":"http://arxiv.org/abs/2405.05146v2","category":"cs.AI"}
{"created":"2024-05-08 15:33:21","title":"Dynamic Size Counting in the Population Protocol Model","abstract":"The population protocol model describes collections of distributed agents that interact in pairs to solve a common task. We consider a dynamic variant of this prominent model, where we assume that an adversary may change the population size at an arbitrary point in time. In this model we tackle the problem of counting the population size: in the dynamic size counting problem the goal is to design an algorithm that computes an approximation of $\\log n$. This estimate can be used to turn static, non-uniform population protocols, i.e., protocols that depend on the population size $n$, into dynamic and loosely-stabilizing protocols.   Our contributions in this paper are three-fold. Starting from an arbitrary initial configuration, we first prove that the agents converge quickly to a valid configuration where each agent has a constant-factor approximation of $\\log n$, and once the agents reach such a valid configuration, they stay in it for a polynomial number of time steps. Second, we show how to use our protocol to define a uniform and loosely-stabilizing phase clock for the population protocol model. Finally, we support our theoretical findings by empirical simulations that show that our protocols work well in practice.","sentences":["The population protocol model describes collections of distributed agents that interact in pairs to solve a common task.","We consider a dynamic variant of this prominent model, where we assume that an adversary may change the population size at an arbitrary point in time.","In this model we tackle the problem of counting the population size: in the dynamic size counting problem the goal is to design an algorithm that computes an approximation of $\\log n$. This estimate can be used to turn static, non-uniform population protocols, i.e., protocols that depend on the population size $n$, into dynamic and loosely-stabilizing protocols.   ","Our contributions in this paper are three-fold.","Starting from an arbitrary initial configuration, we first prove that the agents converge quickly to a valid configuration where each agent has a constant-factor approximation of $\\log n$, and once the agents reach such a valid configuration, they stay in it for a polynomial number of time steps.","Second, we show how to use our protocol to define a uniform and loosely-stabilizing phase clock for the population protocol model.","Finally, we support our theoretical findings by empirical simulations that show that our protocols work well in practice."],"url":"http://arxiv.org/abs/2405.05137v1","category":"cs.DC"}
{"created":"2024-05-08 15:32:20","title":"Identifying every building's function in large-scale urban areas with multi-modality remote-sensing data","abstract":"Buildings, as fundamental man-made structures in urban environments, serve as crucial indicators for understanding various city function zones. Rapid urbanization has raised an urgent need for efficiently surveying building footprints and functions. In this study, we proposed a semi-supervised framework to identify every building's function in large-scale urban areas with multi-modality remote-sensing data. In detail, optical images, building height, and nighttime-light data are collected to describe the morphological attributes of buildings. Then, the area of interest (AOI) and building masks from the volunteered geographic information (VGI) data are collected to form sparsely labeled samples. Furthermore, the multi-modality data and weak labels are utilized to train a segmentation model with a semi-supervised strategy. Finally, results are evaluated by 20,000 validation points and statistical survey reports from the government. The evaluations reveal that the produced function maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings in Shanghai, China. This study has the potential to support large-scale urban management and sustainable urban development. All collected data and produced maps are open access at https://github.com/LiZhuoHong/BuildingMap.","sentences":["Buildings, as fundamental man-made structures in urban environments, serve as crucial indicators for understanding various city function zones.","Rapid urbanization has raised an urgent need for efficiently surveying building footprints and functions.","In this study, we proposed a semi-supervised framework to identify every building's function in large-scale urban areas with multi-modality remote-sensing data.","In detail, optical images, building height, and nighttime-light data are collected to describe the morphological attributes of buildings.","Then, the area of interest (AOI) and building masks from the volunteered geographic information (VGI) data are collected to form sparsely labeled samples.","Furthermore, the multi-modality data and weak labels are utilized to train a segmentation model with a semi-supervised strategy.","Finally, results are evaluated by 20,000 validation points and statistical survey reports from the government.","The evaluations reveal that the produced function maps achieve an OA of 82% and Kappa of 71% among 1,616,796 buildings in Shanghai, China.","This study has the potential to support large-scale urban management and sustainable urban development.","All collected data and produced maps are open access at https://github.com/LiZhuoHong/BuildingMap."],"url":"http://arxiv.org/abs/2405.05133v1","category":"cs.CV"}
{"created":"2024-05-08 15:25:10","title":"Web Intelligence Journal in perspective: an analysis of its two decades trajectory","abstract":"The evolution of a thematic area undergoes various changes of perspective and adopts new theoretical approaches that arise from the interactions of the community and a wide range of social needs. The advent of digital technologies, such as social networks, underlines this factor by spreading knowledge and forging links between different communities. Web intelligence is now on the verge of raising questions that broaden the understanding of how artificial intelligence impacts the Web of People, Data, and Things, among other factors. To the best of our knowledge, there is no study that has conducted a longitudinal analysis of the evolution of this community. Thus, we investigate in this paper how Web intelligence has evolved in the last twenty years by carrying out a literature review and bibliometric analysis. Concerning the impact of this research study, increasing attention is devoted to determining which are the most influential papers in the community by referring to citation networks and discovering the most popular and pressing topics through a co-citation analysis and the keywords co-occurrence. The results obtained can guide the direction of new research projects in the area and update the scope and places of interest found in current trends and the relevant journals.","sentences":["The evolution of a thematic area undergoes various changes of perspective and adopts new theoretical approaches that arise from the interactions of the community and a wide range of social needs.","The advent of digital technologies, such as social networks, underlines this factor by spreading knowledge and forging links between different communities.","Web intelligence is now on the verge of raising questions that broaden the understanding of how artificial intelligence impacts the Web of People, Data, and Things, among other factors.","To the best of our knowledge, there is no study that has conducted a longitudinal analysis of the evolution of this community.","Thus, we investigate in this paper how Web intelligence has evolved in the last twenty years by carrying out a literature review and bibliometric analysis.","Concerning the impact of this research study, increasing attention is devoted to determining which are the most influential papers in the community by referring to citation networks and discovering the most popular and pressing topics through a co-citation analysis and the keywords co-occurrence.","The results obtained can guide the direction of new research projects in the area and update the scope and places of interest found in current trends and the relevant journals."],"url":"http://arxiv.org/abs/2405.05129v1","category":"cs.SI"}
{"created":"2024-05-08 15:05:55","title":"QFMTS: Generating Query-Focused Summaries over Multi-Table Inputs","abstract":"Table summarization is a crucial task aimed at condensing information from tabular data into concise and comprehensible textual summaries. However, existing approaches often fall short of adequately meeting users' information and quality requirements and tend to overlook the complexities of real-world queries. In this paper, we propose a novel method to address these limitations by introducing query-focused multi-table summarization. Our approach, which comprises a table serialization module, a summarization controller, and a large language model (LLM), utilizes textual queries and multiple tables to generate query-dependent table summaries tailored to users' information needs. To facilitate research in this area, we present a comprehensive dataset specifically tailored for this task, consisting of 4909 query-summary pairs, each associated with multiple tables. Through extensive experiments using our curated dataset, we demonstrate the effectiveness of our proposed method compared to baseline approaches. Our findings offer insights into the challenges of complex table reasoning for precise summarization, contributing to the advancement of research in query-focused multi-table summarization.","sentences":["Table summarization is a crucial task aimed at condensing information from tabular data into concise and comprehensible textual summaries.","However, existing approaches often fall short of adequately meeting users' information and quality requirements and tend to overlook the complexities of real-world queries.","In this paper, we propose a novel method to address these limitations by introducing query-focused multi-table summarization.","Our approach, which comprises a table serialization module, a summarization controller, and a large language model (LLM), utilizes textual queries and multiple tables to generate query-dependent table summaries tailored to users' information needs.","To facilitate research in this area, we present a comprehensive dataset specifically tailored for this task, consisting of 4909 query-summary pairs, each associated with multiple tables.","Through extensive experiments using our curated dataset, we demonstrate the effectiveness of our proposed method compared to baseline approaches.","Our findings offer insights into the challenges of complex table reasoning for precise summarization, contributing to the advancement of research in query-focused multi-table summarization."],"url":"http://arxiv.org/abs/2405.05109v1","category":"cs.CL"}
{"created":"2024-05-08 14:24:11","title":"Concerns on Bias in Large Language Models when Creating Synthetic Personae","abstract":"This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs). These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them. The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research.","sentences":["This position paper explores the benefits, drawbacks, and ethical considerations of incorporating synthetic personae in HCI research, particularly focusing on the customization challenges beyond the limitations of current Large Language Models (LLMs).","These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them.","The study aims to establish a foundation for understanding the challenges associated with these models, emphasizing the necessity of thorough testing before utilizing them to create synthetic personae for HCI research."],"url":"http://arxiv.org/abs/2405.05080v1","category":"cs.HC"}
{"created":"2024-05-08 14:22:16","title":"Observation of $t\\bar{t}$ production in the lepton+jets and dilepton channels in $p$+Pb collisions at $\\sqrt{s_\\mathrm{NN}}=8.16$ TeV with the ATLAS detector","abstract":"This paper reports the observation of top-quark pair production in proton-lead collisions in the ATLAS experiment at the Large Hadron Collider. The measurement is performed using 165 nb$^{-1}$ of $p$+Pb data collected at $\\sqrt{s_\\mathrm{NN}}=8.16$ TeV in 2016. Events are categorised in two analysis channels, consisting of either events with exactly one lepton (electron or muon) and at least four jets, or events with two opposite-charge leptons and at least two jets. In both channels at least one $b$-tagged jet is also required. Top-quark pair production is observed with a significance over five standard deviations in each channel. The top-quark pair production cross-section is measured to be $\\sigma_{t\\bar{t}}= 58.1\\pm 2.0\\;\\mathrm{(stat.)\\;^{+4.8}_{-4.4} \\;\\mathrm{(syst.)}}\\;\\mathrm{nb}$, with a total uncertainty of 9%. In addition, the nuclear modification factor is measured to be $R_{p\\mathrm{A}} = 1.090\\pm0.039\\;(\\mathrm{stat.})\\;^{+0.094}_{-0.087}\\;(\\mathrm{syst.})$. The measurements are found to be in good agreement with theory predictions involving nuclear parton distribution functions.","sentences":["This paper reports the observation of top-quark pair production in proton-lead collisions in the ATLAS experiment at the Large Hadron Collider.","The measurement is performed using 165 nb$^{-1}$ of $p$+Pb data collected at $\\sqrt{s_\\mathrm{NN}}=8.16$ TeV in 2016.","Events are categorised in two analysis channels, consisting of either events with exactly one lepton (electron or muon) and at least four jets, or events with two opposite-charge leptons and at least two jets.","In both channels at least one $b$-tagged jet is also required.","Top-quark pair production is observed with a significance over five standard deviations in each channel.","The top-quark pair production cross-section is measured to be $\\sigma_{t\\bar{t}}= 58.1\\pm 2.0\\;\\mathrm{(stat.)\\;^{+4.8}_{-4.4} \\;\\mathrm{(syst.)}}\\;\\mathrm{nb}$, with a total uncertainty of 9%.","In addition, the nuclear modification factor is measured to be $R_{p\\mathrm{A}} = 1.090\\pm0.039\\;(\\mathrm{stat.})\\;^{+0.094}_{-0.087}\\;(\\mathrm{syst.})$.","The measurements are found to be in good agreement with theory predictions involving nuclear parton distribution functions."],"url":"http://arxiv.org/abs/2405.05078v1","category":"nucl-ex"}
{"created":"2024-05-08 14:14:03","title":"Novel Actor-Critic Algorithm for Robust Decision Making of CAV under Delays and Loss of V2X Data","abstract":"Current autonomous driving systems heavily rely on V2X communication data to enhance situational awareness and the cooperation between vehicles. However, a major challenge when using V2X data is that it may not be available periodically because of unpredictable delays and data loss during wireless transmission between road stations and the receiver vehicle. This issue should be considered when designing control strategies for connected and autonomous vehicles. Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm that guarantees robust driving performance in V2X environment with delayed and/or lost data. The novel algorithm incorporates three key mechanisms: a virtual fixed sampling period, a combination of Temporal-Difference and Monte Carlo learning, and a numerical approximation of immediate reward values. To address the temporal aperiodicity problem of V2X data, we first illustrate this challenge. Then, we provide a detailed explanation of the Blind Actor-Critic algorithm where we highlight the proposed components to compensate for the temporal aperiodicity problem of V2X data. We evaluate the performance of our algorithm in a simulation environment and compare it to benchmark approaches. The results demonstrate that training metrics are improved compared to conventional actor-critic algorithms. Additionally, testing results show that our approach provides robust control, even under low V2X network reliability levels.","sentences":["Current autonomous driving systems heavily rely on V2X communication data to enhance situational awareness and the cooperation between vehicles.","However, a major challenge when using V2X data is that it may not be available periodically because of unpredictable delays and data loss during wireless transmission between road stations and the receiver vehicle.","This issue should be considered when designing control strategies for connected and autonomous vehicles.","Therefore, this paper proposes a novel 'Blind Actor-Critic' algorithm that guarantees robust driving performance in V2X environment with delayed and/or lost data.","The novel algorithm incorporates three key mechanisms: a virtual fixed sampling period, a combination of Temporal-Difference and Monte Carlo learning, and a numerical approximation of immediate reward values.","To address the temporal aperiodicity problem of V2X data, we first illustrate this challenge.","Then, we provide a detailed explanation of the Blind Actor-Critic algorithm where we highlight the proposed components to compensate for the temporal aperiodicity problem of V2X data.","We evaluate the performance of our algorithm in a simulation environment and compare it to benchmark approaches.","The results demonstrate that training metrics are improved compared to conventional actor-critic algorithms.","Additionally, testing results show that our approach provides robust control, even under low V2X network reliability levels."],"url":"http://arxiv.org/abs/2405.05072v1","category":"cs.LG"}
{"created":"2024-05-08 14:13:09","title":"Search for production of a single vector-like quark decaying to tH or tZ in the all-hadronic final state in pp collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search for electroweak production of a single vector-like T quark in association with a bottom (b) quark in the all-hadronic decay channel is presented. This search uses proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected by the CMS experiment at the CERN LHC during 2016-2018, corresponding to an integrated luminosity of 138 fb$^{-1}$ The T quark is assumed to have charge 2/3 and decay to a top (t) quark and a Higgs (H) or Z boson. Event kinematics and the presence of jets containing b hadrons are used to reconstruct the hadronic decays of the t quark and H or Z boson. No significant deviation from the standard model prediction is observed in the data. The 95% confidence level upper limits on the product of the production cross section and branching fraction of a T quark produced in association with a b quark and decaying via tH or tZ range from 1260 to 68 fb for T quark masses of 600-1200 GeV.","sentences":["A search for electroweak production of a single vector-like T quark in association with a bottom (b) quark in the all-hadronic decay channel is presented.","This search uses proton-proton collision data at $\\sqrt{s}$ = 13 TeV collected by the CMS experiment at the CERN LHC during 2016-2018, corresponding to an integrated luminosity of 138 fb$^{-1}$","The T quark is assumed to have charge 2/3 and decay to a top (t) quark and a Higgs (H) or Z boson.","Event kinematics and the presence of jets containing b hadrons are used to reconstruct the hadronic decays of the t quark and H or Z boson.","No significant deviation from the standard model prediction is observed in the data.","The 95% confidence level upper limits on the product of the production cross section and branching fraction of a T quark produced in association with a b quark and decaying via tH or tZ range from 1260 to 68 fb for T quark masses of 600-1200 GeV."],"url":"http://arxiv.org/abs/2405.05071v1","category":"hep-ex"}
{"created":"2024-05-08 14:04:35","title":"Designing Skill-Compatible AI: Methodologies and Frameworks in Chess","abstract":"Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility.","sentences":["Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power.","For AI agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts.","We propose a formal evaluation framework for assessing the compatibility of near-optimal AI with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop AI agents that can successfully interact with lower-skill entities.","Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents.","We contribute three methodologies to explicitly create skill-compatible AI agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful AI agents and less-skilled partners.","On these frameworks, our agents outperform state-of-the-art chess AI (based on AlphaZero) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance.","Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility."],"url":"http://arxiv.org/abs/2405.05066v1","category":"cs.AI"}
{"created":"2024-05-08 13:55:52","title":"Impact of Tone-Aware Explanations in Recommender Systems","abstract":"In recommender systems, the presentation of explanations plays a crucial role in supporting users' decision-making processes. Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked. Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication. However, studies on the impact of tone on explanations within the context of recommender systems are insufficient. Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes. We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products. Collected data analysis reveals different perceived effects of tones depending on the domains. Moreover, user attributes such as age and personality traits are found to influence the impact of tone. This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience.","sentences":["In recommender systems, the presentation of explanations plays a crucial role in supporting users' decision-making processes.","Although numerous existing studies have focused on the effects (transparency or persuasiveness) of explanation content, explanation expression is largely overlooked.","Tone, such as formal and humorous, is directly linked to expressiveness and is an important element in human communication.","However, studies on the impact of tone on explanations within the context of recommender systems are insufficient.","Therefore, this study investigates the effect of explanation tones through an online user study from three aspects: perceived effects, domain differences, and user attributes.","We create a dataset using a large language model to generate fictional items and explanations with various tones in the domain of movies, hotels, and home products.","Collected data analysis reveals different perceived effects of tones depending on the domains.","Moreover, user attributes such as age and personality traits are found to influence the impact of tone.","This research underscores the critical role of tones in explanations within recommender systems, suggesting that attention to tone can enhance user experience."],"url":"http://arxiv.org/abs/2405.05061v1","category":"cs.HC"}
{"created":"2024-05-08 13:55:25","title":"Conversational Topic Recommendation in Counseling and Psychotherapy with Decision Transformer and Large Language Models","abstract":"Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems. In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals. The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model. We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model's output as synthetic labels for fine-tuning a large language model for the same task. Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design.","sentences":["Given the increasing demand for mental health assistance, artificial intelligence (AI), particularly large language models (LLMs), may be valuable for integration into automated clinical support systems.","In this work, we leverage a decision transformer architecture for topic recommendation in counseling conversations between patients and mental health professionals.","The architecture is utilized for offline reinforcement learning, and we extract states (dialogue turn embeddings), actions (conversation topics), and rewards (scores measuring the alignment between patient and therapist) from previous turns within a conversation to train a decision transformer model.","We demonstrate an improvement over baseline reinforcement learning methods, and propose a novel system of utilizing our model's output as synthetic labels for fine-tuning a large language model for the same task.","Although our implementation based on LLaMA-2 7B has mixed results, future work can undoubtedly build on the design."],"url":"http://arxiv.org/abs/2405.05060v1","category":"cs.CL"}
{"created":"2024-05-08 13:42:22","title":"Ab initio computations of strongly deformed nuclei around $^{80}$Zr","abstract":"Nuclei around $N\\approx Z\\approx 40$ are strongly deformed and exhibit coexistence of shapes. These phenomena have challenged nuclear models. Here we perform ab initio coupled-cluster computations of low-lying collective states and electromagnetic quadrupole transitions of the even-even nuclei $^{72}$Kr, $^{76,78}$Sr, $^{78,80}$Zr and $^{84}$Mo starting from chiral nucleon-nucleon and three-nucleon forces. Our calculations reproduce the coexistence of oblate and prolate shapes in these nuclei, yield rotational bands and strong electromagnetic transitions, but are not accurate for some observables and nuclei. These results highlight the advances and challenges of ab initio computations of heavy deformed nuclei.","sentences":["Nuclei around $N\\approx Z\\approx 40$ are strongly deformed and exhibit coexistence of shapes.","These phenomena have challenged nuclear models.","Here we perform ab initio coupled-cluster computations of low-lying collective states and electromagnetic quadrupole transitions of the even-even nuclei $^{72}$Kr, $^{76,78}$Sr, $^{78,80}$Zr and $^{84}$Mo starting from chiral nucleon-nucleon and three-nucleon forces.","Our calculations reproduce the coexistence of oblate and prolate shapes in these nuclei, yield rotational bands and strong electromagnetic transitions, but are not accurate for some observables and nuclei.","These results highlight the advances and challenges of ab initio computations of heavy deformed nuclei."],"url":"http://arxiv.org/abs/2405.05052v1","category":"nucl-th"}
{"created":"2024-05-08 13:29:18","title":"Unique continuation at the boundary for divergence form elliptic equations on quasiconvex domains","abstract":"Let $\\Omega \\subset \\mathbb{R}^d$ be a quasiconvex Lipschitz domain and $A(x)$ be a $d \\times d$ uniformly elliptic, symmetric matrix with Lipschitz coefficients. Assume a nontrivial $u$ solves $-\\nabla \\cdot (A(x) \\nabla u) = 0$ in $\\Omega$, and $u$ vanishes on $\\Sigma = \\partial \\Omega \\cap B$ for some ball $B$. The main contribution of this paper is to demonstrate the existence of a countable collection of open balls $(B_i)_i$ such that the restriction of $u$ to $B_i \\cap \\Omega$ maintains a consistent sign. Furthermore, for any compact subset $K$ of $\\Sigma$, the set difference $K \\setminus \\bigcup_i B_i$ is shown to possess a Minkowski dimension that is strictly less than $d - 1 - \\epsilon$. As a consequence, we prove Lin's conjecture in quasiconvex domains.","sentences":["Let $\\Omega \\subset \\mathbb{R}^d$ be a quasiconvex Lipschitz domain and $A(x)$ be a $d \\times d$ uniformly elliptic, symmetric matrix with Lipschitz coefficients.","Assume a nontrivial $u$ solves $-\\nabla \\cdot (A(x) \\nabla u) = 0$ in $\\Omega$, and $u$ vanishes on $\\Sigma = \\partial \\Omega \\cap B$ for some ball $B$. The main contribution of this paper is to demonstrate the existence of a countable collection of open balls $(B_i)_i$ such that the restriction of $u$ to $B_i \\cap \\Omega$ maintains a consistent sign.","Furthermore, for any compact subset $K$ of $\\Sigma$, the set difference $K \\setminus \\bigcup_i B_i$ is shown to possess a Minkowski dimension that is strictly less than $d - 1 - \\epsilon$. As a consequence, we prove Lin's conjecture in quasiconvex domains."],"url":"http://arxiv.org/abs/2405.05044v2","category":"math.AP"}
{"created":"2024-05-08 13:13:02","title":"Reviewing Intelligent Cinematography: AI research for camera-based video production","abstract":"This paper offers a comprehensive review of artificial intelligence (AI) research in the context of real camera content acquisition for entertainment purposes and is aimed at both researchers and cinematographers. Considering the breadth of computer vision research and the lack of review papers tied to intelligent cinematography (IC), this review introduces a holistic view of the IC landscape while providing the technical insight for experts across across disciplines. We preface the main discussion with technical background on generative AI, object detection, automated camera calibration and 3-D content acquisition, and link explanatory articles to assist non-technical readers. The main discussion categorizes work by four production types: General Production, Virtual Production, Live Production and Aerial Production. Note that for Virtual Production we do not discuss research relating to virtual content acquisition, including work on automated video generation, like Stable Diffusion. Within each section, we (1) sub-classify work by the technical field of research - reflected by the subsections, and (2) evaluate the trends and challenge w.r.t to each type of production. In the final chapter, we present our concluding remarks on the greater scope of IC research and outline work that we believe has significant potential to influence the whole industry. We find that work relating to virtual production has the greatest potential to impact other mediums of production, driven by the growing interest in LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture for a virtual modelling of real world scenes and actors. This is the first piece of literature to offer a structured and comprehensive examination of IC research. Consequently, we address ethical and legal concerns regarding the use of creative AI involving artists, actors and the general public, in the...","sentences":["This paper offers a comprehensive review of artificial intelligence (AI) research in the context of real camera content acquisition for entertainment purposes and is aimed at both researchers and cinematographers.","Considering the breadth of computer vision research and the lack of review papers tied to intelligent cinematography (IC), this review introduces a holistic view of the IC landscape while providing the technical insight for experts across across disciplines.","We preface the main discussion with technical background on generative AI, object detection, automated camera calibration and 3-D content acquisition, and link explanatory articles to assist non-technical readers.","The main discussion categorizes work by four production types: General Production, Virtual Production, Live Production and Aerial Production.","Note that for Virtual Production we do not discuss research relating to virtual content acquisition, including work on automated video generation, like Stable Diffusion.","Within each section, we (1) sub-classify work by the technical field of research - reflected by the subsections, and (2) evaluate the trends and challenge w.r.t to each type of production.","In the final chapter, we present our concluding remarks on the greater scope of IC research and outline work that we believe has significant potential to influence the whole industry.","We find that work relating to virtual production has the greatest potential to impact other mediums of production, driven by the growing interest in LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture for a virtual modelling of real world scenes and actors.","This is the first piece of literature to offer a structured and comprehensive examination of IC research.","Consequently, we address ethical and legal concerns regarding the use of creative AI involving artists, actors and the general public, in the..."],"url":"http://arxiv.org/abs/2405.05039v1","category":"cs.CV"}
{"created":"2024-05-08 12:57:53","title":"StyleMamba : State Space Model for Efficient Text-driven Image Style Transfer","abstract":"We present StyleMamba, an efficient image style transfer framework that translates text prompts into corresponding visual styles while preserving the content integrity of the original images. Existing text-guided stylization requires hundreds of training iterations and takes a lot of computing resources. To speed up the process, we propose a conditional State Space Model for Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that sequentially aligns the image features to the target text prompts. To enhance the local and global style consistency between text and image, we propose masked and second-order directional losses to optimize the stylization direction to significantly reduce the training iterations by 5 times and the inference time by 3 times. Extensive experiments and qualitative evaluation confirm the robust and superior stylization performance of our methods compared to the existing baselines.","sentences":["We present StyleMamba, an efficient image style transfer framework that translates text prompts into corresponding visual styles while preserving the content integrity of the original images.","Existing text-guided stylization requires hundreds of training iterations and takes a lot of computing resources.","To speed up the process, we propose a conditional State Space Model for Efficient Text-driven Image Style Transfer, dubbed StyleMamba, that sequentially aligns the image features to the target text prompts.","To enhance the local and global style consistency between text and image, we propose masked and second-order directional losses to optimize the stylization direction to significantly reduce the training iterations by 5 times and the inference time by 3 times.","Extensive experiments and qualitative evaluation confirm the robust and superior stylization performance of our methods compared to the existing baselines."],"url":"http://arxiv.org/abs/2405.05027v1","category":"cs.CV"}
{"created":"2024-05-08 12:56:33","title":"Learning Structural Causal Models through Deep Generative Models: Methods, Guarantees, and Challenges","abstract":"This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures. It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries. Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling. It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs.","sentences":["This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures.","It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries.","Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling.","It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs."],"url":"http://arxiv.org/abs/2405.05025v1","category":"stat.ML"}
{"created":"2024-05-08 12:48:01","title":"HackCar: a test platform for attacks and defenses on a cost-contained automotive architecture","abstract":"In this paper, we introduce the design of HackCar, a testing platform for replicating attacks and defenses on a generic automotive system without requiring access to a complete vehicle. This platform empowers security researchers to illustrate the consequences of attacks targeting an automotive system on a realistic platform, facilitating the development and testing of security countermeasures against both existing and novel attacks. The HackCar platform is built upon an F1-10th model, to which various automotive-grade microcontrollers are connected through automotive communication protocols. This solution is crafted to be entirely modular, allowing for the creation of diverse test scenarios. Researchers and practitioners can thus develop innovative security solutions while adhering to the constraints of automotive-grade microcontrollers. We showcase our design by comparing it with a real, licensed, and unmodified vehicle. Additionally, we analyze the behavior of the HackCar in both an attack-free scenario and a scenario where an attack on in-vehicle communication is deployed.","sentences":["In this paper, we introduce the design of HackCar, a testing platform for replicating attacks and defenses on a generic automotive system without requiring access to a complete vehicle.","This platform empowers security researchers to illustrate the consequences of attacks targeting an automotive system on a realistic platform, facilitating the development and testing of security countermeasures against both existing and novel attacks.","The HackCar platform is built upon an F1-10th model, to which various automotive-grade microcontrollers are connected through automotive communication protocols.","This solution is crafted to be entirely modular, allowing for the creation of diverse test scenarios.","Researchers and practitioners can thus develop innovative security solutions while adhering to the constraints of automotive-grade microcontrollers.","We showcase our design by comparing it with a real, licensed, and unmodified vehicle.","Additionally, we analyze the behavior of the HackCar in both an attack-free scenario and a scenario where an attack on in-vehicle communication is deployed."],"url":"http://arxiv.org/abs/2405.05023v1","category":"cs.CR"}
{"created":"2024-05-08 12:31:35","title":"Concrete Dense Network for Long-Sequence Time Series Clustering","abstract":"Time series clustering is fundamental in data analysis for discovering temporal patterns. Despite recent advancements, learning cluster-friendly representations is still challenging, particularly with long and complex time series. Deep temporal clustering methods have been trying to integrate the canonical k-means into end-to-end training of neural networks but fall back on surrogate losses due to the non-differentiability of the hard cluster assignment, yielding sub-optimal solutions. In addition, the autoregressive strategy used in the state-of-the-art RNNs is subject to error accumulation and slow training, while recent research findings have revealed that Transformers are less effective due to time points lacking semantic meaning, to the permutation invariance of attention that discards the chronological order and high computation cost. In light of these observations, we present LoSTer which is a novel dense autoencoder architecture for the long-sequence time series clustering problem (LSTC) capable of optimizing the k-means objective via the Gumbel-softmax reparameterization trick and designed specifically for accurate and fast clustering of long time series. Extensive experiments on numerous benchmark datasets and two real-world applications prove the effectiveness of LoSTer over state-of-the-art RNNs and Transformer-based deep clustering methods.","sentences":["Time series clustering is fundamental in data analysis for discovering temporal patterns.","Despite recent advancements, learning cluster-friendly representations is still challenging, particularly with long and complex time series.","Deep temporal clustering methods have been trying to integrate the canonical k-means into end-to-end training of neural networks but fall back on surrogate losses due to the non-differentiability of the hard cluster assignment, yielding sub-optimal solutions.","In addition, the autoregressive strategy used in the state-of-the-art RNNs is subject to error accumulation and slow training, while recent research findings have revealed that Transformers are less effective due to time points lacking semantic meaning, to the permutation invariance of attention that discards the chronological order and high computation cost.","In light of these observations, we present LoSTer which is a novel dense autoencoder architecture for the long-sequence time series clustering problem (LSTC) capable of optimizing the k-means objective via the Gumbel-softmax reparameterization trick and designed specifically for accurate and fast clustering of long time series.","Extensive experiments on numerous benchmark datasets and two real-world applications prove the effectiveness of LoSTer over state-of-the-art RNNs and Transformer-based deep clustering methods."],"url":"http://arxiv.org/abs/2405.05015v1","category":"cs.LG"}
{"created":"2024-05-08 12:24:52","title":"ADELIE: Aligning Large Language Models on Information Extraction","abstract":"Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks. This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data. In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE. We first collect and construct a high-quality alignment corpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on IEInstruct. We further train ADELIE_SFT with direct preference optimization (DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO) achieve state-of-the-art (SoTA) performance among open-source models. We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline. We will release the code, data, and models to facilitate further research.","sentences":["Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks.","This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data.","In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE.","We first collect and construct a high-quality alignment corpus IEInstruct for IE.","Then we train ADELIE_SFT using instruction tuning on IEInstruct.","We further train ADELIE_SFT with direct preference optimization (DPO) objective, resulting in ADELIE_DPO.","Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO) achieve state-of-the-art (SoTA) performance among open-source models.","We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline.","We will release the code, data, and models to facilitate further research."],"url":"http://arxiv.org/abs/2405.05008v1","category":"cs.CL"}
{"created":"2024-05-08 12:04:18","title":"Negative-energy spin waves in antiferromagnets for spin-current amplification and analogue gravity","abstract":"Magnonic black holes-analogue event horizons for the spin-wave collective excitations of ordered magnets-can be used for fundamental research, for example for investigating Hawking radiation, but also for technological applications of spin waves. Here we show how to engineer a horizon for spin waves in antiferromagnets, which have the attractive feature of fast magnetization dynamics and linear dispersion relation. We propose a set-up with spatially varying exchange interaction with spin transfer torque to implement the horizon and a second set-up for the amplification of spin waves consisting of an antiferromagnet subject to a spatially varying external magnetic field that is driven by spin orbit torque. We compute the values of parameters needed to implement the horizon and to have amplification of spin waves. We develop the corresponding Klein-Gordon equation and quantify the amplification. Our work paves the way for investigation of Hawking radiation of spin waves and for antiferromagnet-based spin-waves amplifiers.","sentences":["Magnonic black holes-analogue event horizons for the spin-wave collective excitations of ordered magnets-can be used for fundamental research, for example for investigating Hawking radiation, but also for technological applications of spin waves.","Here we show how to engineer a horizon for spin waves in antiferromagnets, which have the attractive feature of fast magnetization dynamics and linear dispersion relation.","We propose a set-up with spatially varying exchange interaction with spin transfer torque to implement the horizon and a second set-up for the amplification of spin waves consisting of an antiferromagnet subject to a spatially varying external magnetic field that is driven by spin orbit torque.","We compute the values of parameters needed to implement the horizon and to have amplification of spin waves.","We develop the corresponding Klein-Gordon equation and quantify the amplification.","Our work paves the way for investigation of Hawking radiation of spin waves and for antiferromagnet-based spin-waves amplifiers."],"url":"http://arxiv.org/abs/2405.04996v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 11:58:55","title":"NAVRepair: Node-type Aware C/C++ Code Vulnerability Repair","abstract":"The rapid advancement of deep learning has led to the development of Large Language Models (LLMs). In the field of vulnerability repair, previous research has leveraged rule-based fixing, pre-trained models, and LLM's prompt engineering. However, existing approaches have limitations in terms of the integration of code structure with error types. Besides, due to certain features of C/C++ language, vulnerability repair in C/C++ proves to be exceptionally challenging. To address these challenges, we propose NAVRepair, a novel framework that combines the node-type information extracted from Abstract Syntax Trees (ASTs) with error types, specifically targeting C/C++ vulnerabilities. Specifically, our approach employs type analysis to localize the minimum edit node (MEN) and customizes context information collection based on different error types. In the offline stage, NAVRepair parses code patches to locate MENs and designs rules to extract relevant contextual information for each MEN type. In the online repairing stage, it analyzes the suspicious code, combines it with vulnerability type templates derived from the Common Weakness Enumeration (CWE), and generates targeted repair prompts. We evaluate NAVRepair on multiple popular LLMs and demonstrate its effectiveness in improving the performance of code vulnerability repair. Notably, our framework is independent of any specific LLMs and can quickly adapt to new vulnerability types. Extensive experiments validate that NAVRepair achieves excellent results in assisting LLMs to accurately detect and fix C/C++ vulnerabilities. We achieve a 26% higher accuracy compared to an existing LLM-based C/C++ vulnerability repair method. We believe our node type-aware approach has promising application prospects for enhancing real-world C/C++ code security.","sentences":["The rapid advancement of deep learning has led to the development of Large Language Models (LLMs).","In the field of vulnerability repair, previous research has leveraged rule-based fixing, pre-trained models, and LLM's prompt engineering.","However, existing approaches have limitations in terms of the integration of code structure with error types.","Besides, due to certain features of C/C++ language, vulnerability repair in C/C++ proves to be exceptionally challenging.","To address these challenges, we propose NAVRepair, a novel framework that combines the node-type information extracted from Abstract Syntax Trees (ASTs) with error types, specifically targeting C/C++ vulnerabilities.","Specifically, our approach employs type analysis to localize the minimum edit node (MEN) and customizes context information collection based on different error types.","In the offline stage, NAVRepair parses code patches to locate MENs and designs rules to extract relevant contextual information for each MEN type.","In the online repairing stage, it analyzes the suspicious code, combines it with vulnerability type templates derived from the Common Weakness Enumeration (CWE), and generates targeted repair prompts.","We evaluate NAVRepair on multiple popular LLMs and demonstrate its effectiveness in improving the performance of code vulnerability repair.","Notably, our framework is independent of any specific LLMs and can quickly adapt to new vulnerability types.","Extensive experiments validate that NAVRepair achieves excellent results in assisting LLMs to accurately detect and fix C/C++ vulnerabilities.","We achieve a 26% higher accuracy compared to an existing LLM-based C/C++ vulnerability repair method.","We believe our node type-aware approach has promising application prospects for enhancing real-world C/C++ code security."],"url":"http://arxiv.org/abs/2405.04994v1","category":"cs.SE"}
{"created":"2024-05-08 11:54:15","title":"Health Index Estimation Through Integration of General Knowledge with Unsupervised Learning","abstract":"Accurately estimating a Health Index (HI) from condition monitoring data (CM) is essential for reliable and interpretable prognostics and health management (PHM) in complex systems. In most scenarios, complex systems operate under varying operating conditions and can exhibit different fault modes, making unsupervised inference of an HI from CM data a significant challenge. Hybrid models combining prior knowledge about degradation with deep learning models have been proposed to overcome this challenge. However, previously suggested hybrid models for HI estimation usually rely heavily on system-specific information, limiting their transferability to other systems. In this work, we propose an unsupervised hybrid method for HI estimation that integrates general knowledge about degradation into the convolutional autoencoder's model architecture and learning algorithm, enhancing its applicability across various systems. The effectiveness of the proposed method is demonstrated in two case studies from different domains: turbofan engines and lithium batteries. The results show that the proposed method outperforms other competitive alternatives, including residual-based methods, in terms of HI quality and their utility for Remaining Useful Life (RUL) predictions. The case studies also highlight the comparable performance of our proposed method with a supervised model trained with HI labels.","sentences":["Accurately estimating a Health Index (HI) from condition monitoring data (CM) is essential for reliable and interpretable prognostics and health management (PHM) in complex systems.","In most scenarios, complex systems operate under varying operating conditions and can exhibit different fault modes, making unsupervised inference of an HI from CM data a significant challenge.","Hybrid models combining prior knowledge about degradation with deep learning models have been proposed to overcome this challenge.","However, previously suggested hybrid models for HI estimation usually rely heavily on system-specific information, limiting their transferability to other systems.","In this work, we propose an unsupervised hybrid method for HI estimation that integrates general knowledge about degradation into the convolutional autoencoder's model architecture and learning algorithm, enhancing its applicability across various systems.","The effectiveness of the proposed method is demonstrated in two case studies from different domains: turbofan engines and lithium batteries.","The results show that the proposed method outperforms other competitive alternatives, including residual-based methods, in terms of HI quality and their utility for Remaining Useful Life (RUL) predictions.","The case studies also highlight the comparable performance of our proposed method with a supervised model trained with HI labels."],"url":"http://arxiv.org/abs/2405.04990v1","category":"cs.LG"}
{"created":"2024-05-08 11:47:32","title":"An Artificial Intelligence Approach for Interpreting Creative Combinational Designs","abstract":"Combinational creativity, a form of creativity involving the blending of familiar ideas, is pivotal in design innovation. While most research focuses on how combinational creativity in design is achieved through blending elements, this study focuses on the computational interpretation, specifically identifying the 'base' and 'additive' components that constitute a creative design. To achieve this goal, the authors propose a heuristic algorithm integrating computer vision and natural language processing technologies, and implement multiple approaches based on both discriminative and generative artificial intelligence architectures. A comprehensive evaluation was conducted on a dataset created for studying combinational creativity. Among the implementations of the proposed algorithm, the most effective approach demonstrated a high accuracy in interpretation, achieving 87.5% for identifying 'base' and 80% for 'additive'. We conduct a modular analysis and an ablation experiment to assess the performance of each part in our implementations. Additionally, the study includes an analysis of error cases and bottleneck issues, providing critical insights into the limitations and challenges inherent in the computational interpretation of creative designs.","sentences":["Combinational creativity, a form of creativity involving the blending of familiar ideas, is pivotal in design innovation.","While most research focuses on how combinational creativity in design is achieved through blending elements, this study focuses on the computational interpretation, specifically identifying the 'base' and 'additive' components that constitute a creative design.","To achieve this goal, the authors propose a heuristic algorithm integrating computer vision and natural language processing technologies, and implement multiple approaches based on both discriminative and generative artificial intelligence architectures.","A comprehensive evaluation was conducted on a dataset created for studying combinational creativity.","Among the implementations of the proposed algorithm, the most effective approach demonstrated a high accuracy in interpretation, achieving 87.5% for identifying 'base' and 80% for 'additive'.","We conduct a modular analysis and an ablation experiment to assess the performance of each part in our implementations.","Additionally, the study includes an analysis of error cases and bottleneck issues, providing critical insights into the limitations and challenges inherent in the computational interpretation of creative designs."],"url":"http://arxiv.org/abs/2405.04985v1","category":"cs.AI"}
{"created":"2024-05-08 11:33:19","title":"RF-based Energy Harvesting: Nonlinear Models, Applications and Challenges","abstract":"So far, various aspects associated with wireless energy harvesting (EH) have been investigated from diverse perspectives, including energy sources and models, usage protocols, energy scheduling and optimization, and EH implementation in different wireless communication systems. However, a comprehensive survey specifically focusing on models of radio frequency (RF)-based EH behaviors has not yet been presented. To address this gap, this article provides an overview of the mainstream mathematical models that capture the nonlinear behavior of practical EH circuits, serving as a valuable handbook of mathematical models for EH application research. Moreover, we summarize the application of each nonlinear EH model, including the associated challenges and precautions. We also analyze the impact and advancements of each EH model on RF-based EH systems in wireless communication, utilizing artificial intelligence (AI) techniques. Additionally, we highlight emerging research directions in the context of nonlinear RF-based EH. This article aims to contribute to the future application of RF-based EH in novel communication research domains to a significant extent.","sentences":["So far, various aspects associated with wireless energy harvesting (EH) have been investigated from diverse perspectives, including energy sources and models, usage protocols, energy scheduling and optimization, and EH implementation in different wireless communication systems.","However, a comprehensive survey specifically focusing on models of radio frequency (RF)-based EH behaviors has not yet been presented.","To address this gap, this article provides an overview of the mainstream mathematical models that capture the nonlinear behavior of practical EH circuits, serving as a valuable handbook of mathematical models for EH application research.","Moreover, we summarize the application of each nonlinear EH model, including the associated challenges and precautions.","We also analyze the impact and advancements of each EH model on RF-based EH systems in wireless communication, utilizing artificial intelligence (AI) techniques.","Additionally, we highlight emerging research directions in the context of nonlinear RF-based EH.","This article aims to contribute to the future application of RF-based EH in novel communication research domains to a significant extent."],"url":"http://arxiv.org/abs/2405.04976v1","category":"cs.IT"}
{"created":"2024-05-08 11:32:50","title":"Prototype2Code: End-to-end Front-end Code Generation from UI Design Prototypes","abstract":"UI-to-code technology has streamlined the front-end development process, reducing repetitive tasks for engineers. prior research mainly use design prototypes as inputs, with the effectiveness of the generated code heavily dependent on these prototypes' quality, leading to compromised robustness. Moreover, these approaches also exhibit shortcomings in code quality, including issues such as disorganized UI structures and the inability to support responsive layouts. To address these challenges, we introduce Prototype2Code, which achieves end-to-end front-end code generation with business demands. For Prototype2Code, we incorporate design linting into the workflow, addressing the detection of fragmented elements and perceptual groups, enhancing the robustness of the generated outcomes. By optimizing the hierarchical structure and intelligently recognizing UI element types, Prototype2Code generates code that is more readable and structurally clearer. To meet responsive design requirements, Prototype2Code primarily supports flexbox layout model, ensuring code compatibility across various device sizes. To validate the efficacy, we compare Prototype2Code with the commercial code generation platform CodeFun and Screenshot-to-code based on GPT-4 with vision. Employing structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and mean squared error (MSE) for visual similarity assessment, Prototype2Code's rendered UI effects align most closely with the design prototypes, exhibiting the minimal errors. We also conduct a user study with five experienced front-end engineers, inviting them to review and revise code generated by the three methods. As a result, Prototype2Code surpasses other methods in readability, usability, and maintainability, better meeting the business needs of industrial development.","sentences":["UI-to-code technology has streamlined the front-end development process, reducing repetitive tasks for engineers.","prior research mainly use design prototypes as inputs, with the effectiveness of the generated code heavily dependent on these prototypes' quality, leading to compromised robustness.","Moreover, these approaches also exhibit shortcomings in code quality, including issues such as disorganized UI structures and the inability to support responsive layouts.","To address these challenges, we introduce Prototype2Code, which achieves end-to-end front-end code generation with business demands.","For Prototype2Code, we incorporate design linting into the workflow, addressing the detection of fragmented elements and perceptual groups, enhancing the robustness of the generated outcomes.","By optimizing the hierarchical structure and intelligently recognizing UI element types, Prototype2Code generates code that is more readable and structurally clearer.","To meet responsive design requirements, Prototype2Code primarily supports flexbox layout model, ensuring code compatibility across various device sizes.","To validate the efficacy, we compare Prototype2Code with the commercial code generation platform CodeFun and Screenshot-to-code based on GPT-4 with vision.","Employing structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), and mean squared error (MSE) for visual similarity assessment, Prototype2Code's rendered UI effects align most closely with the design prototypes, exhibiting the minimal errors.","We also conduct a user study with five experienced front-end engineers, inviting them to review and revise code generated by the three methods.","As a result, Prototype2Code surpasses other methods in readability, usability, and maintainability, better meeting the business needs of industrial development."],"url":"http://arxiv.org/abs/2405.04975v1","category":"cs.SE"}
{"created":"2024-05-08 11:26:49","title":"Discrepancy-based Diffusion Models for Lesion Detection in Brain MRI","abstract":"Diffusion probabilistic models (DPMs) have exhibited significant effectiveness in computer vision tasks, particularly in image generation. However, their notable performance heavily relies on labelled datasets, which limits their application in medical images due to the associated high-cost annotations. Current DPM-related methods for lesion detection in medical imaging, which can be categorized into two distinct approaches, primarily rely on image-level annotations. The first approach, based on anomaly detection, involves learning reference healthy brain representations and identifying anomalies based on the difference in inference results. In contrast, the second approach, resembling a segmentation task, employs only the original brain multi-modalities as prior information for generating pixel-level annotations. In this paper, our proposed model - discrepancy distribution medical diffusion (DDMD) - for lesion detection in brain MRI introduces a novel framework by incorporating distinctive discrepancy features, deviating from the conventional direct reliance on image-level annotations or the original brain modalities. In our method, the inconsistency in image-level annotations is translated into distribution discrepancies among heterogeneous samples while preserving information within homogeneous samples. This property retains pixel-wise uncertainty and facilitates an implicit ensemble of segmentation, ultimately enhancing the overall detection performance. Thorough experiments conducted on the BRATS2020 benchmark dataset containing multimodal MRI scans for brain tumour detection demonstrate the great performance of our approach in comparison to state-of-the-art methods.","sentences":["Diffusion probabilistic models (DPMs) have exhibited significant effectiveness in computer vision tasks, particularly in image generation.","However, their notable performance heavily relies on labelled datasets, which limits their application in medical images due to the associated high-cost annotations.","Current DPM-related methods for lesion detection in medical imaging, which can be categorized into two distinct approaches, primarily rely on image-level annotations.","The first approach, based on anomaly detection, involves learning reference healthy brain representations and identifying anomalies based on the difference in inference results.","In contrast, the second approach, resembling a segmentation task, employs only the original brain multi-modalities as prior information for generating pixel-level annotations.","In this paper, our proposed model - discrepancy distribution medical diffusion (DDMD) - for lesion detection in brain MRI introduces a novel framework by incorporating distinctive discrepancy features, deviating from the conventional direct reliance on image-level annotations or the original brain modalities.","In our method, the inconsistency in image-level annotations is translated into distribution discrepancies among heterogeneous samples while preserving information within homogeneous samples.","This property retains pixel-wise uncertainty and facilitates an implicit ensemble of segmentation, ultimately enhancing the overall detection performance.","Thorough experiments conducted on the BRATS2020 benchmark dataset containing multimodal MRI scans for brain tumour detection demonstrate the great performance of our approach in comparison to state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.04974v1","category":"cs.CV"}
{"created":"2024-05-08 11:25:04","title":"Overcoming Anchoring Bias: The Potential of AI and XAI-based Decision Support","abstract":"Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals' decision-making (e.g., by manipulating purchase decisions). Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions. So far, the potential of these technological advances to overcome anchoring bias remains widely unclear. To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias. Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias. Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases.","sentences":["Information systems (IS) are frequently designed to leverage the negative effect of anchoring bias to influence individuals' decision-making (e.g., by manipulating purchase decisions).","Recent advances in Artificial Intelligence (AI) and the explanations of its decisions through explainable AI (XAI) have opened new opportunities for mitigating biased decisions.","So far, the potential of these technological advances to overcome anchoring bias remains widely unclear.","To this end, we conducted two online experiments with a total of N=390 participants in the context of purchase decisions to examine the impact of AI and XAI-based decision support on anchoring bias.","Our results show that AI alone and its combination with XAI help to mitigate the negative effect of anchoring bias.","Ultimately, our findings have implications for the design of AI and XAI-based decision support and IS to overcome cognitive biases."],"url":"http://arxiv.org/abs/2405.04972v1","category":"cs.CY"}
{"created":"2024-05-08 11:15:20","title":"A review on discriminative self-supervised learning methods","abstract":"In the field of computer vision, self-supervised learning has emerged as a method to extract robust features from unlabeled data, where models derive labels autonomously from the data itself, without the need for manual annotation. This paper provides a comprehensive review of discriminative approaches of self-supervised learning within the domain of computer vision, examining their evolution and current status. Through an exploration of various methods including contrastive, self-distillation, knowledge distillation, feature decorrelation, and clustering techniques, we investigate how these approaches leverage the abundance of unlabeled data. Finally, we have comparison of self-supervised learning methods on the standard ImageNet classification benchmark.","sentences":["In the field of computer vision, self-supervised learning has emerged as a method to extract robust features from unlabeled data, where models derive labels autonomously from the data itself, without the need for manual annotation.","This paper provides a comprehensive review of discriminative approaches of self-supervised learning within the domain of computer vision, examining their evolution and current status.","Through an exploration of various methods including contrastive, self-distillation, knowledge distillation, feature decorrelation, and clustering techniques, we investigate how these approaches leverage the abundance of unlabeled data.","Finally, we have comparison of self-supervised learning methods on the standard ImageNet classification benchmark."],"url":"http://arxiv.org/abs/2405.04969v1","category":"cs.CV"}
{"created":"2024-05-08 11:12:37","title":"Communication-Efficient Collaborative Perception via Information Filling with Codebook","abstract":"Collaborative perception empowers each agent to improve its perceptual ability through the exchange of perceptual messages with other agents. It inherently results in a fundamental trade-off between perception ability and communication cost. To address this bottleneck issue, our core idea is to optimize the collaborative messages from two key aspects: representation and selection. The proposed codebook-based message representation enables the transmission of integer codes, rather than high-dimensional feature maps. The proposed information-filling-driven message selection optimizes local messages to collectively fill each agent's information demand, preventing information overflow among multiple agents. By integrating these two designs, we propose CodeFilling, a novel communication-efficient collaborative perception system, which significantly advances the perception-communication trade-off and is inclusive to both homogeneous and heterogeneous collaboration settings. We evaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new simulation dataset, OPV2VH+. Results show that CodeFilling outperforms previous SOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication volume. Our code is available at https://github.com/PhyllisH/CodeFilling.","sentences":["Collaborative perception empowers each agent to improve its perceptual ability through the exchange of perceptual messages with other agents.","It inherently results in a fundamental trade-off between perception ability and communication cost.","To address this bottleneck issue, our core idea is to optimize the collaborative messages from two key aspects: representation and selection.","The proposed codebook-based message representation enables the transmission of integer codes, rather than high-dimensional feature maps.","The proposed information-filling-driven message selection optimizes local messages to collectively fill each agent's information demand, preventing information overflow among multiple agents.","By integrating these two designs, we propose CodeFilling, a novel communication-efficient collaborative perception system, which significantly advances the perception-communication trade-off and is inclusive to both homogeneous and heterogeneous collaboration settings.","We evaluate CodeFilling in both a real-world dataset, DAIR-V2X, and a new simulation dataset, OPV2VH+.","Results show that CodeFilling outperforms previous SOTA Where2comm on DAIR-V2X/OPV2VH+ with 1,333/1,206 times lower communication volume.","Our code is available at https://github.com/PhyllisH/CodeFilling."],"url":"http://arxiv.org/abs/2405.04966v1","category":"cs.IT"}
{"created":"2024-05-08 11:06:40","title":"Audio Matters Too! Enhancing Markerless Motion Capture with Audio Signals for String Performance Capture","abstract":"In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection.","sentences":["In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements.","To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances.","The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow.","Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses.","This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements.","We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues.","Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio.","We validate the proposed framework and conduct ablation studies to demonstrate its efficacy.","Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality.","To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection."],"url":"http://arxiv.org/abs/2405.04963v1","category":"cs.MM"}
{"created":"2024-05-08 10:49:39","title":"Improving Long Text Understanding with Knowledge Distilled from Summarization Model","abstract":"Long text understanding is important yet challenging for natural language processing. A long article or document usually contains many redundant words that are not pertinent to its gist and sometimes can be regarded as noise. With recent advances of abstractive summarization, we propose our \\emph{Gist Detector} to leverage the gist detection ability of a summarization model and integrate the extracted gist into downstream models to enhance their long text understanding ability. Specifically, Gist Detector first learns the gist detection knowledge distilled from a summarization model, and then produces gist-aware representations to augment downstream models. We evaluate our method on three different tasks: long document classification, distantly supervised open-domain question answering, and non-parallel text style transfer. The experimental results show that our method can significantly improve the performance of baseline models on all tasks.","sentences":["Long text understanding is important yet challenging for natural language processing.","A long article or document usually contains many redundant words that are not pertinent to its gist and sometimes can be regarded as noise.","With recent advances of abstractive summarization, we propose our \\emph{Gist Detector} to leverage the gist detection ability of a summarization model and integrate the extracted gist into downstream models to enhance their long text understanding ability.","Specifically, Gist Detector first learns the gist detection knowledge distilled from a summarization model, and then produces gist-aware representations to augment downstream models.","We evaluate our method on three different tasks: long document classification, distantly supervised open-domain question answering, and non-parallel text style transfer.","The experimental results show that our method can significantly improve the performance of baseline models on all tasks."],"url":"http://arxiv.org/abs/2405.04955v1","category":"cs.CL"}
{"created":"2024-05-08 10:42:48","title":"VisionGraph: Leveraging Large Multimodal Models for Graph Theory Problems in Visual Context","abstract":"Large Multimodal Models (LMMs) have achieved impressive success in visual understanding and reasoning, remarkably improving the performance of mathematical reasoning in a visual context. Yet, a challenging type of visual math lies in the multimodal graph theory problem, which demands that LMMs understand the graphical structures accurately and perform multi-step reasoning on the visual graph. Additionally, exploring multimodal graph theory problems will lead to more effective strategies in fields like biology, transportation, and robotics planning. To step forward in this direction, we are the first to design a benchmark named VisionGraph, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems. It encompasses eight complex graph problem tasks, from connectivity to shortest path problems. Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning. Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance.","sentences":["Large Multimodal Models (LMMs) have achieved impressive success in visual understanding and reasoning, remarkably improving the performance of mathematical reasoning in a visual context.","Yet, a challenging type of visual math lies in the multimodal graph theory problem, which demands that LMMs understand the graphical structures accurately and perform multi-step reasoning on the visual graph.","Additionally, exploring multimodal graph theory problems will lead to more effective strategies in fields like biology, transportation, and robotics planning.","To step forward in this direction, we are the first to design a benchmark named VisionGraph, used to explore the capabilities of advanced LMMs in solving multimodal graph theory problems.","It encompasses eight complex graph problem tasks, from connectivity to shortest path problems.","Subsequently, we present a Description-Program-Reasoning (DPR) chain to enhance the logical accuracy of reasoning processes through graphical structure description generation and algorithm-aware multi-step reasoning.","Our extensive study shows that 1) GPT-4V outperforms Gemini Pro in multi-step graph reasoning; 2) All LMMs exhibit inferior perception accuracy for graphical structures, whether in zero/few-shot settings or with supervised fine-tuning (SFT), which further affects problem-solving performance; 3) DPR significantly improves the multi-step graph reasoning capabilities of LMMs and the GPT-4V (DPR) agent achieves SOTA performance."],"url":"http://arxiv.org/abs/2405.04950v1","category":"cs.CV"}
{"created":"2024-05-08 10:22:49","title":"Imprecise Probabilities Meet Partial Observability: Game Semantics for Robust POMDPs","abstract":"Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known. Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets. While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods. We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values. These novel semantics for RPOMDPS give access to results for the widely studied POSG model; concretely, we show the existence of a Nash equilibrium. Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate.","sentences":["Partially observable Markov decision processes (POMDPs) rely on the key assumption that probability distributions are precisely known.","Robust POMDPs (RPOMDPs) alleviate this concern by defining imprecise probabilities, referred to as uncertainty sets.","While robust MDPs have been studied extensively, work on RPOMDPs is limited and primarily focuses on algorithmic solution methods.","We expand the theoretical understanding of RPOMDPs by showing that 1) different assumptions on the uncertainty sets affect optimal policies and values; 2) RPOMDPs have a partially observable stochastic game (POSG) semantic; and 3) the same RPOMDP with different assumptions leads to semantically different POSGs and, thus, different policies and values.","These novel semantics for RPOMDPS give access to results for the widely studied POSG model; concretely, we show the existence of a Nash equilibrium.","Finally, we classify the existing RPOMDP literature using our semantics, clarifying under which uncertainty assumptions these existing works operate."],"url":"http://arxiv.org/abs/2405.04941v1","category":"cs.AI"}
{"created":"2024-05-08 10:08:45","title":"Developing trustworthy AI applications with foundation models","abstract":"The trustworthiness of AI applications has been the subject of recent research and is also addressed in the EU's recently adopted AI Regulation. The currently emerging foundation models in the field of text, speech and image processing offer completely new possibilities for developing AI applications. This whitepaper shows how the trustworthiness of an AI application developed with foundation models can be evaluated and ensured. For this purpose, the application-specific, risk-based approach for testing and ensuring the trustworthiness of AI applications, as developed in the 'AI Assessment Catalog - Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is transferred to the context of foundation models. Special consideration is given to the fact that specific risks of foundation models can have an impact on the AI application and must also be taken into account when checking trustworthiness. Chapter 1 of the white paper explains the fundamental relationship between foundation models and AI applications based on them in terms of trustworthiness. Chapter 2 provides an introduction to the technical construction of foundation models and Chapter 3 shows how AI applications can be developed based on them. Chapter 4 provides an overview of the resulting risks regarding trustworthiness. Chapter 5 shows which requirements for AI applications and foundation models are to be expected according to the draft of the European Union's AI Regulation and Chapter 6 finally shows the system and procedure for meeting trustworthiness requirements.","sentences":["The trustworthiness of AI applications has been the subject of recent research and is also addressed in the EU's recently adopted AI Regulation.","The currently emerging foundation models in the field of text, speech and image processing offer completely new possibilities for developing AI applications.","This whitepaper shows how the trustworthiness of an AI application developed with foundation models can be evaluated and ensured.","For this purpose, the application-specific, risk-based approach for testing and ensuring the trustworthiness of AI applications, as developed in the 'AI Assessment Catalog - Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is transferred to the context of foundation models.","Special consideration is given to the fact that specific risks of foundation models can have an impact on the AI application and must also be taken into account when checking trustworthiness.","Chapter 1 of the white paper explains the fundamental relationship between foundation models and AI applications based on them in terms of trustworthiness.","Chapter 2 provides an introduction to the technical construction of foundation models and Chapter 3 shows how AI applications can be developed based on them.","Chapter 4 provides an overview of the resulting risks regarding trustworthiness.","Chapter 5 shows which requirements for AI applications and foundation models are to be expected according to the draft of the European Union's AI Regulation and Chapter 6 finally shows the system and procedure for meeting trustworthiness requirements."],"url":"http://arxiv.org/abs/2405.04937v1","category":"cs.AI"}
{"created":"2024-05-08 09:45:54","title":"DataSP: A Differential All-to-All Shortest Path Algorithm for Learning Costs and Predicting Paths with Context","abstract":"Learning latent costs of transitions on graphs from trajectories demonstrations under various contextual features is challenging but useful for path planning. Yet, existing methods either oversimplify cost assumptions or scale poorly with the number of observed trajectories. This paper introduces DataSP, a differentiable all-to-all shortest path algorithm to facilitate learning latent costs from trajectories. It allows to learn from a large number of trajectories in each learning step without additional computation. Complex latent cost functions from contextual features can be represented in the algorithm through a neural network approximation. We further propose a method to sample paths from DataSP in order to reconstruct/mimic observed paths' distributions. We prove that the inferred distribution follows the maximum entropy principle. We show that DataSP outperforms state-of-the-art differentiable combinatorial solver and classical machine learning approaches in predicting paths on graphs.","sentences":["Learning latent costs of transitions on graphs from trajectories demonstrations under various contextual features is challenging but useful for path planning.","Yet, existing methods either oversimplify cost assumptions or scale poorly with the number of observed trajectories.","This paper introduces DataSP, a differentiable all-to-all shortest path algorithm to facilitate learning latent costs from trajectories.","It allows to learn from a large number of trajectories in each learning step without additional computation.","Complex latent cost functions from contextual features can be represented in the algorithm through a neural network approximation.","We further propose a method to sample paths from DataSP in order to reconstruct/mimic observed paths' distributions.","We prove that the inferred distribution follows the maximum entropy principle.","We show that DataSP outperforms state-of-the-art differentiable combinatorial solver and classical machine learning approaches in predicting paths on graphs."],"url":"http://arxiv.org/abs/2405.04923v1","category":"cs.LG"}
{"created":"2024-05-08 09:42:08","title":"Impact of phylogeny on the inference of functional sectors from protein sequence data","abstract":"Statistical analysis of multiple sequence alignments of homologous proteins has revealed groups of coevolving amino acids called sectors. These groups of amino-acid sites feature collective correlations in their amino-acid usage, and they are associated to functional properties. Modeling showed that natural selection on an additive functional trait of a protein is generically expected to give rise to a functional sector. These modeling results motivated a principled method, called ICOD, which is designed to identify functional sectors, as well as mutational effects, from sequence data. However, a challenge for all methods aiming to identify sectors from multiple sequence alignments is that correlations in amino-acid usage can also arise from the mere fact that homologous sequences share common ancestry, i.e. from phylogeny. Here, we generate controlled synthetic data from a minimal model comprising both phylogeny and functional sectors. We use this data to dissect the impact of phylogeny on sector identification and on mutational effect inference by different methods. We find that ICOD is most robust to phylogeny, but that conservation is also quite robust. Next, we consider natural multiple sequence alignments of protein families for which deep mutational scan experimental data is available. We show that in this natural data, conservation and ICOD best identify sites with strong functional roles, in agreement with our results on synthetic data. Importantly, these two methods have different premises, since they respectively focus on conservation and on correlations. Thus, their joint use can reveal complementary information.","sentences":["Statistical analysis of multiple sequence alignments of homologous proteins has revealed groups of coevolving amino acids called sectors.","These groups of amino-acid sites feature collective correlations in their amino-acid usage, and they are associated to functional properties.","Modeling showed that natural selection on an additive functional trait of a protein is generically expected to give rise to a functional sector.","These modeling results motivated a principled method, called ICOD, which is designed to identify functional sectors, as well as mutational effects, from sequence data.","However, a challenge for all methods aiming to identify sectors from multiple sequence alignments is that correlations in amino-acid usage can also arise from the mere fact that homologous sequences share common ancestry, i.e. from phylogeny.","Here, we generate controlled synthetic data from a minimal model comprising both phylogeny and functional sectors.","We use this data to dissect the impact of phylogeny on sector identification and on mutational effect inference by different methods.","We find that ICOD is most robust to phylogeny, but that conservation is also quite robust.","Next, we consider natural multiple sequence alignments of protein families for which deep mutational scan experimental data is available.","We show that in this natural data, conservation and ICOD best identify sites with strong functional roles, in agreement with our results on synthetic data.","Importantly, these two methods have different premises, since they respectively focus on conservation and on correlations.","Thus, their joint use can reveal complementary information."],"url":"http://arxiv.org/abs/2405.04920v1","category":"q-bio.PE"}
{"created":"2024-05-08 09:38:16","title":"Delve into Base-Novel Confusion: Redundancy Exploration for Few-Shot Class-Incremental Learning","abstract":"Few-shot class-incremental learning (FSCIL) aims to acquire knowledge from novel classes with limited samples while retaining information about base classes. Existing methods address catastrophic forgetting and overfitting by freezing the feature extractor during novel-class learning. However, these methods usually tend to cause the confusion between base and novel classes, i.e., classifying novel-class samples into base classes. In this paper, we delve into this phenomenon to study its cause and solution. We first interpret the confusion as the collision between the novel-class and the base-class region in the feature space. Then, we find the collision is caused by the label-irrelevant redundancies within the base-class feature and pixel space. Through qualitative and quantitative experiments, we identify this redundancy as the shortcut in the base-class training, which can be decoupled to alleviate the collision. Based on this analysis, to alleviate the collision between base and novel classes, we propose a method for FSCIL named Redundancy Decoupling and Integration (RDI). RDI first decouples redundancies from base-class space to shrink the intra-base-class feature space. Then, it integrates the redundancies as a dummy class to enlarge the inter-base-class feature space. This process effectively compresses the base-class feature space, creating buffer space for novel classes and alleviating the model's confusion between the base and novel classes. Extensive experiments across benchmark datasets, including CIFAR-100, miniImageNet, and CUB-200-2011 demonstrate that our method achieves state-of-the-art performance.","sentences":["Few-shot class-incremental learning (FSCIL) aims to acquire knowledge from novel classes with limited samples while retaining information about base classes.","Existing methods address catastrophic forgetting and overfitting by freezing the feature extractor during novel-class learning.","However, these methods usually tend to cause the confusion between base and novel classes, i.e., classifying novel-class samples into base classes.","In this paper, we delve into this phenomenon to study its cause and solution.","We first interpret the confusion as the collision between the novel-class and the base-class region in the feature space.","Then, we find the collision is caused by the label-irrelevant redundancies within the base-class feature and pixel space.","Through qualitative and quantitative experiments, we identify this redundancy as the shortcut in the base-class training, which can be decoupled to alleviate the collision.","Based on this analysis, to alleviate the collision between base and novel classes, we propose a method for FSCIL named Redundancy Decoupling and Integration (RDI).","RDI first decouples redundancies from base-class space to shrink the intra-base-class feature space.","Then, it integrates the redundancies as a dummy class to enlarge the inter-base-class feature space.","This process effectively compresses the base-class feature space, creating buffer space for novel classes and alleviating the model's confusion between the base and novel classes.","Extensive experiments across benchmark datasets, including CIFAR-100, miniImageNet, and CUB-200-2011 demonstrate that our method achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.04918v1","category":"cs.CV"}
{"created":"2024-05-08 09:35:33","title":"ATLAS searches for additional scalars and exotic Higgs boson decays with the LHC Run 2 dataset","abstract":"This report reviews the published results of searches for possible additional scalar particles and exotic decays of the Higgs boson performed by the ATLAS Collaboration using up to 140 fb$^{-1}$ of 13 TeV proton-proton collision data collected during Run 2 of the Large Hadron Collider. Key results are examined, and observed excesses, while never statistically compelling, are noted. Constraints are placed on parameters of several models which extend the Standard Model, for example by adding one or more singlet or doublet fields, or offering exotic Higgs boson decay channels. Summaries of new searches as well as extensions of previous searches are discussed. These new results have a wider reach or attain stronger exclusion limits. New experimental techniques that were developed for these searches are highlighted. Search channels which have not yet been examined are also listed, as these provide insight into possible future areas of exploration.","sentences":["This report reviews the published results of searches for possible additional scalar particles and exotic decays of the Higgs boson performed by the ATLAS Collaboration using up to 140 fb$^{-1}$ of 13 TeV proton-proton collision data collected during Run 2 of the Large Hadron Collider.","Key results are examined, and observed excesses, while never statistically compelling, are noted.","Constraints are placed on parameters of several models which extend the Standard Model, for example by adding one or more singlet or doublet fields, or offering exotic Higgs boson decay channels.","Summaries of new searches as well as extensions of previous searches are discussed.","These new results have a wider reach or attain stronger exclusion limits.","New experimental techniques that were developed for these searches are highlighted.","Search channels which have not yet been examined are also listed, as these provide insight into possible future areas of exploration."],"url":"http://arxiv.org/abs/2405.04914v1","category":"hep-ex"}
{"created":"2024-05-08 09:28:26","title":"Learning with Posterior Sampling for Revenue Management under Time-varying Demand","abstract":"This paper discusses the revenue management (RM) problem to maximize revenue by pricing items or services. One challenge in this problem is that the demand distribution is unknown and varies over time in real applications such as airline and retail industries. In particular, the time-varying demand has not been well studied under scenarios of unknown demand due to the difficulty of jointly managing the remaining inventory and estimating the demand. To tackle this challenge, we first introduce an episodic generalization of the RM problem motivated by typical application scenarios. We then propose a computationally efficient algorithm based on posterior sampling, which effectively optimizes prices by solving linear programming. We derive a Bayesian regret upper bound of this algorithm for general models where demand parameters can be correlated between time periods, while also deriving a regret lower bound for generic algorithms. Our empirical study shows that the proposed algorithm performs better than other benchmark algorithms and comparably to the optimal policy in hindsight. We also propose a heuristic modification of the proposed algorithm, which further efficiently learns the pricing policy in the experiments.","sentences":["This paper discusses the revenue management (RM) problem to maximize revenue by pricing items or services.","One challenge in this problem is that the demand distribution is unknown and varies over time in real applications such as airline and retail industries.","In particular, the time-varying demand has not been well studied under scenarios of unknown demand due to the difficulty of jointly managing the remaining inventory and estimating the demand.","To tackle this challenge, we first introduce an episodic generalization of the RM problem motivated by typical application scenarios.","We then propose a computationally efficient algorithm based on posterior sampling, which effectively optimizes prices by solving linear programming.","We derive a Bayesian regret upper bound of this algorithm for general models where demand parameters can be correlated between time periods, while also deriving a regret lower bound for generic algorithms.","Our empirical study shows that the proposed algorithm performs better than other benchmark algorithms and comparably to the optimal policy in hindsight.","We also propose a heuristic modification of the proposed algorithm, which further efficiently learns the pricing policy in the experiments."],"url":"http://arxiv.org/abs/2405.04910v1","category":"cs.LG"}
{"created":"2024-05-08 09:28:04","title":"Traj-LLM: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models","abstract":"Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving. Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of the complex traffic semantics. This paper proposes Traj-LLM, the first to investigate the potential of using Large Language Models (LLMs) without explicit prompt engineering to generate future motion from agents' past/observed trajectories and scene semantics. Traj-LLM starts with sparse context joint coding to dissect the agent and scene features into a form that LLMs understand. On this basis, we innovatively explore LLMs' powerful comprehension abilities to capture a spectrum of high-level scene knowledge and interactive information. Emulating the human-like lane focus cognitive function and enhancing Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the pioneering Mamba module. Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant multi-modal predictions. Extensive experiments manifest that Traj-LLM, fortified by LLMs' strong prior knowledge and understanding prowess, together with lane-aware probability learning, outstrips state-of-the-art methods across evaluation metrics. Moreover, the few-shot analysis further substantiates Traj-LLM's performance, wherein with just 50% of the dataset, it outperforms the majority of benchmarks relying on complete data utilization. This study explores equipping the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent motion in a new way.","sentences":["Predicting the future trajectories of dynamic traffic actors is a cornerstone task in autonomous driving.","Though existing notable efforts have resulted in impressive performance improvements, a gap persists in scene cognitive and understanding of the complex traffic semantics.","This paper proposes Traj-LLM, the first to investigate the potential of using Large Language Models (LLMs) without explicit prompt engineering to generate future motion from agents' past/observed trajectories and scene semantics.","Traj-LLM starts with sparse context joint coding to dissect the agent and scene features into a form that LLMs understand.","On this basis, we innovatively explore LLMs' powerful comprehension abilities to capture a spectrum of high-level scene knowledge and interactive information.","Emulating the human-like lane focus cognitive function and enhancing Traj-LLM's scene comprehension, we introduce lane-aware probabilistic learning powered by the pioneering Mamba module.","Finally, a multi-modal Laplace decoder is designed to achieve scene-compliant multi-modal predictions.","Extensive experiments manifest that Traj-LLM, fortified by LLMs' strong prior knowledge and understanding prowess, together with lane-aware probability learning, outstrips state-of-the-art methods across evaluation metrics.","Moreover, the few-shot analysis further substantiates Traj-LLM's performance, wherein with just 50% of the dataset, it outperforms the majority of benchmarks relying on complete data utilization.","This study explores equipping the trajectory prediction task with advanced capabilities inherent in LLMs, furnishing a more universal and adaptable solution for forecasting agent motion in a new way."],"url":"http://arxiv.org/abs/2405.04909v1","category":"cs.CV"}
{"created":"2024-05-08 09:24:51","title":"Empowering Wireless Networks with Artificial Intelligence Generated Graph","abstract":"In wireless communications, transforming network into graphs and processing them using deep learning models, such as Graph Neural Networks (GNNs), is one of the mainstream network optimization approaches. While effective, the generative AI (GAI) shows stronger capabilities in graph analysis, processing, and generation, than conventional methods such as GNN, offering a broader exploration space for graph-based network optimization. Therefore, this article proposes to use GAI-based graph generation to support wireless networks. Specifically, we first explore applications of graphs in wireless networks. Then, we introduce and analyze common GAI models from the perspective of graph generation. On this basis, we propose a framework that incorporates the conditional diffusion model and an evaluation network, which can be trained with reward functions and conditions customized by network designers and users. Once trained, the proposed framework can create graphs based on new conditions, helping to tackle problems specified by the user in wireless networks. Finally, using the link selection in integrated sensing and communication (ISAC) as an example, the effectiveness of the proposed framework is validated.","sentences":["In wireless communications, transforming network into graphs and processing them using deep learning models, such as Graph Neural Networks (GNNs), is one of the mainstream network optimization approaches.","While effective, the generative AI (GAI) shows stronger capabilities in graph analysis, processing, and generation, than conventional methods such as GNN, offering a broader exploration space for graph-based network optimization.","Therefore, this article proposes to use GAI-based graph generation to support wireless networks.","Specifically, we first explore applications of graphs in wireless networks.","Then, we introduce and analyze common GAI models from the perspective of graph generation.","On this basis, we propose a framework that incorporates the conditional diffusion model and an evaluation network, which can be trained with reward functions and conditions customized by network designers and users.","Once trained, the proposed framework can create graphs based on new conditions, helping to tackle problems specified by the user in wireless networks.","Finally, using the link selection in integrated sensing and communication (ISAC) as an example, the effectiveness of the proposed framework is validated."],"url":"http://arxiv.org/abs/2405.04907v1","category":"cs.NI"}
{"created":"2024-05-08 09:20:58","title":"Dependence-based fuzzy clustering of functional time series","abstract":"Time series clustering is an important data mining task with a wide variety of applications. While most methods focus on time series taking values on the real line, very few works consider functional time series. However, functional objects frequently arise in many fields, such as actuarial science, demography or finance. Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space. In this paper, the problem of clustering functional time series is addressed. To this aim, a distance between functional time series is introduced and used to construct a clustering procedure. The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting. Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning. Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient. Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach.","sentences":["Time series clustering is an important data mining task with a wide variety of applications.","While most methods focus on time series taking values on the real line, very few works consider functional time series.","However, functional objects frequently arise in many fields, such as actuarial science, demography or finance.","Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space.","In this paper, the problem of clustering functional time series is addressed.","To this aim, a distance between functional time series is introduced and used to construct a clustering procedure.","The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting.","Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees.","The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning.","Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient.","Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach."],"url":"http://arxiv.org/abs/2405.04904v1","category":"stat.ME"}
{"created":"2024-05-08 09:13:42","title":"HAGAN: Hybrid Augmented Generative Adversarial Network for Medical Image Synthesis","abstract":"Medical Image Synthesis (MIS) plays an important role in the intelligent medical field, which greatly saves the economic and time costs of medical diagnosis. However, due to the complexity of medical images and similar characteristics of different tissue cells, existing methods face great challenges in meeting their biological consistency. To this end, we propose the Hybrid Augmented Generative Adversarial Network (HAGAN) to maintain the authenticity of structural texture and tissue cells. HAGAN contains Attention Mixed (AttnMix) Generator, Hierarchical Discriminator and Reverse Skip Connection between Discriminator and Generator. The AttnMix consistency differentiable regularization encourages the perception in structural and textural variations between real and fake images, which improves the pathological integrity of synthetic images and the accuracy of features in local areas. The Hierarchical Discriminator introduces pixel-by-pixel discriminant feedback to generator for enhancing the saliency and discriminance of global and local details simultaneously. The Reverse Skip Connection further improves the accuracy for fine details by fusing real and synthetic distribution features. Our experimental evaluations on three datasets of different scales, i.e., COVID-CT, ACDC and BraTS2018, demonstrate that HAGAN outperforms the existing methods and achieves state-of-the-art performance in both high-resolution and low-resolution.","sentences":["Medical Image Synthesis (MIS) plays an important role in the intelligent medical field, which greatly saves the economic and time costs of medical diagnosis.","However, due to the complexity of medical images and similar characteristics of different tissue cells, existing methods face great challenges in meeting their biological consistency.","To this end, we propose the Hybrid Augmented Generative Adversarial Network (HAGAN) to maintain the authenticity of structural texture and tissue cells.","HAGAN contains Attention Mixed (AttnMix) Generator, Hierarchical Discriminator and Reverse Skip Connection between Discriminator and Generator.","The AttnMix consistency differentiable regularization encourages the perception in structural and textural variations between real and fake images, which improves the pathological integrity of synthetic images and the accuracy of features in local areas.","The Hierarchical Discriminator introduces pixel-by-pixel discriminant feedback to generator for enhancing the saliency and discriminance of global and local details simultaneously.","The Reverse Skip Connection further improves the accuracy for fine details by fusing real and synthetic distribution features.","Our experimental evaluations on three datasets of different scales, i.e., COVID-CT, ACDC and BraTS2018, demonstrate that HAGAN outperforms the existing methods and achieves state-of-the-art performance in both high-resolution and low-resolution."],"url":"http://arxiv.org/abs/2405.04902v1","category":"eess.IV"}
{"created":"2024-05-08 09:13:13","title":"Development of a method to measure trace level of uranium and thorium in scintillation films","abstract":"We have established a method to measure picograms-per-gram (pg g$^{-1}$) levels of $^{238}$U and $^{232}$Th in scintillation films by combining the dry ashing method and inductively coupled plasma mass spectrometry. Trace amounts of $^{238}$U and $^{232}$Th were measured in up to 2~g of the scintillation film with almost 100% collection efficiency. This paper details the experimental procedure, including the pretreatment of the samples and labware, detection limit of the method, collection efficiencies of $^{238}$U and $^{232}$Th, and measurement of $^{238}$U and $^{232}$Th in a polyethylene naphthalate film. This method is also applicable to $^{238}$U and $^{232}$Th measurements in other low-background organic materials for rare event search experiments.","sentences":["We have established a method to measure picograms-per-gram (pg g$^{-1}$) levels of $^{238}$U and $^{232}$Th in scintillation films by combining the dry ashing method and inductively coupled plasma mass spectrometry.","Trace amounts of $^{238}$U and $^{232}$Th were measured in up to 2~g of the scintillation film with almost 100% collection efficiency.","This paper details the experimental procedure, including the pretreatment of the samples and labware, detection limit of the method, collection efficiencies of $^{238}$U and $^{232}$Th, and measurement of $^{238}$U and $^{232}$Th in a polyethylene naphthalate film.","This method is also applicable to $^{238}$U and $^{232}$Th measurements in other low-background organic materials for rare event search experiments."],"url":"http://arxiv.org/abs/2405.04901v1","category":"physics.ins-det"}
{"created":"2024-05-08 09:05:02","title":"Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset","abstract":"Recent social media posts on the cholera outbreak in Hammanskraal have highlighted the diverse range of emotions people experienced in response to such an event. The extent of people's opinions varies greatly depending on their level of knowledge and information about the disease. The documented re-search about Cholera lacks investigations into the classification of emotions. This study aims to examine the emotions expressed in social media posts about Chol-era. A dataset of 23,000 posts was extracted and pre-processed. The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer library was applied to deter-mine the emotional significance of each text. Additionally, Machine Learning (ML) models were applied for emotion classification, including Long short-term memory (LSTM), Logistic regression, Decision trees, and the Bidirectional En-coder Representations from Transformers (BERT) model. The results of this study demonstrated that LSTM achieved the highest accuracy of 75%. Emotion classification presents a promising tool for gaining a deeper understanding of the impact of Cholera on society. The findings of this study might contribute to the development of effective interventions in public health strategies.","sentences":["Recent social media posts on the cholera outbreak in Hammanskraal have highlighted the diverse range of emotions people experienced in response to such an event.","The extent of people's opinions varies greatly depending on their level of knowledge and information about the disease.","The documented re-search about Cholera lacks investigations into the classification of emotions.","This study aims to examine the emotions expressed in social media posts about Chol-era.","A dataset of 23,000 posts was extracted and pre-processed.","The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer library was applied to deter-mine the emotional significance of each text.","Additionally, Machine Learning (ML) models were applied for emotion classification, including Long short-term memory (LSTM), Logistic regression, Decision trees, and the Bidirectional En-coder Representations from Transformers (BERT) model.","The results of this study demonstrated that LSTM achieved the highest accuracy of 75%.","Emotion classification presents a promising tool for gaining a deeper understanding of the impact of Cholera on society.","The findings of this study might contribute to the development of effective interventions in public health strategies."],"url":"http://arxiv.org/abs/2405.04897v1","category":"cs.CL"}
{"created":"2024-05-08 09:03:48","title":"On Correlation and Prediction Interval Reduction","abstract":"Pearson's correlation coefficient is a popular statistical measure to summarize the strength of association between two continuous variables. It is usually interpreted via its square as percentage of variance of one variable predicted by the other in a linear regression model. It can be generalized for multiple regression via the coefficient of determination, which is not straightforward to interpret in terms of prediction accuracy. In this paper, we propose to assess the prediction accuracy of a linear model via the prediction interval reduction (PIR) by comparing the width of the prediction interval derived from this model with the width of the prediction interval obtained without this model. At the population level, PIR is one-to-one related to the correlation and the coefficient of determination. In particular, a correlation of 0.5 corresponds to a PIR of only 13%. It is also the one's complement of the coefficient of alienation introduced at the beginning of last century. We argue that PIR is easily interpretable and useful to keep in mind how difficult it is to make accurate individual predictions, an important message in the era of precision medicine and artificial intelligence. Different estimates of PIR are compared in the context of a linear model and an extension of the PIR concept to non-linear models is outlined.","sentences":["Pearson's correlation coefficient is a popular statistical measure to summarize the strength of association between two continuous variables.","It is usually interpreted via its square as percentage of variance of one variable predicted by the other in a linear regression model.","It can be generalized for multiple regression via the coefficient of determination, which is not straightforward to interpret in terms of prediction accuracy.","In this paper, we propose to assess the prediction accuracy of a linear model via the prediction interval reduction (PIR) by comparing the width of the prediction interval derived from this model with the width of the prediction interval obtained without this model.","At the population level, PIR is one-to-one related to the correlation and the coefficient of determination.","In particular, a correlation of 0.5 corresponds to a PIR of only 13%.","It is also the one's complement of the coefficient of alienation introduced at the beginning of last century.","We argue that PIR is easily interpretable and useful to keep in mind how difficult it is to make accurate individual predictions, an important message in the era of precision medicine and artificial intelligence.","Different estimates of PIR are compared in the context of a linear model and an extension of the PIR concept to non-linear models is outlined."],"url":"http://arxiv.org/abs/2405.04895v1","category":"stat.ME"}
{"created":"2024-05-08 08:39:25","title":"GISR: Geometric Initialization and Silhouette-based Refinement for Single-View Robot Pose and Configuration Estimation","abstract":"For autonomous robotics applications, it is crucial that robots are able to accurately measure their potential state and perceive their environment, including other agents within it (e.g., cobots interacting with humans). The redundancy of these measurements is important, as it allows for planning and execution of recovery protocols in the event of sensor failure or external disturbances. Visual estimation can provide this redundancy through the use of low-cost sensors and server as a standalone source of proprioception when no encoder-based sensing is available. Therefore, we estimate the configuration of the robot jointly with its pose, which provides a complete spatial understanding of the observed robot. We present GISR - a method for deep configuration and robot-to-camera pose estimation that prioritizes real-time execution. GISR is comprised of two modules: (i) a geometric initialization module, efficiently computing an approximate robot pose and configuration, and (ii) an iterative silhouette-based refinement module that refines the initial solution in only a few iterations. We evaluate our method on a publicly available dataset and show that GISR performs competitively with existing state-of-the-art approaches, while being significantly faster compared to existing methods of the same class. Our code is available at https://github.com/iwhitey/GISR-robot.","sentences":["For autonomous robotics applications, it is crucial that robots are able to accurately measure their potential state and perceive their environment, including other agents within it (e.g., cobots interacting with humans).","The redundancy of these measurements is important, as it allows for planning and execution of recovery protocols in the event of sensor failure or external disturbances.","Visual estimation can provide this redundancy through the use of low-cost sensors and server as a standalone source of proprioception when no encoder-based sensing is available.","Therefore, we estimate the configuration of the robot jointly with its pose, which provides a complete spatial understanding of the observed robot.","We present GISR - a method for deep configuration and robot-to-camera pose estimation that prioritizes real-time execution.","GISR is comprised of two modules: (i) a geometric initialization module, efficiently computing an approximate robot pose and configuration, and (ii) an iterative silhouette-based refinement module that refines the initial solution in only a few iterations.","We evaluate our method on a publicly available dataset and show that GISR performs competitively with existing state-of-the-art approaches, while being significantly faster compared to existing methods of the same class.","Our code is available at https://github.com/iwhitey/GISR-robot."],"url":"http://arxiv.org/abs/2405.04890v1","category":"cs.RO"}
{"created":"2024-05-08 08:32:34","title":"Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion","abstract":"Unified multi-model representation spaces are the foundation of multimodal understanding and generation. However, the billions of model parameters and catastrophic forgetting problems make it challenging to further enhance pre-trained unified spaces. In this work, we propose Molecule-Space, an idea that treats multimodal representation spaces as \"molecules\", and augments pre-trained unified space by integrating knowledge from extra expert spaces via \"molecules space reactions\". Specifically, we introduce two kinds of basic space reactions: 1) Space Displacement Reaction and 2) Space Combination Reaction. Based on these defined basic reactions, we design Complex Sequential & Parallel Reactions to effectively integrate multiple spaces simultaneously. Benefiting from the modularization concept, we further propose a coarse-to-fine customized inference strategy to flexibly adjust the enhanced unified space for different purposes. Experimentally, we fuse the audio-image-text space of ImageBind with the image-text and audio-text expert spaces. The resulting space outperforms ImageBind on 5 downstream tasks across 9 datasets. Moreover, via customized inference, it even surpasses the used image-text and audio-text expert spaces.","sentences":["Unified multi-model representation spaces are the foundation of multimodal understanding and generation.","However, the billions of model parameters and catastrophic forgetting problems make it challenging to further enhance pre-trained unified spaces.","In this work, we propose Molecule-Space, an idea that treats multimodal representation spaces as \"molecules\", and augments pre-trained unified space by integrating knowledge from extra expert spaces via \"molecules space reactions\".","Specifically, we introduce two kinds of basic space reactions: 1) Space Displacement Reaction and 2) Space Combination Reaction.","Based on these defined basic reactions, we design Complex Sequential & Parallel Reactions to effectively integrate multiple spaces simultaneously.","Benefiting from the modularization concept, we further propose a coarse-to-fine customized inference strategy to flexibly adjust the enhanced unified space for different purposes.","Experimentally, we fuse the audio-image-text space of ImageBind with the image-text and audio-text expert spaces.","The resulting space outperforms ImageBind on 5 downstream tasks across 9 datasets.","Moreover, via customized inference, it even surpasses the used image-text and audio-text expert spaces."],"url":"http://arxiv.org/abs/2405.04883v1","category":"cs.CV"}
{"created":"2024-05-08 08:28:40","title":"The Codecfake Dataset and Countermeasures for the Universally Detection of Deepfake Audio","abstract":"With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for effective detection methods. Unlike traditional deepfake audio generation, which often involves multi-step processes culminating in vocoder usage, ALM directly utilizes neural codec methods to decode discrete codes into audio. Moreover, driven by large-scale data, ALMs exhibit remarkable robustness and versatility, posing a significant challenge to current audio deepfake detection (ADD) models. To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform. We initially construct the Codecfake dataset, an open-source large-scale dataset, including two languages, millions of audio samples, and various test conditions, tailored for ALM-based audio detection. Additionally, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original SAM, we propose the CSAM strategy to learn a domain balanced and generalized minima. Experiment results demonstrate that co-training on Codecfake dataset and vocoded dataset with CSAM strategy yield the lowest average Equal Error Rate (EER) of 0.616% across all test conditions compared to baseline models.","sentences":["With the proliferation of Audio Language Model (ALM) based deepfake audio, there is an urgent need for effective detection methods.","Unlike traditional deepfake audio generation, which often involves multi-step processes culminating in vocoder usage, ALM directly utilizes neural codec methods to decode discrete codes into audio.","Moreover, driven by large-scale data, ALMs exhibit remarkable robustness and versatility, posing a significant challenge to current audio deepfake detection (ADD) models.","To effectively detect ALM-based deepfake audio, we focus on the mechanism of the ALM-based audio generation method, the conversion from neural codec to waveform.","We initially construct the Codecfake dataset, an open-source large-scale dataset, including two languages, millions of audio samples, and various test conditions, tailored for ALM-based audio detection.","Additionally, to achieve universal detection of deepfake audio and tackle domain ascent bias issue of original SAM, we propose the CSAM strategy to learn a domain balanced and generalized minima.","Experiment results demonstrate that co-training on Codecfake dataset and vocoded dataset with CSAM strategy yield the lowest average Equal Error Rate (EER) of 0.616% across all test conditions compared to baseline models."],"url":"http://arxiv.org/abs/2405.04880v1","category":"cs.SD"}
{"created":"2024-05-08 08:23:25","title":"The Need Of Trustworthy Announcements To Achieve Driving Comfort","abstract":"An Intelligent Transport System (ITS) is more demanding nowadays and it can be achieved through deploying Vehicular Ad Hoc Networks (VANETs). Vehicles and Roadside Units (RSUs) exchange traffic events. Malicious drivers generate false events. Thus, they need to be identified to maintain trustworthy communication. When an authorised user acts maliciously, the security scheme typically fails. However, a trust model can isolate false messages. In this paper, the significance of trustworthy announcements for VANETs is analysed. To this end, a series of experiments is conducted in Veins to illustrate how the trustworthiness of announcements affects travel time. A traffic scenario is created where vehicles detour to an alternate route with an announcement from the leading vehicle. Both true and false announcements are considered. Results confirm that false announcements and refraining from announcements increase travel time. However, the travel time is reduced with trustworthy announcements. From this analysis, it can be concluded that trustworthy announcements facilitate driver comfort.","sentences":["An Intelligent Transport System (ITS) is more demanding nowadays and it can be achieved through deploying Vehicular Ad Hoc Networks (VANETs).","Vehicles and Roadside Units (RSUs) exchange traffic events.","Malicious drivers generate false events.","Thus, they need to be identified to maintain trustworthy communication.","When an authorised user acts maliciously, the security scheme typically fails.","However, a trust model can isolate false messages.","In this paper, the significance of trustworthy announcements for VANETs is analysed.","To this end, a series of experiments is conducted in Veins to illustrate how the trustworthiness of announcements affects travel time.","A traffic scenario is created where vehicles detour to an alternate route with an announcement from the leading vehicle.","Both true and false announcements are considered.","Results confirm that false announcements and refraining from announcements increase travel time.","However, the travel time is reduced with trustworthy announcements.","From this analysis, it can be concluded that trustworthy announcements facilitate driver comfort."],"url":"http://arxiv.org/abs/2405.04878v1","category":"cs.CR"}
{"created":"2024-05-08 17:45:57","title":"Semi-infinite particle systems with exclusion interaction and heterogeneous jump rates","abstract":"We study semi-infinite particle systems on the one-dimensional integer lattice, where each particle performs a continuous-time nearest-neighbour random walk, with jump rates intrinsic to each particle, subject to an exclusion interaction which suppresses jumps that would lead to more than one particle occupying any site. Under appropriate hypotheses on the jump rates (uniformly bounded rates is sufficient) and started from an initial condition that is a finite perturbation of the close-packed configuration, we give conditions under which the particles evolve as a single, semi-infinite \"stable cloud\". More precisely, we show that inter-particle separations converge to a product-geometric stationary distribution, and that the location of every particle obeys a strong law of large numbers with the same characteristic speed.","sentences":["We study semi-infinite particle systems on the one-dimensional integer lattice, where each particle performs a continuous-time nearest-neighbour random walk, with jump rates intrinsic to each particle, subject to an exclusion interaction which suppresses jumps that would lead to more than one particle occupying any site.","Under appropriate hypotheses on the jump rates (uniformly bounded rates is sufficient) and started from an initial condition that is a finite perturbation of the close-packed configuration, we give conditions under which the particles evolve as a single, semi-infinite \"stable cloud\".","More precisely, we show that inter-particle separations converge to a product-geometric stationary distribution, and that the location of every particle obeys a strong law of large numbers with the same characteristic speed."],"url":"http://arxiv.org/abs/2405.05246v1","category":"math.PR"}
{"created":"2024-05-08 17:42:49","title":"Advancing Blockchain Scalability: A Linear Optimization Framework for Diversified Node Allocation in Shards","abstract":"Blockchain technology, while revolutionary in enabling decentralized transactions, faces scalability challenges as the ledger must be replicated across all nodes of the chain, limiting throughput and efficiency. Sharding, which divides the chain into smaller segments, called shards, offers a solution by enabling parallel transaction processing. However, sharding introduces new complexities, notably how to allocate nodes to shards without compromising the network's security.   This paper introduces a novel linear optimization framework for node allocation to shards that addresses decentralization constraints while minimizing resource consumption. In contrast to traditional methods that depend on random or trust-based assignments, our approach evaluates node characteristics, including ownership, hardware, and geographical distribution, and requires an explicit specification of decentralization targets with respect to these characteristics. By employing linear optimization, the framework identifies a resource-efficient node set meeting these targets. Adopted by the Internet Computer Protocol (ICP) community, this framework proves its utility in real-world blockchain applications. It provides a quantitative tool for node onboarding and offboarding decisions, balancing decentralization and resource considerations.","sentences":["Blockchain technology, while revolutionary in enabling decentralized transactions, faces scalability challenges as the ledger must be replicated across all nodes of the chain, limiting throughput and efficiency.","Sharding, which divides the chain into smaller segments, called shards, offers a solution by enabling parallel transaction processing.","However, sharding introduces new complexities, notably how to allocate nodes to shards without compromising the network's security.   ","This paper introduces a novel linear optimization framework for node allocation to shards that addresses decentralization constraints while minimizing resource consumption.","In contrast to traditional methods that depend on random or trust-based assignments, our approach evaluates node characteristics, including ownership, hardware, and geographical distribution, and requires an explicit specification of decentralization targets with respect to these characteristics.","By employing linear optimization, the framework identifies a resource-efficient node set meeting these targets.","Adopted by the Internet Computer Protocol (ICP) community, this framework proves its utility in real-world blockchain applications.","It provides a quantitative tool for node onboarding and offboarding decisions, balancing decentralization and resource considerations."],"url":"http://arxiv.org/abs/2405.05245v1","category":"cs.DC"}
{"created":"2024-05-08 17:36:14","title":"Cellular Traffic Prediction Using Online Prediction Algorithms","abstract":"The advent of 5G technology promises a paradigm shift in the realm of telecommunications, offering unprecedented speeds and connectivity. However, the efficient management of traffic in 5G networks remains a critical challenge. It is due to the dynamic and heterogeneous nature of network traffic, varying user behaviors, extended network size, and diverse applications, all of which demand highly accurate and adaptable prediction models to optimize network resource allocation and management. This paper investigates the efficacy of live prediction algorithms for forecasting cellular network traffic in real-time scenarios. We apply two live prediction algorithms on machine learning models, one of which is recently proposed Fast LiveStream Prediction (FLSP) algorithm. We examine the performance of these algorithms under two distinct data gathering methodologies: synchronous, where all network cells report statistics simultaneously, and asynchronous, where reporting occurs across consecutive time slots. Our study delves into the impact of these gathering scenarios on the predictive performance of traffic models. Our study reveals that the FLSP algorithm can halve the required bandwidth for asynchronous data reporting compared to conventional online prediction algorithms, while simultaneously enhancing prediction accuracy and reducing processing load. Additionally, we conduct a thorough analysis of algorithmic complexity and memory requirements across various machine learning models. Through empirical evaluation, we provide insights into the trade-offs inherent in different prediction strategies, offering valuable guidance for network optimization and resource allocation in dynamic environments.","sentences":["The advent of 5G technology promises a paradigm shift in the realm of telecommunications, offering unprecedented speeds and connectivity.","However, the efficient management of traffic in 5G networks remains a critical challenge.","It is due to the dynamic and heterogeneous nature of network traffic, varying user behaviors, extended network size, and diverse applications, all of which demand highly accurate and adaptable prediction models to optimize network resource allocation and management.","This paper investigates the efficacy of live prediction algorithms for forecasting cellular network traffic in real-time scenarios.","We apply two live prediction algorithms on machine learning models, one of which is recently proposed Fast LiveStream Prediction (FLSP) algorithm.","We examine the performance of these algorithms under two distinct data gathering methodologies: synchronous, where all network cells report statistics simultaneously, and asynchronous, where reporting occurs across consecutive time slots.","Our study delves into the impact of these gathering scenarios on the predictive performance of traffic models.","Our study reveals that the FLSP algorithm can halve the required bandwidth for asynchronous data reporting compared to conventional online prediction algorithms, while simultaneously enhancing prediction accuracy and reducing processing load.","Additionally, we conduct a thorough analysis of algorithmic complexity and memory requirements across various machine learning models.","Through empirical evaluation, we provide insights into the trade-offs inherent in different prediction strategies, offering valuable guidance for network optimization and resource allocation in dynamic environments."],"url":"http://arxiv.org/abs/2405.05239v1","category":"eess.SY"}
{"created":"2024-05-08 17:36:05","title":"Fast Exact/Conservative Monte Carlo Confidence Intervals","abstract":"Monte Carlo tests about parameters can be \"inverted\" to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\\alpha$. When the tests are exact or conservative -- as some families of such tests are -- so are the confidence sets. Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases. When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way. For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible. An open-source Python implementation of the approach for the one-sample and two-sample problems is available.","sentences":["Monte Carlo tests about parameters can be \"inverted\" to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\\alpha$.","When the tests are exact or conservative -- as some families of such tests are -- so are the confidence sets.","Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data.","The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases.","When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way.","For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible.","An open-source Python implementation of the approach for the one-sample and two-sample problems is available."],"url":"http://arxiv.org/abs/2405.05238v1","category":"stat.CO"}
{"created":"2024-05-08 17:22:26","title":"Are Economically Advanced Countries More Efficient in Basic and Applied Research?","abstract":"Research and development (R&D) of countries play a major role in a long-term development of the economy. We measure the R&D efficiency of all 28 member countries of the European Union in the years 2008--2014. Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted. We use the number of citations as output of basic research, the number of patents as output of applied research and R&D expenditures with manpower as inputs. To meet DEA assumptions and to capture R&D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs. We find that the efficiency of general R&D is higher for countries with higher GDP per capita. This relation also holds for specialized efficiencies of basic and applied research. However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured. Our findings are important in the evaluation of research and policy making.","sentences":["Research and development (R&D) of countries play a major role in a long-term development of the economy.","We measure the R&D efficiency of all 28 member countries of the European Union in the years 2008--2014.","Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted.","We use the number of citations as output of basic research, the number of patents as output of applied research and R&D expenditures with manpower as inputs.","To meet DEA assumptions and to capture R&D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs.","We find that the efficiency of general R&D is higher for countries with higher GDP per capita.","This relation also holds for specialized efficiencies of basic and applied research.","However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured.","Our findings are important in the evaluation of research and policy making."],"url":"http://arxiv.org/abs/2405.05227v1","category":"stat.AP"}
{"created":"2024-05-08 17:22:25","title":"SuFIA: Language-Guided Augmented Dexterity for Robotic Surgical Assistants","abstract":"In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants. SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution. This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives. SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks. We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions. Project website: orbit-surgical.github.io/sufia","sentences":["In this work, we present SuFIA, the first framework for natural language-guided augmented dexterity for robotic surgical assistants.","SuFIA incorporates the strong reasoning capabilities of large language models (LLMs) with perception modules to implement high-level planning and low-level control of a robot for surgical sub-task execution.","This enables a learning-free approach to surgical augmented dexterity without any in-context examples or motion primitives.","SuFIA uses a human-in-the-loop paradigm by restoring control to the surgeon in the case of insufficient information, mitigating unexpected errors for mission-critical tasks.","We evaluate SuFIA on four surgical sub-tasks in a simulation environment and two sub-tasks on a physical surgical robotic platform in the lab, demonstrating its ability to perform common surgical sub-tasks through supervised autonomous operation under challenging physical and workspace conditions.","Project website: orbit-surgical.github.io/sufia"],"url":"http://arxiv.org/abs/2405.05226v1","category":"cs.RO"}
{"created":"2024-05-08 17:18:37","title":"\"Community Guidelines Make this the Best Party on the Internet\": An In-Depth Study of Online Platforms' Content Moderation Policies","abstract":"Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech. Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content. Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics. This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content. We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components. We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings. We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users.","sentences":["Moderating user-generated content on online platforms is crucial for balancing user safety and freedom of speech.","Particularly in the United States, platforms are not subject to legal constraints prescribing permissible content.","Each platform has thus developed bespoke content moderation policies, but there is little work towards a comparative understanding of these policies across platforms and topics.","This paper presents the first systematic study of these policies from the 43 largest online platforms hosting user-generated content, focusing on policies around copyright infringement, harmful speech, and misleading content.","We build a custom web-scraper to obtain policy text and develop a unified annotation scheme to analyze the text for the presence of critical components.","We find significant structural and compositional variation in policies across topics and platforms, with some variation attributable to disparate legal groundings.","We lay the groundwork for future studies of ever-evolving content moderation policies and their impact on users."],"url":"http://arxiv.org/abs/2405.05225v1","category":"cs.HC"}
{"created":"2024-05-08 17:15:18","title":"Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation","abstract":"Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation.","sentences":["Diffusion models are a powerful generative framework, but come with expensive inference.","Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime.","In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps.","Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference-time technique that enhances sample quality by addressing singularities in noise prediction.","Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations.","Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation."],"url":"http://arxiv.org/abs/2405.05224v1","category":"cs.CV"}
{"created":"2024-05-08 17:11:01","title":"Clustering Retail Products Based on Customer Behaviour","abstract":"The categorization of retail products is essential for the business decision-making process. It is a common practice to classify products based on their quantitative and qualitative characteristics. In this paper we use a purely data-driven approach. Our clustering of products is based exclusively on the customer behaviour. We propose a method for clustering retail products using market basket data. Our model is formulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on simulated data how our method behaves in different settings. The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization.","sentences":["The categorization of retail products is essential for the business decision-making process.","It is a common practice to classify products based on their quantitative and qualitative characteristics.","In this paper we use a purely data-driven approach.","Our clustering of products is based exclusively on the customer behaviour.","We propose a method for clustering retail products using market basket data.","Our model is formulated as an optimization problem which is solved by a genetic algorithm.","It is demonstrated on simulated data how our method behaves in different settings.","The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts.","The number of clusters is a parameter of our algorithm.","We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization."],"url":"http://arxiv.org/abs/2405.05218v1","category":"stat.AP"}
{"created":"2024-05-08 17:07:56","title":"Restricted Randomized Benchmarking with Universal Gates of Fixed Sequence Length","abstract":"The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible. Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation. We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one. This makes our protocol highly resource efficient and practical for small qubit numbers. We exemplify our protocol for creating Haar-randomness in the case of single and two qubits. Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique. We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise.","sentences":["The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible.","Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation.","We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one.","This makes our protocol highly resource efficient and practical for small qubit numbers.","We exemplify our protocol for creating Haar-randomness in the case of single and two qubits.","Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique.","We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise."],"url":"http://arxiv.org/abs/2405.05215v1","category":"quant-ph"}
{"created":"2024-05-08 17:06:32","title":"SPIDER: Improved Succinct Rank and Select Performance","abstract":"Rank and select data structures seek to preprocess a bit vector to quickly answer two kinds of queries: rank(i) gives the number of 1 bits in slots 0 through i, and select(j) gives the first slot s with rank(s) = j. A succinct data structure can answer these queries while using space much smaller than the size of the original bit vector.   State of the art succinct rank and select data structures use as little as 4% extra space while answering rank and select queries quickly. Rank queries can be answered using only a handful of array accesses. Select queries can be answered by starting with similar array accesses, followed by a linear scan.   Despite these strong results, a tradeoff remains: data structures that use under 4% space are significantly slower at answering rank and select queries than less-space-efficient data structures (using, say, > 20% extra space).   In this paper we make significant progress towards closing this gap. We give a new data structure, SPIDER, which uses 3.82% extra space. SPIDER gives the best rank query time for data sets of 8 billion or more bits, even compared to less space-efficient data structures. For select queries, SPIDER outperforms all data structures that use less than 4% space, and significantly closes the gap in select performance between data structures with less than 4% space, and those that use more (over 20%) space.   SPIDER makes two main technical contributions. For rank queries, it improves performance by interleaving the metadata with the bit vector to improve cache efficiency. For select queries, it uses predictions to almost eliminate the cost of the linear scan. These predictions are inspired by recent results on data structures with machine-learned predictions, adapted to the succinct data structure setting. Our results hold on both real and synthetic data, showing that these predictions are effective in practice.","sentences":["Rank and select data structures seek to preprocess a bit vector to quickly answer two kinds of queries: rank(i) gives the number of 1 bits in slots 0","through i, and select(j) gives the first slot s with rank(s) = j.","A succinct data structure can answer these queries while using space much smaller than the size of the original bit vector.   ","State of the art succinct rank and select data structures use as little as 4% extra space while answering rank and select queries quickly.","Rank queries can be answered using only a handful of array accesses.","Select queries can be answered by starting with similar array accesses, followed by a linear scan.   ","Despite these strong results, a tradeoff remains: data structures that use under 4% space are significantly slower at answering rank and select queries than less-space-efficient data structures (using, say, > 20% extra space).   ","In this paper we make significant progress towards closing this gap.","We give a new data structure, SPIDER, which uses 3.82% extra space.","SPIDER gives the best rank query time for data sets of 8 billion or more bits, even compared to less space-efficient data structures.","For select queries, SPIDER outperforms all data structures that use less than 4% space, and significantly closes the gap in select performance between data structures with less than 4% space, and those that use more (over 20%) space.   ","SPIDER makes two main technical contributions.","For rank queries, it improves performance by interleaving the metadata with the bit vector to improve cache efficiency.","For select queries, it uses predictions to almost eliminate the cost of the linear scan.","These predictions are inspired by recent results on data structures with machine-learned predictions, adapted to the succinct data structure setting.","Our results hold on both real and synthetic data, showing that these predictions are effective in practice."],"url":"http://arxiv.org/abs/2405.05214v1","category":"cs.DS"}
{"created":"2024-05-08 16:58:22","title":"MOTLEE: Collaborative Multi-Object Tracking Using Temporal Consistency for Neighboring Robot Frame Alignment","abstract":"Knowing the locations of nearby moving objects is important for a mobile robot to operate safely in a dynamic environment. Dynamic object tracking performance can be improved if robots share observations of tracked objects with nearby team members in real-time. To share observations, a robot must make up-to-date estimates of the transformation from its coordinate frame to the frame of each neighbor, which can be challenging because of odometry drift. We present Multiple Object Tracking with Localization Error Elimination (MOTLEE), a complete system for a multi-robot team to accurately estimate frame transformations and collaboratively track dynamic objects. To accomplish this, robots use open-set image-segmentation methods to build object maps of their environment and then use our Temporally Consistent Alignment of Frames Filter (TCAFF) to align maps and estimate coordinate frame transformations without any initial knowledge of neighboring robot poses. We show that our method for aligning frames enables a team of four robots to collaboratively track six pedestrians with accuracy similar to that of a system with ground truth localization in a challenging hardware demonstration. The code and hardware dataset are available at https://github.com/mit-acl/motlee.","sentences":["Knowing the locations of nearby moving objects is important for a mobile robot to operate safely in a dynamic environment.","Dynamic object tracking performance can be improved if robots share observations of tracked objects with nearby team members in real-time.","To share observations, a robot must make up-to-date estimates of the transformation from its coordinate frame to the frame of each neighbor, which can be challenging because of odometry drift.","We present Multiple Object Tracking with Localization Error Elimination (MOTLEE), a complete system for a multi-robot team to accurately estimate frame transformations and collaboratively track dynamic objects.","To accomplish this, robots use open-set image-segmentation methods to build object maps of their environment and then use our Temporally Consistent Alignment of Frames Filter (TCAFF) to align maps and estimate coordinate frame transformations without any initial knowledge of neighboring robot poses.","We show that our method for aligning frames enables a team of four robots to collaboratively track six pedestrians with accuracy similar to that of a system with ground truth localization in a challenging hardware demonstration.","The code and hardware dataset are available at https://github.com/mit-acl/motlee."],"url":"http://arxiv.org/abs/2405.05210v1","category":"cs.RO"}
{"created":"2024-05-08 16:47:04","title":"In-depth analysis of LISA Pathfinder performance results: time evolution, noise projection, physical models, and implications for LISA","abstract":"We present an analysis of the LISA Pathfinder differential acceleration performance over the entire mission . We show that the Brownian noise level, detected for frequencies $f\\gtrsim \\SI{1}{mHz}$, has been evolving consistently with the outgassing of a single gaseous species, with an activation temperature of $(7.0\\pm 0.2)\\,\\text{kK}$. In excess to the Brownian noise, the acceleration amplitude spectral density (ASD) always shows a sub-mHz tail which is reasonably well fit, between $f=\\SI{36}{\\micro\\hertz}$ and $\\SI{1}{\\milli\\hertz}$, to $\\widetilde{S}_{\\Delta g}^{1/2}(1\\, \\text{mHz}/f)$. A Bayesian estimate of $\\widetilde{S}_{\\Delta g}^{1/2}$ on a partition of the entire set of measurements in 27 data stretches, each 2.75\\,d long, gives $\\widetilde{S}_{\\Delta g}^{1/2}=(1.1\\pm0.3)\\,\\si{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no particular time pattern over the course of the mission. The width the posterior contains, in excess of the statistical uncertainty, a true physical fluctuation of $\\widetilde{S}_{\\Delta g}^{1/2}$ from run to run, of about $\\SI{0.2}{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no correlation with specific operating conditions. At the lowest considered frequency of $f=\\SI{18}{\\micro\\hertz}$, the ASD significantly deviates from the $1/f$ behavior, because of temperature fluctuations that appear to modulate a quasi-static pressure gradient, sustained by the asymmetries of outgassing . We also present a projection of acceleration noise on the sources for which we had either a correlation measurement, or an estimate from dedicated experiments.These sources account for about 40\\% of the noise power the $1/f$ tail. We discuss the possible sources of the unaccounted-for fraction, present a series of analyses that rule many of them out, and identify the possible measures that may be taken to keep the remaining ones under control in LISA.","sentences":["We present an analysis of the LISA Pathfinder differential acceleration performance over the entire mission .","We show that the Brownian noise level, detected for frequencies $f\\gtrsim \\SI{1}{mHz}$, has been evolving consistently with the outgassing of a single gaseous species, with an activation temperature of $(7.0\\pm 0.2)\\,\\text{kK}$. In excess to the Brownian noise, the acceleration amplitude spectral density (ASD) always shows a sub-mHz tail which is reasonably well fit, between $f=\\SI{36}{\\micro\\hertz}$ and $\\SI{1}{\\milli\\hertz}$, to $\\widetilde{S}_{\\Delta g}^{1/2}(1\\, \\text{mHz}/f)$. A Bayesian estimate of $\\widetilde{S}_{\\Delta g}^{1/2}$ on a partition of the entire set of measurements in 27 data stretches, each 2.75\\,d long, gives $\\widetilde{S}_{\\Delta g}^{1/2}=(1.1\\pm0.3)\\,\\si{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no particular time pattern over the course of the mission.","The width the posterior contains, in excess of the statistical uncertainty, a true physical fluctuation of $\\widetilde{S}_{\\Delta g}^{1/2}$ from run to run, of about $\\SI{0.2}{\\femto\\meter\\,\\second^{-2}/\\rtHz}$, with no correlation with specific operating conditions.","At the lowest considered frequency of $f=\\SI{18}{\\micro\\hertz}$, the ASD significantly deviates from the $1/f$ behavior, because of temperature fluctuations that appear to modulate a quasi-static pressure gradient, sustained by the asymmetries of outgassing .","We also present a projection of acceleration noise on the sources for which we had either a correlation measurement, or an estimate from dedicated experiments.","These sources account for about 40\\% of the noise power the $1/f$ tail.","We discuss the possible sources of the unaccounted-for fraction, present a series of analyses that rule many of them out, and identify the possible measures that may be taken to keep the remaining ones under control in LISA."],"url":"http://arxiv.org/abs/2405.05207v1","category":"astro-ph.IM"}
{"created":"2024-05-08 16:32:05","title":"Existence and dynamics of normalized solutions to Schr\u00f6dinger equations with generic double-behaviour nonlinearities","abstract":"We study the existence of solutions $(\\underline u,\\lambda_{\\underline u})\\in H^1(\\mathbb{R}^N; \\mathbb{R}) \\times \\mathbb{R}$ to \\[ -\\Delta u + \\lambda u = f(u) \\quad \\text{in } \\mathbb{R}^N \\] with $N \\ge 3$ and prescribed $L^2$ norm, and the dynamics of the solutions to \\[ \\begin{cases} \\mathrm{i} \\partial_t \\Psi + \\Delta \\Psi = f(\\Psi)\\\\ \\Psi(\\cdot,0) = \\psi_0 \\in H^1(\\mathbb{R}^N; \\mathbb{C}) \\end{cases} \\] with $\\psi_0$ close to $\\underline u$. Here, the nonlinear term $f$ has mass-subcritical growth at the origin, mass-supercritical growth at infinity, and is more general than the sum of two powers. Under different assumptions, we prove the existence of a locally least-energy solution, the orbital stability of all such solutions, the existence of a second solution with higher energy, and the strong instability of such a solution.","sentences":["We study the existence of solutions $(\\underline u,\\lambda_{\\underline u})\\in H^1(\\mathbb{R}^N; \\mathbb{R})","\\times \\mathbb{R}$ to \\[ -\\Delta u + \\lambda u = f(u)","\\quad \\text{in } \\mathbb{R}^N \\] with $N \\ge 3$ and prescribed $L^2$ norm, and the dynamics of the solutions to \\[ \\begin{cases} \\mathrm{i} \\partial_t \\Psi + \\Delta \\Psi = f(\\Psi)\\\\ \\Psi(\\cdot,0) = \\psi_0 \\in H^1(\\mathbb{R}^N; \\mathbb{C}) \\end{cases} \\] with $\\psi_0$ close to $\\underline u$. Here, the nonlinear term $f$ has mass-subcritical growth at the origin, mass-supercritical growth at infinity, and is more general than the sum of two powers.","Under different assumptions, we prove the existence of a locally least-energy solution, the orbital stability of all such solutions, the existence of a second solution with higher energy, and the strong instability of such a solution."],"url":"http://arxiv.org/abs/2405.05194v1","category":"math.AP"}
{"created":"2024-05-08 16:30:45","title":"Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs with infinite activity","abstract":"In this paper, we present a randomized extension of the deep splitting algorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)] using random neural networks suitable to approximately solve both high-dimensional nonlinear parabolic PDEs and PIDEs with jumps having (possibly) infinite activity. We provide a full error analysis of our so-called random deep splitting method. In particular, we prove that our random deep splitting method converges to the (unique viscosity) solution of the nonlinear PDE or PIDE under consideration. Moreover, we empirically analyze our random deep splitting method by considering several numerical examples including both nonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of financial derivatives under default risk. In particular, we empirically demonstrate in all examples that our random deep splitting method can approximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within seconds.","sentences":["In this paper, we present a randomized extension of the deep splitting algorithm introduced in [Beck, Becker, Cheridito, Jentzen, and Neufeld (2021)] using random neural networks suitable to approximately solve both high-dimensional nonlinear parabolic PDEs and PIDEs with jumps having (possibly) infinite activity.","We provide a full error analysis of our so-called random deep splitting method.","In particular, we prove that our random deep splitting method converges to the (unique viscosity) solution of the nonlinear PDE or PIDE under consideration.","Moreover, we empirically analyze our random deep splitting method by considering several numerical examples including both nonlinear PDEs and nonlinear PIDEs relevant in the context of pricing of financial derivatives under default risk.","In particular, we empirically demonstrate in all examples that our random deep splitting method can approximately solve nonlinear PDEs and PIDEs in 10'000 dimensions within seconds."],"url":"http://arxiv.org/abs/2405.05192v1","category":"math.NA"}
{"created":"2024-05-08 16:30:37","title":"Bell's and Mermin's inequalities, entangled coherent states and unitary operators","abstract":"We elaborate on the recent proposal of employing unitary operators in Quantum Mechanics. The Bell and Mermin inequalities for entangled coherent states are scrutinized by making use of the unitary displacement operators. A violation of the Mermin inequality close to the maximum allowed value is reported, in agreement with the existing literature.","sentences":["We elaborate on the recent proposal of employing unitary operators in Quantum Mechanics.","The Bell and Mermin inequalities for entangled coherent states are scrutinized by making use of the unitary displacement operators.","A violation of the Mermin inequality close to the maximum allowed value is reported, in agreement with the existing literature."],"url":"http://arxiv.org/abs/2405.05191v1","category":"quant-ph"}
{"created":"2024-05-08 16:17:14","title":"Randomized quasi-Monte Carlo and Owen's boundary growth condition: A spectral analysis","abstract":"In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen's boundary growth condition [Owen, 2006] via spectral analysis. Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol' sequence, applying the Fourier transform and Walsh--Fourier transform, respectively, for this analysis. Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator's variance closely aligns with the exponent specified in Owen's boundary growth condition for both sequence types. We also provide guidance on choosing the importance sampling density to minimize RQMC estimator variance.","sentences":["In this work, we analyze the convergence rate of randomized quasi-Monte Carlo (RQMC) methods under Owen's boundary growth condition [Owen, 2006] via spectral analysis.","Specifically, we examine the RQMC estimator variance for the two commonly studied sequences: the lattice rule and the Sobol' sequence, applying the Fourier transform and Walsh--Fourier transform, respectively, for this analysis.","Assuming certain regularity conditions, our findings reveal that the asymptotic convergence rate of the RQMC estimator's variance closely aligns with the exponent specified in Owen's boundary growth condition for both sequence types.","We also provide guidance on choosing the importance sampling density to minimize RQMC estimator variance."],"url":"http://arxiv.org/abs/2405.05181v1","category":"math.NA"}
{"created":"2024-05-08 16:13:49","title":"Network mutual information measures for graph similarity","abstract":"A wide range of tasks in exploratory network analysis and machine learning, such as clustering network populations or identifying anomalies in temporal graph streams, require a measure of the similarity between two graphs. To provide a meaningful data summary for downstream scientific analyses, the graph similarity measures used in these unsupervised settings must be principled, interpretable, and capable of distinguishing meaningful overlapping network structure from statistical noise at different scales of interest. Here we derive a family of graph mutual information measures that satisfy these criteria and are constructed using only fundamental information theoretic principles. Our measures capture the information shared among networks according to different encodings of their structural information, with our mesoscale mutual information measure allowing for network comparison under any specified network coarse-graining. We test our measures in a range of applications on real and synthetic network data, finding that they effectively highlight intuitive aspects of network similarity across scales in a variety of systems.","sentences":["A wide range of tasks in exploratory network analysis and machine learning, such as clustering network populations or identifying anomalies in temporal graph streams, require a measure of the similarity between two graphs.","To provide a meaningful data summary for downstream scientific analyses, the graph similarity measures used in these unsupervised settings must be principled, interpretable, and capable of distinguishing meaningful overlapping network structure from statistical noise at different scales of interest.","Here we derive a family of graph mutual information measures that satisfy these criteria and are constructed using only fundamental information theoretic principles.","Our measures capture the information shared among networks according to different encodings of their structural information, with our mesoscale mutual information measure allowing for network comparison under any specified network coarse-graining.","We test our measures in a range of applications on real and synthetic network data, finding that they effectively highlight intuitive aspects of network similarity across scales in a variety of systems."],"url":"http://arxiv.org/abs/2405.05177v1","category":"physics.soc-ph"}
{"created":"2024-05-08 16:13:40","title":"Encoder-Decoder Framework for Interactive Free Verses with Generation with Controllable High-Quality Rhyming","abstract":"Composing poetry or lyrics involves several creative factors, but a challenging aspect of generation is the adherence to a more or less strict metric and rhyming pattern. To address this challenge specifically, previous work on the task has mainly focused on reverse language modeling, which brings the critical selection of each rhyming word to the forefront of each verse. On the other hand, reversing the word order requires that models be trained from scratch with this task-specific goal and cannot take advantage of transfer learning from a Pretrained Language Model (PLM). We propose a novel fine-tuning approach that prepends the rhyming word at the start of each lyric, which allows the critical rhyming decision to be made before the model commits to the content of the lyric (as during reverse language modeling), but maintains compatibility with the word order of regular PLMs as the lyric itself is still generated in left-to-right order. We conducted extensive experiments to compare this fine-tuning against the current state-of-the-art strategies for rhyming, finding that our approach generates more readable text and better rhyming capabilities. Furthermore, we furnish a high-quality dataset in English and 12 other languages, analyse the approach's feasibility in a multilingual context, provide extensive experimental results shedding light on good and bad practices for lyrics generation, and propose metrics to compare methods in the future.","sentences":["Composing poetry or lyrics involves several creative factors, but a challenging aspect of generation is the adherence to a more or less strict metric and rhyming pattern.","To address this challenge specifically, previous work on the task has mainly focused on reverse language modeling, which brings the critical selection of each rhyming word to the forefront of each verse.","On the other hand, reversing the word order requires that models be trained from scratch with this task-specific goal and cannot take advantage of transfer learning from a Pretrained Language Model (PLM).","We propose a novel fine-tuning approach that prepends the rhyming word at the start of each lyric, which allows the critical rhyming decision to be made before the model commits to the content of the lyric (as during reverse language modeling), but maintains compatibility with the word order of regular PLMs as the lyric itself is still generated in left-to-right order.","We conducted extensive experiments to compare this fine-tuning against the current state-of-the-art strategies for rhyming, finding that our approach generates more readable text and better rhyming capabilities.","Furthermore, we furnish a high-quality dataset in English and 12 other languages, analyse the approach's feasibility in a multilingual context, provide extensive experimental results shedding light on good and bad practices for lyrics generation, and propose metrics to compare methods in the future."],"url":"http://arxiv.org/abs/2405.05176v1","category":"cs.CL"}
{"created":"2024-05-08 16:09:40","title":"Sobolev mappings on metric spaces and Minkowski dimension","abstract":"We introduce the class of compactly H\\\"older mappings between metric spaces and determine the extent to which they distort the Minkowski dimension of a given set. These mappings are defined purely with metric notions and can be seen as a generalization of Sobolev mappings, without the requirement for a measure on the source space. In fact, we show that if $f:X\\rightarrow Y$ is a continuous mapping lying in some super-critical Newtonian-Sobolev space $N^{1,p}(X,\\mu)$, under standard assumptions on the metric measure space $(X,d,\\mu)$, it is then a compactly H\\\"older mapping. The dimension distortion result we obtain is new even for Sobolev mappings between weighted Euclidean spaces and generalizes previous results of Kaufman and Bishop-Hakobyan-Williams.","sentences":["We introduce the class of compactly H\\\"older mappings between metric spaces and determine the extent to which they distort the Minkowski dimension of a given set.","These mappings are defined purely with metric notions and can be seen as a generalization of Sobolev mappings, without the requirement for a measure on the source space.","In fact, we show that if $f:X\\rightarrow Y$ is a continuous mapping lying in some super-critical Newtonian-Sobolev space $N^{1,p}(X,\\mu)$, under standard assumptions on the metric measure space $(X,d,\\mu)$, it is then a compactly H\\\"older mapping.","The dimension distortion result we obtain is new even for Sobolev mappings between weighted Euclidean spaces and generalizes previous results of Kaufman and Bishop-Hakobyan-Williams."],"url":"http://arxiv.org/abs/2405.05172v1","category":"math.DS"}
{"created":"2024-05-08 16:06:57","title":"Picking watermarks from noise (PWFN): an improved robust watermarking model against intensive distortions","abstract":"Digital watermarking is the process of embedding secret information by altering images in a way that is undetectable to the human eye. To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-decoder architecture by adding different noises to the noise layer. The decoder then extracts the watermarked information from the distorted image. However, this method can only resist weak noise attacks. To improve the robustness of the algorithm against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder. The module is aimed at reducing noise and recovering some of the information lost during an attack. Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency. Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities. In addition, ablation experiments show the superiority of our proposed module.","sentences":["Digital watermarking is the process of embedding secret information by altering images in a way that is undetectable to the human eye.","To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-decoder architecture by adding different noises to the noise layer.","The decoder then extracts the watermarked information from the distorted image.","However, this method can only resist weak noise attacks.","To improve the robustness of the algorithm against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder.","The module is aimed at reducing noise and recovering some of the information lost during an attack.","Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency.","Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities.","In addition, ablation experiments show the superiority of our proposed module."],"url":"http://arxiv.org/abs/2405.05170v1","category":"cs.MM"}
{"created":"2024-05-08 16:02:29","title":"Discovery of T center-like quantum defects in silicon","abstract":"Quantum technologies would benefit from the development of high performance quantum defects acting as single-photon emitters or spin-photon interface. Finding such a quantum defect in silicon is especially appealing in view of its favorable spin bath and high processability. While some color centers in silicon have been emerging in quantum applications, there is still a need to search and develop new high performance quantum emitters. Searching a high-throughput computational database of more than 22,000 charged complex defects in silicon, we identify a series of defects formed by a group III element combined with carbon ((A-C)$\\rm _{Si}$ with A=B,Al,Ga,In,Tl) and substituting on a silicon site. These defects are analogous structurally, electronically and chemically to the well-known T center in silicon ((C-C-H)$\\rm_{Si}$) and their optical properties are mainly driven by an unpaired electron in a carbon $p$ orbital. They all emit in the telecom and some of these color centers show improved properties compared to the T center in terms of computed radiative lifetime or emission efficiency. We also show that the synthesis of hydrogenated T center-like defects followed by a dehydrogenation annealing step could be an efficient way of synthesis. All the T center-like defects show a higher symmetry than the T center making them easier to align with magnetic fields. Our work motivates further studies on the synthesis and control of this new family of quantum defects, and also demonstrates the use of high-throughput computational screening to detect new complex quantum defects.","sentences":["Quantum technologies would benefit from the development of high performance quantum defects acting as single-photon emitters or spin-photon interface.","Finding such a quantum defect in silicon is especially appealing in view of its favorable spin bath and high processability.","While some color centers in silicon have been emerging in quantum applications, there is still a need to search and develop new high performance quantum emitters.","Searching a high-throughput computational database of more than 22,000 charged complex defects in silicon, we identify a series of defects formed by a group III element combined with carbon ((A-C)$\\rm _{Si}$ with A=B,Al,Ga,In,Tl) and substituting on a silicon site.","These defects are analogous structurally, electronically and chemically to the well-known T center in silicon ((C-C-H)$\\rm_{Si}$) and their optical properties are mainly driven by an unpaired electron in a carbon $p$ orbital.","They all emit in the telecom and some of these color centers show improved properties compared to the T center in terms of computed radiative lifetime or emission efficiency.","We also show that the synthesis of hydrogenated T center-like defects followed by a dehydrogenation annealing step could be an efficient way of synthesis.","All the T center-like defects show a higher symmetry than the T center making them easier to align with magnetic fields.","Our work motivates further studies on the synthesis and control of this new family of quantum defects, and also demonstrates the use of high-throughput computational screening to detect new complex quantum defects."],"url":"http://arxiv.org/abs/2405.05165v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 15:49:24","title":"Filtering and smoothing estimation algorithms from uncertain nonlinear observations with time-correlated additive noise and random deception attacks","abstract":"This paper discusses the problem of estimating a stochastic signal from nonlinear uncertain observations with time-correlated additive noise described by a first-order Markov process. Random deception attacks are assumed to be launched by an adversary, and both this phenomenon and the uncertainty in the observations are modelled by two sets of Bernoulli random variables. Under the assumption that the evolution model generating the signal to be estimated is unknown and only the mean and covariance functions of the processes involved in the observation equation are available, recursive algorithms based on linear approximations of the real observations are proposed for the least-squares filtering and fixed-point smoothing problems. Finally, the feasibility and effectiveness of the developed estimation algorithms are verified by a numerical simulation example, where the impact of uncertain observation and deception attack probabilities on estimation accuracy is evaluated.","sentences":["This paper discusses the problem of estimating a stochastic signal from nonlinear uncertain observations with time-correlated additive noise described by a first-order Markov process.","Random deception attacks are assumed to be launched by an adversary, and both this phenomenon and the uncertainty in the observations are modelled by two sets of Bernoulli random variables.","Under the assumption that the evolution model generating the signal to be estimated is unknown and only the mean and covariance functions of the processes involved in the observation equation are available, recursive algorithms based on linear approximations of the real observations are proposed for the least-squares filtering and fixed-point smoothing problems.","Finally, the feasibility and effectiveness of the developed estimation algorithms are verified by a numerical simulation example, where the impact of uncertain observation and deception attack probabilities on estimation accuracy is evaluated."],"url":"http://arxiv.org/abs/2405.05157v1","category":"eess.SP"}
{"created":"2024-05-08 15:47:38","title":"Crystal structure identification with 3D convolutional neural networks with application to high-pressure phase transitions in SiO$_2$","abstract":"Efficient, reliable and easy-to-use structure recognition of atomic environments is essential for the analysis of atomic scale computer simulations. In this work, we train two neuronal network (NN) architectures, namely PointNet and dynamic graph convolutional NN (DG-CNN) using different hyperparameters and training regimes to assess their performance in structure identification tasks of atomistic structure data. We show benchmarks on simple crystal structures, where we can compare against established methods. The approach is subsequently extended to structurally more complex SiO$_2$ phases. By making use of this structure recognition tool, we are able to achieve a deeper understanding of the crystallization process in amorphous SiO$_2$ under shock compression. Lastly, we show how the NN based structure identification workflows can be integrated into OVITO using its python interface.","sentences":["Efficient, reliable and easy-to-use structure recognition of atomic environments is essential for the analysis of atomic scale computer simulations.","In this work, we train two neuronal network (NN) architectures, namely PointNet and dynamic graph convolutional NN (DG-CNN) using different hyperparameters and training regimes to assess their performance in structure identification tasks of atomistic structure data.","We show benchmarks on simple crystal structures, where we can compare against established methods.","The approach is subsequently extended to structurally more complex SiO$_2$ phases.","By making use of this structure recognition tool, we are able to achieve a deeper understanding of the crystallization process in amorphous SiO$_2$ under shock compression.","Lastly, we show how the NN based structure identification workflows can be integrated into OVITO using its python interface."],"url":"http://arxiv.org/abs/2405.05156v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 15:46:47","title":"An efficient truncation scheme for Eulerian and total Lagrangian SPH methods","abstract":"In smoothed particle hydrodynamics (SPH) method, the particle-based approximations are implemented via kernel functions, and the evaluation of performance involves two key criteria: numerical accuracy and computational efficiency. In the SPH community, the Wendland kernel reigns as the prevailing choice due to its commendable accuracy and reasonable computational efficiency. Nevertheless, there exists an urgent need to enhance the computational efficiency of numerical methods while upholding accuracy. In this paper, we employ a truncation approach to limit the compact support of the Wendland kernel to 1.6h. This decision is based on the observation that particles within the range of 1.6h to 2h make negligible contributions, practically approaching zero, to the SPH approximation. To address integration errors stemming from the truncation, we incorporate the Laguerre-Gauss kernel for particle relaxation due to the fact that this kernel has been demonstrated to enable the attainment of particle distributions with reduced residue and integration errors \\cite{wang2023fourth}. Furthermore, we introduce the kernel gradient correction to rectify numerical errors from the SPH approximation of kernel gradient and the truncated compact support. A comprehensive set of numerical examples including fluid dynamics in Eulerian formulation and solid dynamics in total Lagrangian formulation are tested and have demonstrated that truncated and standard Wendland kernels enable achieve the same level accuracy but the former significantly increase the computational efficiency.","sentences":["In smoothed particle hydrodynamics (SPH) method, the particle-based approximations are implemented via kernel functions, and the evaluation of performance involves two key criteria: numerical accuracy and computational efficiency.","In the SPH community, the Wendland kernel reigns as the prevailing choice due to its commendable accuracy and reasonable computational efficiency.","Nevertheless, there exists an urgent need to enhance the computational efficiency of numerical methods while upholding accuracy.","In this paper, we employ a truncation approach to limit the compact support of the Wendland kernel to 1.6h.","This decision is based on the observation that particles within the range of 1.6h to 2h make negligible contributions, practically approaching zero, to the SPH approximation.","To address integration errors stemming from the truncation, we incorporate the Laguerre-Gauss kernel for particle relaxation due to the fact that this kernel has been demonstrated to enable the attainment of particle distributions with reduced residue and integration errors \\cite{wang2023fourth}.","Furthermore, we introduce the kernel gradient correction to rectify numerical errors from the SPH approximation of kernel gradient and the truncated compact support.","A comprehensive set of numerical examples including fluid dynamics in Eulerian formulation and solid dynamics in total Lagrangian formulation are tested and have demonstrated that truncated and standard Wendland kernels enable achieve the same level accuracy but the former significantly increase the computational efficiency."],"url":"http://arxiv.org/abs/2405.05155v1","category":"cs.CE"}
{"created":"2024-05-08 15:35:34","title":"Multivariate group sequential tests for global summary statistics","abstract":"We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses. We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic. An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints. We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation. In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places. Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model. Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements.","sentences":["We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses.","We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic.","An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints.","We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation.","In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places.","Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model.","Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements."],"url":"http://arxiv.org/abs/2405.05139v1","category":"stat.ME"}
{"created":"2024-05-08 15:16:12","title":"Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference","abstract":"Estimating causal effects under interference is pertinent to many real-world settings. However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information. A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information. However, their use of polynomial extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population. We analyze the bias and variance of an interpolation-style estimator under this experimental design. Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator. Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small.","sentences":["Estimating causal effects under interference is pertinent to many real-world settings.","However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information.","A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information.","However, their use of polynomial extrapolation can lead to prohibitively high variance.","To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population.","We analyze the bias and variance of an interpolation-style estimator under this experimental design.","Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator.","Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small."],"url":"http://arxiv.org/abs/2405.05119v1","category":"stat.ME"}
{"created":"2024-05-08 15:09:00","title":"High Voltage Determination and Stabilization for Collinear Laser Spectroscopy Applications","abstract":"Fast beam collinear laser spectroscopy is the established method to investigate nuclear ground state properties such as the spin, the electromagnetic moments, and the charge radius of exotic nuclei. These are extracted with high precision from atomic observables, i.e., the hyperfine splitting and its the isotope shift, which becomes possible due to a large reduction of the Doppler broadening by compressing the velocity width of the ion beam through electrostatic acceleration. With the advancement of the experimental methods and applied devices, e.g., to measure and stabilize the laser frequency, the acceleration potential became the dominant systematic uncertainty contribution. To overcome this, we present a custom-built high-voltage divider, which was developed and tested at the German metrology institute (PTB), and a feedback loop that enabled collinear laser spectroscopy to be performed at a 100-kHz level. Furthermore, we describe the impact of field penetration into the laser-ion-interaction region. This strongly affects the determined isotope shifts and hyperfine splittings, if Doppler tuning is applied, i.e., the ion beam energy is altered instead of scanning the laser frequency. Using different laser frequencies that were referenced to a frequency comb, the field penetration was extracted laser spectroscopically. This allowed us to define an effective scanning potential to still apply the faster and easier Doppler tuning without introducing systematic deviations.","sentences":["Fast beam collinear laser spectroscopy is the established method to investigate nuclear ground state properties such as the spin, the electromagnetic moments, and the charge radius of exotic nuclei.","These are extracted with high precision from atomic observables, i.e., the hyperfine splitting and its the isotope shift, which becomes possible due to a large reduction of the Doppler broadening by compressing the velocity width of the ion beam through electrostatic acceleration.","With the advancement of the experimental methods and applied devices, e.g., to measure and stabilize the laser frequency, the acceleration potential became the dominant systematic uncertainty contribution.","To overcome this, we present a custom-built high-voltage divider, which was developed and tested at the German metrology institute (PTB), and a feedback loop that enabled collinear laser spectroscopy to be performed at a 100-kHz level.","Furthermore, we describe the impact of field penetration into the laser-ion-interaction region.","This strongly affects the determined isotope shifts and hyperfine splittings, if Doppler tuning is applied, i.e., the ion beam energy is altered instead of scanning the laser frequency.","Using different laser frequencies that were referenced to a frequency comb, the field penetration was extracted laser spectroscopically.","This allowed us to define an effective scanning potential to still apply the faster and easier Doppler tuning without introducing systematic deviations."],"url":"http://arxiv.org/abs/2405.05112v1","category":"physics.ins-det"}
{"created":"2024-05-08 15:06:02","title":"Uncertainty quantification in metric spaces","abstract":"This paper introduces a novel uncertainty quantification framework for regression models where the response takes values in a separable metric space, and the predictors are in a Euclidean space. The proposed algorithms can efficiently handle large datasets and are agnostic to the predictive base model used. Furthermore, the algorithms possess asymptotic consistency guarantees and, in some special homoscedastic cases, we provide non-asymptotic guarantees. To illustrate the effectiveness of the proposed uncertainty quantification framework, we use a linear regression model for metric responses (known as the global Fr\\'echet model) in various clinical applications related to precision and digital medicine. The different clinical outcomes analyzed are represented as complex statistical objects, including multivariate Euclidean data, Laplacian graphs, and probability distributions.","sentences":["This paper introduces a novel uncertainty quantification framework for regression models where the response takes values in a separable metric space, and the predictors are in a Euclidean space.","The proposed algorithms can efficiently handle large datasets and are agnostic to the predictive base model used.","Furthermore, the algorithms possess asymptotic consistency guarantees and, in some special homoscedastic cases, we provide non-asymptotic guarantees.","To illustrate the effectiveness of the proposed uncertainty quantification framework, we use a linear regression model for metric responses (known as the global Fr\\'echet model) in various clinical applications related to precision and digital medicine.","The different clinical outcomes analyzed are represented as complex statistical objects, including multivariate Euclidean data, Laplacian graphs, and probability distributions."],"url":"http://arxiv.org/abs/2405.05110v1","category":"math.ST"}
{"created":"2024-05-08 15:04:22","title":"Leveraging AES Padding: dBs for Nothing and FEC for Free in IoT Systems","abstract":"The Internet of Things (IoT) represents a significant advancement in digital technology, with its rapidly growing network of interconnected devices. This expansion, however, brings forth critical challenges in data security and reliability, especially under the threat of increasing cyber vulnerabilities. Addressing the security concerns, the Advanced Encryption Standard (AES) is commonly employed for secure encryption in IoT systems. Our study explores an innovative use of AES, by repurposing AES padding bits for error correction and thus introducing a dual-functional method that seamlessly integrates error-correcting capabilities into the standard encryption process. The integration of the state-of-the-art Guessing Random Additive Noise Decoder (GRAND) in the receiver's architecture facilitates the joint decoding and decryption process. This strategic approach not only preserves the existing structure of the transmitter but also significantly enhances communication reliability in noisy environments, achieving a notable over 3 dB gain in Block Error Rate (BLER). Remarkably, this enhanced performance comes with a minimal power overhead at the receiver - less than 15% compared to the traditional decryption-only process, underscoring the efficiency of our hardware design for IoT applications. This paper discusses a comprehensive analysis of our approach, particularly in energy efficiency and system performance, presenting a novel and practical solution for reliable IoT communications.","sentences":["The Internet of Things (IoT) represents a significant advancement in digital technology, with its rapidly growing network of interconnected devices.","This expansion, however, brings forth critical challenges in data security and reliability, especially under the threat of increasing cyber vulnerabilities.","Addressing the security concerns, the Advanced Encryption Standard (AES) is commonly employed for secure encryption in IoT systems.","Our study explores an innovative use of AES, by repurposing AES padding bits for error correction and thus introducing a dual-functional method that seamlessly integrates error-correcting capabilities into the standard encryption process.","The integration of the state-of-the-art Guessing Random Additive Noise Decoder (GRAND) in the receiver's architecture facilitates the joint decoding and decryption process.","This strategic approach not only preserves the existing structure of the transmitter but also significantly enhances communication reliability in noisy environments, achieving a notable over 3 dB gain in Block Error Rate (BLER).","Remarkably, this enhanced performance comes with a minimal power overhead at the receiver - less than 15% compared to the traditional decryption-only process, underscoring the efficiency of our hardware design for IoT applications.","This paper discusses a comprehensive analysis of our approach, particularly in energy efficiency and system performance, presenting a novel and practical solution for reliable IoT communications."],"url":"http://arxiv.org/abs/2405.05107v1","category":"cs.ET"}
{"created":"2024-05-08 14:40:45","title":"What doesn't kill Gaia makes her stronger","abstract":"Life on Earth has experienced numerous upheavals over its approximately 4 billion year history. In previous work we have discussed how interruptions to stability lead, on average, to increases in habitability over time, a tendency we called Entropic Gaia. Here we continue this exploration, working with the Tangled Nature Model of co-evolution, to understand how the evolutionary history of life is shaped by periods of acute environmental stress. We find that while these periods of stress pose a risk of complete extinction, they also create opportunities for evolutionary exploration which would otherwise be impossible, leading to more populous and stable states among the survivors than in alternative histories without a stress period. We also study how the duration, repetition and number of refugia into which life escapes during the perturbation affects the final outcome. The model results are discussed in relation to both Earth history and the search for alien life.","sentences":["Life on Earth has experienced numerous upheavals over its approximately 4 billion year history.","In previous work we have discussed how interruptions to stability lead, on average, to increases in habitability over time, a tendency we called Entropic Gaia.","Here we continue this exploration, working with the Tangled Nature Model of co-evolution, to understand how the evolutionary history of life is shaped by periods of acute environmental stress.","We find that while these periods of stress pose a risk of complete extinction, they also create opportunities for evolutionary exploration which would otherwise be impossible, leading to more populous and stable states among the survivors than in alternative histories without a stress period.","We also study how the duration, repetition and number of refugia into which life escapes during the perturbation affects the final outcome.","The model results are discussed in relation to both Earth history and the search for alien life."],"url":"http://arxiv.org/abs/2405.05091v1","category":"q-bio.PE"}
{"created":"2024-05-08 14:33:29","title":"Cryptocurrency Risk, Trust, and Acceptance in Thailand: A Comparative Study with Switzerland","abstract":"The adoption of the Pao Tang digital wallet in Thailand, promoted under the Khon la Krueng (50-50 Co-Payment) Scheme, illustrates Thailand's receptiveness to digital financial instruments, amassing over 40 million users in just three years during the COVID-19 social distancing era. Nevertheless, acceptance of this platform does not confirm a broad understanding of cryptocurrencies and Web 3.0 technologies in the region. Through a mix of documentary research, online surveys and a targeted interview with the Pao Tang app's founder, this study evaluates the factors behind the Pao Tang platform's success and contrasts it with digital practices in Switzerland. Preliminary outcomes reveal a pronounced knowledge gap in Thailand regarding decentralized technologies. With regulatory frameworks for Web 3.0 and digital currencies still nascent, this research underscores the need for further exploration, serving as a blueprint for shaping strategies, policies, and awareness campaigns in both countries.","sentences":["The adoption of the Pao Tang digital wallet in Thailand, promoted under the Khon la Krueng (50-50 Co-Payment) Scheme, illustrates Thailand's receptiveness to digital financial instruments, amassing over 40 million users in just three years during the COVID-19 social distancing era.","Nevertheless, acceptance of this platform does not confirm a broad understanding of cryptocurrencies and Web 3.0 technologies in the region.","Through a mix of documentary research, online surveys and a targeted interview with the Pao Tang app's founder, this study evaluates the factors behind the Pao Tang platform's success and contrasts it with digital practices in Switzerland.","Preliminary outcomes reveal a pronounced knowledge gap in Thailand regarding decentralized technologies.","With regulatory frameworks for Web 3.0 and digital currencies still nascent, this research underscores the need for further exploration, serving as a blueprint for shaping strategies, policies, and awareness campaigns in both countries."],"url":"http://arxiv.org/abs/2405.05086v1","category":"cs.CE"}
{"created":"2024-05-08 14:32:09","title":"Fair Voting Outcomes with Impact and Novelty Compromises? Unraveling Biases of Equal Shares in Participatory Budgeting","abstract":"Participatory budgeting, as a paradigm for democratic innovations, engages citizens in the distribution of a public budget to projects, which they propose and vote for implementation. So far, voting algorithms have been devised and studied in social choice literature to elect projects that are popular, while others prioritize on a proportional representation of voters' preferences, for instance, equal shares. However, the anticipated impact and novelty in the broader society by the winning projects, as selected by different algorithms, remains totally under-explored, lacking both a universal theory of impact for voting and a rigorous framework for impact and novelty assessments. This papers tackles this grand challenge towards new axiomatic foundations for designing effective and fair voting methods. This is via new and striking insights derived from a large-scale analysis of biases over 345 real-world voting outcomes, characterized for the first time by a novel portfolio of impact and novelty metrics. We find strong causal evidence that equal shares comes with impact loss in several infrastructural projects of different cost levels that have been so far over-represented. However, it also comes with a novel, yet over-represented, impact gain in welfare, education and culture. We discuss broader implications of these results and how impact loss can be mitigated at the stage of campaign design and project ideation.","sentences":["Participatory budgeting, as a paradigm for democratic innovations, engages citizens in the distribution of a public budget to projects, which they propose and vote for implementation.","So far, voting algorithms have been devised and studied in social choice literature to elect projects that are popular, while others prioritize on a proportional representation of voters' preferences, for instance, equal shares.","However, the anticipated impact and novelty in the broader society by the winning projects, as selected by different algorithms, remains totally under-explored, lacking both a universal theory of impact for voting and a rigorous framework for impact and novelty assessments.","This papers tackles this grand challenge towards new axiomatic foundations for designing effective and fair voting methods.","This is via new and striking insights derived from a large-scale analysis of biases over 345 real-world voting outcomes, characterized for the first time by a novel portfolio of impact and novelty metrics.","We find strong causal evidence that equal shares comes with impact loss in several infrastructural projects of different cost levels that have been so far over-represented.","However, it also comes with a novel, yet over-represented, impact gain in welfare, education and culture.","We discuss broader implications of these results and how impact loss can be mitigated at the stage of campaign design and project ideation."],"url":"http://arxiv.org/abs/2405.05085v2","category":"cs.MA"}
{"created":"2024-05-08 14:25:40","title":"Robust deep learning from weakly dependent data","abstract":"Recent developments on deep learning established some theoretical properties of deep neural networks estimators. However, most of the existing works on this topic are restricted to bounded loss functions or (sub)-Gaussian or bounded input. This paper considers robust deep learning from weakly dependent observations, with unbounded loss function and unbounded input/output. It is only assumed that the output variable has a finite $r$ order moment, with $r >1$. Non asymptotic bounds for the expected excess risk of the deep neural network estimator are established under strong mixing, and $\\psi$-weak dependence assumptions on the observations. We derive a relationship between these bounds and $r$, and when the data have moments of any order (that is $r=\\infty$), the convergence rate is close to some well-known results. When the target predictor belongs to the class of H\\\"older smooth functions with sufficiently large smoothness index, the rate of the expected excess risk for exponentially strongly mixing data is close to or as same as those for obtained with i.i.d. samples. Application to robust nonparametric regression and robust nonparametric autoregression are considered. The simulation study for models with heavy-tailed errors shows that, robust estimators with absolute loss and Huber loss function outperform the least squares method.","sentences":["Recent developments on deep learning established some theoretical properties of deep neural networks estimators.","However, most of the existing works on this topic are restricted to bounded loss functions or (sub)-Gaussian or bounded input.","This paper considers robust deep learning from weakly dependent observations, with unbounded loss function and unbounded input/output.","It is only assumed that the output variable has a finite $r$ order moment, with $r >1$.","Non asymptotic bounds for the expected excess risk of the deep neural network estimator are established under strong mixing, and $\\psi$-weak dependence assumptions on the observations.","We derive a relationship between these bounds and $r$, and when the data have moments of any order (that is $r=\\infty$), the convergence rate is close to some well-known results.","When the target predictor belongs to the class of H\\\"older smooth functions with sufficiently large smoothness index, the rate of the expected excess risk for exponentially strongly mixing data is close to or as same as those for obtained with i.i.d. samples.","Application to robust nonparametric regression and robust nonparametric autoregression are considered.","The simulation study for models with heavy-tailed errors shows that, robust estimators with absolute loss and Huber loss function outperform the least squares method."],"url":"http://arxiv.org/abs/2405.05081v1","category":"stat.ML"}
{"created":"2024-05-08 17:59:58","title":"OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies","abstract":"Event-based semantic segmentation (ESS) is a fundamental yet challenging task for event camera sensing. The difficulties in interpreting and annotating event data limit its scalability. While domain adaptation from images to event data can help to mitigate this issue, there exist data representational differences that require additional effort to resolve. In this work, for the first time, we synergize information from image, text, and event-data domains and introduce OpenESS to enable scalable ESS in an open-world, annotation-efficient manner. We achieve this goal by transferring the semantically rich CLIP knowledge from image-text pairs to event streams. To pursue better cross-modality adaptation, we propose a frame-to-event contrastive distillation and a text-to-event semantic consistency regularization. Experimental results on popular ESS benchmarks showed our approach outperforms existing methods. Notably, we achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either event or frame labels.","sentences":["Event-based semantic segmentation (ESS) is a fundamental yet challenging task for event camera sensing.","The difficulties in interpreting and annotating event data limit its scalability.","While domain adaptation from images to event data can help to mitigate this issue, there exist data representational differences that require additional effort to resolve.","In this work, for the first time, we synergize information from image, text, and event-data domains and introduce OpenESS to enable scalable ESS in an open-world, annotation-efficient manner.","We achieve this goal by transferring the semantically rich CLIP knowledge from image-text pairs to event streams.","To pursue better cross-modality adaptation, we propose a frame-to-event contrastive distillation and a text-to-event semantic consistency regularization.","Experimental results on popular ESS benchmarks showed our approach outperforms existing methods.","Notably, we achieve 53.93% and 43.31% mIoU on DDD17 and DSEC-Semantic without using either event or frame labels."],"url":"http://arxiv.org/abs/2405.05259v1","category":"cs.CV"}
{"created":"2024-05-08 17:30:50","title":"Stability and Performance Analysis of Discrete-Time ReLU Recurrent Neural Networks","abstract":"This paper presents sufficient conditions for the stability and $\\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions. These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs. We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU. Our stability and performance condition uses these QCs along with a \"lifted\" representation for the ReLU RNN. We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU. We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon.","sentences":["This paper presents sufficient conditions for the stability and $\\ell_2$-gain performance of recurrent neural networks (RNNs) with ReLU activation functions.","These conditions are derived by combining Lyapunov/dissipativity theory with Quadratic Constraints (QCs) satisfied by repeated ReLUs.","We write a general class of QCs for repeated RELUs using known properties for the scalar ReLU.","Our stability and performance condition uses these QCs along with a \"lifted\" representation for the ReLU RNN.","We show that the positive homogeneity property satisfied by a scalar ReLU does not expand the class of QCs for the repeated ReLU.","We present examples to demonstrate the stability / performance condition and study the effect of the lifting horizon."],"url":"http://arxiv.org/abs/2405.05236v1","category":"eess.SY"}
{"created":"2024-05-08 17:13:34","title":"Causal Duration Analysis with Diff-in-Diff","abstract":"In economic program evaluation, it is common to obtain panel data in which outcomes are indicators that an individual has reached an absorbing state. For example, they may indicate whether an individual has exited a period of unemployment, passed an exam, left a marriage, or had their parole revoked. The parallel trends assumption that underpins difference-in-differences generally fails in such settings. We suggest identifying conditions that are analogous to those of difference-in-differences but apply to hazard rates rather than mean outcomes. These alternative assumptions motivate estimators that retain the simplicity and transparency of standard diff-in-diff, and we suggest analogous specification tests. Our approach can be adapted to general linear restrictions between the hazard rates of different groups, motivating duration analogues of the triple differences and synthetic control methods. We apply our procedures to examine the impact of a policy that increased the generosity of unemployment benefits, using a cross-cohort comparison.","sentences":["In economic program evaluation, it is common to obtain panel data in which outcomes are indicators that an individual has reached an absorbing state.","For example, they may indicate whether an individual has exited a period of unemployment, passed an exam, left a marriage, or had their parole revoked.","The parallel trends assumption that underpins difference-in-differences generally fails in such settings.","We suggest identifying conditions that are analogous to those of difference-in-differences but apply to hazard rates rather than mean outcomes.","These alternative assumptions motivate estimators that retain the simplicity and transparency of standard diff-in-diff, and we suggest analogous specification tests.","Our approach can be adapted to general linear restrictions between the hazard rates of different groups, motivating duration analogues of the triple differences and synthetic control methods.","We apply our procedures to examine the impact of a policy that increased the generosity of unemployment benefits, using a cross-cohort comparison."],"url":"http://arxiv.org/abs/2405.05220v1","category":"econ.EM"}
{"created":"2024-05-08 17:09:03","title":"FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models","abstract":"The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts. To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \\textbf{FinePOSE}. It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance. (2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality. (3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step. Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods. We further extend FinePOSE to multi-human pose estimation. Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios. Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024.","sentences":["The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space.","Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task.","Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in different body parts.","To this end, we present a new Fine-Grained Prompt-Driven Denoiser based on a diffusion model for 3D HPE, named \\textbf{FinePOSE}.","It consists of three core blocks enhancing the reverse process of the diffusion model: (1) Fine-grained Part-aware Prompt learning (FPP) block constructs fine-grained part-aware prompts via coupling accessible texts and naturally feasible knowledge of body parts with learnable prompts to model implicit guidance.","(2) Fine-grained Prompt-pose Communication (FPC) block establishes fine-grained communications between learned part-aware prompts and poses to improve the denoising quality.","(3) Prompt-driven Timestamp Stylization (PTS) block integrates learned prompt embedding and temporal information related to the noise level to enable adaptive adjustment at each denoising step.","Extensive experiments on public single-human pose estimation datasets show that FinePOSE outperforms state-of-the-art methods.","We further extend FinePOSE to multi-human pose estimation.","Achieving 34.3mm average MPJPE on the EgoHumans dataset demonstrates the potential of FinePOSE to deal with complex multi-human scenarios.","Code is available at https://github.com/PKU-ICST-MIPL/FinePOSE_CVPR2024."],"url":"http://arxiv.org/abs/2405.05216v1","category":"cs.CV"}
{"created":"2024-05-08 17:01:41","title":"Exponential time propagators for elastodynamics","abstract":"We propose a computationally efficient and systematically convergent approach for elastodynamics simulations. We recast the second-order dynamical equation of elastodynamics into an equivalent first-order system of coupled equations, so as to express the solution in the form of a Magnus expansion. With any spatial discretization, it entails computing the exponential of a matrix acting upon a vector. We employ an adaptive Krylov subspace approach to inexpensively and and accurately evaluate the action of the exponential matrix on a vector. In particular, we use an apriori error estimate to predict the optimal Kyrlov subspace size required for each time-step size. We show that the Magnus expansion truncated after its first term provides quadratic and superquadratic convergence in the time-step for nonlinear and linear elastodynamics, respectively. We demonstrate the accuracy and efficiency of the proposed method for one linear (linear cantilever beam) and three nonlinear (nonlinear cantilever beam, soft tissue elastomer, and hyperelastic rubber) benchmark systems. For a desired accuracy in energy, displacement, and velocity, our method allows for $10-100\\times$ larger time-steps than conventional time-marching schemes such as Newmark-$\\beta$ method. Computationally, it translates to a $\\sim$$1000\\times$ and $\\sim$$10-100\\times$ speed-up over conventional time-marching schemes for linear and nonlinear elastodynamics, respectively.","sentences":["We propose a computationally efficient and systematically convergent approach for elastodynamics simulations.","We recast the second-order dynamical equation of elastodynamics into an equivalent first-order system of coupled equations, so as to express the solution in the form of a Magnus expansion.","With any spatial discretization, it entails computing the exponential of a matrix acting upon a vector.","We employ an adaptive Krylov subspace approach to inexpensively and and accurately evaluate the action of the exponential matrix on a vector.","In particular, we use an apriori error estimate to predict the optimal Kyrlov subspace size required for each time-step size.","We show that the Magnus expansion truncated after its first term provides quadratic and superquadratic convergence in the time-step for nonlinear and linear elastodynamics, respectively.","We demonstrate the accuracy and efficiency of the proposed method for one linear (linear cantilever beam) and three nonlinear (nonlinear cantilever beam, soft tissue elastomer, and hyperelastic rubber) benchmark systems.","For a desired accuracy in energy, displacement, and velocity, our method allows for $10-100\\times$ larger time-steps than conventional time-marching schemes such as Newmark-$\\beta$ method.","Computationally, it translates to a $\\sim$$1000\\times$ and $\\sim$$10-100\\times$ speed-up over conventional time-marching schemes for linear and nonlinear elastodynamics, respectively."],"url":"http://arxiv.org/abs/2405.05213v1","category":"math.NA"}
{"created":"2024-05-08 16:15:19","title":"Unclocklike biological oscillators with frequency memory","abstract":"Entrainment experiments on the vertebrate segmentation clock have revealed that embryonic oscillators actively change their internal frequency to adapt to the driving signal. This is neither consistent with a one-dimensional clock model nor with a limit-cycle model, but rather suggests a new \"unclocklike\" behavior. In this work, we propose simple biologically realistic descriptions of such internal frequency adaptation, where a phase oscillator activates a memory variable controlling the oscillator's frequency. We study two opposite limits for the control of the memory variable, one with a smooth phase-averaging memory field, and the other with a pulsatile, phase-dependent activation. Both models recapitulate intriguing properties of the entrained segmentation clock, such as very broad Arnold tongues and an entrainment phase plateauing with detuning. We compute analytically multiple properties of such systems, such as the entrainment phases and cycle shapes. We further describe new phenomena, including hysteresis in entrainment, bistability in the frequency of the entrained oscillator, and probabilistic entrainment. Our work shows oscillators with frequency memory can exhibit new classes of unclocklike properties, that can be tested experimentally.","sentences":["Entrainment experiments on the vertebrate segmentation clock have revealed that embryonic oscillators actively change their internal frequency to adapt to the driving signal.","This is neither consistent with a one-dimensional clock model nor with a limit-cycle model, but rather suggests a new \"unclocklike\" behavior.","In this work, we propose simple biologically realistic descriptions of such internal frequency adaptation, where a phase oscillator activates a memory variable controlling the oscillator's frequency.","We study two opposite limits for the control of the memory variable, one with a smooth phase-averaging memory field, and the other with a pulsatile, phase-dependent activation.","Both models recapitulate intriguing properties of the entrained segmentation clock, such as very broad Arnold tongues and an entrainment phase plateauing with detuning.","We compute analytically multiple properties of such systems, such as the entrainment phases and cycle shapes.","We further describe new phenomena, including hysteresis in entrainment, bistability in the frequency of the entrained oscillator, and probabilistic entrainment.","Our work shows oscillators with frequency memory can exhibit new classes of unclocklike properties, that can be tested experimentally."],"url":"http://arxiv.org/abs/2405.05180v1","category":"physics.bio-ph"}
{"created":"2024-05-08 16:07:56","title":"Custom Gradient Estimators are Straight-Through Estimators in Disguise","abstract":"Quantization-aware training comes with a fundamental challenge: the derivative of quantization functions such as rounding are zero almost everywhere and nonexistent elsewhere. Various differentiable approximations of quantization functions have been proposed to address this issue. In this paper, we prove that when the learning rate is sufficiently small, a large class of weight gradient estimators is equivalent with the straight through estimator (STE). Specifically, after swapping in the STE and adjusting both the weight initialization and the learning rate in SGD, the model will train in almost exactly the same way as it did with the original gradient estimator. Moreover, we show that for adaptive learning rate algorithms like Adam, the same result can be seen without any modifications to the weight initialization and learning rate. We experimentally show that these results hold for both a small convolutional model trained on the MNIST dataset and for a ResNet50 model trained on ImageNet.","sentences":["Quantization-aware training comes with a fundamental challenge: the derivative of quantization functions such as rounding are zero almost everywhere and nonexistent elsewhere.","Various differentiable approximations of quantization functions have been proposed to address this issue.","In this paper, we prove that when the learning rate is sufficiently small, a large class of weight gradient estimators is equivalent with the straight through estimator (STE).","Specifically, after swapping in the STE and adjusting both the weight initialization and the learning rate in SGD, the model will train in almost exactly the same way as it did with the original gradient estimator.","Moreover, we show that for adaptive learning rate algorithms like Adam, the same result can be seen without any modifications to the weight initialization and learning rate.","We experimentally show that these results hold for both a small convolutional model trained on the MNIST dataset and for a ResNet50 model trained on ImageNet."],"url":"http://arxiv.org/abs/2405.05171v1","category":"cs.LG"}
{"created":"2024-05-08 15:54:19","title":"A Dual-Motor Actuator for Ceiling Robots with High Force and High Speed Capabilities","abstract":"Patient transfer devices allow to move patients passively in hospitals and care centers. Instead of hoisting the patient, it would be beneficial in some cases to assist their movement, enabling them to move by themselves. However, patient assistance requires devices capable of precisely controlling output forces at significantly higher speeds than those used for patient transfers alone, and a single motor solution would be over-sized and show poor efficiency to do both functions. This paper presents a dual-motor actuator and control schemes adapted for a patient mobility equipment that can be used to transfer patients, assist patient in their movement, and help prevent falls. The prototype is shown to be able to lift patients weighing up to 318 kg, to assist a patient with a desired force of up to 100 kg with a precision of 7.8%. Also, a smart control scheme to manage falls is shown to be able to stop a patient who is falling by applying a desired deceleration.","sentences":["Patient transfer devices allow to move patients passively in hospitals and care centers.","Instead of hoisting the patient, it would be beneficial in some cases to assist their movement, enabling them to move by themselves.","However, patient assistance requires devices capable of precisely controlling output forces at significantly higher speeds than those used for patient transfers alone, and a single motor solution would be over-sized and show poor efficiency to do both functions.","This paper presents a dual-motor actuator and control schemes adapted for a patient mobility equipment that can be used to transfer patients, assist patient in their movement, and help prevent falls.","The prototype is shown to be able to lift patients weighing up to 318 kg, to assist a patient with a desired force of up to 100 kg with a precision of 7.8%.","Also, a smart control scheme to manage falls is shown to be able to stop a patient who is falling by applying a desired deceleration."],"url":"http://arxiv.org/abs/2405.05162v1","category":"cs.RO"}
{"created":"2024-05-08 15:13:33","title":"XAMPLER: Learning to Retrieve Cross-Lingual In-Context Examples","abstract":"Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in-context learning of English. However, adapting these methods to other languages, especially low-resource ones, presents challenges due to the scarcity of available cross-lingual retrievers and annotated data. In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data. XAMPLER first trains a retriever with positive/negative English samples, which are constructed based on the predictions of the multilingual large language model for in-context learning. Then, the trained retriever is directly employed to retrieve English examples as few-shot examples for in-context learning of target languages. Experiments on the massively multilingual text classification benchmark of SIB200 with 176 languages demonstrate that XAMPLER substantially improves the in-context learning performance across languages. Our code is available at https://github.com/cisnlp/XAMPLER.","sentences":["Recent studies have shown that leveraging off-the-shelf or fine-tuned retrievers, capable of retrieving high-quality in-context examples, significantly improves in-context learning of English.","However, adapting these methods to other languages, especially low-resource ones, presents challenges due to the scarcity of available cross-lingual retrievers and annotated data.","In this paper, we introduce XAMPLER: Cross-Lingual Example Retrieval, a method tailored to tackle the challenge of cross-lingual in-context learning using only annotated English data.","XAMPLER first trains a retriever with positive/negative English samples, which are constructed based on the predictions of the multilingual large language model for in-context learning.","Then, the trained retriever is directly employed to retrieve English examples as few-shot examples for in-context learning of target languages.","Experiments on the massively multilingual text classification benchmark of SIB200 with 176 languages demonstrate that XAMPLER substantially improves the in-context learning performance across languages.","Our code is available at https://github.com/cisnlp/XAMPLER."],"url":"http://arxiv.org/abs/2405.05116v1","category":"cs.CL"}
{"created":"2024-05-08 14:35:39","title":"Longitudinal spin polarization in a thermal model with dissipative corrections","abstract":"In this work, we address the problem of longitudinal spin polarization of the $\\Lambda$ hyperons produced in relativistic heavy-ion collisions. We combine a relativistic kinetic-theory framework that includes spin degrees of freedom treated in a classical way with the freeze-out parametrization used in previous investigations. The use of the kinetic theory allows us to incorporate dissipative corrections (due to the thermal shear and gradients of thermal vorticity) into the Pauli-Lubanski vector that determines spin polarization and can be directly compared with the experimental data. As in earlier similar studies, it turns out that a successful description of data can only be achieved with additional assumptions -- in our case, they involve the use of projected thermal vorticity and a suitably adjusted time for spin relaxation ($\\tau_s$). From our analysis, we find that $\\tau_s \\sim 5$ fm/$c$, which is comparable with other estimates.","sentences":["In this work, we address the problem of longitudinal spin polarization of the $\\Lambda$ hyperons produced in relativistic heavy-ion collisions.","We combine a relativistic kinetic-theory framework that includes spin degrees of freedom treated in a classical way with the freeze-out parametrization used in previous investigations.","The use of the kinetic theory allows us to incorporate dissipative corrections (due to the thermal shear and gradients of thermal vorticity) into the Pauli-Lubanski vector that determines spin polarization and can be directly compared with the experimental data.","As in earlier similar studies, it turns out that a successful description of data can only be achieved with additional assumptions -- in our case, they involve the use of projected thermal vorticity and a suitably adjusted time for spin relaxation ($\\tau_s$).","From our analysis, we find that $\\tau_s \\sim 5$ fm/$c$, which is comparable with other estimates."],"url":"http://arxiv.org/abs/2405.05089v1","category":"hep-ph"}
{"created":"2024-05-08 13:35:01","title":"An adaptive finite element multigrid solver using GPU acceleration","abstract":"Adaptive finite elements combined with geometric multigrid solvers are one of the most efficient numerical methods for problems such as the instationary Navier-Stokes equations. Yet despite their efficiency, computations remain expensive and the simulation of, for example, complex flow problems can take many hours or days. GPUs provide an interesting avenue to speed up the calculations due to their very large theoretical peak performance. However, the large degree of parallelism and non-standard API make the use of GPUs in scientific computing challenging. In this work, we develop a GPU acceleration for the adaptive finite element library Gascoigne and study its effectiveness for different systems of partial differential equations. Through the systematic formulation of all computations as linear algebra operations, we can employ GPU-accelerated linear algebra libraries, which simplifies the implementation and ensures the maintainability of the code while achieving very efficient GPU utilizations. Our results for a transport-diffusion equation, linear elasticity, and the instationary Navier-Stokes equations show substantial speedups of up to 20X compared to multi-core CPU implementations.","sentences":["Adaptive finite elements combined with geometric multigrid solvers are one of the most efficient numerical methods for problems such as the instationary Navier-Stokes equations.","Yet despite their efficiency, computations remain expensive and the simulation of, for example, complex flow problems can take many hours or days.","GPUs provide an interesting avenue to speed up the calculations due to their very large theoretical peak performance.","However, the large degree of parallelism and non-standard API make the use of GPUs in scientific computing challenging.","In this work, we develop a GPU acceleration for the adaptive finite element library Gascoigne and study its effectiveness for different systems of partial differential equations.","Through the systematic formulation of all computations as linear algebra operations, we can employ GPU-accelerated linear algebra libraries, which simplifies the implementation and ensures the maintainability of the code while achieving very efficient GPU utilizations.","Our results for a transport-diffusion equation, linear elasticity, and the instationary Navier-Stokes equations show substantial speedups of up to 20X compared to multi-core CPU implementations."],"url":"http://arxiv.org/abs/2405.05047v1","category":"math.NA"}
{"created":"2024-05-08 13:26:35","title":"Range separation of the interaction potential in intermolecular and intramolecular symmetry-adapted perturbation theory","abstract":"Symmetry-adapted perturbation theory (SAPT) is a popular and versatile tool to compute and decompose noncovalent interaction energies between molecules. The intramolecular SAPT (ISAPT) variant provides a similar energy decomposition between two nonbonded fragments of the same molecule, covalently connected by a third fragment. In this work, we explore an alternative approach where the noncovalent interaction is singled out by a range separation of the Coulomb potential. We investigate two common splittings of the $1/r$ potential into long-range and short-range parts based on the Gaussian and error functions, and approximate either the entire intermolecular/interfragment interaction or only its attractive terms by the long-range contribution. These range separation schemes are tested for a number of intermolecular and intramolecular complexes. We find that the energy corrections from range-separated SAPT or ISAPT are in reasonable agreement with complete SAPT/ISAPT data. This result should be contrasted with the inability of the long-range multipole expansion to describe crucial short-range charge penetration and exchange effects; it shows that the long-range interaction potential does not just recover the asymptotic interaction energy but also provides a useful account of short-range terms. The best consistency is attained for the error-function separation applied to all interaction terms, both attractive and repulsive. This study is the first step towards a fragmentation-free decomposition of intramolecular nonbonded energy.","sentences":["Symmetry-adapted perturbation theory (SAPT) is a popular and versatile tool to compute and decompose noncovalent interaction energies between molecules.","The intramolecular SAPT (ISAPT) variant provides a similar energy decomposition between two nonbonded fragments of the same molecule, covalently connected by a third fragment.","In this work, we explore an alternative approach where the noncovalent interaction is singled out by a range separation of the Coulomb potential.","We investigate two common splittings of the $1/r$ potential into long-range and short-range parts based on the Gaussian and error functions, and approximate either the entire intermolecular/interfragment interaction or only its attractive terms by the long-range contribution.","These range separation schemes are tested for a number of intermolecular and intramolecular complexes.","We find that the energy corrections from range-separated SAPT or ISAPT are in reasonable agreement with complete SAPT/ISAPT data.","This result should be contrasted with the inability of the long-range multipole expansion to describe crucial short-range charge penetration and exchange effects; it shows that the long-range interaction potential does not just recover the asymptotic interaction energy but also provides a useful account of short-range terms.","The best consistency is attained for the error-function separation applied to all interaction terms, both attractive and repulsive.","This study is the first step towards a fragmentation-free decomposition of intramolecular nonbonded energy."],"url":"http://arxiv.org/abs/2405.05041v1","category":"physics.chem-ph"}
{"created":"2024-05-08 13:07:34","title":"Dissipativity Conditions for Maximum Dynamic Loadability","abstract":"In this paper we consider a possibility of stabilizing very fast electromagnetic interactions between Inverter Based Resources (IBRs), known as the Control Induced System Stability problems. We propose that when these oscillatory interactions are controlled the ability of the grid to deliver power to loads at high rates will be greatly increased. We refer to this grid property as the dynamic grid loadability. The approach is to start by modeling the dynamical behavior of all components. Next, to avoid excessive complexity, interactions between components are captured in terms of unified technology-agnostic aggregate variables, instantaneous power and rate of change of instantaneous reactive power. Sufficient dissipativity conditions in terms of rate of change of energy conversion in components themselves and bounds on their rate of change of interactions are derived in support of achieving the maximum system loadability. These physically intuitive conditions are then used to derive methods to increase loadability using high switching frequency reactive power sources. Numerical simulations confirm the theoretical calculations, and shows dynamic load-side reactive power support increases stable dynamic loadability regions.","sentences":["In this paper we consider a possibility of stabilizing very fast electromagnetic interactions between Inverter Based Resources (IBRs), known as the Control Induced System Stability problems.","We propose that when these oscillatory interactions are controlled the ability of the grid to deliver power to loads at high rates will be greatly increased.","We refer to this grid property as the dynamic grid loadability.","The approach is to start by modeling the dynamical behavior of all components.","Next, to avoid excessive complexity, interactions between components are captured in terms of unified technology-agnostic aggregate variables, instantaneous power and rate of change of instantaneous reactive power.","Sufficient dissipativity conditions in terms of rate of change of energy conversion in components themselves and bounds on their rate of change of interactions are derived in support of achieving the maximum system loadability.","These physically intuitive conditions are then used to derive methods to increase loadability using high switching frequency reactive power sources.","Numerical simulations confirm the theoretical calculations, and shows dynamic load-side reactive power support increases stable dynamic loadability regions."],"url":"http://arxiv.org/abs/2405.05036v1","category":"eess.SY"}
{"created":"2024-05-08 12:42:50","title":"AI-based Dynamic Schedule Calculation in Time Sensitive Networks using GCN-TD3","abstract":"Offline scheduling in Time Sensitive Networking (TSN) utilizing the Time Aware Shaper (TAS) facilitates optimal deterministic latency and jitter-bounds calculation for Time- Triggered (TT) flows. However, the dynamic nature of traffic in industrial settings necessitates a strategy for adaptively scheduling flows without interrupting existing schedules. Our research identifies critical gaps in current dynamic scheduling methods for TSN and introduces the novel GCN-TD3 approach. This novel approach utilizes a Graph Convolutional Network (GCN) for representing the various relations within different components of TSN and employs the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to dynamically schedule any incoming flow. Additionally, an Integer Linear Programming (ILP) based offline scheduler is used both to initiate the simulation and serve as a fallback mechanism. This mechanism is triggered to recalculate the entire schedule when the predefined threshold of Gate Control List(GCL) length exceeds. Comparative analyses demonstrate that GCN-TD3 outperforms existing methods like Deep Double Q-Network (DDQN) and Deep Deterministic Policy Gradient (DDPG), exhibiting convergence within 4000 epochs with a 90\\% dynamic TT flow admission rate while maintaining deadlines and reducing jitter to as low as 2us. Finally, two modules were developed for the OMNeT++ simulator, facilitating dynamic simulation to evaluate the methodology.","sentences":["Offline scheduling in Time Sensitive Networking (TSN) utilizing the Time Aware Shaper (TAS) facilitates optimal deterministic latency and jitter-bounds calculation for Time- Triggered (TT) flows.","However, the dynamic nature of traffic in industrial settings necessitates a strategy for adaptively scheduling flows without interrupting existing schedules.","Our research identifies critical gaps in current dynamic scheduling methods for TSN and introduces the novel GCN-TD3 approach.","This novel approach utilizes a Graph Convolutional Network (GCN) for representing the various relations within different components of TSN and employs the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to dynamically schedule any incoming flow.","Additionally, an Integer Linear Programming (ILP) based offline scheduler is used both to initiate the simulation and serve as a fallback mechanism.","This mechanism is triggered to recalculate the entire schedule when the predefined threshold of Gate Control List(GCL) length exceeds.","Comparative analyses demonstrate that GCN-TD3 outperforms existing methods like Deep Double Q-Network (DDQN) and Deep Deterministic Policy Gradient (DDPG), exhibiting convergence within 4000 epochs with a 90\\% dynamic TT flow admission rate while maintaining deadlines and reducing jitter to as low as 2us.","Finally, two modules were developed for the OMNeT++ simulator, facilitating dynamic simulation to evaluate the methodology."],"url":"http://arxiv.org/abs/2405.05019v1","category":"cs.NI"}
{"created":"2024-05-08 12:26:15","title":"The Entropy Enigma: Success and Failure of Entropy Minimization","abstract":"Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time. EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes. In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps. We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy. After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy. Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels. Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy. Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\\%$, an improvement of $29.62\\%$ over the previous SoTA on this task. Our code is available at https://github.com/oripress/EntropyEnigma","sentences":["Entropy minimization (EM) is frequently used to increase the accuracy of classification models when they're faced with new data at test time.","EM is a self-supervised learning method that optimizes classifiers to assign even higher probabilities to their top predicted classes.","In this paper, we analyze why EM works when adapting a model for a few steps and why it eventually fails after adapting for many steps.","We show that, at first, EM causes the model to embed test images close to training images, thereby increasing model accuracy.","After many steps of optimization, EM makes the model embed test images far away from the embeddings of training images, which results in a degradation of accuracy.","Building upon our insights, we present a method for solving a practical problem: estimating a model's accuracy on a given arbitrary dataset without having access to its labels.","Our method estimates accuracy by looking at how the embeddings of input images change as the model is optimized to minimize entropy.","Experiments on 23 challenging datasets show that our method sets the SoTA with a mean absolute error of $5.75\\%$, an improvement of $29.62\\%$ over the previous SoTA on this task.","Our code is available at https://github.com/oripress/EntropyEnigma"],"url":"http://arxiv.org/abs/2405.05012v1","category":"cs.CV"}
{"created":"2024-05-08 12:20:10","title":"Analyzing design principles for competitive evolution strategies in constrained search spaces","abstract":"In the context of the 2018 IEEE Congress of Evolutionary Computation, the Matrix Adaptation Evolution Strategy for constrained optimization turned out to be notably successful in the competition on constrained single objective real-parameter optimization. Across all considered instances the so-called $\\epsilon$MAg-ES achieved the second rank. However, it can be considered to be the most successful participant in high dimensions. Unfortunately, the competition result does not provide any information about the modus operandi of a successful algorithm or its suitability for problems of a particular shape. To this end, the present paper is concerned with an extensive empirical analysis of the $\\epsilon$MAg-ES working principles that is expected to provide insights about the performance contribution of specific algorithmic components. To avoid rankings with respect to insignificant differences within the algorithm realizations, the paper additionally introduces significance testing into the ranking process.","sentences":["In the context of the 2018 IEEE Congress of Evolutionary Computation, the Matrix Adaptation Evolution Strategy for constrained optimization turned out to be notably successful in the competition on constrained single objective real-parameter optimization.","Across all considered instances the so-called $\\epsilon$MAg-ES achieved the second rank.","However, it can be considered to be the most successful participant in high dimensions.","Unfortunately, the competition result does not provide any information about the modus operandi of a successful algorithm or its suitability for problems of a particular shape.","To this end, the present paper is concerned with an extensive empirical analysis of the $\\epsilon$MAg-ES working principles that is expected to provide insights about the performance contribution of specific algorithmic components.","To avoid rankings with respect to insignificant differences within the algorithm realizations, the paper additionally introduces significance testing into the ranking process."],"url":"http://arxiv.org/abs/2405.05005v1","category":"cs.NE"}
{"created":"2024-05-08 12:19:08","title":"TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking","abstract":"There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion. However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data. To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity. In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes. The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF). Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively. Our code will be available at https://github.com/SSSpc333/TENet.","sentences":["There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion.","However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data.","To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity.","In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes.","The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF).","Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively.","Our code will be available at https://github.com/SSSpc333/TENet."],"url":"http://arxiv.org/abs/2405.05004v1","category":"cs.CV"}
{"created":"2024-05-08 09:38:11","title":"Guiding adaptive shrinkage by co-data to improve regression-based prediction and feature selection","abstract":"The high dimensional nature of genomics data complicates feature selection, in particular in low sample size studies - not uncommon in clinical prediction settings. It is widely recognized that complementary data on the features, `co-data', may improve results. Examples are prior feature groups or p-values from a related study. Such co-data are ubiquitous in genomics settings due to the availability of public repositories. Yet, the uptake of learning methods that structurally use such co-data is limited. We review guided adaptive shrinkage methods: a class of regression-based learners that use co-data to adapt the shrinkage parameters, crucial for the performance of those learners. We discuss technical aspects, but also the applicability in terms of types of co-data that can be handled. This class of methods is contrasted with several others. In particular, group-adaptive shrinkage is compared with the better-known sparse group-lasso by evaluating feature selection. Finally, we demonstrate the versatility of the guided shrinkage methodology by showing how to `do-it-yourself': we integrate implementations of a co-data learner and the spike-and-slab prior for the purpose of improving feature selection in genetics studies.","sentences":["The high dimensional nature of genomics data complicates feature selection, in particular in low sample size studies - not uncommon in clinical prediction settings.","It is widely recognized that complementary data on the features, `co-data', may improve results.","Examples are prior feature groups or p-values from a related study.","Such co-data are ubiquitous in genomics settings due to the availability of public repositories.","Yet, the uptake of learning methods that structurally use such co-data is limited.","We review guided adaptive shrinkage methods: a class of regression-based learners that use co-data to adapt the shrinkage parameters, crucial for the performance of those learners.","We discuss technical aspects, but also the applicability in terms of types of co-data that can be handled.","This class of methods is contrasted with several others.","In particular, group-adaptive shrinkage is compared with the better-known sparse group-lasso by evaluating feature selection.","Finally, we demonstrate the versatility of the guided shrinkage methodology by showing how to `do-it-yourself': we integrate implementations of a co-data learner and the spike-and-slab prior for the purpose of improving feature selection in genetics studies."],"url":"http://arxiv.org/abs/2405.04917v1","category":"stat.ME"}
{"created":"2024-05-08 09:13:10","title":"Self-supervised Gait-based Emotion Representation Learning from Selective Strongly Augmented Skeleton Sequences","abstract":"Emotion recognition is an important part of affective computing. Extracting emotional cues from human gaits yields benefits such as natural interaction, a nonintrusive nature, and remote detection. Recently, the introduction of self-supervised learning techniques offers a practical solution to the issues arising from the scarcity of labeled data in the field of gait-based emotion recognition. However, due to the limited diversity of gaits and the incompleteness of feature representations for skeletons, the existing contrastive learning methods are usually inefficient for the acquisition of gait emotions. In this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for self-supervised gait-based emotion representation, which aims to derive effective representations from limited labeled gait data. First, we propose an SSA method for the gait emotion recognition task, which includes upper body jitter and random spatiotemporal mask. The goal of SSA is to generate more diverse and targeted positive samples and prompt the model to learn more distinctive and robust feature representations. Then, we design a complementary feature fusion network (CFFN) that facilitates the integration of cross-domain information to acquire topological structural and global adaptive features. Finally, we implement the distributional divergence minimization loss to supervise the representation learning of the generally and strongly augmented queries. Our approach is validated on the Emotion-Gait (E-Gait) and Emilya datasets and outperforms the state-of-the-art methods under different evaluation protocols.","sentences":["Emotion recognition is an important part of affective computing.","Extracting emotional cues from human gaits yields benefits such as natural interaction, a nonintrusive nature, and remote detection.","Recently, the introduction of self-supervised learning techniques offers a practical solution to the issues arising from the scarcity of labeled data in the field of gait-based emotion recognition.","However, due to the limited diversity of gaits and the incompleteness of feature representations for skeletons, the existing contrastive learning methods are usually inefficient for the acquisition of gait emotions.","In this paper, we propose a contrastive learning framework utilizing selective strong augmentation (SSA) for self-supervised gait-based emotion representation, which aims to derive effective representations from limited labeled gait data.","First, we propose an SSA method for the gait emotion recognition task, which includes upper body jitter and random spatiotemporal mask.","The goal of SSA is to generate more diverse and targeted positive samples and prompt the model to learn more distinctive and robust feature representations.","Then, we design a complementary feature fusion network (CFFN) that facilitates the integration of cross-domain information to acquire topological structural and global adaptive features.","Finally, we implement the distributional divergence minimization loss to supervise the representation learning of the generally and strongly augmented queries.","Our approach is validated on the Emotion-Gait (E-Gait) and Emilya datasets and outperforms the state-of-the-art methods under different evaluation protocols."],"url":"http://arxiv.org/abs/2405.04900v1","category":"cs.CV"}
{"created":"2024-05-08 17:54:43","title":"Radiative corrections to the dynamics of a tracer particle coupled to a Bose scalar field","abstract":"We consider a tracer particle coupled to a Bose scalar field and study the regime where the field's propagation speed approaches infinity. For initial states devoid of field excitations, we introduce an effective approximation of the time-evolved wave function and prove its validity in Hilbert space norm. In this approximation, the field remains in the vacuum state while the tracer particle propagates with a modified dispersion relation. Physically, the new dispersion relation can be understood as the effect of radiative corrections due to interactions with virtual bosons. Mathematically, it is defined as the solution of a self-consistent equation, whose form depends on the relevant time scale.","sentences":["We consider a tracer particle coupled to a Bose scalar field and study the regime where the field's propagation speed approaches infinity.","For initial states devoid of field excitations, we introduce an effective approximation of the time-evolved wave function and prove its validity in Hilbert space norm.","In this approximation, the field remains in the vacuum state while the tracer particle propagates with a modified dispersion relation.","Physically, the new dispersion relation can be understood as the effect of radiative corrections due to interactions with virtual bosons.","Mathematically, it is defined as the solution of a self-consistent equation, whose form depends on the relevant time scale."],"url":"http://arxiv.org/abs/2405.05251v1","category":"math-ph"}
{"created":"2024-05-08 17:54:18","title":"DanceCam: atmospheric turbulence mitigation in wide-field astronomical images with short-exposure video streams","abstract":"We introduce a novel technique to mitigate the adverse effects of atmospheric turbulence on astronomical imaging. Utilizing a video-to-image neural network trained on simulated data, our method processes a sliding sequence of short-exposure ($\\sim$0.2s) stellar field images to reconstruct an image devoid of both turbulence and noise. We demonstrate the method with simulated and observed stellar fields, and show that the brief exposure sequence allows the network to accurately associate speckles to their originating stars and effectively disentangle light from adjacent sources across a range of seeing conditions, all while preserving flux to a lower signal-to-noise ratio than an average stack. This approach results in a marked improvement in angular resolution without compromising the astrometric stability of the final image.","sentences":["We introduce a novel technique to mitigate the adverse effects of atmospheric turbulence on astronomical imaging.","Utilizing a video-to-image neural network trained on simulated data, our method processes a sliding sequence of short-exposure ($\\sim$0.2s) stellar field images to reconstruct an image devoid of both turbulence and noise.","We demonstrate the method with simulated and observed stellar fields, and show that the brief exposure sequence allows the network to accurately associate speckles to their originating stars and effectively disentangle light from adjacent sources across a range of seeing conditions, all while preserving flux to a lower signal-to-noise ratio than an average stack.","This approach results in a marked improvement in angular resolution without compromising the astrometric stability of the final image."],"url":"http://arxiv.org/abs/2405.05250v1","category":"astro-ph.IM"}
{"created":"2024-05-08 17:27:11","title":"DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN Training","abstract":"Graph neural networks (GNNs) are machine learning models specialized for graph data and widely used in many applications. To train GNNs on large graphs that exceed CPU memory, several systems store data on disk and conduct out-of-core processing. However, these systems suffer from either read amplification when reading node features that are usually smaller than a disk page or degraded model accuracy by treating the graph as disconnected partitions. To close this gap, we build a system called DiskGNN, which achieves high I/O efficiency and thus fast training without hurting model accuracy. The key technique used by DiskGNN is offline sampling, which helps decouple graph sampling from model computation. In particular, by conducting graph sampling beforehand, DiskGNN acquires the node features that will be accessed by model computation, and such information is utilized to pack the target node features contiguously on disk to avoid read amplification. Besides, \\name{} also adopts designs including four-level feature store to fully utilize the memory hierarchy to cache node features and reduce disk access, batched packing to accelerate the feature packing process, and pipelined training to overlap disk access with other operations. We compare DiskGNN with Ginex and MariusGNN, which are state-of-the-art systems for out-of-core GNN training. The results show that DiskGNN can speed up the baselines by over 8x while matching their best model accuracy.","sentences":["Graph neural networks (GNNs) are machine learning models specialized for graph data and widely used in many applications.","To train GNNs on large graphs that exceed CPU memory, several systems store data on disk and conduct out-of-core processing.","However, these systems suffer from either read amplification when reading node features that are usually smaller than a disk page or degraded model accuracy by treating the graph as disconnected partitions.","To close this gap, we build a system called DiskGNN, which achieves high I/O efficiency and thus fast training without hurting model accuracy.","The key technique used by DiskGNN is offline sampling, which helps decouple graph sampling from model computation.","In particular, by conducting graph sampling beforehand, DiskGNN acquires the node features that will be accessed by model computation, and such information is utilized to pack the target node features contiguously on disk to avoid read amplification.","Besides, \\name{} also adopts designs including four-level feature store to fully utilize the memory hierarchy to cache node features and reduce disk access, batched packing to accelerate the feature packing process, and pipelined training to overlap disk access with other operations.","We compare DiskGNN with Ginex and MariusGNN, which are state-of-the-art systems for out-of-core GNN training.","The results show that DiskGNN can speed up the baselines by over 8x while matching their best model accuracy."],"url":"http://arxiv.org/abs/2405.05231v1","category":"cs.LG"}
{"created":"2024-05-08 17:15:06","title":"The Harnack inequality fails for nonlocal kinetic equations","abstract":"We prove that the Harnack inequality fails for nonlocal kinetic equations. Such equations arise as linearized models for the Boltzmann equation without cutoff and are of hypoelliptic type. We provide a counterexample for the simplest equation in this theory, the fractional Kolmogorov equation. Our result reflects a purely nonlocal phenomenon since the Harnack inequality holds true for local kinetic equations like the Kolmogorov equation.","sentences":["We prove that the Harnack inequality fails for nonlocal kinetic equations.","Such equations arise as linearized models for the Boltzmann equation without cutoff and are of hypoelliptic type.","We provide a counterexample for the simplest equation in this theory, the fractional Kolmogorov equation.","Our result reflects a purely nonlocal phenomenon since the Harnack inequality holds true for local kinetic equations like the Kolmogorov equation."],"url":"http://arxiv.org/abs/2405.05223v1","category":"math.AP"}
{"created":"2024-05-08 16:23:06","title":"ContEvol formalism: possibly a new twist on computational physics","abstract":"We present the ContEvol (continuous evolution) formalism, a family of implicit numerical methods which only need to solve linear equations and are almost symplectic. Combining values and derivatives of functions, ContEvol outputs allow users to recover full history and render full distributions. Using classic harmonic oscillator as a prototype case, we show that ContEvol methods lead to lower-order errors than two commonly used Runge--Kutta methods. Applying first-order ContEvol to simple celestial mechanics problems, we demonstrate that deviation from equation(s) of motion of ContEvol tracks is still $\\mathcal{O}(h^5)$ ($h$ is the step length) by our definition. Numerical experiments with an eccentric elliptical orbit indicate that first-order ContEvol is a viable alternative to classic Runge--Kutta or the symplectic leapfrog integrator. Solving stationary Schr\\\"odinger equation in quantum mechanics, we manifest ability of ContEvol to handle boundary value or eigenvalue problems. Important directions for future work, including mathematical foundation, higher dimensions, and technical improvements, are discussed at the end of this article.","sentences":["We present the ContEvol (continuous evolution) formalism, a family of implicit numerical methods which only need to solve linear equations and are almost symplectic.","Combining values and derivatives of functions, ContEvol outputs allow users to recover full history and render full distributions.","Using classic harmonic oscillator as a prototype case, we show that ContEvol methods lead to lower-order errors than two commonly used Runge--Kutta methods.","Applying first-order ContEvol to simple celestial mechanics problems, we demonstrate that deviation from equation(s) of motion of ContEvol tracks is still $\\mathcal{O}(h^5)$ ($h$ is the step length) by our definition.","Numerical experiments with an eccentric elliptical orbit indicate that first-order ContEvol is a viable alternative to classic Runge--Kutta or the symplectic leapfrog integrator.","Solving stationary Schr\\\"odinger equation in quantum mechanics, we manifest ability of ContEvol to handle boundary value or eigenvalue problems.","Important directions for future work, including mathematical foundation, higher dimensions, and technical improvements, are discussed at the end of this article."],"url":"http://arxiv.org/abs/2405.05188v1","category":"astro-ph.IM"}
{"created":"2024-05-08 16:22:47","title":"A score-based particle method for homogeneous Landau equation","abstract":"We propose a novel score-based particle method for solving the Landau equation in plasmas, that seamlessly integrates learning with structure-preserving particle methods [arXiv:1910.03080]. Building upon the Lagrangian viewpoint of the Landau equation, a central challenge stems from the nonlinear dependence of the velocity field on the density. Our primary innovation lies in recognizing that this nonlinearity is in the form of the score function, which can be approximated dynamically via techniques from score-matching. The resulting method inherits the conservation properties of the deterministic particle method while sidestepping the necessity for kernel density estimation in [arXiv:1910.03080]. This streamlines computation and enhances scalability with dimensionality. Furthermore, we provide a theoretical estimate by demonstrating that the KL divergence between our approximation and the true solution can be effectively controlled by the score-matching loss. Additionally, by adopting the flow map viewpoint, we derive an update formula for exact density computation. Extensive examples have been provided to show the efficiency of the method, including a physically relevant case of Coulomb interaction.","sentences":["We propose a novel score-based particle method for solving the Landau equation in plasmas, that seamlessly integrates learning with structure-preserving particle methods [arXiv:1910.03080].","Building upon the Lagrangian viewpoint of the Landau equation, a central challenge stems from the nonlinear dependence of the velocity field on the density.","Our primary innovation lies in recognizing that this nonlinearity is in the form of the score function, which can be approximated dynamically via techniques from score-matching.","The resulting method inherits the conservation properties of the deterministic particle method while sidestepping the necessity for kernel density estimation in [arXiv:1910.03080].","This streamlines computation and enhances scalability with dimensionality.","Furthermore, we provide a theoretical estimate by demonstrating that the KL divergence between our approximation and the true solution can be effectively controlled by the score-matching loss.","Additionally, by adopting the flow map viewpoint, we derive an update formula for exact density computation.","Extensive examples have been provided to show the efficiency of the method, including a physically relevant case of Coulomb interaction."],"url":"http://arxiv.org/abs/2405.05187v1","category":"math.NA"}
{"created":"2024-05-08 16:21:48","title":"Exploring the limits of the law of mass action in the mean field description of epidemics on Erd\u00f6s-R\u00e9nyi networks","abstract":"The manner epidemics occurs in a social network depends on various elements, with two of the most influential being the relationships among individuals in the population and the mechanism of transmission. In this paper, we assume that the social network has a homogeneous random topology of Erd\\\"os-R\\'enyi type. Regarding the contagion process, we assume that the probability of infection is proportional to the proportion of infected neighbours.   We consider a constant population, whose individuals are the nodes of the social network, formed by two variable subpopulations: Susceptible and Infected (SI model). We simulate the epidemics on this random network and study whether the average dynamics can be described using a mean field approach in terms of Differential Equations, employing the law of mass action. We show that a macroscopic description could be applied for low average connectivity, adjusting the value of the contagion rate in a precise function. This dependence is illustrated by calculating the transient times for each connectivity.   This study contributes valuable insights into the interplay between network connectivity, contagion dynamics, and the applicability of mean-field approximations. The delineation of critical thresholds and the distinctive behaviour at lower connectivity enable a deeper understanding of epidemic dynamics.","sentences":["The manner epidemics occurs in a social network depends on various elements, with two of the most influential being the relationships among individuals in the population and the mechanism of transmission.","In this paper, we assume that the social network has a homogeneous random topology of Erd\\\"os-R\\'enyi type.","Regarding the contagion process, we assume that the probability of infection is proportional to the proportion of infected neighbours.   ","We consider a constant population, whose individuals are the nodes of the social network, formed by two variable subpopulations: Susceptible and Infected (SI model).","We simulate the epidemics on this random network and study whether the average dynamics can be described using a mean field approach in terms of Differential Equations, employing the law of mass action.","We show that a macroscopic description could be applied for low average connectivity, adjusting the value of the contagion rate in a precise function.","This dependence is illustrated by calculating the transient times for each connectivity.   ","This study contributes valuable insights into the interplay between network connectivity, contagion dynamics, and the applicability of mean-field approximations.","The delineation of critical thresholds and the distinctive behaviour at lower connectivity enable a deeper understanding of epidemic dynamics."],"url":"http://arxiv.org/abs/2405.05186v1","category":"math.DS"}
{"created":"2024-05-08 16:20:06","title":"Exact solution of Dynamical Mean-Field Theory for a linear system with annealed disorder","abstract":"We investigate a disordered multi-dimensional linear system in which the interaction parameters vary stochastically in time with defined temporal correlations. We refer to this type of disorder as \"annealed\", in contrast to quenched disorder in which couplings are fixed in time. We extend Dynamical Mean-Field Theory to accommodate annealed disorder and employ it to find the exact solution of the linear model in the limit of a large number of degrees of freedom. Our analysis yields analytical results for the non-stationary auto-correlation, the stationary variance, the power spectral density, and the phase diagram of the model. Interestingly, some unexpected features emerge upon changing the correlation time of the interactions. The stationary variance of the system and the critical variance of the disorder are generally found to be a non-monotonic function of the correlation time of the interactions. We also find that in some cases a re-entrant phase transition takes place when this correlation time is varied.","sentences":["We investigate a disordered multi-dimensional linear system in which the interaction parameters vary stochastically in time with defined temporal correlations.","We refer to this type of disorder as \"annealed\", in contrast to quenched disorder in which couplings are fixed in time.","We extend Dynamical Mean-Field Theory to accommodate annealed disorder and employ it to find the exact solution of the linear model in the limit of a large number of degrees of freedom.","Our analysis yields analytical results for the non-stationary auto-correlation, the stationary variance, the power spectral density, and the phase diagram of the model.","Interestingly, some unexpected features emerge upon changing the correlation time of the interactions.","The stationary variance of the system and the critical variance of the disorder are generally found to be a non-monotonic function of the correlation time of the interactions.","We also find that in some cases a re-entrant phase transition takes place when this correlation time is varied."],"url":"http://arxiv.org/abs/2405.05183v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-08 16:12:04","title":"The local cohomology of vector fields","abstract":"We compute the local cohomology of vector fields on a manifold. In the smooth case this recovers the diagonal cohomology studied in work of Losik, Guillemin, Fuks and others. In the holomorphic case this cohomology has recently appeared in work of Hennion and Kapranov in their study of the Lie algebra cohomology of vector fields on a complex manifold. Additionally, we construct explicit representatives for cocycles in Gelfand--Fuks cohomology via descent.","sentences":["We compute the local cohomology of vector fields on a manifold.","In the smooth case this recovers the diagonal cohomology studied in work of Losik, Guillemin, Fuks and others.","In the holomorphic case this cohomology has recently appeared in work of Hennion and Kapranov in their study of the Lie algebra cohomology of vector fields on a complex manifold.","Additionally, we construct explicit representatives for cocycles in Gelfand--Fuks cohomology via descent."],"url":"http://arxiv.org/abs/2405.05174v1","category":"math.DG"}
{"created":"2024-05-08 16:06:22","title":"Asymmetric Symmetry Breaking: Unequal Probabilities of Vacuum Selection","abstract":"We study the probabilities of a field, subject to random perturbations, to roll down from the top of a potential, where the top is only $C^1$ continuous. We find that the probability to roll down to the left or right depends the square root of the second derivative of the potential at the top. We solve this problem theoretically by using the Fokker-Planck equations in stochastic process and verify our findings numerically. This study may potentially be a new mechanism to explain the origins of asymmetries in the Universe.","sentences":["We study the probabilities of a field, subject to random perturbations, to roll down from the top of a potential, where the top is only $C^1$ continuous.","We find that the probability to roll down to the left or right depends the square root of the second derivative of the potential at the top.","We solve this problem theoretically by using the Fokker-Planck equations in stochastic process and verify our findings numerically.","This study may potentially be a new mechanism to explain the origins of asymmetries in the Universe."],"url":"http://arxiv.org/abs/2405.05168v1","category":"hep-th"}
{"created":"2024-05-08 16:04:47","title":"Riemann problem for polychromatic soliton gases: a testbed for the spectral kinetic theory","abstract":"We use Riemann problem for soliton gas as a benchmark for a detailed numerical validation of the spectral kinetic theory for the Korteweg-de Vries (KdV) and the focusing nonlinear Schr\\\"odinger (fNLS) equations. We construct weak solutions to the kinetic equation for soliton gas describing collision of two dense \"polychromatic\" soliton gases composed of a finite number of \"monochromatic\" components, each consisting of solitons with nearly identical spectral parameters of the scattering operator in the Lax pair. The interaction between the gas components plays the key role in the emergent, large-scale hydrodynamic evolution. We then use the solutions of the spectral kinetic equation to evaluate macroscopic physical observables in KdV and fNLS soliton gases and compare them with the respective ensemble averages extracted from the \"exact\" soliton gas numerical solutions of the KdV and fNLS equations. To numerically synthesise dense polychromatic soliton gases we develop a new method which combines recent advances in the spectral theory of the so-called soliton condensates and the effective algorithms for the numerical realisation of $n$-soliton solutions with large $n$.","sentences":["We use Riemann problem for soliton gas as a benchmark for a detailed numerical validation of the spectral kinetic theory for the Korteweg-de Vries (KdV) and the focusing nonlinear Schr\\\"odinger (fNLS) equations.","We construct weak solutions to the kinetic equation for soliton gas describing collision of two dense \"polychromatic\" soliton gases composed of a finite number of \"monochromatic\" components, each consisting of solitons with nearly identical spectral parameters of the scattering operator in the Lax pair.","The interaction between the gas components plays the key role in the emergent, large-scale hydrodynamic evolution.","We then use the solutions of the spectral kinetic equation to evaluate macroscopic physical observables in KdV and fNLS soliton gases and compare them with the respective ensemble averages extracted from the \"exact\" soliton gas numerical solutions of the KdV and fNLS equations.","To numerically synthesise dense polychromatic soliton gases we develop a new method which combines recent advances in the spectral theory of the so-called soliton condensates and the effective algorithms for the numerical realisation of $n$-soliton solutions with large $n$."],"url":"http://arxiv.org/abs/2405.05166v1","category":"nlin.PS"}
{"created":"2024-05-08 15:51:32","title":"Analysis of the SQP Method for Hyperbolic PDE-Constrained Optimization in Acoustic Full Waveform Inversion","abstract":"In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered. The model arises from the acoustic full waveform inversion in the time domain. The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure. This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques. We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations. Our final theoretical result is the R-superlinear convergence of the SQP method.","sentences":["In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered.","The model arises from the acoustic full waveform inversion in the time domain.","The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure.","This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques.","We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations.","Our final theoretical result is the R-superlinear convergence of the SQP method."],"url":"http://arxiv.org/abs/2405.05158v1","category":"math.NA"}
{"created":"2024-05-08 15:40:00","title":"Early and elongated epochs of planetesimal dynamo generation","abstract":"Accreting in the first few Ma after Solar System formation, planetesimals record conditions in the protoplanetary disc and are the remnants of planetary formation processes. The meteorite paleomagnetic record carries key insights into the thermal history of planetesimals and their extent of differentiation. The current paradigm splits the paleomagnetic record into three magnetic field generation epochs: an early nebula field (<5Ma after CAI formation), followed by thermal dynamos (5-34 Ma after CAI formation), then a gap in dynamo generation, before the onset of core solidification and compositional dynamos. The split between these epochs has been defined using thermal evolution and dynamo generation models of planetesimals. Here we demonstrate these epochs are not as distinct as previously thought based on our refined thermal evolution model that includes more realistic parametrisations for mantle convection, non-eutectic core solidification and radiogenic $^{60}Fe$ in the core. Inclusion of $^{60}$ in the core brings forward the onset of dynamo generation to 1-2 Ma after CAI formation, which overlaps with the existence of the nebula field. The second epoch of dynamo generation begins prior to the onset of core solidification, suggesting this epoch is not purely compositionally driven. Planetesimal radius is the dominant control on dynamo generation, and the choice of reference viscosity can widen the gap between epochs of dynamo generation from 0-200 Ma. Overall, timings of different planetesimal magnetic field generation mechanisms are more variable. This alters the information we can glean from the meteorite paleomagnetic record about the early Solar System. Evidence for the nebula field requires more careful interpretation and young paleomagnetic remanences, for example in the pallasites, may not be evidence for planetesimal core solidification.","sentences":["Accreting in the first few Ma after Solar System formation, planetesimals record conditions in the protoplanetary disc and are the remnants of planetary formation processes.","The meteorite paleomagnetic record carries key insights into the thermal history of planetesimals and their extent of differentiation.","The current paradigm splits the paleomagnetic record into three magnetic field generation epochs: an early nebula field (<5Ma after CAI formation), followed by thermal dynamos (5-34 Ma after CAI formation), then a gap in dynamo generation, before the onset of core solidification and compositional dynamos.","The split between these epochs has been defined using thermal evolution and dynamo generation models of planetesimals.","Here we demonstrate these epochs are not as distinct as previously thought based on our refined thermal evolution model that includes more realistic parametrisations for mantle convection, non-eutectic core solidification and radiogenic $^{60}Fe$ in the core.","Inclusion of $^{60}$ in the core brings forward the onset of dynamo generation to 1-2 Ma after CAI formation, which overlaps with the existence of the nebula field.","The second epoch of dynamo generation begins prior to the onset of core solidification, suggesting this epoch is not purely compositionally driven.","Planetesimal radius is the dominant control on dynamo generation, and the choice of reference viscosity can widen the gap between epochs of dynamo generation from 0-200 Ma.","Overall, timings of different planetesimal magnetic field generation mechanisms are more variable.","This alters the information we can glean from the meteorite paleomagnetic record about the early Solar System.","Evidence for the nebula field requires more careful interpretation and young paleomagnetic remanences, for example in the pallasites, may not be evidence for planetesimal core solidification."],"url":"http://arxiv.org/abs/2405.05147v1","category":"astro-ph.EP"}
{"created":"2024-05-08 15:21:02","title":"A Gauss-Newton Method for ODE Optimal Tracking Control","abstract":"This paper introduces and analyses a continuous optimization approach to solve optimal control problems involving ordinary differential equations (ODEs) and tracking type objectives. Our aim is to determine control or input functions, and potentially uncertain model parameters, for a dynamical system described by an ODE. We establish the mathematical framework and define the optimal control problem with a tracking functional, incorporating regularization terms and box-constraints for model parameters and input functions. Treating the problem as an infinite-dimensional optimization problem, we employ a Gauss-Newton method within a suitable function space framework. This leads to an iterative process where, at each step, we solve a linearization of the problem by considering a linear surrogate model around the current solution estimate. The resulting linear auxiliary problem resembles a linear-quadratic ODE optimal tracking control problem, which we tackle using either a gradient descent method in function spaces or a Riccati-based approach. Finally, we present and analyze the efficacy of our method through numerical experiments.","sentences":["This paper introduces and analyses a continuous optimization approach to solve optimal control problems involving ordinary differential equations (ODEs) and tracking type objectives.","Our aim is to determine control or input functions, and potentially uncertain model parameters, for a dynamical system described by an ODE.","We establish the mathematical framework and define the optimal control problem with a tracking functional, incorporating regularization terms and box-constraints for model parameters and input functions.","Treating the problem as an infinite-dimensional optimization problem, we employ a Gauss-Newton method within a suitable function space framework.","This leads to an iterative process where, at each step, we solve a linearization of the problem by considering a linear surrogate model around the current solution estimate.","The resulting linear auxiliary problem resembles a linear-quadratic ODE optimal tracking control problem, which we tackle using either a gradient descent method in function spaces or a Riccati-based approach.","Finally, we present and analyze the efficacy of our method through numerical experiments."],"url":"http://arxiv.org/abs/2405.05124v1","category":"math.OC"}
{"created":"2024-05-08 15:09:51","title":"Anharmonic phonons with Gaussian processes","abstract":"We provide a method for calculating anharmonic lattice dynamics, by building a surrogate model based on Gaussian Processes (GPs). Due to the underlying Gaussian form of a GP, the model is infinitely differentiable. This allows us to train the model trained directly on forces (the derivative of PESs) reducing the evaluations required for a given accuracy. We can extend this differentiation to directly calculate second and third order force-constants using automatic differentiation (AD). For the five model materials we study, we find that the force-constants are in close agreement with a standard finite-displacement approach. Our method appears to be linear scaling in the number of atoms at predicting both second and third-order (anharmonic) force-constants.","sentences":["We provide a method for calculating anharmonic lattice dynamics, by building a surrogate model based on Gaussian Processes (GPs).","Due to the underlying Gaussian form of a GP, the model is infinitely differentiable.","This allows us to train the model trained directly on forces (the derivative of PESs) reducing the evaluations required for a given accuracy.","We can extend this differentiation to directly calculate second and third order force-constants using automatic differentiation (AD).","For the five model materials we study, we find that the force-constants are in close agreement with a standard finite-displacement approach.","Our method appears to be linear scaling in the number of atoms at predicting both second and third-order (anharmonic) force-constants."],"url":"http://arxiv.org/abs/2405.05113v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 14:49:27","title":"Biology-inspired joint distribution neurons based on Hierarchical Correlation Reconstruction allowing for multidirectional neural networks","abstract":"Popular artificial neural networks (ANN) optimize parameters for unidirectional value propagation, assuming some guessed parametrization type like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN). In contrast, for biological neurons e.g. \"it is not uncommon for axonal propagation of action potentials to happen in both directions\" \\cite{axon} - suggesting they are optimized to continuously operate in multidirectional way. Additionally, statistical dependencies a single neuron could model is not just (expected) value dependence, but entire joint distributions including also higher moments. Such agnostic joint distribution neuron would allow for multidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or $\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing. There will be discussed Hierarchical Correlation Reconstruction (HCR) for such neuron model: assuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type parametrization of joint distribution with polynomial basis $f_i$, which allows for flexible, inexpensive processing including nonlinearities, direct model estimation and update, trained through standard backpropagation or novel ways for such structure up to tensor decomposition. Using only pairwise (input-output) dependencies, its expected value prediction becomes KAN-like with trained activation functions as polynomials, can be extended by adding higher order dependencies through included products - in conscious interpretable way, allowing for multidirectional propagation of both values and probability densities.","sentences":["Popular artificial neural networks (ANN) optimize parameters for unidirectional value propagation, assuming some guessed parametrization type like Multi-Layer Perceptron (MLP) or Kolmogorov-Arnold Network (KAN).","In contrast, for biological neurons e.g. \"it is not uncommon for axonal propagation of action potentials to happen in both directions\" \\cite{axon} - suggesting they are optimized to continuously operate in multidirectional way.","Additionally, statistical dependencies a single neuron could model is not just (expected) value dependence, but entire joint distributions including also higher moments.","Such agnostic joint distribution neuron would allow for multidirectional propagation (of distributions or values) e.g. $\\rho(x|y,z)$ or $\\rho(y,z|x)$ by substituting to $\\rho(x,y,z)$ and normalizing.","There will be discussed Hierarchical Correlation Reconstruction (HCR) for such neuron model: assuming $\\rho(x,y,z)=\\sum_{ijk} a_{ijk} f_i(x) f_j(y) f_k(z)$ type parametrization of joint distribution with polynomial basis $f_i$, which allows for flexible, inexpensive processing including nonlinearities, direct model estimation and update, trained through standard backpropagation or novel ways for such structure up to tensor decomposition.","Using only pairwise (input-output) dependencies, its expected value prediction becomes KAN-like with trained activation functions as polynomials, can be extended by adding higher order dependencies through included products - in conscious interpretable way, allowing for multidirectional propagation of both values and probability densities."],"url":"http://arxiv.org/abs/2405.05097v1","category":"cs.LG"}
{"created":"2024-05-08 14:43:20","title":"A Hierarchical Approach to Quantum Many-Body Systems in Structured Environments","abstract":"Cavity quantum materials combine the rich many-body physics of condensed matter systems with strong coupling to the surrounding electromagnetic field, which presents both novel prospects and intricate challenges. One is often interested in the properties of one specific aspect of the material, e.g. the electronic many-body dynamics, subject to a structured bath of phononic and photonic modes. Open quantum systems featuring non-Markovian dynamics are routinely solved using techniques such as the Hierarchical Equations of Motion (HEOM) but their usage of the system density-matrix renders them intractable for many-body systems. Here, we combine the HEOM with the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy to reach a consistent and rigorous description of open many-body systems and their quantum dynamics. We demonstrate first the strength and limitations of this stacked hierarchy for superradiant emission and spin-squeezing of established quantum optical models before presenting its full potential for quantum many-body systems. In particular, we explicitly simulate the impact of charge noise on the dynamic of the Fermi-Hubbard model subject to a structured bath comprising cavity and vibro-phononic environment. Strong optical coupling not only modifies the dynamic of the many-body system but serves furthermore as measurement channel providing information about the correlated motion imprinted by charge noise. Our work establishes an accessible, yet rigorous, route between condensed matter and quantum optics, fostering the growth of a new domain at their interface.","sentences":["Cavity quantum materials combine the rich many-body physics of condensed matter systems with strong coupling to the surrounding electromagnetic field, which presents both novel prospects and intricate challenges.","One is often interested in the properties of one specific aspect of the material, e.g. the electronic many-body dynamics, subject to a structured bath of phononic and photonic modes.","Open quantum systems featuring non-Markovian dynamics are routinely solved using techniques such as the Hierarchical Equations of Motion (HEOM) but their usage of the system density-matrix renders them intractable for many-body systems.","Here, we combine the HEOM with the Bogoliubov-Born-Green-Kirkwood-Yvon (BBGKY) hierarchy to reach a consistent and rigorous description of open many-body systems and their quantum dynamics.","We demonstrate first the strength and limitations of this stacked hierarchy for superradiant emission and spin-squeezing of established quantum optical models before presenting its full potential for quantum many-body systems.","In particular, we explicitly simulate the impact of charge noise on the dynamic of the Fermi-Hubbard model subject to a structured bath comprising cavity and vibro-phononic environment.","Strong optical coupling not only modifies the dynamic of the many-body system but serves furthermore as measurement channel providing information about the correlated motion imprinted by charge noise.","Our work establishes an accessible, yet rigorous, route between condensed matter and quantum optics, fostering the growth of a new domain at their interface."],"url":"http://arxiv.org/abs/2405.05093v1","category":"quant-ph"}
{"created":"2024-05-08 14:18:36","title":"Subsystem Information Capacity in Random Circuits and Hamiltonian Dynamics","abstract":"In this study, we explore the information capacity of open quantum systems, focusing on the effective channels formed by the subsystem of random quantum circuits and quantum Hamiltonian evolution. By analyzing the subsystem information capacity, which is closely linked to quantum coherent information of these effective quantum channels, we uncover a diverse range of dynamical and steady behaviors depending on the types of evolution. Therefore, the subsystem information capacity serves as a valuable tool for studying the intrinsic nature of various dynamical phases, such as integrable, localized, thermalized, and topological systems. We also reveal the impact of different initial information encoding schemes on information dynamics including one-to-one, one-to-many, and many-to-many. To support our findings, we provide representative examples for numerical simulations, including random quantum circuits with or without mid-circuit measurements, random Clifford Floquet circuits, free and interacting Aubry-Andr\\'e models, and Su-Schrieffer-Heeger models. Those numerical results are further quantitatively explained using the effective statistical model mapping and the quasiparticle picture in the cases of random circuits and non-interacting Hamiltonian dynamics, respectively.","sentences":["In this study, we explore the information capacity of open quantum systems, focusing on the effective channels formed by the subsystem of random quantum circuits and quantum Hamiltonian evolution.","By analyzing the subsystem information capacity, which is closely linked to quantum coherent information of these effective quantum channels, we uncover a diverse range of dynamical and steady behaviors depending on the types of evolution.","Therefore, the subsystem information capacity serves as a valuable tool for studying the intrinsic nature of various dynamical phases, such as integrable, localized, thermalized, and topological systems.","We also reveal the impact of different initial information encoding schemes on information dynamics including one-to-one, one-to-many, and many-to-many.","To support our findings, we provide representative examples for numerical simulations, including random quantum circuits with or without mid-circuit measurements, random Clifford Floquet circuits, free and interacting Aubry-Andr\\'e models, and Su-Schrieffer-Heeger models.","Those numerical results are further quantitatively explained using the effective statistical model mapping and the quasiparticle picture in the cases of random circuits and non-interacting Hamiltonian dynamics, respectively."],"url":"http://arxiv.org/abs/2405.05076v1","category":"quant-ph"}
{"created":"2024-05-08 14:01:48","title":"Quasi-Banach Schatten-von Neumann properties in Weyl-H\u00f6rmander calculus","abstract":"We study structural properties of Wiener-Lebesgue spaces with respect to a slowly varying metrics and certain Lebesgue parameters. For $p\\in (0,1]$, we deduce Schatten-$p$ properties for pseudo-differential operators whose symbols, together with their derivatives, obey suitable Wiener-Lebesgue-boundedness conditions. Especially, we perform such investigations for the Weyl-H\\\"ormander calculus. Finally, we apply our results to global-type SG and Shubin pseudo-differential operators.","sentences":["We study structural properties of Wiener-Lebesgue spaces with respect to a slowly varying metrics and certain Lebesgue parameters.","For $p\\in (0,1]$, we deduce Schatten-$p$ properties for pseudo-differential operators whose symbols, together with their derivatives, obey suitable Wiener-Lebesgue-boundedness conditions.","Especially, we perform such investigations for the Weyl-H\\\"ormander calculus.","Finally, we apply our results to global-type SG and Shubin pseudo-differential operators."],"url":"http://arxiv.org/abs/2405.05065v1","category":"math.FA"}
{"created":"2024-05-08 13:03:55","title":"Multi-fidelity Hamiltonian Monte Carlo","abstract":"Numerous applications in biology, statistics, science, and engineering require generating samples from high-dimensional probability distributions. In recent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a state-of-the-art Markov chain Monte Carlo technique, exploiting the shape of such high-dimensional target distributions to efficiently generate samples. Despite its impressive empirical success and increasing popularity, its wide-scale adoption remains limited due to the high computational cost of gradient calculation. Moreover, applying this method is impossible when the gradient of the posterior cannot be computed (for example, with black-box simulators). To overcome these challenges, we propose a novel two-stage Hamiltonian Monte Carlo algorithm with a surrogate model. In this multi-fidelity algorithm, the acceptance probability is computed in the first stage via a standard HMC proposal using an inexpensive differentiable surrogate model, and if the proposal is accepted, the posterior is evaluated in the second stage using the high-fidelity (HF) numerical solver. Splitting the standard HMC algorithm into these two stages allows for approximating the gradient of the posterior efficiently, while producing accurate posterior samples by using HF numerical solvers in the second stage. We demonstrate the effectiveness of this algorithm for a range of problems, including linear and nonlinear Bayesian inverse problems with in-silico data and experimental data. The proposed algorithm is shown to seamlessly integrate with various low-fidelity and HF models, priors, and datasets. Remarkably, our proposed method outperforms the traditional HMC algorithm in both computational and statistical efficiency by several orders of magnitude, all while retaining or improving the accuracy in computed posterior statistics.","sentences":["Numerous applications in biology, statistics, science, and engineering require generating samples from high-dimensional probability distributions.","In recent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a state-of-the-art Markov chain Monte Carlo technique, exploiting the shape of such high-dimensional target distributions to efficiently generate samples.","Despite its impressive empirical success and increasing popularity, its wide-scale adoption remains limited due to the high computational cost of gradient calculation.","Moreover, applying this method is impossible when the gradient of the posterior cannot be computed (for example, with black-box simulators).","To overcome these challenges, we propose a novel two-stage Hamiltonian Monte Carlo algorithm with a surrogate model.","In this multi-fidelity algorithm, the acceptance probability is computed in the first stage via a standard HMC proposal using an inexpensive differentiable surrogate model, and if the proposal is accepted, the posterior is evaluated in the second stage using the high-fidelity (HF) numerical solver.","Splitting the standard HMC algorithm into these two stages allows for approximating the gradient of the posterior efficiently, while producing accurate posterior samples by using HF numerical solvers in the second stage.","We demonstrate the effectiveness of this algorithm for a range of problems, including linear and nonlinear Bayesian inverse problems with in-silico data and experimental data.","The proposed algorithm is shown to seamlessly integrate with various low-fidelity and HF models, priors, and datasets.","Remarkably, our proposed method outperforms the traditional HMC algorithm in both computational and statistical efficiency by several orders of magnitude, all while retaining or improving the accuracy in computed posterior statistics."],"url":"http://arxiv.org/abs/2405.05033v1","category":"cs.CE"}
{"created":"2024-05-08 13:00:56","title":"Mitigating Bias Using Model-Agnostic Data Attribution","abstract":"Mitigating bias in machine learning models is a critical endeavor for ensuring fairness and equity. In this paper, we propose a novel approach to address bias by leveraging pixel image attributions to identify and regularize regions of images containing significant information about bias attributes. Our method utilizes a model-agnostic approach to extract pixel attributions by employing a convolutional neural network (CNN) classifier trained on small image patches. By training the classifier to predict a property of the entire image using only a single patch, we achieve region-based attributions that provide insights into the distribution of important information across the image. We propose utilizing these attributions to introduce targeted noise into datasets with confounding attributes that bias the data, thereby constraining neural networks from learning these biases and emphasizing the primary attributes. Our approach demonstrates its efficacy in enabling the training of unbiased classifiers on heavily biased datasets.","sentences":["Mitigating bias in machine learning models is a critical endeavor for ensuring fairness and equity.","In this paper, we propose a novel approach to address bias by leveraging pixel image attributions to identify and regularize regions of images containing significant information about bias attributes.","Our method utilizes a model-agnostic approach to extract pixel attributions by employing a convolutional neural network (CNN) classifier trained on small image patches.","By training the classifier to predict a property of the entire image using only a single patch, we achieve region-based attributions that provide insights into the distribution of important information across the image.","We propose utilizing these attributions to introduce targeted noise into datasets with confounding attributes that bias the data, thereby constraining neural networks from learning these biases and emphasizing the primary attributes.","Our approach demonstrates its efficacy in enabling the training of unbiased classifiers on heavily biased datasets."],"url":"http://arxiv.org/abs/2405.05031v1","category":"cs.CV"}
{"created":"2024-05-08 12:58:39","title":"Stability And Uncertainty Propagation In Power Networks: A Lyapunov-based Approach With Applications To Renewable Resources Allocation","abstract":"The rapid increase in the integration of intermittent and stochastic renewable energy resources (RER) introduces challenging issues related to power system stability. Interestingly, identifying grid nodes that can best support stochastic loads from RER, has gained recent interest. Methods based on Lyapunov stability are commonly exploited to assess the stability of power networks. These strategies approach quantifying system stability while considering: (i) simplified reduced order power system models that do not model power flow constraints, or (ii) datadriven methods that are prone to measurement noise and hence can inaccurately depict stochastic loads as system instability. In this paper, while considering a nonlinear differential algebraic equation (NL-DAE) model, we introduce a new method for assessing the impact of uncertain renewable power injections on the stability of power system nodes/buses. The identification of stable nodes informs the operator/utility on how renewables injections affect the stability of the grid. The proposed method is based on optimizing metrics equivalent to the Lyapunov spectrum of exponents; its underlying properties result in a computationally efficient and scalable stable node identification algorithm for renewable energy resources allocation. The proposed method is validated on the IEEE 9-bus and 200-bus networks","sentences":["The rapid increase in the integration of intermittent and stochastic renewable energy resources (RER) introduces challenging issues related to power system stability.","Interestingly, identifying grid nodes that can best support stochastic loads from RER, has gained recent interest.","Methods based on Lyapunov stability are commonly exploited to assess the stability of power networks.","These strategies approach quantifying system stability while considering: (i) simplified reduced order power system models that do not model power flow constraints, or (ii) datadriven methods that are prone to measurement noise and hence can inaccurately depict stochastic loads as system instability.","In this paper, while considering a nonlinear differential algebraic equation (NL-DAE) model, we introduce a new method for assessing the impact of uncertain renewable power injections on the stability of power system nodes/buses.","The identification of stable nodes informs the operator/utility on how renewables injections affect the stability of the grid.","The proposed method is based on optimizing metrics equivalent to the Lyapunov spectrum of exponents; its underlying properties result in a computationally efficient and scalable stable node identification algorithm for renewable energy resources allocation.","The proposed method is validated on the IEEE 9-bus and 200-bus networks"],"url":"http://arxiv.org/abs/2405.05028v1","category":"eess.SY"}
{"created":"2024-05-08 12:54:52","title":"On some intrinsic differentiability properties for Absolutely continuous functions between Carnot groups and the Area formula","abstract":"We discuss Q-absolutely continuous functions between Carnot groups, following Maly's definition for maps of several variables. Such maps enjoy nice regularity properties, like continuity, Pansu differentiability a.e., weak differentiability and an Area formula. Furthermore, we extend Stein's result concerning the sharp condition for continuity and differentiability a.e. of a Sobolev map in terms of the integrability of the weak gradient: more precisely, we prove that a Sobolev map between Carnot groups with horizontal gradient of its sections uniformly bounded in L(Q,1) admits a representative which is Q-absolutely continuous.","sentences":["We discuss Q-absolutely continuous functions between Carnot groups, following Maly's definition for maps of several variables.","Such maps enjoy nice regularity properties, like continuity, Pansu differentiability a.e., weak differentiability and an Area formula.","Furthermore, we extend Stein's result concerning the sharp condition for continuity and differentiability a.e. of a Sobolev map in terms of the integrability of the weak gradient: more precisely, we prove that a Sobolev map between Carnot groups with horizontal gradient of its sections uniformly bounded in L(Q,1) admits a representative which is Q-absolutely continuous."],"url":"http://arxiv.org/abs/2405.05024v1","category":"math.FA"}
{"created":"2024-05-08 12:32:07","title":"TGTM: TinyML-based Global Tone Mapping for HDR Sensors","abstract":"Advanced driver assistance systems (ADAS) relying on multiple cameras are increasingly prevalent in vehicle technology. Yet, conventional imaging sensors struggle to capture clear images in conditions with intense illumination contrast, such as tunnel exits, due to their limited dynamic range. Introducing high dynamic range (HDR) sensors addresses this issue. However, the process of converting HDR content to a displayable range via tone mapping often leads to inefficient computations, when performed directly on pixel data. In this paper, we focus on HDR image tone mapping using a lightweight neural network applied on image histogram data. Our proposed TinyML-based global tone mapping method, termed as TGTM, operates at 9,000 FLOPS per RGB image of any resolution. Additionally, TGTM offers a generic approach that can be incorporated to any classical tone mapping method. Experimental results demonstrate that TGTM outperforms state-of-the-art methods on real HDR camera images by up to 5.85 dB higher PSNR with orders of magnitude less computations.","sentences":["Advanced driver assistance systems (ADAS) relying on multiple cameras are increasingly prevalent in vehicle technology.","Yet, conventional imaging sensors struggle to capture clear images in conditions with intense illumination contrast, such as tunnel exits, due to their limited dynamic range.","Introducing high dynamic range (HDR) sensors addresses this issue.","However, the process of converting HDR content to a displayable range via tone mapping often leads to inefficient computations, when performed directly on pixel data.","In this paper, we focus on HDR image tone mapping using a lightweight neural network applied on image histogram data.","Our proposed TinyML-based global tone mapping method, termed as TGTM, operates at 9,000 FLOPS per RGB image of any resolution.","Additionally, TGTM offers a generic approach that can be incorporated to any classical tone mapping method.","Experimental results demonstrate that TGTM outperforms state-of-the-art methods on real HDR camera images by up to 5.85 dB higher PSNR with orders of magnitude less computations."],"url":"http://arxiv.org/abs/2405.05016v1","category":"cs.CV"}
{"created":"2024-05-08 12:25:21","title":"${M^2D}$NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields","abstract":"Neural fields (NeRF) have emerged as a promising approach for representing continuous 3D scenes. Nevertheless, the lack of semantic encoding in NeRFs poses a significant challenge for scene decomposition. To address this challenge, we present a single model, Multi-Modal Decomposition NeRF (${M^2D}$NeRF), that is capable of both text-based and visual patch-based edits. Specifically, we use multi-modal feature distillation to integrate teacher features from pretrained visual and language models into 3D semantic feature volumes, thereby facilitating consistent 3D editing. To enforce consistency between the visual and language features in our 3D feature volumes, we introduce a multi-modal similarity constraint. We also introduce a patch-based joint contrastive loss that helps to encourage object-regions to coalesce in the 3D feature space, resulting in more precise boundaries. Experiments on various real-world scenes show superior performance in 3D scene decomposition tasks compared to prior NeRF-based methods.","sentences":["Neural fields (NeRF) have emerged as a promising approach for representing continuous 3D scenes.","Nevertheless, the lack of semantic encoding in NeRFs poses a significant challenge for scene decomposition.","To address this challenge, we present a single model, Multi-Modal Decomposition NeRF (${M^2D}$NeRF), that is capable of both text-based and visual patch-based edits.","Specifically, we use multi-modal feature distillation to integrate teacher features from pretrained visual and language models into 3D semantic feature volumes, thereby facilitating consistent 3D editing.","To enforce consistency between the visual and language features in our 3D feature volumes, we introduce a multi-modal similarity constraint.","We also introduce a patch-based joint contrastive loss that helps to encourage object-regions to coalesce in the 3D feature space, resulting in more precise boundaries.","Experiments on various real-world scenes show superior performance in 3D scene decomposition tasks compared to prior NeRF-based methods."],"url":"http://arxiv.org/abs/2405.05010v1","category":"cs.CV"}
{"created":"2024-05-08 12:25:20","title":"On Solutions of Systems of Differential Equations on Half-Line with Summable Coefficients","abstract":"We consider a system of differential equations and obtain its solutions with exponential asymptotics and analyticity with respect to the spectral parameter. Solutions of such type have importance in studying spectral properties of differential operators. Here, we consider the system of first-order differential equations on a half-line with summable coefficients, containing a nonlinear dependence on the spectral parameter. We obtain fundamental systems of solutions with analyticity in certain sectors, in which it is possible to apply the method of successive approximations. We also construct non-fundamental systems of solutions with analyticity in a large sector, including two previously considered neighboring sectors. The obtained results admit applications in studying inverse spectral problems for the higher-order differential operators with distribution coefficients.","sentences":["We consider a system of differential equations and obtain its solutions with exponential asymptotics and analyticity with respect to the spectral parameter.","Solutions of such type have importance in studying spectral properties of differential operators.","Here, we consider the system of first-order differential equations on a half-line with summable coefficients, containing a nonlinear dependence on the spectral parameter.","We obtain fundamental systems of solutions with analyticity in certain sectors, in which it is possible to apply the method of successive approximations.","We also construct non-fundamental systems of solutions with analyticity in a large sector, including two previously considered neighboring sectors.","The obtained results admit applications in studying inverse spectral problems for the higher-order differential operators with distribution coefficients."],"url":"http://arxiv.org/abs/2405.05009v1","category":"math.CA"}
{"created":"2024-05-08 12:24:03","title":"Stochastic spatial Lotka-Volterra predator-prey models","abstract":"Stochastic, spatially extended models for predator-prey interaction display spatio-temporal structures that are not captured by the Lotka-Volterra mean-field rate equations. These spreading activity fronts reflect persistent correlations between predators and prey that can be analyzed through field-theoretic methods. Introducing local restrictions on the prey population induces a predator extinction threshold, with the critical dynamics at this continuous active-to-absorbing state transition governed by the scaling exponents of directed percolation. Novel features in biologically motivated model variants include the stabilizing effect of a periodically varying carrying capacity that describes seasonally oscillating resource availability; enhanced mean species densities and local fluctuations caused by spatially varying reaction rates; and intriguing evolutionary dynamics emerging when variable interaction rates are affixed to individuals combined with trait inheritance to their offspring. The basic susceptible-infected-susceptible and susceptible-infected-recovered models for infectious disease spreading near their epidemic thresholds are respectively captured by the directed and dynamic isotropic percolation universality classes. Systems with three cyclically competing species akin to spatial rock-paper-scissors games may display striking spiral patterns, yet conservation laws can prevent such noise-induced structure formation. In diffusively coupled inhomogeneous settings, one may observe the stabilization of vulnerable ecologies prone to finite-size extinction or fixation due to immigration waves emanating from the interfaces.","sentences":["Stochastic, spatially extended models for predator-prey interaction display spatio-temporal structures that are not captured by the Lotka-Volterra mean-field rate equations.","These spreading activity fronts reflect persistent correlations between predators and prey that can be analyzed through field-theoretic methods.","Introducing local restrictions on the prey population induces a predator extinction threshold, with the critical dynamics at this continuous active-to-absorbing state transition governed by the scaling exponents of directed percolation.","Novel features in biologically motivated model variants include the stabilizing effect of a periodically varying carrying capacity that describes seasonally oscillating resource availability; enhanced mean species densities and local fluctuations caused by spatially varying reaction rates; and intriguing evolutionary dynamics emerging when variable interaction rates are affixed to individuals combined with trait inheritance to their offspring.","The basic susceptible-infected-susceptible and susceptible-infected-recovered models for infectious disease spreading near their epidemic thresholds are respectively captured by the directed and dynamic isotropic percolation universality classes.","Systems with three cyclically competing species akin to spatial rock-paper-scissors games may display striking spiral patterns, yet conservation laws can prevent such noise-induced structure formation.","In diffusively coupled inhomogeneous settings, one may observe the stabilization of vulnerable ecologies prone to finite-size extinction or fixation due to immigration waves emanating from the interfaces."],"url":"http://arxiv.org/abs/2405.05006v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 12:17:21","title":"Nilpotent structures of oriented neutral vector bundles","abstract":"In this paper, we study nilpotent structures of an oriented vector bundle $E$ of rank $4n$ with a neutral metric $h$ and an $h$-connection $\\nabla$. We define $H$-nilpotent structures of $(E, h, \\nabla )$ for a Lie subgroup $H$ of $SO(2n, 2n)$ related to neutral hyperK\\\"{a}hler structures. We observe that there exist a complex structure $I$ and paracomplex structures $J_1$, $J_2$ of $E$ such that $h$, $\\nabla$, $I$, $J_1$, $J_2$ form a neutral hyperK\\\"{a}hler structure of $E$ if and only if there exists an $H$-nilpotent structure of $(E, h, \\nabla )$.","sentences":["In this paper, we study nilpotent structures of an oriented vector bundle $E$ of rank $4n$ with a neutral metric $h$ and an $h$-connection $\\nabla$. We define $H$-nilpotent structures of $(E, h, \\nabla )$ for a Lie subgroup $H$ of $SO(2n, 2n)$ related to neutral hyperK\\\"{a}hler structures.","We observe that there exist a complex structure $I$ and paracomplex structures $J_1$, $J_2$ of $E$ such that $h$, $\\nabla$, $I$, $J_1$, $J_2$ form a neutral hyperK\\\"{a}hler structure of $E$ if and only if there exists an $H$-nilpotent structure of $(E, h, \\nabla )$."],"url":"http://arxiv.org/abs/2405.05003v2","category":"math.DG"}
{"created":"2024-05-08 12:14:34","title":"HMANet: Hybrid Multi-Axis Aggregation Network for Image Super-Resolution","abstract":"Transformer-based methods have demonstrated excellent performance on super-resolution visual tasks, surpassing conventional convolutional neural networks. However, existing work typically restricts self-attention computation to non-overlapping windows to save computational costs. This means that Transformer-based networks can only use input information from a limited spatial range. Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA) is proposed in this paper to exploit feature potential information better. HMA is constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid Attention Blocks(GAB). On the one side, RHTB combines channel attention and self-attention to enhance non-local feature fusion and produce more attractive visual results. Conversely, GAB is used in cross-domain information interaction to jointly model similar features and obtain a larger perceptual field. For the super-resolution task in the training phase, a novel pre-training method is designed to enhance the model representation capabilities further and validate the proposed model's effectiveness through many experiments. The experimental results show that HMA outperforms the state-of-the-art methods on the benchmark dataset. We provide code and models at https://github.com/korouuuuu/HMA.","sentences":["Transformer-based methods have demonstrated excellent performance on super-resolution visual tasks, surpassing conventional convolutional neural networks.","However, existing work typically restricts self-attention computation to non-overlapping windows to save computational costs.","This means that Transformer-based networks can only use input information from a limited spatial range.","Therefore, a novel Hybrid Multi-Axis Aggregation network (HMA) is proposed in this paper to exploit feature potential information better.","HMA is constructed by stacking Residual Hybrid Transformer Blocks(RHTB) and Grid Attention Blocks(GAB).","On the one side, RHTB combines channel attention and self-attention to enhance non-local feature fusion and produce more attractive visual results.","Conversely, GAB is used in cross-domain information interaction to jointly model similar features and obtain a larger perceptual field.","For the super-resolution task in the training phase, a novel pre-training method is designed to enhance the model representation capabilities further and validate the proposed model's effectiveness through many experiments.","The experimental results show that HMA outperforms the state-of-the-art methods on the benchmark dataset.","We provide code and models at https://github.com/korouuuuu/HMA."],"url":"http://arxiv.org/abs/2405.05001v1","category":"cs.CV"}
{"created":"2024-05-08 12:11:10","title":"Small ball probability for multiple singular values of symmetric random matrices","abstract":"Let $A_n$ be an $n\\times n$ random symmetric matrix with $(A_{ij})_{i< j}$ i.i.d. mean $0$, variance 1, following a subGaussian distribution and diagonal elements i.i.d. following a subGaussian distribution with a fixed variance. We investigate the joint small ball probability that $A_n$ has eigenvalues near two fixed locations $\\lambda_1$ and $\\lambda_2$, where $\\lambda_1$ and $\\lambda_2$ are sufficiently separated and in the bulk of the semicircle law. More precisely we prove that for a wide class of entry distributions of $A_{ij}$ that involve all Gaussian convolutions (where $\\sigma_{min}(\\cdot)$ denotes the least singular value of a square matrix), $$\\mathbb{P}(\\sigma_{min}(A_n-\\lambda_1 I_n)\\leq\\delta_1n^{-1/2},\\sigma_{min}(A_n-\\lambda_2 I_n)\\leq\\delta_2n^{-1/2})\\leq c\\delta_1\\delta_2+e^{-cn}.$$ The given estimate approximately factorizes as the product of the estimates for the two individual events, which is an indication of quantitative independence. The estimate readily generalizes to $d$ distinct locations. As an application, we upper bound the probability that there exist $d$ eigenvalues of $A_n$ asymptotically satisfying any fixed linear equation, which in particular gives a lower bound of the distance to this linear relation from any possible eigenvalue pair that holds with probability $1-o(1)$, and rules out the existence of two equal singular values in generic regions of the spectrum.","sentences":["Let $A_n$ be an $n\\times n$ random symmetric matrix with $(A_{ij})_{i< j}$ i.i.d. mean $0$, variance 1, following a subGaussian distribution and diagonal elements i.i.d.","following a subGaussian distribution with a fixed variance.","We investigate the joint small ball probability that $A_n$ has eigenvalues near two fixed locations $\\lambda_1$ and $\\lambda_2$, where $\\lambda_1$ and $\\lambda_2$ are sufficiently separated and in the bulk of the semicircle law.","More precisely we prove that for a wide class of entry distributions of $A_{ij}$ that involve all Gaussian convolutions (where $\\sigma_{min}(\\cdot)$ denotes the least singular value of a square matrix), $$\\mathbb{P}(\\sigma_{min}(A_n-\\lambda_1 I_n)\\leq\\delta_1n^{-1/2},\\sigma_{min}(A_n-\\lambda_2 I_n)\\leq\\delta_2n^{-1/2})\\leq c\\delta_1\\delta_2+e^{-cn}.$$","The given estimate approximately factorizes as the product of the estimates for the two individual events, which is an indication of quantitative independence.","The estimate readily generalizes to $d$ distinct locations.","As an application, we upper bound the probability that there exist $d$ eigenvalues of $A_n$ asymptotically satisfying any fixed linear equation, which in particular gives a lower bound of the distance to this linear relation from any possible eigenvalue pair that holds with probability $1-o(1)$, and rules out the existence of two equal singular values in generic regions of the spectrum."],"url":"http://arxiv.org/abs/2405.04999v1","category":"math.PR"}
{"created":"2024-05-08 12:04:43","title":"Bridging the Gap Between Saliency Prediction and Image Quality Assessment","abstract":"Over the past few years, deep neural models have made considerable advances in image quality assessment (IQA). However, the underlying reasons for their success remain unclear, owing to the complex nature of deep neural networks. IQA aims to describe how the human visual system (HVS) works and to create its efficient approximations. On the other hand, Saliency Prediction task aims to emulate HVS via determining areas of visual interest. Thus, we believe that saliency plays a crucial role in human perception. In this work, we conduct an empirical study that reveals the relation between IQA and Saliency Prediction tasks, demonstrating that the former incorporates knowledge of the latter. Moreover, we introduce a novel SACID dataset of saliency-aware compressed images and conduct a large-scale comparison of classic and neural-based IQA methods. All supplementary code and data will be available at the time of publication.","sentences":["Over the past few years, deep neural models have made considerable advances in image quality assessment (IQA).","However, the underlying reasons for their success remain unclear, owing to the complex nature of deep neural networks.","IQA aims to describe how the human visual system (HVS) works and to create its efficient approximations.","On the other hand, Saliency Prediction task aims to emulate HVS via determining areas of visual interest.","Thus, we believe that saliency plays a crucial role in human perception.","In this work, we conduct an empirical study that reveals the relation between IQA and Saliency Prediction tasks, demonstrating that the former incorporates knowledge of the latter.","Moreover, we introduce a novel SACID dataset of saliency-aware compressed images and conduct a large-scale comparison of classic and neural-based IQA methods.","All supplementary code and data will be available at the time of publication."],"url":"http://arxiv.org/abs/2405.04997v1","category":"cs.CV"}
{"created":"2024-05-08 11:39:16","title":"Predictive Mapping of Spectral Signatures from RGB Imagery for Off-Road Terrain Analysis","abstract":"Accurate identification of complex terrain characteristics, such as soil composition and coefficient of friction, is essential for model-based planning and control of mobile robots in off-road environments. Spectral signatures leverage distinct patterns of light absorption and reflection to identify various materials, enabling precise characterization of their inherent properties. Recent research in robotics has explored the adoption of spectroscopy to enhance perception and interaction with environments. However, the significant cost and elaborate setup required for mounting these sensors present formidable barriers to widespread adoption. In this study, we introduce RS-Net (RGB to Spectral Network), a deep neural network architecture designed to map RGB images to corresponding spectral signatures. We illustrate how RS-Net can be synergistically combined with Co-Learning techniques for terrain property estimation. Initial results demonstrate the effectiveness of this approach in characterizing spectral signatures across an extensive off-road real-world dataset. These findings highlight the feasibility of terrain property estimation using only RGB cameras.","sentences":["Accurate identification of complex terrain characteristics, such as soil composition and coefficient of friction, is essential for model-based planning and control of mobile robots in off-road environments.","Spectral signatures leverage distinct patterns of light absorption and reflection to identify various materials, enabling precise characterization of their inherent properties.","Recent research in robotics has explored the adoption of spectroscopy to enhance perception and interaction with environments.","However, the significant cost and elaborate setup required for mounting these sensors present formidable barriers to widespread adoption.","In this study, we introduce RS-Net (RGB to Spectral Network), a deep neural network architecture designed to map RGB images to corresponding spectral signatures.","We illustrate how RS-Net can be synergistically combined with Co-Learning techniques for terrain property estimation.","Initial results demonstrate the effectiveness of this approach in characterizing spectral signatures across an extensive off-road real-world dataset.","These findings highlight the feasibility of terrain property estimation using only RGB cameras."],"url":"http://arxiv.org/abs/2405.04979v1","category":"cs.RO"}
{"created":"2024-05-08 11:18:51","title":"Numerical analysis of small-strain elasto-plastic deformation using local Radial Basis Function approximation with Picard iteration","abstract":"This paper deals with a numerical analysis of plastic deformation under various conditions, utilizing Radial Basis Function (RBF) approximation. The focus is on the elasto-plastic von Mises problem under plane-strain assumption. Elastic deformation is modelled using the Navier-Cauchy equation. In regions where the von Mises stress surpasses the yield stress, corrections are applied locally through a return mapping algorithm. The non-linear deformation problem in the plastic domain is solved using the Picard iteration.   The solutions for the Navier-Cauchy equation are computed using the Radial Basis Function-Generated Finite Differences (RBF-FD) meshless method using only scattered nodes in a strong form. Verification of the method is performed through the analysis of an internally pressurized thick-walled cylinder subjected to varying loading conditions. These conditions induce states of elastic expansion, perfectly-plastic yielding, and plastic yielding with linear hardening. The results are benchmarked against analytical solutions and traditional Finite Element Method (FEM) solutions. The paper also showcases the robustness of this approach by solving case of thick-walled cylinder with cut-outs. The results affirm that the RBF-FD method produces results comparable to those obtained through FEM, while offering substantial benefits in managing complex geometries without the necessity for conventional meshing, along with other benefits of meshless methods.","sentences":["This paper deals with a numerical analysis of plastic deformation under various conditions, utilizing Radial Basis Function (RBF) approximation.","The focus is on the elasto-plastic von Mises problem under plane-strain assumption.","Elastic deformation is modelled using the Navier-Cauchy equation.","In regions where the von Mises stress surpasses the yield stress, corrections are applied locally through a return mapping algorithm.","The non-linear deformation problem in the plastic domain is solved using the Picard iteration.   ","The solutions for the Navier-Cauchy equation are computed using the Radial Basis Function-Generated Finite Differences (RBF-FD) meshless method using only scattered nodes in a strong form.","Verification of the method is performed through the analysis of an internally pressurized thick-walled cylinder subjected to varying loading conditions.","These conditions induce states of elastic expansion, perfectly-plastic yielding, and plastic yielding with linear hardening.","The results are benchmarked against analytical solutions and traditional Finite Element Method (FEM) solutions.","The paper also showcases the robustness of this approach by solving case of thick-walled cylinder with cut-outs.","The results affirm that the RBF-FD method produces results comparable to those obtained through FEM, while offering substantial benefits in managing complex geometries without the necessity for conventional meshing, along with other benefits of meshless methods."],"url":"http://arxiv.org/abs/2405.04970v1","category":"math.NA"}
{"created":"2024-05-08 11:14:14","title":"Predicting positon solutions of a family of Nonlinear Schr\u00f6dinger equations through Deep Learning algorithm","abstract":"We consider a hierarchy of nonlinear Schr\\\"{o}dinger equations (NLSEs) and forecast the evolution of positon solutions using a deep learning approach called Physics Informed Neural Networks (PINN). Notably, the PINN algorithm accurately predicts positon solutions not only in the standard NLSE but also in other higher order versions, including cubic, quartic and quintic NLSEs. The PINN approach also effectively handles two coupled NLSEs and two coupled Hirota equations. In addition to the above, we report exact second-order positon solutions of the sextic NLSE and coupled generalized NLSE. These solutions are not available in the existing literature and we construct them through generalized Darboux transformation method. Further, we utilize PINNs to forecast their behaviour as well. To validate PINN's accuracy, we compare the predicted solutions with exact solutions obtained from analytical methods. The results show high fidelity and low mean squared error in the predictions generated by our PINN model.","sentences":["We consider a hierarchy of nonlinear Schr\\\"{o}dinger equations (NLSEs) and forecast the evolution of positon solutions using a deep learning approach called Physics Informed Neural Networks (PINN).","Notably, the PINN algorithm accurately predicts positon solutions not only in the standard NLSE but also in other higher order versions, including cubic, quartic and quintic NLSEs.","The PINN approach also effectively handles two coupled NLSEs and two coupled Hirota equations.","In addition to the above, we report exact second-order positon solutions of the sextic NLSE and coupled generalized NLSE.","These solutions are not available in the existing literature and we construct them through generalized Darboux transformation method.","Further, we utilize PINNs to forecast their behaviour as well.","To validate PINN's accuracy, we compare the predicted solutions with exact solutions obtained from analytical methods.","The results show high fidelity and low mean squared error in the predictions generated by our PINN model."],"url":"http://arxiv.org/abs/2405.04968v1","category":"nlin.PS"}
{"created":"2024-05-08 11:09:24","title":"Frequency-Assisted Mamba for Remote Sensing Image Super-Resolution","abstract":"Recent progress in remote sensing image (RSI) super-resolution (SR) has exhibited remarkable performance using deep neural networks, e.g., Convolutional Neural Networks and Transformers. However, existing SR methods often suffer from either a limited receptive field or quadratic computational overhead, resulting in sub-optimal global representation and unacceptable computational costs in large-scale RSI. To alleviate these issues, we develop the first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR, which specializes in processing large-scale RSI by capturing long-range dependency with linear complexity. To achieve better SR reconstruction, building upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed FMSR, to explore the spatial and frequent correlations. In particular, our FMSR features a multi-level fusion architecture equipped with the Frequency Selection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate Module (HGM) to grasp their merits for effective spatial-frequency fusion. Recognizing that global and local dependencies are complementary and both beneficial for SR, we further recalibrate these multi-level features for accurate feature fusion via learnable scaling adaptors. Extensive experiments on AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms state-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on average, while consuming only 28.05% and 19.08% of its memory consumption and complexity, respectively.","sentences":["Recent progress in remote sensing image (RSI) super-resolution (SR) has exhibited remarkable performance using deep neural networks, e.g., Convolutional Neural Networks and Transformers.","However, existing SR methods often suffer from either a limited receptive field or quadratic computational overhead, resulting in sub-optimal global representation and unacceptable computational costs in large-scale RSI.","To alleviate these issues, we develop the first attempt to integrate the Vision State Space Model (Mamba) for RSI-SR, which specializes in processing large-scale RSI by capturing long-range dependency with linear complexity.","To achieve better SR reconstruction, building upon Mamba, we devise a Frequency-assisted Mamba framework, dubbed FMSR, to explore the spatial and frequent correlations.","In particular, our FMSR features a multi-level fusion architecture equipped with the Frequency Selection Module (FSM), Vision State Space Module (VSSM), and Hybrid Gate Module (HGM) to grasp their merits for effective spatial-frequency fusion.","Recognizing that global and local dependencies are complementary and both beneficial for SR, we further recalibrate these multi-level features for accurate feature fusion via learnable scaling adaptors.","Extensive experiments on AID, DOTA, and DIOR benchmarks demonstrate that our FMSR outperforms state-of-the-art Transformer-based methods HAT-L in terms of PSNR by 0.11 dB on average, while consuming only 28.05% and 19.08% of its memory consumption and complexity, respectively."],"url":"http://arxiv.org/abs/2405.04964v1","category":"cs.CV"}
{"created":"2024-05-08 10:55:47","title":"Computation of some dispersive equations through their iterated linearisation","abstract":"It is often the case that, while the numerical solution of the non-linear dispersive equation $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}(u(t),t)u(t)$ represents a formidable challenge, it is fairly easy and cheap to solve closely related linear equations of the form $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}_1(t)u(t)+\\widetilde{\\mathcal H}_2(t)u(t)$, where $\\mathcal{H}_1(t)+\\mathcal{H}_2(v,t)=\\mathcal{H}(v,t)$. In that case we advocate an iterative linearisation procedure that involves fixed-point iteration of the latter equation to solve the former. A typical case is when the original problem is a nonlinear Schr\\\"odinger or Gross--Pitaevskii equation, while the `easy' equation is linear Schr\\\"odinger with time-dependent potential.   We analyse in detail the iterative scheme and its practical implementation, prove that each iteration increases the order, derive upper bounds on the speed of convergence and discuss in the case of nonlinear Schr\\\"odinger equation with cubic potential the preservation of structural features of the underlying equation: the $\\mathrm{L}_2$ norm, momentum and Hamiltonian energy. A key ingredient in our approach is the use of the Magnus expansion in conjunction with Hermite quadratures, which allows effective solutions of the linearised but non-autonomous equations in an iterative fashion. The resulting Magnus--Hermite methods can be combined with a wide range of numerical approximations to the matrix exponential. The paper concludes with a number of numerical experiments, demonstrating the power of the proposed approach.","sentences":["It is often the case that, while the numerical solution of the non-linear dispersive equation $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}(u(t),t)u(t)$ represents a formidable challenge, it is fairly easy and cheap to solve closely related linear equations of the form $\\mathrm{i}\\partial_t u(t)=\\mathcal{H}_1(t)u(t)+\\widetilde{\\mathcal H}_2(t)u(t)$, where $\\mathcal{H}_1(t)+\\mathcal{H}_2(v,t)=\\mathcal{H}(v,t)$.","In that case we advocate an iterative linearisation procedure that involves fixed-point iteration of the latter equation to solve the former.","A typical case is when the original problem is a nonlinear Schr\\\"odinger or Gross--Pitaevskii equation, while the `easy' equation is linear Schr\\\"odinger with time-dependent potential.   ","We analyse in detail the iterative scheme and its practical implementation, prove that each iteration increases the order, derive upper bounds on the speed of convergence and discuss in the case of nonlinear Schr\\\"odinger equation with cubic potential the preservation of structural features of the underlying equation: the $\\mathrm{L}_2$ norm, momentum and Hamiltonian energy.","A key ingredient in our approach is the use of the Magnus expansion in conjunction with Hermite quadratures, which allows effective solutions of the linearised but non-autonomous equations in an iterative fashion.","The resulting Magnus--Hermite methods can be combined with a wide range of numerical approximations to the matrix exponential.","The paper concludes with a number of numerical experiments, demonstrating the power of the proposed approach."],"url":"http://arxiv.org/abs/2405.04958v1","category":"math.NA"}
{"created":"2024-05-08 10:51:08","title":"Modified version of open TASEP with dynamic defects","abstract":"We propose a modification to the study of site-wise dynamically disordered totally asymmetric simple exclusion process (TASEP). Motivated by the process of gene transcription, a study in ref. [39] introduced an extension of TASEP, where the defects (or obstacles) bind/un-bind dynamically to the sites of the lattice and the hopping of the particles on lattice faces a hindrance if the arrival site is occupied by an obstacle. In addition, the particle is only allowed to enter the lattice provided the first site is defect-free. In our study, we propose that the particle movement at the entry of the lattice must face an equal hindrance that is provided by the obstacles to the rest of the particles on the lattice. For open boundaries, the continuum mean-field equations are derived and solved numerically to obtain steady-state phase diagrams and density profiles. The presence of obstacles produces a shift in the phase boundaries obtained but the same three phases as obtained for the standard TASEP. Contrary to the model introduced in ref. \\cite{waclaw2019totally}, the idea to introduce the modification at the entrance shows that the limiting case $p_d \\rightarrow 1$ converges to the standard TASEP, where $p_d$ refers to the affected hopping rate due to presence of obstacle. The mean-field solutions are validated using extensive Monte Carlo simulations.","sentences":["We propose a modification to the study of site-wise dynamically disordered totally asymmetric simple exclusion process (TASEP).","Motivated by the process of gene transcription, a study in ref.","[39] introduced an extension of TASEP, where the defects (or obstacles) bind/un-bind dynamically to the sites of the lattice and the hopping of the particles on lattice faces a hindrance if the arrival site is occupied by an obstacle.","In addition, the particle is only allowed to enter the lattice provided the first site is defect-free.","In our study, we propose that the particle movement at the entry of the lattice must face an equal hindrance that is provided by the obstacles to the rest of the particles on the lattice.","For open boundaries, the continuum mean-field equations are derived and solved numerically to obtain steady-state phase diagrams and density profiles.","The presence of obstacles produces a shift in the phase boundaries obtained but the same three phases as obtained for the standard TASEP.","Contrary to the model introduced in ref.","\\cite{waclaw2019totally}, the idea to introduce the modification at the entrance shows that the limiting case $p_d \\rightarrow 1$ converges to the standard TASEP, where $p_d$ refers to the affected hopping rate due to presence of obstacle.","The mean-field solutions are validated using extensive Monte Carlo simulations."],"url":"http://arxiv.org/abs/2405.04956v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 10:39:15","title":"Kiselman Minimum Principle and Rooftop Envelopes in Complex Hessian Equations","abstract":"We initiate the study of $m$-subharmonic functions with respect to a semipositive $(1,1)$-form in Euclidean domains, providing a significant element in understanding geodesics within the context of complex Hessian equations. Based on the foundational Perron envelope construction, we prove a decomposition of $m$-subharmonic solutions, and a general comparison principle that effectively manages singular Hessian measures. Additionally, we establish a rooftop equality and an analogue of the Kiselman minimum principle, which are crucial ingredients in establishing a criterion for geodesic connectivity among $m$-subharmonic functions, expressed in terms of their asymptotic envelopes.","sentences":["We initiate the study of $m$-subharmonic functions with respect to a semipositive $(1,1)$-form in Euclidean domains, providing a significant element in understanding geodesics within the context of complex Hessian equations.","Based on the foundational Perron envelope construction, we prove a decomposition of $m$-subharmonic solutions, and a general comparison principle that effectively manages singular Hessian measures.","Additionally, we establish a rooftop equality and an analogue of the Kiselman minimum principle, which are crucial ingredients in establishing a criterion for geodesic connectivity among $m$-subharmonic functions, expressed in terms of their asymptotic envelopes."],"url":"http://arxiv.org/abs/2405.04948v1","category":"math.CV"}
{"created":"2024-05-08 10:33:19","title":"On quadrirational pentagon maps","abstract":"We classify rational solutions of a specific type of the set theoretical version of the pentagon equation. That is, we find all quadrirational maps $R:(x,y)\\mapsto (u(x,y),v(x,y)),$ where $u, v$ are two rational functions on two arguments, that serve as solutions of the pentagon equation. Furthermore, provided a pentagon map that admits a partial inverse, we obtain genuine entwining pentagon set theoretical solutions.","sentences":["We classify rational solutions of a specific type of the set theoretical version of the pentagon equation.","That is, we find all quadrirational maps $R:(x,y)\\mapsto (u(x,y),v(x,y)),$ where $u, v$ are two rational functions on two arguments, that serve as solutions of the pentagon equation.","Furthermore, provided a pentagon map that admits a partial inverse, we obtain genuine entwining pentagon set theoretical solutions."],"url":"http://arxiv.org/abs/2405.04945v1","category":"nlin.SI"}
{"created":"2024-05-08 10:27:05","title":"Unsupervised Skin Feature Tracking with Deep Neural Networks","abstract":"Facial feature tracking is essential in imaging ballistocardiography for accurate heart rate estimation and enables motor degradation quantification in Parkinson's disease through skin feature tracking. While deep convolutional neural networks have shown remarkable accuracy in tracking tasks, they typically require extensive labeled data for supervised training. Our proposed pipeline employs a convolutional stacked autoencoder to match image crops with a reference crop containing the target feature, learning deep feature encodings specific to the object category in an unsupervised manner, thus reducing data requirements. To overcome edge effects making the performance dependent on crop size, we introduced a Gaussian weight on the residual errors of the pixels when calculating the loss function. Training the autoencoder on facial images and validating its performance on manually labeled face and hand videos, our Deep Feature Encodings (DFE) method demonstrated superior tracking accuracy with a mean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods like SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and CoTracker. Overall, our unsupervised learning approach excels in tracking various skin features under significant motion conditions, providing superior feature descriptors for tracking, matching, and image registration compared to both traditional and state-of-the-art supervised learning methods.","sentences":["Facial feature tracking is essential in imaging ballistocardiography for accurate heart rate estimation and enables motor degradation quantification in Parkinson's disease through skin feature tracking.","While deep convolutional neural networks have shown remarkable accuracy in tracking tasks, they typically require extensive labeled data for supervised training.","Our proposed pipeline employs a convolutional stacked autoencoder to match image crops with a reference crop containing the target feature, learning deep feature encodings specific to the object category in an unsupervised manner, thus reducing data requirements.","To overcome edge effects making the performance dependent on crop size, we introduced a Gaussian weight on the residual errors of the pixels when calculating the loss function.","Training the autoencoder on facial images and validating its performance on manually labeled face and hand videos, our Deep Feature Encodings (DFE) method demonstrated superior tracking accuracy with a mean error ranging from 0.6 to 3.3 pixels, outperforming traditional methods like SIFT, SURF, Lucas Kanade, and the latest transformers like PIPs++ and CoTracker.","Overall, our unsupervised learning approach excels in tracking various skin features under significant motion conditions, providing superior feature descriptors for tracking, matching, and image registration compared to both traditional and state-of-the-art supervised learning methods."],"url":"http://arxiv.org/abs/2405.04943v1","category":"cs.CV"}
{"created":"2024-05-08 10:23:08","title":"Dual-domain Collaborative Denoising for Social Recommendation","abstract":"Social recommendation leverages social network to complement user-item interaction data for recommendation task, aiming to mitigate the data sparsity issue in recommender systems. However, existing social recommendation methods encounter the following challenge: both social network and interaction data contain substaintial noise, and the propagation of such noise through Graph Neural Networks (GNNs) not only fails to enhance recommendation performance but may also interfere with the model's normal training. Despite the importance of denoising for social network and interaction data, only a limited number of studies have considered the denoising for social network and all of them overlook that for interaction data, hindering the denoising effect and recommendation performance. Based on this, we propose a novel model called Dual-domain Collaborative Denoising for Social Recommendation ($\\textbf{DCDSR}$). DCDSR comprises two primary modules: the structure-level collaborative denoising module and the embedding-space collaborative denoising module. In the structure-level collaborative denoising module, information from interaction domain is first employed to guide social network denoising. Subsequently, the denoised social network is used to supervise the denoising for interaction data. The embedding-space collaborative denoising module devotes to resisting the noise cross-domain diffusion problem through contrastive learning with dual-domain embedding collaborative perturbation. Additionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is introduced to better harness the denoising capability of contrastive learning. Evaluating our model on three real-world datasets verifies that DCDSR has a considerable denoising effect, thus outperforms the state-of-the-art social recommendation methods.","sentences":["Social recommendation leverages social network to complement user-item interaction data for recommendation task, aiming to mitigate the data sparsity issue in recommender systems.","However, existing social recommendation methods encounter the following challenge: both social network and interaction data contain substaintial noise, and the propagation of such noise through Graph Neural Networks (GNNs) not only fails to enhance recommendation performance but may also interfere with the model's normal training.","Despite the importance of denoising for social network and interaction data, only a limited number of studies have considered the denoising for social network and all of them overlook that for interaction data, hindering the denoising effect and recommendation performance.","Based on this, we propose a novel model called Dual-domain Collaborative Denoising for Social Recommendation ($\\textbf{DCDSR}$).","DCDSR comprises two primary modules: the structure-level collaborative denoising module and the embedding-space collaborative denoising module.","In the structure-level collaborative denoising module, information from interaction domain is first employed to guide social network denoising.","Subsequently, the denoised social network is used to supervise the denoising for interaction data.","The embedding-space collaborative denoising module devotes to resisting the noise cross-domain diffusion problem through contrastive learning with dual-domain embedding collaborative perturbation.","Additionally, a novel contrastive learning strategy, named Anchor-InfoNCE, is introduced to better harness the denoising capability of contrastive learning.","Evaluating our model on three real-world datasets verifies that DCDSR has a considerable denoising effect, thus outperforms the state-of-the-art social recommendation methods."],"url":"http://arxiv.org/abs/2405.04942v1","category":"cs.IR"}
{"created":"2024-05-08 10:00:33","title":"The Importance of Being Symmetric: Flat Rotation Curves from Exact Axisymmetric Static Vacuum Spacetimes","abstract":"Starting from the vacuum Einstein Field Equations and a static axisymmetric ansatz, we find two new solutions describing an axisymmetric static vacuum spacetime with cylindrical symmetry: One of this exhibits an additional symmetry in $z$-direction and the other has $z$-coordinate dependent coefficients. In analogy to the Schwarzschild solution, these metrics describe a static vacuum spacetime and apply in similar settings except for the changed symmetry conditions. Analyzing the low-velocity limit corresponding to the Newtonian approximation of the Schwarzschild metric, we find an effective logarithmic potential. This yields flat rotation curves for test particles undergoing rotational motion within the spacetime described by the line elements, in contrast to Newtonian rotation curves. This analysis highlights how important the symmetry assumptions are for deriving general relativistic solutions.   One example of physical objects that are generally described in the static vacuum low-velocity limit (reducing to Newtonian gravity in the spherically symmetric case) and exhibit axial symmetry are disk galaxies. We show that symmetries and appropriate line elements that respect them are crucial to consider in such settings. In particular, the solutions presented here result in flat rotation curves without any need for dark matter. While these exact solutions are limited to static vacuum spacetimes, their application to physical galaxies relies on appropriate approximations. Nonetheless, they offer valuable insights into explanations for flat rotation curves in galaxies and their implications for dark matter.","sentences":["Starting from the vacuum Einstein Field Equations and a static axisymmetric ansatz, we find two new solutions describing an axisymmetric static vacuum spacetime with cylindrical symmetry: One of this exhibits an additional symmetry in $z$-direction and the other has $z$-coordinate dependent coefficients.","In analogy to the Schwarzschild solution, these metrics describe a static vacuum spacetime and apply in similar settings except for the changed symmetry conditions.","Analyzing the low-velocity limit corresponding to the Newtonian approximation of the Schwarzschild metric, we find an effective logarithmic potential.","This yields flat rotation curves for test particles undergoing rotational motion within the spacetime described by the line elements, in contrast to Newtonian rotation curves.","This analysis highlights how important the symmetry assumptions are for deriving general relativistic solutions.   ","One example of physical objects that are generally described in the static vacuum low-velocity limit (reducing to Newtonian gravity in the spherically symmetric case) and exhibit axial symmetry are disk galaxies.","We show that symmetries and appropriate line elements that respect them are crucial to consider in such settings.","In particular, the solutions presented here result in flat rotation curves without any need for dark matter.","While these exact solutions are limited to static vacuum spacetimes, their application to physical galaxies relies on appropriate approximations.","Nonetheless, they offer valuable insights into explanations for flat rotation curves in galaxies and their implications for dark matter."],"url":"http://arxiv.org/abs/2405.04933v1","category":"gr-qc"}
{"created":"2024-05-08 09:55:40","title":"Minimal time of the pointwise controllability for degenerate singular operators and related numerical results via B-splines","abstract":"The goal of this paper is to analyze the pointwise controllability properties of a one-dimensional degenerate/singular equation. We prove the conditions that characterize approximate and null controllability. Besides, a numerical simulation based on B-splines will be provided, in which the state $u$ and the control function $h$ are represented in terms of B-spline basis functions. The numerical results obtained match the theoretical ones.","sentences":["The goal of this paper is to analyze the pointwise controllability properties of a one-dimensional degenerate/singular equation.","We prove the conditions that characterize approximate and null controllability.","Besides, a numerical simulation based on B-splines will be provided, in which the state $u$ and the control function $h$ are represented in terms of B-spline basis functions.","The numerical results obtained match the theoretical ones."],"url":"http://arxiv.org/abs/2405.04930v1","category":"math.OC"}
{"created":"2024-05-08 09:53:53","title":"$C^\\infty$ well-posedness of higher order hyperbolic pseudo-differential equations with multiplicities","abstract":"In this paper, we study higher order hyperbolic pseudo-differential equations with variable multiplicities. We work in arbitrary space dimension and we assume that the principal part is time-dependent only. We identify sufficient conditions on the roots and the lower order terms (Levi conditions) under which the corresponding Cauchy problem is $C^\\infty$ well-posed. This is achieved via transformation into a first order system, reduction into upper-triangular form and application of suitable Fourier integral operator methods previously developed for hyperbolic non-diagonalisable systems. We also discuss how our result compares with the literature on second and third order hyperbolic equations.","sentences":["In this paper, we study higher order hyperbolic pseudo-differential equations with variable multiplicities.","We work in arbitrary space dimension and we assume that the principal part is time-dependent only.","We identify sufficient conditions on the roots and the lower order terms (Levi conditions) under which the corresponding Cauchy problem is $C^\\infty$ well-posed.","This is achieved via transformation into a first order system, reduction into upper-triangular form and application of suitable Fourier integral operator methods previously developed for hyperbolic non-diagonalisable systems.","We also discuss how our result compares with the literature on second and third order hyperbolic equations."],"url":"http://arxiv.org/abs/2405.04927v1","category":"math.AP"}
{"created":"2024-05-08 09:42:44","title":"The simplest model of a scalarized black hole in the Einstein-Klein-Gordon theory","abstract":"We investigate scalarized black holes in the Einstein-minimally coupled scalar theory with a negative potential $V(\\phi)=-\\alpha^2\\phi^6$. The tachyonic instability is absent from analyzing the linearized scalar equation, which could not allow for spontaneous scalarization. However, we obtain the black hole solutions with scalar hair by solving three full equations because this scalar potential violates the weak energy condition. This shows clearly that scalarized black holes can be obtained without introducing a non-minimal scalar coupling term. We perform the stability analysis for scalarized black holes by adopting radial perturbations, implying that all scalarized black holes belonging to a single branch are unstable.","sentences":["We investigate scalarized black holes in the Einstein-minimally coupled scalar theory with a negative potential $V(\\phi)=-\\alpha^2\\phi^6$.","The tachyonic instability is absent from analyzing the linearized scalar equation, which could not allow for spontaneous scalarization.","However, we obtain the black hole solutions with scalar hair by solving three full equations because this scalar potential violates the weak energy condition.","This shows clearly that scalarized black holes can be obtained without introducing a non-minimal scalar coupling term.","We perform the stability analysis for scalarized black holes by adopting radial perturbations, implying that all scalarized black holes belonging to a single branch are unstable."],"url":"http://arxiv.org/abs/2405.04921v1","category":"gr-qc"}
{"created":"2024-05-08 09:24:10","title":"A progressive data-augmented RANS model for enhanced wind-farm simulations","abstract":"The development of advanced simulation tools is essential, both presently and in the future, for improving wind-energy design strategies, paving the way for a complete transition to sustainable solutions. The Reynolds-averaged Navier-Stokes (RANS) models are pivotal in enhancing our comprehension of the complex flow within and around wind farms and, hence, improving their capacity to accurately model turbulence within this context is a vital research goal. The enhancement is essential for a precise prediction of wake recovery and for capturing intricate flow phenomena such as secondary flows of Prandtl's second kind behind the turbines. To reach these objectives, here, we propose a progressive data-augmentation approach. We first incorporate the turbine-induced forces in the turbulent kinetic energy equation of the widely used $k-\\omega\\text{SST}$ model. Afterward, we utilize data from large-eddy simulations to progressively enhance the Reynolds-stress prediction of this baseline model, accurately capturing the evolution of eddy viscosity in the wake, as well as the emergence of secondary flows. We then apply the optimized model to two unseen cases with distinct layouts and conduct a comparative analysis focusing on the obtained quantities such as normalized streamwise velocity deficit, turbulence intensity, and power output. We also examine the success rate of the augmented model in predicting the secondary flows in the wake region. Our comparisons and validations demonstrate the superior performance of the progressive data-augmented model over the standard version in all cases considered in this study.","sentences":["The development of advanced simulation tools is essential, both presently and in the future, for improving wind-energy design strategies, paving the way for a complete transition to sustainable solutions.","The Reynolds-averaged Navier-Stokes (RANS) models are pivotal in enhancing our comprehension of the complex flow within and around wind farms and, hence, improving their capacity to accurately model turbulence within this context is a vital research goal.","The enhancement is essential for a precise prediction of wake recovery and for capturing intricate flow phenomena such as secondary flows of Prandtl's second kind behind the turbines.","To reach these objectives, here, we propose a progressive data-augmentation approach.","We first incorporate the turbine-induced forces in the turbulent kinetic energy equation of the widely used $k-\\omega\\text{SST}$ model.","Afterward, we utilize data from large-eddy simulations to progressively enhance the Reynolds-stress prediction of this baseline model, accurately capturing the evolution of eddy viscosity in the wake, as well as the emergence of secondary flows.","We then apply the optimized model to two unseen cases with distinct layouts and conduct a comparative analysis focusing on the obtained quantities such as normalized streamwise velocity deficit, turbulence intensity, and power output.","We also examine the success rate of the augmented model in predicting the secondary flows in the wake region.","Our comparisons and validations demonstrate the superior performance of the progressive data-augmented model over the standard version in all cases considered in this study."],"url":"http://arxiv.org/abs/2405.04906v1","category":"physics.flu-dyn"}
{"created":"2024-05-08 09:16:54","title":"Imbalanced Graph Classification with Multi-scale Oversampling Graph Neural Networks","abstract":"One main challenge in imbalanced graph classification is to learn expressive representations of the graphs in under-represented (minority) classes. Existing generic imbalanced learning methods, such as oversampling and imbalanced learning loss functions, can be adopted for enabling graph representation learning models to cope with this challenge. However, these methods often directly operate on the graph representations, ignoring rich discriminative information within the graphs and their interactions. To tackle this issue, we introduce a novel multi-scale oversampling graph neural network (MOSGNN) that learns expressive minority graph representations based on intra- and inter-graph semantics resulting from oversampled graphs at multiple scales - subgraph, graph, and pairwise graphs. It achieves this by jointly optimizing subgraph-level, graph-level, and pairwise-graph learning tasks to learn the discriminative information embedded within and between the minority graphs. Extensive experiments on 16 imbalanced graph datasets show that MOSGNN i) significantly outperforms five state-of-the-art models, and ii) offers a generic framework, in which different advanced imbalanced learning loss functions can be easily plugged in and obtain significantly improved classification performance.","sentences":["One main challenge in imbalanced graph classification is to learn expressive representations of the graphs in under-represented (minority) classes.","Existing generic imbalanced learning methods, such as oversampling and imbalanced learning loss functions, can be adopted for enabling graph representation learning models to cope with this challenge.","However, these methods often directly operate on the graph representations, ignoring rich discriminative information within the graphs and their interactions.","To tackle this issue, we introduce a novel multi-scale oversampling graph neural network (MOSGNN) that learns expressive minority graph representations based on intra- and inter-graph semantics resulting from oversampled graphs at multiple scales - subgraph, graph, and pairwise graphs.","It achieves this by jointly optimizing subgraph-level, graph-level, and pairwise-graph learning tasks to learn the discriminative information embedded within and between the minority graphs.","Extensive experiments on 16 imbalanced graph datasets show that MOSGNN i) significantly outperforms five state-of-the-art models, and ii) offers a generic framework, in which different advanced imbalanced learning loss functions can be easily plugged in and obtain significantly improved classification performance."],"url":"http://arxiv.org/abs/2405.04903v1","category":"cs.LG"}
{"created":"2024-05-08 08:36:45","title":"Investigating the accelerated expansion of the Universe through updated constraints on viable $f(R)$ models within the Palatini formalism","abstract":"The observed accelerated expansion of the Universe at present epoch can be explained by some of the $f(R)$ models without invoking the existence of dark energy or any other such exotic component in cosmic fluid. The $f(R)$ models in Palatini formalism is relatively less explored in recent times with respect to their counterpart in metric formalism. We study seven $f(R)$ models in Palatini formalism: Hu-Sawicki (two cases), Starobinsky, exponential, Tsujikawa, $f(R) = R -\\beta /R^ n$, and $f(R)= R + \\alpha \\ln(R) - \\beta$. Following standard statistical procedure and utilizing data sets: type Ia supernovae data, cosmic chronometer observations, baryonic acoustic oscillations data, data from H \\textsc{ii} starburst galaxies, local measurements of the \\emph{Hubble} parameter ($H_{0}$), and distance priors of cosmic microwave background radiation data, we obtain constraints on the model parameters. When compared with the standard `lambda-cold dark matter model', for many data set combinations, the support for $f(R)$ models is significant. We obtain the relevant quantities for characterizing the accelerated expansion of the Universe, and these quantities are consistent with those obtained in a model-independent way by others. The curve of effective/total equation-of-state parameter, obtained from parameter constraints, clearly shows correct phases of the expansion history: the radiation-dominated epochs and the matter-dominated epochs, of the past, and the current accelerated expansion epoch eventually evolving to de-Sitter phase in the distant future. Overall, our results advocate in favour of pursuing $f(R)$ models in Palatini formalism as a potential alternative for explaining accelerated expansion of the Universe.","sentences":["The observed accelerated expansion of the Universe at present epoch can be explained by some of the $f(R)$ models without invoking the existence of dark energy or any other such exotic component in cosmic fluid.","The $f(R)$ models in Palatini formalism is relatively less explored in recent times with respect to their counterpart in metric formalism.","We study seven $f(R)$ models in Palatini formalism: Hu-Sawicki (two cases), Starobinsky, exponential, Tsujikawa, $f(R) = R -\\beta","/R^ n$, and $f(R)= R + \\alpha \\ln(R) - \\beta$. Following standard statistical procedure and utilizing data sets: type Ia supernovae data, cosmic chronometer observations, baryonic acoustic oscillations data, data from H \\textsc{ii} starburst galaxies, local measurements of the \\emph{Hubble} parameter ($H_{0}$), and distance priors of cosmic microwave background radiation data, we obtain constraints on the model parameters.","When compared with the standard `lambda-cold dark matter model', for many data set combinations, the support for $f(R)$ models is significant.","We obtain the relevant quantities for characterizing the accelerated expansion of the Universe, and these quantities are consistent with those obtained in a model-independent way by others.","The curve of effective/total equation-of-state parameter, obtained from parameter constraints, clearly shows correct phases of the expansion history: the radiation-dominated epochs and the matter-dominated epochs, of the past, and the current accelerated expansion epoch eventually evolving to de-Sitter phase in the distant future.","Overall, our results advocate in favour of pursuing $f(R)$ models in Palatini formalism as a potential alternative for explaining accelerated expansion of the Universe."],"url":"http://arxiv.org/abs/2405.04886v1","category":"astro-ph.CO"}
{"created":"2024-05-08 08:32:04","title":"Starshaped compact hypersurfaces in warped product manifolds II: a class of Hessian type equations","abstract":"We prove several results concerning one particular class of Hessian type equations, which has attracted much attention in recent years.","sentences":["We prove several results concerning one particular class of Hessian type equations, which has attracted much attention in recent years."],"url":"http://arxiv.org/abs/2405.04882v1","category":"math.DG"}
{"created":"2024-05-08 17:59:53","title":"Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving","abstract":"Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.","sentences":["Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods.","Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets.","We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning.","Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models.","The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution.","Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets.","Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines.","This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems."],"url":"http://arxiv.org/abs/2405.05258v1","category":"cs.CV"}
{"created":"2024-05-08 17:59:03","title":"Diffusion-HMC: Parameter Inference with Diffusion Model driven Hamiltonian Monte Carlo","abstract":"Diffusion generative models have excelled at diverse image generation and reconstruction tasks across fields. A less explored avenue is their application to discriminative tasks involving regression or classification problems. The cornerstone of modern cosmology is the ability to generate predictions for observed astrophysical fields from theory and constrain physical models from observations using these predictions. This work uses a single diffusion generative model to address these interlinked objectives -- as a surrogate model or emulator for cold dark matter density fields conditional on input cosmological parameters, and as a parameter inference model that solves the inverse problem of constraining the cosmological parameters of an input field. The model is able to emulate fields with summary statistics consistent with those of the simulated target distribution. We then leverage the approximate likelihood of the diffusion generative model to derive tight constraints on cosmology by using the Hamiltonian Monte Carlo method to sample the posterior on cosmological parameters for a given test image. Finally, we demonstrate that this parameter inference approach is more robust to the addition of noise than baseline parameter inference networks.","sentences":["Diffusion generative models have excelled at diverse image generation and reconstruction tasks across fields.","A less explored avenue is their application to discriminative tasks involving regression or classification problems.","The cornerstone of modern cosmology is the ability to generate predictions for observed astrophysical fields from theory and constrain physical models from observations using these predictions.","This work uses a single diffusion generative model to address these interlinked objectives -- as a surrogate model or emulator for cold dark matter density fields conditional on input cosmological parameters, and as a parameter inference model that solves the inverse problem of constraining the cosmological parameters of an input field.","The model is able to emulate fields with summary statistics consistent with those of the simulated target distribution.","We then leverage the approximate likelihood of the diffusion generative model to derive tight constraints on cosmology by using the Hamiltonian Monte Carlo method to sample the posterior on cosmological parameters for a given test image.","Finally, we demonstrate that this parameter inference approach is more robust to the addition of noise than baseline parameter inference networks."],"url":"http://arxiv.org/abs/2405.05255v1","category":"astro-ph.CO"}
{"created":"2024-05-08 17:36:29","title":"An LSTM-Based Chord Generation System Using Chroma Histogram Representations","abstract":"This paper proposes a system for chord generation to monophonic symbolic melodies using an LSTM-based model trained on chroma histogram representations of chords. Chroma representations promise more harmonically rich generation than chord label-based approaches, whilst maintaining a small number of dimensions in the dataset. This system is shown to be suitable for limited real-time use. While it does not meet the state-of-the-art for coherent long-term generation, it does show diatonic generation with cadential chord relationships. The need for further study into chroma histograms as an extracted feature in chord generation tasks is highlighted.","sentences":["This paper proposes a system for chord generation to monophonic symbolic melodies using an LSTM-based model trained on chroma histogram representations of chords.","Chroma representations promise more harmonically rich generation than chord label-based approaches, whilst maintaining a small number of dimensions in the dataset.","This system is shown to be suitable for limited real-time use.","While it does not meet the state-of-the-art for coherent long-term generation, it does show diatonic generation with cadential chord relationships.","The need for further study into chroma histograms as an extracted feature in chord generation tasks is highlighted."],"url":"http://arxiv.org/abs/2405.05240v1","category":"cs.SD"}
{"created":"2024-05-08 16:57:44","title":"Gamification in Software Engineering Education: a Tertiary Study","abstract":"As the significance of Software Engineering (SE) professionals continues to grow in the industry, the adoption of gamification techniques for training purposes has gained traction due to its potential to enhance class appeal through game-derived elements. This paper presents a tertiary study investigating the application of gamification in Software Engineering (SE) education. The study was conducted in response to recent systematic literature reviews and mappings on the topic. The findings reveal that the areas of SE most frequently gamified are Software Testing and Software Quality, with competition and cooperation being the most commonly utilized gamification elements. Additionally, the majority of studies focus on structural gamification, where game elements are employed to modify the learning environment without altering the content. The results demonstrate the potential of gamification to improve students' engagement and motivation throughout the SE learning process, while also impacting other aspects such as performance improvement, skill development, and fostering good SE practices. However, caution is advised as unplanned and incorrectly applied gamification measures may lead to significant declines in performance and motivation. (English Version of the paper in Portuguese available here: HTTP://doi.org/10.1145/3613372.3614193","sentences":["As the significance of Software Engineering (SE) professionals continues to grow in the industry, the adoption of gamification techniques for training purposes has gained traction due to its potential to enhance class appeal through game-derived elements.","This paper presents a tertiary study investigating the application of gamification in Software Engineering (SE) education.","The study was conducted in response to recent systematic literature reviews and mappings on the topic.","The findings reveal that the areas of SE most frequently gamified are Software Testing and Software Quality, with competition and cooperation being the most commonly utilized gamification elements.","Additionally, the majority of studies focus on structural gamification, where game elements are employed to modify the learning environment without altering the content.","The results demonstrate the potential of gamification to improve students' engagement and motivation throughout the SE learning process, while also impacting other aspects such as performance improvement, skill development, and fostering good SE practices.","However, caution is advised as unplanned and incorrectly applied gamification measures may lead to significant declines in performance and motivation.","(English Version of the paper in Portuguese available here: HTTP://doi.org/10.1145/3613372.3614193"],"url":"http://arxiv.org/abs/2405.05209v1","category":"cs.SE"}
{"created":"2024-05-08 16:43:50","title":"Anomaly Detection in Certificate Transparency Logs","abstract":"We propose an anomaly detection technique for X.509 certificates utilizing Isolation Forest. This method can be beneficial when compliance testing with X.509 linters proves unsatisfactory, and we seek to identify anomalies beyond standards compliance. The technique is validated on a sample of certificates from Certificate Transparency logs.","sentences":["We propose an anomaly detection technique for X.509 certificates utilizing Isolation Forest.","This method can be beneficial when compliance testing with X.509 linters proves unsatisfactory, and we seek to identify anomalies beyond standards compliance.","The technique is validated on a sample of certificates from Certificate Transparency logs."],"url":"http://arxiv.org/abs/2405.05206v1","category":"cs.CR"}
{"created":"2024-05-08 16:40:18","title":"CARE-SD: Classifier-based analysis for recognizing and eliminating stigmatizing and doubt marker labels in electronic health records: model development and validation","abstract":"Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques. Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs. The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation. These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset. For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers. Results: Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions. Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of .84 and .79, positive-label recall and precision values ranging from .71 to .86, and accuracies aligning closely with human annotator agreement (.87). Discussion: This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text, and identified trends in stigmatizing language use in an EHR setting. Additional labeled data may help improve lower scare quote model performance. Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems.","sentences":["Objective: To detect and classify features of stigmatizing and biased language in intensive care electronic health records (EHRs) using natural language processing techniques.","Materials and Methods: We first created a lexicon and regular expression lists from literature-driven stem words for linguistic features of stigmatizing patient labels, doubt markers, and scare quotes within EHRs.","The lexicon was further extended using Word2Vec and GPT 3.5, and refined through human evaluation.","These lexicons were used to search for matches across 18 million sentences from the de-identified Medical Information Mart for Intensive Care-III (MIMIC-III) dataset.","For each linguistic bias feature, 1000 sentence matches were sampled, labeled by expert clinical and public health annotators, and used to supervised learning classifiers.","Results:","Lexicon development from expanded literature stem-word lists resulted in a doubt marker lexicon containing 58 expressions, and a stigmatizing labels lexicon containing 127 expressions.","Classifiers for doubt markers and stigmatizing labels had the highest performance, with macro F1-scores of .84 and .79, positive-label recall and precision values ranging from .71 to .86, and accuracies aligning closely with human annotator agreement (.87).","Discussion:","This study demonstrated the feasibility of supervised classifiers in automatically identifying stigmatizing labels and doubt markers in medical text, and identified trends in stigmatizing language use in an EHR setting.","Additional labeled data may help improve lower scare quote model performance.","Conclusions: Classifiers developed in this study showed high model performance and can be applied to identify patterns and target interventions to reduce stigmatizing labels and doubt markers in healthcare systems."],"url":"http://arxiv.org/abs/2405.05204v1","category":"cs.CL"}
{"created":"2024-05-08 16:39:59","title":"Guided Combinatorial Algorithms for Submodular Maximization","abstract":"For constrained, not necessarily monotone submodular maximization, guiding the measured continuous greedy algorithm with a local search algorithm currently obtains the state-of-the-art approximation factor of 0.401 \\citep{buchbinder2023constrained}. These algorithms rely upon the multilinear extension and the Lovasz extension of a submodular set function. However, the state-of-the-art approximation factor of combinatorial algorithms has remained $1/e \\approx 0.367$ \\citep{buchbinder2014submodular}. In this work, we develop combinatorial analogues of the guided measured continuous greedy algorithm and obtain approximation ratio of $0.385$ in $\\oh{ kn }$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. Further, we derandomize these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$.","sentences":["For constrained, not necessarily monotone submodular maximization, guiding the measured continuous greedy algorithm with a local search algorithm currently obtains the state-of-the-art approximation factor of 0.401 \\citep{buchbinder2023constrained}.","These algorithms rely upon the multilinear extension and the Lovasz extension of a submodular set function.","However, the state-of-the-art approximation factor of combinatorial algorithms has remained $1/e \\approx 0.367$ \\citep{buchbinder2014submodular}.","In this work, we develop combinatorial analogues of the guided measured continuous greedy algorithm and obtain approximation ratio of $0.385$ in $\\oh{ kn }$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint.","Further, we derandomize these algorithms, maintaining the same ratio and asymptotic time complexity.","Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$."],"url":"http://arxiv.org/abs/2405.05202v1","category":"cs.DS"}
{"created":"2024-05-08 16:37:58","title":"Graded Relevance Scoring of Written Essays with Dense Retrieval","abstract":"Automated Essay Scoring automates the grading process of essays, providing a great advantage for improving the writing proficiency of students. While holistic essay scoring research is prevalent, a noticeable gap exists in scoring essays for specific quality traits. In this work, we focus on the relevance trait, which measures the ability of the student to stay on-topic throughout the entire essay. We propose a novel approach for graded relevance scoring of written essays that employs dense retrieval encoders. Dense representations of essays at different relevance levels then form clusters in the embeddings space, such that their centroids are potentially separate enough to effectively represent their relevance levels. We hence use the simple 1-Nearest-Neighbor classification over those centroids to determine the relevance level of an unseen essay. As an effective unsupervised dense encoder, we leverage Contriever, which is pre-trained with contrastive learning and demonstrated comparable performance to supervised dense retrieval models. We tested our approach on both task-specific (i.e., training and testing on same task) and cross-task (i.e., testing on unseen task) scenarios using the widely used ASAP++ dataset. Our method establishes a new state-of-the-art performance in the task-specific scenario, while its extension for the cross-task scenario exhibited a performance that is on par with the state-of-the-art model for that scenario. We also analyzed the performance of our approach in a more practical few-shot scenario, showing that it can significantly reduce the labeling cost while sacrificing only 10% of its effectiveness.","sentences":["Automated Essay Scoring automates the grading process of essays, providing a great advantage for improving the writing proficiency of students.","While holistic essay scoring research is prevalent, a noticeable gap exists in scoring essays for specific quality traits.","In this work, we focus on the relevance trait, which measures the ability of the student to stay on-topic throughout the entire essay.","We propose a novel approach for graded relevance scoring of written essays that employs dense retrieval encoders.","Dense representations of essays at different relevance levels then form clusters in the embeddings space, such that their centroids are potentially separate enough to effectively represent their relevance levels.","We hence use the simple 1-Nearest-Neighbor classification over those centroids to determine the relevance level of an unseen essay.","As an effective unsupervised dense encoder, we leverage Contriever, which is pre-trained with contrastive learning and demonstrated comparable performance to supervised dense retrieval models.","We tested our approach on both task-specific (i.e., training and testing on same task) and cross-task (i.e., testing on unseen task) scenarios using the widely used ASAP++ dataset.","Our method establishes a new state-of-the-art performance in the task-specific scenario, while its extension for the cross-task scenario exhibited a performance that is on par with the state-of-the-art model for that scenario.","We also analyzed the performance of our approach in a more practical few-shot scenario, showing that it can significantly reduce the labeling cost while sacrificing only 10% of its effectiveness."],"url":"http://arxiv.org/abs/2405.05200v1","category":"cs.IR"}
{"created":"2024-05-08 16:35:06","title":"SINBAD: Saliency-informed detection of breakage caused by ad blocking","abstract":"Privacy-enhancing blocking tools based on filter-list rules tend to break legitimate functionality. Filter-list maintainers could benefit from automated breakage detection tools that allow them to proactively fix problematic rules before deploying them to millions of users. We introduce SINBAD, an automated breakage detector that improves the accuracy over the state of the art by 20%, and is the first to detect dynamic breakage and breakage caused by style-oriented filter rules. The success of SINBAD is rooted in three innovations: (1) the use of user-reported breakage issues in forums that enable the creation of a high-quality dataset for training in which only breakage that users perceive as an issue is included; (2) the use of 'web saliency' to automatically identify user-relevant regions of a website on which to prioritize automated interactions aimed at triggering breakage; and (3) the analysis of webpages via subtrees which enables fine-grained identification of problematic filter rules.","sentences":["Privacy-enhancing blocking tools based on filter-list rules tend to break legitimate functionality.","Filter-list maintainers could benefit from automated breakage detection tools that allow them to proactively fix problematic rules before deploying them to millions of users.","We introduce SINBAD, an automated breakage detector that improves the accuracy over the state of the art by 20%, and is the first to detect dynamic breakage and breakage caused by style-oriented filter rules.","The success of SINBAD is rooted in three innovations: (1) the use of user-reported breakage issues in forums that enable the creation of a high-quality dataset for training in which only breakage that users perceive as an issue is included; (2) the use of 'web saliency' to automatically identify user-relevant regions of a website on which to prioritize automated interactions aimed at triggering breakage; and (3) the analysis of webpages via subtrees which enables fine-grained identification of problematic filter rules."],"url":"http://arxiv.org/abs/2405.05196v1","category":"cs.CR"}
{"created":"2024-05-08 16:26:49","title":"Is Transductive Learning Equivalent to PAC Learning?","abstract":"Most work in the area of learning theory has focused on designing effective Probably Approximately Correct (PAC) learners. Recently, other models of learning such as transductive error have seen more scrutiny. We move toward showing that these problems are equivalent by reducing agnostic learning with a PAC guarantee to agnostic learning with a transductive guarantee by adding a small number of samples to the dataset. We first rederive the result of Aden-Ali et al. arXiv:2304.09167 reducing PAC learning to transductive learning in the realizable setting using simpler techniques and at more generality as background for our main positive result. Our agnostic transductive to PAC conversion technique extends the aforementioned argument to the agnostic case, showing that an agnostic transductive learner can be efficiently converted to an agnostic PAC learner. Finally, we characterize the performance of the agnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 for binary classification, and show that plugging it into our reduction leads to an agnostic PAC learner that is essentially optimal. Our results imply that transductive and PAC learning are essentially equivalent for supervised learning with pseudometric losses in the realizable setting, and for binary classification in the agnostic setting. We conjecture this is true more generally for the agnostic setting.","sentences":["Most work in the area of learning theory has focused on designing effective Probably Approximately Correct (PAC) learners.","Recently, other models of learning such as transductive error have seen more scrutiny.","We move toward showing that these problems are equivalent by reducing agnostic learning with a PAC guarantee to agnostic learning with a transductive guarantee by adding a small number of samples to the dataset.","We first rederive the result of Aden-Ali et al.","arXiv:2304.09167 reducing PAC learning to transductive learning in the realizable setting using simpler techniques and at more generality as background for our main positive result.","Our agnostic transductive to PAC conversion technique extends the aforementioned argument to the agnostic case, showing that an agnostic transductive learner can be efficiently converted to an agnostic PAC learner.","Finally, we characterize the performance of the agnostic one inclusion graph algorithm of Asilis et al. arXiv:2309.13692 for binary classification, and show that plugging it into our reduction leads to an agnostic PAC learner that is essentially optimal.","Our results imply that transductive and PAC learning are essentially equivalent for supervised learning with pseudometric losses in the realizable setting, and for binary classification in the agnostic setting.","We conjecture this is true more generally for the agnostic setting."],"url":"http://arxiv.org/abs/2405.05190v1","category":"stat.ML"}
{"created":"2024-05-08 16:20:47","title":"Machine Learning Assisted Dynamical Classification of Trans-Neptunian Objects","abstract":"Trans-Neptunian objects (TNOs) are small, icy bodies in the outer solar system. They are observed to have a complex orbital distribution that was shaped by the early dynamical history and migration of the giant planets. Comparisons between the different dynamical classes of modeled and observed TNOs can help constrain the history of the outer solar system. Because of the complex dynamics of TNOs, particularly those in and near mean motion resonances with Neptune, classification has traditionally been done by human inspection of plots of the time evolution of orbital parameters. This is very inefficient. The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to increase the number of known TNOs by a factor of $\\sim$10, necessitating a much more automated process. In this chapter we present an improved supervised machine learning classifier for TNOs. Using a large and diverse training set as well as carefully chosen, dynamically motivated data features calculated from numerical integrations of TNO orbits, our classifier returns results that match those of a human classifier 98% of the time, and dynamically relevant classifications 99.7% of the time. This classifier is dramatically more efficient than human classification, and it will improve classification of both observed and modeled TNO data.","sentences":["Trans-Neptunian objects (TNOs) are small, icy bodies in the outer solar system.","They are observed to have a complex orbital distribution that was shaped by the early dynamical history and migration of the giant planets.","Comparisons between the different dynamical classes of modeled and observed TNOs can help constrain the history of the outer solar system.","Because of the complex dynamics of TNOs, particularly those in and near mean motion resonances with Neptune, classification has traditionally been done by human inspection of plots of the time evolution of orbital parameters.","This is very inefficient.","The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to increase the number of known TNOs by a factor of $\\sim$10, necessitating a much more automated process.","In this chapter we present an improved supervised machine learning classifier for TNOs.","Using a large and diverse training set as well as carefully chosen, dynamically motivated data features calculated from numerical integrations of TNO orbits, our classifier returns results that match those of a human classifier 98% of the time, and dynamically relevant classifications 99.7% of the time.","This classifier is dramatically more efficient than human classification, and it will improve classification of both observed and modeled TNO data."],"url":"http://arxiv.org/abs/2405.05185v1","category":"astro-ph.EP"}
{"created":"2024-05-08 16:12:45","title":"Air Gap: Protecting Privacy-Conscious Conversational Agents","abstract":"The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns. While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors. We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.   Grounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task. Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality. For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective.","sentences":["The growing use of large language model (LLM)-based conversational agents to manage sensitive user data raises significant privacy concerns.","While these agents excel at understanding and acting on context, this capability can be exploited by malicious actors.","We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.   ","Grounded in the framework of contextual integrity, we introduce AirGapAgent, a privacy-conscious agent designed to prevent unintended data leakage by restricting the agent's access to only the data necessary for a specific task.","Extensive experiments using Gemini, GPT, and Mistral models as agents validate our approach's effectiveness in mitigating this form of context hijacking while maintaining core agent functionality.","For example, we show that a single-query context hijacking attack on a Gemini Ultra agent reduces its ability to protect user data from 94% to 45%, while an AirGapAgent achieves 97% protection, rendering the same attack ineffective."],"url":"http://arxiv.org/abs/2405.05175v1","category":"cs.CR"}
{"created":"2024-05-08 16:04:50","title":"Data-Error Scaling in Machine Learning on Natural Discrete Combinatorial Mutation-prone Sets: Case Studies on Peptides and Small Molecules","abstract":"We investigate trends in the data-error scaling behavior of machine learning (ML) models trained on discrete combinatorial spaces that are prone-to-mutation, such as proteins or organic small molecules. We trained and evaluated kernel ridge regression machines using variable amounts of computationally generated training data. Our synthetic datasets comprise i) two na\\\"ive functions based on many-body theory; ii) binding energy estimates between a protein and a mutagenised peptide; and iii) solvation energies of two 6-heavy atom structural graphs. In contrast to typical data-error scaling, our results showed discontinuous monotonic phase transitions during learning, observed as rapid drops in the test error at particular thresholds of training data. We observed two learning regimes, which we call saturated and asymptotic decay, and found that they are conditioned by the level of complexity (i.e. number of mutations) enclosed in the training set. We show that during training on this class of problems, the predictions were clustered by the ML models employed in the calibration plots. Furthermore, we present an alternative strategy to normalize learning curves (LCs) and the concept of mutant based shuffling. This work has implications for machine learning on mutagenisable discrete spaces such as chemical properties or protein phenotype prediction, and improves basic understanding of concepts in statistical learning theory.","sentences":["We investigate trends in the data-error scaling behavior of machine learning (ML) models trained on discrete combinatorial spaces that are prone-to-mutation, such as proteins or organic small molecules.","We trained and evaluated kernel ridge regression machines using variable amounts of computationally generated training data.","Our synthetic datasets comprise i) two na\\\"ive functions based on many-body theory; ii) binding energy estimates between a protein and a mutagenised peptide; and iii) solvation energies of two 6-heavy atom structural graphs.","In contrast to typical data-error scaling, our results showed discontinuous monotonic phase transitions during learning, observed as rapid drops in the test error at particular thresholds of training data.","We observed two learning regimes, which we call saturated and asymptotic decay, and found that they are conditioned by the level of complexity (i.e. number of mutations) enclosed in the training set.","We show that during training on this class of problems, the predictions were clustered by the ML models employed in the calibration plots.","Furthermore, we present an alternative strategy to normalize learning curves (LCs) and the concept of mutant based shuffling.","This work has implications for machine learning on mutagenisable discrete spaces such as chemical properties or protein phenotype prediction, and improves basic understanding of concepts in statistical learning theory."],"url":"http://arxiv.org/abs/2405.05167v1","category":"physics.chem-ph"}
{"created":"2024-05-08 15:27:58","title":"DenserRadar: A 4D millimeter-wave radar point cloud detector based on dense LiDAR point clouds","abstract":"The 4D millimeter-wave (mmWave) radar, with its robustness in extreme environments, extensive detection range, and capabilities for measuring velocity and elevation, has demonstrated significant potential for enhancing the perception abilities of autonomous driving systems in corner-case scenarios. Nevertheless, the inherent sparsity and noise of 4D mmWave radar point clouds restrict its further development and practical application. In this paper, we introduce a novel 4D mmWave radar point cloud detector, which leverages high-resolution dense LiDAR point clouds. Our approach constructs dense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a specially designed network named DenserRadar. The proposed method surpasses existing probability-based and learning-based radar point cloud detectors in terms of both point cloud density and accuracy on the K-Radar dataset.","sentences":["The 4D millimeter-wave (mmWave) radar, with its robustness in extreme environments, extensive detection range, and capabilities for measuring velocity and elevation, has demonstrated significant potential for enhancing the perception abilities of autonomous driving systems in corner-case scenarios.","Nevertheless, the inherent sparsity and noise of 4D mmWave radar point clouds restrict its further development and practical application.","In this paper, we introduce a novel 4D mmWave radar point cloud detector, which leverages high-resolution dense LiDAR point clouds.","Our approach constructs dense 3D occupancy ground truth from stitched LiDAR point clouds, and employs a specially designed network named DenserRadar.","The proposed method surpasses existing probability-based and learning-based radar point cloud detectors in terms of both point cloud density and accuracy on the K-Radar dataset."],"url":"http://arxiv.org/abs/2405.05131v1","category":"cs.RO"}
{"created":"2024-05-08 15:27:08","title":"Multi-scale Bottleneck Transformer for Weakly Supervised Multimodal Violence Detection","abstract":"Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available. In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges. In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges. Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features. Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features. Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance. Code is available at https://github.com/shengyangsun/MSBT.","sentences":["Weakly supervised multimodal violence detection aims to learn a violence detection model by leveraging multiple modalities such as RGB, optical flow, and audio, while only video-level annotations are available.","In the pursuit of effective multimodal violence detection (MVD), information redundancy, modality imbalance, and modality asynchrony are identified as three key challenges.","In this work, we propose a new weakly supervised MVD method that explicitly addresses these challenges.","Specifically, we introduce a multi-scale bottleneck transformer (MSBT) based fusion module that employs a reduced number of bottleneck tokens to gradually condense information and fuse each pair of modalities and utilizes a bottleneck token-based weighting scheme to highlight more important fused features.","Furthermore, we propose a temporal consistency contrast loss to semantically align pairwise fused features.","Experiments on the largest-scale XD-Violence dataset demonstrate that the proposed method achieves state-of-the-art performance.","Code is available at https://github.com/shengyangsun/MSBT."],"url":"http://arxiv.org/abs/2405.05130v1","category":"cs.CV"}
{"created":"2024-05-08 15:16:02","title":"Full Version: (De/Re)-Composition of Data-Parallel Computations via Multi-Dimensional Homomorphisms","abstract":"We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.","sentences":["We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\".","Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures.","To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy.","We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts).","Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning."],"url":"http://arxiv.org/abs/2405.05118v1","category":"cs.PL"}
{"created":"2024-05-08 14:44:34","title":"Approximation properties relative to continuous scale space for hybrid discretizations of Gaussian derivative operators","abstract":"This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences. The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.   While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.   In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods.","sentences":["This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences.","The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.   ","While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.   ","In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods."],"url":"http://arxiv.org/abs/2405.05095v1","category":"math.NA"}
{"created":"2024-05-08 14:42:35","title":"Understanding solid nitrogen through machine learning simulation","abstract":"We construct a fast, transferable, general purpose, machine-learning interatomic potential suitable for large-scale simulations of $N_2$. The potential is trained only on high quality quantum chemical molecule-molecule interactions, no condensed phase information is used. The potential reproduces the experimental phase diagram including the melt curve and the molecular solid phases of nitrogen up to 10 GPa. This demonstrates that many-molecule interactions are unnecessary to explain the condensed phases of $N_2$. With increased pressure, transitions are observed from cubic ($\\alpha-N_2$), which optimises quadrupole-quadrupole interactions, through tetragonal ($\\gamma-N_2$) which allows more efficient packing, through to monoclinic ($\\lambda-N_2$) which packs still more efficiently. On heating, we obtain the hcp 3D rotor phase ($\\beta-N_2$) and, at pressure, the cubic $\\delta-N_2$ phase which contains both 3D and 2D rotors, tetragonal $\\delta^\\star-N_2$ phase with 2D rotors and the rhombohedral $\\epsilon-N_2$. Molecular dynamics demonstrates where these phases are indeed rotors, rather than frustrated order. The model does not support the existence of the wide range of bondlengths reported for the complex $\\iota-N_2$ phase. The thermodynamic transitions involve both shifts of molecular centres and rotations of molecules. We simulate these phase transitions between finding that the onset of rotation is rapid whereas motion of molecular centres is inhibited and the cause of the observed sluggishness of transitions. Routine density functional theory calculations give a similar picture to the potential.","sentences":["We construct a fast, transferable, general purpose, machine-learning interatomic potential suitable for large-scale simulations of $N_2$. The potential is trained only on high quality quantum chemical molecule-molecule interactions, no condensed phase information is used.","The potential reproduces the experimental phase diagram including the melt curve and the molecular solid phases of nitrogen up to 10 GPa.","This demonstrates that many-molecule interactions are unnecessary to explain the condensed phases of $N_2$. With increased pressure, transitions are observed from cubic ($\\alpha-N_2$), which optimises quadrupole-quadrupole interactions, through tetragonal ($\\gamma-N_2$) which allows more efficient packing, through to monoclinic ($\\lambda-N_2$) which packs still more efficiently.","On heating, we obtain the hcp 3D rotor phase ($\\beta-N_2$) and, at pressure, the cubic $\\delta-N_2$ phase which contains both 3D and 2D rotors, tetragonal $\\delta^\\star-N_2$ phase with 2D rotors and the rhombohedral $\\epsilon-N_2$. Molecular dynamics demonstrates where these phases are indeed rotors, rather than frustrated order.","The model does not support the existence of the wide range of bondlengths reported for the complex $\\iota-N_2$ phase.","The thermodynamic transitions involve both shifts of molecular centres and rotations of molecules.","We simulate these phase transitions between finding that the onset of rotation is rapid whereas motion of molecular centres is inhibited and the cause of the observed sluggishness of transitions.","Routine density functional theory calculations give a similar picture to the potential."],"url":"http://arxiv.org/abs/2405.05092v2","category":"physics.comp-ph"}
{"created":"2024-05-08 14:18:13","title":"Towards Efficient Training and Evaluation of Robust Models against $l_0$ Bounded Adversarial Perturbations","abstract":"This work studies sparse adversarial perturbations bounded by $l_0$ norm. We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations. Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations. Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations. Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios. More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks. Codes are available at https://github.com/CityU-MLO/sPGD.","sentences":["This work studies sparse adversarial perturbations bounded by $l_0$ norm.","We propose a white-box PGD-like attack method named sparse-PGD to effectively and efficiently generate such perturbations.","Furthermore, we combine sparse-PGD with a black-box attack to comprehensively and more reliably evaluate the models' robustness against $l_0$ bounded adversarial perturbations.","Moreover, the efficiency of sparse-PGD enables us to conduct adversarial training to build robust models against sparse perturbations.","Extensive experiments demonstrate that our proposed attack algorithm exhibits strong performance in different scenarios.","More importantly, compared with other robust models, our adversarially trained model demonstrates state-of-the-art robustness against various sparse attacks.","Codes are available at https://github.com/CityU-MLO/sPGD."],"url":"http://arxiv.org/abs/2405.05075v1","category":"cs.LG"}
{"created":"2024-05-08 13:48:21","title":"More than 200 Globular Clusters in the Milky Way, and still no super-Solar metallicity ones","abstract":"Many globular clusters (GCs) in the Milky Way (MW) have been studied in recent years, especially in hidden regions such as those of the Galactic bulge. Our main goal is to understand what we can learn if we include these new objects into the MWGC system that we know today. We catalogue 37 recently discovered GCs. We use different distributions for investigating the MWGC system: metallicity distribution (MD), luminosity function (LF), and age distribution. We first treat separately the new GCs sample from the known and well-characterised GCs. We merge these two samples, upgrading the MWGC system. We performed a comparison between our clusters sample and field star (FS) population. We find a double peaked distribution for the LF, which shows an elongated faint end tail. Considering the \"merged\" sample, the LF and the MDs display a bimodality trend. We construct the MD for the FS sample, and comparing this with that one of the GCs, we learn that a high percentage of FS show [Fe/H]$>0$, whereas we do not detect any GCs in the same metallicity range. In order to understand this inconsistency, we construct the age-metallicity diagram for both samples, noting that the old and metal-poor population (age$\\geq8$ Gyr and [Fe/H]$\\leq -1.0$) is represented by GCs, while the young and metal-rich population (age$<8$ Gyr and [Fe/H]$>-1.0$) corresponds to FS. From the analysis of the GC LF and MD, we can conclude that many GCs, probably those very faint, have survived strong dynamical processes, typical of the Bulge regions. We cannot exclude the possibility that some of them have been accreted during past merging events, especially the metal-poor component, whereas the metal-rich population may be related to the formation of the bulge and/or disk. Finally, the difference that we notice between the GC and FS samples should be sought in the evolutionary difference between these two stellar populations.","sentences":["Many globular clusters (GCs) in the Milky Way (MW) have been studied in recent years, especially in hidden regions such as those of the Galactic bulge.","Our main goal is to understand what we can learn if we include these new objects into the MWGC system that we know today.","We catalogue 37 recently discovered GCs.","We use different distributions for investigating the MWGC system: metallicity distribution (MD), luminosity function (LF), and age distribution.","We first treat separately the new GCs sample from the known and well-characterised GCs.","We merge these two samples, upgrading the MWGC system.","We performed a comparison between our clusters sample and field star (FS) population.","We find a double peaked distribution for the LF, which shows an elongated faint end tail.","Considering the \"merged\" sample, the LF and the MDs display a bimodality trend.","We construct the MD for the FS sample, and comparing this with that one of the GCs, we learn that a high percentage of FS show [Fe/H]$>0$, whereas we do not detect any GCs in the same metallicity range.","In order to understand this inconsistency, we construct the age-metallicity diagram for both samples, noting that the old and metal-poor population (age$\\geq8$ Gyr and [Fe/H]$\\leq -1.0$) is represented by GCs, while the young and metal-rich population (age$<8$ Gyr and [Fe/H]$>-1.0$) corresponds to FS.","From the analysis of the GC LF and MD, we can conclude that many GCs, probably those very faint, have survived strong dynamical processes, typical of the Bulge regions.","We cannot exclude the possibility that some of them have been accreted during past merging events, especially the metal-poor component, whereas the metal-rich population may be related to the formation of the bulge and/or disk.","Finally, the difference that we notice between the GC and FS samples should be sought in the evolutionary difference between these two stellar populations."],"url":"http://arxiv.org/abs/2405.05055v1","category":"astro-ph.GA"}
{"created":"2024-05-08 13:38:56","title":"Seeds of Stereotypes: A Large-Scale Textual Analysis of Race and Gender Associations with Diseases in Online Sources","abstract":"Background Advancements in Large Language Models (LLMs) hold transformative potential in healthcare, however, recent work has raised concern about the tendency of these models to produce outputs that display racial or gender biases. Although training data is a likely source of such biases, exploration of disease and demographic associations in text data at scale has been limited.   Methods We conducted a large-scale textual analysis using a dataset comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl. The study analyzed the context in which various diseases are discussed alongside markers of race and gender. Given that LLMs are pre-trained on similar datasets, this approach allowed us to examine the potential biases that LLMs may learn and internalize. We compared these findings with actual demographic disease prevalence as well as GPT-4 outputs in order to evaluate the extent of bias representation.   Results Our findings indicate that demographic terms are disproportionately associated with specific disease concepts in online texts. gender terms are prominently associated with disease concepts, while racial terms are much less frequently associated. We find widespread disparities in the associations of specific racial and gender terms with the 18 diseases analyzed. Most prominently, we see an overall significant overrepresentation of Black race mentions in comparison to population proportions.   Conclusions Our results highlight the need for critical examination and transparent reporting of biases in LLM pretraining datasets. Our study suggests the need to develop mitigation strategies to counteract the influence of biased training data in LLMs, particularly in sensitive domains such as healthcare.","sentences":["Background Advancements in Large Language Models (LLMs) hold transformative potential in healthcare, however, recent work has raised concern about the tendency of these models to produce outputs that display racial or gender biases.","Although training data is a likely source of such biases, exploration of disease and demographic associations in text data at scale has been limited.   ","Methods We conducted a large-scale textual analysis using a dataset comprising diverse web sources, including Arxiv, Wikipedia, and Common Crawl.","The study analyzed the context in which various diseases are discussed alongside markers of race and gender.","Given that LLMs are pre-trained on similar datasets, this approach allowed us to examine the potential biases that LLMs may learn and internalize.","We compared these findings with actual demographic disease prevalence as well as GPT-4 outputs in order to evaluate the extent of bias representation.   ","Results Our findings indicate that demographic terms are disproportionately associated with specific disease concepts in online texts.","gender terms are prominently associated with disease concepts, while racial terms are much less frequently associated.","We find widespread disparities in the associations of specific racial and gender terms with the 18 diseases analyzed.","Most prominently, we see an overall significant overrepresentation of Black race mentions in comparison to population proportions.   ","Conclusions Our results highlight the need for critical examination and transparent reporting of biases in LLM pretraining datasets.","Our study suggests the need to develop mitigation strategies to counteract the influence of biased training data in LLMs, particularly in sensitive domains such as healthcare."],"url":"http://arxiv.org/abs/2405.05049v1","category":"cs.CL"}
{"created":"2024-05-08 12:57:46","title":"An anti-noise seismic inversion method based on diffusion model","abstract":"Seismic impedance inversion is one of the most important part of geophysical exploration. However, due to random noise, the traditional semi-supervised learning (SSL) methods lack generalization and stability. To solve this problem, some authors have proposed SSL methods with anti-noise function to improve noise robustness and inversion accuracy. However, such methods are often not ideal when faced with strong noise. In addition, Low-frequency impedance models can mitigate this problem, but creating accurate low-frequency models is difficult and error-prone when well-log data is sparse and subsurface structures are complex. To address those issues, we propose a novel deep learning inversion method called DSIM-USSL (Unsupervised and Semi-supervised joint Learning for Seismic Inversion based on diffusion model). Specifically, we are the first to introduce a diffusion model with strong noise tendency and construct a diffusion seismic inversion model (DSIM). In the reverse diffusion of DSIM, we design the encoder-decoder which combines CNN for capturing local features and GRU for global sequence modeling; and we choose U-net to learn the distribution of random noise, enhancing the generalization and stability of proposed method. Furthermore, to further improve generalization of the proposed method, a two-step training approach (USSL) is utilized. First, an unsupervised trained encoder-decoder is used as the initial network model in place of the traditional low-frequency wave impedance model that is difficult to accurately acquire. Then, the SSL is employed to further optimize the encoder-decoder model. Experimental results on the Marmousi2 model and field data demonstrate that the DSIM-USSL method achieves higher accuracy in the presence of seismic data with random noise, and maintains high stability even under strong noise conditions.","sentences":["Seismic impedance inversion is one of the most important part of geophysical exploration.","However, due to random noise, the traditional semi-supervised learning (SSL) methods lack generalization and stability.","To solve this problem, some authors have proposed SSL methods with anti-noise function to improve noise robustness and inversion accuracy.","However, such methods are often not ideal when faced with strong noise.","In addition, Low-frequency impedance models can mitigate this problem, but creating accurate low-frequency models is difficult and error-prone when well-log data is sparse and subsurface structures are complex.","To address those issues, we propose a novel deep learning inversion method called DSIM-USSL (Unsupervised and Semi-supervised joint Learning for Seismic Inversion based on diffusion model).","Specifically, we are the first to introduce a diffusion model with strong noise tendency and construct a diffusion seismic inversion model (DSIM).","In the reverse diffusion of DSIM, we design the encoder-decoder which combines CNN for capturing local features and GRU for global sequence modeling; and we choose U-net to learn the distribution of random noise, enhancing the generalization and stability of proposed method.","Furthermore, to further improve generalization of the proposed method, a two-step training approach (USSL) is utilized.","First, an unsupervised trained encoder-decoder is used as the initial network model in place of the traditional low-frequency wave impedance model that is difficult to accurately acquire.","Then, the SSL is employed to further optimize the encoder-decoder model.","Experimental results on the Marmousi2 model and field data demonstrate that the DSIM-USSL method achieves higher accuracy in the presence of seismic data with random noise, and maintains high stability even under strong noise conditions."],"url":"http://arxiv.org/abs/2405.05026v1","category":"physics.geo-ph"}
{"created":"2024-05-08 15:31:30","title":"Low-Distortion Clustering in Bounded Growth Graphs","abstract":"The well-known clustering algorithm of Miller, Peng, and Xu (SPAA 2013) is useful for many applications, including low-diameter decomposition and low-energy distributed algorithms. One nice property of their clustering, shown in previous work by Chang, Dani, Hayes, and Pettie (PODC 2020), is that distances in the cluster graph are rescaled versions of distances in the original graph, up to an $O(\\log n)$ distortion factor and rounding issues. Minimizing this distortion factor is important for efficiency in computing the clustering, as well as in other applications.   We prove that there exist graphs for which an $\\Omega((\\log n)^{1/3})$ distortion factor is necessary for any clustering. We also consider a class of nice graphs which we call uniformly bounded independence graphs. These include, for example, paths, lattice graphs, and \"dense\" unit disk graphs. For these graphs, we prove that clusterings of distortion $O(1)$ always exist, and moreover, we give new efficient distributed algorithms to construct them. This clustering is based on Voronoi cells centered at the vertices of a maximal independent set in a suitable power graph.   Applications include low-energy simulation of distributed algorithms in the LOCAL, CONGEST, and RADIO-CONGEST models and efficient approximate solutions to distributed combinatorial optimization problems. We also investigate related lower bounds.","sentences":["The well-known clustering algorithm of Miller, Peng, and Xu (SPAA 2013) is useful for many applications, including low-diameter decomposition and low-energy distributed algorithms.","One nice property of their clustering, shown in previous work by Chang, Dani, Hayes, and Pettie (PODC 2020), is that distances in the cluster graph are rescaled versions of distances in the original graph, up to an $O(\\log n)$ distortion factor and rounding issues.","Minimizing this distortion factor is important for efficiency in computing the clustering, as well as in other applications.   ","We prove that there exist graphs for which an $\\Omega((\\log n)^{1/3})$ distortion factor is necessary for any clustering.","We also consider a class of nice graphs which we call uniformly bounded independence graphs.","These include, for example, paths, lattice graphs, and \"dense\" unit disk graphs.","For these graphs, we prove that clusterings of distortion $O(1)$ always exist, and moreover, we give new efficient distributed algorithms to construct them.","This clustering is based on Voronoi cells centered at the vertices of a maximal independent set in a suitable power graph.   ","Applications include low-energy simulation of distributed algorithms in the LOCAL, CONGEST, and RADIO-CONGEST models and efficient approximate solutions to distributed combinatorial optimization problems.","We also investigate related lower bounds."],"url":"http://arxiv.org/abs/2405.05132v1","category":"cs.DC"}
{"created":"2024-05-08 15:20:37","title":"An Imprecise Maxwell's Demon with Feedback Delay: An Exactly Solvable Information Engine Model","abstract":"A finite cycle time information engine based on a two-level system in contact with a thermal reservoir is studied analytically. The model for the engine incorporates an error in measuring the system's state and time delay between the measurement and the feedback process. The efficiency and power of the engine in steady state are derived as a function of level spacing, feedback delay time, engine cycle time, and measurement error. For a fixed value of level spacing and feedback delay, there is an upper bound on measurement error such that the engine can extract positive work. This threshold value of error is found to be independent of the cycle time. For a range of values of level spacing and feedback delay time, efficiency has a non-monotonic dependence on the measurement error, implying that there is an optimal measurement error for the information engine to operate efficiently. At high temperatures and with precise measurement, the engine's ability to extract positive work is extended over a larger range of feedback delay time.","sentences":["A finite cycle time information engine based on a two-level system in contact with a thermal reservoir is studied analytically.","The model for the engine incorporates an error in measuring the system's state and time delay between the measurement and the feedback process.","The efficiency and power of the engine in steady state are derived as a function of level spacing, feedback delay time, engine cycle time, and measurement error.","For a fixed value of level spacing and feedback delay, there is an upper bound on measurement error such that the engine can extract positive work.","This threshold value of error is found to be independent of the cycle time.","For a range of values of level spacing and feedback delay time, efficiency has a non-monotonic dependence on the measurement error, implying that there is an optimal measurement error for the information engine to operate efficiently.","At high temperatures and with precise measurement, the engine's ability to extract positive work is extended over a larger range of feedback delay time."],"url":"http://arxiv.org/abs/2405.05123v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-08 14:52:35","title":"Energy stable gradient flow schemes for shape and topology optimization in Navier-Stokes flows","abstract":"We study topology optimization governed by the incompressible Navier-Stokes flows using a phase field model. Novel stabilized semi-implicit schemes for the gradient flows of Allen-Cahn and Cahn-Hilliard types are proposed for solving the resulting optimal control problem. Unconditional energy stability is shown for the gradient flow schemes in continuous and discrete spaces. Numerical experiments of computational fluid dynamics in 2d and 3d show the effectiveness and robustness of the optimization algorithms proposed.","sentences":["We study topology optimization governed by the incompressible Navier-Stokes flows using a phase field model.","Novel stabilized semi-implicit schemes for the gradient flows of Allen-Cahn and Cahn-Hilliard types are proposed for solving the resulting optimal control problem.","Unconditional energy stability is shown for the gradient flow schemes in continuous and discrete spaces.","Numerical experiments of computational fluid dynamics in 2d and 3d show the effectiveness and robustness of the optimization algorithms proposed."],"url":"http://arxiv.org/abs/2405.05098v1","category":"math.NA"}
{"created":"2024-05-08 14:49:01","title":"Rapid Co-design of Task-Specialized Whegged Robots for Ad-Hoc Needs","abstract":"In this work, we investigate the use of co-design methods to iterate upon robot designs in the field, performing time sensitive, ad-hoc tasks. Our method optimizes the morphology and wheg trajectory for a MiniRHex robot, producing 3D printable structures and leg trajectory parameters. Tested in four terrains, we show that robots optimized in simulation exhibit strong sim-to-real transfer and are nearly twice as efficient as the nominal platform when tested in hardware.","sentences":["In this work, we investigate the use of co-design methods to iterate upon robot designs in the field, performing time sensitive, ad-hoc tasks.","Our method optimizes the morphology and wheg trajectory for a MiniRHex robot, producing 3D printable structures and leg trajectory parameters.","Tested in four terrains, we show that robots optimized in simulation exhibit strong sim-to-real transfer and are nearly twice as efficient as the nominal platform when tested in hardware."],"url":"http://arxiv.org/abs/2405.05096v1","category":"cs.RO"}
{"created":"2024-05-08 14:22:39","title":"Power Variable Projection for Initialization-Free Large-Scale Bundle Adjustment","abstract":"Initialization-free bundle adjustment (BA) remains largely uncharted. While Levenberg-Marquardt algorithm is the golden method to solve the BA problem, it generally relies on a good initialization. In contrast, the under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization. Coupled with object space error formulation, recent works have shown its ability to solve (small-scale) initialization-free bundle adjustment problem. We introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series. Importantly, we link the power series expansion to Riemannian manifold optimization. This projective framework is crucial to solve large-scale bundle adjustment problem without initialization. Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy. In particular, our work is the first, to our knowledge, that addresses the scalability of BA without initialization and opens new venues for initialization-free Structure-from-Motion.","sentences":["Initialization-free bundle adjustment (BA) remains largely uncharted.","While Levenberg-Marquardt algorithm is the golden method to solve the BA problem, it generally relies on a good initialization.","In contrast, the under-explored Variable Projection algorithm (VarPro) exhibits a wide convergence basin even without initialization.","Coupled with object space error formulation, recent works have shown its ability to solve (small-scale) initialization-free bundle adjustment problem.","We introduce Power Variable Projection (PoVar), extending a recent inverse expansion method based on power series.","Importantly, we link the power series expansion to Riemannian manifold optimization.","This projective framework is crucial to solve large-scale bundle adjustment problem without initialization.","Using the real-world BAL dataset, we experimentally demonstrate that our solver achieves state-of-the-art results in terms of speed and accuracy.","In particular, our work is the first, to our knowledge, that addresses the scalability of BA without initialization and opens new venues for initialization-free Structure-from-Motion."],"url":"http://arxiv.org/abs/2405.05079v2","category":"cs.CV"}
{"created":"2024-05-08 14:11:01","title":"Equivalence analysis between Quasi-coarse-grained and Atomistic Simulations","abstract":"In recent years, simulation methods based on the scaling of atomic potential functions, such as quasi-coarse-grained dynamics and coarse-grained dynamics, have shown promising results for modeling crystalline systems at multiple scales. However, this letter presents evidence suggesting that the spatiotemporal trajectories of coarse-grained systems generated by such simulation methods exhibit a complete correspondence with those of specific molecular dynamics systems. In essence, current coarse-grained simulation methods involve a direct amplification of the results obtained from molecular dynamics simulations across spatial and temporal scales, yet they may lack the capability to adequately capture authentic scale effects. Consequently, the findings of related studies warrant careful re-evaluation. Furthermore, this study underscores the importance of not only verifying the consistency of mesoscale simulation methods with microscopic simulations but also meticulously assessing their capability to accurately forecast mesoscale physical phenomena.","sentences":["In recent years, simulation methods based on the scaling of atomic potential functions, such as quasi-coarse-grained dynamics and coarse-grained dynamics, have shown promising results for modeling crystalline systems at multiple scales.","However, this letter presents evidence suggesting that the spatiotemporal trajectories of coarse-grained systems generated by such simulation methods exhibit a complete correspondence with those of specific molecular dynamics systems.","In essence, current coarse-grained simulation methods involve a direct amplification of the results obtained from molecular dynamics simulations across spatial and temporal scales, yet they may lack the capability to adequately capture authentic scale effects.","Consequently, the findings of related studies warrant careful re-evaluation.","Furthermore, this study underscores the importance of not only verifying the consistency of mesoscale simulation methods with microscopic simulations but also meticulously assessing their capability to accurately forecast mesoscale physical phenomena."],"url":"http://arxiv.org/abs/2405.05070v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-08 13:52:14","title":"Real-Time Motion Detection Using Dynamic Mode Decomposition","abstract":"Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit timeseries data to a linear dynamical system. In doing so, DMD decomposes dynamic data into spatially coherent modes that evolve in time according to exponential growth/decay or with a fixed frequency of oscillation. A prolific application of DMD has been to video, where one interprets the high-dimensional pixel space evolving through time as the video plays. In this work, we propose a simple and interpretable motion detection algorithm for streaming video data rooted in DMD. Our method leverages the fact that there exists a correspondence between the evolution of important video features, such as foreground motion, and the eigenvalues of the matrix which results from applying DMD to segments of video. We apply the method to a database of test videos which emulate security footage under varying realistic conditions. Effectiveness is analyzed using receiver operating characteristic curves, while we use cross-validation to optimize the threshold parameter that identifies movement.","sentences":["Dynamic Mode Decomposition (DMD) is a numerical method that seeks to fit timeseries data to a linear dynamical system.","In doing so, DMD decomposes dynamic data into spatially coherent modes that evolve in time according to exponential growth/decay or with a fixed frequency of oscillation.","A prolific application of DMD has been to video, where one interprets the high-dimensional pixel space evolving through time as the video plays.","In this work, we propose a simple and interpretable motion detection algorithm for streaming video data rooted in DMD.","Our method leverages the fact that there exists a correspondence between the evolution of important video features, such as foreground motion, and the eigenvalues of the matrix which results from applying DMD to segments of video.","We apply the method to a database of test videos which emulate security footage under varying realistic conditions.","Effectiveness is analyzed using receiver operating characteristic curves, while we use cross-validation to optimize the threshold parameter that identifies movement."],"url":"http://arxiv.org/abs/2405.05057v1","category":"cs.CV"}
{"created":"2024-05-08 13:08:39","title":"Locally-Measured R\u00e9nyi Divergences","abstract":"We propose an extension of the classical R\\'enyi divergences to quantum states through an optimization over probability distributions induced by restricted sets of measurements. In particular, we define the notion of locally-measured R\\'enyi divergences, where the set of allowed measurements originates from variants of locality constraints between (distant) parties $A$ and $B$. We then derive variational bounds on the locally-measured R\\'enyi divergences and systematically discuss when these bounds become exact characterizations. As an application, we evaluate the locally-measured R\\'enyi divergences on variants of highly symmetric data-hiding states, showcasing the reduced distinguishing power of locality-constrained measurements. For $n$-fold tensor powers, we further employ our variational formulae to derive corresponding additivity results, which gives the locally-measured R\\'enyi divergences operational meaning as optimal rate exponents in asymptotic locally-measured hypothesis testing.","sentences":["We propose an extension of the classical R\\'enyi divergences to quantum states through an optimization over probability distributions induced by restricted sets of measurements.","In particular, we define the notion of locally-measured R\\'enyi divergences, where the set of allowed measurements originates from variants of locality constraints between (distant) parties $A$ and $B$. We then derive variational bounds on the locally-measured R\\'enyi divergences and systematically discuss when these bounds become exact characterizations.","As an application, we evaluate the locally-measured R\\'enyi divergences on variants of highly symmetric data-hiding states, showcasing the reduced distinguishing power of locality-constrained measurements.","For $n$-fold tensor powers, we further employ our variational formulae to derive corresponding additivity results, which gives the locally-measured R\\'enyi divergences operational meaning as optimal rate exponents in asymptotic locally-measured hypothesis testing."],"url":"http://arxiv.org/abs/2405.05037v1","category":"quant-ph"}
{"created":"2024-05-08 12:44:37","title":"Quantum Circuit Ansatz: Abstraction and Reuse of Quantum Algorithm Design","abstract":"Quantum computing holds the potential to revolutionize various fields by efficiently tackling complex problems. At its core are quantum circuits, sequences of quantum gates manipulating quantum states. The selection of the right quantum circuit ansatz, which defines initial circuit structures and serves as the basis for optimization techniques, is crucial in quantum algorithm design.This paper presents a categorized catalog of quantum circuit ansatzes aimed at supporting quantum algorithm design and implementation. Each ansatz is described with details such as intent, motivation, applicability, circuit diagram, implementation, example, and see also. Practical examples are provided to illustrate their application in quantum algorithm design.The catalog aims to assist quantum algorithm designers by offering insights into the strengths and limitations of different ansatzes, thereby facilitating decision-making for specific tasks.","sentences":["Quantum computing holds the potential to revolutionize various fields by efficiently tackling complex problems.","At its core are quantum circuits, sequences of quantum gates manipulating quantum states.","The selection of the right quantum circuit ansatz, which defines initial circuit structures and serves as the basis for optimization techniques, is crucial in quantum algorithm design.","This paper presents a categorized catalog of quantum circuit ansatzes aimed at supporting quantum algorithm design and implementation.","Each ansatz is described with details such as intent, motivation, applicability, circuit diagram, implementation, example, and see also.","Practical examples are provided to illustrate their application in quantum algorithm design.","The catalog aims to assist quantum algorithm designers by offering insights into the strengths and limitations of different ansatzes, thereby facilitating decision-making for specific tasks."],"url":"http://arxiv.org/abs/2405.05021v1","category":"cs.SE"}
{"created":"2024-05-08 11:56:20","title":"On Stochastic Fundamental Limits in a Downlink Integrated Sensing and Communication Network","abstract":"This paper aims to analyze the stochastic performance of a multiple input multiple output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario, where a base station (BS) transmits a dual-functional radar-communication (DFRC) signal matrix, serving the purpose of transmitting communication data to the user while simultaneously sensing the angular location of a target. The channel between the BS and the user is modeled as a random channel with Rayleigh fading distribution, and the azimuth angle of the target is assumed to follow a uniform distribution. Due to the randomness inherent in the network, the challenge is to consider suitable performance metrics for this randomness. To address this issue, for users, we employ the user's rate outage probability (OP) and ergodic rate, while for target, we propose using the OP of the Cram\\'er-Rao lower bound (CRLB) for the angle of arrival and the ergodic CRLB. We have obtained the expressions of these metrics for scenarios where the BS employs two different beamforming methods. Our approach to deriving these metrics involves computing the probability density function (PDF) of the signal-to-noise ratio for users and the CRLB for the target. We have demonstrated that the central limit theorem provides a viable approach for deriving these PDFs. In our numerical results, we demonstrate the trade-off between sensing and communication (S \\& C) by characterizing the region of S \\& C metrics and by obtaining the Pareto optimal boundary points, confirmed with simulations.","sentences":["This paper aims to analyze the stochastic performance of a multiple input multiple output (MIMO) integrated sensing and communication (ISAC) system in a downlink scenario, where a base station (BS) transmits a dual-functional radar-communication (DFRC) signal matrix, serving the purpose of transmitting communication data to the user while simultaneously sensing the angular location of a target.","The channel between the BS and the user is modeled as a random channel with Rayleigh fading distribution, and the azimuth angle of the target is assumed to follow a uniform distribution.","Due to the randomness inherent in the network, the challenge is to consider suitable performance metrics for this randomness.","To address this issue, for users, we employ the user's rate outage probability (OP) and ergodic rate, while for target, we propose using the OP of the Cram\\'er-Rao lower bound (CRLB) for the angle of arrival and the ergodic CRLB.","We have obtained the expressions of these metrics for scenarios where the BS employs two different beamforming methods.","Our approach to deriving these metrics involves computing the probability density function (PDF) of the signal-to-noise ratio for users and the CRLB for the target.","We have demonstrated that the central limit theorem provides a viable approach for deriving these PDFs.","In our numerical results, we demonstrate the trade-off between sensing and communication (S \\& C) by characterizing the region of S \\& C metrics and by obtaining the Pareto optimal boundary points, confirmed with simulations."],"url":"http://arxiv.org/abs/2405.04993v1","category":"cs.IT"}
{"created":"2024-05-08 11:51:30","title":"The Riemannian geometry of Sinkhorn divergences","abstract":"We propose a new metric between probability measures on a compact metric space that mirrors the Riemannian manifold-like structure of quadratic optimal transport but includes entropic regularization. Its metric tensor is given by the Hessian of the Sinkhorn divergence, a debiased variant of entropic optimal transport. We precisely identify the tangent space it induces, which turns out to be related to a Reproducing Kernel Hilbert Space (RKHS). As usual in Riemannian geometry, the distance is built by looking for shortest paths. We prove that our distance is geodesic, metrizes the weak-star topology, and is equivalent to a RKHS norm. Still it retains the geometric flavor of optimal transport: as a paradigmatic example, translations are geodesics for the quadratic cost on $\\mathbb{R}^d$. We also show two negative results on the Sinkhorn divergence that may be of independent interest: that it is not jointly convex, and that its square root is not a distance because it fails to satisfy the triangle inequality.","sentences":["We propose a new metric between probability measures on a compact metric space that mirrors the Riemannian manifold-like structure of quadratic optimal transport but includes entropic regularization.","Its metric tensor is given by the Hessian of the Sinkhorn divergence, a debiased variant of entropic optimal transport.","We precisely identify the tangent space it induces, which turns out to be related to a Reproducing Kernel Hilbert Space (RKHS).","As usual in Riemannian geometry, the distance is built by looking for shortest paths.","We prove that our distance is geodesic, metrizes the weak-star topology, and is equivalent to a RKHS norm.","Still it retains the geometric flavor of optimal transport: as a paradigmatic example, translations are geodesics for the quadratic cost on $\\mathbb{R}^d$. We also show two negative results on the Sinkhorn divergence that may be of independent interest: that it is not jointly convex, and that its square root is not a distance because it fails to satisfy the triangle inequality."],"url":"http://arxiv.org/abs/2405.04987v1","category":"math.OC"}
{"created":"2024-05-08 11:46:00","title":"Dynamic Data Layout Optimization with Worst-case Guarantees","abstract":"Many data analytics systems store and process large datasets in partitions containing millions of rows. By mapping rows to partitions in an optimized way, it is possible to improve query performance by skipping over large numbers of irrelevant partitions during query processing. This mapping is referred to as a data layout. Recent works have shown that customizing the data layout to the anticipated query workload greatly improves query performance, but the performance benefits may disappear if the workload changes. Reorganizing data layouts to accommodate workload drift can resolve this issue, but reorganization costs could exceed query savings if not done carefully.   In this paper, we present an algorithmic framework OReO that makes online reorganization decisions to balance the benefits of improved query performance with the costs of reorganization. Our framework extends results from Metrical Task Systems to provide a tight bound on the worst-case performance guarantee for online reorganization, without prior knowledge of the query workload. Through evaluation on real-world datasets and query workloads, our experiments demonstrate that online reorganization with OReO can lead to an up to 32% improvement in combined query and reorganization time compared to using a single, optimized data layout for the entire workload.","sentences":["Many data analytics systems store and process large datasets in partitions containing millions of rows.","By mapping rows to partitions in an optimized way, it is possible to improve query performance by skipping over large numbers of irrelevant partitions during query processing.","This mapping is referred to as a data layout.","Recent works have shown that customizing the data layout to the anticipated query workload greatly improves query performance, but the performance benefits may disappear if the workload changes.","Reorganizing data layouts to accommodate workload drift can resolve this issue, but reorganization costs could exceed query savings if not done carefully.   ","In this paper, we present an algorithmic framework OReO that makes online reorganization decisions to balance the benefits of improved query performance with the costs of reorganization.","Our framework extends results from Metrical Task Systems to provide a tight bound on the worst-case performance guarantee for online reorganization, without prior knowledge of the query workload.","Through evaluation on real-world datasets and query workloads, our experiments demonstrate that online reorganization with OReO can lead to an up to 32% improvement in combined query and reorganization time compared to using a single, optimized data layout for the entire workload."],"url":"http://arxiv.org/abs/2405.04984v1","category":"cs.DB"}
{"created":"2024-05-08 11:01:21","title":"P-ICL: Point In-Context Learning for Named Entity Recognition with Large Language Models","abstract":"In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL). However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself. In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type. With such significant information, the LLM can achieve entity classification more precisely. To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering. Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection.","sentences":["In recent years, the rise of large language models (LLMs) has made it possible to directly achieve named entity recognition (NER) without any demonstration samples or only using a few samples through in-context learning (ICL).","However, standard ICL only helps LLMs understand task instructions, format and input-label mapping, but neglects the particularity of the NER task itself.","In this paper, we propose a new prompting framework P-ICL to better achieve NER with LLMs, in which some point entities are leveraged as the auxiliary information to recognize each entity type.","With such significant information, the LLM can achieve entity classification more precisely.","To obtain optimal point entities for prompting LLMs, we also proposed a point entity selection method based on K-Means clustering.","Our extensive experiments on some representative NER benchmarks verify the effectiveness of our proposed strategies in P-ICL and point entity selection."],"url":"http://arxiv.org/abs/2405.04960v1","category":"cs.CL"}
{"created":"2024-05-08 10:46:22","title":"Evolving R2 to R2+: Optimal, Delayed Line-of-sight Vector-based Path Planning","abstract":"A vector-based any-angle path planner, R2, is evolved in to R2+ in this paper. By delaying line-of-sight, R2 and R2+ search times are largely unaffected by the distance between the start and goal points, but are exponential in the worst case with respect to the number of collisions during searches. To improve search times, additional discarding conditions in the overlap rule are introduced in R2+. In addition, R2+ resolves interminable chases in R2 by replacing ad hoc points with limited occupied-sector traces from target nodes, and simplifies R2 by employing new abstract structures and ensuring target progression during a trace. R2+ preserves the speed of R2 when paths are expected to detour around few obstacles, and searches significantly faster than R2 in maps with many disjoint obstacles.","sentences":["A vector-based any-angle path planner, R2, is evolved in to R2+ in this paper.","By delaying line-of-sight, R2 and R2+ search times are largely unaffected by the distance between the start and goal points, but are exponential in the worst case with respect to the number of collisions during searches.","To improve search times, additional discarding conditions in the overlap rule are introduced in R2+.","In addition, R2+ resolves interminable chases in R2 by replacing ad hoc points with limited occupied-sector traces from target nodes, and simplifies R2 by employing new abstract structures and ensuring target progression during a trace.","R2+ preserves the speed of R2 when paths are expected to detour around few obstacles, and searches significantly faster than R2 in maps with many disjoint obstacles."],"url":"http://arxiv.org/abs/2405.04952v1","category":"cs.RO"}
{"created":"2024-05-08 10:38:10","title":"The Spectral Gap of a Gaussian Quantum Markovian Generator","abstract":"Gaussian quantum Markov semigroups are the natural non-commutative extension of classical Ornstein-Uhlenbeck semigroups. They arise in open quantum systems of bosons where canonical non-commuting random variables of positions and momenta come into play. If there exits a faithful invariant density we explicitly compute the optimal exponential convergence rate, namely the spectral gap of the generator, in non-commutative $L^2$ spaces determined by the invariant density showing that the exact value is the lowest eigenvalue of a certain matrix determined by the diffusion and drift matrices. The spectral gap turns out to depend on the non-commutative $L^2$ space considered, whether the one determined by the so-called GNS or KMS multiplication by the square root of the invariant density. In the first case, it is strictly positive if and only if there is the maximum number of linearly independent noises. While, we exhibit explicit examples in which it is strictly positive only with KMS multiplication. We do not assume any symmetry or quantum detailed balance condition with respect to the invariant density.","sentences":["Gaussian quantum Markov semigroups are the natural non-commutative extension of classical Ornstein-Uhlenbeck semigroups.","They arise in open quantum systems of bosons where canonical non-commuting random variables of positions and momenta come into play.","If there exits a faithful invariant density we explicitly compute the optimal exponential convergence rate, namely the spectral gap of the generator, in non-commutative $L^2$ spaces determined by the invariant density showing that the exact value is the lowest eigenvalue of a certain matrix determined by the diffusion and drift matrices.","The spectral gap turns out to depend on the non-commutative $L^2$ space considered, whether the one determined by the so-called GNS or KMS multiplication by the square root of the invariant density.","In the first case, it is strictly positive if and only if there is the maximum number of linearly independent noises.","While, we exhibit explicit examples in which it is strictly positive only with KMS multiplication.","We do not assume any symmetry or quantum detailed balance condition with respect to the invariant density."],"url":"http://arxiv.org/abs/2405.04947v1","category":"math.FA"}
{"created":"2024-05-08 10:10:24","title":"Fault Identification Enhancement with Reinforcement Learning (FIERL)","abstract":"This letter presents a novel approach in the field of Active Fault Detection (AFD), by explicitly separating the task into two parts: Passive Fault Detection (PFD) and control input design. This formulation is very general, and most existing AFD literature can be viewed through this lens. By recognizing this separation, PFD methods can be leveraged to provide components that make efficient use of the available information, while the control input is designed in order to optimize the gathering of information. The core contribution of this work is FIERL, a general simulation-based approach for the design of such control strategies, using Constrained Reinforcement Learning (CRL) to optimize the performance of arbitrary passive detectors. The control policy is learned without the need of knowing the passive detector inner workings, making FIERL broadly applicable. However, it is especially useful when paired with the design of an efficient passive component. Unlike most AFD approaches, FIERL can handle fairly complex scenarios such as continuous sets of fault modes. The effectiveness of FIERL is tested on a benchmark problem for actuator fault diagnosis, where FIERL is shown to be fairly robust, being able to generalize to fault dynamics not seen in training.","sentences":["This letter presents a novel approach in the field of Active Fault Detection (AFD), by explicitly separating the task into two parts: Passive Fault Detection (PFD) and control input design.","This formulation is very general, and most existing AFD literature can be viewed through this lens.","By recognizing this separation, PFD methods can be leveraged to provide components that make efficient use of the available information, while the control input is designed in order to optimize the gathering of information.","The core contribution of this work is FIERL, a general simulation-based approach for the design of such control strategies, using Constrained Reinforcement Learning (CRL) to optimize the performance of arbitrary passive detectors.","The control policy is learned without the need of knowing the passive detector inner workings, making FIERL broadly applicable.","However, it is especially useful when paired with the design of an efficient passive component.","Unlike most AFD approaches, FIERL can handle fairly complex scenarios such as continuous sets of fault modes.","The effectiveness of FIERL is tested on a benchmark problem for actuator fault diagnosis, where FIERL is shown to be fairly robust, being able to generalize to fault dynamics not seen in training."],"url":"http://arxiv.org/abs/2405.04938v1","category":"cs.LG"}
{"created":"2024-05-08 09:55:59","title":"Mapping reaction mechanism during overcharge of a LiNiO2/Graphite-silicon lithium-ion battery: a correlative operando approach by simultaneous gas analysis and synchrotron scattering techniques","abstract":"Li-ion battery degradation processes are multi-scale, heterogeneous, dynamic and involve multiple cell components through cross talk mechanisms. Correlated operando characterization capable of measuring several key parameters are needed to accelerate understanding on these complex degradation processes. In particular, degradation mechanisms during overcharge of LiNiO2/Graphite-Silicon is well known at the material level featuring O2 gas release and concomitant surface reconstruction of LiNiO2. However, there are still debates regarding the role of high voltage O1 phase formation on gas production and no information on the effect of produced gases on the cell components (anode or sensors), or effect of overcharge on electrode level behavior. In this work, we simultaneously measured the gas produced using operando mass spectrometry while spatially resolving nanostructure and lattice changes using operando micro SAXS/WAXS mapping during the formation and over charge of a LiNiO2/Gr-Si pouch cell. This new correlated operando characterization experiment allowed to (1) confirm the absence of O1 phase even with substantial gas produced at end of charge, (2) unveil the effect of gases on reference and negative electrodes, (3) show that overcharge increases in-plane reaction heterogeneities by creating local degraded spots lagging behind the ensemble electrochemistry. These findings will be important to optimize ageing of devices based on similar chemistries, in particular Ni-rich NMC, while showing the strength of correlated characterization leading to more efficient and robust information on complex mechanisms.","sentences":["Li-ion battery degradation processes are multi-scale, heterogeneous, dynamic and involve multiple cell components through cross talk mechanisms.","Correlated operando characterization capable of measuring several key parameters are needed to accelerate understanding on these complex degradation processes.","In particular, degradation mechanisms during overcharge of LiNiO2/Graphite-Silicon is well known at the material level featuring O2 gas release and concomitant surface reconstruction of LiNiO2.","However, there are still debates regarding the role of high voltage O1 phase formation on gas production and no information on the effect of produced gases on the cell components (anode or sensors), or effect of overcharge on electrode level behavior.","In this work, we simultaneously measured the gas produced using operando mass spectrometry while spatially resolving nanostructure and lattice changes using operando micro SAXS/WAXS mapping during the formation and over charge of a LiNiO2/Gr-Si pouch cell.","This new correlated operando characterization experiment allowed to (1) confirm the absence of O1 phase even with substantial gas produced at end of charge, (2) unveil the effect of gases on reference and negative electrodes, (3) show that overcharge increases in-plane reaction heterogeneities by creating local degraded spots lagging behind the ensemble electrochemistry.","These findings will be important to optimize ageing of devices based on similar chemistries, in particular Ni-rich NMC, while showing the strength of correlated characterization leading to more efficient and robust information on complex mechanisms."],"url":"http://arxiv.org/abs/2405.04931v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-08 09:52:05","title":"Power-Domain Interference Graph Estimation for Full-Duplex Millimeter-Wave Backhauling","abstract":"Traditional wisdom for network resource management allocates separate frequency-time resources for measurement and data transmission tasks. As a result, the two types of tasks have to compete for resources, and a heavy measurement task inevitably reduces available resources for data transmission. This prevents interference graph estimation (IGE), a heavy yet important measurement task, from being widely used in practice. To resolve this issue, we propose to use power as a new dimension for interference measurement in full-duplex millimeter-wave backhaul networks, such that data transmission and measurement can be done simultaneously using the same frequency-time resources. Our core insight is to consider the mmWave network as a linear system, where the received power of a node is a linear combination of the channel gains. By controlling the powers of transmitters, we can find unique solutions for the channel gains of interference links and use them to estimate the interference. To accomplish resource allocation and IGE simultaneously, we jointly optimize resource allocation and IGE with power control. Extensive simulations show that significant links in the interference graph can be accurately estimated with minimal extra power consumption, independent of the time and carrier frequency offsets between nodes.","sentences":["Traditional wisdom for network resource management allocates separate frequency-time resources for measurement and data transmission tasks.","As a result, the two types of tasks have to compete for resources, and a heavy measurement task inevitably reduces available resources for data transmission.","This prevents interference graph estimation (IGE), a heavy yet important measurement task, from being widely used in practice.","To resolve this issue, we propose to use power as a new dimension for interference measurement in full-duplex millimeter-wave backhaul networks, such that data transmission and measurement can be done simultaneously using the same frequency-time resources.","Our core insight is to consider the mmWave network as a linear system, where the received power of a node is a linear combination of the channel gains.","By controlling the powers of transmitters, we can find unique solutions for the channel gains of interference links and use them to estimate the interference.","To accomplish resource allocation and IGE simultaneously, we jointly optimize resource allocation and IGE with power control.","Extensive simulations show that significant links in the interference graph can be accurately estimated with minimal extra power consumption, independent of the time and carrier frequency offsets between nodes."],"url":"http://arxiv.org/abs/2405.04926v2","category":"cs.NI"}
{"created":"2024-05-08 09:30:59","title":"Urban Boundary Delineation from Commuting Data with Bayesian Stochastic Blockmodeling: Scale, Contiguity, and Hierarchy","abstract":"A common method for delineating urban and suburban boundaries is to identify clusters of spatial units that are highly interconnected in a network of commuting flows, each cluster signaling a cohesive economic submarket. It is critical that the clustering methods employed for this task are principled and free of unnecessary tunable parameters to avoid unwanted inductive biases while remaining scalable for high resolution mobility networks. Here we systematically assess the benefits and limitations of a wide array of Stochastic Block Models (SBMs)$\\unicode{x2014}$a family of principled, nonparametric models for identifying clusters in networks$\\unicode{x2014}$for delineating urban spatial boundaries with commuting data. We find that the data compression capability and relative performance of different SBM variants heavily depends on the spatial extent of the commuting network, its aggregation scale, and the method used for weighting network edges. We also construct a new measure to assess the degree to which community detection algorithms find spatially contiguous partitions, finding that traditional SBMs may produce substantial spatial discontiguities that make them challenging to use in general for urban boundary delineation. We propose a fast nonparametric regionalization algorithm that can alleviate this issue, achieving data compression close to that of unconstrained SBM models while ensuring spatial contiguity, benefiting from a deterministic optimization procedure, and being generalizable to a wide range of community detection objective functions.","sentences":["A common method for delineating urban and suburban boundaries is to identify clusters of spatial units that are highly interconnected in a network of commuting flows, each cluster signaling a cohesive economic submarket.","It is critical that the clustering methods employed for this task are principled and free of unnecessary tunable parameters to avoid unwanted inductive biases while remaining scalable for high resolution mobility networks.","Here we systematically assess the benefits and limitations of a wide array of Stochastic Block Models (SBMs)$\\unicode{x2014}$a family of principled, nonparametric models for identifying clusters in networks$\\unicode{x2014}$for delineating urban spatial boundaries with commuting data.","We find that the data compression capability and relative performance of different SBM variants heavily depends on the spatial extent of the commuting network, its aggregation scale, and the method used for weighting network edges.","We also construct a new measure to assess the degree to which community detection algorithms find spatially contiguous partitions, finding that traditional SBMs may produce substantial spatial discontiguities that make them challenging to use in general for urban boundary delineation.","We propose a fast nonparametric regionalization algorithm that can alleviate this issue, achieving data compression close to that of unconstrained SBM models while ensuring spatial contiguity, benefiting from a deterministic optimization procedure, and being generalizable to a wide range of community detection objective functions."],"url":"http://arxiv.org/abs/2405.04911v1","category":"physics.soc-ph"}
