{"created":"2024-02-02 18:57:24","title":"A Control Theoretical Approach to Mean Field Games and Associated Master Equations","abstract":"We prove the global-in-time well-posedness for a broad class of mean field game problems, which is beyond the special linear-quadratic setting, as long as the mean field sensitivity is not too large. Through the stochastic maximum principle, we adopt the FBSDE approach to investigate the unique existence of the corresponding equilibrium strategies. The corresponding FBSDEs are first solved locally in time, then by controlling the sensitivity of the backward solutions with respect to the initial condition via some suitable apriori estimates for the corresponding Jacobian flows, the global-in-time solution is warranted. Further analysis on these Jacobian flows will be discussed to establish the regularities, such as linear functional differentiability, of the respective value functions that leads to the ultimate classical well-posedness of the master equation on $\\mathbb{R}^d$. To the best of our knowledge, it is the first article to deal with the mean field game problem, as well as its associated master equation, with general cost functionals having quadratic growth under the small mean field effect. In this current approach, we directly impose the structural conditions on the cost functionals, rather than conditions on the Hamiltonian. The advantages of this are threefold: (i) compared with imposing conditions on Hamiltonian, the structural conditions imposed in this work are easily verified, and less demanding on the regularity requirements of the cost functionals while solving the master equation; (ii) the displacement monotonicity is basically just a direct consequence of small mean field effect in the structural conditions; and (iii) when the mean field effect is not that small, we can still provide an accurate lifespan for the local existence. The method in this work can be readily extended to the case with nonlinear drift and non-separable cost functionals.","sentences":["We prove the global-in-time well-posedness for a broad class of mean field game problems, which is beyond the special linear-quadratic setting, as long as the mean field sensitivity is not too large.","Through the stochastic maximum principle, we adopt the FBSDE approach to investigate the unique existence of the corresponding equilibrium strategies.","The corresponding FBSDEs are first solved locally in time, then by controlling the sensitivity of the backward solutions with respect to the initial condition via some suitable apriori estimates for the corresponding Jacobian flows, the global-in-time solution is warranted.","Further analysis on these Jacobian flows will be discussed to establish the regularities, such as linear functional differentiability, of the respective value functions that leads to the ultimate classical well-posedness of the master equation on $\\mathbb{R}^d$. To the best of our knowledge, it is the first article to deal with the mean field game problem, as well as its associated master equation, with general cost functionals having quadratic growth under the small mean field effect.","In this current approach, we directly impose the structural conditions on the cost functionals, rather than conditions on the Hamiltonian.","The advantages of this are threefold: (i) compared with imposing conditions on Hamiltonian, the structural conditions imposed in this work are easily verified, and less demanding on the regularity requirements of the cost functionals while solving the master equation; (ii) the displacement monotonicity is basically just a direct consequence of small mean field effect in the structural conditions; and (iii) when the mean field effect is not that small, we can still provide an accurate lifespan for the local existence.","The method in this work can be readily extended to the case with nonlinear drift and non-separable cost functionals."],"url":"http://arxiv.org/abs/2402.01639v1","category":"math.OC"}
{"created":"2024-02-02 18:57:14","title":"Free Quantum Codes from Twisted Unitary $t$-groups","abstract":"We introduce twisted unitary $t$-groups, a generalization of unitary $t$-groups under a twisting by an irreducible representation. We then apply representation theoretic methods to the Knill-Laflamme error correction conditions to show that twisted unitary $t$-groups automatically correspond to quantum codes with distance $d=t+1$. By construction these codes have many transversal gates, which are naturally fault tolerant.","sentences":["We introduce twisted unitary $t$-groups, a generalization of unitary $t$-groups under a twisting by an irreducible representation.","We then apply representation theoretic methods to the Knill-Laflamme error correction conditions to show that twisted unitary $t$-groups automatically correspond to quantum codes with distance $d=t+1$. By construction these codes have many transversal gates, which are naturally fault tolerant."],"url":"http://arxiv.org/abs/2402.01638v1","category":"quant-ph"}
{"created":"2024-02-02 18:54:09","title":"Tidal Forces in Majumdar-Papapetrou Spacetimes","abstract":"Tidal disruption events occur when astrophysical objects are destroyed by black holes due to strong tidal force effects. Tidal forces have been studied in a variety of black hole spacetimes, including Reissner-Nordstr\\\"om and Kerr spacetimes. Despite the vast literature on the subject, tidal forces around black holes in static equilibrium have never been investigated before. The aim of this work is to fill in this gap and explore tidal forces in the Majumdar-Papapetrou spacetime describing two extremely charged binary black holes in equilibrium. We focus on tidal forces associated with radial and circular geodesics of massive neutral particles moving on the plane equidistant to the black holes. In particular, we study the behavior of the tidal forces as a function of the distance from the black holes and as a function of the energy of the geodesics. We also investigate the numerical solutions of the geodesic deviation equation for different initial conditions.","sentences":["Tidal disruption events occur when astrophysical objects are destroyed by black holes due to strong tidal force effects.","Tidal forces have been studied in a variety of black hole spacetimes, including Reissner-Nordstr\\\"om and Kerr spacetimes.","Despite the vast literature on the subject, tidal forces around black holes in static equilibrium have never been investigated before.","The aim of this work is to fill in this gap and explore tidal forces in the Majumdar-Papapetrou spacetime describing two extremely charged binary black holes in equilibrium.","We focus on tidal forces associated with radial and circular geodesics of massive neutral particles moving on the plane equidistant to the black holes.","In particular, we study the behavior of the tidal forces as a function of the distance from the black holes and as a function of the energy of the geodesics.","We also investigate the numerical solutions of the geodesic deviation equation for different initial conditions."],"url":"http://arxiv.org/abs/2402.01634v1","category":"gr-qc"}
{"created":"2024-02-02 18:50:25","title":"From gas to stars: MUSEings on the internal evolution of IC 1613","abstract":"The kinematics and chemical composition of stellar populations of different ages provide crucial information about the evolution of a galaxy. We aim to provide such information for IC 1613, an isolated, gas-rich, star-forming dwarf galaxy in the Local Group. We present here the results of a new spectroscopic study performed with MUSE, an integral-field spectrograph on the Very Large Telescope. We extracted from the data cubes more than 2000 sources from which we separated stellar objects for further spectroscopic analysis. The quality of the data set allowed us to obtain accurate classifications and line-of-sight velocities for about 800 stars. Our sample includes not only Red Giant Branch (RGB) and Main Sequence (MS) stars, but also a number of probable Be and C stars. We also obtained reliable metallicities for about 300 RGB stars. The kinematic analysis revealed for the first time the presence of stellar rotation with high significance. We found a general agreement with the velocity field of the neutral gas component, although the stars showed on average a larger velocity dispersion and slower rotation due to the asymmetric drift. When examining the kinematics of the different stellar components, MS stars appear to closely follow that of the gas, and the velocity dispersion seems to increase towards older stars. Chemical analysis of the RGB stars revealed mean properties comparable to those of other Local Group dwarf galaxies. Our work represents a step forward in understanding the internal processes governing the dynamical evolution of a low-mass galaxy.","sentences":["The kinematics and chemical composition of stellar populations of different ages provide crucial information about the evolution of a galaxy.","We aim to provide such information for IC 1613, an isolated, gas-rich, star-forming dwarf galaxy in the Local Group.","We present here the results of a new spectroscopic study performed with MUSE, an integral-field spectrograph on the Very Large Telescope.","We extracted from the data cubes more than 2000 sources from which we separated stellar objects for further spectroscopic analysis.","The quality of the data set allowed us to obtain accurate classifications and line-of-sight velocities for about 800 stars.","Our sample includes not only Red Giant Branch (RGB) and Main Sequence (MS) stars, but also a number of probable Be and C stars.","We also obtained reliable metallicities for about 300 RGB stars.","The kinematic analysis revealed for the first time the presence of stellar rotation with high significance.","We found a general agreement with the velocity field of the neutral gas component, although the stars showed on average a larger velocity dispersion and slower rotation due to the asymmetric drift.","When examining the kinematics of the different stellar components, MS stars appear to closely follow that of the gas, and the velocity dispersion seems to increase towards older stars.","Chemical analysis of the RGB stars revealed mean properties comparable to those of other Local Group dwarf galaxies.","Our work represents a step forward in understanding the internal processes governing the dynamical evolution of a low-mass galaxy."],"url":"http://arxiv.org/abs/2402.01631v1","category":"astro-ph.GA"}
{"created":"2024-02-02 18:44:37","title":"Position Paper: Generalized grammar rules and structure-based generalization beyond classical equivariance for lexical tasks and transduction","abstract":"Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks. We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks. Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases. We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research.","sentences":["Compositional generalization is one of the main properties which differentiates lexical learning in humans from state-of-art neural networks.","We propose a general framework for building models that can generalize compositionally using the concept of Generalized Grammar Rules (GGRs), a class of symmetry-based compositional constraints for transduction tasks, which we view as a transduction analogue of equivariance constraints in physics-inspired tasks.","Besides formalizing generalized notions of symmetry for language transduction, our framework is general enough to contain many existing works as special cases.","We present ideas on how GGRs might be implemented, and in the process draw connections to reinforcement learning and other areas of research."],"url":"http://arxiv.org/abs/2402.01629v1","category":"cs.CL"}
{"created":"2024-02-02 18:39:51","title":"TravelPlanner: A Benchmark for Real-World Planning with Language Agents","abstract":"Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.","sentences":["Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking.","Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning.","Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents?","To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario.","It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans.","Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%.","Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints.","However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress.","TravelPlanner provides a challenging yet meaningful testbed for future language agents."],"url":"http://arxiv.org/abs/2402.01622v1","category":"cs.CL"}
{"created":"2024-02-02 18:39:40","title":"Stochastic Two Points Method for Deep Model Zeroth-order Optimization","abstract":"Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms standard methods across various model types and scales, with 2 $\\times$ speed-up in training over most conducted tasks.","sentences":["Large foundation models, such as large language models, have performed exceptionally well in various application scenarios.","Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation.","The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model.","This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime.","We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions.","The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training.","Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms standard methods across various model types and scales, with 2 $\\times$ speed-up in training over most conducted tasks."],"url":"http://arxiv.org/abs/2402.01621v1","category":"cs.LG"}
{"created":"2024-02-02 18:35:14","title":"MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models","abstract":"Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency - an inference technique that relies on model diversity.","sentences":["Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks.","However, these involve long generations from multiple models across several rounds, making them expensive.","Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference.","To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs.","MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure.","Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers.","Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers.","We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency - an inference technique that relies on model diversity."],"url":"http://arxiv.org/abs/2402.01620v1","category":"cs.CL"}
{"created":"2024-02-02 18:31:15","title":"Style Vectors for Steering Generative Large Language Model","abstract":"This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.","sentences":["This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation.","We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches.","Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering.","The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems."],"url":"http://arxiv.org/abs/2402.01618v1","category":"cs.CL"}
{"created":"2024-02-02 18:27:21","title":"A GP-based Robust Motion Planning Framework for Agile Autonomous Robot Navigation and Recovery in Unknown Environments","abstract":"For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions. In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures. To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure. When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal. The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms. Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion.","sentences":["For autonomous mobile robots, uncertainties in the environment and system model can lead to failure in the motion planning pipeline, resulting in potential collisions.","In order to achieve a high level of robust autonomy, these robots should be able to proactively predict and recover from such failures.","To this end, we propose a Gaussian Process (GP) based model for proactively detecting the risk of future motion planning failure.","When this risk exceeds a certain threshold, a recovery behavior is triggered that leverages the same GP model to find a safe state from which the robot may continue towards the goal.","The proposed approach is trained in simulation only and can generalize to real world environments on different robotic platforms.","Simulations and physical experiments demonstrate that our framework is capable of both predicting planner failures and recovering the robot to states where planner success is likely, all while producing agile motion."],"url":"http://arxiv.org/abs/2402.01617v1","category":"cs.RO"}
{"created":"2024-02-02 18:26:09","title":"Computation of Quark Masses from String Theory","abstract":"We present a numerical computation, based on neural network techniques, of the physical Yukawa couplings in a heterotic string theory compactification on a smooth Calabi-Yau threefold with non-standard embedding. The model belongs to a large class of heterotic line bundle models that have previously been identified and whose low-energy spectrum precisely matches that of the MSSM plus fields uncharged under the Standard Model group. The relevant quantities for the calculation, that is, the Ricci-flat Calabi-Yau metric, the Hermitian Yang-Mills bundle metrics and the harmonic bundle-valued forms, are all computed by training suitable neural networks. For illustration, we consider a one-parameter family in complex structure moduli space. The computation at each point along this locus takes about half a day on a single twelve-core CPU. Our results for the Yukawa couplings are estimated to be within 10\\% of the expected analytic result. We find that the effect of the matter field normalisation can be significant and can contribute towards generating hierarchical couplings. We also demonstrate that a zeroth order, semi-analytic calculation, based on the Fubini-Study metric and its counterparts for the bundle metric and the bundle-valued forms, leads to roughly correct results, about 25\\% away from the numerical ones. The method can be applied to other heterotic line bundle models and generalised to other constructions, including to F-theory models.","sentences":["We present a numerical computation, based on neural network techniques, of the physical Yukawa couplings in a heterotic string theory compactification on a smooth Calabi-Yau threefold with non-standard embedding.","The model belongs to a large class of heterotic line bundle models that have previously been identified and whose low-energy spectrum precisely matches that of the MSSM plus fields uncharged under the Standard Model group.","The relevant quantities for the calculation, that is, the Ricci-flat Calabi-Yau metric, the Hermitian Yang-Mills bundle metrics and the harmonic bundle-valued forms, are all computed by training suitable neural networks.","For illustration, we consider a one-parameter family in complex structure moduli space.","The computation at each point along this locus takes about half a day on a single twelve-core CPU.","Our results for the Yukawa couplings are estimated to be within 10\\% of the expected analytic result.","We find that the effect of the matter field normalisation can be significant and can contribute towards generating hierarchical couplings.","We also demonstrate that a zeroth order, semi-analytic calculation, based on the Fubini-Study metric and its counterparts for the bundle metric and the bundle-valued forms, leads to roughly correct results, about 25\\% away from the numerical ones.","The method can be applied to other heterotic line bundle models and generalised to other constructions, including to F-theory models."],"url":"http://arxiv.org/abs/2402.01615v1","category":"hep-th"}
{"created":"2024-02-02 18:24:37","title":"L2G2G: a Scalable Local-to-Global Network Embedding with Graph Autoencoders","abstract":"For analysing real-world networks, graph representation learning is a popular tool. These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction. While GAEs tend to be fairly accurate, they suffer from scalability issues. For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy. Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability. This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs. It also benefits from the decoder computing an only local patch loss. Hence, aligning the local embeddings in each epoch utilises more information from the graph than a single post-training alignment does, while maintaining scalability. We illustrate on synthetic benchmarks, as well as real-world examples, that L2G2G achieves higher accuracy than the standard Local2Global approach and scales efficiently on the larger data sets. We find that for large and dense networks, it even outperforms the slow, but assumed more accurate, GAEs.","sentences":["For analysing real-world networks, graph representation learning is a popular tool.","These methods, such as a graph autoencoder (GAE), typically rely on low-dimensional representations, also called embeddings, which are obtained through minimising a loss function; these embeddings are used with a decoder for downstream tasks such as node classification and edge prediction.","While GAEs tend to be fairly accurate, they suffer from scalability issues.","For improved speed, a Local2Global approach, which combines graph patch embeddings based on eigenvector synchronisation, was shown to be fast and achieve good accuracy.","Here we propose L2G2G, a Local2Global method which improves GAE accuracy without sacrificing scalability.","This improvement is achieved by dynamically synchronising the latent node representations, while training the GAEs.","It also benefits from the decoder computing an only local patch loss.","Hence, aligning the local embeddings in each epoch utilises more information from the graph than a single post-training alignment does, while maintaining scalability.","We illustrate on synthetic benchmarks, as well as real-world examples, that L2G2G achieves higher accuracy than the standard Local2Global approach and scales efficiently on the larger data sets.","We find that for large and dense networks, it even outperforms the slow, but assumed more accurate, GAEs."],"url":"http://arxiv.org/abs/2402.01614v1","category":"cs.LG"}
{"created":"2024-02-02 18:23:18","title":"Nomic Embed: Training a Reproducible Long Context Text Embedder","abstract":"This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks. We release the training code and model weights under an Apache 2 license. In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1. You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors","sentences":["This technical report describes the training of nomic-embed-text-v1, the first fully reproducible, open-source, open-weights, open-data, 8192 context length English text embedding model that outperforms both OpenAI Ada-002 and OpenAI text-embedding-3-small on short and long-context tasks.","We release the training code and model weights under an Apache 2 license.","In contrast with other open-source models, we release a training data loader with 235 million curated text pairs that allows for the full replication of nomic-embed-text-v1.","You can find code and data to replicate the model at https://github.com/nomic-ai/contrastors"],"url":"http://arxiv.org/abs/2402.01613v1","category":"cs.CL"}
{"created":"2024-02-02 18:14:33","title":"Runtime phylogenetic analysis enables extreme subsampling for test-based problems","abstract":"A phylogeny describes the evolutionary history of an evolving population. Evolutionary search algorithms can perfectly track the ancestry of candidate solutions, illuminating a population's trajectory through the search space. However, phylogenetic analyses are typically limited to post-hoc studies of search performance. We introduce phylogeny-informed subsampling, a new class of subsampling methods that exploit runtime phylogenetic analyses for solving test-based problems. Specifically, we assess two phylogeny-informed subsampling methods -- individualized random subsampling and ancestor-based subsampling -- on three diagnostic problems and ten genetic programming (GP) problems from program synthesis benchmark suites. Overall, we found that phylogeny-informed subsampling methods enable problem-solving success at extreme subsampling levels where other subsampling methods fail. For example, phylogeny-informed subsampling methods more reliably solved program synthesis problems when evaluating just one training case per-individual, per-generation. However, at moderate subsampling levels, phylogeny-informed subsampling generally performed no better than random subsampling on GP problems. Our diagnostic experiments show that phylogeny-informed subsampling improves diversity maintenance relative to random subsampling, but its effects on a selection scheme's capacity to rapidly exploit fitness gradients varied by selection scheme. Continued refinements of phylogeny-informed subsampling techniques offer a promising new direction for scaling up evolutionary systems to handle problems with many expensive-to-evaluate fitness criteria.","sentences":["A phylogeny describes the evolutionary history of an evolving population.","Evolutionary search algorithms can perfectly track the ancestry of candidate solutions, illuminating a population's trajectory through the search space.","However, phylogenetic analyses are typically limited to post-hoc studies of search performance.","We introduce phylogeny-informed subsampling, a new class of subsampling methods that exploit runtime phylogenetic analyses for solving test-based problems.","Specifically, we assess two phylogeny-informed subsampling methods -- individualized random subsampling and ancestor-based subsampling -- on three diagnostic problems and ten genetic programming (GP) problems from program synthesis benchmark suites.","Overall, we found that phylogeny-informed subsampling methods enable problem-solving success at extreme subsampling levels where other subsampling methods fail.","For example, phylogeny-informed subsampling methods more reliably solved program synthesis problems when evaluating just one training case per-individual, per-generation.","However, at moderate subsampling levels, phylogeny-informed subsampling generally performed no better than random subsampling on GP problems.","Our diagnostic experiments show that phylogeny-informed subsampling improves diversity maintenance relative to random subsampling, but its effects on a selection scheme's capacity to rapidly exploit fitness gradients varied by selection scheme.","Continued refinements of phylogeny-informed subsampling techniques offer a promising new direction for scaling up evolutionary systems to handle problems with many expensive-to-evaluate fitness criteria."],"url":"http://arxiv.org/abs/2402.01610v1","category":"cs.NE"}
{"created":"2024-02-02 18:11:43","title":"Natural Counterfactuals With Necessary Backtracking","abstract":"Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.","sentences":["Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions.","While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible.","In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution.","Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios.","To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion.","Empirical experiments indicate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.01607v1","category":"cs.AI"}
{"created":"2024-02-02 18:05:38","title":"Ergodic and chaotic properties of some biological models","abstract":"In this note we present two types of biological models which have interesting ergodic and chaotic properties. The first type are one-dimensional transformations, like a logistic map, which are used to describe the change in population size in successive generations. We study ergodic properties of such transformations using Frobenius--Perron operators. The second type are some structured populations models, for example a space-structured model, or a model of maturity-distribution of precursors of blood cells. These models are described by partial differential equations, which generate semiflows on the space of functions. We construct strong mixing invariant measures for these semiflows using stochastic precesses. From properties of invariant measures we deduce some chaotic properties of semiflows such as the existence of dense trajectories and strong instability of all trajectories.","sentences":["In this note we present two types of biological models which have interesting ergodic and chaotic properties.","The first type are one-dimensional transformations, like a logistic map, which are used to describe the change in population size in successive generations.","We study ergodic properties of such transformations using Frobenius--Perron operators.","The second type are some structured populations models, for example a space-structured model, or a model of maturity-distribution of precursors of blood cells.","These models are described by partial differential equations, which generate semiflows on the space of functions.","We construct strong mixing invariant measures for these semiflows using stochastic precesses.","From properties of invariant measures we deduce some chaotic properties of semiflows such as the existence of dense trajectories and strong instability of all trajectories."],"url":"http://arxiv.org/abs/2402.01603v1","category":"math.DS"}
{"created":"2024-02-02 18:00:35","title":"Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning","abstract":"Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles. The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems.","sentences":["Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks.","However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability.","Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand.","In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning.","Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output.","We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles.","The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems."],"url":"http://arxiv.org/abs/2402.01602v1","category":"cs.AI"}
{"created":"2024-02-02 17:58:28","title":"Groups with presentations in EDT0L","abstract":"To any class of languages LAN, let us associate the class, denoted $\\pi(\\text{LAN})$, of finitely generated groups that admit a group presentation whose set of relators forms a language in LAN. We show that the class of L-presented groups, as introduced by the first author in 2003, is exactly the class of groups that admit presentations in the class of languages EDT0L. We show that the marked isomorphism problem is not semi-decidable for groups given by EDT0L presentations, contrary to the finite presentation case. We then extend and unify results of the first author with Eick and Hartung about nilpotent and finite quotients, by showing that it is possible to compute the marked hyperbolic and marked metabelian quotients of a group given by an EDT0L presentation. Finally, we show how the results about quotient computations enable the construction of recursively presented groups that do not have EDT0L presentations, thus proving $\\pi(\\text{EDT0L})\\ne \\pi(\\text{REC})$. This is done by building a residually nilpotent group with solvable word problem whose sequence of maximal nilpotent quotients is non-computable.","sentences":["To any class of languages LAN, let us associate the class, denoted $\\pi(\\text{LAN})$, of finitely generated groups that admit a group presentation whose set of relators forms a language in LAN.","We show that the class of L-presented groups, as introduced by the first author in 2003, is exactly the class of groups that admit presentations in the class of languages","EDT0L.","We show that the marked isomorphism problem is not semi-decidable for groups given by EDT0L presentations, contrary to the finite presentation case.","We then extend and unify results of the first author with Eick and Hartung about nilpotent and finite quotients, by showing that it is possible to compute the marked hyperbolic and marked metabelian quotients of a group given by an EDT0L presentation.","Finally, we show how the results about quotient computations enable the construction of recursively presented groups that do not have EDT0L presentations, thus proving $\\pi(\\text{EDT0L})\\ne \\pi(\\text{REC})$.","This is done by building a residually nilpotent group with solvable word problem whose sequence of maximal nilpotent quotients is non-computable."],"url":"http://arxiv.org/abs/2402.01601v1","category":"math.GR"}
{"created":"2024-02-02 17:36:34","title":"Statistical Accuracy of Approximate Filtering Methods","abstract":"Estimating the statistics of the state of a dynamical system, from partial and noisy observations, is both mathematically challenging and finds wide application. Furthermore, the applications are of great societal importance, including problems such as probabilistic weather forecasting and prediction of epidemics. Particle filters provide a well-founded approach to the problem, leading to provably accurate approximations of the statistics. However these methods perform poorly in high dimensions. In 1994 the idea of ensemble Kalman filtering was introduced by Evensen, leading to a methodology that has been widely adopted in the geophysical sciences and also finds application to quite general inverse problems. However, ensemble Kalman filters have defied rigorous analysis of their statistical accuracy, except in the linear Gaussian setting. In this article we describe recent work which takes first steps to analyze the statistical accuracy of ensemble Kalman filters beyond the linear Gaussian setting. The subject is inherently technical, as it involves the evolution of probability measures according to a nonlinear and nonautonomous dynamical system; and the approximation of this evolution. It can nonetheless be presented in a fairly accessible fashion, understandable with basic knowledge of dynamical systems, numerical analysis and probability.","sentences":["Estimating the statistics of the state of a dynamical system, from partial and noisy observations, is both mathematically challenging and finds wide application.","Furthermore, the applications are of great societal importance, including problems such as probabilistic weather forecasting and prediction of epidemics.","Particle filters provide a well-founded approach to the problem, leading to provably accurate approximations of the statistics.","However these methods perform poorly in high dimensions.","In 1994 the idea of ensemble Kalman filtering was introduced by Evensen, leading to a methodology that has been widely adopted in the geophysical sciences and also finds application to quite general inverse problems.","However, ensemble Kalman filters have defied rigorous analysis of their statistical accuracy, except in the linear Gaussian setting.","In this article we describe recent work which takes first steps to analyze the statistical accuracy of ensemble Kalman filters beyond the linear Gaussian setting.","The subject is inherently technical, as it involves the evolution of probability measures according to a nonlinear and nonautonomous dynamical system; and the approximation of this evolution.","It can nonetheless be presented in a fairly accessible fashion, understandable with basic knowledge of dynamical systems, numerical analysis and probability."],"url":"http://arxiv.org/abs/2402.01593v1","category":"math.NA"}
{"created":"2024-02-02 17:34:53","title":"BAT: Learning to Reason about Spatial Sounds with Large Language Models","abstract":"Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.","sentences":["Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound.","In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability.","To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0.","Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning.","The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation.","By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment.","Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments."],"url":"http://arxiv.org/abs/2402.01591v1","category":"eess.AS"}
{"created":"2024-02-02 17:34:25","title":"NeuroCine: Decoding Vivid Video Sequences from Human Brain Activties","abstract":"In the pursuit to understand the intricacies of human brain's visual processing, reconstructing dynamic visual experiences from brain activities emerges as a challenging yet fascinating endeavor. While recent advancements have achieved success in reconstructing static images from non-invasive brain recordings, the domain of translating continuous brain activities into video format remains underexplored. In this work, we introduce NeuroCine, a novel dual-phase framework to targeting the inherent challenges of decoding fMRI data, such as noises, spatial redundancy and temporal lags. This framework proposes spatial masking and temporal interpolation-based augmentation for contrastive learning fMRI representations and a diffusion model enhanced by dependent prior noise for video generation. Tested on a publicly available fMRI dataset, our method shows promising results, outperforming the previous state-of-the-art models by a notable margin of ${20.97\\%}$, ${31.00\\%}$ and ${12.30\\%}$ respectively on decoding the brain activities of three subjects in the fMRI dataset, as measured by SSIM. Additionally, our attention analysis suggests that the model aligns with existing brain structures and functions, indicating its biological plausibility and interpretability.","sentences":["In the pursuit to understand the intricacies of human brain's visual processing, reconstructing dynamic visual experiences from brain activities emerges as a challenging yet fascinating endeavor.","While recent advancements have achieved success in reconstructing static images from non-invasive brain recordings, the domain of translating continuous brain activities into video format remains underexplored.","In this work, we introduce NeuroCine, a novel dual-phase framework to targeting the inherent challenges of decoding fMRI data, such as noises, spatial redundancy and temporal lags.","This framework proposes spatial masking and temporal interpolation-based augmentation for contrastive learning fMRI representations and a diffusion model enhanced by dependent prior noise for video generation.","Tested on a publicly available fMRI dataset, our method shows promising results, outperforming the previous state-of-the-art models by a notable margin of ${20.97\\%}$, ${31.00\\%}$ and ${12.30\\%}$ respectively on decoding the brain activities of three subjects in the fMRI dataset, as measured by SSIM.","Additionally, our attention analysis suggests that the model aligns with existing brain structures and functions, indicating its biological plausibility and interpretability."],"url":"http://arxiv.org/abs/2402.01590v1","category":"cs.CV"}
{"created":"2024-02-02 17:26:23","title":"TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution","abstract":"The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.","sentences":["The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area.","As agents can directly interact with the physical environment, their reliability and safety is critical.","This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents.","This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection.","Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers.","Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficacy as a safe agent.","This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments.","Data and code are available at https://github.com/agiresearch/TrustAgent."],"url":"http://arxiv.org/abs/2402.01586v1","category":"cs.CL"}
{"created":"2024-02-02 17:20:16","title":"Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study","abstract":"We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes. We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it comparably effective to sound laws from expert annotation. Our code is publicly available at https://github.com/cmu-llab/aiscp.","sentences":["We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes.","We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm.","In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines.","We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction.","We also experiment with a minimal generalization learner for automatic sound law induction, finding it comparably effective to sound laws from expert annotation.","Our code is publicly available at https://github.com/cmu-llab/aiscp."],"url":"http://arxiv.org/abs/2402.01582v1","category":"cs.CL"}
{"created":"2024-02-02 17:19:20","title":"Generative AI for Education (GAIED): Advances, Opportunities, and Challenges","abstract":"This survey article has grown out of the GAIED (pronounced \"guide\") workshop organized by the authors at the NeurIPS 2023 conference. We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education. This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED.","sentences":["This survey article has grown out of the GAIED (pronounced \"guide\") workshop organized by the authors at the NeurIPS 2023 conference.","We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education.","This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED."],"url":"http://arxiv.org/abs/2402.01580v1","category":"cs.CY"}
{"created":"2024-02-02 17:16:07","title":"Training Adversarial yet Safe Agent to Characterize Safety Performance of Highly Automated Vehicles","abstract":"This paper focuses on safety performance testing and characterization of black-box highly automated vehicles (HAV). Existing testing approaches typically obtain the testing outcomes by deploying the HAV into a specific testing environment. Such a testing environment can involve various passively given testing strategies presented by other traffic participants such as (i) the naturalistic driving policy learned from human drivers, (ii) extracted concrete scenarios from real-world driving data, and (iii) model-based or data-driven adversarial testing methodologies focusing on forcing safety-critical events. The safety performance of HAV is further characterized by analyzing the obtained testing outcomes with a particular selected measure, such as the observed collision risk. The aforementioned testing practices suffer from the scarcity of safety-critical events, have limited operational design domain (ODD) coverage, or are biased toward long-tail unsafe cases. This paper presents a novel and informative testing strategy that differs from these existing practices. The proposal is inspired by the intuition that a relatively safer HAV driving policy would allow the traffic vehicles to exhibit a higher level of aggressiveness to achieve a certain fixed level of an overall safe outcome. One can specifically characterize such a HAV and traffic interactive strategy and use it as a safety performance indicator for the HAV. Under the proposed testing scheme, the HAV is evaluated under its full ODD with a reward function that represents a trade-off between safety and adversity in generating safety-critical events. The proposed methodology is demonstrated in simulation with various HAV designs under different operational design domains.","sentences":["This paper focuses on safety performance testing and characterization of black-box highly automated vehicles (HAV).","Existing testing approaches typically obtain the testing outcomes by deploying the HAV into a specific testing environment.","Such a testing environment can involve various passively given testing strategies presented by other traffic participants such as (i) the naturalistic driving policy learned from human drivers, (ii) extracted concrete scenarios from real-world driving data, and (iii) model-based or data-driven adversarial testing methodologies focusing on forcing safety-critical events.","The safety performance of HAV is further characterized by analyzing the obtained testing outcomes with a particular selected measure, such as the observed collision risk.","The aforementioned testing practices suffer from the scarcity of safety-critical events, have limited operational design domain (ODD) coverage, or are biased toward long-tail unsafe cases.","This paper presents a novel and informative testing strategy that differs from these existing practices.","The proposal is inspired by the intuition that a relatively safer HAV driving policy would allow the traffic vehicles to exhibit a higher level of aggressiveness to achieve a certain fixed level of an overall safe outcome.","One can specifically characterize such a HAV and traffic interactive strategy and use it as a safety performance indicator for the HAV.","Under the proposed testing scheme, the HAV is evaluated under its full ODD with a reward function that represents a trade-off between safety and adversity in generating safety-critical events.","The proposed methodology is demonstrated in simulation with various HAV designs under different operational design domains."],"url":"http://arxiv.org/abs/2402.01576v1","category":"cs.RO"}
{"created":"2024-02-02 17:13:03","title":"Efficient and Interaction-Aware Trajectory Planning for Autonomous Vehicles with Particle Swarm Optimization","abstract":"This paper introduces a novel numerical approach to achieving smooth lane-change trajectories in autonomous driving scenarios. Our trajectory generation approach leverages particle swarm optimization (PSO) techniques, incorporating Neural Network (NN) predictions for trajectory refinement. The generation of smooth and dynamically feasible trajectories for the lane change maneuver is facilitated by combining polynomial curve fitting with particle propagation, which can account for vehicle dynamics. The proposed planning algorithm is capable of determining feasible trajectories with real-time computation capability. We conduct comparative analyses with two baseline methods for lane changing, involving analytic solutions and heuristic techniques in numerical simulations. The simulation results validate the efficacy and effectiveness of our proposed approach.","sentences":["This paper introduces a novel numerical approach to achieving smooth lane-change trajectories in autonomous driving scenarios.","Our trajectory generation approach leverages particle swarm optimization (PSO) techniques, incorporating Neural Network (NN) predictions for trajectory refinement.","The generation of smooth and dynamically feasible trajectories for the lane change maneuver is facilitated by combining polynomial curve fitting with particle propagation, which can account for vehicle dynamics.","The proposed planning algorithm is capable of determining feasible trajectories with real-time computation capability.","We conduct comparative analyses with two baseline methods for lane changing, involving analytic solutions and heuristic techniques in numerical simulations.","The simulation results validate the efficacy and effectiveness of our proposed approach."],"url":"http://arxiv.org/abs/2402.01575v1","category":"cs.RO"}
{"created":"2024-02-02 17:12:28","title":"DRL-Based Dynamic Channel Access and SCLAR Maximization for Networks Under Jamming","abstract":"This paper investigates a deep reinforcement learning (DRL)-based approach for managing channel access in wireless networks. Specifically, we consider a scenario in which an intelligent user device (iUD) shares a time-varying uplink wireless channel with several fixed transmission schedule user devices (fUDs) and an unknown-schedule malicious jammer. The iUD aims to harmoniously coexist with the fUDs, avoid the jammer, and adaptively learn an optimal channel access strategy in the face of dynamic channel conditions, to maximize the network's sum cross-layer achievable rate (SCLAR). Through extensive simulations, we demonstrate that when we appropriately define the state space, action space, and rewards within the DRL framework, the iUD can effectively coexist with other UDs and optimize the network's SCLAR. We show that the proposed algorithm outperforms the tabular Q-learning and a fully connected deep neural network approach.","sentences":["This paper investigates a deep reinforcement learning (DRL)-based approach for managing channel access in wireless networks.","Specifically, we consider a scenario in which an intelligent user device (iUD) shares a time-varying uplink wireless channel with several fixed transmission schedule user devices (fUDs) and an unknown-schedule malicious jammer.","The iUD aims to harmoniously coexist with the fUDs, avoid the jammer, and adaptively learn an optimal channel access strategy in the face of dynamic channel conditions, to maximize the network's sum cross-layer achievable rate (SCLAR).","Through extensive simulations, we demonstrate that when we appropriately define the state space, action space, and rewards within the DRL framework, the iUD can effectively coexist with other UDs and optimize the network's SCLAR.","We show that the proposed algorithm outperforms the tabular Q-learning and a fully connected deep neural network approach."],"url":"http://arxiv.org/abs/2402.01574v1","category":"eess.SP"}
{"created":"2024-02-02 17:08:02","title":"Transformation semigroups and their applications","abstract":"In this chapter we present transformation semigroups and their applications. We begin with Klein's approach to geometry based on invariants of transformation groups. Then we present symmetry groups in chemistry and in classical mechanics. Next we introduce one-parameter semigroups of transformations and their applications in ergodic theory. Our main subject are one-parameter semigroups of operators, in particular stochastic semigroups. We present general results on their existence and long-time behaviour. We also give examples of one-parameter semigroups related to Markov chains, diffusion and processes with jumps. We focus on the applications of semigroups of operators in biology. Among other things, we study models of: DNA evolution; growth of erythrocyte population; gene expression; cell cycle; the movement of bacteria and insects. We also consider models with stochastic noise and different population models.","sentences":["In this chapter we present transformation semigroups and their applications.","We begin with Klein's approach to geometry based on invariants of transformation groups.","Then we present symmetry groups in chemistry and in classical mechanics.","Next we introduce one-parameter semigroups of transformations and their applications in ergodic theory.","Our main subject are one-parameter semigroups of operators, in particular stochastic semigroups.","We present general results on their existence and long-time behaviour.","We also give examples of one-parameter semigroups related to Markov chains, diffusion and processes with jumps.","We focus on the applications of semigroups of operators in biology.","Among other things, we study models of: DNA evolution; growth of erythrocyte population; gene expression; cell cycle; the movement of bacteria and insects.","We also consider models with stochastic noise and different population models."],"url":"http://arxiv.org/abs/2402.01572v1","category":"math.FA"}
{"created":"2024-02-02 16:59:48","title":"Boximator: Generating Rich and Controllable Motions for Video Synthesis","abstract":"Generating rich and controllable motion is a pivotal challenge in video synthesis. We propose Boximator, a new approach for fine-grained motion control. Boximator introduces two constraint types: hard box and soft box. Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames. Boximator functions as a plug-in for existing video diffusion models. Its training process preserves the base model's knowledge by freezing the original weights and training only the control module. To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations. Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints. Its robust motion controllability is validated by drastic increases in the bounding box alignment metric. Human evaluation also shows that users favor Boximator generation results over the base model.","sentences":["Generating rich and controllable motion is a pivotal challenge in video synthesis.","We propose Boximator, a new approach for fine-grained motion control.","Boximator introduces two constraint types: hard box and soft box.","Users select objects in the conditional frame using hard boxes and then use either type of boxes to roughly or rigorously define the object's position, shape, or motion path in future frames.","Boximator functions as a plug-in for existing video diffusion models.","Its training process preserves the base model's knowledge by freezing the original weights and training only the control module.","To address training challenges, we introduce a novel self-tracking technique that greatly simplifies the learning of box-object correlations.","Empirically, Boximator achieves state-of-the-art video quality (FVD) scores, improving on two base models, and further enhanced after incorporating box constraints.","Its robust motion controllability is validated by drastic increases in the bounding box alignment metric.","Human evaluation also shows that users favor Boximator generation results over the base model."],"url":"http://arxiv.org/abs/2402.01566v1","category":"cs.CV"}
{"created":"2024-02-02 16:54:25","title":"Testing $\u03b1$-attractor quintessential inflation against CMB and low-redshift data","abstract":"Due to universality and attractor properties, $\\alpha$-attractor quintessential inflation establishes direct relations between inflationary observables such as the scalar tilt $n_s$ and the tensor-to-scalar ratio $r$, and late-time dark energy equation of state parameters $w_0$ and $w_a$. In this work, we examine three different physically motivated regimes, considering complete freedom in the parameter $\\alpha$, models inspired by supergravity where $\\alpha$ takes on values up to $\\alpha=7/3$, and Starobinsky inflation ($\\alpha=1$). We investigate the consistency and constraints imposed by Cosmic Microwave Background measurements from the Planck satellite, B-mode polarization data from the BICEP/Keck collaboration, and low-redshift observations. Additionally, we consider small-scale CMB measurements released by the Atacama Cosmology Telescope, which give results approaching the Harrison-Zel'dovich spectrum ($n_s \\approx 1$). Here $\\alpha$-attractors lead to an improved fit over $\\Lambda$CDM. For the large-scale CMB measurements, $\\alpha\\gtrsim2$ models can provide equally good fits as $\\Lambda$CDM.","sentences":["Due to universality and attractor properties, $\\alpha$-attractor quintessential inflation establishes direct relations between inflationary observables such as the scalar tilt $n_s$ and the tensor-to-scalar ratio $r$, and late-time dark energy equation of state parameters $w_0$ and $w_a$. In this work, we examine three different physically motivated regimes, considering complete freedom in the parameter $\\alpha$, models inspired by supergravity where $\\alpha$ takes on values up to $\\alpha=7/3$, and Starobinsky inflation ($\\alpha=1$).","We investigate the consistency and constraints imposed by Cosmic Microwave Background measurements from the Planck satellite, B-mode polarization data from the BICEP/Keck collaboration, and low-redshift observations.","Additionally, we consider small-scale CMB measurements released by the Atacama Cosmology Telescope, which give results approaching the Harrison-Zel'dovich spectrum ($n_s \\approx 1$).","Here $\\alpha$-attractors lead to an improved fit over $\\Lambda$CDM.","For the large-scale CMB measurements, $\\alpha\\gtrsim2$ models can provide equally good fits as $\\Lambda$CDM."],"url":"http://arxiv.org/abs/2402.01560v1","category":"astro-ph.CO"}
{"created":"2024-02-02 16:54:18","title":"Resolution dependence of most probable pathways with state-dependent diffusivity","abstract":"Recent experiments have probed the relative likelihoods of trajectories in stochastic systems by observing survival probabilities within a tube of radius $R$ in spacetime. We measure such probabilities here for a colloidal particle in a corrugated channel, corresponding to a bistable potential with state-dependent diffusivity. In contrast to previous findings for state-independent noise, we find that the most probable pathway changes qualitatively as the tube radius $R$ is altered. We explain this by computing the survival probabilities predicted by overdamped Langevin dynamics. At high enough resolution (small enough $R$), survival probabilities depend solely on diffusivity variations, independent of deterministic forces; finite $R$ corrections yield a generalization of the Onsager-Machlup action. As corollary, ratios of survival probabilities are singular as $R \\to 0$, but become regular, and described by the classical Onsager-Machlup action, only in the special case of state-independent noise.","sentences":["Recent experiments have probed the relative likelihoods of trajectories in stochastic systems by observing survival probabilities within a tube of radius $R$ in spacetime.","We measure such probabilities here for a colloidal particle in a corrugated channel, corresponding to a bistable potential with state-dependent diffusivity.","In contrast to previous findings for state-independent noise, we find that the most probable pathway changes qualitatively as the tube radius $R$ is altered.","We explain this by computing the survival probabilities predicted by overdamped Langevin dynamics.","At high enough resolution (small enough $R$), survival probabilities depend solely on diffusivity variations, independent of deterministic forces; finite $R$ corrections yield a generalization of the Onsager-Machlup action.","As corollary, ratios of survival probabilities are singular as $R \\to 0$, but become regular, and described by the classical Onsager-Machlup action, only in the special case of state-independent noise."],"url":"http://arxiv.org/abs/2402.01559v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-02 16:47:18","title":"SLYKLatent, a Learning Framework for Facial Features Estimation","abstract":"In this research, we present SLYKLatent, a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization. SLYKLatent utilizes Self-Supervised Learning for initial training with facial expression datasets, followed by refinement with a patch-based tri-branch network and an inverse explained variance-weighted training loss function. Our evaluation on benchmark datasets achieves an 8.7% improvement on Gaze360, rivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%, surpassing existing methods by significant margins. Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel components. This approach has strong potential in human-robot interaction.","sentences":["In this research, we present SLYKLatent, a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization.","SLYKLatent utilizes Self-Supervised Learning for initial training with facial expression datasets, followed by refinement with a patch-based tri-branch network and an inverse explained variance-weighted training loss function.","Our evaluation on benchmark datasets achieves an 8.7% improvement on Gaze360, rivals top MPIIFaceGaze results, and leads on a subset of ETH-XGaze by 13%, surpassing existing methods by significant margins.","Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively.","Ablation studies confirm the effectiveness of SLYKLatent's novel components.","This approach has strong potential in human-robot interaction."],"url":"http://arxiv.org/abs/2402.01555v1","category":"cs.CV"}
{"created":"2024-02-02 16:41:28","title":"Gravitational lensing of massive particles by a black-bounce-Schwarzschild black hole","abstract":"We investigate in detail the weak-field gravitational lensing of a relativistic neutral massive particle induced by a regular black-bounce-Schwarzschild black hole proposed by Simpson and Visser. Starting with the calculation of the gravitational deflection of the massive particle up to the third post-Minkowskian order, the Virbhadra-Ellis lens equation is solved perturbatively beyond the weak-deflection limit to achieve the expressions for the lensing observables of the primary and secondary images of a point-like particle source. The main observables contain not only the positions, the flux magnifications, and the gravitational time delays of the individual images, but also the positional relations, the magnification relations (including the total magnification), the magnification centroid, and the differential time delay. We then discuss the velocity-induced effects originated from the deviation of the particle's initial velocity from the speed of light on the black-bounce-Schwarzschild lensing observables of the images of a point-like light source, and the effects induced by the bounce parameter of the spacetime on the measurable image properties of Schwarzschild lensing of the massive particle. As an application of the results, we model the supermassive black hole in the Galactic Center (i.e., Sgr A$^{\\ast}$) as the lens, and focus on evaluating the possibilities to detect the new velocity-induced and bounce-induced effects on the practical lensing observables and analyzing the dependence of those effects on the parameters.","sentences":["We investigate in detail the weak-field gravitational lensing of a relativistic neutral massive particle induced by a regular black-bounce-Schwarzschild black hole proposed by Simpson and Visser.","Starting with the calculation of the gravitational deflection of the massive particle up to the third post-Minkowskian order, the Virbhadra-Ellis lens equation is solved perturbatively beyond the weak-deflection limit to achieve the expressions for the lensing observables of the primary and secondary images of a point-like particle source.","The main observables contain not only the positions, the flux magnifications, and the gravitational time delays of the individual images, but also the positional relations, the magnification relations (including the total magnification), the magnification centroid, and the differential time delay.","We then discuss the velocity-induced effects originated from the deviation of the particle's initial velocity from the speed of light on the black-bounce-Schwarzschild lensing observables of the images of a point-like light source, and the effects induced by the bounce parameter of the spacetime on the measurable image properties of Schwarzschild lensing of the massive particle.","As an application of the results, we model the supermassive black hole in the Galactic Center (i.e., Sgr A$^{\\ast}$) as the lens, and focus on evaluating the possibilities to detect the new velocity-induced and bounce-induced effects on the practical lensing observables and analyzing the dependence of those effects on the parameters."],"url":"http://arxiv.org/abs/2402.01548v1","category":"gr-qc"}
{"created":"2024-02-02 16:40:50","title":"Contingency Detection in Modern Power Systems: A Stochastic Hybrid System Method","abstract":"This paper introduces a new stochastic hybrid system (SHS) framework for contingency detection in modern power systems (MPS). The framework uses stochastic hybrid system representations in state space models to expand and facilitate capability of contingency detection. In typical microgrids (MGs), buses may contain various synchronous generators, renewable generators, controllable loads, battery systems, regular loads, etc. For development of SHS models in power systems, this paper introduces the concept of dynamic and non-dynamic buses. By converting a physical power grid into a virtual linearized state space model and representing contingencies as random switching of system structures and parameters, this paper formulates the contingency detection problem as a joint estimation problem of discrete event and continuous states in stochastic hybrid systems. This method offers unique advantages, including using common measurement signals on voltage and current synchrophasors to detect different types and locations of contingencies, avoiding expensive local direct fault measurements and detecting certain contingencies that cannot be directly measured. The method employs a small and suitably-designed probing signal to sustain the ability of persistent contingency detection. Joint estimation algorithms are presented with their proven convergence and reliability properties. Examples that use an IEEE 5-bus system demonstrate the main ideas and derivation steps. Simulation case studies on an IEEE 33-bus system are used for detecting transmission line faults and sensor interruptions.","sentences":["This paper introduces a new stochastic hybrid system (SHS) framework for contingency detection in modern power systems (MPS).","The framework uses stochastic hybrid system representations in state space models to expand and facilitate capability of contingency detection.","In typical microgrids (MGs), buses may contain various synchronous generators, renewable generators, controllable loads, battery systems, regular loads, etc.","For development of SHS models in power systems, this paper introduces the concept of dynamic and non-dynamic buses.","By converting a physical power grid into a virtual linearized state space model and representing contingencies as random switching of system structures and parameters, this paper formulates the contingency detection problem as a joint estimation problem of discrete event and continuous states in stochastic hybrid systems.","This method offers unique advantages, including using common measurement signals on voltage and current synchrophasors to detect different types and locations of contingencies, avoiding expensive local direct fault measurements and detecting certain contingencies that cannot be directly measured.","The method employs a small and suitably-designed probing signal to sustain the ability of persistent contingency detection.","Joint estimation algorithms are presented with their proven convergence and reliability properties.","Examples that use an IEEE 5-bus system demonstrate the main ideas and derivation steps.","Simulation case studies on an IEEE 33-bus system are used for detecting transmission line faults and sensor interruptions."],"url":"http://arxiv.org/abs/2402.01547v1","category":"eess.SY"}
{"created":"2024-02-02 16:39:08","title":"Privacy-Preserving Distributed Learning for Residential Short-Term Load Forecasting","abstract":"In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy. Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security. While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks. To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage. However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenarios where one or more sub-centers are unavailable. To address these challenges, we introduce a Markovian Switching-based distributed training framework, the convergence of which is substantiated through rigorous theoretical analysis. The Distributed Markovian Switching (DMS) topology shows strong robustness towards the poisoning attacks as well. Case studies employing real-world power system load data validate the efficacy of our proposed algorithm. It not only significantly minimizes communication complexity but also maintains accuracy levels comparable to traditional FL methods, thereby enhancing the scalability of our load forecasting algorithm.","sentences":["In the realm of power systems, the increasing involvement of residential users in load forecasting applications has heightened concerns about data privacy.","Specifically, the load data can inadvertently reveal the daily routines of residential users, thereby posing a risk to their property security.","While federated learning (FL) has been employed to safeguard user privacy by enabling model training without the exchange of raw data, these FL models have shown vulnerabilities to emerging attack techniques, such as Deep Leakage from Gradients and poisoning attacks.","To counteract these, we initially employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty computation cryptographic techniques to mitigate the risk of gradient leakage.","However, the introduction of SecAgg necessitates the deployment of additional sub-center servers for executing the multiparty computation protocol, thereby escalating computational complexity and reducing system robustness, especially in scenarios where one or more sub-centers are unavailable.","To address these challenges, we introduce a Markovian Switching-based distributed training framework, the convergence of which is substantiated through rigorous theoretical analysis.","The Distributed Markovian Switching (DMS) topology shows strong robustness towards the poisoning attacks as well.","Case studies employing real-world power system load data validate the efficacy of our proposed algorithm.","It not only significantly minimizes communication complexity but also maintains accuracy levels comparable to traditional FL methods, thereby enhancing the scalability of our load forecasting algorithm."],"url":"http://arxiv.org/abs/2402.01546v1","category":"cs.LG"}
{"created":"2024-02-02 16:38:34","title":"Pre-Flight Calibration of PIXL for X-ray Fluorescence Elemental Quantification","abstract":"The Planetary Instrument for X-ray Lithochemistry (PIXL) is a rasterable focused-beam X-ray fluorescence (XRF) spectrometer mounted on the arm of National Aeronautics and Space Administration's (NASA) Mars 2020 Perseverance rover. To ensure that PIXL would be capable of performing accurate in-flight compositional analysis of martian targets, in situ, an elemental calibration was performed pre-flight on the PIXL flight instrument in a simulated martian environment. The details of this calibration, and implications for measuring unknown materials on Mars are the subjects of this paper. The major goals of this calibration were both to align the spectrometer to perform accurate elemental analysis and, to derive a matrix of uncertainties that are applied to XRF measurements of all elements in unknown materials. A small set of pure element and pure compound targets and geologically relevant reference materials were measured with the flight hardware in a simulated martian environment. Elemental calibration and quantifications were carried out using PIXL's XRF quantification software (PIQUANT). Uncertainties generated were implemented into the PIQUANT software version employed by the PIXL's data visualization software (PIXLISE). We outline in this work, a list of factors that impact micro-XRF accuracy, the methodology and steps involved in the calibration, details on the fabrication of the uncertainty matrix, instructions on the use and interpretations of the uncertainties applied to unknowns and an assessment on the limitations and areas open to future improvement as part of subsequent calibration efforts.","sentences":["The Planetary Instrument for X-ray Lithochemistry (PIXL) is a rasterable focused-beam X-ray fluorescence (XRF) spectrometer mounted on the arm of National Aeronautics and Space Administration's (NASA)","Mars 2020 Perseverance rover.","To ensure that PIXL would be capable of performing accurate in-flight compositional analysis of martian targets, in situ, an elemental calibration was performed pre-flight on the PIXL flight instrument in a simulated martian environment.","The details of this calibration, and implications for measuring unknown materials on Mars are the subjects of this paper.","The major goals of this calibration were both to align the spectrometer to perform accurate elemental analysis and, to derive a matrix of uncertainties that are applied to XRF measurements of all elements in unknown materials.","A small set of pure element and pure compound targets and geologically relevant reference materials were measured with the flight hardware in a simulated martian environment.","Elemental calibration and quantifications were carried out using PIXL's XRF quantification software (PIQUANT).","Uncertainties generated were implemented into the PIQUANT software version employed by the PIXL's data visualization software (PIXLISE).","We outline in this work, a list of factors that impact micro-XRF accuracy, the methodology and steps involved in the calibration, details on the fabrication of the uncertainty matrix, instructions on the use and interpretations of the uncertainties applied to unknowns and an assessment on the limitations and areas open to future improvement as part of subsequent calibration efforts."],"url":"http://arxiv.org/abs/2402.01544v1","category":"physics.app-ph"}
{"created":"2024-02-02 16:35:51","title":"Adaptive Optimization for Prediction with Missing Data","abstract":"When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.","sentences":["When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions.","In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features.","We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially.","We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models.","In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy."],"url":"http://arxiv.org/abs/2402.01543v1","category":"cs.LG"}
{"created":"2024-02-02 16:35:02","title":"Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation","abstract":"In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy","sentences":["In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs.","Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation.","We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples.","Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy"],"url":"http://arxiv.org/abs/2402.01542v1","category":"physics.chem-ph"}
{"created":"2024-02-02 16:32:50","title":"The galactic bubbles of starburst galaxies The influence of galactic large-scale magnetic fields","abstract":"Context. The galactic winds of starburst galaxies (SBGs) give rise to remarkable structures on kiloparsec scales. However, the evolution and shape of these giant wind bubbles, as well as the properties of the shocks they develop, are not yet fully understood. Aims. We aim to understand what shapes the galactic winds of SBGs, with a particular focus on the role of large-scale magnetic fields in the dynamical evolution of galactic wind-inflated bubbles. In addition, we aim to explore where the conditions for efficient particle acceleration are met in these systems. Methods. We performed magnetohydrodynamic simulations with the AMRVAC code (Adaptive Mesh Refinement Versatile Advection Code) with various configurations of the galactic medium density profile and magnetization. Results. We observe that the large-scale magnetic field, in which galactic winds expand, can impact the structure and evolution of inflated bubbles. However, the typical structures observed in starburst galaxies, such as M82, cannot be solely explained by the magnetic field structures that have been considered. This highlights the importance of other factors, such as the galactic disk, in shaping the galactic bubble. Furthermore, in all the magnetized cases we investigated, the forward wave resulting from the expanding bubbles only results in compression waves, whereas the wind termination shock features high Mach numbers, making it a promising site for diffusive shock acceleration up to $\\sim 10^{2}$ PeV. The synthetic X-ray images generated from our models reveal an envelope surrounding the bubbles that extends up to 2 kpc, which could correspond to the polarized emission observed from planar geometry in M82, as well as a large structure inside the bubble corresponding to the shocked galactic wind.","sentences":["Context.","The galactic winds of starburst galaxies (SBGs) give rise to remarkable structures on kiloparsec scales.","However, the evolution and shape of these giant wind bubbles, as well as the properties of the shocks they develop, are not yet fully understood.","Aims.","We aim to understand what shapes the galactic winds of SBGs, with a particular focus on the role of large-scale magnetic fields in the dynamical evolution of galactic wind-inflated bubbles.","In addition, we aim to explore where the conditions for efficient particle acceleration are met in these systems.","Methods.","We performed magnetohydrodynamic simulations with the AMRVAC code (Adaptive Mesh Refinement Versatile Advection Code) with various configurations of the galactic medium density profile and magnetization.","Results.","We observe that the large-scale magnetic field, in which galactic winds expand, can impact the structure and evolution of inflated bubbles.","However, the typical structures observed in starburst galaxies, such as M82, cannot be solely explained by the magnetic field structures that have been considered.","This highlights the importance of other factors, such as the galactic disk, in shaping the galactic bubble.","Furthermore, in all the magnetized cases we investigated, the forward wave resulting from the expanding bubbles only results in compression waves, whereas the wind termination shock features high Mach numbers, making it a promising site for diffusive shock acceleration up to $\\sim 10^{2}$ PeV.","The synthetic X-ray images generated from our models reveal an envelope surrounding the bubbles that extends up to 2 kpc, which could correspond to the polarized emission observed from planar geometry in M82, as well as a large structure inside the bubble corresponding to the shocked galactic wind."],"url":"http://arxiv.org/abs/2402.01541v1","category":"astro-ph.GA"}
{"created":"2024-02-02 16:31:13","title":"Backward Responsibility in Transition Systems Using General Power Indices","abstract":"To improve reliability and the understanding of AI systems, there is increasing interest in the use of formal methods, e.g. model checking. Model checking tools produce a counterexample when a model does not satisfy a property. Understanding these counterexamples is critical for efficient debugging, as it allows the developer to focus on the parts of the program that caused the issue.   To this end, we present a new technique that ascribes a responsibility value to each state in a transition system that does not satisfy a given safety property. The value is higher if the non-deterministic choices in a state have more power to change the outcome, given the behaviour observed in the counterexample. For this, we employ a concept from cooperative game theory -- namely general power indices, such as the Shapley value -- to compute the responsibility of the states.   We present an optimistic and pessimistic version of responsibility that differ in how they treat the states that do not lie on the counterexample. We give a characterisation of optimistic responsibility that leads to an efficient algorithm for it and show computational hardness of the pessimistic version. We also present a tool to compute responsibility and show how a stochastic algorithm can be used to approximate responsibility in larger models. These methods can be deployed in the design phase, at runtime and at inspection time to gain insights on causal relations within the behavior of AI systems.","sentences":["To improve reliability and the understanding of AI systems, there is increasing interest in the use of formal methods, e.g. model checking.","Model checking tools produce a counterexample when a model does not satisfy a property.","Understanding these counterexamples is critical for efficient debugging, as it allows the developer to focus on the parts of the program that caused the issue.   ","To this end, we present a new technique that ascribes a responsibility value to each state in a transition system that does not satisfy a given safety property.","The value is higher if the non-deterministic choices in a state have more power to change the outcome, given the behaviour observed in the counterexample.","For this, we employ a concept from cooperative game theory -- namely general power indices, such as the Shapley value -- to compute the responsibility of the states.   ","We present an optimistic and pessimistic version of responsibility that differ in how they treat the states that do not lie on the counterexample.","We give a characterisation of optimistic responsibility that leads to an efficient algorithm for it and show computational hardness of the pessimistic version.","We also present a tool to compute responsibility and show how a stochastic algorithm can be used to approximate responsibility in larger models.","These methods can be deployed in the design phase, at runtime and at inspection time to gain insights on causal relations within the behavior of AI systems."],"url":"http://arxiv.org/abs/2402.01539v1","category":"cs.FL"}
{"created":"2024-02-02 16:27:45","title":"Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing Trimodal Data","abstract":"In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information. However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns. One possibility to overcome these vulnerabilities is to resort to different modalities. For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers. Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist. To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically. With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data utilizing conditional image-to-image translation. By employing this approach, we generate trimodal data that can be leveraged to train models for settings with limited data, bad lightning conditions, or privacy-sensitive areas.","sentences":["In pervasive machine learning, especially in Human Behavior Analysis (HBA), RGB has been the primary modality due to its accessibility and richness of information.","However, linked with its benefits are challenges, including sensitivity to lighting conditions and privacy concerns.","One possibility to overcome these vulnerabilities is to resort to different modalities.","For instance, thermal is particularly adept at accentuating human forms, while depth adds crucial contextual layers.","Despite their known benefits, only a few HBA-specific datasets that integrate these modalities exist.","To address this shortage, our research introduces a novel generative technique for creating trimodal, i.e., RGB, thermal, and depth, human-focused datasets.","This technique capitalizes on human segmentation masks derived from RGB images, combined with thermal and depth backgrounds that are sourced automatically.","With these two ingredients, we synthesize depth and thermal counterparts from existing RGB data utilizing conditional image-to-image translation.","By employing this approach, we generate trimodal data that can be leveraged to train models for settings with limited data, bad lightning conditions, or privacy-sensitive areas."],"url":"http://arxiv.org/abs/2402.01537v1","category":"cs.CV"}
{"created":"2024-02-02 16:27:11","title":"Homogenization Effects of Large Language Models on Human Creative Ideation","abstract":"Large language models (LLMs) are now being used in a wide variety of contexts, including as creativity support tools (CSTs) intended to help their users come up with new ideas. But do LLMs actually support user creativity? We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users. We conducted a 36-participant comparative user study and found, in accordance with the homogenization hypothesis, that different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST. Additionally, ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated. We discuss potential implications of these findings for users, designers, and developers of LLM-based CSTs.","sentences":["Large language models (LLMs) are now being used in a wide variety of contexts, including as creativity support tools (CSTs) intended to help their users come up with new ideas.","But do LLMs actually support user creativity?","We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users.","We conducted a 36-participant comparative user study and found, in accordance with the homogenization hypothesis, that different users tended to produce less semantically distinct ideas with ChatGPT than with an alternative CST.","Additionally, ChatGPT users generated a greater number of more detailed ideas, but felt less responsible for the ideas they generated.","We discuss potential implications of these findings for users, designers, and developers of LLM-based CSTs."],"url":"http://arxiv.org/abs/2402.01536v1","category":"cs.HC"}
{"created":"2024-02-02 16:26:52","title":"An Empirical Analysis of Diversity in Argument Summarization","abstract":"Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.","sentences":["Presenting high-level arguments is a crucial task for fostering participation in online societal discussions.","Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives.","We introduce three aspects of diversity: those of opinions, annotators, and sources.","We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations.","We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths.","Further, we observe that diversification of training data may ameliorate generalization.","Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity."],"url":"http://arxiv.org/abs/2402.01535v1","category":"cs.CL"}
{"created":"2024-02-02 16:16:54","title":"Realistic Bell tests with homodyne measurements","abstract":"We analyze Bell inequalities violations in photonic experiments for which the measurement apparatuses are restricted to homodyne measurements. Through numerical optimization of the Clauser-Horne-Shimony-Holt inequality over homodyne measurements and binning choices, we demonstrate large violations for states with a bounded number of photons. When considering states defined within qubit local subspaces of two Fock states, such as NOON states, a violation is observed solely within the qubit Fock space spanned by zero and two photons. For more generic states, large violations are obtained. Significant violations are observed even for states containing three photons locally and under realistic values of noise and losses. We propose concrete implementations to achieve such violations, opening new avenues for Bell experiments with homodyne detectors.","sentences":["We analyze Bell inequalities violations in photonic experiments for which the measurement apparatuses are restricted to homodyne measurements.","Through numerical optimization of the Clauser-Horne-Shimony-Holt inequality over homodyne measurements and binning choices, we demonstrate large violations for states with a bounded number of photons.","When considering states defined within qubit local subspaces of two Fock states, such as NOON states, a violation is observed solely within the qubit Fock space spanned by zero and two photons.","For more generic states, large violations are obtained.","Significant violations are observed even for states containing three photons locally and under realistic values of noise and losses.","We propose concrete implementations to achieve such violations, opening new avenues for Bell experiments with homodyne detectors."],"url":"http://arxiv.org/abs/2402.01530v1","category":"quant-ph"}
{"created":"2024-02-02 16:15:24","title":"Decoding Speculative Decoding","abstract":"Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be used to decide the right draft model for a given workload. Further, using our insights we design a new draft model for LLaMA-65B which can provide 30% higher throughput than existing draft models.","sentences":["Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome.","When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens.","The speedup provided by speculative decoding heavily depends on the choice of the draft model.","It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput.","However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases.","To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups.","Based on our experiments we describe an analytical model which can be used to decide the right draft model for a given workload.","Further, using our insights we design a new draft model for LLaMA-65B which can provide 30% higher throughput than existing draft models."],"url":"http://arxiv.org/abs/2402.01528v1","category":"cs.LG"}
{"created":"2024-02-02 16:10:29","title":"HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation","abstract":"Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images. However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent. To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference. The hypernetwork gathers information from the training data and generates an update for universal weights. As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step. This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study.","sentences":["Neural radiance fields (NeRFs) are a widely accepted standard for synthesizing new 3D object views from a small number of base images.","However, NeRFs have limited generalization properties, which means that we need to use significant computational resources to train individual architectures for each item we want to represent.","To address this issue, we propose a few-shot learning approach based on the hypernetwork paradigm that does not require gradient optimization during inference.","The hypernetwork gathers information from the training data and generates an update for universal weights.","As a result, we have developed an efficient method for generating a high-quality 3D object representation from a small number of images in a single step.","This has been confirmed by direct comparison with the state-of-the-art solutions and a comprehensive ablation study."],"url":"http://arxiv.org/abs/2402.01524v1","category":"cs.CV"}
{"created":"2024-02-02 16:07:05","title":"K-Level Reasoning with Large Language Models","abstract":"While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named \"K-Level Reasoning\". This approach adopts the perspective of rivals to recursively employ k-level thinking based on available historical information, which significantly improves the prediction accuracy of rivals' subsequent moves and informs more strategic decision-making. This research not only sets a robust quantitative benchmark for the assessment of dynamic reasoning but also markedly enhances the proficiency of LLMs in dynamic contexts.","sentences":["While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored.","To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments.","We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making.","These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities.","Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works.","To address this, we propose a novel reasoning approach for LLMs, named \"K-Level Reasoning\".","This approach adopts the perspective of rivals to recursively employ k-level thinking based on available historical information, which significantly improves the prediction accuracy of rivals' subsequent moves and informs more strategic decision-making.","This research not only sets a robust quantitative benchmark for the assessment of dynamic reasoning but also markedly enhances the proficiency of LLMs in dynamic contexts."],"url":"http://arxiv.org/abs/2402.01521v1","category":"cs.CL"}
{"created":"2024-02-02 16:05:19","title":"Blow-up estimates and a priori bounds for the positive solutions of a class of superlinear indefinite elliptic problems","abstract":"In this paper we find out some new blow-up estimates for the positive explosive solutions of a paradigmatic class of elliptic boundary value problems of superlinear indefinite type. These estimates are obtained by combining the scaling technique of Guidas-Spruck together with a generalized De Giorgi-Moser weak Harnack inequality found, very recently, by Sirakov. In a further step, based on a comparison result of Amann and L\\'opez-G\\'omez, we will show how these bounds provide us with some sharp a priori estimates for the classical positive solutions of a wide variety of superlinear indefinite problems. It turns out that this is the first general result where the decay rates of the potential in front of the nonlinearity $a(x)$ do not play any role for getting a priori bounds for the positive solutions when $N\\geq 3$.","sentences":["In this paper we find out some new blow-up estimates for the positive explosive solutions of a paradigmatic class of elliptic boundary value problems of superlinear indefinite type.","These estimates are obtained by combining the scaling technique of Guidas-Spruck together with a generalized De Giorgi-Moser weak Harnack inequality found, very recently, by Sirakov.","In a further step, based on a comparison result of Amann and L\\'opez-G\\'omez, we will show how these bounds provide us with some sharp a priori estimates for the classical positive solutions of a wide variety of superlinear indefinite problems.","It turns out that this is the first general result where the decay rates of the potential in front of the nonlinearity $a(x)$ do not play any role for getting a priori bounds for the positive solutions when $N\\geq 3$."],"url":"http://arxiv.org/abs/2402.01519v1","category":"math.AP"}
{"created":"2024-02-02 15:57:13","title":"Cross-view Masked Diffusion Transformers for Person Image Synthesis","abstract":"We present X-MDPT (Cross-view Masked Diffusion Prediction Transformers), a novel diffusion model designed for pose-guided human image generation. X-MDPT distinguishes itself by employing masked diffusion transformers that operate on latent patches, a departure from the commonly-used Unet structures in existing works. The model comprises three key modules: 1) a denoising diffusion Transformer, 2) an aggregation network that consolidates conditions into a single vector for the diffusion process, and 3) a mask cross-prediction module that enhances representation learning with semantic information from the reference image. X-MDPT demonstrates scalability, improving FID, SSIM, and LPIPS with larger models. Despite its simple design, our model outperforms state-of-the-art approaches on the DeepFashion dataset while exhibiting efficiency in terms of training parameters, training time, and inference speed. Our compact 33MB model achieves an FID of 7.42, surpassing a prior Unet latent diffusion approach (FID 8.07) using only $11\\times$ fewer parameters. Our best model surpasses the pixel-based diffusion with $\\frac{2}{3}$ of the parameters and achieves $5.43 \\times$ faster inference.","sentences":["We present X-MDPT (Cross-view Masked Diffusion Prediction Transformers), a novel diffusion model designed for pose-guided human image generation.","X-MDPT distinguishes itself by employing masked diffusion transformers that operate on latent patches, a departure from the commonly-used Unet structures in existing works.","The model comprises three key modules: 1) a denoising diffusion Transformer, 2) an aggregation network that consolidates conditions into a single vector for the diffusion process, and 3) a mask cross-prediction module that enhances representation learning with semantic information from the reference image.","X-MDPT demonstrates scalability, improving FID, SSIM, and LPIPS with larger models.","Despite its simple design, our model outperforms state-of-the-art approaches on the DeepFashion dataset while exhibiting efficiency in terms of training parameters, training time, and inference speed.","Our compact 33MB model achieves an FID of 7.42, surpassing a prior Unet latent diffusion approach (FID 8.07) using only $11\\times$ fewer parameters.","Our best model surpasses the pixel-based diffusion with $\\frac{2}{3}$ of the parameters and achieves $5.43 \\times$ faster inference."],"url":"http://arxiv.org/abs/2402.01516v1","category":"cs.CV"}
{"created":"2024-02-02 15:55:25","title":"Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence","abstract":"Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\\nabla f_t(x_t)$ and an additional acceleration term $\\frac{2|\\langle v_t, \\nabla f_t(x_t) \\rangle|}{\\|v_t\\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\\langle v_t, \\nabla f_t(x_t) \\rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \\textbf{Reject Accelerating} and \\textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.","sentences":["Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc.","However, their convergence analysis under non-convex conditions is challenging.","In this work, we propose a unified framework to address this issue.","For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\\nabla f_t(x_t)$ and an additional acceleration term $\\frac{2|\\langle v_t, \\nabla f_t(x_t) \\rangle|}{\\|v_t\\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\\langle v_t, \\nabla f_t(x_t)","\\rangle$.","Through our framework, we have discovered two plug-and-play acceleration methods: \\textbf{Reject Accelerating} and \\textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate."],"url":"http://arxiv.org/abs/2402.01515v1","category":"cs.LG"}
{"created":"2024-02-02 15:54:19","title":"Multilingual Gradient Word-Order Typology from Universal Dependencies","abstract":"While information from the field of linguistic typology has the potential to improve performance on NLP tasks, reliable typological data is a prerequisite. Existing typological databases, including WALS and Grambank, suffer from inconsistencies primarily caused by their categorical format. Furthermore, typological categorisations by definition differ significantly from the continuous nature of phenomena, as found in natural language corpora. In this paper, we introduce a new seed dataset made up of continuous-valued data, rather than categorical data, that can better reflect the variability of language. While this initial dataset focuses on word-order typology, we also present the methodology used to create the dataset, which can be easily adapted to generate data for a broader set of features and languages.","sentences":["While information from the field of linguistic typology has the potential to improve performance on NLP tasks, reliable typological data is a prerequisite.","Existing typological databases, including WALS and Grambank, suffer from inconsistencies primarily caused by their categorical format.","Furthermore, typological categorisations by definition differ significantly from the continuous nature of phenomena, as found in natural language corpora.","In this paper, we introduce a new seed dataset made up of continuous-valued data, rather than categorical data, that can better reflect the variability of language.","While this initial dataset focuses on word-order typology, we also present the methodology used to create the dataset, which can be easily adapted to generate data for a broader set of features and languages."],"url":"http://arxiv.org/abs/2402.01513v1","category":"cs.CL"}
{"created":"2024-02-02 15:53:31","title":"Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation","abstract":"Distractors are important in learning evaluation. This paper surveys distractor generation tasks using English multiple-choice question datasets for textual and multimodal contexts. In particular, this paper presents a thorough literature review of the recent studies on distractor generation tasks, discusses multiple choice components and their characteristics, analyzes the related datasets, and summarizes the evaluation metrics of distractor generation. Our investigation reveals that more than half of datasets are human-generated from educational sources in specific domains such as Science and English, which are largely text-based, with a lack of open domain and multimodal datasets.","sentences":["Distractors are important in learning evaluation.","This paper surveys distractor generation tasks using English multiple-choice question datasets for textual and multimodal contexts.","In particular, this paper presents a thorough literature review of the recent studies on distractor generation tasks, discusses multiple choice components and their characteristics, analyzes the related datasets, and summarizes the evaluation metrics of distractor generation.","Our investigation reveals that more than half of datasets are human-generated from educational sources in specific domains such as Science and English, which are largely text-based, with a lack of open domain and multimodal datasets."],"url":"http://arxiv.org/abs/2402.01512v1","category":"cs.CL"}
{"created":"2024-02-02 15:44:28","title":"A Hybrid Strategy for Chat Transcript Summarization","abstract":"Text summarization is the process of condensing a piece of text to fewer sentences, while still preserving its content. Chat transcript, in this context, is a textual copy of a digital or online conversation between a customer (caller) and agent(s). This paper presents an indigenously (locally) developed hybrid method that first combines extractive and abstractive summarization techniques in compressing ill-punctuated or un-punctuated chat transcripts to produce more readable punctuated summaries and then optimizes the overall quality of summarization through reinforcement learning. Extensive testing, evaluations, comparisons, and validation have demonstrated the efficacy of this approach for large-scale deployment of chat transcript summarization, in the absence of manually generated reference (annotated) summaries.","sentences":["Text summarization is the process of condensing a piece of text to fewer sentences, while still preserving its content.","Chat transcript, in this context, is a textual copy of a digital or online conversation between a customer (caller) and agent(s).","This paper presents an indigenously (locally) developed hybrid method that first combines extractive and abstractive summarization techniques in compressing ill-punctuated or un-punctuated chat transcripts to produce more readable punctuated summaries and then optimizes the overall quality of summarization through reinforcement learning.","Extensive testing, evaluations, comparisons, and validation have demonstrated the efficacy of this approach for large-scale deployment of chat transcript summarization, in the absence of manually generated reference (annotated) summaries."],"url":"http://arxiv.org/abs/2402.01510v1","category":"cs.CL"}
{"created":"2024-02-02 15:43:51","title":"Advancing Brain Tumor Inpainting with Generative Models","abstract":"Synthesizing healthy brain scans from diseased brain scans offers a potential solution to address the limitations of general-purpose algorithms, such as tissue segmentation and brain extraction algorithms, which may not effectively handle diseased images. We consider this a 3D inpainting task and investigate the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic resonance imaging(MRI) data. Our contributions encompass potential modifications tailored to MRI-specific needs, and we conducted evaluations of multiple inpainting techniques using the BraTS2023 Inpainting datasets to assess their efficacy and limitations.","sentences":["Synthesizing healthy brain scans from diseased brain scans offers a potential solution to address the limitations of general-purpose algorithms, such as tissue segmentation and brain extraction algorithms, which may not effectively handle diseased images.","We consider this a 3D inpainting task and investigate the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic resonance imaging(MRI) data.","Our contributions encompass potential modifications tailored to MRI-specific needs, and we conducted evaluations of multiple inpainting techniques using the BraTS2023 Inpainting datasets to assess their efficacy and limitations."],"url":"http://arxiv.org/abs/2402.01509v1","category":"eess.IV"}
{"created":"2024-02-02 15:41:27","title":"Deep learning path-like collective variable for enhanced sampling molecular dynamics","abstract":"Several enhanced sampling techniques rely on the definition of collective variables to effectively explore free energy landscapes. Existing variables that describe the progression along a reactive pathway offer an elegant solution but face a number of limitations. In this paper, we address these challenges by introducing a new path-like collective variable called the `Deep-locally-non-linear-embedding', which is inspired by principles of the locally linear embedding technique and is trained on a reactive trajectory. The variable mimics the ideal reaction coordinate by automatically generating a non-linear combination of features through a differentiable generalized autoencoder that combines a neural network with a continuous k-nearest-neighbor selection. Among the key advantages of this method is its capability to automatically choose the metric for searching neighbors and to learn the path from state A to state B without the need to handpick landmarks a priori. We demonstrate the effectiveness of DeepLNE by showing that the progression along the path variable closely approximates the ideal reaction coordinate in toy models such as the M\\\"uller-Brown-potential and alanine dipeptide. We then use it in molecular dynamics simulations of an RNA tetraloop, where we highlight its capability to accelerate transitions and converge the free energy of folding.","sentences":["Several enhanced sampling techniques rely on the definition of collective variables to effectively explore free energy landscapes.","Existing variables that describe the progression along a reactive pathway offer an elegant solution but face a number of limitations.","In this paper, we address these challenges by introducing a new path-like collective variable called the `Deep-locally-non-linear-embedding', which is inspired by principles of the locally linear embedding technique and is trained on a reactive trajectory.","The variable mimics the ideal reaction coordinate by automatically generating a non-linear combination of features through a differentiable generalized autoencoder that combines a neural network with a continuous k-nearest-neighbor selection.","Among the key advantages of this method is its capability to automatically choose the metric for searching neighbors and to learn the path from state A to state B without the need to handpick landmarks a priori.","We demonstrate the effectiveness of DeepLNE by showing that the progression along the path variable closely approximates the ideal reaction coordinate in toy models such as the M\\\"uller-Brown-potential and alanine dipeptide.","We then use it in molecular dynamics simulations of an RNA tetraloop, where we highlight its capability to accelerate transitions and converge the free energy of folding."],"url":"http://arxiv.org/abs/2402.01508v1","category":"physics.chem-ph"}
{"created":"2024-02-02 15:36:43","title":"Why do Random Forests Work? Understanding Tree Ensembles as Self-Regularizing Adaptive Smoothers","abstract":"Despite their remarkable effectiveness and broad application, the drivers of success underlying ensembles of trees are still not fully understood. In this paper, we highlight how interpreting tree ensembles as adaptive and self-regularizing smoothers can provide new intuition and deeper insight to this topic. We use this perspective to show that, when studied as smoothers, randomized tree ensembles not only make predictions that are quantifiably more smooth than the predictions of the individual trees they consist of, but also further regulate their smoothness at test-time based on the dissimilarity between testing and training inputs. First, we use this insight to revisit, refine and reconcile two recent explanations of forest success by providing a new way of quantifying the conjectured behaviors of tree ensembles objectively by measuring the effective degree of smoothing they imply. Then, we move beyond existing explanations for the mechanisms by which tree ensembles improve upon individual trees and challenge the popular wisdom that the superior performance of forests should be understood as a consequence of variance reduction alone. We argue that the current high-level dichotomy into bias- and variance-reduction prevalent in statistics is insufficient to understand tree ensembles -- because the prevailing definition of bias does not capture differences in the expressivity of the hypothesis classes formed by trees and forests. Instead, we show that forests can improve upon trees by three distinct mechanisms that are usually implicitly entangled. In particular, we demonstrate that the smoothing effect of ensembling can reduce variance in predictions due to noise in outcome generation, reduce variability in the quality of the learned function given fixed input data and reduce potential bias in learnable functions by enriching the available hypothesis space.","sentences":["Despite their remarkable effectiveness and broad application, the drivers of success underlying ensembles of trees are still not fully understood.","In this paper, we highlight how interpreting tree ensembles as adaptive and self-regularizing smoothers can provide new intuition and deeper insight to this topic.","We use this perspective to show that, when studied as smoothers, randomized tree ensembles not only make predictions that are quantifiably more smooth than the predictions of the individual trees they consist of, but also further regulate their smoothness at test-time based on the dissimilarity between testing and training inputs.","First, we use this insight to revisit, refine and reconcile two recent explanations of forest success by providing a new way of quantifying the conjectured behaviors of tree ensembles objectively by measuring the effective degree of smoothing they imply.","Then, we move beyond existing explanations for the mechanisms by which tree ensembles improve upon individual trees and challenge the popular wisdom that the superior performance of forests should be understood as a consequence of variance reduction alone.","We argue that the current high-level dichotomy into bias- and variance-reduction prevalent in statistics is insufficient to understand tree ensembles -- because the prevailing definition of bias does not capture differences in the expressivity of the hypothesis classes formed by trees and forests.","Instead, we show that forests can improve upon trees by three distinct mechanisms that are usually implicitly entangled.","In particular, we demonstrate that the smoothing effect of ensembling can reduce variance in predictions due to noise in outcome generation, reduce variability in the quality of the learned function given fixed input data and reduce potential bias in learnable functions by enriching the available hypothesis space."],"url":"http://arxiv.org/abs/2402.01502v1","category":"stat.ML"}
{"created":"2024-02-02 15:31:08","title":"Developing and Evaluating a Design Method for Positive Artificial Intelligence","abstract":"As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing \"AI for good\" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them moderately high for feasibility, desirability, and plausibility of achieving intended wellbeing benefits. Together, these studies provide preliminary validation of the method's ability to improve AI design, while surfacing areas needing refinement like developing support for complex steps. Proposed adaptations such as examples and evaluation heuristics could address weaknesses. Further research should examine sustained application over multiple projects. This human-centered approach shows promise for realizing the vision of 'AI for Wellbeing' that does not just avoid harm, but actively benefits humanity.","sentences":["As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life.","However, developing \"AI for good\" poses substantial challenges around aligning systems with complex human values.","Presently, we lack mature methods for addressing these challenges.","This article presents and evaluates the Positive AI design method aimed at addressing this gap.","The method provides a human-centered process to translate wellbeing aspirations into concrete practices.","First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles.","We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability.","Next, an expert evaluation study assessed the quality of the resulting concepts, rating them moderately high for feasibility, desirability, and plausibility of achieving intended wellbeing benefits.","Together, these studies provide preliminary validation of the method's ability to improve AI design, while surfacing areas needing refinement like developing support for complex steps.","Proposed adaptations such as examples and evaluation heuristics could address weaknesses.","Further research should examine sustained application over multiple projects.","This human-centered approach shows promise for realizing the vision of 'AI for Wellbeing' that does not just avoid harm, but actively benefits humanity."],"url":"http://arxiv.org/abs/2402.01499v1","category":"cs.AI"}
{"created":"2024-02-02 15:29:55","title":"Exponentiation of Parametric Hamiltonians via Unitary interpolation","abstract":"The effort to generate matrix exponentials and associated differentials, required to determine the time evolution of quantum systems, frequently constrains the evaluation of problems in quantum control theory, variational circuit compilation, or Monte-Carlo sampling. We introduce two ideas for the time-efficient approximation of matrix exponentials of linear multi-parametric Hamiltonians. We modify the Suzuki-Trotter product formula from an approximation to an interpolation schemes to improve both accuracy and computational time. This allows us to achieve high fidelities within a single interpolation step, which can be computed directly from cached matrices. We furthermore define the interpolation on a grid of system parameters, and show that the infidelity of the interpolation converges with $4^\\mathrm{th}$ order in the number of interpolation bins.","sentences":["The effort to generate matrix exponentials and associated differentials, required to determine the time evolution of quantum systems, frequently constrains the evaluation of problems in quantum control theory, variational circuit compilation, or Monte-Carlo sampling.","We introduce two ideas for the time-efficient approximation of matrix exponentials of linear multi-parametric Hamiltonians.","We modify the Suzuki-Trotter product formula from an approximation to an interpolation schemes to improve both accuracy and computational time.","This allows us to achieve high fidelities within a single interpolation step, which can be computed directly from cached matrices.","We furthermore define the interpolation on a grid of system parameters, and show that the infidelity of the interpolation converges with $4^\\mathrm{th}$ order in the number of interpolation bins."],"url":"http://arxiv.org/abs/2402.01498v1","category":"quant-ph"}
{"created":"2024-02-02 15:26:39","title":"A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation","abstract":"Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance.","sentences":["Generating natural language text from graph-structured data is essential for conversational information seeking.","Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate.","This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information.","In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples.","We compare four large language models of varying sizes with different prompting techniques.","Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions.","Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance."],"url":"http://arxiv.org/abs/2402.01495v1","category":"cs.CL"}
{"created":"2024-02-02 15:22:06","title":"Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates","abstract":"The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for general measures. The convergence analysis relies on the Lipschitz property associated to the SW integrand. Several numerical experiments demonstrate the superior performance of SHCV against state-of-the-art methods for SW distance computation.","sentences":["The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections.","As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance.","Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere.","Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates.","The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables.","Moreover, an improved rate of convergence, compared to Monte Carlo, is established for general measures.","The convergence analysis relies on the Lipschitz property associated to the SW integrand.","Several numerical experiments demonstrate the superior performance of SHCV against state-of-the-art methods for SW distance computation."],"url":"http://arxiv.org/abs/2402.01493v1","category":"stat.ML"}
{"created":"2024-02-02 15:18:45","title":"Moving Aggregate Modified Autoregressive Copula-Based Time Series Models (MAGMAR-Copulas) Without Markov Restriction","abstract":"Copula-based time series models implicitly assume a finite Markov order. In reality a time series may not follow the Markov property. We modify the copula-based time series models by introducing a moving aggregate (MAG) part into the model updating equation. The functional form of the MAG-part is given as the inverse of a conditional copula. The resulting MAG-modified Autoregressive Copula-Based Time Series model (MAGMAR-Copula) is discussed in detail and distributional properties are derived in a D-vine framework. The model nests the classical ARMA model as well as the copula-based time series model. The modeling performance is compared with the model from \\cite{mcneil2022time} modeling US inflation. Our model is competitive in terms of information criteria. It is a generalization of ARMA and also copula-based time series models and is in spirit similar to other moving average time series models such as ARMA and GARCH.","sentences":["Copula-based time series models implicitly assume a finite Markov order.","In reality a time series may not follow the Markov property.","We modify the copula-based time series models by introducing a moving aggregate (MAG) part into the model updating equation.","The functional form of the MAG-part is given as the inverse of a conditional copula.","The resulting MAG-modified Autoregressive Copula-Based Time Series model (MAGMAR-Copula) is discussed in detail and distributional properties are derived in a D-vine framework.","The model nests the classical ARMA model as well as the copula-based time series model.","The modeling performance is compared with the model from \\cite{mcneil2022time} modeling US inflation.","Our model is competitive in terms of information criteria.","It is a generalization of ARMA and also copula-based time series models and is in spirit similar to other moving average time series models such as ARMA and GARCH."],"url":"http://arxiv.org/abs/2402.01491v1","category":"stat.ME"}
{"created":"2024-02-02 15:16:37","title":"Giant DC Residual Current Generated by Subcycle Laser Pulses","abstract":"Experimental indications have been reported suggesting that laser pulses shining on materials with relativistic dispersion can produce currents that survive long after the illumination has died out. Such residual currents ('remnants') have applications in petahertz logical gates. The remnants' strength strongly depends on the pulse-shape. We develop an analytical formula that allows to optimize the pulse-shape for remnant production; we predict remnants exceeding the values observed so far by orders of magnitude. In fact, remnants can be almost as strong as the peak current under irradiation.","sentences":["Experimental indications have been reported suggesting that laser pulses shining on materials with relativistic dispersion can produce currents that survive long after the illumination has died out.","Such residual currents ('remnants') have applications in petahertz logical gates.","The remnants' strength strongly depends on the pulse-shape.","We develop an analytical formula that allows to optimize the pulse-shape for remnant production; we predict remnants exceeding the values observed so far by orders of magnitude.","In fact, remnants can be almost as strong as the peak current under irradiation."],"url":"http://arxiv.org/abs/2402.01490v1","category":"physics.optics"}
{"created":"2024-02-02 15:14:47","title":"Dynamic Occupancy Grids for Object Detection: A Radar-Centric Approach","abstract":"Dynamic Occupancy Grid Mapping is a technique used to generate a local map of the environment containing both static and dynamic information. Typically, these maps are primarily generated using lidar measurements. However, with improvements in radar sensing, resulting in better accuracy and higher resolution, radar is emerging as a viable alternative to lidar as the primary sensor for mapping. In this paper, we propose a radar-centric dynamic occupancy grid mapping algorithm with adaptations to the state computation, inverse sensor model, and field-of-view computation tailored to the specifics of radar measurements. We extensively evaluate our approach using real data to demonstrate its effectiveness and establish the first benchmark for radar-based dynamic occupancy grid mapping using the publicly available Radarscenes dataset.","sentences":["Dynamic Occupancy Grid Mapping is a technique used to generate a local map of the environment containing both static and dynamic information.","Typically, these maps are primarily generated using lidar measurements.","However, with improvements in radar sensing, resulting in better accuracy and higher resolution, radar is emerging as a viable alternative to lidar as the primary sensor for mapping.","In this paper, we propose a radar-centric dynamic occupancy grid mapping algorithm with adaptations to the state computation, inverse sensor model, and field-of-view computation tailored to the specifics of radar measurements.","We extensively evaluate our approach using real data to demonstrate its effectiveness and establish the first benchmark for radar-based dynamic occupancy grid mapping using the publicly available Radarscenes dataset."],"url":"http://arxiv.org/abs/2402.01488v1","category":"cs.RO"}
{"created":"2024-02-02 15:14:41","title":"Disformal gravitational waves","abstract":"Contrary to conformal transformations, disformal transformations can change the principal null directions of a spacetime geometry. Thus, depending on the frame a gravitational wave (GW) detector minimally couples to, the properties of GWs may change. In this letter, we provide necessary and sufficient conditions which determine whether GWs change under disformal transformations or not. Our argument is coordinate-independent and can be applied to any spacetime geometry at the fully non-linear level. As an example, we show that an exact solution of Einstein-scalar gravity describing a breathing wave emitted by a scalar monopole acquires a non-linear superposition of tensorial and breathing waves in the disformed frame. This type of non-linear effect can be completely overlooked in the usual linear perturbation theory.","sentences":["Contrary to conformal transformations, disformal transformations can change the principal null directions of a spacetime geometry.","Thus, depending on the frame a gravitational wave (GW) detector minimally couples to, the properties of GWs may change.","In this letter, we provide necessary and sufficient conditions which determine whether GWs change under disformal transformations or not.","Our argument is coordinate-independent and can be applied to any spacetime geometry at the fully non-linear level.","As an example, we show that an exact solution of Einstein-scalar gravity describing a breathing wave emitted by a scalar monopole acquires a non-linear superposition of tensorial and breathing waves in the disformed frame.","This type of non-linear effect can be completely overlooked in the usual linear perturbation theory."],"url":"http://arxiv.org/abs/2402.01487v1","category":"gr-qc"}
{"created":"2024-02-02 15:07:09","title":"Multi-level protein pre-training with Vabs-Net","abstract":"In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein chains to learn meaningful representations of both residues and atoms. This leads to a simple yet effective approach to learning protein representation suitable for diverse downstream tasks. Extensive experimental results on binding site prediction and function prediction tasks demonstrate our proposed pre-training approach significantly outperforms other methods. Our code will be made public.","sentences":["In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks.","However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms.","We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking.","Nevertheless, we find that naively combining residue and atom information during pre-training typically fails.","We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations.","To address this issue, we introduce a span mask pre-training strategy on 3D protein chains to learn meaningful representations of both residues and atoms.","This leads to a simple yet effective approach to learning protein representation suitable for diverse downstream tasks.","Extensive experimental results on binding site prediction and function prediction tasks demonstrate our proposed pre-training approach significantly outperforms other methods.","Our code will be made public."],"url":"http://arxiv.org/abs/2402.01481v1","category":"cs.LG"}
{"created":"2024-02-02 15:06:13","title":"SVI solutions to stochastic nonlinear diffusion equations on general measure spaces","abstract":"We establish a framework for the existence and uniqueness of solutions to stochastic nonlinear (possibly multi-valued) diffusion equations driven by multiplicative noise, with the drift operator $L$ being the generator of a transient Dirichlet form on a finite measure space $(E,\\mathcal{B},\\mu)$ and the initial value in $\\mathcal{F}_e^*$, which is the dual space of an extended transient Dirichlet space. $L$ and $\\mathcal{F}_e^*$ replace the Laplace operator $\\Delta$ and $H^{-1}$, respectively, in the classical case. This framework includes stochastic fast diffusion equations, stochastic fractional fast diffusion equations, the Zhang model, and apply to cases with $E$ being a manifold, a fractal or a graph. In addition, our results apply to operators $-f(-L)$, where $f$ is a Bernstein function, e.g. $f(\\lambda)=\\lambda^\\alpha$ or $f(\\lambda)=(\\lambda+1)^\\alpha-1$, $0<\\alpha<1$.","sentences":["We establish a framework for the existence and uniqueness of solutions to stochastic nonlinear (possibly multi-valued) diffusion equations driven by multiplicative noise, with the drift operator $L$ being the generator of a transient Dirichlet form on a finite measure space $(E,\\mathcal{B},\\mu)$ and the initial value in $\\mathcal{F}_e^*$, which is the dual space of an extended transient Dirichlet space.","$L$ and $\\mathcal{F}_e^*$ replace the Laplace operator $\\Delta$ and $H^{-1}$, respectively, in the classical case.","This framework includes stochastic fast diffusion equations, stochastic fractional fast diffusion equations, the Zhang model, and apply to cases with $E$ being a manifold, a fractal or a graph.","In addition, our results apply to operators $-f(-L)$, where $f$ is a Bernstein function, e.g. $f(\\lambda)=\\lambda^\\alpha$ or $f(\\lambda)=(\\lambda+1)^\\alpha-1$, $0<\\alpha<1$."],"url":"http://arxiv.org/abs/2402.01479v1","category":"math.PR"}
{"created":"2024-02-02 15:06:00","title":"A Modular Aerial System Based on Homogeneous Quadrotors with Fault-Tolerant Control","abstract":"The standard quadrotor is one of the most popular and widely used aerial vehicle of recent decades, offering great maneuverability with mechanical simplicity. However, the under-actuation characteristic limits its applications, especially when it comes to generating desired wrench with six degrees of freedom (DOF). Therefore, existing work often compromises between mechanical complexity and the controllable DOF of the aerial system. To take advantage of the mechanical simplicity of a standard quadrotor, we propose a modular aerial system, IdentiQuad, that combines only homogeneous quadrotor-based modules. Each IdentiQuad can be operated alone like a standard quadrotor, but at the same time allows task-specific assembly, increasing the controllable DOF of the system. Each module is interchangeable within its assembly. We also propose a general controller for different configurations of assemblies, capable of tolerating rotor failures and balancing the energy consumption of each module. The functionality and robustness of the system and its controller are validated using physics-based simulations for different assembly configurations.","sentences":["The standard quadrotor is one of the most popular and widely used aerial vehicle of recent decades, offering great maneuverability with mechanical simplicity.","However, the under-actuation characteristic limits its applications, especially when it comes to generating desired wrench with six degrees of freedom (DOF).","Therefore, existing work often compromises between mechanical complexity and the controllable DOF of the aerial system.","To take advantage of the mechanical simplicity of a standard quadrotor, we propose a modular aerial system, IdentiQuad, that combines only homogeneous quadrotor-based modules.","Each IdentiQuad can be operated alone like a standard quadrotor, but at the same time allows task-specific assembly, increasing the controllable DOF of the system.","Each module is interchangeable within its assembly.","We also propose a general controller for different configurations of assemblies, capable of tolerating rotor failures and balancing the energy consumption of each module.","The functionality and robustness of the system and its controller are validated using physics-based simulations for different assembly configurations."],"url":"http://arxiv.org/abs/2402.01477v1","category":"cs.RO"}
{"created":"2024-02-02 15:05:13","title":"Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes","abstract":"While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs). Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric. Moreover, the complexity of deriving the GP posteriors remains high for large-scale data. In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteriors can be based on the inversion of a diagonal matrix containing singular values, contributing to a reduction in time complexity; iii) an evidence lower bound is derived so that variational parameters can be optimized towards this objective. Experiments verify our excellent performances and efficiency on in-distribution, distribution-shift and out-of-distribution benchmarks.","sentences":["While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs).","Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric.","Moreover, the complexity of deriving the GP posteriors remains high for large-scale data.","In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired.","Through KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t.","the attention kernel fully characterizes the asymmetry; ii) using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteriors can be based on the inversion of a diagonal matrix containing singular values, contributing to a reduction in time complexity; iii) an evidence lower bound is derived so that variational parameters can be optimized towards this objective.","Experiments verify our excellent performances and efficiency on in-distribution, distribution-shift and out-of-distribution benchmarks."],"url":"http://arxiv.org/abs/2402.01476v1","category":"cs.LG"}
{"created":"2024-02-02 14:57:42","title":"Synthetic Data for the Mitigation of Demographic Biases in Face Recognition","abstract":"This study investigates the possibility of mitigating the demographic biases that affect face recognition technologies through the use of synthetic data. Demographic biases have the potential to impact individuals from specific demographic groups, and can be identified by observing disparate performance of face recognition systems across demographic groups. They primarily arise from the unequal representations of demographic groups in the training data. In recent times, synthetic data have emerged as a solution to some problems that affect face recognition systems. In particular, during the generation process it is possible to specify the desired demographic and facial attributes of images, in order to control the demographic distribution of the synthesized dataset, and fairly represent the different demographic groups. We propose to fine-tune with synthetic data existing face recognition systems that present some demographic biases. We use synthetic datasets generated with GANDiffFace, a novel framework able to synthesize datasets for face recognition with controllable demographic distribution and realistic intra-class variations. We consider multiple datasets representing different demographic groups for training and evaluation. Also, we fine-tune different face recognition systems, and evaluate their demographic fairness with different metrics. Our results support the proposed approach and the use of synthetic data to mitigate demographic biases in face recognition.","sentences":["This study investigates the possibility of mitigating the demographic biases that affect face recognition technologies through the use of synthetic data.","Demographic biases have the potential to impact individuals from specific demographic groups, and can be identified by observing disparate performance of face recognition systems across demographic groups.","They primarily arise from the unequal representations of demographic groups in the training data.","In recent times, synthetic data have emerged as a solution to some problems that affect face recognition systems.","In particular, during the generation process it is possible to specify the desired demographic and facial attributes of images, in order to control the demographic distribution of the synthesized dataset, and fairly represent the different demographic groups.","We propose to fine-tune with synthetic data existing face recognition systems that present some demographic biases.","We use synthetic datasets generated with GANDiffFace, a novel framework able to synthesize datasets for face recognition with controllable demographic distribution and realistic intra-class variations.","We consider multiple datasets representing different demographic groups for training and evaluation.","Also, we fine-tune different face recognition systems, and evaluate their demographic fairness with different metrics.","Our results support the proposed approach and the use of synthetic data to mitigate demographic biases in face recognition."],"url":"http://arxiv.org/abs/2402.01472v1","category":"cs.CV"}
{"created":"2024-02-02 14:56:48","title":"AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback","abstract":"The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism.","sentences":["The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks.","We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process.","AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules.","This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision.","Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation.","The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback.","Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism."],"url":"http://arxiv.org/abs/2402.01469v1","category":"cs.CL"}
{"created":"2024-02-02 14:55:51","title":"Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents","abstract":"Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.","sentences":["Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent?","If yes, does it contribute to the tasks?","In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input.","The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance.","The model also successfully reproduces local and nonlocal replay, which matches the human experimental data.","Our work provides a new avenue for understanding the mechanisms behind replay."],"url":"http://arxiv.org/abs/2402.01467v1","category":"eess.SY"}
{"created":"2024-02-02 14:54:38","title":"A Reinforcement Learning-Boosted Motion Planning Framework: Comprehensive Generalization Performance in Autonomous Driving","abstract":"This study introduces a novel approach to autonomous motion planning, informing an analytical algorithm with a reinforcement learning (RL) agent within a Frenet coordinate system. The combination directly addresses the challenges of adaptability and safety in autonomous driving. Motion planning algorithms are essential for navigating dynamic and complex scenarios. Traditional methods, however, lack the flexibility required for unpredictable environments, whereas machine learning techniques, particularly reinforcement learning (RL), offer adaptability but suffer from instability and a lack of explainability. Our unique solution synergizes the predictability and stability of traditional motion planning algorithms with the dynamic adaptability of RL, resulting in a system that efficiently manages complex situations and adapts to changing environmental conditions. Evaluation of our integrated approach shows a significant reduction in collisions, improved risk management, and improved goal success rates across multiple scenarios. The code used in this research is publicly available as open-source software and can be accessed at the following link: https://github.com/TUM-AVS/Frenetix-RL.","sentences":["This study introduces a novel approach to autonomous motion planning, informing an analytical algorithm with a reinforcement learning (RL) agent within a Frenet coordinate system.","The combination directly addresses the challenges of adaptability and safety in autonomous driving.","Motion planning algorithms are essential for navigating dynamic and complex scenarios.","Traditional methods, however, lack the flexibility required for unpredictable environments, whereas machine learning techniques, particularly reinforcement learning (RL), offer adaptability but suffer from instability and a lack of explainability.","Our unique solution synergizes the predictability and stability of traditional motion planning algorithms with the dynamic adaptability of RL, resulting in a system that efficiently manages complex situations and adapts to changing environmental conditions.","Evaluation of our integrated approach shows a significant reduction in collisions, improved risk management, and improved goal success rates across multiple scenarios.","The code used in this research is publicly available as open-source software and can be accessed at the following link: https://github.com/TUM-AVS/Frenetix-RL."],"url":"http://arxiv.org/abs/2402.01465v1","category":"cs.RO"}
{"created":"2024-02-02 14:54:30","title":"Local well-posedness of the Benjamin-Ono equation for a class of bounded initial data","abstract":"We prove local well-posedness of the Benjamin-Ono equation for a class of bounded initial data including periodic and bore-like functions. As a consequence, we obtain local well-posedness in $H^s(\\mathbb{R})+H^\\sigma(\\mathbb{T})$ for $s>\\frac{1}{4}$ and $\\sigma>\\frac{7}{2}$. These results follow by studying a generalized forced Benjamin-Ono equation.","sentences":["We prove local well-posedness of the Benjamin-Ono equation for a class of bounded initial data including periodic and bore-like functions.","As a consequence, we obtain local well-posedness in $H^s(\\mathbb{R})+H^\\sigma(\\mathbb{T})$ for $s>\\frac{1}{4}$ and $\\sigma>\\frac{7}{2}$. These results follow by studying a generalized forced Benjamin-Ono equation."],"url":"http://arxiv.org/abs/2402.01464v1","category":"math.AP"}
{"created":"2024-02-02 14:52:41","title":"3D Vertebrae Measurements: Assessing Vertebral Dimensions in Human Spine Mesh Models Using Local Anatomical Vertebral Axes","abstract":"Vertebral morphological measurements are important across various disciplines, including spinal biomechanics and clinical applications, pre- and post-operatively. These measurements also play a crucial role in anthropological longitudinal studies, where spinal metrics are repeatedly documented over extended periods. Traditionally, such measurements have been manually conducted, a process that is time-consuming. In this study, we introduce a novel, fully automated method for measuring vertebral morphology using 3D meshes of lumbar and thoracic spine models.Our experimental results demonstrate the method's capability to accurately measure low-resolution patient-specific vertebral meshes with mean absolute error (MAE) of 1.09 mm and those derived from artificially created lumbar spines, where the average MAE value was 0.7 mm. Our qualitative analysis indicates that measurements obtained using our method on 3D spine models can be accurately reprojected back onto the original medical images if these images are available.","sentences":["Vertebral morphological measurements are important across various disciplines, including spinal biomechanics and clinical applications, pre- and post-operatively.","These measurements also play a crucial role in anthropological longitudinal studies, where spinal metrics are repeatedly documented over extended periods.","Traditionally, such measurements have been manually conducted, a process that is time-consuming.","In this study, we introduce a novel, fully automated method for measuring vertebral morphology using 3D meshes of lumbar and thoracic spine models.","Our experimental results demonstrate the method's capability to accurately measure low-resolution patient-specific vertebral meshes with mean absolute error (MAE) of 1.09 mm and those derived from artificially created lumbar spines, where the average MAE value was 0.7 mm.","Our qualitative analysis indicates that measurements obtained using our method on 3D spine models can be accurately reprojected back onto the original medical images if these images are available."],"url":"http://arxiv.org/abs/2402.01462v1","category":"cs.CV"}
{"created":"2024-02-02 14:52:10","title":"Deep Conditional Generative Learning: Model and Error Analysis","abstract":"We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow. Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network. Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow. Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing conditional density estimation methods.","sentences":["We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow.","Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1.","For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network.","Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow.","Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing conditional density estimation methods."],"url":"http://arxiv.org/abs/2402.01460v1","category":"stat.ML"}
{"created":"2024-02-02 14:50:23","title":"GaMeS: Mesh-Based Adapting and Modification of Gaussian Splatting","abstract":"In recent years, a range of neural network-based methods for image rendering have been introduced. For instance, widely-researched neural radiance fields (NeRF) rely on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images. However, most NeRF models are constrained by long training and inference times. In comparison, Gaussian Splatting (GS) is a novel, state-of-theart technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering. A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components. To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of mesh and a Gaussian distribution, that pin all Gaussians splats on the object surface (mesh). The unique contribution of our methods is defining Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation. As a result, we obtain high-quality renders in the real-time generation of high-quality views. Furthermore, we demonstrate that in the absence of a predefined mesh, it is possible to fine-tune the initial mesh during the learning process.","sentences":["In recent years, a range of neural network-based methods for image rendering have been introduced.","For instance, widely-researched neural radiance fields (NeRF) rely on a neural network to represent 3D scenes, allowing for realistic view synthesis from a small number of 2D images.","However, most NeRF models are constrained by long training and inference times.","In comparison, Gaussian Splatting (GS) is a novel, state-of-theart technique for rendering points in a 3D scene by approximating their contribution to image pixels through Gaussian distributions, warranting fast training and swift, real-time rendering.","A drawback of GS is the absence of a well-defined approach for its conditioning due to the necessity to condition several hundred thousand Gaussian components.","To solve this, we introduce Gaussian Mesh Splatting (GaMeS) model, a hybrid of mesh and a Gaussian distribution, that pin all Gaussians splats on the object surface (mesh).","The unique contribution of our methods is defining Gaussian splats solely based on their location on the mesh, allowing for automatic adjustments in position, scale, and rotation during animation.","As a result, we obtain high-quality renders in the real-time generation of high-quality views.","Furthermore, we demonstrate that in the absence of a predefined mesh, it is possible to fine-tune the initial mesh during the learning process."],"url":"http://arxiv.org/abs/2402.01459v1","category":"cs.CV"}
{"created":"2024-02-02 14:43:19","title":"Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach","abstract":"In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through \"statistical causal prompting (SCP)\" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.","sentences":["In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge.","To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through \"statistical causal prompting (SCP)\" for LLMs and prior knowledge augmentation for SCD.","Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP.","Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset.","The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains."],"url":"http://arxiv.org/abs/2402.01454v1","category":"cs.LG"}
{"created":"2024-02-02 14:39:39","title":"Improving importance estimation in covariate shift for providing accurate prediction error","abstract":"In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. This situation is called covariate shift problem where standard error estimation may be no longer accurate. In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. The main drawback is that it is not easy to compute. The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way. Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. In this direction, this paper explores the potential performance improvement if target information is considered in the computation of the importance. Then, a redefinition of the importance arises in order to be generalized in this way. Besides the potential improvement in performance, including target information make possible the application to a real application about plankton classification that motivates this research and characterized by its great dimensionality, since considering targets rather than covariates reduces the computation and the noise in the covariates. The impact of taking target information is also explored when Logistic Regression (LR), Kernel Mean Matching (KMM), Ensemble Kernel Mean Matching (EKMM) and the naive predecessor of KLIEP called Kernel Density Estimation (KDE) methods estimate the importance. The experimental results lead to a more accurate error estimation using target information, especially in case of the more promising method KLIEP.","sentences":["In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets.","However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged.","This situation is called covariate shift problem where standard error estimation may be no longer accurate.","In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations.","The main drawback is that it is not easy to compute.","The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way.","Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance.","In this direction, this paper explores the potential performance improvement if target information is considered in the computation of the importance.","Then, a redefinition of the importance arises in order to be generalized in this way.","Besides the potential improvement in performance, including target information make possible the application to a real application about plankton classification that motivates this research and characterized by its great dimensionality, since considering targets rather than covariates reduces the computation and the noise in the covariates.","The impact of taking target information is also explored when Logistic Regression (LR), Kernel Mean Matching (KMM), Ensemble Kernel Mean Matching (EKMM) and the naive predecessor of KLIEP called Kernel Density Estimation (KDE) methods estimate the importance.","The experimental results lead to a more accurate error estimation using target information, especially in case of the more promising method KLIEP."],"url":"http://arxiv.org/abs/2402.01450v1","category":"cs.LG"}
{"created":"2024-02-02 14:39:11","title":"Exponential Ergodicity of CBIRE-Processes with Competition and Catastrophes","abstract":"We establish the exponential ergodic property in a weighted total variation distance of continuous-state branching processes with immigration in random environments with competition and catastrophes, under a Lyapunov-type condition and other mild assumptions. The proof is based on a Markov coupling process along with some delicate estimates for the associated coupling generator. In particular, the main result indicates whether and how the competition mechanism, the environment and the catastrophe could balance the branching mechanism respectively to guarantee the exponential ergodicity of the process.","sentences":["We establish the exponential ergodic property in a weighted total variation distance of continuous-state branching processes with immigration in random environments with competition and catastrophes, under a Lyapunov-type condition and other mild assumptions.","The proof is based on a Markov coupling process along with some delicate estimates for the associated coupling generator.","In particular, the main result indicates whether and how the competition mechanism, the environment and the catastrophe could balance the branching mechanism respectively to guarantee the exponential ergodicity of the process."],"url":"http://arxiv.org/abs/2402.01449v1","category":"math.PR"}
{"created":"2024-02-02 14:38:22","title":"The Hamilton space of pseudorandom graphs","abstract":"We show that if $n$ is odd and $p \\ge C \\log n / n$, then with high probability Hamilton cycles in $G(n,p)$ span its cycle space. More generally, we show this holds for a class of graphs satisfying certain natural pseudorandom properties. The proof is based on a novel idea of parity-switchers, which can be thought of as analogues of absorbers in the context of cycle spaces. As another application of our method, we show that Hamilton cycles in a near-Dirac graph $G$, that is, a graph $G$ with odd $n$ vertices and minimum degree $n/2 + C$ for sufficiently large constant $C$, span its cycle space.","sentences":["We show that if $n$ is odd and $p \\ge C \\log n / n$, then with high probability Hamilton cycles in $G(n,p)$ span its cycle space.","More generally, we show this holds for a class of graphs satisfying certain natural pseudorandom properties.","The proof is based on a novel idea of parity-switchers, which can be thought of as analogues of absorbers in the context of cycle spaces.","As another application of our method, we show that Hamilton cycles in a near-Dirac graph $G$, that is, a graph $G$ with odd $n$ vertices and minimum degree $n/2 + C$ for sufficiently large constant $C$, span its cycle space."],"url":"http://arxiv.org/abs/2402.01447v1","category":"math.CO"}
{"created":"2024-02-02 14:38:04","title":"Guidance Graph Optimization for Lifelong Multi-Agent Path Finding","abstract":"We study how to use guidance to improve the throughput of lifelong Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while incorporating guidance, such as highways, can accelerate MAPF algorithms, this often results in a trade-off with solution quality. In addition, how to generate good guidance automatically remains largely unexplored, with current methods falling short of surpassing manually designed ones. In this work, we introduce the directed guidance graph as a versatile representation of guidance for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of optimizing its edge weights. We present two GGO algorithms to automatically generate guidance for arbitrary lifelong MAPF algorithms and maps. The first method directly solves GGO by employing CMA-ES, a black-box optimization algorithm. The second method, PIU, optimizes an update model capable of generating guidance, demonstrating the ability to transfer optimized guidance graphs to larger maps with similar layouts. Empirically, we show that (1) our guidance graphs improve the throughput of three representative lifelong MAPF algorithms in four benchmark maps, and (2) our update model can generate guidance graphs for as large as $93 \\times 91$ maps and as many as 3000 agents.","sentences":["We study how to use guidance to improve the throughput of lifelong Multi-Agent Path Finding (MAPF).","Previous studies have demonstrated that while incorporating guidance, such as highways, can accelerate MAPF algorithms, this often results in a trade-off with solution quality.","In addition, how to generate good guidance automatically remains largely unexplored, with current methods falling short of surpassing manually designed ones.","In this work, we introduce the directed guidance graph as a versatile representation of guidance for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of optimizing its edge weights.","We present two GGO algorithms to automatically generate guidance for arbitrary lifelong MAPF algorithms and maps.","The first method directly solves GGO by employing CMA-ES, a black-box optimization algorithm.","The second method, PIU, optimizes an update model capable of generating guidance, demonstrating the ability to transfer optimized guidance graphs to larger maps with similar layouts.","Empirically, we show that (1) our guidance graphs improve the throughput of three representative lifelong MAPF algorithms in four benchmark maps, and (2) our update model can generate guidance graphs for as large as $93 \\times 91$ maps and as many as 3000 agents."],"url":"http://arxiv.org/abs/2402.01446v1","category":"cs.MA"}
{"created":"2024-02-02 14:37:26","title":"All graph state verification protocols are composably secure","abstract":"Graph state verification protocols allow multiple parties to share a graph state while checking that the state is honestly prepared, even in the presence of malicious parties. Since graph states are the starting point of numerous quantum protocols, it is crucial to ensure that graph state verification protocols can safely be composed with other protocols, this property being known as composable security. Previous works [YDK21] conjectured that such a property could not be proven within the abstract cryptography framework: we disprove this conjecture by showing that all graph state verification protocols can be turned into a composably secure protocol with respect to the natural functionality for graph state preparation. Moreover, we show that any unchanged graph state verification protocols can also be considered as composably secure for a slightly different, yet useful, functionality. Finally, we show that these two results are optimal, in the sense that any such generic result, considering arbitrary black-box protocols, must either modify the protocol or consider a different functionality.   Along the way, we show a protocol to generalize entanglement swapping to arbitrary graph states that might be of independent interest.","sentences":["Graph state verification protocols allow multiple parties to share a graph state while checking that the state is honestly prepared, even in the presence of malicious parties.","Since graph states are the starting point of numerous quantum protocols, it is crucial to ensure that graph state verification protocols can safely be composed with other protocols, this property being known as composable security.","Previous works","[YDK21] conjectured that such a property could not be proven within the abstract cryptography framework: we disprove this conjecture by showing that all graph state verification protocols can be turned into a composably secure protocol with respect to the natural functionality for graph state preparation.","Moreover, we show that any unchanged graph state verification protocols can also be considered as composably secure for a slightly different, yet useful, functionality.","Finally, we show that these two results are optimal, in the sense that any such generic result, considering arbitrary black-box protocols, must either modify the protocol or consider a different functionality.   ","Along the way, we show a protocol to generalize entanglement swapping to arbitrary graph states that might be of independent interest."],"url":"http://arxiv.org/abs/2402.01445v1","category":"quant-ph"}
{"created":"2024-02-02 14:36:50","title":"Mission Critical -- Satellite Data is a Distinct Modality in Machine Learning","abstract":"Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities. As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads. We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data. This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment. We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society.","sentences":["Satellite data has the potential to inspire a seismic shift for machine learning -- one in which we rethink existing practices designed for traditional data modalities.","As machine learning for satellite data (SatML) gains traction for its real-world impact, our field is at a crossroads.","We can either continue applying ill-suited approaches, or we can initiate a new research agenda that centers around the unique characteristics and challenges of satellite data.","This position paper argues that satellite data constitutes a distinct modality for machine learning research and that we must recognize it as such to advance the quality and impact of SatML research across theory, methods, and deployment.","We outline critical discussion questions and actionable suggestions to transform SatML from merely an intriguing application area to a dedicated research discipline that helps move the needle on big challenges for machine learning and society."],"url":"http://arxiv.org/abs/2402.01444v1","category":"cs.LG"}
{"created":"2024-02-02 14:34:25","title":"Generalized framework for admissibility preserving Lax-Wendroff Flux Reconstruction for hyperbolic conservation laws with source terms","abstract":"Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws. We perform a cell average decomposition of the LWFR scheme that is similar to the one used in the admissibility preserving framework of Zhang and Shu (2010). By performing a flux limiting of the time averaged numerical flux, the decomposition is used to obtain an admissibility preserving LWFR scheme. The admissibility preservation framework is further extended to a newly proposed extension of LWFR scheme for conservation laws with source terms. This is the first extension of the high order LW scheme that can handle source terms. The admissibility and accuracy are verified by numerical experiments on the Ten Moment equations of Livermore et al.","sentences":["Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws.","We perform a cell average decomposition of the LWFR scheme that is similar to the one used in the admissibility preserving framework of Zhang and Shu (2010).","By performing a flux limiting of the time averaged numerical flux, the decomposition is used to obtain an admissibility preserving LWFR scheme.","The admissibility preservation framework is further extended to a newly proposed extension of LWFR scheme for conservation laws with source terms.","This is the first extension of the high order LW scheme that can handle source terms.","The admissibility and accuracy are verified by numerical experiments on the Ten Moment equations of Livermore et al."],"url":"http://arxiv.org/abs/2402.01442v1","category":"math.NA"}
{"created":"2024-02-02 14:34:22","title":"Learning the Market: Sentiment-Based Ensemble Trading Agents","abstract":"We propose the integration of sentiment analysis and deep-reinforcement learning ensemble algorithms for stock trading, and design a strategy capable of dynamically altering its employed agent given concurrent market sentiment. In particular, we create a simple-yet-effective method for extracting news sentiment and combine this with general improvements upon existing works, resulting in automated trading agents that effectively consider both qualitative market factors and quantitative stock data. We show that our approach results in a strategy that is profitable, robust, and risk-minimal -- outperforming the traditional ensemble strategy as well as single agent algorithms and market metrics. Our findings determine that the conventional practice of switching ensemble agents every fixed-number of months is sub-optimal, and that a dynamic sentiment-based framework greatly unlocks additional performance within these agents. Furthermore, as we have designed our algorithm with simplicity and efficiency in mind, we hypothesize that the transition of our method from historical evaluation towards real-time trading with live data should be relatively simple.","sentences":["We propose the integration of sentiment analysis and deep-reinforcement learning ensemble algorithms for stock trading, and design a strategy capable of dynamically altering its employed agent given concurrent market sentiment.","In particular, we create a simple-yet-effective method for extracting news sentiment and combine this with general improvements upon existing works, resulting in automated trading agents that effectively consider both qualitative market factors and quantitative stock data.","We show that our approach results in a strategy that is profitable, robust, and risk-minimal -- outperforming the traditional ensemble strategy as well as single agent algorithms and market metrics.","Our findings determine that the conventional practice of switching ensemble agents every fixed-number of months is sub-optimal, and that a dynamic sentiment-based framework greatly unlocks additional performance within these agents.","Furthermore, as we have designed our algorithm with simplicity and efficiency in mind, we hypothesize that the transition of our method from historical evaluation towards real-time trading with live data should be relatively simple."],"url":"http://arxiv.org/abs/2402.01441v1","category":"q-fin.TR"}
{"created":"2024-02-02 14:32:42","title":"Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting","abstract":"Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to catalyze continued innovation in this field.","sentences":["Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements.","Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data.","This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task.","Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions.","We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process.","Within each category, we analyze the relationships among these methods and compare their strengths and limitations.","Finally, we outline prospective future directions for few-shot learning on graphs to catalyze continued innovation in this field."],"url":"http://arxiv.org/abs/2402.01440v1","category":"cs.LG"}
{"created":"2024-02-02 14:30:48","title":"From Words to Molecules: A Survey of Large Language Models in Chemistry","abstract":"In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their application in chemistry tasks. Finally, we identify promising research directions, including further integration with chemical knowledge, advancements in continual learning, and improvements in model interpretability, paving the way for groundbreaking developments in the field.","sentences":["In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas.","However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge.","This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture.","Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods.","We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs.","Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs.","After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their application in chemistry tasks.","Finally, we identify promising research directions, including further integration with chemical knowledge, advancements in continual learning, and improvements in model interpretability, paving the way for groundbreaking developments in the field."],"url":"http://arxiv.org/abs/2402.01439v1","category":"cs.LG"}
{"created":"2024-02-02 14:29:02","title":"New Branching Formulae for Classical Groups and Relations among them","abstract":"We find the branching laws for the classical pairs $\\mathrm{GL}(m, \\mathbb{C}) \\subset \\mathrm{GL}(n, \\mathbb{C})$, $\\mathrm{Sp}(2m, \\mathbb{C}) \\subset \\mathrm{Sp}(2n, \\mathbb{C})$, $\\mathrm{SO}(q, \\mathbb{C}) \\subset \\mathrm{SO}(p, \\mathbb{C})$ for all $m\\leq n$, and all $q\\leq p$, generalizing the well-known results of classical branching laws which exist for $m=n-1$, and $q=p-1$. Our approach provides a common proof applicable to all these groups. We also compare the branching multiplicities among these pairs.","sentences":["We find the branching laws for the classical pairs $\\mathrm{GL}(m, \\mathbb{C})","\\subset \\mathrm{GL}(n, \\mathbb{C})$, $\\mathrm{Sp}(2m, \\mathbb{C})","\\subset \\mathrm{Sp}(2n, \\mathbb{C})$, $\\mathrm{SO}(q, \\mathbb{C})","\\subset \\mathrm{SO}(p, \\mathbb{C})$ for all $m\\leq n$, and all $q\\leq p$, generalizing the well-known results of classical branching laws which exist for $m=n-1$, and $q=p-1$. Our approach provides a common proof applicable to all these groups.","We also compare the branching multiplicities among these pairs."],"url":"http://arxiv.org/abs/2402.01436v1","category":"math.RT"}
{"created":"2024-02-02 14:26:32","title":"Conditioning non-linear and infinite-dimensional diffusion processes","abstract":"Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.","sentences":["Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation.","To incorporate observed data for statistical and learning tasks, one needs to condition on observations.","While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored.","This paper conditions function valued stochastic processes without prior discretisation.","To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score.","We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods."],"url":"http://arxiv.org/abs/2402.01434v1","category":"stat.ML"}
{"created":"2024-02-02 14:16:43","title":"Adjoint Natural Deduction (Extended Version)","abstract":"Adjoint logic is a general approach to combining multiple logics with different structural properties, including linear, affine, strict, and (ordinary) intuitionistic logics, where each proposition has an intrinsic mode of truth. It has been defined in the form of a sequent calculus because the central concept of independence is most clearly understood in this form, and because it permits a proof of cut elimination following standard techniques.   In this paper we present a natural deduction formulation of adjoint logic and show how it is related to the sequent calculus. As a consequence, every provable proposition has a verification (sometimes called a long normal form). We also give a computational interpretation of adjoint logic in the form of a functional language and prove properties of computations that derive from the structure of modes, including freedom from garbage (for modes without weakening and contraction), strictness (for modes disallowing weakening), and erasure (based on a preorder between modes). Finally, we present a surprisingly subtle algorithm for type checking.","sentences":["Adjoint logic is a general approach to combining multiple logics with different structural properties, including linear, affine, strict, and (ordinary) intuitionistic logics, where each proposition has an intrinsic mode of truth.","It has been defined in the form of a sequent calculus because the central concept of independence is most clearly understood in this form, and because it permits a proof of cut elimination following standard techniques.   ","In this paper we present a natural deduction formulation of adjoint logic and show how it is related to the sequent calculus.","As a consequence, every provable proposition has a verification (sometimes called a long normal form).","We also give a computational interpretation of adjoint logic in the form of a functional language and prove properties of computations that derive from the structure of modes, including freedom from garbage (for modes without weakening and contraction), strictness (for modes disallowing weakening), and erasure (based on a preorder between modes).","Finally, we present a surprisingly subtle algorithm for type checking."],"url":"http://arxiv.org/abs/2402.01428v1","category":"cs.LO"}
{"created":"2024-02-02 14:08:34","title":"Different Tastes of Entities: Investigating Human Label Variation in Named Entity Annotations","abstract":"Named Entity Recognition (NER) is a key information extraction task with a long-standing tradition. While recent studies address and aim to correct annotation errors via re-labeling efforts, little is known about the sources of human label variation, such as text ambiguity, annotation error, or guideline divergence. This is especially the case for high-quality datasets and beyond English CoNLL03. This paper studies disagreements in expert-annotated named entity datasets for three languages: English, Danish, and Bavarian. We show that text ambiguity and artificial guideline changes are dominant factors for diverse annotations among high-quality revisions. We survey student annotations on a subset of difficult entities and substantiate the feasibility and necessity of manifold annotations for understanding named entity ambiguities from a distributional perspective.","sentences":["Named Entity Recognition (NER) is a key information extraction task with a long-standing tradition.","While recent studies address and aim to correct annotation errors via re-labeling efforts, little is known about the sources of human label variation, such as text ambiguity, annotation error, or guideline divergence.","This is especially the case for high-quality datasets and beyond English CoNLL03.","This paper studies disagreements in expert-annotated named entity datasets for three languages: English, Danish, and Bavarian.","We show that text ambiguity and artificial guideline changes are dominant factors for diverse annotations among high-quality revisions.","We survey student annotations on a subset of difficult entities and substantiate the feasibility and necessity of manifold annotations for understanding named entity ambiguities from a distributional perspective."],"url":"http://arxiv.org/abs/2402.01423v1","category":"cs.CL"}
{"created":"2024-02-02 14:04:18","title":"EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face Generation","abstract":"Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order to address this challenge, we propose a visual attribute-guided audio decoupler. This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions. To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module. Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix. Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished. Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video. Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization. Project page: https://peterfanfan.github.io/EmoSpeaker/","sentences":["Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content.","Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge.","In order to address this challenge, we propose a visual attribute-guided audio decoupler.","This enables the obtention of content vectors solely related to the audio content, enhancing the stability of subsequent lip movement coefficient predictions.","To achieve more precise emotional expression, we introduce a fine-grained emotion coefficient prediction module.","Additionally, we propose an emotion intensity control method using a fine-grained emotion matrix.","Through these, effective control over emotional expression in the generated videos and finer classification of emotion intensity are accomplished.","Subsequently, a series of 3DMM coefficient generation networks are designed to predict 3D coefficients, followed by the utilization of a rendering network to generate the final video.","Our experimental results demonstrate that our proposed method, EmoSpeaker, outperforms existing emotional talking face generation methods in terms of expression variation and lip synchronization.","Project page: https://peterfanfan.github.io/EmoSpeaker/"],"url":"http://arxiv.org/abs/2402.01422v1","category":"cs.CV"}
{"created":"2024-02-02 13:59:12","title":"The case of the missing VHE GRBs: A retrospective study of Swift gamma-ray bursts with Imaging Atmospheric Cherenkov Telescopes","abstract":"Gamma-ray bursts (GRBs) are particle acceleration sites that can emit photons in the very high-energy (VHE) domain through non-thermal processes. From 2004 until 2018, the current generation of Imaging Atmospheric Cherenkov Telescopes (IACTs) did not detect any GRB in the VHE domain. However, from 2018 to 2020, five detections have been reported. In this work, we try to solve the case of the missing VHE GBRs prior to 2018. We aim to identify GRBs that might have eluded VHE detection in the past years by the H.E.S.S., MAGIC, and VERITAS IACTs. To do so, we study GRBs with known redshift detected by \\emph{Swift} from 2004 until June 2022. We first identify all GRBs that could have been observed by these IACTs since 2004, considering observation conditions and visibility constraints. We assume a relation between the X-rays and the VHE gamma rays based on the VHE GRBs detected to date and combine this with the redshift measurements, instrument response information, and observation conditions to predict the observed VHE gamma-ray flux from the \\emph{Swift}-XRT measurements. We report findings on 12 bright low-redshift GRBs that we identify as most likely to have been detected in the VHE domain by current IACTs. The rate of IACT-detectable GRBs with ideal observation conditions is $<$1 VHE GRB per year with the current configuration. With its lower energy threshold and higher sensitivity, this rate increases to $\\sim$4 VHE GRBs per year with CTA.","sentences":["Gamma-ray bursts (GRBs) are particle acceleration sites that can emit photons in the very high-energy (VHE) domain through non-thermal processes.","From 2004 until 2018, the current generation of Imaging Atmospheric Cherenkov Telescopes (IACTs) did not detect any GRB in the VHE domain.","However, from 2018 to 2020, five detections have been reported.","In this work, we try to solve the case of the missing VHE GBRs prior to 2018.","We aim to identify GRBs that might have eluded VHE detection in the past years by the H.E.S.S., MAGIC, and VERITAS IACTs.","To do so, we study GRBs with known redshift detected by \\emph{Swift} from 2004 until June 2022.","We first identify all GRBs that could have been observed by these IACTs since 2004, considering observation conditions and visibility constraints.","We assume a relation between the X-rays and the VHE gamma rays based on the VHE GRBs detected to date and combine this with the redshift measurements, instrument response information, and observation conditions to predict the observed VHE gamma-ray flux from the \\emph{Swift}-XRT measurements.","We report findings on 12 bright low-redshift GRBs that we identify as most likely to have been detected in the VHE domain by current IACTs.","The rate of IACT-detectable GRBs with ideal observation conditions is $<$1 VHE GRB per year with the current configuration.","With its lower energy threshold and higher sensitivity, this rate increases to $\\sim$4 VHE GRBs per year with CTA."],"url":"http://arxiv.org/abs/2402.01421v1","category":"astro-ph.HE"}
{"created":"2024-02-02 13:58:23","title":"Cohomogeneity one solitons for the isometric flow of $G_2$-structures","abstract":"We consider the existence of cohomogeneity one solitons for the isometric flow of $G_2$-structures on the following classes of torsion-free $G_2$-manifolds: the Euclidean $R^7$ with its standard $G_2$-structure, metric cylinders over Calabi-Yau 3-folds, metric cones over nearly K\\\"ahler 6-manifolds, and the Bryant-Salamon $G_2$-manifolds. In all cases we establish existence of global solutions to the isometric soliton equations, and determine the asymptotic behaviour of the torsion. In particular, existence of shrinking isometric solitons on $R^7$ is proved, giving support to the likely existence of type I singularities for the isometric flow. In each case, the study of the soliton equation reduces to a particular nonlinear ODE with a regular singular point, for which we provide a careful analysis. Finally, to simplify the derivation of the relevant equations in each case, we first establish several useful Riemannian geometric formulas for a general class of cohomogeneity one metrics on total spaces of vector bundles which should have much wider application, as such metrics arise often as explicit examples of special holonomy metrics.","sentences":["We consider the existence of cohomogeneity one solitons for the isometric flow of $G_2$-structures on the following classes of torsion-free $G_2$-manifolds: the Euclidean $R^7$ with its standard $G_2$-structure, metric cylinders over Calabi-Yau 3-folds, metric cones over nearly K\\\"ahler 6-manifolds, and the Bryant-Salamon $G_2$-manifolds.","In all cases we establish existence of global solutions to the isometric soliton equations, and determine the asymptotic behaviour of the torsion.","In particular, existence of shrinking isometric solitons on $R^7$ is proved, giving support to the likely existence of type I singularities for the isometric flow.","In each case, the study of the soliton equation reduces to a particular nonlinear ODE with a regular singular point, for which we provide a careful analysis.","Finally, to simplify the derivation of the relevant equations in each case, we first establish several useful Riemannian geometric formulas for a general class of cohomogeneity one metrics on total spaces of vector bundles which should have much wider application, as such metrics arise often as explicit examples of special holonomy metrics."],"url":"http://arxiv.org/abs/2402.01420v1","category":"math.DG"}
{"created":"2024-02-02 13:56:49","title":"Extensions and factorizations of topological and semitopological universal algebras","abstract":"The possibility of extension operations of topological and semitopological algebras on the Stone-\\v{C}ech compactification, factorization of continuous functions through homomorphisms into metrizable algebras are investigated. Pseudocompact and compact algebras are studied first.","sentences":["The possibility of extension operations of topological and semitopological algebras on the Stone-\\v{C}ech compactification, factorization of continuous functions through homomorphisms into metrizable algebras are investigated.","Pseudocompact and compact algebras are studied first."],"url":"http://arxiv.org/abs/2402.01418v1","category":"math.GN"}
{"created":"2024-02-02 13:55:37","title":"Sequence Shortening for Context-Aware Machine Translation","abstract":"Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that the two methods achieve competitive BLEU and COMET scores and accuracies on the contrastive datasets to the other tested methods while potentially allowing for higher interpretability and reducing the growth of memory requirements with increased context size.","sentences":["Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context.","Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models.","In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches.","Furthermore, we investigate the application of Sequence Shortening to the cached representations.","We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context.","Our experiments show that the two methods achieve competitive BLEU and COMET scores and accuracies on the contrastive datasets to the other tested methods while potentially allowing for higher interpretability and reducing the growth of memory requirements with increased context size."],"url":"http://arxiv.org/abs/2402.01416v1","category":"cs.CL"}
{"created":"2024-02-02 13:53:29","title":"SMLP: Symbolic Machine Learning Prover","abstract":"Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors. SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods. SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level. SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models.","sentences":["Symbolic Machine Learning Prover (SMLP) is a tool and a library for system exploration based on data samples obtained by simulating or executing the system on a number of input vectors.","SMLP aims at exploring the system based on this data by taking a grey-box approach: SMLP combines statistical methods of data exploration with building and exploring machine learning models in close feedback loop with the system's response, and exploring these models by combining probabilistic and formal methods.","SMLP has been applied in industrial setting at Intel for analyzing and optimizing hardware designs at the analog level.","SMLP is a general purpose tool and can be applied to systems that can be sampled and modeled by machine learning models."],"url":"http://arxiv.org/abs/2402.01415v1","category":"cs.LG"}
{"created":"2024-02-02 13:51:50","title":"Simpler characterizations of total orderization invariant maps","abstract":"Given a finite subset $A$ of a distributive lattice, its total orderization $to(A)$ is a natural transformation of $A$ into a totally ordered set. Recently, the author showed that multivariate maps on distributive lattices which remain invariant under total orderizations generalize various maps on vector lattices, including bounded orthosymmetric multilinear maps and finite sums of bounded orthogonally additive polynomials. Therefore, a study of total orderization invariant maps on distributive lattices provides new perspectives for maps widely researched in vector lattice theory. However, the unwieldy notation of total orderizations can make calculations extremely long and difficult. In this paper we resolve this complication by providing considerably simpler characterizations of total orderization maps. Utilizing these easier representations, we then prove that a lattice multi-homomorphism on a distributive lattice is total orderization invariant if and only if it is symmetric, and we show that the diagonal of a symmetric lattice multi-homomorphism is a lattice homomorphism, extending known results for orthosymmetric vector lattice homomorphisms.","sentences":["Given a finite subset $A$ of a distributive lattice, its total orderization $to(A)$ is a natural transformation of $A$ into a totally ordered set.","Recently, the author showed that multivariate maps on distributive lattices which remain invariant under total orderizations generalize various maps on vector lattices, including bounded orthosymmetric multilinear maps and finite sums of bounded orthogonally additive polynomials.","Therefore, a study of total orderization invariant maps on distributive lattices provides new perspectives for maps widely researched in vector lattice theory.","However, the unwieldy notation of total orderizations can make calculations extremely long and difficult.","In this paper we resolve this complication by providing considerably simpler characterizations of total orderization maps.","Utilizing these easier representations, we then prove that a lattice multi-homomorphism on a distributive lattice is total orderization invariant if and only if it is symmetric, and we show that the diagonal of a symmetric lattice multi-homomorphism is a lattice homomorphism, extending known results for orthosymmetric vector lattice homomorphisms."],"url":"http://arxiv.org/abs/2402.01414v1","category":"math.FA"}
{"created":"2024-02-02 13:45:42","title":"Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge","abstract":"Supervised models for speech enhancement are trained using artificially generated mixtures of clean speech and noise signals. However, the synthetic training conditions may not accurately reflect real-world conditions encountered during testing. This discrepancy can result in poor performance when the test domain significantly differs from the synthetic training domain. To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to leverage real-world noisy speech recordings from the test domain for unsupervised domain adaptation of speech enhancement models. Specifically, this test domain corresponds to the CHiME-5 dataset, characterized by real multi-speaker and conversational speech recordings made in noisy and reverberant domestic environments, for which ground-truth clean speech signals are not available. In this paper, we present the objective and subjective evaluations of the systems that were submitted to the CHiME-7 UDASE task, and we provide an analysis of the results. This analysis reveals a limited correlation between subjective ratings and several supervised nonintrusive performance metrics recently proposed for speech enhancement. Conversely, the results suggest that more traditional intrusive objective metrics can be used for in-domain performance evaluation using the reverberant LibriCHiME-5 dataset developed for the challenge. The subjective evaluation indicates that all systems successfully reduced the background noise, but always at the expense of increased distortion. Out of the four speech enhancement methods evaluated subjectively, only one demonstrated an improvement in overall quality compared to the unprocessed noisy speech, highlighting the difficulty of the task. The tools and audio material created for the CHiME-7 UDASE task are shared with the community.","sentences":["Supervised models for speech enhancement are trained using artificially generated mixtures of clean speech and noise signals.","However, the synthetic training conditions may not accurately reflect real-world conditions encountered during testing.","This discrepancy can result in poor performance when the test domain significantly differs from the synthetic training domain.","To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to leverage real-world noisy speech recordings from the test domain for unsupervised domain adaptation of speech enhancement models.","Specifically, this test domain corresponds to the CHiME-5 dataset, characterized by real multi-speaker and conversational speech recordings made in noisy and reverberant domestic environments, for which ground-truth clean speech signals are not available.","In this paper, we present the objective and subjective evaluations of the systems that were submitted to the CHiME-7 UDASE task, and we provide an analysis of the results.","This analysis reveals a limited correlation between subjective ratings and several supervised nonintrusive performance metrics recently proposed for speech enhancement.","Conversely, the results suggest that more traditional intrusive objective metrics can be used for in-domain performance evaluation using the reverberant LibriCHiME-5 dataset developed for the challenge.","The subjective evaluation indicates that all systems successfully reduced the background noise, but always at the expense of increased distortion.","Out of the four speech enhancement methods evaluated subjectively, only one demonstrated an improvement in overall quality compared to the unprocessed noisy speech, highlighting the difficulty of the task.","The tools and audio material created for the CHiME-7 UDASE task are shared with the community."],"url":"http://arxiv.org/abs/2402.01413v1","category":"cs.SD"}
{"created":"2024-02-02 13:44:47","title":"Bass Accompaniment Generation via Latent Diffusion","abstract":"The ability to automatically generate music that appropriately matches an arbitrary input track is a challenging task. We present a novel controllable system for generating single stems to accompany musical mixes of arbitrary length. At the core of our method are audio autoencoders that efficiently compress audio waveform samples into invertible latent representations, and a conditional latent diffusion model that takes as input the latent encoding of a mix and generates the latent encoding of a corresponding stem. To provide control over the timbre of generated samples, we introduce a technique to ground the latent space to a user-provided reference style during diffusion sampling. For further improving audio quality, we adapt classifier-free guidance to avoid distortions at high guidance strengths when generating an unbounded latent space. We train our model on a dataset of pairs of mixes and matching bass stems. Quantitative experiments demonstrate that, given an input mix, the proposed system can generate basslines with user-specified timbres. Our controllable conditional audio generation framework represents a significant step forward in creating generative AI tools to assist musicians in music production.","sentences":["The ability to automatically generate music that appropriately matches an arbitrary input track is a challenging task.","We present a novel controllable system for generating single stems to accompany musical mixes of arbitrary length.","At the core of our method are audio autoencoders that efficiently compress audio waveform samples into invertible latent representations, and a conditional latent diffusion model that takes as input the latent encoding of a mix and generates the latent encoding of a corresponding stem.","To provide control over the timbre of generated samples, we introduce a technique to ground the latent space to a user-provided reference style during diffusion sampling.","For further improving audio quality, we adapt classifier-free guidance to avoid distortions at high guidance strengths when generating an unbounded latent space.","We train our model on a dataset of pairs of mixes and matching bass stems.","Quantitative experiments demonstrate that, given an input mix, the proposed system can generate basslines with user-specified timbres.","Our controllable conditional audio generation framework represents a significant step forward in creating generative AI tools to assist musicians in music production."],"url":"http://arxiv.org/abs/2402.01412v1","category":"cs.SD"}
{"created":"2024-02-02 13:42:50","title":"CodePori: Large Scale Model for Autonomous Software Development by Using Multi-Agents","abstract":"Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) are reshaping the field of Software Engineering (SE). Existing LLM-based multi-agent systems have successfully resolved simple dialogue tasks. However, the potential of LLMs for more complex tasks, such as automated code generation for large and complex projects, have been explored in only a few existing works. This paper introduces CodePori, a novel model designed to automate code generation for extensive and complex software projects based on natural language prompts. We employ LLM-based multi-AI agents to handle creative and challenging tasks in autonomous software development. Each agent engages with a specific task, including system design, code development, code review, code verification, and test engineering. We show in the paper that CodePori is able to generate running code for large-scale projects, completing the entire software development process in minutes rather than hours, and at a cost of a few dollars. It identifies and mitigates potential security vulnerabilities and corrects errors while maintaining a solid code performance level. We also conducted an evaluation of CodePori against existing solutions using HumanEval and the Massively Multitask Benchmark for Python (MBPP) benchmark. The results indicate that CodePori improves upon the benchmarks in terms of code accuracy, efficiency, and overall performance. For example, CodePori improves the pass@1 metric on HumanEval to 87.5% and on MBPP to 86.5%, representing a clear improvement over the existing models. We also assessed CodePori's performance through practitioner evaluations, with 91% expressing satisfaction with the model's performance.","sentences":["Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) are reshaping the field of Software Engineering (SE).","Existing LLM-based multi-agent systems have successfully resolved simple dialogue tasks.","However, the potential of LLMs for more complex tasks, such as automated code generation for large and complex projects, have been explored in only a few existing works.","This paper introduces CodePori, a novel model designed to automate code generation for extensive and complex software projects based on natural language prompts.","We employ LLM-based multi-AI agents to handle creative and challenging tasks in autonomous software development.","Each agent engages with a specific task, including system design, code development, code review, code verification, and test engineering.","We show in the paper that CodePori is able to generate running code for large-scale projects, completing the entire software development process in minutes rather than hours, and at a cost of a few dollars.","It identifies and mitigates potential security vulnerabilities and corrects errors while maintaining a solid code performance level.","We also conducted an evaluation of CodePori against existing solutions using HumanEval and the Massively Multitask Benchmark for Python (MBPP) benchmark.","The results indicate that CodePori improves upon the benchmarks in terms of code accuracy, efficiency, and overall performance.","For example, CodePori improves the pass@1 metric on HumanEval to 87.5% and on MBPP to 86.5%, representing a clear improvement over the existing models.","We also assessed CodePori's performance through practitioner evaluations, with 91% expressing satisfaction with the model's performance."],"url":"http://arxiv.org/abs/2402.01411v1","category":"cs.SE"}
{"created":"2024-02-02 13:42:45","title":"XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision","abstract":"Skin cancer detection through dermoscopy image analysis is a critical task. However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature. In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model. We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes. These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries. Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models.","sentences":["Skin cancer detection through dermoscopy image analysis is a critical task.","However, existing models used for this purpose often lack interpretability and reliability, raising the concern of physicians due to their black-box nature.","In this paper, we propose a novel approach for the diagnosis of melanoma using an interpretable prototypical-part model.","We introduce a guided supervision based on non-expert feedback through the incorporation of: 1) binary masks, obtained automatically using a segmentation network; and 2) user-refined prototypes.","These two distinct information pathways aim to ensure that the learned prototypes correspond to relevant areas within the skin lesion, excluding confounding factors beyond its boundaries.","Experimental results demonstrate that, even without expert supervision, our approach achieves superior performance and generalization compared to non-interpretable models."],"url":"http://arxiv.org/abs/2402.01410v1","category":"cs.CV"}
{"created":"2024-02-02 13:42:12","title":"Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models","abstract":"Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the \"What?\"), explain task predictions (the \"Why?\"), and imagine alternative scenarios that could result in different predictions (the \"What if?\"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the \"What?\"), simple explanations for task predictions (the \"Why?\"), and interpretable counterfactuals (the \"What if?\"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) show users how to get a desired class label, and (iii) propose concept interventions via \"task-driven\" interventions.","sentences":["Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the \"What?\"), explain task predictions (the \"Why?\"), and imagine alternative scenarios that could result in different predictions (the \"What if?\").","The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction.","To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches.","Our results show that CF-CBMs produce: accurate predictions (the \"What?\"), simple explanations for task predictions (the \"Why?\"), and interpretable counterfactuals (the \"What if?\").","CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) show users how to get a desired class label, and (iii) propose concept interventions via \"task-driven\" interventions."],"url":"http://arxiv.org/abs/2402.01408v1","category":"cs.LG"}
{"created":"2024-02-02 13:37:07","title":"On Measuring Context Utilization in Document-Level MT Systems","abstract":"Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context. Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation. Such measures cannot reveal whether the translation model uses the correct supporting context. We propose to complement accuracy-based evaluation with measures of context utilization. We find that perturbation-based analysis (comparing models' performance when provided with correct versus random context) is an effective measure of overall context utilization. For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena. We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annotations are not available. Finally, we highlight the importance of using discourse-rich datasets when assessing context utilization.","sentences":["Document-level translation models are usually evaluated using general metrics such as BLEU, which are not informative about the benefits of context.","Current work on context-aware evaluation, such as contrastive methods, only measure translation accuracy on words that need context for disambiguation.","Such measures cannot reveal whether the translation model uses the correct supporting context.","We propose to complement accuracy-based evaluation with measures of context utilization.","We find that perturbation-based analysis (comparing models' performance when provided with correct versus random context) is an effective measure of overall context utilization.","For a finer-grained phenomenon-specific evaluation, we propose to measure how much the supporting context contributes to handling context-dependent discourse phenomena.","We show that automatically-annotated supporting context gives similar conclusions to human-annotated context and can be used as alternative for cases where human annotations are not available.","Finally, we highlight the importance of using discourse-rich datasets when assessing context utilization."],"url":"http://arxiv.org/abs/2402.01404v1","category":"cs.CL"}
{"created":"2024-02-02 13:33:30","title":"Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization","abstract":"To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of zero-shot unlearning.","sentences":["To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important.","The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance.","In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten.","Under such a definition, existing state-of-the-art methods are insufficient.","Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample.","We show this smoothing successfully results in forgetting while preserving general model performance.","We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of zero-shot unlearning."],"url":"http://arxiv.org/abs/2402.01401v1","category":"cs.LG"}
{"created":"2024-02-02 13:31:24","title":"Query-Efficient Correlation Clustering with Noisy Oracle","abstract":"We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.","sentences":["We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements.","Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy.","We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings.","For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees.","Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard."],"url":"http://arxiv.org/abs/2402.01400v1","category":"stat.ML"}
{"created":"2024-02-02 13:31:17","title":"A Probabilistic Model to explain Self-Supervised Representation Learning","abstract":"Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows the gap to discriminative methods on _content_ classification and, as our analysis predicts, outperforms them where _style_ information is required, taking a step toward task-agnostic representations.","sentences":["Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities.","Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning.","However, a theoretical understanding of the mechanism behind these methods eludes.","We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework.","We also justify links to mutual information and the use of a projection head.","Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows the gap to discriminative methods on _content_ classification and, as our analysis predicts, outperforms them where _style_ information is required, taking a step toward task-agnostic representations."],"url":"http://arxiv.org/abs/2402.01399v1","category":"cs.LG"}
{"created":"2024-02-02 13:19:58","title":"The $(\\infty,2)$-category of internal $(\\infty,1)$-categories","abstract":"We define and study the $(\\infty,2)$-category $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ of $(\\infty,1)$-categories internal to an $(\\infty,1)$-category $\\mathcal{C}$ via an associated externalization construction. In the first part, we show various formal closure properties of $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ regarding limits, tensors, cotensors and internal mapping objects under the assumption of various suitable closure properties of $\\mathcal{C}$. In particular, we show that $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ defines a cartesian closed full sub-$\\infty$-cosmos of the $\\infty$-cosmos $\\mathbf{Fun}(\\mathcal{C}^{op},\\mathbf{Cat}_{\\infty})$ of $\\mathcal{C}$-indexed $(\\infty,1)$-categories under suitable assumptions on $\\mathcal{C}$. We furthermore characterize the objects of $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ by means of a Yoneda lemma that expresses indexed diagrams of internal shape over $\\mathcal{C}$ in terms of an $(\\infty,1)$-categorical totalization. In the second part, we relate the general theory developed to this point to results in the model categorical literature. We show that every model category $\\mathbb{M}$ gives rise to a \"hands-on\" $\\infty$-cosmos $\\mathbf{Cat}_{\\infty}(\\mathbb{M})$ (of not-necessarily cofibrant objects) directly by restriction of the Reedy model structure on $\\mathbb{M}^{\\Delta^{op}}$. We then define an according right derived model categorical externalization functor, and use it to show that the $(\\infty,1)$-categorical and the model categorical constructions correspond to one another whenever $\\mathcal{C}$ is presentable and $\\mathbb{M}$ is a suitable presentation thereof.","sentences":["We define and study the $(\\infty,2)$-category $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ of $(\\infty,1)$-categories internal to an $(\\infty,1)$-category $\\mathcal{C}$ via an associated externalization construction.","In the first part, we show various formal closure properties of $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ regarding limits, tensors, cotensors and internal mapping objects under the assumption of various suitable closure properties of $\\mathcal{C}$. In particular, we show that $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ defines a cartesian closed full sub-$\\infty$-cosmos of the $\\infty$-cosmos $\\mathbf{Fun}(\\mathcal{C}^{op},\\mathbf{Cat}_{\\infty})$ of $\\mathcal{C}$-indexed $(\\infty,1)$-categories under suitable assumptions on $\\mathcal{C}$. We furthermore characterize the objects of $\\mathbf{Cat}_{\\infty}(\\mathcal{C})$ by means of a Yoneda lemma that expresses indexed diagrams of internal shape over $\\mathcal{C}$ in terms of an $(\\infty,1)$-categorical totalization.","In the second part, we relate the general theory developed to this point to results in the model categorical literature.","We show that every model category $\\mathbb{M}$ gives rise to a \"hands-on\" $\\infty$-cosmos $\\mathbf{Cat}_{\\infty}(\\mathbb{M})$ (of not-necessarily cofibrant objects) directly by restriction of the Reedy model structure on $\\mathbb{M}^{\\Delta^{op}}$. We then define an according right derived model categorical externalization functor, and use it to show that the $(\\infty,1)$-categorical and the model categorical constructions correspond to one another","whenever $\\mathcal{C}$ is presentable and $\\mathbb{M}$ is a suitable presentation thereof."],"url":"http://arxiv.org/abs/2402.01396v1","category":"math.CT"}
{"created":"2024-02-02 13:17:19","title":"ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data","abstract":"We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling rate.","sentences":["We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models.","We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method.","These embeddings are then processed by a transformer model trained for object and gesture recognition.","Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors.","We also demonstrate that our asynchronous model can operate at any desired sampling rate."],"url":"http://arxiv.org/abs/2402.01393v1","category":"cs.CV"}
{"created":"2024-02-02 13:14:31","title":"StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback","abstract":"The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks.","sentences":["The advancement of large language models (LLMs) has significantly propelled the field of code generation.","Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality.","However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge.","Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective.","To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization.","In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests.","Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks."],"url":"http://arxiv.org/abs/2402.01391v1","category":"cs.SE"}
{"created":"2024-02-02 13:12:58","title":"Lower bounds for high derivatives of smooth functions with given zeros","abstract":"Let $f: B^n \\rightarrow {\\mathbb R}$ be a $d+1$ times continuously differentiable function on the unit ball $B^n$, with $\\max_{z\\in B^n} |f(z)|=1$. A well-known fact is that if $f$ vanishes on a set $Z\\subset B^n$ with a non-empty interior, then for each $k=1,\\ldots,d+1$ the norm of the $k$-th derivative $\\|f^{(k)}\\|$ is at least $M=M(n,k)>0$. A natural question to ask is: What happens for other sets $Z$? In particular, for finite, but sufficiently dense sets?}   This question was partially answered in ([16],[20-22]). This study can be naturally related to a certain special settings of the classical Whitney's smooth extension problem.   Our goal in the present paper is threefold: first, to provide an overview of the relevant questions and existing results in the general Whitney's problem. Second, we provide an overview of our specific setting and some available results. Third, we provide some new results in our direction. These new results extend the recent result of [21], where an answer to the above question is given via the topological information on $Z$.","sentences":["Let $f: B^n \\rightarrow {\\mathbb R}$ be a $d+1$ times continuously differentiable function on the unit ball $B^n$, with","$\\max_{z\\in B^n} |f(z)|=1$.","A well-known fact is that if $f$ vanishes on a set $Z\\subset B^n$ with a non-empty interior, then for each $k=1,\\ldots,d+1$ the norm of the $k$-th derivative $\\|f^{(k)}\\|$ is at least $M=M(n,k)>0$. A natural question to ask is: What happens for other sets $Z$?","In particular, for finite, but sufficiently dense sets?}   ","This question was partially answered in ([16],[20-22]).","This study can be naturally related to a certain special settings of the classical Whitney's smooth extension problem.   ","Our goal in the present paper is threefold: first, to provide an overview of the relevant questions and existing results in the general Whitney's problem.","Second, we provide an overview of our specific setting and some available results.","Third, we provide some new results in our direction.","These new results extend the recent result of [21], where an answer to the above question is given via the topological information on $Z$."],"url":"http://arxiv.org/abs/2402.01388v1","category":"math.CA"}
{"created":"2024-02-02 13:10:46","title":"Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis","abstract":"Recent advancements in Large Language Models (LLMs) have enabled collaborative human-bot interactions in Software Engineering (SE), similar to many other professions. However, the potential benefits and implications of incorporating LLMs into qualitative data analysis in SE have not been completely explored. For instance, conducting qualitative data analysis manually can be a time-consuming, effort-intensive, and error-prone task for researchers. LLM-based solutions, such as generative AI models trained on massive datasets, can be utilized to automate tasks in software development as well as in qualitative data analysis. To this end, we utilized LLMs to automate and expedite the qualitative data analysis processes. We employed a multi-agent model, where each agent was tasked with executing distinct, individual research related activities. Our proposed model interpreted large quantities of textual documents and interview transcripts to perform several common tasks used in qualitative analysis. The results show that this technical assistant speeds up significantly the data analysis process, enabling researchers to manage larger datasets much more effectively. Furthermore, this approach introduces a new dimension of scalability and accuracy in qualitative research, potentially transforming data interpretation methodologies in SE.","sentences":["Recent advancements in Large Language Models (LLMs) have enabled collaborative human-bot interactions in Software Engineering (SE), similar to many other professions.","However, the potential benefits and implications of incorporating LLMs into qualitative data analysis in SE have not been completely explored.","For instance, conducting qualitative data analysis manually can be a time-consuming, effort-intensive, and error-prone task for researchers.","LLM-based solutions, such as generative AI models trained on massive datasets, can be utilized to automate tasks in software development as well as in qualitative data analysis.","To this end, we utilized LLMs to automate and expedite the qualitative data analysis processes.","We employed a multi-agent model, where each agent was tasked with executing distinct, individual research related activities.","Our proposed model interpreted large quantities of textual documents and interview transcripts to perform several common tasks used in qualitative analysis.","The results show that this technical assistant speeds up significantly the data analysis process, enabling researchers to manage larger datasets much more effectively.","Furthermore, this approach introduces a new dimension of scalability and accuracy in qualitative research, potentially transforming data interpretation methodologies in SE."],"url":"http://arxiv.org/abs/2402.01386v1","category":"cs.SE"}
{"created":"2024-02-02 13:09:49","title":"Del Visual al Auditivo: Sonorizaci\u00f3n de Escenas Guiada por Imagen","abstract":"Recent advances in image, video, text and audio generative techniques, and their use by the general public, are leading to new forms of content generation. Usually, each modality was approached separately, which poses limitations. The automatic sound recording of visual sequences is one of the greatest challenges for the automatic generation of multimodal content. We present a processing flow that, starting from images extracted from videos, is able to sound them. We work with pre-trained models that employ complex encoders, contrastive learning, and multiple modalities, allowing complex representations of the sequences for their sonorization. The proposed scheme proposes different possibilities for audio mapping and text guidance. We evaluated the scheme on a dataset of frames extracted from a commercial video game and sounds extracted from the Freesound platform. Subjective tests have evidenced that the proposed scheme is able to generate and assign audios automatically and conveniently to images. Moreover, it adapts well to user preferences, and the proposed objective metrics show a high correlation with the subjective ratings.","sentences":["Recent advances in image, video, text and audio generative techniques, and their use by the general public, are leading to new forms of content generation.","Usually, each modality was approached separately, which poses limitations.","The automatic sound recording of visual sequences is one of the greatest challenges for the automatic generation of multimodal content.","We present a processing flow that, starting from images extracted from videos, is able to sound them.","We work with pre-trained models that employ complex encoders, contrastive learning, and multiple modalities, allowing complex representations of the sequences for their sonorization.","The proposed scheme proposes different possibilities for audio mapping and text guidance.","We evaluated the scheme on a dataset of frames extracted from a commercial video game and sounds extracted from the Freesound platform.","Subjective tests have evidenced that the proposed scheme is able to generate and assign audios automatically and conveniently to images.","Moreover, it adapts well to user preferences, and the proposed objective metrics show a high correlation with the subjective ratings."],"url":"http://arxiv.org/abs/2402.01385v1","category":"eess.AS"}
{"created":"2024-02-02 13:09:08","title":"The M\u00f8ller-Plesset adiabatic connection at large coupling strengths for open-shell systems","abstract":"We study the adiabatic connection that has as weak-coupling expansion the M{\\o}ller-Plesset perturbation series, generalizing to the open-shell case previous closed-shell results for the large-coupling limit. We first focus on the hydrogen atom with fractional spins, providing results along the adiabatic connection, from small to large coupling strengths. We reveal an intriguing phase diagram, and an equation for the large-coupling leading order that has closed-form solutions for specific choices of its relevant quantum numbers. We then show that the hydrogen atom results provide variational estimates for the large-coupling leading terms for the general many-electron open-shell case in terms of functionals of the Hartree-Fock spin densities.","sentences":["We study the adiabatic connection that has as weak-coupling expansion the M{\\o}ller-Plesset perturbation series, generalizing to the open-shell case previous closed-shell results for the large-coupling limit.","We first focus on the hydrogen atom with fractional spins, providing results along the adiabatic connection, from small to large coupling strengths.","We reveal an intriguing phase diagram, and an equation for the large-coupling leading order that has closed-form solutions for specific choices of its relevant quantum numbers.","We then show that the hydrogen atom results provide variational estimates for the large-coupling leading terms for the general many-electron open-shell case in terms of functionals of the Hartree-Fock spin densities."],"url":"http://arxiv.org/abs/2402.01384v1","category":"physics.chem-ph"}
{"created":"2024-02-02 13:06:35","title":"LLM-based NLG Evaluation: Current Status and Challenges","abstract":"Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. We also discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several open problems in this area and point out future research directions.","sentences":["Evaluating natural language generation (NLG) is a vital but challenging problem in artificial intelligence.","Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years.","Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled evaluation data.","In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively.","We also discuss human-LLM collaboration for NLG evaluation.","Lastly, we discuss several open problems in this area and point out future research directions."],"url":"http://arxiv.org/abs/2402.01383v1","category":"cs.CL"}
{"created":"2024-02-02 13:06:33","title":"Emergence of heavy tails in homogenized stochastic gradient descent","abstract":"It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates. In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.","sentences":["It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters.","Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index.","We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates.","In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index.","Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima."],"url":"http://arxiv.org/abs/2402.01382v1","category":"stat.ML"}
{"created":"2024-02-02 13:03:15","title":"Regularized boosting with an increasing coefficient magnitude stop criterion as meta-learner in hyperparameter optimization stacking ensemble","abstract":"In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them. This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability. Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed. On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO. It just computes the average over a subset of predictions with replacement. But it does not benefit from the generalization power of a learning process. On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble. Yet, one hardly finds advice about which meta-learner is adequate. Besides, some meta-learners may suffer from the effects of multicollinearity or need to be tuned to reduce them. This paper explores meta-learners for stacking ensemble in HPO, free of hyperparameter tuning, able to reduce the effects of multicollinearity and considering the ensemble learning process generalization power. At this respect, the boosting strategy seems promising as a stacking meta-learner. In fact, it completely removes the effects of multicollinearity. This paper also proposes an implicit regularization in the classical boosting method and a novel non-parametric stop criterion suitable only for boosting and specifically designed for HPO. The synergy between these two improvements over boosting exhibits competitive and promising predictive power performance compared to other existing meta-learners and ensemble approaches for HPO other than the stacking ensemble.","sentences":["In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them.","This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability.","Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed.","On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO.","It just computes the average over a subset of predictions with replacement.","But it does not benefit from the generalization power of a learning process.","On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble.","Yet, one hardly finds advice about which meta-learner is adequate.","Besides, some meta-learners may suffer from the effects of multicollinearity or need to be tuned to reduce them.","This paper explores meta-learners for stacking ensemble in HPO, free of hyperparameter tuning, able to reduce the effects of multicollinearity and considering the ensemble learning process generalization power.","At this respect, the boosting strategy seems promising as a stacking meta-learner.","In fact, it completely removes the effects of multicollinearity.","This paper also proposes an implicit regularization in the classical boosting method and a novel non-parametric stop criterion suitable only for boosting and specifically designed for HPO.","The synergy between these two improvements over boosting exhibits competitive and promising predictive power performance compared to other existing meta-learners and ensemble approaches for HPO other than the stacking ensemble."],"url":"http://arxiv.org/abs/2402.01379v1","category":"cs.LG"}
{"created":"2024-02-02 13:00:38","title":"LoTR: Low Tensor Rank Weight Adaptation","abstract":"In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture. Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update. We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition. Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers. Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models. Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning.","sentences":["In this paper we generalize and extend an idea of low-rank adaptation (LoRA) of large language models (LLMs) based on Transformer architecture.","Widely used LoRA-like methods of fine-tuning LLMs are based on matrix factorization of gradient update.","We introduce LoTR, a novel approach for parameter-efficient fine-tuning of LLMs which represents a gradient update to parameters in a form of tensor decomposition.","Low-rank adapter for each layer is constructed as a product of three matrices, and tensor structure arises from sharing left and right multipliers of this product among layers.","Simultaneous compression of a sequence of layers with low-rank tensor representation allows LoTR to archive even better parameter efficiency then LoRA especially for deep models.","Moreover, the core tensor does not depend on original weight dimension and can be made arbitrary small, which allows for extremely cheap and fast downstream fine-tuning."],"url":"http://arxiv.org/abs/2402.01376v1","category":"cs.CL"}
{"created":"2024-02-02 12:59:27","title":"Dive into the Chasm: Probing the Gap between In- and Cross-Topic Generalization","abstract":"Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics. However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control. This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap. Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs. Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models. Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps. Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios.","sentences":["Pre-trained language models (LMs) perform well in In-Topic setups, where training and testing data come from the same topics.","However, they face challenges in Cross-Topic scenarios where testing data is derived from distinct topics -- such as Gun Control.","This study analyzes various LMs with three probing-based experiments to shed light on the reasons behind the In- vs. Cross-Topic generalization gap.","Thereby, we demonstrate, for the first time, that generalization gaps and the robustness of the embedding space vary significantly across LMs.","Additionally, we assess larger LMs and underscore the relevance of our analysis for recent models.","Overall, diverse pre-training objectives, architectural regularization, or data deduplication contribute to more robust LMs and diminish generalization gaps.","Our research contributes to a deeper understanding and comparison of language models across different generalization scenarios."],"url":"http://arxiv.org/abs/2402.01375v1","category":"cs.CL"}
{"created":"2024-02-02 12:53:20","title":"The Freeness Problem for Automaton Semigroups","abstract":"We show that the freeness problems for automaton semigroups and for automaton monoids are undecidable by giving a reduction from Post's Correspondence Problem. This construction seems to be quite versatile and we also immediately obtain that the problems of testing whether a given automaton semigroup (monoid) is (left) cancellative or whether it is equidivisible are undecidable. We also obtain that it is undecidable whether a given map extends into a homomorphism of automaton semigroups. Finally, we adapt our construction to show that it is undecidable whether a given automaton generates a free monoid whose basis is given by the states (but where we allow one state to act as the identity). In the semigroup case, we show a weaker version of this statement.","sentences":["We show that the freeness problems for automaton semigroups and for automaton monoids are undecidable by giving a reduction from Post's Correspondence Problem.","This construction seems to be quite versatile and we also immediately obtain that the problems of testing whether a given automaton semigroup (monoid) is (left) cancellative or whether it is equidivisible are undecidable.","We also obtain that it is undecidable whether a given map extends into a homomorphism of automaton semigroups.","Finally, we adapt our construction to show that it is undecidable whether a given automaton generates a free monoid whose basis is given by the states (but where we allow one state to act as the identity).","In the semigroup case, we show a weaker version of this statement."],"url":"http://arxiv.org/abs/2402.01372v1","category":"cs.FL"}
{"created":"2024-02-02 12:44:39","title":"CC-VPSTO: Chance-Constrained Via-Point-based Stochastic Trajectory Optimisation for Safe and Efficient Online Robot Motion Planning","abstract":"Safety in the face of uncertainty is a key challenge in robotics. In this work, we propose a real-time capable framework to generate safe and task-efficient robot trajectories for stochastic control problems. For that, we first formulate the problem as a chance-constrained optimisation problem, in which the probability of the controlled system to violate a safety constraint is constrained to be below a user-defined threshold. To solve the chance-constrained optimisation problem, we propose a Monte--Carlo approximation relying on samples of the uncertainty to estimate the probability of violating a safety constraint given a controller. We use this approximation in the motion planner VP-STO to solve the sampled-based problem. Consequently, we refer to our adapted approach as CC-VPSTO, which stands for Chance-Constrained VP-STO. We address the crucial issue concerning the Monte--Carlo approximation: given a predetermined number of uncertainty samples, we propose several ways to define the sample-based problem such that it is a reliable over-approximation of the original problem, i.e. any solution to the sample-based problem adheres to the original chance-constrained problem with high confidence. The strengths of our approach lie in i) its generality, as it does not require any specific assumptions on the underlying uncertainty distribution, the dynamics of the system, the cost function, and for some of the proposed sample-based approximations, on the form of inequality constraints; and ii) its applicability to MPC-settings. We demonstrate the validity and efficiency of our approach on both simulation and real-world robot experiments. For additional material, please visit https://sites.google.com/oxfordrobotics.institute/cc-vpsto.","sentences":["Safety in the face of uncertainty is a key challenge in robotics.","In this work, we propose a real-time capable framework to generate safe and task-efficient robot trajectories for stochastic control problems.","For that, we first formulate the problem as a chance-constrained optimisation problem, in which the probability of the controlled system to violate a safety constraint is constrained to be below a user-defined threshold.","To solve the chance-constrained optimisation problem, we propose a Monte--Carlo approximation relying on samples of the uncertainty to estimate the probability of violating a safety constraint given a controller.","We use this approximation in the motion planner VP-STO to solve the sampled-based problem.","Consequently, we refer to our adapted approach as CC-VPSTO, which stands for Chance-Constrained VP-STO.","We address the crucial issue concerning the Monte--Carlo approximation: given a predetermined number of uncertainty samples, we propose several ways to define the sample-based problem such that it is a reliable over-approximation of the original problem, i.e. any solution to the sample-based problem adheres to the original chance-constrained problem with high confidence.","The strengths of our approach lie in i) its generality, as it does not require any specific assumptions on the underlying uncertainty distribution, the dynamics of the system, the cost function, and for some of the proposed sample-based approximations, on the form of inequality constraints; and ii) its applicability to MPC-settings.","We demonstrate the validity and efficiency of our approach on both simulation and real-world robot experiments.","For additional material, please visit https://sites.google.com/oxfordrobotics.institute/cc-vpsto."],"url":"http://arxiv.org/abs/2402.01370v1","category":"cs.RO"}
{"created":"2024-02-02 12:39:49","title":"Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors","abstract":"Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks the first successful attempt of transfer-based attack to commercial T2I models. Our code is publicly available at \\url{https://github.com/ydc123/MMP-Attack}.","sentences":["Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities.","However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt.","Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance.","Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work.","Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object.","The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3.","To the best of our knowledge, this marks the first successful attempt of transfer-based attack to commercial T2I models.","Our code is publicly available at \\url{https://github.com/ydc123/MMP-Attack}."],"url":"http://arxiv.org/abs/2402.01369v1","category":"cs.LG"}
{"created":"2024-02-02 12:37:59","title":"Periodicity and pure periodicity in alternate base systems","abstract":"We study the Cantor real base numeration system which is a common generalization of two positional systems, namely the Cantor system with a sequence of integer bases and the R\\'enyi system with one real base. We focus on the so-called alternate base $B$ given by a purely periodic sequence of real numbers greater than 1. We answer an open question of Charlier et al. on the set of numbers with eventually periodic $B$-expansions. We also investigate for which bases all sufficiently small rationals have a purely periodic $B$-expansion.","sentences":["We study the Cantor real base numeration system which is a common generalization of two positional systems, namely the Cantor system with a sequence of integer bases and the R\\'enyi system with one real base.","We focus on the so-called alternate base $B$ given by a purely periodic sequence of real numbers greater than 1.","We answer an open question of Charlier et al. on the set of numbers with eventually periodic $B$-expansions.","We also investigate for which bases all sufficiently small rationals have a purely periodic $B$-expansion."],"url":"http://arxiv.org/abs/2402.01367v1","category":"math.NT"}
{"created":"2024-02-02 12:34:09","title":"Continual Learning for Large Language Models: A Survey","abstract":"Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.","sentences":["Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale.","However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge.","This paper surveys recent works on continual learning for LLMs.","Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment.","We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing.","Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task."],"url":"http://arxiv.org/abs/2402.01364v1","category":"cs.CL"}
{"created":"2024-02-02 12:30:19","title":"Catching thermal avalanches in disordered XXZ model","abstract":"We study the XXZ model with a random magnetic field in contact with a weakly disordered spin chain, acting as a finite thermal bath. We revise Fermi's golden rule description of the interaction between the thermal bath and the XXZ spin chain, contrasting it with a non-perturbative quantum avalanche scenario for the thermalization of the system. We employ two-point correlation functions to define the extent, $\\xi_d$, of the thermalized region next to the bath. Unbounded growth of $\\xi_d$ proportional to the logarithm of time or faster is a signature of an avalanche. It signifies the thermalization of the system, as we confirm numerically for a generic initial state in the ergodic and critical regimes of the XXZ spin chain. In the many-body localized regime, a clear termination of avalanches is observed for specifically prepared initial states and, surprisingly, is not visible for generic initial product states. Additionally, we extract the localization length of the local integrals of motion and show that a bath made out of a weakly disordered XXZ chain has a similar effect on the system as a bath modeled by a Hamiltonian from a Gaussian Orthogonal Ensemble of random matrices. We also comment on the result of the earlier study (Phys. Rev. B 108, L020201 (2023)), arguing that the observed thermalization is due to external driving of the system and does not occur in the autonomous model. Our work reveals experimentally accessible signatures of quantum avalanches and identifies conditions under which termination of the avalanches may be observed.","sentences":["We study the XXZ model with a random magnetic field in contact with a weakly disordered spin chain, acting as a finite thermal bath.","We revise Fermi's golden rule description of the interaction between the thermal bath and the XXZ spin chain, contrasting it with a non-perturbative quantum avalanche scenario for the thermalization of the system.","We employ two-point correlation functions to define the extent, $\\xi_d$, of the thermalized region next to the bath.","Unbounded growth of $\\xi_d$ proportional to the logarithm of time or faster is a signature of an avalanche.","It signifies the thermalization of the system, as we confirm numerically for a generic initial state in the ergodic and critical regimes of the XXZ spin chain.","In the many-body localized regime, a clear termination of avalanches is observed for specifically prepared initial states and, surprisingly, is not visible for generic initial product states.","Additionally, we extract the localization length of the local integrals of motion and show that a bath made out of a weakly disordered XXZ chain has a similar effect on the system as a bath modeled by a Hamiltonian from a Gaussian Orthogonal Ensemble of random matrices.","We also comment on the result of the earlier study (Phys. Rev. B 108, L020201 (2023)), arguing that the observed thermalization is due to external driving of the system and does not occur in the autonomous model.","Our work reveals experimentally accessible signatures of quantum avalanches and identifies conditions under which termination of the avalanches may be observed."],"url":"http://arxiv.org/abs/2402.01362v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-02 12:24:40","title":"Holographic complexity of the extended Schwarzschild-de Sitter space","abstract":"According to static patch holography, de Sitter space admits a unitary quantum description in terms of a dual theory living on the stretched horizon, that is a timelike surface close to the cosmological horizon. In this manuscript, we compute several holographic complexity conjectures in a periodic extension of the Schwarzschild-de Sitter black hole. We consider multiple configurations of the stretched horizons to which geometric objects are anchored. The holographic complexity proposals admit a hyperfast growth when the gravitational observables only lie in the cosmological patch, except for a class of complexity=anything observables that admit a linear growth. All the complexity conjectures present a linear increase when restricted to the black hole patch, similar to the AdS case. When both the black hole and the cosmological regions are probed, codimension-zero proposals are time-independent, while codimension-one proposals can have non-trivial evolution with linear increase at late times. As a byproduct of our analysis, we find that codimension-one spacelike surfaces are highly constrained in Schwarzschild-de Sitter space. Therefore, different locations of the stretched horizon give rise to different behaviours of the complexity conjectures.","sentences":["According to static patch holography, de Sitter space admits a unitary quantum description in terms of a dual theory living on the stretched horizon, that is a timelike surface close to the cosmological horizon.","In this manuscript, we compute several holographic complexity conjectures in a periodic extension of the Schwarzschild-de Sitter black hole.","We consider multiple configurations of the stretched horizons to which geometric objects are anchored.","The holographic complexity proposals admit a hyperfast growth when the gravitational observables only lie in the cosmological patch, except for a class of complexity=anything observables that admit a linear growth.","All the complexity conjectures present a linear increase when restricted to the black hole patch, similar to the AdS case.","When both the black hole and the cosmological regions are probed, codimension-zero proposals are time-independent, while codimension-one proposals can have non-trivial evolution with linear increase at late times.","As a byproduct of our analysis, we find that codimension-one spacelike surfaces are highly constrained in Schwarzschild-de Sitter space.","Therefore, different locations of the stretched horizon give rise to different behaviours of the complexity conjectures."],"url":"http://arxiv.org/abs/2402.01357v1","category":"hep-th"}
{"created":"2024-02-02 12:22:59","title":"Algebraic properties of the ring $C(X)_\\mathcal{P}$","abstract":"Our aim is to study certain algebraic properties of the ring $C(X)_\\mathcal{P}$ of real-valued functions on $X$ whose closure of discontinuity set is in an ideal of closed sets. We characterize $\\mathcal{P}P$-spaces using $z$-ideals and essential ideals of $C(X)_\\mathcal{P}$ and also almost $\\mathcal{P}P$-spaces using $z^0$-ideals of $C(X)_\\mathcal{P}$ and a topology finer than the original topology on $X$. We deduce that each maximal ideal of $C(X)_F$ \\cite{GGT2018} (resp. $T'(X)$ \\cite{A2010}) is a $z^0$-ideal. We establish that the notions of clean ring, weakly clean ring, semiclean ring, almost clean ring and exchange ring coincide in the ring $C(X)_\\mathcal{P}$. End of this paper, we also characterize $\\mathcal{P}P$-spaces and almost $\\mathcal{P}P$-spaces using certain ideals having depth zero. We exhibit a condition on $\\mathcal{P}$ under which prime and essential ideals of $C(X)_\\mathcal{P}$ have depth zero.","sentences":["Our aim is to study certain algebraic properties of the ring $C(X)_\\mathcal{P}$ of real-valued functions on $X$ whose closure of discontinuity set is in an ideal of closed sets.","We characterize $\\mathcal{P}P$-spaces using $z$-ideals and essential ideals of $C(X)_\\mathcal{P}$ and also almost $\\mathcal{P}P$-spaces using $z^0$-ideals of $C(X)_\\mathcal{P}$ and a topology finer than the original topology on $X$. We deduce that each maximal ideal of $C(X)_F$ \\cite{GGT2018} (resp.","$T'(X)$ \\cite{A2010}) is a $z^0$-ideal.","We establish that the notions of clean ring, weakly clean ring, semiclean ring, almost clean ring and exchange ring coincide in the ring $C(X)_\\mathcal{P}$. End of this paper, we also characterize $\\mathcal{P}P$-spaces and almost $\\mathcal{P}P$-spaces using certain ideals having depth zero.","We exhibit a condition on $\\mathcal{P}$ under which prime and essential ideals of $C(X)_\\mathcal{P}$ have depth zero."],"url":"http://arxiv.org/abs/2402.01356v1","category":"math.GN"}
{"created":"2024-02-02 12:22:41","title":"FindingEmo: An Image Dataset for Emotion Recognition in the Wild","abstract":"We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.","sentences":["We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition.","Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals.","Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific.","Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code."],"url":"http://arxiv.org/abs/2402.01355v1","category":"cs.CV"}
{"created":"2024-02-02 12:20:41","title":"Forecasting Volatility of Oil-based Commodities: The Model of Dynamic Persistence","abstract":"Time variation and persistence are crucial properties of volatility that are often studied separately in oil-based volatility forecasting models. Here, we propose a novel approach that allows shocks with heterogeneous persistence to vary smoothly over time, and thus model the two together. We argue that this is important because such dynamics arise naturally from the dynamic nature of shocks in oil-based commodities. We identify such dynamics from the data using localised regressions and build a model that significantly improves volatility forecasts. Such forecasting models, based on a rich persistence structure that varies smoothly over time, outperform state-of-the-art benchmark models and are particularly useful for forecasting over longer horizons.","sentences":["Time variation and persistence are crucial properties of volatility that are often studied separately in oil-based volatility forecasting models.","Here, we propose a novel approach that allows shocks with heterogeneous persistence to vary smoothly over time, and thus model the two together.","We argue that this is important because such dynamics arise naturally from the dynamic nature of shocks in oil-based commodities.","We identify such dynamics from the data using localised regressions and build a model that significantly improves volatility forecasts.","Such forecasting models, based on a rich persistence structure that varies smoothly over time, outperform state-of-the-art benchmark models and are particularly useful for forecasting over longer horizons."],"url":"http://arxiv.org/abs/2402.01354v1","category":"q-fin.GN"}
{"created":"2024-02-02 12:11:16","title":"Describing Images $\\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes","abstract":"There is an intricate relation between the properties of an image and how humans behave while describing the image. This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image. Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation. Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other. Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation. Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for humans and what leads to variations in human outputs.","sentences":["There is an intricate relation between the properties of an image and how humans behave while describing the image.","This behavior shows ample variation, as manifested in human signals such as eye movements and when humans start to describe the image.","Despite the value of such signals of visuo-linguistic variation, they are virtually disregarded in the training of current pretrained models, which motivates further investigation.","Using a corpus of Dutch image descriptions with concurrently collected eye-tracking data, we explore the nature of the variation in visuo-linguistic signals, and find that they correlate with each other.","Given this result, we hypothesize that variation stems partly from the properties of the images, and explore whether image representations encoded by pretrained vision encoders can capture such variation.","Our results indicate that pretrained models do so to a weak-to-moderate degree, suggesting that the models lack biases about what makes a stimulus complex for humans and what leads to variations in human outputs."],"url":"http://arxiv.org/abs/2402.01352v1","category":"cs.CL"}
{"created":"2024-02-02 12:09:20","title":"FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning","abstract":"Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. The local gating network produces personalized weights for extracted representations from both experts on each data sample. The three models form a local heterogeneous MoE. The weighted mixed representation fuses global generalized and local personalized features and is processed by the local heterogeneous large model's header with personalized prediction information for output. The MoE and prediction header are updated synchronously. (2) The trained local homogeneous small feature extractors are sent to the server for cross-client information fusion via aggregation. Briefly, FedMoE first enhances local model personalization at a fine-grained data level while supporting model heterogeneity.","sentences":["Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity.","This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL).","However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods.","To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs).","It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model.","(1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction.","The local gating network produces personalized weights for extracted representations from both experts on each data sample.","The three models form a local heterogeneous MoE.","The weighted mixed representation fuses global generalized and local personalized features and is processed by the local heterogeneous large model's header with personalized prediction information for output.","The MoE and prediction header are updated synchronously.","(2) The trained local homogeneous small feature extractors are sent to the server for cross-client information fusion via aggregation.","Briefly, FedMoE first enhances local model personalization at a fine-grained data level while supporting model heterogeneity."],"url":"http://arxiv.org/abs/2402.01350v1","category":"cs.LG"}
{"created":"2024-02-02 12:07:00","title":"Beyond the Answers: Reviewing the Rationality of Multiple Choice Question Answering for the Evaluation of Large Language Models","abstract":"In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks. Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community. Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction. This study investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions. Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need for more robust evaluation mechanisms in assessing the performance of LLMs.","sentences":["In the field of natural language processing (NLP), Large Language Models (LLMs) have precipitated a paradigm shift, markedly enhancing performance in natural language generation tasks.","Despite these advancements, the comprehensive evaluation of LLMs remains an inevitable challenge for the community.","Recently, the utilization of Multiple Choice Question Answering (MCQA) as a benchmark for LLMs has gained considerable traction.","This study investigates the rationality of MCQA as an evaluation method for LLMs.","If LLMs genuinely understand the semantics of questions, their performance should exhibit consistency across the varied configurations derived from the same questions.","Contrary to this expectation, our empirical findings suggest a notable disparity in the consistency of LLM responses, which we define as REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current MCQA-based benchmarks may not adequately capture the true capabilities of LLMs, which underscores the need for more robust evaluation mechanisms in assessing the performance of LLMs."],"url":"http://arxiv.org/abs/2402.01349v1","category":"cs.CL"}
{"created":"2024-02-02 12:04:44","title":"CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay","abstract":"This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information. Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer. In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes. CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection. The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer. Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%. Additionally, it significantly enhances the accuracy of the poorest-performing task by 6.30% compared to the top baseline.","sentences":["This paper introduces a novel perspective to significantly mitigate catastrophic forgetting in continuous learning (CL), which emphasizes models' capacity to preserve existing knowledge and assimilate new information.","Current replay-based methods treat every task and data sample equally and thus can not fully exploit the potential of the replay buffer.","In response, we propose COgnitive REplay (CORE), which draws inspiration from human cognitive review processes.","CORE includes two key strategies: Adaptive Quantity Allocation and Quality-Focused Data Selection.","The former adaptively modulates the replay buffer allocation for each task based on its forgetting rate, while the latter guarantees the inclusion of representative data that best encapsulates the characteristics of each task within the buffer.","Our approach achieves an average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline method by 6.52%.","Additionally, it significantly enhances the accuracy of the poorest-performing task by 6.30% compared to the top baseline."],"url":"http://arxiv.org/abs/2402.01348v1","category":"cs.LG"}
{"created":"2024-02-02 12:04:34","title":"The extremal generalised Randi\u0107 index for a given degree range","abstract":"O and Shi proved that the Randi\\'c index of any graph $G$ with minimum degree at least $\\delta$ and maximum degree at most $\\Delta$ is at least $\\frac{\\sqrt{\\delta\\Delta}}{\\delta+\\Delta}|G|$, with equality if and only if the graph is $(\\delta,\\Delta)$-biregular. In this note we give a short proof via a more general statement. We apply the latter to classify the graphs in any given degree range which minimise (or maximise) the generalised Randi\\'c index for any exponent, and describe the transitions between different types of behaviour precisely.","sentences":["O and Shi proved that the Randi\\'c index of any graph $G$ with minimum degree at least $\\delta$ and maximum degree at most $\\Delta$ is at least $\\frac{\\sqrt{\\delta\\Delta}}{\\delta+\\Delta}|G|$, with equality if and only if the graph is $(\\delta,\\Delta)$-biregular.","In this note we give a short proof via a more general statement.","We apply the latter to classify the graphs in any given degree range which minimise (or maximise) the generalised Randi\\'c index for any exponent, and describe the transitions between different types of behaviour precisely."],"url":"http://arxiv.org/abs/2402.01346v1","category":"math.CO"}
{"created":"2024-02-02 12:02:46","title":"Skip $\\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models","abstract":"Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\\textbackslash n\\textbackslash n$'), where the content before and after '$\\textbackslash n\\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\\textbackslash n\\textbackslash n$' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '$\\textbackslash n\\textbackslash n$'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '$\\textbackslash n\\textbackslash n$' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of `\\textbackslash n'.","sentences":["Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language.","Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information.","However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored.","In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations.","Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\\textbackslash n\\textbackslash n$'), where the content before and after '$\\textbackslash n\\textbackslash n$' in the training data frequently exhibit significant semantic changes.","This pattern leads the model to infer that the contents following '$\\textbackslash n\\textbackslash n$' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '$\\textbackslash n\\textbackslash n$'.","We have validated this hypothesis on multiple publicly available LVLMs.","Besides, we find that deliberately inserting '$\\textbackslash n\\textbackslash n$' at the generated description can induce more hallucinations.","A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of `\\textbackslash n'."],"url":"http://arxiv.org/abs/2402.01345v1","category":"cs.CV"}
{"created":"2024-02-02 11:57:53","title":"Shapelet-based Model-agnostic Counterfactual Local Explanations for Time Series Classification","abstract":"In this work, we propose a model-agnostic instance-based post-hoc explainability method for time series classification. The proposed algorithm, namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual explanations for arbitrary time series classifiers. We validate the proposed method on several real-world univariate time series classification tasks from the UCR Time Series Archive. The results indicate that the counterfactual instances generated by Time-CF when compared to state-of-the-art methods, demonstrate better performance in terms of four explainability metrics: closeness, sensibility, plausibility, and sparsity.","sentences":["In this work, we propose a model-agnostic instance-based post-hoc explainability method for time series classification.","The proposed algorithm, namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual explanations for arbitrary time series classifiers.","We validate the proposed method on several real-world univariate time series classification tasks from the UCR Time Series Archive.","The results indicate that the counterfactual instances generated by Time-CF when compared to state-of-the-art methods, demonstrate better performance in terms of four explainability metrics: closeness, sensibility, plausibility, and sparsity."],"url":"http://arxiv.org/abs/2402.01343v1","category":"cs.LG"}
{"created":"2024-02-02 11:57:50","title":"Training-time Neuron Alignment through Permutation Subspace for Improving Linear Mode Connectivity and Model Fusion","abstract":"In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape. Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms. Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation. However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices. Thus, in this paper, we study training-time neuron alignment. Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free. We find that pruning at initialization supports this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training. TNA-PFN is theoretically and empirically validated for reducing LMC barriers. It excels in wide model fusion applications, especially in federated learning, two algorithms based on TNA-FPN that are proposed to show its prospects even under heterogeneous datasets. Moreover, TNA-PFN can enhance the generalization of model soup for vision transformers and ColD fusion for pretrained language models.","sentences":["In deep learning, stochastic gradient descent often yields functionally similar yet widely scattered solutions in the weight space even under the same initialization, causing barriers in the Linear Mode Connectivity (LMC) landscape.","Overcoming these barriers is crucial for understanding deep learning dynamics and enhancing model-fusion algorithms.","Previous studies highlight the role of permutation symmetry in reducing post-training barriers through network permutation.","However, these post-hoc methods, demanding extra computations, are less effective for larger, complex models (e.g., ViT, LLM) due to numerous permutation matrices.","Thus, in this paper, we study training-time neuron alignment.","Our hypothesis suggests that training-time permutation subspace can reduce LMC barriers for free.","We find that pruning at initialization supports this.","Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm using a partial gradient mask during training.","TNA-PFN is theoretically and empirically validated for reducing LMC barriers.","It excels in wide model fusion applications, especially in federated learning, two algorithms based on TNA-FPN that are proposed to show its prospects even under heterogeneous datasets.","Moreover, TNA-PFN can enhance the generalization of model soup for vision transformers and ColD fusion for pretrained language models."],"url":"http://arxiv.org/abs/2402.01342v1","category":"cs.LG"}
{"created":"2024-02-02 11:40:27","title":"Simulator-Free Visual Domain Randomization via Video Games","abstract":"Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content. Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability. This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines. Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content. We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization. BehAVE successfully aligns player behavioral patterns and is able to zero-shot transfer them to multiple unseen FPS games when trained on just one FPS game. In a more challenging setting, BehAVE manages to improve the zero-shot transferability of foundation models to unseen FPS games (up to 22%) even when trained on a game of a different genre (Minecraft). Code and dataset can be found at https://github.com/nrasajski/BehAVE.","sentences":["Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content.","Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability.","This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines.","Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content.","We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization.","BehAVE successfully aligns player behavioral patterns and is able to zero-shot transfer them to multiple unseen FPS games when trained on just one FPS game.","In a more challenging setting, BehAVE manages to improve the zero-shot transferability of foundation models to unseen FPS games (up to 22%) even when trained on a game of a different genre (Minecraft).","Code and dataset can be found at https://github.com/nrasajski/BehAVE."],"url":"http://arxiv.org/abs/2402.01335v1","category":"cs.CV"}
{"created":"2024-02-02 11:33:05","title":"A general framework for rotation invariant point cloud analysis","abstract":"We propose a general method for deep learning based point cloud analysis, which is invariant to rotation on the inputs. Classical methods are vulnerable to rotation, as they usually take aligned point clouds as input. Principle Component Analysis (PCA) is a practical approach to achieve rotation invariance. However, there are still some gaps between theory and practical algorithms. In this work, we present a thorough study on designing rotation invariant algorithms for point cloud analysis. We first formulate it as a permutation invariant problem, then propose a general framework which can be combined with any backbones. Our method is beneficial for further research such as 3D pre-training and multi-modal learning. Experiments show that our method has considerable or better performance compared to state-of-the-art approaches on common benchmarks. Code is available at https://github.com/luoshuqing2001/RI_framework.","sentences":["We propose a general method for deep learning based point cloud analysis, which is invariant to rotation on the inputs.","Classical methods are vulnerable to rotation, as they usually take aligned point clouds as input.","Principle Component Analysis (PCA) is a practical approach to achieve rotation invariance.","However, there are still some gaps between theory and practical algorithms.","In this work, we present a thorough study on designing rotation invariant algorithms for point cloud analysis.","We first formulate it as a permutation invariant problem, then propose a general framework which can be combined with any backbones.","Our method is beneficial for further research such as 3D pre-training and multi-modal learning.","Experiments show that our method has considerable or better performance compared to state-of-the-art approaches on common benchmarks.","Code is available at https://github.com/luoshuqing2001/RI_framework."],"url":"http://arxiv.org/abs/2402.01331v1","category":"cs.CV"}
{"created":"2024-02-02 11:26:18","title":"Supervised Algorithmic Fairness in Distribution Shifts: A Survey","abstract":"Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant challenges, and identify potential directions for future studies.","sentences":["Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains.","In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors.","This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender.","In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature.","Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies.","We further explore the interconnection with related research fields, discuss the significant challenges, and identify potential directions for future studies."],"url":"http://arxiv.org/abs/2402.01327v1","category":"cs.LG"}
{"created":"2024-02-02 11:18:50","title":"A generalized Sonine condition","abstract":"Sonine kernel is characterized by the Sonine condition and is an important class of kernels in nonlocal differential equations and integral equations. This work proposes a generalized Sonine condition, which extends the classical Sonine theory and accommodates more kernels in nonlocal differential equations and integral equations for practical applications. A typical example of such kernel is given, and the first-kind Volterra integral equation with a generalized Sonine kernel is transformed and then analyzed to demonstrate the usage of the generalized Sonine condition.","sentences":["Sonine kernel is characterized by the Sonine condition and is an important class of kernels in nonlocal differential equations and integral equations.","This work proposes a generalized Sonine condition, which extends the classical Sonine theory and accommodates more kernels in nonlocal differential equations and integral equations for practical applications.","A typical example of such kernel is given, and the first-kind Volterra integral equation with a generalized Sonine kernel is transformed and then analyzed to demonstrate the usage of the generalized Sonine condition."],"url":"http://arxiv.org/abs/2402.01323v1","category":"math.CA"}
{"created":"2024-02-02 11:18:26","title":"Large fluctuations in NSPT computations: a lesson from $O(N)$ non-linear sigma models","abstract":"In the last three decades, Numerical Stochastic Perturbation Theory (NSPT) has proven to be an excellent tool for calculating perturbative expansions in theories such as Lattice QCD, for which standard, diagrammatic perturbation theory is known to be cumbersome. Despite the significant success of this stochastic method and the improvements made in recent years, NSPT apparently cannot be successfully implemented in low-dimensional models due to the emergence of huge statistical fluctuations: as the perturbative order gets higher, the signal to noise ratio is simply not good enough. This does not come as a surprise, but on very general grounds, one would expect that the larger the number of degrees of freedom, the less severe the fluctuations will be. By simulating $2D$ $O(N)$ non-linear sigma models for different values of $N$, we show that indeed the fluctuations are tamed in the large $N$ limit, meeting our expectations. Having established this, we conclude discussing interesting applications of NSPT in the context of these theories.","sentences":["In the last three decades, Numerical Stochastic Perturbation Theory (NSPT) has proven to be an excellent tool for calculating perturbative expansions in theories such as Lattice QCD, for which standard, diagrammatic perturbation theory is known to be cumbersome.","Despite the significant success of this stochastic method and the improvements made in recent years, NSPT apparently cannot be successfully implemented in low-dimensional models due to the emergence of huge statistical fluctuations: as the perturbative order gets higher, the signal to noise ratio is simply not good enough.","This does not come as a surprise, but on very general grounds, one would expect that the larger the number of degrees of freedom, the less severe the fluctuations will be.","By simulating $2D$ $O(N)$ non-linear sigma models for different values of $N$, we show that indeed the fluctuations are tamed in the large $N$ limit, meeting our expectations.","Having established this, we conclude discussing interesting applications of NSPT in the context of these theories."],"url":"http://arxiv.org/abs/2402.01322v1","category":"hep-lat"}
{"created":"2024-02-02 11:13:14","title":"Vector Dark Matter with Higgs Portal in Type II Seesaw framework","abstract":"We study the phenomenology of a vector dark matter (VDM) in a $U(1)_X$ gauged extension of the Standard Model (SM) which is connected to the type II seesaw framework via the Higgs portal. When this $U(1)_X$ symmetry is spontaneously broken by the vacuum expectation value (VEV) of a complex scalar singlet, the gauge boson $Z^\\prime$ becomes massive. The stability of the dark matter (DM) is ensured by the introduction of an exact charge conjugation symmetry. On the other hand, the $SU(2)_L$ triplet scalar generates light neutrino masses through the type II seesaw mechanism. We have studied the phenomenology of the usual WIMP DM considering all possible theoretical and experimental constraints that are applicable. Due to the presence of triplet scalar, our scenario can accommodate the observed $2\\sigma$ deviation in $h \\to Z \\gamma $ decay. We have also briefly discussed the possibility of non-thermal production of DM from the decay of the same complex scalar that is responsible for the breaking of this $U(1)_X$ symmetry.","sentences":["We study the phenomenology of a vector dark matter (VDM) in a $U(1)_X$ gauged extension of the Standard Model (SM) which is connected to the type II seesaw framework via the Higgs portal.","When this $U(1)_X$ symmetry is spontaneously broken by the vacuum expectation value (VEV) of a complex scalar singlet, the gauge boson $Z^\\prime$ becomes massive.","The stability of the dark matter (DM) is ensured by the introduction of an exact charge conjugation symmetry.","On the other hand, the $SU(2)_L$ triplet scalar generates light neutrino masses through the type II seesaw mechanism.","We have studied the phenomenology of the usual WIMP DM considering all possible theoretical and experimental constraints that are applicable.","Due to the presence of triplet scalar, our scenario can accommodate the observed $2\\sigma$ deviation in $h \\to Z \\gamma $ decay.","We have also briefly discussed the possibility of non-thermal production of DM from the decay of the same complex scalar that is responsible for the breaking of this $U(1)_X$ symmetry."],"url":"http://arxiv.org/abs/2402.01317v1","category":"hep-ph"}
{"created":"2024-02-02 11:07:27","title":"AutoGCN -- Towards Generic Human Activity Recognition with Neural Architecture Search","abstract":"This paper introduces AutoGCN, a generic Neural Architecture Search (NAS) algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks (GCNs). HAR has gained attention due to advances in deep learning, increased data availability, and enhanced computational capabilities. At the same time, GCNs have shown promising results in modeling relationships between body key points in a skeletal graph. While domain experts often craft dataset-specific GCN-based methods, their applicability beyond this specific context is severely limited. AutoGCN seeks to address this limitation by simultaneously searching for the ideal hyperparameters and architecture combination within a versatile search space using a reinforcement controller while balancing optimal exploration and exploitation behavior with a knowledge reservoir during the search process. We conduct extensive experiments on two large-scale datasets focused on skeleton-based action recognition to assess the proposed algorithm's performance. Our experimental results underscore the effectiveness of AutoGCN in constructing optimal GCN architectures for HAR, outperforming conventional NAS and GCN methods, as well as random search. These findings highlight the significance of a diverse search space and an expressive input representation to enhance the network performance and generalizability.","sentences":["This paper introduces AutoGCN, a generic Neural Architecture Search (NAS) algorithm for Human Activity Recognition (HAR) using Graph Convolution Networks (GCNs).","HAR has gained attention due to advances in deep learning, increased data availability, and enhanced computational capabilities.","At the same time, GCNs have shown promising results in modeling relationships between body key points in a skeletal graph.","While domain experts often craft dataset-specific GCN-based methods, their applicability beyond this specific context is severely limited.","AutoGCN seeks to address this limitation by simultaneously searching for the ideal hyperparameters and architecture combination within a versatile search space using a reinforcement controller while balancing optimal exploration and exploitation behavior with a knowledge reservoir during the search process.","We conduct extensive experiments on two large-scale datasets focused on skeleton-based action recognition to assess the proposed algorithm's performance.","Our experimental results underscore the effectiveness of AutoGCN in constructing optimal GCN architectures for HAR, outperforming conventional NAS and GCN methods, as well as random search.","These findings highlight the significance of a diverse search space and an expressive input representation to enhance the network performance and generalizability."],"url":"http://arxiv.org/abs/2402.01313v1","category":"cs.CV"}
{"created":"2024-02-02 11:06:46","title":"Wave-packet dynamics in non-Hermitian systems subject to complex electric fields","abstract":"Berry phases have long been known to significantly alter the properties of periodic systems, giving rise to anomalous terms in the semiclassical equations of motion describing wave-packet dynamics. In non-Hermitian systems, generalizations of the Berry connection have been proposed and shown to have novel effects on dynamics and transport. In this work, we expand upon these results by deriving the full set of semiclassical equations of motion for wave-packet dynamics in a non-Hermitian system subject to complex external electric fields, which are realizable as gain gradients. We show that the non-Hermiticities of both the band Hamiltonian and the external potential give rise to anomalous weight rate and velocity terms which depend on the geometric properties of the eigenfunctions, including the quantum metric tensor. These analytical results are compared with numerical lattice simulations which reveal these anomalous terms even in one-dimension. Our work expands the range of phenomena expected to be detectable in experimental setups, which should be realizable in currently available metamaterials and classical wave systems, including mechanical, acoustic, and optical.","sentences":["Berry phases have long been known to significantly alter the properties of periodic systems, giving rise to anomalous terms in the semiclassical equations of motion describing wave-packet dynamics.","In non-Hermitian systems, generalizations of the Berry connection have been proposed and shown to have novel effects on dynamics and transport.","In this work, we expand upon these results by deriving the full set of semiclassical equations of motion for wave-packet dynamics in a non-Hermitian system subject to complex external electric fields, which are realizable as gain gradients.","We show that the non-Hermiticities of both the band Hamiltonian and the external potential give rise to anomalous weight rate and velocity terms which depend on the geometric properties of the eigenfunctions, including the quantum metric tensor.","These analytical results are compared with numerical lattice simulations which reveal these anomalous terms even in one-dimension.","Our work expands the range of phenomena expected to be detectable in experimental setups, which should be realizable in currently available metamaterials and classical wave systems, including mechanical, acoustic, and optical."],"url":"http://arxiv.org/abs/2402.01312v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-02 10:53:36","title":"KTO: Model Alignment as Prospect Theoretic Optimization","abstract":"Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether an output is desirable or undesirable for a given input. This makes it far easier to use in the real world, where preference data is scarce and expensive.","sentences":["Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse.","We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\\textit{human-aware loss functions}$ (HALOs).","However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature.","Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do.","We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether an output is desirable or undesirable for a given input.","This makes it far easier to use in the real world, where preference data is scarce and expensive."],"url":"http://arxiv.org/abs/2402.01306v1","category":"cs.LG"}
{"created":"2024-02-02 10:48:43","title":"Phrase Grounding-based Style Transfer for Single-Domain Generalized Object Detection","abstract":"Single-domain generalized object detection aims to enhance a model's generalizability to multiple unseen target domains using only data from a single source domain during training. This is a practical yet challenging task as it requires the model to address domain shift without incorporating target domain data into training. In this paper, we propose a novel phrase grounding-based style transfer (PGST) approach for the task. Specifically, we first define textual prompts to describe potential objects for each unseen target domain. Then, we leverage the grounded language-image pre-training (GLIP) model to learn the style of these target domains and achieve style transfer from the source to the target domain. The style-transferred source visual features are semantically rich and could be close to imaginary counterparts in the target domain. Finally, we employ these style-transferred visual features to fine-tune GLIP. By introducing imaginary counterparts, the detector could be effectively generalized to unseen target domains using only a single source domain for training. Extensive experimental results on five diverse weather driving benchmarks demonstrate our proposed approach achieves state-of-the-art performance, even surpassing some domain adaptive methods that incorporate target domain images into the training process.The source codes and pre-trained models will be made available.","sentences":["Single-domain generalized object detection aims to enhance a model's generalizability to multiple unseen target domains using only data from a single source domain during training.","This is a practical yet challenging task as it requires the model to address domain shift without incorporating target domain data into training.","In this paper, we propose a novel phrase grounding-based style transfer (PGST) approach for the task.","Specifically, we first define textual prompts to describe potential objects for each unseen target domain.","Then, we leverage the grounded language-image pre-training (GLIP) model to learn the style of these target domains and achieve style transfer from the source to the target domain.","The style-transferred source visual features are semantically rich and could be close to imaginary counterparts in the target domain.","Finally, we employ these style-transferred visual features to fine-tune GLIP.","By introducing imaginary counterparts, the detector could be effectively generalized to unseen target domains using only a single source domain for training.","Extensive experimental results on five diverse weather driving benchmarks demonstrate our proposed approach achieves state-of-the-art performance, even surpassing some domain adaptive methods that incorporate target domain images into the training process.","The source codes and pre-trained models will be made available."],"url":"http://arxiv.org/abs/2402.01304v1","category":"cs.CV"}
{"created":"2024-02-02 10:47:08","title":"AGILE: Approach-based Grasp Inference Learned from Element Decomposition","abstract":"Humans, this species expert in grasp detection, can grasp objects by taking into account hand-object positioning information. This work proposes a method to enable a robot manipulator to learn the same, grasping objects in the most optimal way according to how the gripper has approached the object. Built on deep learning, the proposed method consists of two main stages. In order to generalize the network on unseen objects, the proposed Approach-based Grasping Inference involves an element decomposition stage to split an object into its main parts, each with one or more annotated grasps for a particular approach of the gripper. Subsequently, a grasp detection network utilizes the decomposed elements by Mask R-CNN and the information on the approach of the gripper in order to detect the element the gripper has approached and the most optimal grasp. In order to train the networks, the study introduces a robotic grasping dataset collected in the Coppeliasim simulation environment. The dataset involves 10 different objects with annotated element decomposition masks and grasp rectangles. The proposed method acquires a 90% grasp success rate on seen objects and 78% on unseen objects in the Coppeliasim simulation environment. Lastly, simulation-to-reality domain adaptation is performed by applying transformations on the training set collected in simulation and augmenting the dataset, which results in a 70% physical grasp success performance using a Delta parallel robot and a 2 -fingered gripper.","sentences":["Humans, this species expert in grasp detection, can grasp objects by taking into account hand-object positioning information.","This work proposes a method to enable a robot manipulator to learn the same, grasping objects in the most optimal way according to how the gripper has approached the object.","Built on deep learning, the proposed method consists of two main stages.","In order to generalize the network on unseen objects, the proposed Approach-based Grasping Inference involves an element decomposition stage to split an object into its main parts, each with one or more annotated grasps for a particular approach of the gripper.","Subsequently, a grasp detection network utilizes the decomposed elements by Mask R-CNN and the information on the approach of the gripper in order to detect the element the gripper has approached and the most optimal grasp.","In order to train the networks, the study introduces a robotic grasping dataset collected in the Coppeliasim simulation environment.","The dataset involves 10 different objects with annotated element decomposition masks and grasp rectangles.","The proposed method acquires a 90% grasp success rate on seen objects and 78% on unseen objects in the Coppeliasim simulation environment.","Lastly, simulation-to-reality domain adaptation is performed by applying transformations on the training set collected in simulation and augmenting the dataset, which results in a 70% physical grasp success performance using a Delta parallel robot and a 2 -fingered gripper."],"url":"http://arxiv.org/abs/2402.01303v1","category":"cs.RO"}
{"created":"2024-02-02 10:44:42","title":"A Unified Framework for Gradient-based Clustering of Distributed Data","abstract":"We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\\mathcal{F}_\\rho$), is parametrized by $\\rho \\geq 1$, controling the proximity of users' center estimates, with $\\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\\mathcal{F}_\\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\\rho$ and DGC-HL$_\\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $\\rho$. Second, as $\\rho$ increases, the family of fixed points produced by DGC-$\\mathcal{F}_\\rho$ converges to a notion of consensus fixed points. We show that consensus fixed points of DGC-$\\mathcal{F}_{\\rho}$ are equivalent to fixed points of gradient clustering over the full data, guaranteeing a clustering of the full data is produced. For the special case of Bregman losses, we show that our fixed points converge to the set of Lloyd points. Numerical experiments on real data confirm our theoretical findings and demonstrate strong performance of the methods.","sentences":["We develop a family of distributed clustering algorithms that work over networks of users.","In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data.","The proposed family, termed Distributed Gradient Clustering (DGC-$\\mathcal{F}_\\rho$), is parametrized by $\\rho \\geq 1$, controling the proximity of users' center estimates, with $\\mathcal{F}$ determining the clustering loss.","Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\\mathcal{F}_\\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\\rho$ and DGC-HL$_\\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\\rho$. We provide a unified analysis and establish several strong results, under mild assumptions.","First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $\\rho$. Second, as $\\rho$ increases, the family of fixed points produced by DGC-$\\mathcal{F}_\\rho$ converges to a notion of consensus fixed points.","We show that consensus fixed points of DGC-$\\mathcal{F}_{\\rho}$ are equivalent to fixed points of gradient clustering over the full data, guaranteeing a clustering of the full data is produced.","For the special case of Bregman losses, we show that our fixed points converge to the set of Lloyd points.","Numerical experiments on real data confirm our theoretical findings and demonstrate strong performance of the methods."],"url":"http://arxiv.org/abs/2402.01302v1","category":"cs.LG"}
{"created":"2024-02-02 10:42:36","title":"On Inflation and Axionic Dark Matter in a Scaled Gravity","abstract":"Motivated by the modified gravity theories $F(R)\\neq R$ and inflationary physics, we first propose and investigate an inflation model in a scaled gravity $F(R)=R\\,+\\beta R$, where $\\beta $ is a dimensionless scaling parameter. The latter is also implemented in a particular potential $V(\\phi )=M^{4}\\left[ 1-\\cos \\left( \\frac{\\phi }{\\mu}\\right)^{\\beta }\\right] $ being considered to drive the inflation via a parameter coupling scenario. Using the slow-roll approximations, the gravity scale parameter $\\beta $ is approached with respect to the range of the associated computed cosmological observables $n_{s}$ and $r$ according to the recent Planck and BICEP/Keck data. Then, we discuss the axionic dark matter in the suggested gravity model by considering the case where the inflaton is taken to be identified with an axion-like field $\\phi =f_{a}\\theta $ with the decay constant $f_{a}=\\mu$. Referring to the known data, the underlying inflation scale $M$ is constrained to be much lower than the corresponding axion scale $M\\ll f_{a}$.","sentences":["Motivated by the modified gravity theories $F(R)\\neq R$ and inflationary physics, we first propose and investigate an inflation model in a scaled gravity $F(R)=R\\,+\\beta R$, where $\\beta $ is a dimensionless scaling parameter.","The latter is also implemented in a particular potential $V(\\phi )=M^{4}\\left[ 1-\\cos \\left( \\frac{\\phi }{\\mu}\\right)^{\\beta }\\right] $ being considered to drive the inflation via a parameter coupling scenario.","Using the slow-roll approximations, the gravity scale parameter $\\beta $ is approached with respect to the range of the associated computed cosmological observables $n_{s}$ and $r$ according to the recent Planck and BICEP/Keck data.","Then, we discuss the axionic dark matter in the suggested gravity model by considering the case where the inflaton is taken to be identified with an axion-like field $\\phi =f_{a}\\theta $ with the decay constant $f_{a}=\\mu$. Referring to the known data, the underlying inflation scale $M$ is constrained to be much lower than the corresponding axion scale $M\\ll f_{a}$."],"url":"http://arxiv.org/abs/2402.01301v1","category":"hep-th"}
{"created":"2024-02-02 10:41:46","title":"Almost sure and moment convergence for triangular P\u00f3lya urns","abstract":"We consider triangular P\\'olya urns and show under very weak conditions a general strong limit theorem of the form $X_{ni}/a_{ni}\\to \\mathcal{X}_i$ a.s., where $X_{ni}$ is the number of balls of colour $i$ after $n$ draws; the constants $a_{ni}$ are explicit and of the form $n^\\alpha\\log^\\gamma n$; the limit is a.s. positive, and may be either deterministic or random, but is in general unknown.   The result extends to urns with subtractions under weak conditions, but a counterexample shows that some conditions are needed.   For balanced urns we also prove moment convergence in the main results if the replacements have the corresponding moments.   The proofs are based on studying the corresponding continuous-time urn using martingale methods, and showing corresponding results there. We assume for convenience that all replacements have finite second moments.","sentences":["We consider triangular P\\'olya urns and show under very weak conditions a general strong limit theorem of the form $X_{ni}/a_{ni}\\to \\mathcal{X}_i$ a.s., where $X_{ni}$ is the number of balls of colour $i$ after $n$ draws; the constants $a_{ni}$ are explicit and of the form $n^\\alpha\\log^\\gamma n$; the limit is a.s. positive, and may be either deterministic or random, but is in general unknown.   ","The result extends to urns with subtractions under weak conditions, but a counterexample shows that some conditions are needed.   ","For balanced urns we also prove moment convergence in the main results if the replacements have the corresponding moments.   ","The proofs are based on studying the corresponding continuous-time urn using martingale methods, and showing corresponding results there.","We assume for convenience that all replacements have finite second moments."],"url":"http://arxiv.org/abs/2402.01299v1","category":"math.PR"}
{"created":"2024-02-02 10:39:58","title":"Learning Semantic Information from Raw Audio Signal Using Both Contextual and Phonetic Representations","abstract":"We propose a framework to learn semantics from raw audio signals using two types of representations, encoding contextual and phonetic information respectively. Specifically, we introduce a speech-to-unit processing pipeline that captures two types of representations with different time resolutions. For the language model, we adopt a dual-channel architecture to incorporate both types of representation. We also present new training objectives, masked context reconstruction and masked context prediction, that push models to learn semantics effectively. Experiments on the sSIMI metric of Zero Resource Speech Benchmark 2021 and Fluent Speech Command dataset show our framework learns semantics better than models trained with only one type of representation.","sentences":["We propose a framework to learn semantics from raw audio signals using two types of representations, encoding contextual and phonetic information respectively.","Specifically, we introduce a speech-to-unit processing pipeline that captures two types of representations with different time resolutions.","For the language model, we adopt a dual-channel architecture to incorporate both types of representation.","We also present new training objectives, masked context reconstruction and masked context prediction, that push models to learn semantics effectively.","Experiments on the sSIMI metric of Zero Resource Speech Benchmark 2021 and Fluent Speech Command dataset show our framework learns semantics better than models trained with only one type of representation."],"url":"http://arxiv.org/abs/2402.01298v1","category":"eess.AS"}
{"created":"2024-02-02 10:36:53","title":"Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum","abstract":"We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension. For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting. This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new random matrix theory (RMT) techniques with recent tools in the kernel ridge regression (KRR) literature.","sentences":["We derive new bounds for the condition number of kernel matrices, which we then use to enhance existing non-asymptotic test error bounds for kernel ridgeless regression in the over-parameterized regime for a fixed input dimension.","For kernels with polynomial spectral decay, we recover the bound from previous work; for exponential decay, our bound is non-trivial and novel.   ","Our conclusion on overfitting is two-fold: (i) kernel regressors whose eigenspectrum decays polynomially must generalize well, even in the presence of noisy labeled training data; these models exhibit so-called tempered overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays exponentially, then it generalizes poorly, i.e., it exhibits catastrophic overfitting.","This adds to the available characterization of kernel ridge regressors exhibiting benign overfitting as the extremal case where the eigenspectrum of the kernel decays sub-polynomially.","Our analysis combines new random matrix theory (RMT) techniques with recent tools in the kernel ridge regression (KRR) literature."],"url":"http://arxiv.org/abs/2402.01297v1","category":"cs.LG"}
{"created":"2024-02-02 10:34:13","title":"ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast","abstract":"Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our solution can achieve state-of-the-art performance in extreme weather prediction, while maintaining the overall forecast accuracy comparable to the top medium-range forecast models.","sentences":["Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models.","However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction.","Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values.","To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast.","Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness.","Combined with an advanced global weather forecast model, extensive experiments show that our solution can achieve state-of-the-art performance in extreme weather prediction, while maintaining the overall forecast accuracy comparable to the top medium-range forecast models."],"url":"http://arxiv.org/abs/2402.01295v1","category":"cs.LG"}
{"created":"2024-02-02 10:30:05","title":"Can MLLMs Perform Text-to-Image In-Context Learning?","abstract":"The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart. Existing such studies have primarily concentrated on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored. To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the inherent complexity of multimodality and image generation. To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements. Our code and dataset are available at \\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.","sentences":["The evolution from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to its multimodal counterpart.","Existing such studies have primarily concentrated on image-to-text ICL.","However, the Text-to-Image ICL (T2I-ICL), with its unique characteristics and potential applications, remains underexplored.","To address this gap, we formally define the task of T2I-ICL and present CoBSAT, the first T2I-ICL benchmark dataset, encompassing ten tasks.","Utilizing our dataset to benchmark six state-of-the-art MLLMs, we uncover considerable difficulties MLLMs encounter in solving T2I-ICL.","We identify the primary challenges as the inherent complexity of multimodality and image generation.","To overcome these challenges, we explore strategies like fine-tuning and Chain-of-Thought prompting, demonstrating notable improvements.","Our code and dataset are available at \\url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}."],"url":"http://arxiv.org/abs/2402.01293v1","category":"cs.LG"}
{"created":"2024-02-02 10:28:24","title":"Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence","abstract":"Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.","sentences":["Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches.","A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation.","In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis.","Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach.","Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines."],"url":"http://arxiv.org/abs/2402.01292v1","category":"cs.AI"}
{"created":"2024-02-02 10:26:14","title":"A Beginner's Guide to Black Hole Imaging and Associated Tests of General Relativity","abstract":"Following the 2019 release by the Event Horizon Telescope Collaboration of the first pictures of a supermassive black hole, there has been an explosion of interest in black hole images, their theoretical interpretation, and their potential use in tests of general relativity. The literature on the subject has now become so vast that an introductory guide for newcomers would appear welcome. Here, we aim to provide an accessible entry point to this growing field, with a particular focus on the black hole \"photon ring\": the bright, narrow ring of light that dominates images of a black hole and belongs to the black hole itself, rather than to its surrounding plasma. Far from an exhaustive review, this beginner's guide offers a pedagogical review of the key basic concepts and a brief summary of some results at the research frontier.","sentences":["Following the 2019 release by the Event Horizon Telescope Collaboration of the first pictures of a supermassive black hole, there has been an explosion of interest in black hole images, their theoretical interpretation, and their potential use in tests of general relativity.","The literature on the subject has now become so vast that an introductory guide for newcomers would appear welcome.","Here, we aim to provide an accessible entry point to this growing field, with a particular focus on the black hole \"photon ring\": the bright, narrow ring of light that dominates images of a black hole and belongs to the black hole itself, rather than to its surrounding plasma.","Far from an exhaustive review, this beginner's guide offers a pedagogical review of the key basic concepts and a brief summary of some results at the research frontier."],"url":"http://arxiv.org/abs/2402.01290v1","category":"gr-qc"}
{"created":"2024-02-02 10:22:05","title":"Directional emission and photon bunching from a qubit pair in waveguide","abstract":"Waveguide quantum electrodynamics represents a powerful platform to generate entanglement and tailor photonic states. We consider a pair of identical qubits coupled to a parity invariant waveguide in the microwave domain. By working in the one- and two-excitation sectors, we provide a unified view of decay processes and we show the common origin of directional single photon emission and two photon directional bunching. Unveiling the quantum trajectories, we demonstrate that both phenomena are rooted in the selective coupling of orthogonal qubits Bell states with different photon propagation directions. We comment on how to use this mechanism to implement optimized post-selection of Bell states, heralded by the detection of photons on one qubits side.","sentences":["Waveguide quantum electrodynamics represents a powerful platform to generate entanglement and tailor photonic states.","We consider a pair of identical qubits coupled to a parity invariant waveguide in the microwave domain.","By working in the one- and two-excitation sectors, we provide a unified view of decay processes and we show the common origin of directional single photon emission and two photon directional bunching.","Unveiling the quantum trajectories, we demonstrate that both phenomena are rooted in the selective coupling of orthogonal qubits Bell states with different photon propagation directions.","We comment on how to use this mechanism to implement optimized post-selection of Bell states, heralded by the detection of photons on one qubits side."],"url":"http://arxiv.org/abs/2402.01286v1","category":"quant-ph"}
{"created":"2024-02-02 10:20:50","title":"Coherence for logicians","abstract":"This paper is addressed to logicians not familiar with category theory. It gives a new proof of coherence for symmetric monoidal closed categories, proven by Kelly and Mac Lane in early 1970s. We find this result of great importance for proof theory and it is formulated here in pure logical terminology free of categorial notions. Coherence is related to the generality conjecture in general proof theory and we hope that our formulation will make it closer to the proof-theoretical community.","sentences":["This paper is addressed to logicians not familiar with category theory.","It gives a new proof of coherence for symmetric monoidal closed categories, proven by Kelly and Mac Lane in early 1970s.","We find this result of great importance for proof theory and it is formulated here in pure logical terminology free of categorial notions.","Coherence is related to the generality conjecture in general proof theory and we hope that our formulation will make it closer to the proof-theoretical community."],"url":"http://arxiv.org/abs/2402.01285v1","category":"math.LO"}
{"created":"2024-02-02 10:05:25","title":"Federated Unlearning: a Perspective of Stability and Fairness","abstract":"This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity. We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs. Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework. Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU. Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms. We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis.","sentences":["This paper explores the multifaceted consequences of federated unlearning (FU) with data heterogeneity.","We introduce key metrics for FU assessment, concentrating on verification, global stability, and local fairness, and investigate the inherent trade-offs.","Furthermore, we formulate the unlearning process with data heterogeneity through an optimization framework.","Our key contribution lies in a comprehensive theoretical analysis of the trade-offs in FU and provides insights into data heterogeneity's impacts on FU.","Leveraging these insights, we propose FU mechanisms to manage the trade-offs, guiding further development for FU mechanisms.","We empirically validate that our FU mechanisms effectively balance trade-offs, confirming insights derived from our theoretical analysis."],"url":"http://arxiv.org/abs/2402.01276v1","category":"cs.AI"}
{"created":"2024-02-02 09:51:25","title":"Dynamical system analysis in descending dark energy model","abstract":"In this paper, we study the dynamical system analysis for a recently proposed decaying dark energy model, namely, Q-SC-CDM. First we investigate the stationary points to find the stable attractor solution under the conditions discussed recently in the literature. In this case, we do not find any stable attractor solution. Therefore, we avoid the parameter space of Q-SC-CDM model discussed in arXiv:2201.07704. Second, we make different choice for the model parameters and re-investigate the stationary points and their stability. Our analysis shows that a simple choice of model parameters allows to capture a stable attractor solution. Moreover, we obtain phase portrait where all trajectories move towards the stable attractor point.","sentences":["In this paper, we study the dynamical system analysis for a recently proposed decaying dark energy model, namely, Q-SC-CDM.","First we investigate the stationary points to find the stable attractor solution under the conditions discussed recently in the literature.","In this case, we do not find any stable attractor solution.","Therefore, we avoid the parameter space of Q-SC-CDM model discussed in arXiv:2201.07704.","Second, we make different choice for the model parameters and re-investigate the stationary points and their stability.","Our analysis shows that a simple choice of model parameters allows to capture a stable attractor solution.","Moreover, we obtain phase portrait where all trajectories move towards the stable attractor point."],"url":"http://arxiv.org/abs/2402.01270v1","category":"gr-qc"}
{"created":"2024-02-02 09:47:26","title":"Spectrum-guided Feature Enhancement Network for Event Person Re-Identification","abstract":"As a cutting-edge biosensor, the event camera holds significant potential in the field of computer vision, particularly regarding privacy preservation. However, compared to traditional cameras, event streams often contain noise and possess extremely sparse semantics, posing a formidable challenge for event-based person re-identification (event Re-ID). To address this, we introduce a novel event person re-identification network: the Spectrum-guided Feature Enhancement Network (SFE-Net). This network consists of two innovative components: the Multi-grain Spectrum Attention Mechanism (MSAM) and the Consecutive Patch Dropout Module (CPDM). MSAM employs a fourier spectrum transform strategy to filter event noise, while also utilizing an event-guided multi-granularity attention strategy to enhance and capture discriminative person semantics. CPDM employs a consecutive patch dropout strategy to generate multiple incomplete feature maps, encouraging the deep Re-ID model to equally perceive each effective region of the person's body and capture robust person descriptors. Extensive experiments on Event Re-ID datasets demonstrate that our SFE-Net achieves the best performance in this task.","sentences":["As a cutting-edge biosensor, the event camera holds significant potential in the field of computer vision, particularly regarding privacy preservation.","However, compared to traditional cameras, event streams often contain noise and possess extremely sparse semantics, posing a formidable challenge for event-based person re-identification (event Re-ID).","To address this, we introduce a novel event person re-identification network: the Spectrum-guided Feature Enhancement Network (SFE-Net).","This network consists of two innovative components: the Multi-grain Spectrum Attention Mechanism (MSAM) and the Consecutive Patch Dropout Module (CPDM).","MSAM employs a fourier spectrum transform strategy to filter event noise, while also utilizing an event-guided multi-granularity attention strategy to enhance and capture discriminative person semantics.","CPDM employs a consecutive patch dropout strategy to generate multiple incomplete feature maps, encouraging the deep Re-ID model to equally perceive each effective region of the person's body and capture robust person descriptors.","Extensive experiments on Event Re-ID datasets demonstrate that our SFE-Net achieves the best performance in this task."],"url":"http://arxiv.org/abs/2402.01269v1","category":"cs.CV"}
{"created":"2024-02-02 09:41:51","title":"The Human and the Mechanical: logos, truthfulness, and ChatGPT","abstract":"The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that. Our paper adds a semantic argument to the current debate. The act of human assertion requires the formation of a veridicality judgment. Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee. Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs. `Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence. Therefore they lack the ability to form a belief about the world and a veridicality judgments altogether. They can only mimic that judgment, but the output is not ground in the very foundations for it.","sentences":["The paper addresses the question of whether it is appropriate to talk about `mechanical minds' at all, and whether ChatGPT models can indeed be thought of as realizations of that.","Our paper adds a semantic argument to the current debate.","The act of human assertion requires the formation of a veridicality judgment.","Modification of assertions with modals (John must be at home) and the use of subjective elements (John is obviously at home) indicate that the speaker is manipulating her judgments and, in a cooperative context, intends her epistemic state to be transparent to the addressee.","Veridicality judgments are formed on the basis of two components: (i) evidence that relates to reality (exogenous evidence) and (ii) endogenous evidence, such as preferences and private beliefs.","`Mechanical minds' lack these two components: (i) they do not relate to reality and (ii) do not have endogenous evidence.","Therefore they lack the ability to form a belief about the world and a veridicality judgments altogether.","They can only mimic that judgment, but the output is not ground in the very foundations for it."],"url":"http://arxiv.org/abs/2402.01267v1","category":"cs.CL"}
{"created":"2024-02-02 09:36:48","title":"Deviated Fixed-route Microtransit: Design and Operations","abstract":"Microtransit offers opportunities to enhance urban mobility by combining the reliability of public transit and the flexibility of ride-sharing. This paper optimizes the design and operations of a deviated fixed-route microtransit system that relies on reference lines but is allowed to deviate in response to passenger demand. We formulate a Microtransit Network Design (MiND) model via two-stage stochastic optimization. The model features a tight second-stage formulation thanks to a subpath-based representation of microtransit operations in a load-expanded network, which optimizes on-demand deviations between checkpoint stops. We develop a double-decomposition algorithm combining Benders decomposition and subpath-based column generation armed with a tailored label-setting algorithm. Using real-world data from Manhattan, results suggest that our method scales to large practical instances, with up to 10-100 candidate lines and hundreds of stations. Comparisons with transit and ride-sharing benchmarks suggest that microtransit can provide win-win outcomes toward efficient mobility (high demand coverage, low operating costs, high level of service), equitable mobility (broad geographic reach) and sustainable mobility (limited environmental footprint). We provide an open-source implementation in an online repository to enable replication.","sentences":["Microtransit offers opportunities to enhance urban mobility by combining the reliability of public transit and the flexibility of ride-sharing.","This paper optimizes the design and operations of a deviated fixed-route microtransit system that relies on reference lines but is allowed to deviate in response to passenger demand.","We formulate a Microtransit Network Design (MiND) model via two-stage stochastic optimization.","The model features a tight second-stage formulation thanks to a subpath-based representation of microtransit operations in a load-expanded network, which optimizes on-demand deviations between checkpoint stops.","We develop a double-decomposition algorithm combining Benders decomposition and subpath-based column generation armed with a tailored label-setting algorithm.","Using real-world data from Manhattan, results suggest that our method scales to large practical instances, with up to 10-100 candidate lines and hundreds of stations.","Comparisons with transit and ride-sharing benchmarks suggest that microtransit can provide win-win outcomes toward efficient mobility (high demand coverage, low operating costs, high level of service), equitable mobility (broad geographic reach) and sustainable mobility (limited environmental footprint).","We provide an open-source implementation in an online repository to enable replication."],"url":"http://arxiv.org/abs/2402.01265v1","category":"math.OC"}
{"created":"2024-02-02 09:36:06","title":"Direct side information learning for zero-shot regression","abstract":"Zero-shot learning provides models for targets for which instances are not available, commonly called unobserved targets. The availability of target side information becomes crucial in this context in order to properly induce models for these targets. The literature is plenty of strategies to cope with this scenario, but specifically designed on the basis of a zero-shot classification scenario, mostly in computer vision and image classification, but they are either not applicable or easily extensible for a zero-shot regression framework for which a continuos value is required to be predicted rather than a label. In fact, there is a considerable lack of methods for zero-shot regression in the literature. Two approaches for zero-shot regression that work in a two-phase procedure were recently proposed. They first learn the observed target models through a classical regression learning ignoring the target side information. Then, they aggregate those observed target models afterwards exploiting the target side information and the models for the unobserved targets are induced. Despite both have shown quite good performance because of the different treatment they grant to the common features and to the side information, they exploit features and side information separately, avoiding a global optimization for providing the unobserved target models. The proposal of this paper is a novel method that jointly takes features and side information in a one-phase learning process, but treating side information properly and in a more deserving way than as common features. A specific kernel that properly merges features and side information is proposed for this purpose resulting in a novel approach that exhibits better performance over both artificial and real datasets.","sentences":["Zero-shot learning provides models for targets for which instances are not available, commonly called unobserved targets.","The availability of target side information becomes crucial in this context in order to properly induce models for these targets.","The literature is plenty of strategies to cope with this scenario, but specifically designed on the basis of a zero-shot classification scenario, mostly in computer vision and image classification, but they are either not applicable or easily extensible for a zero-shot regression framework for which a continuos value is required to be predicted rather than a label.","In fact, there is a considerable lack of methods for zero-shot regression in the literature.","Two approaches for zero-shot regression that work in a two-phase procedure were recently proposed.","They first learn the observed target models through a classical regression learning ignoring the target side information.","Then, they aggregate those observed target models afterwards exploiting the target side information and the models for the unobserved targets are induced.","Despite both have shown quite good performance because of the different treatment they grant to the common features and to the side information, they exploit features and side information separately, avoiding a global optimization for providing the unobserved target models.","The proposal of this paper is a novel method that jointly takes features and side information in a one-phase learning process, but treating side information properly and in a more deserving way than as common features.","A specific kernel that properly merges features and side information is proposed for this purpose resulting in a novel approach that exhibits better performance over both artificial and real datasets."],"url":"http://arxiv.org/abs/2402.01264v1","category":"cs.LG"}
{"created":"2024-02-02 09:34:49","title":"A Differentiable POGLM with Forward-Backward Message Passing","abstract":"The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a better performance on one synthetic and two real-world datasets. Furthermore, our new method yields more interpretable parameters, underscoring its significance in neuroscience.","sentences":["The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons.","With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model.","There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance.","For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works.","For (2), we propose the forward-backward message-passing sampling scheme for the variational model.","Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a better performance on one synthetic and two real-world datasets.","Furthermore, our new method yields more interpretable parameters, underscoring its significance in neuroscience."],"url":"http://arxiv.org/abs/2402.01263v1","category":"cs.LG"}
{"created":"2024-02-02 09:32:03","title":"TEDDY: Trimming Edges with Degree-based Discrimination strategY","abstract":"Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information. Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient descent on the $\\ell_0$ ball. Given the target sparsity levels for both the graph structure and the model parameters, our TEDDY facilitates efficient and rapid realization of GLT within a single training. Remarkably, our experimental results demonstrate that TEDDY significantly surpasses conventional iterative approaches in generalization, even when conducting one-shot sparsification that solely utilizes graph structures, without taking node features into account.","sentences":["Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks.","In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies.","Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient.","To address these limitations, we introduce TEDDY, a one-shot edge sparsification framework that leverages structural information by incorporating edge-degree information.","Following edge sparsification, we encourage the parameter sparsity during training via simple projected gradient descent on the $\\ell_0$ ball.","Given the target sparsity levels for both the graph structure and the model parameters, our TEDDY facilitates efficient and rapid realization of GLT within a single training.","Remarkably, our experimental results demonstrate that TEDDY significantly surpasses conventional iterative approaches in generalization, even when conducting one-shot sparsification that solely utilizes graph structures, without taking node features into account."],"url":"http://arxiv.org/abs/2402.01261v1","category":"cs.LG"}
{"created":"2024-02-02 09:31:03","title":"Global Flows of Foliated Gravity-Matter Systems","abstract":"Asymptotic safety is a promising mechanism for obtaining a consistent and predictive quantum theory for gravity. The ADM formalism allows to introduce a (Euclidean) time-direction in this framework. It equips spacetime with a foliation structure by encoding the gravitational degrees of freedom in a lapse function, shift vector, and a metric measuring distances on the spatial slices. We use the Wetterich equation to study the renormalization group flow of the graviton 2-point function extracted from the spatial metric. The flow is driven by the 3- and 4-point vertices generated by the foliated Einstein-Hilbert action supplemented by minimally coupled scalar and vector fields. We derive bounds on the number of matter fields cast by asymptotic safety. Moreover, we show that the phase diagram obtained in the pure gravity case is qualitatively stable within these bounds. An intriguing feature is the presence of an IR-fixed point for the graviton mass which prevents the squared mass taking negative values. This feature persists for any number of matter fields and, in particular, also in situations where there is no suitable interacting fixed point rendering the theory asymptotically safe. Our work complements earlier studies of the subject by taking contributions from the matter fields into account.","sentences":["Asymptotic safety is a promising mechanism for obtaining a consistent and predictive quantum theory for gravity.","The ADM formalism allows to introduce a (Euclidean) time-direction in this framework.","It equips spacetime with a foliation structure by encoding the gravitational degrees of freedom in a lapse function, shift vector, and a metric measuring distances on the spatial slices.","We use the Wetterich equation to study the renormalization group flow of the graviton 2-point function extracted from the spatial metric.","The flow is driven by the 3- and 4-point vertices generated by the foliated Einstein-Hilbert action supplemented by minimally coupled scalar and vector fields.","We derive bounds on the number of matter fields cast by asymptotic safety.","Moreover, we show that the phase diagram obtained in the pure gravity case is qualitatively stable within these bounds.","An intriguing feature is the presence of an IR-fixed point for the graviton mass which prevents the squared mass taking negative values.","This feature persists for any number of matter fields and, in particular, also in situations where there is no suitable interacting fixed point rendering the theory asymptotically safe.","Our work complements earlier studies of the subject by taking contributions from the matter fields into account."],"url":"http://arxiv.org/abs/2402.01260v1","category":"hep-th"}
{"created":"2024-02-02 09:30:27","title":"Position Aware 60 GHz mmWave Beamforming for V2V Communications Utilizing Deep Learning","abstract":"Beamforming techniques are considered as essential parts to compensate the severe path loss in millimeter-wave (mmWave) communications by adopting large antenna arrays and formulating narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches, mainly relied on channel state information, typically impose significant latency and computing overheads, which is often infeasible in vehicle-to-vehicle (V2V) communications like highly dynamic scenarios. In contrast, utilizing out-of-band contextual information, such as vehicular position information, is a potential alternative to reduce such overheads. In this context, this paper presents a deep learning-based solution on utilizing the vehicular position information for predicting the optimal beams having sufficient mmWave received powers so that the best V2V line-of-sight links can be ensured proactively. After experimental evaluation of the proposed solution on real-world measured mmWave sensing and communications datasets, the results show that the solution can achieve up to 84.58% of received power of link status on average, which confirm a promising solution for beamforming in mmWave at 60 GHz enabled V2V communications.","sentences":["Beamforming techniques are considered as essential parts to compensate the severe path loss in millimeter-wave (mmWave) communications by adopting large antenna arrays and formulating narrow beams to obtain satisfactory received powers.","However, performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches, mainly relied on channel state information, typically impose significant latency and computing overheads, which is often infeasible in vehicle-to-vehicle (V2V) communications like highly dynamic scenarios.","In contrast, utilizing out-of-band contextual information, such as vehicular position information, is a potential alternative to reduce such overheads.","In this context, this paper presents a deep learning-based solution on utilizing the vehicular position information for predicting the optimal beams having sufficient mmWave received powers so that the best V2V line-of-sight links can be ensured proactively.","After experimental evaluation of the proposed solution on real-world measured mmWave sensing and communications datasets, the results show that the solution can achieve up to 84.58% of received power of link status on average, which confirm a promising solution for beamforming in mmWave at 60 GHz enabled V2V communications."],"url":"http://arxiv.org/abs/2402.01259v1","category":"cs.NI"}
{"created":"2024-02-02 09:29:40","title":"Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape","abstract":"Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.","sentences":["Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context.","However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks.","In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer.","The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning.","We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign.","We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points.","Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points.","This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest."],"url":"http://arxiv.org/abs/2402.01258v1","category":"stat.ML"}
{"created":"2024-02-02 09:28:51","title":"Polygonal corona limit on multigrid dual tilings","abstract":"The growth pattern of an invasive cell-to-cell propagation (called the successive coronas) on the square grid is a tilted square. On the triangular and hexagonal grids, it is an hexagon. It is remarkable that, on the aperiodic structure of Penrose tilings, this cell-to-cell diffusion process tends to a regular decagon (at the limit). In this article we generalize this result to any regular multigrid dual tiling, by defining the characteristic polygon of a multigrid and its dual tiling. Exploiting this elegant duality allows to fully understand why such surprising phenomena, of seeing highly regular polygonal shapes emerge from aperiodic underlying structures, happen.","sentences":["The growth pattern of an invasive cell-to-cell propagation (called the successive coronas) on the square grid is a tilted square.","On the triangular and hexagonal grids, it is an hexagon.","It is remarkable that, on the aperiodic structure of Penrose tilings, this cell-to-cell diffusion process tends to a regular decagon (at the limit).","In this article we generalize this result to any regular multigrid dual tiling, by defining the characteristic polygon of a multigrid and its dual tiling.","Exploiting this elegant duality allows to fully understand why such surprising phenomena, of seeing highly regular polygonal shapes emerge from aperiodic underlying structures, happen."],"url":"http://arxiv.org/abs/2402.01257v1","category":"cs.DM"}
{"created":"2024-02-02 09:23:28","title":"Probing of magnetic dimensional crossover in CrSiTe$_{3}$ through picosecond strain pulses","abstract":"Elucidating the emergence of long-range magnetic ordering from its precursor short-range magnetic ordering (SRMO) in two-dimensional van der Waals materials holds profound implications for fundamental research and technological advancements. However, directly observing the intricate stages of this magnetic dimensional crossover (MDC) remains a significant experimental challenge. While magneto-elastic coupling offers a promising avenue, detecting the minute lattice response to SRMO proves challenging. Recent investigations utilizing second harmonic generation have unveiled a two-step MDC in a van der Waals ferromagnetic insulator. However, an unambiguous detection of MDC through the time-resolved techniques remains elusive. To meet this goal, we have executed an alternative approach by employing picosecond acoustic strain pulses generated by femtosecond lasers to probe the various stages of MDC through the magneto-elastic coupling for the first time. By analyzing the shape of the strain pulse in both the time and frequency domains as a function of temperature, we clearly demonstrate the detection of the subtle influence of spin fluctuations on the lattice. Additionally, the ultrafast carrier dynamics also show signatures of MDC. Our measurements pave the way towards characterizing magnetic materials in time-resolved experiments that are crucial in designing a new generation of spin-based optoelectronic devices.","sentences":["Elucidating the emergence of long-range magnetic ordering from its precursor short-range magnetic ordering (SRMO) in two-dimensional van der Waals materials holds profound implications for fundamental research and technological advancements.","However, directly observing the intricate stages of this magnetic dimensional crossover (MDC) remains a significant experimental challenge.","While magneto-elastic coupling offers a promising avenue, detecting the minute lattice response to SRMO proves challenging.","Recent investigations utilizing second harmonic generation have unveiled a two-step MDC in a van der Waals ferromagnetic insulator.","However, an unambiguous detection of MDC through the time-resolved techniques remains elusive.","To meet this goal, we have executed an alternative approach by employing picosecond acoustic strain pulses generated by femtosecond lasers to probe the various stages of MDC through the magneto-elastic coupling for the first time.","By analyzing the shape of the strain pulse in both the time and frequency domains as a function of temperature, we clearly demonstrate the detection of the subtle influence of spin fluctuations on the lattice.","Additionally, the ultrafast carrier dynamics also show signatures of MDC.","Our measurements pave the way towards characterizing magnetic materials in time-resolved experiments that are crucial in designing a new generation of spin-based optoelectronic devices."],"url":"http://arxiv.org/abs/2402.01256v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 09:21:09","title":"Neural Trajectory Model: Implicit Neural Trajectory Representation for Trajectories Generation","abstract":"Trajectory planning is a fundamental problem in robotics. It facilitates a wide range of applications in navigation and motion planning, control, and multi-agent coordination. Trajectory planning is a difficult problem due to its computational complexity and real-world environment complexity with uncertainty, non-linearity, and real-time requirements. The multi-agent trajectory planning problem adds another dimension of difficulty due to inter-agent interaction. Existing solutions are either search-based or optimization-based approaches with simplified assumptions of environment, limited planning speed, and limited scalability in the number of agents. In this work, we make the first attempt to reformulate single agent and multi-agent trajectory planning problem as query problems over an implicit neural representation of trajectories. We formulate such implicit representation as Neural Trajectory Models (NTM) which can be queried to generate nearly optimal trajectory in complex environments. We conduct experiments in simulation environments and demonstrate that NTM can solve single-agent and multi-agent trajectory planning problems. In the experiments, NTMs achieve (1) sub-millisecond panning time using GPUs, (2) almost avoiding all environment collision, (3) almost avoiding all inter-agent collision, and (4) generating almost shortest paths. We also demonstrate that the same NTM framework can also be used for trajectories correction and multi-trajectory conflict resolution refining low quality and conflicting multi-agent trajectories into nearly optimal solutions efficiently. (Open source code will be available at https://github.com/laser2099/neural-trajectory-model)","sentences":["Trajectory planning is a fundamental problem in robotics.","It facilitates a wide range of applications in navigation and motion planning, control, and multi-agent coordination.","Trajectory planning is a difficult problem due to its computational complexity and real-world environment complexity with uncertainty, non-linearity, and real-time requirements.","The multi-agent trajectory planning problem adds another dimension of difficulty due to inter-agent interaction.","Existing solutions are either search-based or optimization-based approaches with simplified assumptions of environment, limited planning speed, and limited scalability in the number of agents.","In this work, we make the first attempt to reformulate single agent and multi-agent trajectory planning problem as query problems over an implicit neural representation of trajectories.","We formulate such implicit representation as Neural Trajectory Models (NTM) which can be queried to generate nearly optimal trajectory in complex environments.","We conduct experiments in simulation environments and demonstrate that NTM can solve single-agent and multi-agent trajectory planning problems.","In the experiments, NTMs achieve (1) sub-millisecond panning time using GPUs, (2) almost avoiding all environment collision, (3) almost avoiding all inter-agent collision, and (4) generating almost shortest paths.","We also demonstrate that the same NTM framework can also be used for trajectories correction and multi-trajectory conflict resolution refining low quality and conflicting multi-agent trajectories into nearly optimal solutions efficiently.","(Open source code will be available at https://github.com/laser2099/neural-trajectory-model)"],"url":"http://arxiv.org/abs/2402.01254v1","category":"cs.RO"}
{"created":"2024-02-02 09:20:48","title":"HimiRec: Modeling Hierarchical Multi-interest for Recommendation","abstract":"Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items. The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention. Frequently, users show hierarchical multi-interests reflected in a heavy user of a certain NBA team Golden State Warriors in Sports, who is also a light user of almost the whole Animation. Both Sports and Animation are at the same level. However, most existing methods implicitly learn this hierarchical difference, making more fine-grained interest information to be averaged and limiting detailed understanding of the user's different needs in heavy interests and other light interests. Therefore, we propose a novel two-stage approach to explicitly modeling hierarchical multi-interest for recommendation in this work. In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model adaptively generate circles or sub-circles that users are interested in. In the second stage, the partition of retrieval space allows the EBR models to only deal with items within each circle and accurately capture user's refined interests. Experimental results show that the proposed approach achieves state-of-the-art performance. Our framework has also successfully deployed at Lofter (one of the largest derivative content communities with 10 million monthly active users) for over four months.","sentences":["Industrial recommender systems usually consist of the retrieval stage and the ranking stage, to handle the billion-scale of users and items.","The retrieval stage retrieves candidate items relevant to user interests for recommendations and has attracted much attention.","Frequently, users show hierarchical multi-interests reflected in a heavy user of a certain NBA team Golden State Warriors in Sports, who is also a light user of almost the whole Animation.","Both Sports and Animation are at the same level.","However, most existing methods implicitly learn this hierarchical difference, making more fine-grained interest information to be averaged and limiting detailed understanding of the user's different needs in heavy interests and other light interests.","Therefore, we propose a novel two-stage approach to explicitly modeling hierarchical multi-interest for recommendation in this work.","In the first hierarchical multi-interest mining stage, the hierarchical clustering and transformer-based model adaptively generate circles or sub-circles that users are interested in.","In the second stage, the partition of retrieval space allows the EBR models to only deal with items within each circle and accurately capture user's refined interests.","Experimental results show that the proposed approach achieves state-of-the-art performance.","Our framework has also successfully deployed at Lofter (one of the largest derivative content communities with 10 million monthly active users) for over four months."],"url":"http://arxiv.org/abs/2402.01253v1","category":"cs.IR"}
{"created":"2024-02-02 09:19:45","title":"Target inductive methods for zero-shot regression","abstract":"This research arises from the need to predict the amount of air pollutants in meteorological stations. Air pollution depends on the location of the stations (weather conditions and activities in the surroundings). Frequently, the surrounding information is not considered in the learning process. This information is known beforehand in the absence of unobserved weather conditions and remains constant for the same station. Considering the surrounding information as side information facilitates the generalization for predicting pollutants in new stations, leading to a zero-shot regression scenario. Available methods in zero-shot typically lean towards classification, and are not easily extensible to regression. This paper proposes two zero-shot methods for regression. The first method is a similarity based approach that learns models from features and aggregates them using side information. However, potential knowledge of the feature models may be lost in the aggregation. The second method overcomes this drawback by replacing the aggregation procedure and learning the correspondence between side information and feature-induced models, instead. Both proposals are compared with a baseline procedure using artificial datasets, UCI repository communities and crime datasets, and the pollutants. Both approaches outperform the baseline method, but the parameter learning approach manifests its superiority over the similarity based method.","sentences":["This research arises from the need to predict the amount of air pollutants in meteorological stations.","Air pollution depends on the location of the stations (weather conditions and activities in the surroundings).","Frequently, the surrounding information is not considered in the learning process.","This information is known beforehand in the absence of unobserved weather conditions and remains constant for the same station.","Considering the surrounding information as side information facilitates the generalization for predicting pollutants in new stations, leading to a zero-shot regression scenario.","Available methods in zero-shot typically lean towards classification, and are not easily extensible to regression.","This paper proposes two zero-shot methods for regression.","The first method is a similarity based approach that learns models from features and aggregates them using side information.","However, potential knowledge of the feature models may be lost in the aggregation.","The second method overcomes this drawback by replacing the aggregation procedure and learning the correspondence between side information and feature-induced models, instead.","Both proposals are compared with a baseline procedure using artificial datasets, UCI repository communities and crime datasets, and the pollutants.","Both approaches outperform the baseline method, but the parameter learning approach manifests its superiority over the similarity based method."],"url":"http://arxiv.org/abs/2402.01252v1","category":"cs.LG"}
{"created":"2024-02-02 09:17:02","title":"Maximal noncompactness of limiting Sobolev embeddings","abstract":"We develop a new method suitable for establishing lower bounds on the ball measure of noncompactness of operators acting between considerably general quasinormed function spaces. This new method removes some of the restrictions oft-presented in the previous work. Most notably, the target function space need not be disjointly superadditivity nor equipped with a norm. Instead, a property that is far more often at our disposal is exploited, namely the absolute continuity of the target quasinorm.   We use this new method to prove that limiting Sobolev embeddings into spaces of Brezis--Wainger type are so-called maximally noncompact, i.e., their ball measure of noncompactness is the worst possible.","sentences":["We develop a new method suitable for establishing lower bounds on the ball measure of noncompactness of operators acting between considerably general quasinormed function spaces.","This new method removes some of the restrictions oft-presented in the previous work.","Most notably, the target function space need not be disjointly superadditivity nor equipped with a norm.","Instead, a property that is far more often at our disposal is exploited, namely the absolute continuity of the target quasinorm.   ","We use this new method to prove that limiting Sobolev embeddings into spaces of Brezis--Wainger type are so-called maximally noncompact, i.e., their ball measure of noncompactness is the worst possible."],"url":"http://arxiv.org/abs/2402.01250v1","category":"math.FA"}
{"created":"2024-02-02 09:16:29","title":"Constructive projective extension of an incidence plane","abstract":"A standard procedure in classical projective geometry, using pencils of lines to extend an incidence plane to a projective plane, is examined from a constructive viewpoint. Brouwerian counterexamples reveal the limitations of traditional pencils. Generalized definitions are adopted to construct a projective extension. The main axioms of projective geometry are verified. The methods used are in accord with Bishop-type modern constructivism.","sentences":["A standard procedure in classical projective geometry, using pencils of lines to extend an incidence plane to a projective plane, is examined from a constructive viewpoint.","Brouwerian counterexamples reveal the limitations of traditional pencils.","Generalized definitions are adopted to construct a projective extension.","The main axioms of projective geometry are verified.","The methods used are in accord with Bishop-type modern constructivism."],"url":"http://arxiv.org/abs/2402.01249v1","category":"math.MG"}
{"created":"2024-02-02 09:16:06","title":"Single production of vectorlike quarks with charge 5/3 at the 14 TeV LHC","abstract":"Vector-like quarks arise in various new physics scenarios beyond the Standard Model (SM). In a framework of the SM simply extended by an SU(2) doublet $\\left(X,T\\right)$ including a vector-like quark $X$~(VLQ-$X$), with electric charge $|Q_{X}| = 5/3$, we investigate the single production of the VLQ-$X$ induced by the couplings between the VLQ-$X$ with the first and the third generation quarks at the Large Hadron Collider (LHC) Run-III and High-Luminosity LHC (HL-LHC) operating at $\\sqrt{s}=14$ TeV. The signal is searched in events including same-sign dileptons (electrons or muons), one $b$-tagged jet and missing energy, where the X quark is assumed to decay into a top quark and a W boson, both decaying leptonically. After a rapid simulation of signal and background events, the 95\\% CL exclusion limits and the $5\\sigma$ discovery reach are respectively obtained at the 14 TeV LHC with an integrated luminosity of 300 and 3000 fb$^{-1}$, respectively.","sentences":["Vector-like quarks arise in various new physics scenarios beyond the Standard Model (SM).","In a framework of the SM simply extended by an SU(2) doublet $\\left(X,T\\right)$ including a vector-like quark $X$~(VLQ-$X$), with electric charge $|Q_{X}| = 5/3$, we investigate the single production of the VLQ-$X$ induced by the couplings between the VLQ-$X$ with the first and the third generation quarks at the Large Hadron Collider (LHC) Run-III and High-Luminosity LHC (HL-LHC) operating at $\\sqrt{s}=14$ TeV. The signal is searched in events including same-sign dileptons (electrons or muons), one $b$-tagged jet and missing energy, where the X quark is assumed to decay into a top quark and a W boson, both decaying leptonically.","After a rapid simulation of signal and background events, the 95\\% CL exclusion limits and the $5\\sigma$ discovery reach are respectively obtained at the 14 TeV LHC with an integrated luminosity of 300 and 3000 fb$^{-1}$, respectively."],"url":"http://arxiv.org/abs/2402.01248v1","category":"hep-ph"}
{"created":"2024-02-02 09:13:22","title":"LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving","abstract":"The emergence of Multimodal Large Language Models ((M)LLMs) has ushered in new avenues in artificial intelligence, particularly for autonomous driving by offering enhanced understanding and reasoning capabilities. This paper introduces LimSim++, an extended version of LimSim designed for the application of (M)LLMs in autonomous driving. Acknowledging the limitations of existing simulation platforms, LimSim++ addresses the need for a long-term closed-loop infrastructure supporting continuous learning and improved generalization in autonomous driving. The platform offers extended-duration, multi-scenario simulations, providing crucial information for (M)LLM-driven vehicles. Users can engage in prompt engineering, model evaluation, and framework enhancement, making LimSim++ a versatile tool for research and practice. This paper additionally introduces a baseline (M)LLM-driven framework, systematically validated through quantitative experiments across diverse scenarios. The open-source resources of LimSim++ are available at: https://pjlab-adg.github.io/limsim_plus/.","sentences":["The emergence of Multimodal Large Language Models ((M)LLMs) has ushered in new avenues in artificial intelligence, particularly for autonomous driving by offering enhanced understanding and reasoning capabilities.","This paper introduces LimSim++, an extended version of LimSim designed for the application of (M)LLMs in autonomous driving.","Acknowledging the limitations of existing simulation platforms, LimSim++ addresses the need for a long-term closed-loop infrastructure supporting continuous learning and improved generalization in autonomous driving.","The platform offers extended-duration, multi-scenario simulations, providing crucial information for (M)LLM-driven vehicles.","Users can engage in prompt engineering, model evaluation, and framework enhancement, making LimSim++ a versatile tool for research and practice.","This paper additionally introduces a baseline (M)LLM-driven framework, systematically validated through quantitative experiments across diverse scenarios.","The open-source resources of LimSim++ are available at: https://pjlab-adg.github.io/limsim_plus/."],"url":"http://arxiv.org/abs/2402.01246v1","category":"cs.RO"}
{"created":"2024-02-02 09:11:45","title":"Continuous logic in a classical setting","abstract":"Let $\\mathcal{L}$ be a first-order two-sorted language and consider a class of $\\mathcal{L}$-structures of the form $\\langle M, X \\rangle$ where $M$ varies among structures of the first sort, while $X$ is fixed in the second sort, and it is assumed to be a compact Hausdorff space. When $X$ is a compact subset of the real line, one way to treat classes of this kind model-theoretically is via continuous-valued logic, as in [Ben Yaacov-Berenstein-Henson-Usvyatsov 2010]. Prior to that, Henson and Iovino proposed an approach based on the notion of positive formulas [Henson-Iovino 2002]. Their work is tailored to the model theory of Banach spaces. Here we show that a similar approach is possible for a more general class of models. We introduce suitable versions of elementarity, compactness, saturation, quantifier elimination and other basic tools, and we develop basic model theory.","sentences":["Let $\\mathcal{L}$ be a first-order two-sorted language and consider a class of $\\mathcal{L}$-structures of the form $\\langle M, X \\rangle$ where $M$ varies among structures of the first sort, while $X$ is fixed in the second sort, and it is assumed to be a compact Hausdorff space.","When $X$ is a compact subset of the real line, one way to treat classes of this kind model-theoretically is via continuous-valued logic, as in [Ben Yaacov-Berenstein-Henson-Usvyatsov 2010].","Prior to that, Henson and Iovino proposed an approach based on the notion of positive formulas [Henson-Iovino 2002].","Their work is tailored to the model theory of Banach spaces.","Here we show that a similar approach is possible for a more general class of models.","We introduce suitable versions of elementarity, compactness, saturation, quantifier elimination and other basic tools, and we develop basic model theory."],"url":"http://arxiv.org/abs/2402.01245v1","category":"math.LO"}
{"created":"2024-02-02 09:10:35","title":"Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness","abstract":"Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology & semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to guide this process, effectively balancing the preservation of both topological and semantic information. Ultimately, GST produces a sparse graph with maximum topological integrity and no performance degradation. Extensive experiments on 6 datasets and 5 backbones showcase that GST (I) identifies subgraphs at higher graph sparsity levels (1.67%~15.85% $\\uparrow$) than state-of-the-art sparsification methods, (II) preserves more key spectral properties, (III) achieves 1.27-3.42$\\times$ speedup in GNN inference and (IV) successfully helps graph adversarial defense and graph lottery tickets.","sentences":["Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs.","A promising solution is to remove non-essential edges to reduce the computational overheads in GNN.","Previous literature generally falls into two categories: topology-guided and semantic-guided.","The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training.","The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels.","With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level.","Specifically, GST initially constructs a topology & semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor.","We introduce the Equilibria Sparsification Principle to guide this process, effectively balancing the preservation of both topological and semantic information.","Ultimately, GST produces a sparse graph with maximum topological integrity and no performance degradation.","Extensive experiments on 6 datasets and 5 backbones showcase that GST (I) identifies subgraphs at higher graph sparsity levels (1.67%~15.85% $\\uparrow$) than state-of-the-art sparsification methods, (II) preserves more key spectral properties, (III) achieves 1.27-3.42$\\times$ speedup in GNN inference and (IV) successfully helps graph adversarial defense and graph lottery tickets."],"url":"http://arxiv.org/abs/2402.01242v1","category":"cs.LG"}
{"created":"2024-02-02 09:09:23","title":"Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?","abstract":"Recent advancements in deep generative models, particularly with the application of CLIP (Contrastive Language Image Pretraining) to Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable effectiveness in text to image generation. The well structured embedding space of CLIP has also been extended to image to shape generation with DDPMs, yielding notable results. Despite these successes, some fundamental questions arise: Does CLIP ensure the best results in shape generation from images? Can we leverage conditioning to bring explicit 3D knowledge into the generative process and obtain better quality? This study introduces CISP (Contrastive Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D shapes in a shared embedding space, specifically capturing 3D characteristics potentially overlooked by CLIP's text image focus. Our comprehensive analysis assesses CISP's guidance performance against CLIP guided models, focusing on generation quality, diversity, and coherence of the produced shapes with the conditioning image. We find that, while matching CLIP in generation quality and diversity, CISP substantially improves coherence with input images, underscoring the value of incorporating 3D knowledge into generative models. These findings suggest a promising direction for advancing the synthesis of 3D visual content by integrating multimodal systems with 3D representations.","sentences":["Recent advancements in deep generative models, particularly with the application of CLIP (Contrastive Language Image Pretraining) to Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable effectiveness in text to image generation.","The well structured embedding space of CLIP has also been extended to image to shape generation with DDPMs, yielding notable results.","Despite these successes, some fundamental questions arise: Does CLIP ensure the best results in shape generation from images?","Can we leverage conditioning to bring explicit 3D knowledge into the generative process and obtain better quality?","This study introduces CISP (Contrastive Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D images.","CISP aims to enrich the CLIP framework by aligning 2D images with 3D shapes in a shared embedding space, specifically capturing 3D characteristics potentially overlooked by CLIP's text image focus.","Our comprehensive analysis assesses CISP's guidance performance against CLIP guided models, focusing on generation quality, diversity, and coherence of the produced shapes with the conditioning image.","We find that, while matching CLIP in generation quality and diversity, CISP substantially improves coherence with input images, underscoring the value of incorporating 3D knowledge into generative models.","These findings suggest a promising direction for advancing the synthesis of 3D visual content by integrating multimodal systems with 3D representations."],"url":"http://arxiv.org/abs/2402.01241v1","category":"cs.CV"}
{"created":"2024-02-02 09:07:09","title":"Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting","abstract":"The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting web trackers in Chrome and Firefox. However, real-world application testing remains pending, and the distinction between tracker types and broader label sources could be explored in future studies.","sentences":["The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking.","Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked.","This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers.","Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set.","Eleven supervised models were trained on Chrome data and tested across all browsers.","The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set.","The research suggests that these classifiers are viable for detecting web trackers in Chrome and Firefox.","However, real-world application testing remains pending, and the distinction between tracker types and broader label sources could be explored in future studies."],"url":"http://arxiv.org/abs/2402.01240v1","category":"cs.CR"}
{"created":"2024-02-02 09:07:00","title":"PRIME: Protect Your Videos From Malicious Editing","abstract":"With the development of generative models, the quality of generated content keeps increasing. Recently, open-source models have made it surprisingly easy to manipulate and edit photos and videos, with just a few simple prompts. While these cutting-edge technologies have gained popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals. Malicious users can exploit these tools for deceptive or illegal purposes. Although some previous works focus on protecting photos against generative models, we find there are still gaps between protecting videos and images in the aspects of efficiency and effectiveness. Therefore, we introduce our protection method, PRIME, to significantly reduce the time cost and improve the protection performance. Moreover, to evaluate our proposed protection method, we consider both objective metrics and human subjective metrics. Our evaluation results indicate that PRIME only costs 8.3% GPU hours of the cost of the previous state-of-the-art method and achieves better protection results on both human evaluation and objective metrics. Code can be found in https://github.com/GuanlinLee/prime.","sentences":["With the development of generative models, the quality of generated content keeps increasing.","Recently, open-source models have made it surprisingly easy to manipulate and edit photos and videos, with just a few simple prompts.","While these cutting-edge technologies have gained popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals.","Malicious users can exploit these tools for deceptive or illegal purposes.","Although some previous works focus on protecting photos against generative models, we find there are still gaps between protecting videos and images in the aspects of efficiency and effectiveness.","Therefore, we introduce our protection method, PRIME, to significantly reduce the time cost and improve the protection performance.","Moreover, to evaluate our proposed protection method, we consider both objective metrics and human subjective metrics.","Our evaluation results indicate that PRIME only costs 8.3% GPU hours of the cost of the previous state-of-the-art method and achieves better protection results on both human evaluation and objective metrics.","Code can be found in https://github.com/GuanlinLee/prime."],"url":"http://arxiv.org/abs/2402.01239v1","category":"cs.CV"}
{"created":"2024-02-02 09:03:38","title":"Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training","abstract":"Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VIB. Furthermore, in terms of calibration performance, FVIB outperforms other IB and calibration methods by enabling continuous optimization of $\\beta$. Our codes are available at https://github.com/sotakudo/fvib.","sentences":["Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable.","In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\\beta$ through multiple training cycles, which is computationally expensive.","In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\\beta$ with single, computationally efficient training.","We theoretically demonstrate that across all values of reasonable $\\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method.","Then we empirically show that FVIB can learn the VIB objective as effectively as VIB.","Furthermore, in terms of calibration performance, FVIB outperforms other IB and calibration methods by enabling continuous optimization of $\\beta$. Our codes are available at https://github.com/sotakudo/fvib."],"url":"http://arxiv.org/abs/2402.01238v1","category":"cs.LG"}
{"created":"2024-02-02 08:56:52","title":"Connection of event shapes to the heavy-flavor baryon enhancement","abstract":"Recent results from ALICE and CMS show a low-transverse-momentum enhancement of charm baryon-to-meson production ratios over model predictions based on $e^+e^-$ collisions. This new development challenges the universality of fragmentation. We studied the charm-baryon enhancement in collision events generated by PYTHIA 8 and applied a color-reconnection model beyond leading color approximation. We proposed a measurement method based on several event-activity classifiers, to identify the origin of the charm-baryon enhancement. In this work we extend our studies to a new event classifier, flattenicity, that considers a broad pseudorapidity range. We have also studied the role of isospin and strangeness by comparing the production of different charmed baryons. The observables we explored provide a unique opportunity in the upcoming measurements from the high-luminosity LHC Run 3 period to better understand heavy-flavor fragmentation mechanisms, and will help the further development of models.","sentences":["Recent results from ALICE and CMS show a low-transverse-momentum enhancement of charm baryon-to-meson production ratios over model predictions based on $e^+e^-$ collisions.","This new development challenges the universality of fragmentation.","We studied the charm-baryon enhancement in collision events generated by PYTHIA 8 and applied a color-reconnection model beyond leading color approximation.","We proposed a measurement method based on several event-activity classifiers, to identify the origin of the charm-baryon enhancement.","In this work we extend our studies to a new event classifier, flattenicity, that considers a broad pseudorapidity range.","We have also studied the role of isospin and strangeness by comparing the production of different charmed baryons.","The observables we explored provide a unique opportunity in the upcoming measurements from the high-luminosity LHC Run 3 period to better understand heavy-flavor fragmentation mechanisms, and will help the further development of models."],"url":"http://arxiv.org/abs/2402.01234v1","category":"hep-ph"}
{"created":"2024-02-02 08:55:31","title":"Forecast of foreground cleaning strategies for AliCPT-1","abstract":"We report the test results of several independent foreground-cleaning pipelines used in the Ali CMB Polarization Telescope experiment (AliCPT-1), a high-altitude CMB imager in the Northern hemisphere with thousands of detectors dedicated to the search for a primordial CMB polarization $B$-mode signature. Based on simulated data from 4 detector modules and a single season of observation, which we refer to as DC1 data, we employ different and independent pipelines to examine the robustness and effectiveness of the estimates on foreground parameters and the primordial $B$-mode detection. The foreground-cleaning strategies used in the pipelines include the parametric method of template fitting (TF) and the non-parametric methods of the constrained internal linear combination (cILC), the analytical blind separation (ABS), and the generalized least squares (GLS). We examine the impact of possible foreground residuals on the estimate of the CMB tensor-to-scalar ratio ($r$) for each pipeline by changing the contamination components in the simulated maps and varying the foreground models and sky patches for various tests. According to the DC1 data with the simulation input value $r_{\\rm true}=0.023$, the foreground residual contamination levels in the TF/ABS/cILC/GLS pipelines are well within the corresponding statistical errors at the $2\\sigma$ level. For a selected patch with relatively stronger foreground contamination, all of the proposed pipelines perform robustly in testing. Furthermore, by utilizing the tension estimator, which helps identify significant residual foreground contamination in the detection of the primordial $B$-mode signal by quantifying the discrepancy between various $r$ measurements, we conclude that the presence of small foreground residuals does not lead to any significant inconsistency in the estimation of $r$.","sentences":["We report the test results of several independent foreground-cleaning pipelines used in the Ali CMB Polarization Telescope experiment (AliCPT-1), a high-altitude CMB imager in the Northern hemisphere with thousands of detectors dedicated to the search for a primordial CMB polarization $B$-mode signature.","Based on simulated data from 4 detector modules and a single season of observation, which we refer to as DC1 data, we employ different and independent pipelines to examine the robustness and effectiveness of the estimates on foreground parameters and the primordial $B$-mode detection.","The foreground-cleaning strategies used in the pipelines include the parametric method of template fitting (TF) and the non-parametric methods of the constrained internal linear combination (cILC), the analytical blind separation (ABS), and the generalized least squares (GLS).","We examine the impact of possible foreground residuals on the estimate of the CMB tensor-to-scalar ratio ($r$) for each pipeline by changing the contamination components in the simulated maps and varying the foreground models and sky patches for various tests.","According to the DC1 data with the simulation input value $r_{\\rm true}=0.023$, the foreground residual contamination levels in the TF/ABS/cILC/GLS pipelines are well within the corresponding statistical errors at the $2\\sigma$ level.","For a selected patch with relatively stronger foreground contamination, all of the proposed pipelines perform robustly in testing.","Furthermore, by utilizing the tension estimator, which helps identify significant residual foreground contamination in the detection of the primordial $B$-mode signal by quantifying the discrepancy between various $r$ measurements, we conclude that the presence of small foreground residuals does not lead to any significant inconsistency in the estimation of $r$."],"url":"http://arxiv.org/abs/2402.01233v1","category":"astro-ph.CO"}
{"created":"2024-02-02 08:48:42","title":"N=2 supersymmetry in the twistor description of higher-spin holography","abstract":"We study the holographic duality between higher-spin (HS) gravity in 4d and free vector models in 3d, with special attention to the role of N=2 supersymmetry (SUSY). For the type-A bosonic bulk theory, dual to spin-0 fields on the boundary, there exists a twistor-space description; this maps both single-trace boundary operators and linearized bulk fields to spacetime-independent twistor functions, whose HS-algebra products compute all boundary correlators. Here, we extend this description to the type-B bosonic theory (dual to spin-1/2 fields on the boundary), and to the supersymmetric theory containing both. A key role is played by boundary bilocals, which in type-A are dual to the Didenko-Vasiliev 1/2-BPS \"black hole\". We extend this to an infinite family of linearized 1/2-BPS \"black hole\" solutions. Remarkably, the full supersymmetric theory (along with the SUSY generators) fits in the same space of twistor functions as the type-A theory. Instead of two sets of bosonic bulk fields, the formalism sees one set of linearized fields, but with both types of boundary data allowed.","sentences":["We study the holographic duality between higher-spin (HS) gravity in 4d and free vector models in 3d, with special attention to the role of N=2 supersymmetry (SUSY).","For the type-A bosonic bulk theory, dual to spin-0 fields on the boundary, there exists a twistor-space description; this maps both single-trace boundary operators and linearized bulk fields to spacetime-independent twistor functions, whose HS-algebra products compute all boundary correlators.","Here, we extend this description to the type-B bosonic theory (dual to spin-1/2 fields on the boundary), and to the supersymmetric theory containing both.","A key role is played by boundary bilocals, which in type-A are dual to the Didenko-Vasiliev 1/2-BPS \"black hole\".","We extend this to an infinite family of linearized 1/2-BPS \"black hole\" solutions.","Remarkably, the full supersymmetric theory (along with the SUSY generators) fits in the same space of twistor functions as the type-A theory.","Instead of two sets of bosonic bulk fields, the formalism sees one set of linearized fields, but with both types of boundary data allowed."],"url":"http://arxiv.org/abs/2402.01228v1","category":"hep-th"}
{"created":"2024-02-02 08:46:57","title":"STAA-Net: A Sparse and Transferable Adversarial Attack for Speech Emotion Recognition","abstract":"Speech contains rich information on the emotions of humans, and Speech Emotion Recognition (SER) has been an important topic in the area of human-computer interaction. The robustness of SER models is crucial, particularly in privacy-sensitive and reliability-demanding domains like private healthcare. Recently, the vulnerability of deep neural networks in the audio domain to adversarial attacks has become a popular area of research. However, prior works on adversarial attacks in the audio domain primarily rely on iterative gradient-based techniques, which are time-consuming and prone to overfitting the specific threat model. Furthermore, the exploration of sparse perturbations, which have the potential for better stealthiness, remains limited in the audio domain. To address these challenges, we propose a generator-based attack method to generate sparse and transferable adversarial examples to deceive SER models in an end-to-end and efficient manner. We evaluate our method on two widely-used SER datasets, Database of Elicited Mood in Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP), and demonstrate its ability to generate successful sparse adversarial examples in an efficient manner. Moreover, our generated adversarial examples exhibit model-agnostic transferability, enabling effective adversarial attacks on advanced victim models.","sentences":["Speech contains rich information on the emotions of humans, and Speech Emotion Recognition (SER) has been an important topic in the area of human-computer interaction.","The robustness of SER models is crucial, particularly in privacy-sensitive and reliability-demanding domains like private healthcare.","Recently, the vulnerability of deep neural networks in the audio domain to adversarial attacks has become a popular area of research.","However, prior works on adversarial attacks in the audio domain primarily rely on iterative gradient-based techniques, which are time-consuming and prone to overfitting the specific threat model.","Furthermore, the exploration of sparse perturbations, which have the potential for better stealthiness, remains limited in the audio domain.","To address these challenges, we propose a generator-based attack method to generate sparse and transferable adversarial examples to deceive SER models in an end-to-end and efficient manner.","We evaluate our method on two widely-used SER datasets, Database of Elicited Mood in Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP), and demonstrate its ability to generate successful sparse adversarial examples in an efficient manner.","Moreover, our generated adversarial examples exhibit model-agnostic transferability, enabling effective adversarial attacks on advanced victim models."],"url":"http://arxiv.org/abs/2402.01227v1","category":"cs.SD"}
{"created":"2024-02-02 08:44:58","title":"Taut foliations from knot diagrams","abstract":"We prove that if a knot $K$ has a particular type of diagram then all non-trivial surgeries on $K$ contain a coorientable taut foliation. Knots admitting such diagrams include many two-bridge knots, many pretzel knots, many Montesinos knots and more generally all arborescent knots defined by weighted planar trees with more than one vertex such that $1)$ all weights have absolute value greater than one, and $2)$ there exists a weight with absolute value greater than two. The ideas involved in the proof can also be adapted to study surgeries on links and as an application we show that for all surgeries $M$ on the Borromean link it holds that $M$ is not an $L$-space if and only if $M$ contains a coorientable taut foliation.","sentences":["We prove that if a knot $K$ has a particular type of diagram then all non-trivial surgeries on $K$ contain a coorientable taut foliation.","Knots admitting such diagrams include many two-bridge knots, many pretzel knots, many Montesinos knots and more generally all arborescent knots defined by weighted planar trees with more than one vertex such that $1)$ all weights have absolute value greater than one, and $2)$ there exists a weight with absolute value greater than two.","The ideas involved in the proof can also be adapted to study surgeries on links and as an application we show that for all surgeries $M$ on the Borromean link it holds that $M$ is not an $L$-space if and only if $M$ contains a coorientable taut foliation."],"url":"http://arxiv.org/abs/2402.01225v1","category":"math.GT"}
{"created":"2024-02-02 08:41:15","title":"AI Code Generators for Security: Friend or Foe?","abstract":"Recent advances of artificial intelligence (AI) code generators are opening new opportunities in software security research, including misuse by malicious actors. We review use cases for AI code generators for security and introduce an evaluation benchmark.","sentences":["Recent advances of artificial intelligence (AI) code generators are opening new opportunities in software security research, including misuse by malicious actors.","We review use cases for AI code generators for security and introduce an evaluation benchmark."],"url":"http://arxiv.org/abs/2402.01219v1","category":"cs.CR"}
{"created":"2024-02-02 08:40:03","title":"Double or nothing: a Kolmogorov extension theorem for multitime (bi)probabilities in quantum mechanics","abstract":"The multitime probability distributions obtained by repeatedly probing a quantum system via the measurement of an observable generally violate Kolmogorov's consistency property. Therefore, one cannot interpret such distributions as the result of the sampling of a single trajectory. We show that, nonetheless, they do result from the sampling of one \\emph{pair} of trajectories. In this sense, rather than give up on trajectories, quantum mechanics requires to double down on them. To this purpose, we prove a generalization of the Kolmogorov extension theorem that applies to families of complex-valued bi-probability distributions (that is, defined on pairs of elements of the original sample spaces), and we employ this result in the quantum mechanical scenario. We also discuss the relation of our results with the quantum comb formalism.","sentences":["The multitime probability distributions obtained by repeatedly probing a quantum system via the measurement of an observable generally violate Kolmogorov's consistency property.","Therefore, one cannot interpret such distributions as the result of the sampling of a single trajectory.","We show that, nonetheless, they do result from the sampling of one \\emph{pair} of trajectories.","In this sense, rather than give up on trajectories, quantum mechanics requires to double down on them.","To this purpose, we prove a generalization of the Kolmogorov extension theorem that applies to families of complex-valued bi-probability distributions (that is, defined on pairs of elements of the original sample spaces), and we employ this result in the quantum mechanical scenario.","We also discuss the relation of our results with the quantum comb formalism."],"url":"http://arxiv.org/abs/2402.01218v1","category":"quant-ph"}
{"created":"2024-02-02 08:39:51","title":"Taming Uncertainty in Sparse-view Generalizable NeRF via Indirect Diffusion Guidance","abstract":"Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing novel views. However, their reliance on dense inputs and scene-specific optimization has limited their broader applicability. Generalizable NeRFs (Gen-NeRF), while intended to address this, often produce blurring artifacts in unobserved regions with sparse inputs, which are full of uncertainty. In this paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings. We assume that NeRF's inability to effectively mitigate this uncertainty stems from its inherent lack of generative capacity. Therefore, we innovatively propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address this uncertainty from a generative perspective by leveraging a distilled diffusion prior as guidance. Specifically, to avoid model confusion caused by directly regularizing with inconsistent samplings as in previous methods, our approach introduces a strategy to indirectly inject the inherently missing imagination into the learned implicit function through a diffusion-guided latent space. Empirical evaluation across various benchmarks demonstrates the superior performance of our approach in handling uncertainty with sparse inputs.","sentences":["Neural Radiance Fields (NeRF) have demonstrated effectiveness in synthesizing novel views.","However, their reliance on dense inputs and scene-specific optimization has limited their broader applicability.","Generalizable NeRFs (Gen-NeRF), while intended to address this, often produce blurring artifacts in unobserved regions with sparse inputs, which are full of uncertainty.","In this paper, we aim to diminish the uncertainty in Gen-NeRF for plausible renderings.","We assume that NeRF's inability to effectively mitigate this uncertainty stems from its inherent lack of generative capacity.","Therefore, we innovatively propose an Indirect Diffusion-guided NeRF framework, termed ID-NeRF, to address this uncertainty from a generative perspective by leveraging a distilled diffusion prior as guidance.","Specifically, to avoid model confusion caused by directly regularizing with inconsistent samplings as in previous methods, our approach introduces a strategy to indirectly inject the inherently missing imagination into the learned implicit function through a diffusion-guided latent space.","Empirical evaluation across various benchmarks demonstrates the superior performance of our approach in handling uncertainty with sparse inputs."],"url":"http://arxiv.org/abs/2402.01217v1","category":"cs.CV"}
{"created":"2024-02-02 08:39:39","title":"Robust Commutation Design: Applied to Switched Reluctance Motors","abstract":"Switched Reluctance Motors (SRMs) are cost-effective electric actuators that utilize magnetic reluctance to generate torque, with torque ripple arising from unaccounted manufacturing defects in the rotor tooth geometry. This paper aims to design a versatile, resource-efficient commutation function for accurate closed-loop control of a range of SRMs, mitigating torque ripple despite manufacturing variations across SRMs and individual rotor teeth. The developed commutation function optimally distributes current between coils by leveraging the variance in the torque-current-angle model and is designed with few parameters for easy integration on affordable hardware. Monte Carlo simulations and experimental results show a tracking error reduction of up to 31% and 11%, respectively. The developed approach is beneficial for applications using a single driver for multiple systems and those constrained by memory or modeling effort, providing an economical solution for improved tracking performance and reduced acoustic noise.","sentences":["Switched Reluctance Motors (SRMs) are cost-effective electric actuators that utilize magnetic reluctance to generate torque, with torque ripple arising from unaccounted manufacturing defects in the rotor tooth geometry.","This paper aims to design a versatile, resource-efficient commutation function for accurate closed-loop control of a range of SRMs, mitigating torque ripple despite manufacturing variations across SRMs and individual rotor teeth.","The developed commutation function optimally distributes current between coils by leveraging the variance in the torque-current-angle model and is designed with few parameters for easy integration on affordable hardware.","Monte Carlo simulations and experimental results show a tracking error reduction of up to 31% and 11%, respectively.","The developed approach is beneficial for applications using a single driver for multiple systems and those constrained by memory or modeling effort, providing an economical solution for improved tracking performance and reduced acoustic noise."],"url":"http://arxiv.org/abs/2402.01216v1","category":"eess.SY"}
{"created":"2024-02-02 08:38:50","title":"Intraday Power Trading for Imbalance Markets: An Adaptive Risk-Averse Strategy using Mixture Models","abstract":"Efficient markets are characterised by profit-driven participants continuously refining their positions towards the latest insights. Margins for profit generation are generally small, shaping a difficult landscape for automated trading strategies. This paper introduces a novel, fully-automated cross-border intraday (XBID) trading strategy tailored for single-price imbalance energy markets. This strategy relies on a strategically devised mixture model to predict future system imbalance prices, which, upon benchmarking against several state-of-the-art models, outperforms its counterparts across every metric. However, these models were fit to a finite amount of training data typically causing them to perform worse on unseen data when compared to their training set. To address this issue, a coherent risk measure is added to the cost function to take additional uncertainties in the prediction model into account. This paper introduces a methodology to select the tuning parameter of this risk measure adaptively by continuously quantifying the model accuracy on a window of recently observed data. The performance of this strategy is validated with a simulation on the Belgian energy market using real-time market data. The adaptive tuning approach enables the strategy to achieve higher absolute profits with a reduced number of trades.","sentences":["Efficient markets are characterised by profit-driven participants continuously refining their positions towards the latest insights.","Margins for profit generation are generally small, shaping a difficult landscape for automated trading strategies.","This paper introduces a novel, fully-automated cross-border intraday (XBID) trading strategy tailored for single-price imbalance energy markets.","This strategy relies on a strategically devised mixture model to predict future system imbalance prices, which, upon benchmarking against several state-of-the-art models, outperforms its counterparts across every metric.","However, these models were fit to a finite amount of training data typically causing them to perform worse on unseen data when compared to their training set.","To address this issue, a coherent risk measure is added to the cost function to take additional uncertainties in the prediction model into account.","This paper introduces a methodology to select the tuning parameter of this risk measure adaptively by continuously quantifying the model accuracy on a window of recently observed data.","The performance of this strategy is validated with a simulation on the Belgian energy market using real-time market data.","The adaptive tuning approach enables the strategy to achieve higher absolute profits with a reduced number of trades."],"url":"http://arxiv.org/abs/2402.01215v1","category":"cs.CE"}
{"created":"2024-02-02 08:37:38","title":"TSJNet: A Multi-modality Target and Semantic Awareness Joint-driven Image Fusion Network","abstract":"Multi-modality image fusion involves integrating complementary information from different modalities into a single image. Current methods primarily focus on enhancing image fusion with a single advanced task such as incorporating semantic or object-related information into the fusion process. This method creates challenges in achieving multiple objectives simultaneously. We introduce a target and semantic awareness joint-driven fusion network called TSJNet. TSJNet comprises fusion, detection, and segmentation subnetworks arranged in a series structure. It leverages object and semantically relevant information derived from dual high-level tasks to guide the fusion network. Additionally, We propose a local significant feature extraction module with a double parallel branch structure to fully capture the fine-grained features of cross-modal images and foster interaction among modalities, targets, and segmentation information. We conducted extensive experiments on four publicly available datasets (MSRS, M3FD, RoadScene, and LLVIP). The results demonstrate that TSJNet can generate visually pleasing fused results, achieving an average increase of 2.84% and 7.47% in object detection and segmentation mAP @0.5 and mIoU, respectively, compared to the state-of-the-art methods.","sentences":["Multi-modality image fusion involves integrating complementary information from different modalities into a single image.","Current methods primarily focus on enhancing image fusion with a single advanced task such as incorporating semantic or object-related information into the fusion process.","This method creates challenges in achieving multiple objectives simultaneously.","We introduce a target and semantic awareness joint-driven fusion network called TSJNet.","TSJNet comprises fusion, detection, and segmentation subnetworks arranged in a series structure.","It leverages object and semantically relevant information derived from dual high-level tasks to guide the fusion network.","Additionally, We propose a local significant feature extraction module with a double parallel branch structure to fully capture the fine-grained features of cross-modal images and foster interaction among modalities, targets, and segmentation information.","We conducted extensive experiments on four publicly available datasets (MSRS, M3FD, RoadScene, and LLVIP).","The results demonstrate that TSJNet can generate visually pleasing fused results, achieving an average increase of 2.84% and 7.47% in object detection and segmentation mAP @0.5 and mIoU, respectively, compared to the state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.01212v1","category":"cs.CV"}
{"created":"2024-02-02 08:37:07","title":"Big Bang Nucleosynthesis constraints on the Energy-Momentum Squared Gravity: The $\\mathbb{T}^{2}$ model","abstract":"A scale-independent energy-momentum squared gravity (EMSG) allows different gravitational couplings for different types of sources and has been proven to have interesting implications in cosmology. In this paper, the Big Bang Nucleosynthesis (BBN) formalism and the latest observational constraints are being used in order to extract constraints on this class of modified gravity models. The model has been constrained using the light element reaction rates. Using the tight constraint from BBN on the correction term in the Friedman equation due to EMSG, we find a significant deviation in the matter power spectrum of the cosmic microwave background (CMB) even for a very small allowed value of the modification parameter, thus pointing out the sensitivity of the matter power spectrum at the small scales.","sentences":["A scale-independent energy-momentum squared gravity (EMSG) allows different gravitational couplings for different types of sources and has been proven to have interesting implications in cosmology.","In this paper, the Big Bang Nucleosynthesis (BBN) formalism and the latest observational constraints are being used in order to extract constraints on this class of modified gravity models.","The model has been constrained using the light element reaction rates.","Using the tight constraint from BBN on the correction term in the Friedman equation due to EMSG, we find a significant deviation in the matter power spectrum of the cosmic microwave background (CMB) even for a very small allowed value of the modification parameter, thus pointing out the sensitivity of the matter power spectrum at the small scales."],"url":"http://arxiv.org/abs/2402.01210v1","category":"astro-ph.CO"}
{"created":"2024-02-02 08:34:30","title":"Solution of the Probabilistic Lambert's Problem: Optimal Transport Approach","abstract":"The deterministic variant of the Lambert's problem was posed by Lambert in the 18th century and its solution for conic trajectory has been derived by many, including Euler, Lambert, Lagrange, Laplace, Gauss and Legendre. The solution amounts to designing velocity control for steering a spacecraft from a given initial to a given terminal position subject to gravitational potential and flight time constraints. In recent years, a probabilistic variant of the Lambert's problem has received attention in the aerospace community where the endpoint position constraints are softened to endpoint joint probability distributions over the respective positions. Such probabilistic specifications account for the estimation errors, modeling uncertainties, etc. Building on a deterministic optimal control reformulation via analytical mechanics, we show that the probabilistic Lambert's problem is a generalized dynamic optimal mass transport problem where the gravitational potential plays the role of an additive state cost. This allows us to rigorously prove the existence-uniqueness of the solution for the probabilistic Lambert problem both with and without process noise. In the latter case, the problem and its solution correspond to a generalized Schr\\\"odinger bridge, much like how classical Schrodinger bridge can be seen as stochastic regularization of the optimal mass transport. We deduce the large deviation principle enjoyed by the Lambertian Schr\\\"odinger bridge. Leveraging these newfound connections, we design a computational algorithm to illustrate the nonparametric numerical solution of the probabilistic Lambert's problem.","sentences":["The deterministic variant of the Lambert's problem was posed by Lambert in the 18th century and its solution for conic trajectory has been derived by many, including Euler, Lambert, Lagrange, Laplace, Gauss and Legendre.","The solution amounts to designing velocity control for steering a spacecraft from a given initial to a given terminal position subject to gravitational potential and flight time constraints.","In recent years, a probabilistic variant of the Lambert's problem has received attention in the aerospace community where the endpoint position constraints are softened to endpoint joint probability distributions over the respective positions.","Such probabilistic specifications account for the estimation errors, modeling uncertainties, etc.","Building on a deterministic optimal control reformulation via analytical mechanics, we show that the probabilistic Lambert's problem is a generalized dynamic optimal mass transport problem where the gravitational potential plays the role of an additive state cost.","This allows us to rigorously prove the existence-uniqueness of the solution for the probabilistic Lambert problem both with and without process noise.","In the latter case, the problem and its solution correspond to a generalized Schr\\\"odinger bridge, much like how classical Schrodinger bridge can be seen as stochastic regularization of the optimal mass transport.","We deduce the large deviation principle enjoyed by the Lambertian Schr\\\"odinger bridge.","Leveraging these newfound connections, we design a computational algorithm to illustrate the nonparametric numerical solution of the probabilistic Lambert's problem."],"url":"http://arxiv.org/abs/2402.01209v1","category":"math.OC"}
{"created":"2024-02-02 08:26:42","title":"Location Agnostic Adaptive Rain Precipitation Prediction using Deep Learning","abstract":"Rain precipitation prediction is a challenging task as it depends on weather and meteorological features which vary from location to location. As a result, a prediction model that performs well at one location does not perform well at other locations due to the distribution shifts. In addition, due to global warming, the weather patterns are changing very rapidly year by year which creates the possibility of ineffectiveness of those models even at the same location as time passes. In our work, we have proposed an adaptive deep learning-based framework in order to provide a solution to the aforementioned challenges. Our method can generalize the model for the prediction of precipitation for any location where the methods without adaptation fail. Our method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a deep neural network for predicting the precipitation of Paris, Los Angeles, and Tokyo, respectively.","sentences":["Rain precipitation prediction is a challenging task as it depends on weather and meteorological features which vary from location to location.","As a result, a prediction model that performs well at one location does not perform well at other locations due to the distribution shifts.","In addition, due to global warming, the weather patterns are changing very rapidly year by year which creates the possibility of ineffectiveness of those models even at the same location as time passes.","In our work, we have proposed an adaptive deep learning-based framework in order to provide a solution to the aforementioned challenges.","Our method can generalize the model for the prediction of precipitation for any location where the methods without adaptation fail.","Our method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a deep neural network for predicting the precipitation of Paris, Los Angeles, and Tokyo, respectively."],"url":"http://arxiv.org/abs/2402.01208v1","category":"cs.LG"}
{"created":"2024-02-02 08:25:32","title":"Efficient Causal Graph Discovery Using Large Language Models","abstract":"We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.","sentences":["We propose a novel framework that leverages LLMs for full causal graph discovery.","While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs.","In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries.","We also show that the proposed method can easily incorporate observational data when available, to improve performance.","In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes.","The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains."],"url":"http://arxiv.org/abs/2402.01207v1","category":"cs.LG"}
{"created":"2024-02-02 17:16:23","title":"Deep Active Learning for Data Mining from Conflict Text Corpora","abstract":"High-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like UCDP GED, ACLED etc. However, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data. Information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data. However, most datasets rely on a rich corpus of textual data allowing further mining of further information connected to each event. This paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input. Active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics. The approach shows performance similar to human (gold-standard) coding while reducing the amount of required human annotation by as much as 99%.","sentences":["High-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like UCDP GED, ACLED etc.","However, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data.","Information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data.","However, most datasets rely on a rich corpus of textual data allowing further mining of further information connected to each event.","This paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input.","Active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics.","The approach shows performance similar to human (gold-standard) coding while reducing the amount of required human annotation by as much as 99%."],"url":"http://arxiv.org/abs/2402.01577v1","category":"cs.CY"}
{"created":"2024-02-02 17:06:18","title":"Doping Liquid Argon with Xenon in ProtoDUNE Single-Phase: Effects on Scintillation Light","abstract":"Doping of liquid argon TPCs (LArTPCs) with a small concentration of xenon is a technique for light-shifting and facilitates the detection of the liquid argon scintillation light. In this paper, we present the results of the first doping test ever performed in a kiloton-scale LArTPC. From February to May 2020, we carried out this special run in the single-phase DUNE Far Detector prototype (ProtoDUNE-SP) at CERN, featuring 770 t of total liquid argon mass with 410 t of fiducial mass. The goal of the run was to measure the light and charge response of the detector to the addition of xenon, up to a concentration of 18.8 ppm. The main purpose was to test the possibility for reduction of non-uniformities in light collection, caused by deployment of photon detectors only within the anode planes. Light collection was analysed as a function of the xenon concentration, by using the pre-existing photon detection system (PDS) of ProtoDUNE-SP and an additional smaller set-up installed specifically for this run. In this paper we first summarize our current understanding of the argon-xenon energy transfer process and the impact of the presence of nitrogen in argon with and without xenon dopant. We then describe the key elements of ProtoDUNE-SP and the injection method deployed. Two dedicated photon detectors were able to collect the light produced by xenon and the total light. The ratio of these components was measured to be about 0.65 as 18.8 ppm of xenon were injected. We performed studies of the collection efficiency as a function of the distance between tracks and light detectors, demonstrating enhanced uniformity of response for the anode-mounted PDS. We also show that xenon doping can substantially recover light losses due to contamination of the liquid argon by nitrogen.","sentences":["Doping of liquid argon TPCs (LArTPCs) with a small concentration of xenon is a technique for light-shifting and facilitates the detection of the liquid argon scintillation light.","In this paper, we present the results of the first doping test ever performed in a kiloton-scale LArTPC.","From February to May 2020, we carried out this special run in the single-phase DUNE Far Detector prototype (ProtoDUNE-SP) at CERN, featuring 770 t of total liquid argon mass with 410 t of fiducial mass.","The goal of the run was to measure the light and charge response of the detector to the addition of xenon, up to a concentration of 18.8 ppm.","The main purpose was to test the possibility for reduction of non-uniformities in light collection, caused by deployment of photon detectors only within the anode planes.","Light collection was analysed as a function of the xenon concentration, by using the pre-existing photon detection system (PDS) of ProtoDUNE-SP and an additional smaller set-up installed specifically for this run.","In this paper we first summarize our current understanding of the argon-xenon energy transfer process and the impact of the presence of nitrogen in argon with and without xenon dopant.","We then describe the key elements of ProtoDUNE-SP and the injection method deployed.","Two dedicated photon detectors were able to collect the light produced by xenon and the total light.","The ratio of these components was measured to be about 0.65 as 18.8 ppm of xenon were injected.","We performed studies of the collection efficiency as a function of the distance between tracks and light detectors, demonstrating enhanced uniformity of response for the anode-mounted PDS.","We also show that xenon doping can substantially recover light losses due to contamination of the liquid argon by nitrogen."],"url":"http://arxiv.org/abs/2402.01568v1","category":"physics.ins-det"}
{"created":"2024-02-02 16:10:36","title":"Non-Linear Analog Processing Gains in Task-Based Quantization","abstract":"In task-based quantization, a multivariate analog signal is transformed into a digital signal using a limited number of low-resolution analog-to-digital converters (ADCs). This process aims to minimize a fidelity criterion, which is assessed against an unobserved task variable that is correlated with the analog signal. The scenario models various applications of interest such as channel estimation, medical imaging applications, and object localization. This work explores the integration of analog processing components -- such as analog delay elements, polynomial operators, and envelope detectors -- prior to ADC quantization. Specifically, four scenarios, involving different collections of analog processing operators are considered: (i) arbitrary polynomial operators with analog delay elements, (ii) limited-degree polynomial operators, excluding delay elements, (iii) sequences of envelope detectors, and (iv) a combination of analog delay elements and linear combiners. For each scenario, the minimum achievable distortion is quantified through derivation of computable expressions in various statistical settings. It is shown that analog processing can significantly reduce the distortion in task reconstruction. Numerical simulations in a Gaussian example are provided to give further insights into the aforementioned analog processing gains.","sentences":["In task-based quantization, a multivariate analog signal is transformed into a digital signal using a limited number of low-resolution analog-to-digital converters (ADCs).","This process aims to minimize a fidelity criterion, which is assessed against an unobserved task variable that is correlated with the analog signal.","The scenario models various applications of interest such as channel estimation, medical imaging applications, and object localization.","This work explores the integration of analog processing components -- such as analog delay elements, polynomial operators, and envelope detectors -- prior to ADC quantization.","Specifically, four scenarios, involving different collections of analog processing operators are considered: (i) arbitrary polynomial operators with analog delay elements, (ii) limited-degree polynomial operators, excluding delay elements, (iii) sequences of envelope detectors, and (iv) a combination of analog delay elements and linear combiners.","For each scenario, the minimum achievable distortion is quantified through derivation of computable expressions in various statistical settings.","It is shown that analog processing can significantly reduce the distortion in task reconstruction.","Numerical simulations in a Gaussian example are provided to give further insights into the aforementioned analog processing gains."],"url":"http://arxiv.org/abs/2402.01525v1","category":"cs.IT"}
{"created":"2024-02-02 15:54:53","title":"Mapping the Multiverse of Latent Representations","abstract":"Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations. Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations. Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions. As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperparameter search spaces.","sentences":["Echoing recent calls to counter reliability and robustness concerns in machine learning via multiverse analysis, we present PRESTO, a principled framework for mapping the multiverse of machine-learning models that rely on latent representations.","Although such models enjoy widespread adoption, the variability in their embeddings remains poorly understood, resulting in unnecessary complexity and untrustworthy representations.","Our framework uses persistent homology to characterize the latent spaces arising from different combinations of diverse machine-learning methods, (hyper)parameter configurations, and datasets, allowing us to measure their pairwise (dis)similarity and statistically reason about their distributions.","As we demonstrate both theoretically and empirically, our pipeline preserves desirable properties of collections of latent representations, and it can be leveraged to perform sensitivity analysis, detect anomalous embeddings, or efficiently and effectively navigate hyperparameter search spaces."],"url":"http://arxiv.org/abs/2402.01514v1","category":"cs.LG"}
{"created":"2024-02-02 15:12:35","title":"Di-NeRF: Distributed NeRF for Collaborative Learning with Unknown Relative Poses","abstract":"Collaborative mapping of unknown environments can be done faster and more robustly than a single robot. However, a collaborative approach requires a distributed paradigm to be scalable and deal with communication issues. This work presents a fully distributed algorithm enabling a group of robots to collectively optimize the parameters of a Neural Radiance Field (NeRF). The algorithm involves the communication of each robot's trained NeRF parameters over a mesh network, where each robot trains its NeRF and has access to its own visual data only. Additionally, the relative poses of all robots are jointly optimized alongside the model parameters, enabling mapping with unknown relative camera poses. We show that multi-robot systems can benefit from differentiable and robust 3D reconstruction optimized from multiple NeRFs. Experiments on real-world and synthetic data demonstrate the efficiency of the proposed algorithm. See the website of the project for videos of the experiments and supplementary material(https://sites.google.com/view/di-nerf/home).","sentences":["Collaborative mapping of unknown environments can be done faster and more robustly than a single robot.","However, a collaborative approach requires a distributed paradigm to be scalable and deal with communication issues.","This work presents a fully distributed algorithm enabling a group of robots to collectively optimize the parameters of a Neural Radiance Field (NeRF).","The algorithm involves the communication of each robot's trained NeRF parameters over a mesh network, where each robot trains its NeRF and has access to its own visual data only.","Additionally, the relative poses of all robots are jointly optimized alongside the model parameters, enabling mapping with unknown relative camera poses.","We show that multi-robot systems can benefit from differentiable and robust 3D reconstruction optimized from multiple NeRFs.","Experiments on real-world and synthetic data demonstrate the efficiency of the proposed algorithm.","See the website of the project for videos of the experiments and supplementary material(https://sites.google.com/view/di-nerf/home)."],"url":"http://arxiv.org/abs/2402.01485v1","category":"cs.RO"}
{"created":"2024-02-02 13:56:01","title":"THz spin-wave excitations in the transverse conical phase of BiFeO$_3$","abstract":"Although BiFeO$_3$ is one of the most studied multiferroic materials, recent magnetization and neutron scattering studies have revealed a new magnetic phase in this compound - the transverse conical phase. To study the collective spin excitations of this phase, we performed THz spectroscopy in magnetic fields up to 17 T at and above room temperature. We observed five spin-wave branches in the magnetic phase with long wavelength conical modulation. Using a numerical spin dynamics model we found two kinds of excitations with magnetic moments oscillating either along or perpendicular to the static fields. Remarkably, we detected strong directional dichroism, an optical manifestation of the magnetoelectric effect, for two spin-wave modes of the conical phase. According to our experiments, the stability of the conical state is sensitive to the magnetic field history and it can become (meta)stable at or close to zero magnetic field, which may allow exploiting its magnetoelectric properties at room temperature.","sentences":["Although BiFeO$_3$ is one of the most studied multiferroic materials, recent magnetization and neutron scattering studies have revealed a new magnetic phase in this compound - the transverse conical phase.","To study the collective spin excitations of this phase, we performed THz spectroscopy in magnetic fields up to 17 T at and above room temperature.","We observed five spin-wave branches in the magnetic phase with long wavelength conical modulation.","Using a numerical spin dynamics model we found two kinds of excitations with magnetic moments oscillating either along or perpendicular to the static fields.","Remarkably, we detected strong directional dichroism, an optical manifestation of the magnetoelectric effect, for two spin-wave modes of the conical phase.","According to our experiments, the stability of the conical state is sensitive to the magnetic field history and it can become (meta)stable at or close to zero magnetic field, which may allow exploiting its magnetoelectric properties at room temperature."],"url":"http://arxiv.org/abs/2402.01417v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 13:35:05","title":"A comparison study of supervised learning techniques for the approximation of high dimensional functions and feedback control","abstract":"Approximation of high dimensional functions is in the focus of machine learning and data-based scientific computing. In many applications, empirical risk minimisation techniques over nonlinear model classes are employed. Neural networks, kernel methods and tensor decomposition techniques are among the most popular model classes. We provide a numerical study comparing the performance of these methods on various high-dimensional functions with focus on optimal control problems, where the collection of the dataset is based on the application of the State-Dependent Riccati Equation.","sentences":["Approximation of high dimensional functions is in the focus of machine learning and data-based scientific computing.","In many applications, empirical risk minimisation techniques over nonlinear model classes are employed.","Neural networks, kernel methods and tensor decomposition techniques are among the most popular model classes.","We provide a numerical study comparing the performance of these methods on various high-dimensional functions with focus on optimal control problems, where the collection of the dataset is based on the application of the State-Dependent Riccati Equation."],"url":"http://arxiv.org/abs/2402.01402v1","category":"math.NA"}
{"created":"2024-02-02 13:28:03","title":"penalizedclr: an R package for penalized conditional logistic regression for integration of multiple omics layers","abstract":"The matched case-control design, up until recently mostly pertinent to epidemiological studies, is becoming customary in biomedical applications as well. For instance, in omics studies, it is quite common to compare cancer and healthy tissue from the same patient. Furthermore, researchers today routinely collect data from various and variable sources that they wish to relate to the case-control status. This highlights the need to develop and implement statistical methods that can take these tendencies into account. We present an R package penalizedclr, that provides an implementation of the penalized conditional logistic regression model for analyzing matched case-control studies. It allows for different penalties for different blocks of covariates, and it is therefore particularly useful in the presence of multi-source omics data. Both L1 and L2 penalties are implemented. Additionally, the package implements stability selection for variable selection in the considered regression model. The proposed method fills a gap in the available software for fitting high-dimensional conditional logistic regression model accounting for the matched design and block structure of predictors/features. The output consists of a set of selected variables that are significantly associated with case-control status. These features can then be investigated in terms of functional interpretation or validation in further, more targeted studies.","sentences":["The matched case-control design, up until recently mostly pertinent to epidemiological studies, is becoming customary in biomedical applications as well.","For instance, in omics studies, it is quite common to compare cancer and healthy tissue from the same patient.","Furthermore, researchers today routinely collect data from various and variable sources that they wish to relate to the case-control status.","This highlights the need to develop and implement statistical methods that can take these tendencies into account.","We present an R package penalizedclr, that provides an implementation of the penalized conditional logistic regression model for analyzing matched case-control studies.","It allows for different penalties for different blocks of covariates, and it is therefore particularly useful in the presence of multi-source omics data.","Both L1 and L2 penalties are implemented.","Additionally, the package implements stability selection for variable selection in the considered regression model.","The proposed method fills a gap in the available software for fitting high-dimensional conditional logistic regression model accounting for the matched design and block structure of predictors/features.","The output consists of a set of selected variables that are significantly associated with case-control status.","These features can then be investigated in terms of functional interpretation or validation in further, more targeted studies."],"url":"http://arxiv.org/abs/2402.01398v1","category":"stat.ME"}
{"created":"2024-02-02 13:15:48","title":"ATLAS Run 2 searches for electroweak production of supersymmetric particles interpreted within the pMSSM","abstract":"A summary of the constraints from searches performed by the ATLAS Collaboration for the electroweak production of charginos and neutralinos is presented. Results from eight separate ATLAS searches are considered, each using 140 fb$^{-1}$ of proton-proton data at a centre-of-mass energy of $\\sqrt{s}$=13 TeV collected at the Large Hadron Collider during its second data-taking run. The results are interpreted in the context of the 19-parameter phenomenological minimal supersymmetric standard model, where R-parity conservation is assumed and the lightest supersymmetric particle is assumed to be the lightest neutralino. Constraints from previous electroweak, flavour and dark matter related measurements are also considered. The results are presented in terms of constraints on supersymmetric particle masses and are compared with limits from simplified models. Also shown is the impact of ATLAS searches on parameters such as the dark matter relic density and the spin-dependent and spin-independent scattering cross-sections targeted by direct dark matter detection experiments. The Higgs boson and Z boson `funnel regions', where a low-mass neutralino would not oversaturate the dark matter relic abundance, are almost completely excluded by the considered constraints. Example spectra for non-excluded supersymmetric models with light charginos and neutralinos are also presented.","sentences":["A summary of the constraints from searches performed by the ATLAS Collaboration for the electroweak production of charginos and neutralinos is presented.","Results from eight separate ATLAS searches are considered, each using 140 fb$^{-1}$ of proton-proton data at a centre-of-mass energy of $\\sqrt{s}$=13 TeV collected at the Large Hadron Collider during its second data-taking run.","The results are interpreted in the context of the 19-parameter phenomenological minimal supersymmetric standard model, where R-parity conservation is assumed and the lightest supersymmetric particle is assumed to be the lightest neutralino.","Constraints from previous electroweak, flavour and dark matter related measurements are also considered.","The results are presented in terms of constraints on supersymmetric particle masses and are compared with limits from simplified models.","Also shown is the impact of ATLAS searches on parameters such as the dark matter relic density and the spin-dependent and spin-independent scattering cross-sections targeted by direct dark matter detection experiments.","The Higgs boson and Z boson `funnel regions', where a low-mass neutralino would not oversaturate the dark matter relic abundance, are almost completely excluded by the considered constraints.","Example spectra for non-excluded supersymmetric models with light charginos and neutralinos are also presented."],"url":"http://arxiv.org/abs/2402.01392v1","category":"hep-ex"}
{"created":"2024-02-02 11:41:41","title":"Measurements of the branching fraction ratio $\\mathcal{B}(\u03c6\\to \u03bc^+\u03bc^-)/\\mathcal{B}(\u03c6\\to e^+e^-)$ with charm meson decays","abstract":"Measurements of the branching fraction ratio ${\\mathcal{B}(\\phi \\to \\mu^+ \\mu^-)/\\mathcal{B}(\\phi\\to e^+e^-)}$ with ${D_{s}^{+} \\to \\pi^{+} \\phi}$ and ${D^{+} \\to \\pi^{+} \\phi}$ decays, denoted $R^{s}_{\\phi \\pi}$ and $R^{d}_{\\phi \\pi}$, are presented. The analysis is performed using a dataset corresponding to an integrated luminosity of 5.4$\\,\\rm{fb}^{-1}$ of $pp$ collision data collected with the LHCb experiment. The branching fractions are normalised with respect to the ${B^{+} \\to K^{+} J/\\psi(\\to e^+e^-)}$ and ${B^{+} \\to K^{+} J/\\psi(\\to \\mu^+\\mu^-)}$ decay modes. The combination of the results yields $$ R_{\\phi \\pi} = 1.022 \\pm 0.012 \\,({\\rm stat}) \\, \\pm 0.048 \\,({\\rm syst}). $$ The result is compatible with previous measurements of the $\\phi \\to \\ell^{+}\\ell^{-}$ branching fractions and predictions based on the Standard Model.","sentences":["Measurements of the branching fraction ratio ${\\mathcal{B}(\\phi \\to \\mu^+ \\mu^-)/\\mathcal{B}(\\phi\\to e^+e^-)}$ with ${D_{s}^{+} \\to \\pi^{+} \\phi}$ and ${D^{+} \\to \\pi^{+} \\phi}$ decays, denoted $R^{s}_{\\phi \\pi}$ and $R^{d}_{\\phi \\pi}$, are presented.","The analysis is performed using a dataset corresponding to an integrated luminosity of 5.4$\\,\\rm{fb}^{-1}$ of $pp$ collision data collected with the LHCb experiment.","The branching fractions are normalised with respect to the ${B^{+} \\to K^{+} J/\\psi(\\to e^+e^-)}$ and ${B^{+} \\to K^{+} J/\\psi(\\to \\mu^+\\mu^-)}$ decay modes.","The combination of the results yields $$ R_{\\phi \\pi} = 1.022 \\pm 0.012 \\,({\\rm stat}) \\, \\pm 0.048 \\,({\\rm syst}).","$$ The result is compatible with previous measurements of the $\\phi \\to \\ell^{+}\\ell^{-}$ branching fractions and predictions based on the Standard Model."],"url":"http://arxiv.org/abs/2402.01336v1","category":"hep-ex"}
{"created":"2024-02-02 18:56:43","title":"Dynamics of Onsager vortex clustering in decaying turbulent polariton quantum fluids","abstract":"We investigate the turbulent properties of a confined driven-dissipative polariton quantum fluid after a pulsed excitation. Using numerical simulations, we provide insight into the vortex clustering processes that emerge during the relaxation dynamics of the initially injected vortex cloud. A confrontation between conservative and non-conservative dynamics reveals that the onset of clusterization strongly depends on the interplay between the different characteristic system lengths and time scales at stake, with an additional time scale due to dissipation in the non-conservative case. Quantification of the clustering observables allows us to numerically characterize the optimal conditions for observing Onsager condensation in decaying polariton systems, demonstrating its experimental reachability under pulse excitation. These findings hold significance for exploring the onset of turbulent dynamics in open systems, spanning both classical and quantum domains.","sentences":["We investigate the turbulent properties of a confined driven-dissipative polariton quantum fluid after a pulsed excitation.","Using numerical simulations, we provide insight into the vortex clustering processes that emerge during the relaxation dynamics of the initially injected vortex cloud.","A confrontation between conservative and non-conservative dynamics reveals that the onset of clusterization strongly depends on the interplay between the different characteristic system lengths and time scales at stake, with an additional time scale due to dissipation in the non-conservative case.","Quantification of the clustering observables allows us to numerically characterize the optimal conditions for observing Onsager condensation in decaying polariton systems, demonstrating its experimental reachability under pulse excitation.","These findings hold significance for exploring the onset of turbulent dynamics in open systems, spanning both classical and quantum domains."],"url":"http://arxiv.org/abs/2402.01637v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-02 18:45:12","title":"Truncation technique for variational quantum eigensolver for Molecular Hamiltonians","abstract":"The variational quantum eigensolver (VQE) is one of the most promising quantum algorithms for the near-term noisy intermediate-scale quantum (NISQ) devices. The VQE typically involves finding the minimum energy of a quantum Hamiltonian through classical optimization of a parametrized quantum ansatz. One of the bottlenecks in VQEs is the number of quantum circuits to be measured. In this work, we propose a physically intuitive truncation technique that starts the optimization procedure with a truncated Hamiltonian and then gradually transitions to the optimization for the original Hamiltonian via an operator classification method. This strategy allows us to reduce the required number of evaluations for the expectation value of Hamiltonian on a quantum computer. The reduction in required quantum resources for our strategy is substantial and likely scales with the system size. With numerical simulations, we demonstrate our method for various molecular systems.","sentences":["The variational quantum eigensolver (VQE) is one of the most promising quantum algorithms for the near-term noisy intermediate-scale quantum (NISQ) devices.","The VQE typically involves finding the minimum energy of a quantum Hamiltonian through classical optimization of a parametrized quantum ansatz.","One of the bottlenecks in VQEs is the number of quantum circuits to be measured.","In this work, we propose a physically intuitive truncation technique that starts the optimization procedure with a truncated Hamiltonian and then gradually transitions to the optimization for the original Hamiltonian via an operator classification method.","This strategy allows us to reduce the required number of evaluations for the expectation value of Hamiltonian on a quantum computer.","The reduction in required quantum resources for our strategy is substantial and likely scales with the system size.","With numerical simulations, we demonstrate our method for various molecular systems."],"url":"http://arxiv.org/abs/2402.01630v1","category":"quant-ph"}
{"created":"2024-02-02 18:44:16","title":"Asymmetry of the spectral lines of the coronal hole and quiet Sun in the transition region","abstract":"The asymmetry of line profiles, i.e., the secondary component, is crucial to understanding the energy release of coronal holes (CH), quiet sun (QS), and bright points (BPs). We investigate the asymmetry of Si IV 1393.75 {\\AA} of the transition-region (TR) line recorded by Interface Region Imaging Spectrometer (IRIS) and co-spatial-temporal Atmospheric Imaging Assembly (AIA) and Helioseismic and Magnetic Imager (HMI) data onboard Solar Dynamics Observatory (SDO) for three time series on 26 April 2015, 24 July 2014, 26 July 2014. Most asymmetric profiles are in the complex magnetic field regions of the networks. The asymmetric profiles are fitted with both single and double Gaussian models. The mean value of Doppler velocity of the second component is almost zero (with a significant standard deviation) in QS/CH, which may indicate that the physical process to trigger the secondary Gaussian originates at the formation height of Si IV. While the mean Doppler velocity from secondary Gaussian in BPs is around +4.0 km/s (redshifted). The non-thermal velocities of the secondary Gaussian in all three regions are slightly higher than the single Gaussian. The statistical investigation leads to the prevalence of blueshifted secondary components in QS/CH. However, secondary Gaussian in the BPs redshifted, i.e., the BPs redshift behavior could be interpreted due to the site of reconnection located above the formation height of the Si IV line. The peak intensity of the second component for all three regions is likely to follow a power law that is a signature of the small-scale flaring-like trigger mechanism.","sentences":["The asymmetry of line profiles, i.e., the secondary component, is crucial to understanding the energy release of coronal holes (CH), quiet sun (QS), and bright points (BPs).","We investigate the asymmetry of Si IV 1393.75 {\\AA} of the transition-region (TR) line recorded by Interface Region Imaging Spectrometer (IRIS) and co-spatial-temporal Atmospheric Imaging Assembly (AIA) and Helioseismic and Magnetic Imager (HMI) data onboard Solar Dynamics Observatory (SDO) for three time series on 26 April 2015, 24 July 2014, 26 July 2014.","Most asymmetric profiles are in the complex magnetic field regions of the networks.","The asymmetric profiles are fitted with both single and double Gaussian models.","The mean value of Doppler velocity of the second component is almost zero (with a significant standard deviation) in QS/CH, which may indicate that the physical process to trigger the secondary Gaussian originates at the formation height of Si IV.","While the mean Doppler velocity from secondary Gaussian in BPs is around +4.0 km/s (redshifted).","The non-thermal velocities of the secondary Gaussian in all three regions are slightly higher than the single Gaussian.","The statistical investigation leads to the prevalence of blueshifted secondary components in QS/CH.","However, secondary Gaussian in the BPs redshifted, i.e., the BPs redshift behavior could be interpreted due to the site of reconnection located above the formation height of the Si IV line.","The peak intensity of the second component for all three regions is likely to follow a power law that is a signature of the small-scale flaring-like trigger mechanism."],"url":"http://arxiv.org/abs/2402.01628v1","category":"astro-ph.SR"}
{"created":"2024-02-02 18:42:48","title":"Incident Beamline Design for a Modern Cold Triple Axis Spectrometer at the High Flux Isotope Reactor","abstract":"A modern cold triple axis spectrometer is being planned for the High Flux Isotope Reactor (HFIR) at Oak Ridge National Laboratory. Here, we describe the design of an incident beamline that will put a flux of $\\sim 10^8\\mathrm{\\frac{n}{cm^2 s}}$ on a sample with an area of 2 cm x 2 cm. It takes current physical constraints at HFIR into account and it can accommodate both single and multiplexed analyzer-detector secondary spectrometers and large superconducting magnets. The proposed incident beamline includes a multi-channel guide with horizontal focusing, a neutron velocity selector, components to facilitate an incident beam polarization option, and a double-focusing pyrolytic graphite monochromator. This work describes the process of optimizing the guide system and monochromator and summarizes the expected performance of the incident beamline for non-polarized operation.","sentences":["A modern cold triple axis spectrometer is being planned for the High Flux Isotope Reactor (HFIR) at Oak Ridge National Laboratory.","Here, we describe the design of an incident beamline that will put a flux of $\\sim 10^8\\mathrm{\\frac{n}{cm^2 s}}$ on a sample with an area of 2 cm x 2 cm.","It takes current physical constraints at HFIR into account and it can accommodate both single and multiplexed analyzer-detector secondary spectrometers and large superconducting magnets.","The proposed incident beamline includes a multi-channel guide with horizontal focusing, a neutron velocity selector, components to facilitate an incident beam polarization option, and a double-focusing pyrolytic graphite monochromator.","This work describes the process of optimizing the guide system and monochromator and summarizes the expected performance of the incident beamline for non-polarized operation."],"url":"http://arxiv.org/abs/2402.01626v1","category":"physics.ins-det"}
{"created":"2024-02-02 18:41:04","title":"Solving coupled Non-linear Schr\u00f6dinger Equations via Quantum Imaginary Time Evolution","abstract":"Coupled non-linear Schr\\\"{o}dinger equations are crucial in describing dynamics of many particle systems. We present a quantum imaginary time evolution (ITE) algorithm as a solution to such equations in the case of nuclear Hartree-Fock equations. Under a simplified Skyrme interaction model, we calculate the ground state energy of an oxygen-16 nucleus and demonstrate that the result is in agreement with the classical ITE algorithm.","sentences":["Coupled non-linear Schr\\\"{o}dinger equations are crucial in describing dynamics of many particle systems.","We present a quantum imaginary time evolution (ITE) algorithm as a solution to such equations in the case of nuclear Hartree-Fock equations.","Under a simplified Skyrme interaction model, we calculate the ground state energy of an oxygen-16 nucleus and demonstrate that the result is in agreement with the classical ITE algorithm."],"url":"http://arxiv.org/abs/2402.01623v1","category":"nucl-th"}
{"created":"2024-02-02 18:32:24","title":"KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases","abstract":"Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI methods for low-resourced KBs, and even approaches the performance of supervised methods. Our code and data are available at https://github.com/THU-KEG/KB-Plugin.","sentences":["Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions.","Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data.","To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB.","Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin.","Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB.","Experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with 25$\\times$ smaller backbone LLM compared to SoTA PI methods for low-resourced KBs, and even approaches the performance of supervised methods.","Our code and data are available at https://github.com/THU-KEG/KB-Plugin."],"url":"http://arxiv.org/abs/2402.01619v1","category":"cs.CL"}
{"created":"2024-02-02 18:14:16","title":"Contingency Analysis of a Grid of Connected EVs for Primary Frequency Control of an Industrial Microgrid Using Efficient Control Scheme","abstract":"After over a century of internal combustion engines ruling the transport sector, electric vehicles appear to be on the verge of gaining traction due to a slew of advantages, including lower operating costs and lower CO2 emissions. By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are utilized as load) approach, EVs can operate as both a load and a source. Primary frequency regulation and congestion management are two essential characteristics of this technology that are added to an industrial microgrid. Industrial Microgrids are made up of different energy sources such as wind farms and PV farms, storage systems, and loads. EVs have gained a lot of interest as a technique for frequency management because of their ability to regulate quickly. Grid reliability depends on this quick reaction. Different contingency, state of charge of the electric vehicles, and a varying number of EVs in an EV fleet are considered in this work, and a proposed control scheme for frequency management is presented. This control scheme enables bidirectional power flow, allowing for primary frequency regulation during the various scenarios that an industrial microgrid may encounter over the course of a 24-h period. The presented controller will provide dependable frequency regulation support to the industrial microgrid during contingencies, as will be demonstrated by simulation results, achieving a more reliable system. However, simulation results will show that by increasing a number of the EVs in a fleet for the Vehicle-to-Grid approach, an industrial microgrid\\'s frequency can be enhanced even further.","sentences":["After over a century of internal combustion engines ruling the transport sector, electric vehicles appear to be on the verge of gaining traction due to a slew of advantages, including lower operating costs and lower CO2 emissions.","By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are utilized as load) approach, EVs can operate as both a load and a source.","Primary frequency regulation and congestion management are two essential characteristics of this technology that are added to an industrial microgrid.","Industrial Microgrids are made up of different energy sources such as wind farms and PV farms, storage systems, and loads.","EVs have gained a lot of interest as a technique for frequency management because of their ability to regulate quickly.","Grid reliability depends on this quick reaction.","Different contingency, state of charge of the electric vehicles, and a varying number of EVs in an EV fleet are considered in this work, and a proposed control scheme for frequency management is presented.","This control scheme enables bidirectional power flow, allowing for primary frequency regulation during the various scenarios that an industrial microgrid may encounter over the course of a 24-h period.","The presented controller will provide dependable frequency regulation support to the industrial microgrid during contingencies, as will be demonstrated by simulation results, achieving a more reliable system.","However, simulation results will show that by increasing a number of the EVs in a fleet for the Vehicle-to-Grid approach, an industrial microgrid\\'s frequency can be enhanced even further."],"url":"http://arxiv.org/abs/2402.01608v1","category":"cs.LG"}
{"created":"2024-02-02 18:07:40","title":"A Lyapunov theory demonstrating a fundamental limit on the speed of systems consolidation","abstract":"The nervous system reorganizes memories from an early site to a late site, a commonly observed feature of learning and memory systems known as systems consolidation. Previous work has suggested learning rules by which consolidation may occur. Here, we provide conditions under which such rules are guaranteed to lead to stable convergence of learning and consolidation. We use the theory of Lyapunov functions, which enforces stability by requiring learning rules to decrease an energy-like (Lyapunov) function. We present the theory in the context of a simple circuit architecture motivated by classic models of learning in systems consolidation mediated by the cerebellum. Stability is only guaranteed if the learning rate in the late stage is not faster than the learning rate in the early stage. Further, the slower the learning rate at the late stage, the larger the perturbation the system can tolerate with a guarantee of stability. We provide intuition for this result by mapping the consolidation model to a damped driven oscillator system, and showing that the ratio of early- to late-stage learning rates in the consolidation model can be directly identified with the (square of the) oscillator's damping ratio. This work suggests the power of the Lyapunov approach to provide constraints on nervous system function.","sentences":["The nervous system reorganizes memories from an early site to a late site, a commonly observed feature of learning and memory systems known as systems consolidation.","Previous work has suggested learning rules by which consolidation may occur.","Here, we provide conditions under which such rules are guaranteed to lead to stable convergence of learning and consolidation.","We use the theory of Lyapunov functions, which enforces stability by requiring learning rules to decrease an energy-like (Lyapunov) function.","We present the theory in the context of a simple circuit architecture motivated by classic models of learning in systems consolidation mediated by the cerebellum.","Stability is only guaranteed if the learning rate in the late stage is not faster than the learning rate in the early stage.","Further, the slower the learning rate at the late stage, the larger the perturbation the system can tolerate with a guarantee of stability.","We provide intuition for this result by mapping the consolidation model to a damped driven oscillator system, and showing that the ratio of early- to late-stage learning rates in the consolidation model can be directly identified with the (square of the) oscillator's damping ratio.","This work suggests the power of the Lyapunov approach to provide constraints on nervous system function."],"url":"http://arxiv.org/abs/2402.01605v1","category":"q-bio.NC"}
{"created":"2024-02-02 17:49:43","title":"Relativistic Dissipative Magnetohydrodynamics from the Boltzmann equation for a two-component gas","abstract":"We derive the equations of motion of relativistic magnetohydrodynamics, as well as microscopic expressions for all of its transport coefficients, from the Boltzmann equation using the method of moments. In contrast to reference Phys. Rev. D 98(7) 2018, where a single component gas was considered, we perform our derivation for a locally neutral fluid composed of two massless particle species with opposite charges. We demonstrate that the magnetohydrodynamical equations of motion become dramatically different for this more realistic system. The shear-stress tensor no longer obeys a single differential equation; it breaks into three non-degenerate components with respect to the magnetic field, each evolving according to different dynamical equations. For large magnetic fields, we further show that the solution of this theory displays oscillatory behaviour that can no longer be described by an Israel-Stewart-like theory. Finally, we investigate the derived equations in a Bjorken flow scenario.","sentences":["We derive the equations of motion of relativistic magnetohydrodynamics, as well as microscopic expressions for all of its transport coefficients, from the Boltzmann equation using the method of moments.","In contrast to reference Phys.","Rev. D 98(7) 2018, where a single component gas was considered, we perform our derivation for a locally neutral fluid composed of two massless particle species with opposite charges.","We demonstrate that the magnetohydrodynamical equations of motion become dramatically different for this more realistic system.","The shear-stress tensor no longer obeys a single differential equation; it breaks into three non-degenerate components with respect to the magnetic field, each evolving according to different dynamical equations.","For large magnetic fields, we further show that the solution of this theory displays oscillatory behaviour that can no longer be described by an Israel-Stewart-like theory.","Finally, we investigate the derived equations in a Bjorken flow scenario."],"url":"http://arxiv.org/abs/2402.01597v1","category":"nucl-th"}
{"created":"2024-02-02 17:35:49","title":"Towards Sustainable Workplace Mental Health: A Novel Approach to Early Intervention and Support","abstract":"Employee well-being is a critical concern in the contemporary workplace, as highlighted by the American Psychological Association's 2021 report, indicating that 71% of employees experience stress or tension. This stress contributes significantly to workplace attrition and absenteeism, with 61% of attrition and 16% of sick days attributed to poor mental health. A major challenge for employers is that employees often remain unaware of their mental health issues until they reach a crisis point, resulting in limited utilization of corporate well-being benefits. This research addresses this challenge by presenting a groundbreaking stress detection algorithm that provides real-time support preemptively. Leveraging automated chatbot technology, the algorithm objectively measures mental health levels by analyzing chat conversations, offering personalized treatment suggestions in real-time based on linguistic biomarkers. The study explores the feasibility of integrating these innovations into practical learning applications within real-world contexts and introduces a chatbot-style system integrated into the broader employee experience platform. This platform, encompassing various features, aims to enhance overall employee well-being, detect stress in real time, and proactively engage with individuals to improve support effectiveness, demonstrating a 22% increase when assistance is provided early. Overall, the study emphasizes the importance of fostering a supportive workplace environment for employees' mental health.","sentences":["Employee well-being is a critical concern in the contemporary workplace, as highlighted by the American Psychological Association's 2021 report, indicating that 71% of employees experience stress or tension.","This stress contributes significantly to workplace attrition and absenteeism, with 61% of attrition and 16% of sick days attributed to poor mental health.","A major challenge for employers is that employees often remain unaware of their mental health issues until they reach a crisis point, resulting in limited utilization of corporate well-being benefits.","This research addresses this challenge by presenting a groundbreaking stress detection algorithm that provides real-time support preemptively.","Leveraging automated chatbot technology, the algorithm objectively measures mental health levels by analyzing chat conversations, offering personalized treatment suggestions in real-time based on linguistic biomarkers.","The study explores the feasibility of integrating these innovations into practical learning applications within real-world contexts and introduces a chatbot-style system integrated into the broader employee experience platform.","This platform, encompassing various features, aims to enhance overall employee well-being, detect stress in real time, and proactively engage with individuals to improve support effectiveness, demonstrating a 22% increase when assistance is provided early.","Overall, the study emphasizes the importance of fostering a supportive workplace environment for employees' mental health."],"url":"http://arxiv.org/abs/2402.01592v1","category":"cs.CL"}
{"created":"2024-02-02 17:31:13","title":"Bisimulations and Logics for Higher-Dimensional Automata","abstract":"Higher-dimensional automata (HDAs) are models of non-in\\-ter\\-leav\\-ing concurrency for analyzing concurrent systems. There is a rich literature that deals with bisimulations for concurrent systems, and some of them have been extended to HDAs. However, no logical characterizations of these relations are currently available for HDAs. In this work, we address this gap by introducing Ipomset modal logic, a Hennessy-Milner type logic over HDAs, and show that it characterizes Path-bisimulation, a variant of ST-bisimulation existing in the literature. We also define a notion of Cell-bisimulation, using the open-maps framework of Joyal, Nielsen, and Winskel, and establish the relationship between these bisimulations (and also their \"strong\" variants, which take restrictions into account). In our work, we rely on the new categorical definition of HDAs as presheaves over concurrency lists and on track objects.","sentences":["Higher-dimensional automata (HDAs) are models of non-in\\-ter\\-leav\\-ing concurrency for analyzing concurrent systems.","There is a rich literature that deals with bisimulations for concurrent systems, and some of them have been extended to HDAs.","However, no logical characterizations of these relations are currently available for HDAs.","In this work, we address this gap by introducing Ipomset modal logic, a Hennessy-Milner type logic over HDAs, and show that it characterizes Path-bisimulation, a variant of ST-bisimulation existing in the literature.","We also define a notion of Cell-bisimulation, using the open-maps framework of Joyal, Nielsen, and Winskel, and establish the relationship between these bisimulations (and also their \"strong\" variants, which take restrictions into account).","In our work, we rely on the new categorical definition of HDAs as presheaves over concurrency lists and on track objects."],"url":"http://arxiv.org/abs/2402.01589v1","category":"cs.LO"}
{"created":"2024-02-02 17:28:46","title":"Critical Casimir effect in a disordered $O(2)$-symmetric model","abstract":"Critical Casimir effect appears when critical fluctuations of an order parameter interact with classical boundaries. We investigate this effect in the setting of a Landau-Ginzburg model with continuous symmetry in the presence of quenched disorder. The quenched free energy is written as an asymptotic series of moments of the models partition function. Our main result is that, in the presence of a strong disorder, Goldstone modes of the system contribute either with an attractive or with a repulsive force. This result was obtained using the distributional zeta-function method without relying on any particular ansatz in the functional space of the moments of the partition function.","sentences":["Critical Casimir effect appears when critical fluctuations of an order parameter interact with classical boundaries.","We investigate this effect in the setting of a Landau-Ginzburg model with continuous symmetry in the presence of quenched disorder.","The quenched free energy is written as an asymptotic series of moments of the models partition function.","Our main result is that, in the presence of a strong disorder, Goldstone modes of the system contribute either with an attractive or with a repulsive force.","This result was obtained using the distributional zeta-function method without relying on any particular ansatz in the functional space of the moments of the partition function."],"url":"http://arxiv.org/abs/2402.01588v1","category":"cond-mat.soft"}
{"created":"2024-02-02 17:24:25","title":"Market proliferation and the impact of locational complexity on network restructuring","abstract":"This manuscript investigates the problem of locational complexity, a type of complexity that emanates from a companys territorial strategy. Using an entropy-based measure for supply chain structural complexity ( pars-complexity), we develop a theoretical framework for analysing the effects of locational complexity on the profitability of service/manufacturing networks. The proposed model is used to shed light on the reasons why network restructuring strategies may result ineffective at reducing complexity-related costs. Our contribution is three-fold. First, we develop a novel mathematical formulation of a facility location problem that integrates the pars-complexity measure in the decision process. Second, using this model, we propose a decomposition of the penalties imposed by locational complexity into (a) an intrinsic cost of structural complexity; and (b) an avoidable cost of ignoring such complexity in the decision process. Such a decomposition is a valuable tool for identifying more effective measures for tackling locational complexity, moreover, it has allowed us to provide an explanation to the so-called addiction to growth within the locational context. Finally, we propose three alternative strategies that attempt to mimic different approaches used in practice by companies that have engaged in network restructuring processes. The impact of those approaches is evaluated through extensive numerical experiments. Our experimental results suggest that network restructuring efforts that are not accompanied by a substantial reduction on the target market of the company, fail at reducing complexity-related costs and, therefore, have a limited impact on the companys profitability.","sentences":["This manuscript investigates the problem of locational complexity, a type of complexity that emanates from a companys territorial strategy.","Using an entropy-based measure for supply chain structural complexity ( pars-complexity), we develop a theoretical framework for analysing the effects of locational complexity on the profitability of service/manufacturing networks.","The proposed model is used to shed light on the reasons why network restructuring strategies may result ineffective at reducing complexity-related costs.","Our contribution is three-fold.","First, we develop a novel mathematical formulation of a facility location problem that integrates the pars-complexity measure in the decision process.","Second, using this model, we propose a decomposition of the penalties imposed by locational complexity into (a) an intrinsic cost of structural complexity; and (b) an avoidable cost of ignoring such complexity in the decision process.","Such a decomposition is a valuable tool for identifying more effective measures for tackling locational complexity, moreover, it has allowed us to provide an explanation to the so-called addiction to growth within the locational context.","Finally, we propose three alternative strategies that attempt to mimic different approaches used in practice by companies that have engaged in network restructuring processes.","The impact of those approaches is evaluated through extensive numerical experiments.","Our experimental results suggest that network restructuring efforts that are not accompanied by a substantial reduction on the target market of the company, fail at reducing complexity-related costs and, therefore, have a limited impact on the companys profitability."],"url":"http://arxiv.org/abs/2402.01585v1","category":"math.OC"}
{"created":"2024-02-02 17:07:39","title":"Spiking Music: Audio Compression with Event Based Auto-encoders","abstract":"Neurons in the brain communicate information via punctual events called spikes. The timing of spikes is thought to carry rich information, but it is not clear how to leverage this in digital systems. We demonstrate that event-based encoding is efficient for audio compression. To build this event-based representation we use a deep binary auto-encoder, and under high sparsity pressure, the model enters a regime where the binary event matrix is stored more efficiently with sparse matrix storage algorithms. We test this on the large MAESTRO dataset of piano recordings against vector quantized auto-encoders. Not only does our \"Spiking Music compression\" algorithm achieve a competitive compression/reconstruction trade-off, but selectivity and synchrony between encoded events and piano key strikes emerge without supervision in the sparse regime.","sentences":["Neurons in the brain communicate information via punctual events called spikes.","The timing of spikes is thought to carry rich information, but it is not clear how to leverage this in digital systems.","We demonstrate that event-based encoding is efficient for audio compression.","To build this event-based representation we use a deep binary auto-encoder, and under high sparsity pressure, the model enters a regime where the binary event matrix is stored more efficiently with sparse matrix storage algorithms.","We test this on the large MAESTRO dataset of piano recordings against vector quantized auto-encoders.","Not only does our \"Spiking Music compression\" algorithm achieve a competitive compression/reconstruction trade-off, but selectivity and synchrony between encoded events and piano key strikes emerge without supervision in the sparse regime."],"url":"http://arxiv.org/abs/2402.01571v1","category":"cs.SD"}
{"created":"2024-02-02 16:57:10","title":"Magnetism of CuCr$_2$X$_4$ (X= S and Se) spinels studied with muon spin rotation and relaxation ($\u03bc$SR)","abstract":"We present muon spin rotation and relaxation ($\\mu$SR) results for chalcogenide spinels CuCr$_2$X$_4$ with X= S and Se. Both compounds are known as ferromagnetic metals with high Curie temperatures. Our $\\mu$SR and magnetization data show clear signatures for additional magnetic transitions far below the respective Curie temperatures. They can be related to changes in the Cr valence system from the mixed valence between Cr$^{3+}$ and Cr$^{4+}$ at high temperatures and collinear ferromagnetism to a charge-ordered state at low temperatures with a conical or even more complex structure. Our results demonstrate that the electronic systems and the related spin structures of both compounds are more complex than assumed so far.","sentences":["We present muon spin rotation and relaxation ($\\mu$SR) results for chalcogenide spinels CuCr$_2$X$_4$ with X= S and Se.","Both compounds are known as ferromagnetic metals with high Curie temperatures.","Our $\\mu$SR and magnetization data show clear signatures for additional magnetic transitions far below the respective Curie temperatures.","They can be related to changes in the Cr valence system from the mixed valence between Cr$^{3+}$ and Cr$^{4+}$ at high temperatures and collinear ferromagnetism to a charge-ordered state at low temperatures with a conical or even more complex structure.","Our results demonstrate that the electronic systems and the related spin structures of both compounds are more complex than assumed so far."],"url":"http://arxiv.org/abs/2402.01562v1","category":"cond-mat.str-el"}
{"created":"2024-02-02 16:50:18","title":"Deep Continuous Networks","abstract":"CNNs and computational models of biological vision share some fundamental principles, which opened new avenues of research. However, fruitful cross-field research is hampered by conventional CNN architectures being based on spatially and depthwise discrete representations, which cannot accommodate certain aspects of biological complexity such as continuously varying receptive field sizes and dynamics of neuronal responses. Here we propose deep continuous networks (DCNs), which combine spatially continuous filters, with the continuous depth framework of neural ODEs. This allows us to learn the spatial support of the filters during training, as well as model the continuous evolution of feature maps, linking DCNs closely to biological models. We show that DCNs are versatile and highly applicable to standard image classification and reconstruction problems, where they improve parameter and data efficiency, and allow for meta-parametrization. We illustrate the biological plausibility of the scale distributions learned by DCNs and explore their performance in a neuroscientifically inspired pattern completion task. Finally, we investigate an efficient implementation of DCNs by changing input contrast.","sentences":["CNNs and computational models of biological vision share some fundamental principles, which opened new avenues of research.","However, fruitful cross-field research is hampered by conventional CNN architectures being based on spatially and depthwise discrete representations, which cannot accommodate certain aspects of biological complexity such as continuously varying receptive field sizes and dynamics of neuronal responses.","Here we propose deep continuous networks (DCNs), which combine spatially continuous filters, with the continuous depth framework of neural ODEs.","This allows us to learn the spatial support of the filters during training, as well as model the continuous evolution of feature maps, linking DCNs closely to biological models.","We show that DCNs are versatile and highly applicable to standard image classification and reconstruction problems, where they improve parameter and data efficiency, and allow for meta-parametrization.","We illustrate the biological plausibility of the scale distributions learned by DCNs and explore their performance in a neuroscientifically inspired pattern completion task.","Finally, we investigate an efficient implementation of DCNs by changing input contrast."],"url":"http://arxiv.org/abs/2402.01557v1","category":"cs.CV"}
{"created":"2024-02-02 16:46:06","title":"Q-factor: A measure of competition between the topper and the average in percolation and in SOC","abstract":"We define the $Q$-factor in the percolation problem as the quotient of the size of the   largest cluster and the average size of all clusters. As the occupation probability $p$   is increased, the $Q$-factor for the system size $L$ grows systematically to its maximum   value $Q_{max}(L)$ at a specific value $p_{max}(L)$ and then gradually decays. Our numerical   study of site percolation problems on the square, triangular and the simple cubic lattices   exhibits that the asymptotic values of $p_{max}$ though close, are distinctly different   from the corresponding percolation thresholds of these lattices. We have also shown using   the scaling analysis that at $p_{max}$ the value of $Q_{max}(L)$ diverges as $L^d$ ($d$   denoting the dimension of the lattice) as the system size approaches to their asymptotic   limit. We have further extended this idea to the non-equilibrium systems such as the   sandpile model of self-organized criticality. Here, the $Q(\\rho,L)$-factor is the quotient   of the size of the largest avalanche and the cumulative average of the sizes of all the   avalanches; $\\rho$ being the drop density of the driving mechanism. This study has been   prompted by some observations in Sociophysics.","sentences":["We define the $Q$-factor in the percolation problem as the quotient of the size of the   largest cluster and the average size of all clusters.","As the occupation probability $p$   is increased, the $Q$-factor for the system size $L$ grows systematically to its maximum   value $Q_{max}(L)$ at a specific value $p_{max}(L)$ and then gradually decays.","Our numerical   study of site percolation problems on the square, triangular and the simple cubic lattices   exhibits that the asymptotic values of $p_{max}$ though close, are distinctly different   from the corresponding percolation thresholds of these lattices.","We have also shown using   the scaling analysis that at $p_{max}$ the value of $Q_{max}(L)$ diverges as $L^d$ ($d$   denoting the dimension of the lattice) as the system size approaches to their asymptotic   limit.","We have further extended this idea to the non-equilibrium systems such as the   sandpile model of self-organized criticality.","Here, the $Q(\\rho,L)$-factor is the quotient   of the size of the largest avalanche and the cumulative average of the sizes of all the   avalanches; $\\rho$ being the drop density of the driving mechanism.","This study has been   prompted by some observations in Sociophysics."],"url":"http://arxiv.org/abs/2402.01553v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-02 16:42:13","title":"Superfluid-supersolid phase transition of elongated dipolar Bose-Einstein Condensates at finite temperatures","abstract":"We analyse the finite-temperature phase diagram of a dipolar Bose Einstein Condensate confined in a tubular geometry. The effect of thermal fluctuations is accounted for by means of Bogoliubov theory employing the local density approximation. In the considered geometry, the superfluid-supersolid phase transition can be of first- and second-order. We discuss how the corresponding transition point is affected by the finite temperature of the system.","sentences":["We analyse the finite-temperature phase diagram of a dipolar Bose Einstein Condensate confined in a tubular geometry.","The effect of thermal fluctuations is accounted for by means of Bogoliubov theory employing the local density approximation.","In the considered geometry, the superfluid-supersolid phase transition can be of first- and second-order.","We discuss how the corresponding transition point is affected by the finite temperature of the system."],"url":"http://arxiv.org/abs/2402.01550v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-02 16:31:20","title":"Equivariant topological complexities","abstract":"Many mechanical systems have configuration spaces that admit symmetries. Mathematically, such symmetries are modelled by the action of a group on a topological space. Several variations of topological complexity have emerged that take symmetry into account in various ways, either by asking that the motion planners themselves admit compatible symmetries, or by exploiting the symmetry to motion plan between functionally equivalent configurations. We will survey the main definitions due to Colman-Grant, Lubawski-Marzantowicz, B\\l{}aszczyk-Kaluba and Dranishnikov, and some related notions. We conclude with a short list of open problems.","sentences":["Many mechanical systems have configuration spaces that admit symmetries.","Mathematically, such symmetries are modelled by the action of a group on a topological space.","Several variations of topological complexity have emerged that take symmetry into account in various ways, either by asking that the motion planners themselves admit compatible symmetries, or by exploiting the symmetry to motion plan between functionally equivalent configurations.","We will survey the main definitions due to Colman-Grant, Lubawski-Marzantowicz, B\\l{}aszczyk-Kaluba and Dranishnikov, and some related notions.","We conclude with a short list of open problems."],"url":"http://arxiv.org/abs/2402.01540v1","category":"math.AT"}
{"created":"2024-02-02 16:23:50","title":"Efficient and Effective Time-Series Forecasting with Spiking Neural Networks","abstract":"Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, provide a unique pathway for capturing the intricacies of temporal data. However, applying SNNs to time-series forecasting is challenging due to difficulties in effective temporal alignment, complexities in encoding processes, and the absence of standardized guidelines for model selection. In this paper, we propose a framework for SNNs in time-series forecasting tasks, leveraging the efficiency of spiking neurons in processing temporal information. Through a series of experiments, we demonstrate that our proposed SNN-based approaches achieve comparable or superior results to traditional time-series forecasting methods on diverse benchmarks with much less energy consumption. Furthermore, we conduct detailed analysis experiments to assess the SNN's capacity to capture temporal dependencies within time-series data, offering valuable insights into its nuanced strengths and effectiveness in modeling the intricate dynamics of temporal data. Our study contributes to the expanding field of SNNs and offers a promising alternative for time-series forecasting tasks, presenting a pathway for the development of more biologically inspired and temporally aware forecasting models.","sentences":["Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, provide a unique pathway for capturing the intricacies of temporal data.","However, applying SNNs to time-series forecasting is challenging due to difficulties in effective temporal alignment, complexities in encoding processes, and the absence of standardized guidelines for model selection.","In this paper, we propose a framework for SNNs in time-series forecasting tasks, leveraging the efficiency of spiking neurons in processing temporal information.","Through a series of experiments, we demonstrate that our proposed SNN-based approaches achieve comparable or superior results to traditional time-series forecasting methods on diverse benchmarks with much less energy consumption.","Furthermore, we conduct detailed analysis experiments to assess the SNN's capacity to capture temporal dependencies within time-series data, offering valuable insights into its nuanced strengths and effectiveness in modeling the intricate dynamics of temporal data.","Our study contributes to the expanding field of SNNs and offers a promising alternative for time-series forecasting tasks, presenting a pathway for the development of more biologically inspired and temporally aware forecasting models."],"url":"http://arxiv.org/abs/2402.01533v1","category":"cs.NE"}
{"created":"2024-02-02 16:22:32","title":"The closed-branch decoder for quantum LDPC codes","abstract":"Quantum error correction is the building block for constructing fault-tolerant quantum processors that can operate reliably even if its constituting elements are corrupted by decoherence. In this context, real-time decoding is a necessity for implementing arbitrary quantum computations on the logical level. In this work, we present a new decoder for Quantum Low Density Parity Check (QLDPC) codes, named the closed-branch decoder, with a worst-case complexity loosely upper bounded by $\\mathcal{O}(n\\text{max}_{\\text{gr}}\\text{max}_{\\text{br}})$, where $\\text{max}_{\\text{gr}}$ and $\\text{max}_{\\text{br}}$ are tunable parameters that pose the accuracy versus speed trade-off of decoding algorithms. For the best precision, the $\\text{max}_{\\text{gr}}\\text{max}_{\\text{br}}$ product is exponentially increasing, but we numerically prove that considering small values that are polynomials of the code distance are enough for good error correction performance. The decoder is described to great extent and compared with the Belief Propagation Ordered Statistics Decoder (BPOSD) operating over data qubit, phenomenological and circuit-level noise models for the class of Bivariate Bicycle (BB) codes. The results showcase a promising performance of the decoder, obtaining similar results with much lower complexity than BPOSD when considering the smallest distance codes, but experiencing some logical error probability degradation for the bigger ones. Ultimately, the performance and complexity of the decoder depends on the product $\\text{max}_{\\text{gr}}\\text{max}_{\\text{br}}$, which can be considered taking into account benefiting one of the two aspects at the expense of the other.","sentences":["Quantum error correction is the building block for constructing fault-tolerant quantum processors that can operate reliably even if its constituting elements are corrupted by decoherence.","In this context, real-time decoding is a necessity for implementing arbitrary quantum computations on the logical level.","In this work, we present a new decoder for Quantum Low Density Parity Check (QLDPC) codes, named the closed-branch decoder, with a worst-case complexity loosely upper bounded by $\\mathcal{O}(n\\text{max}_{\\text{gr}}\\text{max}_{\\text{br}})$, where $\\text{max}_{\\text{gr}}$ and $\\text{max}_{\\text{br}}$ are tunable parameters that pose the accuracy versus speed trade-off of decoding algorithms.","For the best precision, the $\\text{max}_{\\text{gr}}\\text{max}_{\\text{br}}$ product is exponentially increasing, but we numerically prove that considering small values that are polynomials of the code distance are enough for good error correction performance.","The decoder is described to great extent and compared with the Belief Propagation Ordered Statistics Decoder (BPOSD) operating over data qubit, phenomenological and circuit-level noise models for the class of Bivariate Bicycle (BB) codes.","The results showcase a promising performance of the decoder, obtaining similar results with much lower complexity than BPOSD when considering the smallest distance codes, but experiencing some logical error probability degradation for the bigger ones.","Ultimately, the performance and complexity of the decoder depends on the product $\\text{max}_{\\text{gr}}\\text{max}_{\\text{br}}$, which can be considered taking into account benefiting one of the two aspects at the expense of the other."],"url":"http://arxiv.org/abs/2402.01532v1","category":"quant-ph"}
{"created":"2024-02-02 16:13:09","title":"Central WENO schemes through a global average weight","abstract":"A novel central weighted essentially non-oscillatory (central WENO; CWENO)-type scheme for the construction of high-resolution approximations to discontinuous solutions to hyperbolic systems of conservation laws is presented. This procedure is based on the construction of a global average weight using the whole set of Jiang-Shu smoothness indicators associated to every candidate stencil. By this device one does not to have to rely on ideal weights, which, under certain stencil arrangements and interpolating point locations, do not define a convex combination of the lower-degree interpolating polynomials of the corresponding sub-stencils. Moreover, this procedure also prevents some cases of accuracy loss near smooth extrema that are experienced by classical WENO and CWENO schemes. These properties result in a more flexible scheme that overcomes these issues, at the cost of only a few additional computations with respect to classical WENO schemes and with a smaller cost than classical CWENO schemes. Numerical examples illustrate that the proposed CWENO schemes outperform both the traditional WENO and the original CWENO schemes.","sentences":["A novel central weighted essentially non-oscillatory (central WENO; CWENO)-type scheme for the construction of high-resolution approximations to discontinuous solutions to hyperbolic systems of conservation laws is presented.","This procedure is based on the construction of a global average weight using the whole set of Jiang-Shu smoothness indicators associated to every candidate stencil.","By this device one does not to have to rely on ideal weights, which, under certain stencil arrangements and interpolating point locations, do not define a convex combination of the lower-degree interpolating polynomials of the corresponding sub-stencils.","Moreover, this procedure also prevents some cases of accuracy loss near smooth extrema that are experienced by classical WENO and CWENO schemes.","These properties result in a more flexible scheme that overcomes these issues, at the cost of only a few additional computations with respect to classical WENO schemes and with a smaller cost than classical CWENO schemes.","Numerical examples illustrate that the proposed CWENO schemes outperform both the traditional WENO and the original CWENO schemes."],"url":"http://arxiv.org/abs/2402.01526v1","category":"math.NA"}
{"created":"2024-02-02 16:09:47","title":"Active Support of Inverters for Improving Short-Term Voltage Security in 100% IBRsPenetrated Power Systems","abstract":"Due to the energy crisis and environmental pollution, the installed capacity of inverter-based resources (IBRs) in power grids is rapidly increasing, and grid-following control (GFL) is the most prevalent at present. Meanwhile, grid-forming control-based (GFM) devices have been installed in the grid to provide active support for frequency and voltage. In the future GFL devices combined with GFM will be promising, especially in power systems with high penetration or 100% IBRs. When a short-circuit fault occurs in the grid, the controlled current source characteristic of the GFL devices leads to insufficient dynamic voltage support (DVS), while the GFM devices usually reduce the internal voltage to limit the current. Thus, deep voltage sags and undesired disconnections of IBRs may occur. Moreover, due to the dispersed locations and the control strategies' diversity of IBRs, the voltage support of different devices may not be fully coordinated, which is not conducive to short-term voltage security (STVS). To address this issue, a control scheme based on the simulation of transient characteristics of synchronous machines (SMs) is proposed. Then, a new fault ride-through strategy (FRT) is proposed based on the characteristic differences between GFL and GFM devices, and an optimization model of multi-device control parameters is formulated to meet the short-term voltage security constraints (SVSCs) and device capacity constraints. Finally, a fast solution method based on analytical modeling is proposed for the model. Test results based on the doublegenerator-one-load system, the IEEE 14-bus system, and other systems of different sizes show that the proposed method can effectively enhance the active support capability of GFL and GFM to the grid voltage, and avoid the large-scale disconnection of IBRs","sentences":["Due to the energy crisis and environmental pollution, the installed capacity of inverter-based resources (IBRs) in power grids is rapidly increasing, and grid-following control (GFL) is the most prevalent at present.","Meanwhile, grid-forming control-based (GFM) devices have been installed in the grid to provide active support for frequency and voltage.","In the future GFL devices combined with GFM will be promising, especially in power systems with high penetration or 100% IBRs.","When a short-circuit fault occurs in the grid, the controlled current source characteristic of the GFL devices leads to insufficient dynamic voltage support (DVS), while the GFM devices usually reduce the internal voltage to limit the current.","Thus, deep voltage sags and undesired disconnections of IBRs may occur.","Moreover, due to the dispersed locations and the control strategies' diversity of IBRs, the voltage support of different devices may not be fully coordinated, which is not conducive to short-term voltage security (STVS).","To address this issue, a control scheme based on the simulation of transient characteristics of synchronous machines (SMs) is proposed.","Then, a new fault ride-through strategy (FRT) is proposed based on the characteristic differences between GFL and GFM devices, and an optimization model of multi-device control parameters is formulated to meet the short-term voltage security constraints (SVSCs) and device capacity constraints.","Finally, a fast solution method based on analytical modeling is proposed for the model.","Test results based on the doublegenerator-one-load system, the IEEE 14-bus system, and other systems of different sizes show that the proposed method can effectively enhance the active support capability of GFL and GFM to the grid voltage, and avoid the large-scale disconnection of IBRs"],"url":"http://arxiv.org/abs/2402.01523v1","category":"eess.SY"}
{"created":"2024-02-02 16:08:48","title":"Double Red Giant Branch and Red Clump features of Galactic disc stellar populations with Gaia GSPspec","abstract":"To disentangle the different competing physical processes at play in Galactic evolution, a detailed chrono chemicalkinematical, and dynamical characterisation of the disc bimodality is necessary, including high number statistics. Here we make use of an extremely precise subsample of the Gaia DR3 GSP-Spec catalogue of stellar chemophysical parameters. The selected database is composed of 408 800 stars with a median uncertainty of 10 K, 0.03 and 0.01 dex in Teff , log(g) and [M/H], respectively. The stellar parameter precision allows to break the age-metallicity degeneracy of disc stars. For the first time, the disc bimodality in the Kiel diagramme of giant stars is observed, getting rid of interstellar absortion issues. This bimodality produces double Red Giant Branch sequences and Red Clump features for mono-metallicity populations. A comparison with BaSTI isochrones allows to demonstrate that an age gap is needed to explain the evolutionary sequences separation, in agreement with previous age-metallicity relations obtained using Main-Sequence Turn Off stars. A bimodal distribution in the stellar mass-[alpha/Fe] plane is observed at constant metallicity. Finally, a selection of stars with [M/H]=0.45 \\pm 0.03 dex shows that the most metal-rich population in the Milky Way disc presents an important proportion of stars with ages in the range 5-13 Gyr. This old, extremely metal-rich population is possibly a mix of migrated stars from the internal Galactic regions, and old disc stars formed before the last major merger of the Milky Way. The Gaia GSP-Spec Kiel diagrammes of disc mono-abundance stellar populations reveal a complex, non linear age-metallicity relation crafted by internal and external processes of Galactic evolution. Their detailed analysis opens new opportunities to reconstruct the puzzle of the Milky Way disc bimodality.","sentences":["To disentangle the different competing physical processes at play in Galactic evolution, a detailed chrono chemicalkinematical, and dynamical characterisation of the disc bimodality is necessary, including high number statistics.","Here we make use of an extremely precise subsample of the Gaia DR3 GSP-Spec catalogue of stellar chemophysical parameters.","The selected database is composed of 408 800 stars with a median uncertainty of 10 K, 0.03 and 0.01 dex in Teff , log(g) and [M/H], respectively.","The stellar parameter precision allows to break the age-metallicity degeneracy of disc stars.","For the first time, the disc bimodality in the Kiel diagramme of giant stars is observed, getting rid of interstellar absortion issues.","This bimodality produces double Red Giant Branch sequences and Red Clump features for mono-metallicity populations.","A comparison with BaSTI isochrones allows to demonstrate that an age gap is needed to explain the evolutionary sequences separation, in agreement with previous age-metallicity relations obtained using Main-Sequence Turn Off stars.","A bimodal distribution in the stellar mass-[alpha/Fe] plane is observed at constant metallicity.","Finally, a selection of stars with [M/H]=0.45 \\pm 0.03 dex shows that the most metal-rich population in the Milky Way disc presents an important proportion of stars with ages in the range 5-13 Gyr.","This old, extremely metal-rich population is possibly a mix of migrated stars from the internal Galactic regions, and old disc stars formed before the last major merger of the Milky Way.","The Gaia GSP-Spec Kiel diagrammes of disc mono-abundance stellar populations reveal a complex, non linear age-metallicity relation crafted by internal and external processes of Galactic evolution.","Their detailed analysis opens new opportunities to reconstruct the puzzle of the Milky Way disc bimodality."],"url":"http://arxiv.org/abs/2402.01522v1","category":"astro-ph.GA"}
{"created":"2024-02-02 16:04:33","title":"Quadrotor Takeoff Trajectory Planning in a One-Dimensional Uncertain Wind-field Aided by Wind-Sensing Infrastructure","abstract":"This paper investigates optimal takeoff trajectory planning for a quadrotor modeled with vertical-plane rigid body dynamics in an uncertain, one-dimensional wind-field. The wind-field varies horizontally and propagates across an operating region with a known fixed speed. The operating area of the quadrotor is equipped with wind-sensing infrastructure that shares noisy anemometer measurements with a centralized trajectory planner. The measurements are assimilated via Gaussian process regression to predict the wind at unsampled locations and future time instants. A minimum-time optimal control problem is formulated for the quadrotor to take off and reach a desired vertical-plane position in the presence of the predicted wind-field. The problem is solved using numerical optimal control. Several examples illustrate and compare the performance of the trajectory planner under varying wind conditions and sensing characteristics.","sentences":["This paper investigates optimal takeoff trajectory planning for a quadrotor modeled with vertical-plane rigid body dynamics in an uncertain, one-dimensional wind-field.","The wind-field varies horizontally and propagates across an operating region with a known fixed speed.","The operating area of the quadrotor is equipped with wind-sensing infrastructure that shares noisy anemometer measurements with a centralized trajectory planner.","The measurements are assimilated via Gaussian process regression to predict the wind at unsampled locations and future time instants.","A minimum-time optimal control problem is formulated for the quadrotor to take off and reach a desired vertical-plane position in the presence of the predicted wind-field.","The problem is solved using numerical optimal control.","Several examples illustrate and compare the performance of the trajectory planner under varying wind conditions and sensing characteristics."],"url":"http://arxiv.org/abs/2402.01518v1","category":"eess.SY"}
{"created":"2024-02-02 15:52:10","title":"Simulation-based optimization of a production system topology -- a neural network-assisted genetic algorithm","abstract":"There is an abundance of prior research on the optimization of production systems, but there is a research gap when it comes to optimizing which components should be included in a design, and how they should be connected. To overcome this gap, a novel approach is presented for topology optimization of production systems using a genetic algorithm (GA). This GA employs similarity-based mutation and recombination for the creation of offspring, and discrete-event simulation for fitness evaluation. To reduce computational cost, an extension to the GA is presented in which a neural network functions as a surrogate model for simulation. Three types of neural networks are compared, and the type most effective as a surrogate model is chosen based on its optimization performance and computational cost.   Both the unassisted GA and neural network-assisted GA are applied to an industrial case study and a scalability case study. These show that both approaches are effective at finding the optimal solution in industrial settings, and both scale well as the number of potential solutions increases, with the neural network-assisted GA having the better scalability of the two.","sentences":["There is an abundance of prior research on the optimization of production systems, but there is a research gap when it comes to optimizing which components should be included in a design, and how they should be connected.","To overcome this gap, a novel approach is presented for topology optimization of production systems using a genetic algorithm (GA).","This GA employs similarity-based mutation and recombination for the creation of offspring, and discrete-event simulation for fitness evaluation.","To reduce computational cost, an extension to the GA is presented in which a neural network functions as a surrogate model for simulation.","Three types of neural networks are compared, and the type most effective as a surrogate model is chosen based on its optimization performance and computational cost.   ","Both the unassisted GA and neural network-assisted GA are applied to an industrial case study and a scalability case study.","These show that both approaches are effective at finding the optimal solution in industrial settings, and both scale well as the number of potential solutions increases, with the neural network-assisted GA having the better scalability of the two."],"url":"http://arxiv.org/abs/2402.01511v1","category":"cs.NE"}
{"created":"2024-02-02 15:35:15","title":"Satisfiability Modulo Exponential Integer Arithmetic","abstract":"SMT solvers use sophisticated techniques for polynomial (linear or non-linear) integer arithmetic. In contrast, non-polynomial integer arithmetic has mostly been neglected so far. However, in the context of program verification, polynomials are often insufficient to capture the behavior of the analyzed system without resorting to approximations. In the last years, incremental linearization has been applied successfully to satisfiability modulo real arithmetic with transcendental functions. We adapt this approach to an extension of polynomial integer arithmetic with exponential functions. Here, the key challenge is to compute suitable lemmas that eliminate the current model from the search space if it violates the semantics of exponentiation. An empirical evaluation of our implementation shows that our approach is highly effective in practice.","sentences":["SMT solvers use sophisticated techniques for polynomial (linear or non-linear) integer arithmetic.","In contrast, non-polynomial integer arithmetic has mostly been neglected so far.","However, in the context of program verification, polynomials are often insufficient to capture the behavior of the analyzed system without resorting to approximations.","In the last years, incremental linearization has been applied successfully to satisfiability modulo real arithmetic with transcendental functions.","We adapt this approach to an extension of polynomial integer arithmetic with exponential functions.","Here, the key challenge is to compute suitable lemmas that eliminate the current model from the search space if it violates the semantics of exponentiation.","An empirical evaluation of our implementation shows that our approach is highly effective in practice."],"url":"http://arxiv.org/abs/2402.01501v1","category":"cs.LO"}
{"created":"2024-02-02 15:31:25","title":"Operads of decorated cliques II: Noncrossing cliques","abstract":"A complete study of an operad $\\mathrm{NC} \\mathcal{M}$ of noncrossing configurations of chords introduced in previous work of the author is performed. This operad is defined on the linear span of all noncrossing $\\mathcal{M}$-cliques. These are noncrossing configurations of chords with arcs labeled by a unitary magma $\\mathcal{M}$. The magmatic product of $\\mathcal{M}$ intervenes for the computation of the operadic composition of $\\mathcal{M}$-cliques. We show that this operad is binary, quadratic, and Koszul by considering techniques coming from rewrite systems on trees. We also compute a presentation for its Koszul dual. Finally, we explain how $\\mathrm{NC} \\mathcal{M}$ allows one to obtain alternative constructions of already known operads like operads of formal fractions and the operad of bicolored noncrossing configurations.","sentences":["A complete study of an operad $\\mathrm{NC} \\mathcal{M}$ of noncrossing configurations of chords introduced in previous work of the author is performed.","This operad is defined on the linear span of all noncrossing $\\mathcal{M}$-cliques.","These are noncrossing configurations of chords with arcs labeled by a unitary magma $\\mathcal{M}$. The magmatic product of $\\mathcal{M}$ intervenes for the computation of the operadic composition of $\\mathcal{M}$-cliques.","We show that this operad is binary, quadratic, and Koszul by considering techniques coming from rewrite systems on trees.","We also compute a presentation for its Koszul dual.","Finally, we explain how $\\mathrm{NC} \\mathcal{M}$ allows one to obtain alternative constructions of already known operads like operads of formal fractions and the operad of bicolored noncrossing configurations."],"url":"http://arxiv.org/abs/2402.01500v1","category":"math.CO"}
{"created":"2024-02-02 15:24:52","title":"Evaluating UAV Path Planning Algorithms for Realistic Maritime Search and Rescue Missions","abstract":"Unmanned Aerial Vehicles (UAVs) are emerging as very important tools in search and rescue (SAR) missions at sea, enabling swift and efficient deployment for locating individuals or vessels in distress. The successful execution of these critical missions heavily relies on effective path planning algorithms that navigate UAVs through complex maritime environments while considering dynamic factors such as water currents and wind flow. Furthermore, they need to account for the uncertainty in search target locations. However, existing path planning methods often fail to address the inherent uncertainty associated with the precise location of search targets and the uncertainty of oceanic forces. In this paper, we develop a framework to develop and investigate trajectory planning algorithms for maritime SAR scenarios employing UAVs. We adopt it to compare multiple planning strategies, some of them used in practical applications by the United States Coast Guard. Furthermore, we propose a novel planner that aims at bridging the gap between computation heavy, precise algorithms and lightweight strategies applicable to real-world scenarios.","sentences":["Unmanned Aerial Vehicles (UAVs) are emerging as very important tools in search and rescue (SAR) missions at sea, enabling swift and efficient deployment for locating individuals or vessels in distress.","The successful execution of these critical missions heavily relies on effective path planning algorithms that navigate UAVs through complex maritime environments while considering dynamic factors such as water currents and wind flow.","Furthermore, they need to account for the uncertainty in search target locations.","However, existing path planning methods often fail to address the inherent uncertainty associated with the precise location of search targets and the uncertainty of oceanic forces.","In this paper, we develop a framework to develop and investigate trajectory planning algorithms for maritime SAR scenarios employing UAVs.","We adopt it to compare multiple planning strategies, some of them used in practical applications by the United States Coast Guard.","Furthermore, we propose a novel planner that aims at bridging the gap between computation heavy, precise algorithms and lightweight strategies applicable to real-world scenarios."],"url":"http://arxiv.org/abs/2402.01494v1","category":"cs.RO"}
{"created":"2024-02-02 15:14:41","title":"Eclipse timing study of new hierarchical triple star candidates in the Northern Continuous Viewing Zone of TESS","abstract":"We compiled a list of more than 3500 eclipsing binaries located in and near the Northern Continuous Viewing Zone (NCVZ) of the TESS space telescope that have a sufficient amount of TESS photometry to search for additional hidden components in these systems. We obtained the TESS light curves of all targets in an automated way applying convolution-aided differential photometry on the TESS Full-Frame Images from all available sectors up to Sector 60. Using a new self-developed Python GUI, we visually vetted all of these light curves, determined the eclipsing periods of the objects and calculated their eclipse timing variations (ETVs). The ETV curves were used in order to search for nonlinear variations that could be attributed to a light travel time effect (LTTE) or dynamical perturbations caused by additional components in these systems. We pre-selected 351 such candidates and tried to model their ETVs with the analytic formulae of pure LTTE or the combination of LTTE and dynamical perturbations. In total we could fit a model solution for the ETVs of 135 hierarchical triple candidates in which 10 systems were already known in the literature and the remainder of the 125 systems are new discoveries. Among these systems, there are some more noteworthy ones, such as five tight triples very close to their dynamical stability limit with a period ratio of less than 20 and three newly discovered triply eclipsing triples. We point out that dynamical perturbations are occurring in GZ Dra, which turns out to be a triple. We also made a comparison of the distributions of some orbital parameters coming from our solutions with those from the Kepler sample derived by Borkovits et al. (2016). Finally, we checked the correlations between the available parameters for systems that have Gaia Non-Single Star orbital solutions with those from our ETV solutions. (Abridged)","sentences":["We compiled a list of more than 3500 eclipsing binaries located in and near the Northern Continuous Viewing Zone (NCVZ) of the TESS space telescope that have a sufficient amount of TESS photometry to search for additional hidden components in these systems.","We obtained the TESS light curves of all targets in an automated way applying convolution-aided differential photometry on the TESS Full-Frame Images from all available sectors up to Sector 60.","Using a new self-developed Python GUI, we visually vetted all of these light curves, determined the eclipsing periods of the objects and calculated their eclipse timing variations (ETVs).","The ETV curves were used in order to search for nonlinear variations that could be attributed to a light travel time effect (LTTE) or dynamical perturbations caused by additional components in these systems.","We pre-selected 351 such candidates and tried to model their ETVs with the analytic formulae of pure LTTE or the combination of LTTE and dynamical perturbations.","In total we could fit a model solution for the ETVs of 135 hierarchical triple candidates in which 10 systems were already known in the literature and the remainder of the 125 systems are new discoveries.","Among these systems, there are some more noteworthy ones, such as five tight triples very close to their dynamical stability limit with a period ratio of less than 20 and three newly discovered triply eclipsing triples.","We point out that dynamical perturbations are occurring in GZ Dra, which turns out to be a triple.","We also made a comparison of the distributions of some orbital parameters coming from our solutions with those from the Kepler sample derived by Borkovits et al. (2016).","Finally, we checked the correlations between the available parameters for systems that have Gaia Non-Single Star orbital solutions with those from our ETV solutions.","(Abridged)"],"url":"http://arxiv.org/abs/2402.01486v1","category":"astro-ph.SR"}
{"created":"2024-02-02 14:59:47","title":"The imprint of magnetic fields on absorption spectra from circumgalactic wind-cloud systems","abstract":"Galactic winds probe how stellar feedback regulates the mass and metallicity of galaxies. Observations show that galactic winds are multiphase and magnetised. In the local Universe, the dense phase is traced by emission and absorption lines, which reveal the presence of fast-moving clouds embedded in hot streams. Simulations tell us that magnetic fields can shield such clouds and delay their disruption, but there is little discussed on their observational effects. Using 3D MHD simulations, we study the influence of two orientations of the magnetic field (aligned and transverse) on the cloud morphology, temperature and density structure, mixing fraction, ion kinematics, column densities, and absorption spectra. We study supersonic wind-cloud systems with radiative processes, and develop a framework to extract ion column density maps and synthetic absorption spectra. The framework relies on studying ion populations and creating down-the-barrel spectra via an interface that links our PLUTO simulations to TRIDENT using YT, CLOUDY, and STARBURST99. We find that the transverse magnetic field makes the cloud asymmetric, shields and protects dense cold gas, and reduces mixing fractions compared to the aligned case. Ions can reach higher velocities in the transverse field case. The imprints of the initial orientation of the field on the synthetic spectra are: in the cold phase we find no signature of C ii and Si ii when the field is aligned, in the intermediate phase traced by C iv and Si iv we find broader lines in the transverse case, and in the warm phase we find deeper lines for O vi and N v in the aligned case, but they are less sensitive to the field orientation. Magnetic fields significantly affect the absorption spectra of cold clouds. Intermediate ions are the most sensitive to the magnetic field orientation and can potentially yield information about magnetic field topology.","sentences":["Galactic winds probe how stellar feedback regulates the mass and metallicity of galaxies.","Observations show that galactic winds are multiphase and magnetised.","In the local Universe, the dense phase is traced by emission and absorption lines, which reveal the presence of fast-moving clouds embedded in hot streams.","Simulations tell us that magnetic fields can shield such clouds and delay their disruption, but there is little discussed on their observational effects.","Using 3D MHD simulations, we study the influence of two orientations of the magnetic field (aligned and transverse) on the cloud morphology, temperature and density structure, mixing fraction, ion kinematics, column densities, and absorption spectra.","We study supersonic wind-cloud systems with radiative processes, and develop a framework to extract ion column density maps and synthetic absorption spectra.","The framework relies on studying ion populations and creating down-the-barrel spectra via an interface that links our PLUTO simulations to TRIDENT using YT, CLOUDY, and STARBURST99.","We find that the transverse magnetic field makes the cloud asymmetric, shields and protects dense cold gas, and reduces mixing fractions compared to the aligned case.","Ions can reach higher velocities in the transverse field case.","The imprints of the initial orientation of the field on the synthetic spectra are: in the cold phase we find no signature of C ii and Si ii when the field is aligned, in the intermediate phase traced by C iv and","Si iv we find broader lines in the transverse case, and in the warm phase we find deeper lines for O vi and N v in the aligned case, but they are less sensitive to the field orientation.","Magnetic fields significantly affect the absorption spectra of cold clouds.","Intermediate ions are the most sensitive to the magnetic field orientation and can potentially yield information about magnetic field topology."],"url":"http://arxiv.org/abs/2402.01475v1","category":"astro-ph.GA"}
{"created":"2024-02-02 14:58:26","title":"On approximate implicit Taylor methods for ordinary differential equations","abstract":"An efficient approximate version of implicit Taylor methods for initial-value problems of systems of ordinary differential equations (ODEs) is introduced. The approach, based on an approximate formulation of Taylor methods, produces a method that requires less evaluations of the function that defines the ODE and its derivatives than the usual version. On the other hand, an efficient numerical solution of the equation that arises from the discretization by means of Newton's method is introduced for an implicit scheme of any order. Numerical experiments illustrate that the resulting algorithm is simpler to implement and has better performance than its exact counterpart.","sentences":["An efficient approximate version of implicit Taylor methods for initial-value problems of systems of ordinary differential equations (ODEs) is introduced.","The approach, based on an approximate formulation of Taylor methods, produces a method that requires less evaluations of the function that defines the ODE and its derivatives than the usual version.","On the other hand, an efficient numerical solution of the equation that arises from the discretization by means of Newton's method is introduced for an implicit scheme of any order.","Numerical experiments illustrate that the resulting algorithm is simpler to implement and has better performance than its exact counterpart."],"url":"http://arxiv.org/abs/2402.01473v1","category":"math.NA"}
{"created":"2024-02-02 14:55:36","title":"Scaled 360 layouts: Revisiting non-central panoramas","abstract":"From a non-central panorama, 3D lines can be recovered by geometric reasoning. However, their sensitivity to noise and the complex geometric modeling required has led these panoramas being very little investigated. In this work we present a novel approach for 3D layout recovery of indoor environments using single non-central panoramas. We obtain the boundaries of the structural lines of the room from a non-central panorama using deep learning and exploit the properties of non-central projection systems in a new geometrical processing to recover the scaled layout. We solve the problem for Manhattan environments, handling occlusions, and also for Atlanta environments in an unified method. The experiments performed improve the state-of-the-art methods for 3D layout recovery from a single panorama. Our approach is the first work using deep learning with non-central panoramas and recovering the scale of single panorama layouts.","sentences":["From a non-central panorama, 3D lines can be recovered by geometric reasoning.","However, their sensitivity to noise and the complex geometric modeling required has led these panoramas being very little investigated.","In this work we present a novel approach for 3D layout recovery of indoor environments using single non-central panoramas.","We obtain the boundaries of the structural lines of the room from a non-central panorama using deep learning and exploit the properties of non-central projection systems in a new geometrical processing to recover the scaled layout.","We solve the problem for Manhattan environments, handling occlusions, and also for Atlanta environments in an unified method.","The experiments performed improve the state-of-the-art methods for 3D layout recovery from a single panorama.","Our approach is the first work using deep learning with non-central panoramas and recovering the scale of single panorama layouts."],"url":"http://arxiv.org/abs/2402.01466v1","category":"cs.CV"}
{"created":"2024-02-02 14:54:03","title":"Emergence of the molecular geometric phase from exact electron-nuclear dynamics","abstract":"Geometric phases play a crucial role in diverse fields. In chemistry they appear when a reaction path encircles an intersection between adiabatic potential energy surfaces and the molecular wavefunction experiences quantum-mechanical interference effects. This intriguing effect, closely resembling the magnetic Aharonov-Bohm effect, crucially relies on the adiabatic description of the dynamics, and it is uncertain whether and how it persists in an exact quantum dynamical framework. Recent works have shown that the geometric phase is an artifact of the adiabatic approximation, thereby challenging the perceived utility of the geometric phase concept in molecules. Here, we investigate this issue in an exact dynamical framework. We introduce instantaneous, gauge invariant phases separately for the electrons and for the nuclei, and use them to monitor the phase difference between the trailing edges of a wavepacket encircling a conical intersection. In this way we unambiguosly assess the role of the geometric phase in the interference process and shed light on its persistence in molecular systems.","sentences":["Geometric phases play a crucial role in diverse fields.","In chemistry they appear when a reaction path encircles an intersection between adiabatic potential energy surfaces and the molecular wavefunction experiences quantum-mechanical interference effects.","This intriguing effect, closely resembling the magnetic Aharonov-Bohm effect, crucially relies on the adiabatic description of the dynamics, and it is uncertain whether and how it persists in an exact quantum dynamical framework.","Recent works have shown that the geometric phase is an artifact of the adiabatic approximation, thereby challenging the perceived utility of the geometric phase concept in molecules.","Here, we investigate this issue in an exact dynamical framework.","We introduce instantaneous, gauge invariant phases separately for the electrons and for the nuclei, and use them to monitor the phase difference between the trailing edges of a wavepacket encircling a conical intersection.","In this way we unambiguosly assess the role of the geometric phase in the interference process and shed light on its persistence in molecular systems."],"url":"http://arxiv.org/abs/2402.01463v1","category":"quant-ph"}
{"created":"2024-02-02 14:50:06","title":"Beyond single-reference fixed-node approximation in ab initio Diffusion Monte Carlo","abstract":"Diffusion Monte Carlo (DMC) is an exact technique to project out the ground state (GS) of an Hamiltonian. Since the GS is always bosonic, in fermionic systems the projection needs to be carried out while imposing anti-symmetric constraints, which is a nondeterministic polynomial hard problem. In practice, therefore, application of DMC on electronic structure problems is made by employing the fixed-node (FN) approximation, consisting in performing DMC with the constraint of having a fixed predefined nodal surface. Although this constraint makes FN-DMC non-exact, it turns out that it is still very reliable, with an accuracy in line with the most accurate quantum chemistry approaches. How do we get the nodal surface? The typical approach, applied in systems having up to hundreds, or even thousands of electrons, is to obtain the nodal surface from a preliminary mean-field approach (typically, a density functional theory calculation) used to obtain a single Slater determinant. This is known as single reference. In this paper, we propose a new approach, applicable to systems as large as the C$_{60}$ fullerene, which improves the nodes by going beyond the single reference. In practice, we employ an implicitly multireference ansatz, initialized on the preliminary mean-field approach, which is relaxed by optimising a few parameters of the wave function determining the nodal surface by minimizing the FN-DMC energy. We highlight on several examples the improvements of the proposed approach over the standard single reference method and, where feasible, the computational gain over the standard multireference ansatz, which makes the methods applicable to large systems. We also show that physical properties relying on relative energies, such as binding energies, are affordable and reliable within the proposed scheme.","sentences":["Diffusion Monte Carlo (DMC) is an exact technique to project out the ground state (GS) of an Hamiltonian.","Since the GS is always bosonic, in fermionic systems the projection needs to be carried out while imposing anti-symmetric constraints, which is a nondeterministic polynomial hard problem.","In practice, therefore, application of DMC on electronic structure problems is made by employing the fixed-node (FN) approximation, consisting in performing DMC with the constraint of having a fixed predefined nodal surface.","Although this constraint makes FN-DMC non-exact, it turns out that it is still very reliable, with an accuracy in line with the most accurate quantum chemistry approaches.","How do we get the nodal surface?","The typical approach, applied in systems having up to hundreds, or even thousands of electrons, is to obtain the nodal surface from a preliminary mean-field approach (typically, a density functional theory calculation) used to obtain a single Slater determinant.","This is known as single reference.","In this paper, we propose a new approach, applicable to systems as large as the C$_{60}$ fullerene, which improves the nodes by going beyond the single reference.","In practice, we employ an implicitly multireference ansatz, initialized on the preliminary mean-field approach, which is relaxed by optimising a few parameters of the wave function determining the nodal surface by minimizing the FN-DMC energy.","We highlight on several examples the improvements of the proposed approach over the standard single reference method and, where feasible, the computational gain over the standard multireference ansatz, which makes the methods applicable to large systems.","We also show that physical properties relying on relative energies, such as binding energies, are affordable and reliable within the proposed scheme."],"url":"http://arxiv.org/abs/2402.01458v1","category":"physics.comp-ph"}
{"created":"2024-02-02 14:35:26","title":"Frenetix Motion Planner: High-Performance and Modular Trajectory Planning Algorithm for Complex Autonomous Driving Scenarios","abstract":"Our work aims to present a high-performance and modular sampling-based trajectory planning algorithm for autonomous vehicles. This algorithm is tailored to address the complex challenges in solution space construction and optimization problem formulation within the path planning domain. Our method employs a multi-objective optimization strategy for efficient navigation in static and highly dynamic environments, focusing on optimizing trajectory comfort, safety, and path precision. This algorithm was then used to analyze the algorithm performance and success rate in 1750 virtual complex urban and highway scenarios. Our results demonstrate fast calculation times (8ms for 800 trajectories), a high success rate in complex scenarios (88%), and easy adaptability with different modules presented. The most noticeable difference exhibited was the fast trajectory sampling, feasibility check, and cost evaluation step across various trajectory counts. While our study presents promising results, it's important to note that our assessments have been conducted exclusively in simulated environments, and real-world testing is required to fully validate our findings. The code and the additional modules used in this research are publicly available as open-source software and can be accessed at the following link: https://github.com/TUM-AVS/Frenetix-Motion-Planner.","sentences":["Our work aims to present a high-performance and modular sampling-based trajectory planning algorithm for autonomous vehicles.","This algorithm is tailored to address the complex challenges in solution space construction and optimization problem formulation within the path planning domain.","Our method employs a multi-objective optimization strategy for efficient navigation in static and highly dynamic environments, focusing on optimizing trajectory comfort, safety, and path precision.","This algorithm was then used to analyze the algorithm performance and success rate in 1750 virtual complex urban and highway scenarios.","Our results demonstrate fast calculation times (8ms for 800 trajectories), a high success rate in complex scenarios (88%), and easy adaptability with different modules presented.","The most noticeable difference exhibited was the fast trajectory sampling, feasibility check, and cost evaluation step across various trajectory counts.","While our study presents promising results, it's important to note that our assessments have been conducted exclusively in simulated environments, and real-world testing is required to fully validate our findings.","The code and the additional modules used in this research are publicly available as open-source software and can be accessed at the following link: https://github.com/TUM-AVS/Frenetix-Motion-Planner."],"url":"http://arxiv.org/abs/2402.01443v1","category":"cs.RO"}
{"created":"2024-02-02 14:29:13","title":"Structural and optical characterization of NiO polycrystalline thin films fabricated by spray-pyrolysis","abstract":"Nickel (II) oxide, NiO, a wide band gap Mott insulator characterized by strong Coulomb repulsion between d-electrons and displaying antiferromagnetic order at room temperature, has gained attention in recent years as a very promising candidate for applications in a broad set of areas, including chemistry and metallurgy to spintronics and energy harvesting. Here, we report on the synthesis of polycrystalline NiO fabricated using spray-pyrolysis technique, which is a deposition technique able to produce quite uniform films of pure and crystalline materials without the need of high vacuum or inert atmospheres. We then characterized the composition and structure of our NiO thin films using X-ray diffraction, and atomic force and scanning electron microscopies, respectively. We completed our study by looking at the phononic and magnonic properties of our NiO thin films via Raman spectroscopy, and at the ultrafast electron dynamics by using optical pump probe spectroscopy. We found that our NiO samples display the same phononic and magnonic dispersion expected for single crystal NiO at room temperature, and that electron dynamics in our system is similar to those of previously reported NiO mono- and poli-crystalline systems synthesized with different techniques. These results prove that spray-pyrolysis can be used as affordable and large-scale fabrication technique to synthetize strongly correlated materials for a large set of applications.","sentences":["Nickel (II) oxide, NiO, a wide band gap Mott insulator characterized by strong Coulomb repulsion between d-electrons and displaying antiferromagnetic order at room temperature, has gained attention in recent years as a very promising candidate for applications in a broad set of areas, including chemistry and metallurgy to spintronics and energy harvesting.","Here, we report on the synthesis of polycrystalline NiO fabricated using spray-pyrolysis technique, which is a deposition technique able to produce quite uniform films of pure and crystalline materials without the need of high vacuum or inert atmospheres.","We then characterized the composition and structure of our NiO thin films using X-ray diffraction, and atomic force and scanning electron microscopies, respectively.","We completed our study by looking at the phononic and magnonic properties of our NiO thin films via Raman spectroscopy, and at the ultrafast electron dynamics by using optical pump probe spectroscopy.","We found that our NiO samples display the same phononic and magnonic dispersion expected for single crystal NiO at room temperature, and that electron dynamics in our system is similar to those of previously reported NiO mono- and poli-crystalline systems synthesized with different techniques.","These results prove that spray-pyrolysis can be used as affordable and large-scale fabrication technique to synthetize strongly correlated materials for a large set of applications."],"url":"http://arxiv.org/abs/2402.01437v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 14:20:04","title":"Approximate Control for Continuous-Time POMDPs","abstract":"This work proposes a decision-making framework for partially observable systems in continuous time with discrete state and action spaces. As optimal decision-making becomes intractable for large state spaces we employ approximation methods for the filtering and the control problem that scale well with an increasing number of states. Specifically, we approximate the high-dimensional filtering distribution by projecting it onto a parametric family of distributions, and integrate it into a control heuristic based on the fully observable system to obtain a scalable policy. We demonstrate the effectiveness of our approach on several partially observed systems, including queueing systems and chemical reaction networks.","sentences":["This work proposes a decision-making framework for partially observable systems in continuous time with discrete state and action spaces.","As optimal decision-making becomes intractable for large state spaces we employ approximation methods for the filtering and the control problem that scale well with an increasing number of states.","Specifically, we approximate the high-dimensional filtering distribution by projecting it onto a parametric family of distributions, and integrate it into a control heuristic based on the fully observable system to obtain a scalable policy.","We demonstrate the effectiveness of our approach on several partially observed systems, including queueing systems and chemical reaction networks."],"url":"http://arxiv.org/abs/2402.01431v1","category":"cs.LG"}
{"created":"2024-02-02 14:18:16","title":"Atomic Quantum Sensor for Ultralow-Frequency Electric Field Measurements","abstract":"Rydberg-assisted atomic electrometry with thermal vapors offers a promising approach for detecting external electric fields. This technique, however, has proven quite challenging for measuring low frequencies because of the effects of metal-alkali atoms absorbing on the interior surface of the vacuum enclosure. We apply high-contrast Rydberg-electromagnetically-induced-transparency (EIT) spectroscopy to investigate the impact systematically, including laser power and electric field. Our system enhances frequency measurements below 10 Hz, demonstrating significant progress in improving the performance of Rydberg atomic electrometry. Consequently, our study reveals a standard quantum limit for data capacity and experimentally illustrates it across bandwidths ranging from 10 Hz to 1 MHz. This study provides valuable insights for future precision measurements and field sensing applications.","sentences":["Rydberg-assisted atomic electrometry with thermal vapors offers a promising approach for detecting external electric fields.","This technique, however, has proven quite challenging for measuring low frequencies because of the effects of metal-alkali atoms absorbing on the interior surface of the vacuum enclosure.","We apply high-contrast Rydberg-electromagnetically-induced-transparency (EIT) spectroscopy to investigate the impact systematically, including laser power and electric field.","Our system enhances frequency measurements below 10 Hz, demonstrating significant progress in improving the performance of Rydberg atomic electrometry.","Consequently, our study reveals a standard quantum limit for data capacity and experimentally illustrates it across bandwidths ranging from 10 Hz to 1 MHz.","This study provides valuable insights for future precision measurements and field sensing applications."],"url":"http://arxiv.org/abs/2402.01430v1","category":"physics.atom-ph"}
{"created":"2024-02-02 14:14:15","title":"Pilot Length Optimization with RS-LS Channel Estimation for Extremely Large Aperture Arrays","abstract":"Extremely large aperture arrays can enable unprecedented spatial multiplexing in beyond 5G systems due to their extremely narrow beamfocusing capabilities. However, acquiring the spatial correlation matrix to enable efficient channel estimation is a complex task due to the vast number of antenna dimensions. Recently, a new estimation method called the \"reduced-subspace least squares (RS-LS) estimator\" has been proposed for densely packed arrays. This method relies solely on the geometry of the array to limit the estimation resources. In this paper, we address a gap in the existing literature by deriving the average spectral efficiency for a certain distribution of user equipments (UEs) and a lower bound on it when using the RS-LS estimator. This bound is determined by the channel gain and the statistics of the normalized spatial correlation matrices of potential UEs but, importantly, does not require knowledge of a specific UE's spatial correlation matrix. We establish that there exists a pilot length that maximizes this expression. Additionally, we derive an approximate expression for the optimal pilot length under low signal-to-noise ratio (SNR) conditions. Simulation results validate the tightness of the derived lower bound and the effectiveness of using the optimized pilot length.","sentences":["Extremely large aperture arrays can enable unprecedented spatial multiplexing in beyond 5G systems due to their extremely narrow beamfocusing capabilities.","However, acquiring the spatial correlation matrix to enable efficient channel estimation is a complex task due to the vast number of antenna dimensions.","Recently, a new estimation method called the \"reduced-subspace least squares (RS-LS) estimator\" has been proposed for densely packed arrays.","This method relies solely on the geometry of the array to limit the estimation resources.","In this paper, we address a gap in the existing literature by deriving the average spectral efficiency for a certain distribution of user equipments (UEs) and a lower bound on it when using the RS-LS estimator.","This bound is determined by the channel gain and the statistics of the normalized spatial correlation matrices of potential UEs but, importantly, does not require knowledge of a specific UE's spatial correlation matrix.","We establish that there exists a pilot length that maximizes this expression.","Additionally, we derive an approximate expression for the optimal pilot length under low signal-to-noise ratio (SNR) conditions.","Simulation results validate the tightness of the derived lower bound and the effectiveness of using the optimized pilot length."],"url":"http://arxiv.org/abs/2402.01426v1","category":"cs.IT"}
{"created":"2024-02-02 14:11:23","title":"A Data-Driven Analysis of Robust Automatic Piano Transcription","abstract":"Algorithms for automatic piano transcription have improved dramatically in recent years due to new datasets and modeling techniques. Recent developments have focused primarily on adapting new neural network architectures, such as the Transformer and Perceiver, in order to yield more accurate systems. In this work, we study transcription systems from the perspective of their training data. By measuring their performance on out-of-distribution annotated piano data, we show how these models can severely overfit to acoustic properties of the training data. We create a new set of audio for the MAESTRO dataset, captured automatically in a professional studio recording environment via Yamaha Disklavier playback. Using various data augmentation techniques when training with the original and re-performed versions of the MAESTRO dataset, we achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS dataset, without seeing any of its training data. We subsequently analyze these data augmentation techniques in a series of ablation studies to better understand their influence on the resulting models.","sentences":["Algorithms for automatic piano transcription have improved dramatically in recent years due to new datasets and modeling techniques.","Recent developments have focused primarily on adapting new neural network architectures, such as the Transformer and Perceiver, in order to yield more accurate systems.","In this work, we study transcription systems from the perspective of their training data.","By measuring their performance on out-of-distribution annotated piano data, we show how these models can severely overfit to acoustic properties of the training data.","We create a new set of audio for the MAESTRO dataset, captured automatically in a professional studio recording environment via Yamaha Disklavier playback.","Using various data augmentation techniques when training with the original and re-performed versions of the MAESTRO dataset, we achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS dataset, without seeing any of its training data.","We subsequently analyze these data augmentation techniques in a series of ablation studies to better understand their influence on the resulting models."],"url":"http://arxiv.org/abs/2402.01424v1","category":"cs.SD"}
{"created":"2024-02-02 13:57:45","title":"Accretion and magnetism on young eccentric binaries: DQ Tau and AK Sco","abstract":"The accretion and ejection of mass in pre-main sequence (PMS) stars are key processes in stellar evolution as they shape the stellar angular momentum transport necessary for the stars' stability. Magnetospheric accretion onto classical T Tauri stars and low-mass PMS stars has been widely studied in the single-star case. This process can not be directly transferred to PMS binary systems, as tidal and gravitation effects, and/or accretion from a circumbinary disc (with variable separation of the components in the case of eccentric orbits) are in place. This work examines the accretion process of two PMS eccentric binaries, DQ Tau and AK Sco, using high-resolution spectropolarimetric time series. We investigate how magnetospheric accretion can be applied to these systems by studying the accretion-related emission lines and the magnetic field of each system. We discover that both systems are showing signs of magnetospheric accretion, despite their slightly different configurations, and the weak magnetic field of AK Sco. Furthermore, the magnetic topology of DQ Tau A shows a change relative to the previous orbital cycle studied: previously dominated by the poloidal component, it is now dominated by the toroidal component. We also report an increase of the component's accretion and the absence of an accretion burst at the apastron, suggesting that the component's magnetic variation might be the cause of the inter-cycle variations of the system's accretion. We conclude on the presence of magnetospheric accretion for both systems, together with gravitational effects, especially for AK Sco, composed of more massive components.","sentences":["The accretion and ejection of mass in pre-main sequence (PMS) stars are key processes in stellar evolution as they shape the stellar angular momentum transport necessary for the stars' stability.","Magnetospheric accretion onto classical T Tauri stars and low-mass PMS stars has been widely studied in the single-star case.","This process can not be directly transferred to PMS binary systems, as tidal and gravitation effects, and/or accretion from a circumbinary disc (with variable separation of the components in the case of eccentric orbits) are in place.","This work examines the accretion process of two PMS eccentric binaries, DQ Tau and AK Sco, using high-resolution spectropolarimetric time series.","We investigate how magnetospheric accretion can be applied to these systems by studying the accretion-related emission lines and the magnetic field of each system.","We discover that both systems are showing signs of magnetospheric accretion, despite their slightly different configurations, and the weak magnetic field of AK Sco.","Furthermore, the magnetic topology of DQ Tau A shows a change relative to the previous orbital cycle studied: previously dominated by the poloidal component, it is now dominated by the toroidal component.","We also report an increase of the component's accretion and the absence of an accretion burst at the apastron, suggesting that the component's magnetic variation might be the cause of the inter-cycle variations of the system's accretion.","We conclude on the presence of magnetospheric accretion for both systems, together with gravitational effects, especially for AK Sco, composed of more massive components."],"url":"http://arxiv.org/abs/2402.01419v1","category":"astro-ph.SR"}
{"created":"2024-02-02 13:38:45","title":"On the stability and exponential decay of the 3D MHD system with mixed partial dissipation near a equilibrium state","abstract":"A main result of this paper establishes the global stability of the 3D MHD equations with mixed partial dissipation near a background magnetic field in the domain $\\Omega=\\mathbb{T}^2\\times\\mathbb{R}$ with $\\mathbb{T}^2=[0, 1]^2$. More precisely, each velocity equation lacks its own directional dissipation, and the magnetic equation lacks vertical dissipation in the MHD system. The key point to obtain the stability result is that we decompose the solution $(u,b)$ into the zeroth horizontal mode and the non-zeroth modes and complete the desired bound with the strong Poincar\\'{e} type inequalities in the treatment of several nonlinear terms. Then we focus on the large-time behavior of the solution, where the non-zeroth modes decay exponentially in $H^2$, and the solution converges to its zeroth horizontal mode.","sentences":["A main result of this paper establishes the global stability of the 3D MHD equations with mixed partial dissipation near a background magnetic field in the domain $\\Omega=\\mathbb{T}^2\\times\\mathbb{R}$ with $\\mathbb{T}^2=[0, 1]^2$. More precisely, each velocity equation lacks its own directional dissipation, and the magnetic equation lacks vertical dissipation in the MHD system.","The key point to obtain the stability result is that we decompose the solution $(u,b)$ into the zeroth horizontal mode and the non-zeroth modes and complete the desired bound with the strong Poincar\\'{e} type inequalities in the treatment of several nonlinear terms.","Then we focus on the large-time behavior of the solution, where the non-zeroth modes decay exponentially in $H^2$, and the solution converges to its zeroth horizontal mode."],"url":"http://arxiv.org/abs/2402.01406v1","category":"math.AP"}
{"created":"2024-02-02 13:01:56","title":"Shifts on trees versus classical shifts in chain recurrence","abstract":"We construct continuous (and even invertible) linear operators acting on Banach (even Hilbert) spaces whose restrictions to their respective closed linear subspaces of chain recurrent vectors are not chain recurrent operators. This construction completely solves in the negative a problem posed by Nilson C. Bernardes Jr. and Alfred Peris on chain recurrence in Linear Dynamics. In particular: we show that the non-invertible case can be directly solved via relatively simple weighted backward shifts acting on certain unrooted directed trees; then we modify the non-invertible counterexample to address the invertible case, but falling outside the class of weighted shift operators; and we finally show that this behaviour cannot be achieved via classical (unilateral neither bilateral) weighted backward sifts (acting on $\\mathbb{N}$ and $\\mathbb{Z}$ respectively) by noticing that a classical shift is a chain recurrent operator whenever it admits a non-zero chain recurrent vector.","sentences":["We construct continuous (and even invertible) linear operators acting on Banach (even Hilbert) spaces whose restrictions to their respective closed linear subspaces of chain recurrent vectors are not chain recurrent operators.","This construction completely solves in the negative a problem posed by Nilson C. Bernardes Jr. and Alfred Peris on chain recurrence in Linear Dynamics.","In particular: we show that the non-invertible case can be directly solved via relatively simple weighted backward shifts acting on certain unrooted directed trees; then we modify the non-invertible counterexample to address the invertible case, but falling outside the class of weighted shift operators; and we finally show that this behaviour cannot be achieved via classical (unilateral neither bilateral) weighted backward sifts (acting on $\\mathbb{N}$ and $\\mathbb{Z}$ respectively) by noticing that a classical shift is a chain recurrent operator whenever it admits a non-zero chain recurrent vector."],"url":"http://arxiv.org/abs/2402.01377v1","category":"math.FA"}
{"created":"2024-02-02 12:48:49","title":"Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis","abstract":"In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\\epsilon$ which is better than the one obtained for actor-critic in a similar setting. We also show the results of numerical experiments on three benchmark settings and observe that the critic-actor algorithm competes well with the actor-critic algorithm.","sentences":["In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic.","In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented.","In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme.","We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\\mathcal{\\tilde{O}}(\\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\\epsilon$ which is better than the one obtained for actor-critic in a similar setting.","We also show the results of numerical experiments on three benchmark settings and observe that the critic-actor algorithm competes well with the actor-critic algorithm."],"url":"http://arxiv.org/abs/2402.01371v1","category":"cs.LG"}
{"created":"2024-02-02 12:27:32","title":"TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time (Extended Version)","abstract":"Machine learning (ML) plays a pivotal role in detecting malicious software. Despite the high F1-scores reported in numerous studies reaching upwards of 0.99, the issue is not completely solved. Malware detectors often experience performance decay due to constantly evolving operating systems and attack methods, which can render previously learned knowledge insufficient for accurate decision-making on new inputs. This paper argues that commonly reported results are inflated due to two pervasive sources of experimental bias in the detection task: spatial bias caused by data distributions that are not representative of a real-world deployment; and temporal bias caused by incorrect time splits of data, leading to unrealistic configurations. To address these biases, we introduce a set of constraints for fair experiment design, and propose a new metric, AUT, for classifier robustness in real-world settings. We additionally propose an algorithm designed to tune training data to enhance classifier performance. Finally, we present TESSERACT, an open-source framework for realistic classifier comparison. Our evaluation encompasses both traditional ML and deep learning methods, examining published works on an extensive Android dataset with 259,230 samples over a five-year span. Additionally, we conduct case studies in the Windows PE and PDF domains. Our findings identify the existence of biases in previous studies and reveal that significant performance enhancements are possible through appropriate, periodic tuning. We explore how mitigation strategies may support in achieving a more stable and better performance over time by employing multiple strategies to delay performance decay.","sentences":["Machine learning (ML) plays a pivotal role in detecting malicious software.","Despite the high F1-scores reported in numerous studies reaching upwards of 0.99, the issue is not completely solved.","Malware detectors often experience performance decay due to constantly evolving operating systems and attack methods, which can render previously learned knowledge insufficient for accurate decision-making on new inputs.","This paper argues that commonly reported results are inflated due to two pervasive sources of experimental bias in the detection task: spatial bias caused by data distributions that are not representative of a real-world deployment; and temporal bias caused by incorrect time splits of data, leading to unrealistic configurations.","To address these biases, we introduce a set of constraints for fair experiment design, and propose a new metric, AUT, for classifier robustness in real-world settings.","We additionally propose an algorithm designed to tune training data to enhance classifier performance.","Finally, we present TESSERACT, an open-source framework for realistic classifier comparison.","Our evaluation encompasses both traditional ML and deep learning methods, examining published works on an extensive Android dataset with 259,230 samples over a five-year span.","Additionally, we conduct case studies in the Windows PE and PDF domains.","Our findings identify the existence of biases in previous studies and reveal that significant performance enhancements are possible through appropriate, periodic tuning.","We explore how mitigation strategies may support in achieving a more stable and better performance over time by employing multiple strategies to delay performance decay."],"url":"http://arxiv.org/abs/2402.01359v1","category":"cs.LG"}
{"created":"2024-02-02 12:04:41","title":"Quantum Griffiths singularity in three-dimensional MoTiN superconducting films","abstract":"Quantum Griffiths singularity (QGS) has been experimentally observed in a range of two-dimensional (2D) superconducting systems. Although it is theoretically suggested that the QGS also exists in three-dimensional (3D) superconductors, there is almost no experimental support to the theoretical prediction. In the present paper, we observe the occurrence of QGS in a series of $\\sim$80-nm-thick Mo$_{0.8}$Ti$_{0.2}$N$_x$ ($0.84 \\lesssim x \\lesssim 1.12$) superconducting films near the field-driven superconductor-metal transition (SMT). These films have a NaCl structure and are 3D with respect to the superconductivity. For each film, the low-temperature magnetoresistance isotherms, measured at magnetic fields being perpendicular or parallel to the film plane, do not cross at a single point but at a clear wide region. The dynamical critical exponents $z\\nu_{\\perp}$ (for perpendicular field) and $z\\nu_{\\parallel}$ (for parallel field) obtained by analyzing the related magnetoresistance isotherms increase with decreasing temperature and tend to diverge as $T\\rightarrow 0$ K. In addition, the effective resistivity data for the perpendicular and parallel field in the vicinity of the SMTs both obey an activated scaling based on the random transverse-field Ising model. We also fabricate a $\\sim$80-nm-thick (Mo$_{0.8}$Ti$_{0.2}$)$_2$N$_{1.06}$ superconducting film with face-centered cubic structure at low nitrogen partial pressure. It is found that the low-temperature magnetoresistance isotherms for the perpendicular (parallel) field cross at a single point and the resistivity data for the perpendicular (parallel) field in the vicinity of the field-induced SMT obey the power-law scaling deduced from the dirty-boson model. Our results provide unambigous experimental evidence for the existence of QGS in 3D superconductors.","sentences":["Quantum Griffiths singularity (QGS) has been experimentally observed in a range of two-dimensional (2D) superconducting systems.","Although it is theoretically suggested that the QGS also exists in three-dimensional (3D) superconductors, there is almost no experimental support to the theoretical prediction.","In the present paper, we observe the occurrence of QGS in a series of $\\sim$80-nm-thick Mo$_{0.8}$Ti$_{0.2}$N$_x$ ($0.84 \\lesssim x \\lesssim 1.12$) superconducting films near the field-driven superconductor-metal transition (SMT).","These films have a NaCl structure and are 3D with respect to the superconductivity.","For each film, the low-temperature magnetoresistance isotherms, measured at magnetic fields being perpendicular or parallel to the film plane, do not cross at a single point but at a clear wide region.","The dynamical critical exponents $z\\nu_{\\perp}$ (for perpendicular field) and $z\\nu_{\\parallel}$ (for parallel field) obtained by analyzing the related magnetoresistance isotherms increase with decreasing temperature and tend to diverge as $T\\rightarrow 0$ K.","In addition, the effective resistivity data for the perpendicular and parallel field in the vicinity of the SMTs both obey an activated scaling based on the random transverse-field Ising model.","We also fabricate a $\\sim$80-nm-thick (Mo$_{0.8}$Ti$_{0.2}$)$_2$N$_{1.06}$ superconducting film with face-centered cubic structure at low nitrogen partial pressure.","It is found that the low-temperature magnetoresistance isotherms for the perpendicular (parallel) field cross at a single point and the resistivity data for the perpendicular (parallel) field in the vicinity of the field-induced SMT obey the power-law scaling deduced from the dirty-boson model.","Our results provide unambigous experimental evidence for the existence of QGS in 3D superconductors."],"url":"http://arxiv.org/abs/2402.01347v1","category":"cond-mat.supr-con"}
{"created":"2024-02-02 11:47:56","title":"Inferring the Langevin Equation with Uncertainty via Bayesian Neural Networks","abstract":"Pervasive across diverse domains, stochastic systems exhibit fluctuations in processes ranging from molecular dynamics to climate phenomena. The Langevin equation has served as a common mathematical model for studying such systems, enabling predictions of their temporal evolution and analyses of thermodynamic quantities, including absorbed heat, work done on the system, and entropy production. However, inferring the Langevin equation from observed trajectories remains challenging, particularly for nonlinear and high-dimensional systems. In this study, we present a comprehensive framework that employs Bayesian neural networks for inferring Langevin equations in both overdamped and underdamped regimes. Our framework first provides the drift force and diffusion matrix separately and then combines them to construct the Langevin equation. By providing a distribution of predictions instead of a single value, our approach allows us to assess prediction uncertainties, which can prevent potential misunderstandings and erroneous decisions about the system. We demonstrate the effectiveness of our framework in inferring Langevin equations for various scenarios including a neuron model and microscopic engine, highlighting its versatility and potential impact.","sentences":["Pervasive across diverse domains, stochastic systems exhibit fluctuations in processes ranging from molecular dynamics to climate phenomena.","The Langevin equation has served as a common mathematical model for studying such systems, enabling predictions of their temporal evolution and analyses of thermodynamic quantities, including absorbed heat, work done on the system, and entropy production.","However, inferring the Langevin equation from observed trajectories remains challenging, particularly for nonlinear and high-dimensional systems.","In this study, we present a comprehensive framework that employs Bayesian neural networks for inferring Langevin equations in both overdamped and underdamped regimes.","Our framework first provides the drift force and diffusion matrix separately and then combines them to construct the Langevin equation.","By providing a distribution of predictions instead of a single value, our approach allows us to assess prediction uncertainties, which can prevent potential misunderstandings and erroneous decisions about the system.","We demonstrate the effectiveness of our framework in inferring Langevin equations for various scenarios including a neuron model and microscopic engine, highlighting its versatility and potential impact."],"url":"http://arxiv.org/abs/2402.01338v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-02 11:31:49","title":"Video Semantic Communication with Major Object Extraction and Contextual Video Encoding","abstract":"This paper studies an end-to-end video semantic communication system for massive communication. In the considered system, the transmitter must continuously send the video to the receiver to facilitate character reconstruction in immersive applications, such as interactive video conference. However, transmitting the original video information with substantial amounts of data poses a challenge to the limited wireless resources. To address this issue, we reduce the amount of data transmitted by making the transmitter extract and send the semantic information from the video, which refines the major object and the correlation of time and space in the video. Specifically, we first develop a video semantic communication system based on major object extraction (MOE) and contextual video encoding (CVE) to achieve efficient video transmission. Then, we design the MOE and CVE modules with convolutional neural network based motion estimation, contextual extraction and entropy coding. Simulation results show that compared to the traditional coding schemes, the proposed method can reduce the amount of transmitted data by up to 25% while increasing the peak signal-to-noise ratio (PSNR) of the reconstructed video by up to 14%.","sentences":["This paper studies an end-to-end video semantic communication system for massive communication.","In the considered system, the transmitter must continuously send the video to the receiver to facilitate character reconstruction in immersive applications, such as interactive video conference.","However, transmitting the original video information with substantial amounts of data poses a challenge to the limited wireless resources.","To address this issue, we reduce the amount of data transmitted by making the transmitter extract and send the semantic information from the video, which refines the major object and the correlation of time and space in the video.","Specifically, we first develop a video semantic communication system based on major object extraction (MOE) and contextual video encoding (CVE) to achieve efficient video transmission.","Then, we design the MOE and CVE modules with convolutional neural network based motion estimation, contextual extraction and entropy coding.","Simulation results show that compared to the traditional coding schemes, the proposed method can reduce the amount of transmitted data by up to 25% while increasing the peak signal-to-noise ratio (PSNR) of the reconstructed video by up to 14%."],"url":"http://arxiv.org/abs/2402.01330v1","category":"cs.NI"}
{"created":"2024-02-02 11:30:11","title":"Emerging topological states in EuMn$_2$Bi$_2$: A first principles prediction","abstract":"New materials with magnetic order driven topological phases are hugely sought after for their immense application potential. In this work, we propose a new compound EuMn$_2$Bi$_2$ from our first principles density functional theory calculations to host novel topological phases such as Dirac/Weyl semimetal and topological insulator in its different magnetic states which are energetically close to one another. We started with an isostructural compound EuMn$_2$As$_2$ where the magnetic structure has been studied experimentally. From our calculations we could explain the nature of two magnetic transitions observed experimentally in this system and could also establish the correct magnetic ground state. Our electronic structure calculations reveal the insulating nature of the ground state consistent with the experiments. By replacing all As by Bi in EuMn$_2$As$_2$ and by optimizing the new structure, we obtained the new compound EuMn$_2$Bi$_2$. We observe this compound to be dynamically stable from our phonon calculations supporting its experimental preparation in future. By comparing the total energies of various possible magnetic structures we identified the ground state. Though the magnetic ground state is found to be insulating in nature with tiny band gap which is an order of magnitude less than the same in EuMn$_2$As$_2$, there were other magnetic states energetically very close to the ground state which display remarkable non-trivial band topology such as Dirac/Weyl points close to the Fermi level and topological insulator state. The energetic proximity of these magnetic order driven topological phases makes them tunable via external handle which indicates that the proposed new material EuMn$_2$Bi$_2$ would be a very versatile magnetic topological material.","sentences":["New materials with magnetic order driven topological phases are hugely sought after for their immense application potential.","In this work, we propose a new compound EuMn$_2$Bi$_2$ from our first principles density functional theory calculations to host novel topological phases such as Dirac/Weyl semimetal and topological insulator in its different magnetic states which are energetically close to one another.","We started with an isostructural compound EuMn$_2$As$_2$ where the magnetic structure has been studied experimentally.","From our calculations we could explain the nature of two magnetic transitions observed experimentally in this system and could also establish the correct magnetic ground state.","Our electronic structure calculations reveal the insulating nature of the ground state consistent with the experiments.","By replacing all As by Bi in EuMn$_2$As$_2$ and by optimizing the new structure, we obtained the new compound EuMn$_2$Bi$_2$. We observe this compound to be dynamically stable from our phonon calculations supporting its experimental preparation in future.","By comparing the total energies of various possible magnetic structures we identified the ground state.","Though the magnetic ground state is found to be insulating in nature with tiny band gap which is an order of magnitude less than the same in EuMn$_2$As$_2$, there were other magnetic states energetically very close to the ground state which display remarkable non-trivial band topology such as Dirac/Weyl points close to the Fermi level and topological insulator state.","The energetic proximity of these magnetic order driven topological phases makes them tunable via external handle which indicates that the proposed new material EuMn$_2$Bi$_2$ would be a very versatile magnetic topological material."],"url":"http://arxiv.org/abs/2402.01328v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 11:21:06","title":"Unveiling the early stage evolution of local atomic structures in the crystallization process of a metallic glass","abstract":"The early stage evolution of local atomic structures in a multicomponent metallic glass during its crystallization process has been investigated via molecular dynamics simulation. It is found that the initial thermal stability and earliest stage evolution of the local atomic clusters show no strong correlation with their initial short-range orders, and this leads to an observation of a novel symmetry convergence phenomenon, which can be understood as an atomic structure manifestation of the ergodicity. Furthermore, in our system we have quantitatively proved that the crucial factor for the thermal stability against crystallization exhibited by the metallic glass, is not the total amount of icosahedral clusters, but the degree of global connectivity among them.","sentences":["The early stage evolution of local atomic structures in a multicomponent metallic glass during its crystallization process has been investigated via molecular dynamics simulation.","It is found that the initial thermal stability and earliest stage evolution of the local atomic clusters show no strong correlation with their initial short-range orders, and this leads to an observation of a novel symmetry convergence phenomenon, which can be understood as an atomic structure manifestation of the ergodicity.","Furthermore, in our system we have quantitatively proved that the crucial factor for the thermal stability against crystallization exhibited by the metallic glass, is not the total amount of icosahedral clusters, but the degree of global connectivity among them."],"url":"http://arxiv.org/abs/2402.01325v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 11:17:54","title":"On the mean-field limit for Stein variational gradient descent: stability and multilevel approximation","abstract":"In this paper we propose and analyze a novel multilevel version of Stein variational gradient descent (SVGD). SVGD is a recent particle based variational inference method. For Bayesian inverse problems with computationally expensive likelihood evaluations, the method can become prohibitive as it requires to evolve a discrete dynamical system over many time steps, each of which requires likelihood evaluations at all particle locations. To address this, we introduce a multilevel variant that involves running several interacting particle dynamics in parallel corresponding to different approximation levels of the likelihood. By carefully tuning the number of particles at each level, we prove that a significant reduction in computational complexity can be achieved. As an application we provide a numerical experiment for a PDE driven inverse problem, which confirms the speed up suggested by our theoretical results.","sentences":["In this paper we propose and analyze a novel multilevel version of Stein variational gradient descent (SVGD).","SVGD is a recent particle based variational inference method.","For Bayesian inverse problems with computationally expensive likelihood evaluations, the method can become prohibitive as it requires to evolve a discrete dynamical system over many time steps, each of which requires likelihood evaluations at all particle locations.","To address this, we introduce a multilevel variant that involves running several interacting particle dynamics in parallel corresponding to different approximation levels of the likelihood.","By carefully tuning the number of particles at each level, we prove that a significant reduction in computational complexity can be achieved.","As an application we provide a numerical experiment for a PDE driven inverse problem, which confirms the speed up suggested by our theoretical results."],"url":"http://arxiv.org/abs/2402.01320v1","category":"math.NA"}
{"created":"2024-02-02 10:57:40","title":"Controlling NMR spin systems for quantum computation","abstract":"Nuclear magnetic resonance is arguably both the best available quantum technology for implementing simple quantum computing experiments and the worst technology for building large scale quantum computers that has ever been seriously put forward. After a few years of rapid growth, leading to an implementation of Shor's quantum factoring algorithm in a seven-spin system, the field started to reach its natural limits and further progress became challenging. Rather than pursuing more complex algorithms on larger systems, interest has now largely moved into developing techniques for the precise and efficient manipulation of spin states with the aim of developing methods that can be applied in other more scalable technologies and within conventional NMR. However, the user friendliness of NMR implementations means that they remain popular for proof-of-principle demonstrations of simple quantum information protocols.","sentences":["Nuclear magnetic resonance is arguably both the best available quantum technology for implementing simple quantum computing experiments and the worst technology for building large scale quantum computers that has ever been seriously put forward.","After a few years of rapid growth, leading to an implementation of Shor's quantum factoring algorithm in a seven-spin system, the field started to reach its natural limits and further progress became challenging.","Rather than pursuing more complex algorithms on larger systems, interest has now largely moved into developing techniques for the precise and efficient manipulation of spin states with the aim of developing methods that can be applied in other more scalable technologies and within conventional NMR.","However, the user friendliness of NMR implementations means that they remain popular for proof-of-principle demonstrations of simple quantum information protocols."],"url":"http://arxiv.org/abs/2402.01308v1","category":"quant-ph"}
{"created":"2024-02-02 10:56:49","title":"Integrated anti-electronics for positron annihilation spectroscopy","abstract":"Imaging the features of a sample using Positron Annihilation Spectroscopy (PAS) is currently achieved by rastering, i.e. by scanning the sample surface with a sharply focused positron beam. However, a beam of arbitrary shape (sculpted beam) would allow the application of more versatile single-pixel imaging (SPI) techniques. I introduce the design of a microelectronic device employing a 2D array of Zener diodes as an active positron moderator, capable of sculpting positron beams with 6um resolution. The re-emitted positrons are accelerated towards the sample through a miniaturised electrostatic lens system and reaching 100nm resolution. The fast switch-on (90ps) and switch-off (250ps) time of the device would enable state-of-the-art positron annihilation lifetime spectroscopy (PALS) and PAS imaging with high spatial and temporal resolution.","sentences":["Imaging the features of a sample using Positron Annihilation Spectroscopy (PAS) is currently achieved by rastering, i.e. by scanning the sample surface with a sharply focused positron beam.","However, a beam of arbitrary shape (sculpted beam) would allow the application of more versatile single-pixel imaging (SPI) techniques.","I introduce the design of a microelectronic device employing a 2D array of Zener diodes as an active positron moderator, capable of sculpting positron beams with 6um resolution.","The re-emitted positrons are accelerated towards the sample through a miniaturised electrostatic lens system and reaching 100nm resolution.","The fast switch-on (90ps) and switch-off (250ps) time of the device would enable state-of-the-art positron annihilation lifetime spectroscopy (PALS) and PAS imaging with high spatial and temporal resolution."],"url":"http://arxiv.org/abs/2402.01307v1","category":"physics.ins-det"}
{"created":"2024-02-02 10:51:14","title":"Modeling the hallmarks of avascular tumors","abstract":"We present a stochastic computational model of avascular tumors, emphasizing the detailed implementation of the first four so-called hallmarks of cancer: self-sufficiency in growth factors, resistance to growth inhibitors, avoidance of apoptosis, and unlimited growth potential. Our goal is to provide a foundational understanding of the first steps of cancer malignancy while addressing modeling uncertainties, thus bringing us closer to a first-principles grasp of this process. Preliminary numerical simulations illustrate the comprehensiveness of our perspective.","sentences":["We present a stochastic computational model of avascular tumors, emphasizing the detailed implementation of the first four so-called hallmarks of cancer: self-sufficiency in growth factors, resistance to growth inhibitors, avoidance of apoptosis, and unlimited growth potential.","Our goal is to provide a foundational understanding of the first steps of cancer malignancy while addressing modeling uncertainties, thus bringing us closer to a first-principles grasp of this process.","Preliminary numerical simulations illustrate the comprehensiveness of our perspective."],"url":"http://arxiv.org/abs/2402.01305v1","category":"q-bio.PE"}
{"created":"2024-02-02 10:35:05","title":"Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted Inference","abstract":"Privacy-preserving neural networks have attracted increasing attention in recent years, and various algorithms have been developed to keep the balance between accuracy, computational complexity and information security from the cryptographic view. This work takes a different view from the input data and structure of neural networks. We decompose the input data (e.g., some images) into sensitive and insensitive segments according to importance and privacy. The sensitive segment includes some important and private information such as human faces and we take strong homomorphic encryption to keep security, whereas the insensitive one contains some background and we add perturbations. We propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal with two segments, respectively, and ciphertext branch could utilize the information from plaintext branch by unidirectional connections. We adopt knowledge distillation for our bi-CryptoNets by transferring representations from a well-trained teacher neural network. Empirical studies show the effectiveness and decrease of inference latency for our bi-CryptoNets.","sentences":["Privacy-preserving neural networks have attracted increasing attention in recent years, and various algorithms have been developed to keep the balance between accuracy, computational complexity and information security from the cryptographic view.","This work takes a different view from the input data and structure of neural networks.","We decompose the input data (e.g., some images) into sensitive and insensitive segments according to importance and privacy.","The sensitive segment includes some important and private information such as human faces and we take strong homomorphic encryption to keep security, whereas the insensitive one contains some background and we add perturbations.","We propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal with two segments, respectively, and ciphertext branch could utilize the information from plaintext branch by unidirectional connections.","We adopt knowledge distillation for our bi-CryptoNets by transferring representations from a well-trained teacher neural network.","Empirical studies show the effectiveness and decrease of inference latency for our bi-CryptoNets."],"url":"http://arxiv.org/abs/2402.01296v1","category":"cs.LG"}
{"created":"2024-02-02 10:31:28","title":"Minimizing Regret in Billboard Advertisement under Zonal Influence Constraint","abstract":"In a typical billboard advertisement technique, a number of digital billboards are owned by an influence provider, and many advertisers approach the influence provider for a specific number of views of their advertisement content on a payment basis. If the influence provider provides the demanded or more influence, then he will receive the full payment or else a partial payment. In the context of an influence provider, if he provides more or less than an advertiser's demanded influence, it is a loss for him. This is formalized as 'Regret', and naturally, in the context of the influence provider, the goal will be to allocate the billboard slots among the advertisers such that the total regret is minimized. In this paper, we study this problem as a discrete optimization problem and propose four solution approaches. The first one selects the billboard slots from the available ones in an incremental greedy manner, and we call this method the Budget Effective Greedy approach. In the second one, we introduce randomness with the first one, where we perform the marginal gain computation for a sample of randomly chosen billboard slots. The remaining two approaches are further improvements over the second one. We analyze all the algorithms to understand their time and space complexity. We implement them with real-life trajectory and billboard datasets and conduct a number of experiments. It has been observed that the randomized budget effective greedy approach takes reasonable computational time while minimizing the regret.","sentences":["In a typical billboard advertisement technique, a number of digital billboards are owned by an influence provider, and many advertisers approach the influence provider for a specific number of views of their advertisement content on a payment basis.","If the influence provider provides the demanded or more influence, then he will receive the full payment or else a partial payment.","In the context of an influence provider, if he provides more or less than an advertiser's demanded influence, it is a loss for him.","This is formalized as 'Regret', and naturally, in the context of the influence provider, the goal will be to allocate the billboard slots among the advertisers such that the total regret is minimized.","In this paper, we study this problem as a discrete optimization problem and propose four solution approaches.","The first one selects the billboard slots from the available ones in an incremental greedy manner, and we call this method the Budget Effective Greedy approach.","In the second one, we introduce randomness with the first one, where we perform the marginal gain computation for a sample of randomly chosen billboard slots.","The remaining two approaches are further improvements over the second one.","We analyze all the algorithms to understand their time and space complexity.","We implement them with real-life trajectory and billboard datasets and conduct a number of experiments.","It has been observed that the randomized budget effective greedy approach takes reasonable computational time while minimizing the regret."],"url":"http://arxiv.org/abs/2402.01294v1","category":"cs.DB"}
{"created":"2024-02-02 10:26:44","title":"On quasiconformal dimension distortion for subsets of the real line","abstract":"Optimal quasiconformal dimension distortions bounds for subsets of the complex plane have been established by Astala. We show that these estimates can be improved when one considers subsets of the real line of arbitrary Hausdorff dimension. We present some explicit numerical bounds.","sentences":["Optimal quasiconformal dimension distortions bounds for subsets of the complex plane have been established by Astala.","We show that these estimates can be improved when one considers subsets of the real line of arbitrary Hausdorff dimension.","We present some explicit numerical bounds."],"url":"http://arxiv.org/abs/2402.01291v1","category":"math.CV"}
{"created":"2024-02-02 10:25:20","title":"Induced Norm Analysis of Linear Systems for Nonnegative Input Signals","abstract":"This paper is concerned with the analysis of the $L_p\\ (p\\in[1,\\infty), p=\\infty)$ induced norms of continuous-time linear systems where input signals are restricted to be nonnegative. This norm is referred to as the $L_{p+}$ induced norm in this paper. It has been shown recently that the $L_{2+}$ induced norm is effective for the stability analysis of nonlinear feedback systems where the nonlinearity returns only nonnegative signals. However, the exact computation of the $L_{2+}$ induced norm is essentially difficult. To get around this difficulty, in the first part of this paper, we provide a copositive-programming-based method for the upper bound computation by capturing the nonnegativity of the input signals by copositive multipliers. Then, in the second part of the paper, we derive uniform lower bounds of the $L_{p+}\\ (p\\in[1,\\infty), p=\\infty)$ induced norms with respect to the standard $L_{p}$ induced norms that are valid for all linear systems including infinite-dimensional ones. For each linear system, we finally derive a computation method of the lower bounds of the $L_{2+}$ induced norm that are larger than (or equal to) the uniform one. The effectiveness of the upper/lower bound computation methods are fully illustrated by numerical examples.","sentences":["This paper is concerned with the analysis of the $L_p\\ (p\\in[1,\\infty), p=\\infty)$ induced norms of continuous-time linear systems where input signals are restricted to be nonnegative.","This norm is referred to as the $L_{p+}$ induced norm in this paper.","It has been shown recently that the $L_{2+}$ induced norm is effective for the stability analysis of nonlinear feedback systems where the nonlinearity returns only nonnegative signals.","However, the exact computation of the $L_{2+}$ induced norm is essentially difficult.","To get around this difficulty, in the first part of this paper, we provide a copositive-programming-based method for the upper bound computation by capturing the nonnegativity of the input signals by copositive multipliers.","Then, in the second part of the paper, we derive uniform lower bounds of the $L_{p+}\\ (p\\in[1,\\infty), p=\\infty)$ induced norms with respect to the standard $L_{p}$ induced norms that are valid for all linear systems including infinite-dimensional ones.","For each linear system, we finally derive a computation method of the lower bounds of the $L_{2+}$ induced norm that are larger than (or equal to) the uniform one.","The effectiveness of the upper/lower bound computation methods are fully illustrated by numerical examples."],"url":"http://arxiv.org/abs/2402.01288v1","category":"math.OC"}
{"created":"2024-02-02 10:18:16","title":"A natural correspondence between quasiconcave functions and fuzzy norms","abstract":"In this note we show that the usual notion of fuzzy norm defined on a linear space is equivalent to that of quasiconcave function, in the sense that every fuzzy norm $N:X\\times\\mathbb{R}[0,1]$ defined on a (real or complex) linear space X is uniquely determined by a quasiconcave function $f:X\\to[0, 1]$. We explore the minimum requirements that we need to impose to some quasiconcave function $f:X\\to[0, 1]$ in order to define a fuzzy norm $N:X\\times\\mathbb{R}[0,1]$. Later we use this equivalence to prove some properties of fuzzy norms, like a generalisation of the celebrated Decomposition Theorem.","sentences":["In this note we show that the usual notion of fuzzy norm defined on a linear space is equivalent to that of quasiconcave function, in the sense that every fuzzy norm $N:X\\times\\mathbb{R}[0,1]$ defined on a (real or complex) linear space X is uniquely determined by a quasiconcave function $f:X\\to[0, 1]$. We explore the minimum requirements that we need to impose to some quasiconcave function $f:X\\to[0, 1]$ in order to define a fuzzy norm $N:X\\times\\mathbb{R}[0,1]$. Later we use this equivalence to prove some properties of fuzzy norms, like a generalisation of the celebrated Decomposition Theorem."],"url":"http://arxiv.org/abs/2402.01283v1","category":"math.MG"}
{"created":"2024-02-02 10:11:42","title":"An Improved Viterbi Algorithm for a Class of Optimal Binary Convolutional Codes","abstract":"The most famous error-decoding algorithm for convolutional codes is the Viterbi algorithm. In this paper, we present a new reduced complexity version of this algorithm which can be applied to a class of binary convolutional codes with optimum column distances called k-partial simplex convolutional codes.","sentences":["The most famous error-decoding algorithm for convolutional codes is the Viterbi algorithm.","In this paper, we present a new reduced complexity version of this algorithm which can be applied to a class of binary convolutional codes with optimum column distances called k-partial simplex convolutional codes."],"url":"http://arxiv.org/abs/2402.01279v1","category":"cs.IT"}
{"created":"2024-02-02 09:58:11","title":"Nonconventional screening of Coulomb interaction in two-dimensional semiconductors and metals: A comprehensive cRPA study of MX2 (M=Mo, W, Nb, Ta; X=S, Se, Te)","abstract":"Experimental observations of large exciton binding energies and non-hydrogenic Rydberg series in 2D semiconducting TMDs, along with deviations in plasmon dispersion in 2D metallic TMDs, suggest the presence of a nonconventional screening of the Coulomb interaction. The experimentally observed Mott insulating state in the charge density wave (CDW) reconstructed lattice of TMDs containing 4d and 5d elements further confirms the presence of strong Coulomb interactions in these systems. In this study, we use first-principles electronic structure calculations and constrained random-phase approximation to calculate the Coulomb interaction parameters (partially screened U and fully screened W) between localized $d$ electrons in 2D TMDs. We specifically explore materials represented by the formula MX2 (M=Nb, Ta, Mo, W, and X=S, Se, Te) and consider three different phases (1H, 1T, and 1T'). Our results show that the short-range interactions are strongly screened in all three phases, whereas the long-range interactions remain significant even in metallic systems. This nonconventional screening provides a compelling explanation for the deviations observed in the usual hydrogenic Rydberg series and conventional plasmon dispersion in 2D semiconducting and metallic TMDs, respectively. Our calculations yield on-site Coulomb interaction parameters U within the ranges of 0.8-2.5 eV, 0.8-1.9 eV, and 0.9-2.4 eV for the 1H, 1T, and 1T' structures, respectively. Furthermore, our findings indicate a substantially high ratio of on-site effective Coulomb interaction to bandwidth (U_eff/W_b >> 1) in CDW TMDs, providing robust evidence for the experimentally observed strongly correlated Mott phase.","sentences":["Experimental observations of large exciton binding energies and non-hydrogenic Rydberg series in 2D semiconducting TMDs, along with deviations in plasmon dispersion in 2D metallic TMDs, suggest the presence of a nonconventional screening of the Coulomb interaction.","The experimentally observed Mott insulating state in the charge density wave (CDW) reconstructed lattice of TMDs containing 4d and 5d elements further confirms the presence of strong Coulomb interactions in these systems.","In this study, we use first-principles electronic structure calculations and constrained random-phase approximation to calculate the Coulomb interaction parameters (partially screened U and fully screened W) between localized $d$ electrons in 2D TMDs.","We specifically explore materials represented by the formula MX2 (M=Nb, Ta, Mo, W, and X=S, Se, Te) and consider three different phases (1H, 1T, and 1T').","Our results show that the short-range interactions are strongly screened in all three phases, whereas the long-range interactions remain significant even in metallic systems.","This nonconventional screening provides a compelling explanation for the deviations observed in the usual hydrogenic Rydberg series and conventional plasmon dispersion in 2D semiconducting and metallic TMDs, respectively.","Our calculations yield on-site Coulomb interaction parameters U within the ranges of 0.8-2.5 eV, 0.8-1.9 eV, and 0.9-2.4 eV for the 1H, 1T, and 1T' structures, respectively.","Furthermore, our findings indicate a substantially high ratio of on-site effective Coulomb interaction to bandwidth (U_eff/W_b >> 1) in CDW TMDs, providing robust evidence for the experimentally observed strongly correlated Mott phase."],"url":"http://arxiv.org/abs/2402.01273v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 09:39:32","title":"Decoupled few-femtosecond phase transitions in vanadium dioxide","abstract":"The nature of the insulator-to-metal phase transition in vanadium dioxide (VO2) is one of the longest-standing problems in condensed-matter physics. Ultrafast spectroscopy has long promised to determine whether the transition is primarily driven by the electronic or structural degree of freedom, but measurements to date have been stymied by their sensitivity to only one of these components and/or their limited temporal resolution. Here we use ultra-broadband few-femtosecond pump-probe spectroscopy to resolve the electronic and structural phase transitions in VO2 at their fundamental time scales. We find that the system transforms into a bad-metallic phase within 10 fs after photoexcitation, but requires another 100 fs to complete the transition, during which we observe electronic oscillations and a partial re-opening of the bandgap, signalling a transient semi-metallic state. Comparisons with tensor-network simulations and density-functional theory calculations show these features originate from oscillations around the equilibrium high-symmetry atomic positions during an unprecedentedly fast structural transition, in which the vanadium dimers separate and untwist with two different timescales. Our results resolve the complete structural and electronic nature of the light-induced phase transition in VO2 and establish ultra-broadband few-femtosecond spectroscopy as a powerful new tool for studying quantum materials out of equilibrium.","sentences":["The nature of the insulator-to-metal phase transition in vanadium dioxide (VO2) is one of the longest-standing problems in condensed-matter physics.","Ultrafast spectroscopy has long promised to determine whether the transition is primarily driven by the electronic or structural degree of freedom, but measurements to date have been stymied by their sensitivity to only one of these components and/or their limited temporal resolution.","Here we use ultra-broadband few-femtosecond pump-probe spectroscopy to resolve the electronic and structural phase transitions in VO2 at their fundamental time scales.","We find that the system transforms into a bad-metallic phase within 10 fs after photoexcitation, but requires another 100 fs to complete the transition, during which we observe electronic oscillations and a partial re-opening of the bandgap, signalling a transient semi-metallic state.","Comparisons with tensor-network simulations and density-functional theory calculations show these features originate from oscillations around the equilibrium high-symmetry atomic positions during an unprecedentedly fast structural transition, in which the vanadium dimers separate and untwist with two different timescales.","Our results resolve the complete structural and electronic nature of the light-induced phase transition in VO2 and establish ultra-broadband few-femtosecond spectroscopy as a powerful new tool for studying quantum materials out of equilibrium."],"url":"http://arxiv.org/abs/2402.01266v1","category":"cond-mat.str-el"}
{"created":"2024-02-02 09:14:33","title":"Light-induced phase transitions in vanadium dioxide: a tensor network study","abstract":"Nonequilibrium phase transitions driven by light pulses represent a rapidly developing field in condensed matter physics. As one of the archetypal strongly correlated materials, vanadium dioxide (VO2) undergoes a structural phase transition (SPT) from a monoclinic (M1) to rutile (R) structure and an insulator-to-metal transition (IMT) either when heated above 340 K or when excited by an ultrafast laser pulse. Here, we present a tensor network study of the light-induced phase transitions in VO2 based on a quasi-one-dimensional model with all the important ingredients -- multi-orbital character, electron-lattice coupling, and electron-electron correlations -- being included. We show that this model qualitatively captures the equilibrium properties of VO2 by calculating the ground state phase diagram and finite-temperature phase transitions. A hybrid quantum-classical tensor-network method is used to simulate the dynamics following photoexcitation. We find that the structure can transform faster than the harmonic phonon modes of M1 phase, suggesting lattice nonlinearity is key in the SPT. We also find separate timescales for the evolution of dimerization and tilt distortions in the lattice dynamics, as well as the loss and subsequent partial restoration behavior of the displacements, which can provide an explanation for the complex dynamics observed in recent experiments [C. Brahms et al., arXiv:XXXX.XXXXX]. Moreover, decoupled SPT and IMT dynamics are observed in the numerical simulations: while the initial M1 structure transforms to the R one in tens of femtoseconds, the IMT occurs quasi-instantaneously, consistent with recent experimental findings. Our theoretical studies provide insight into the light-induced phase transitions of VO2, revealing unexpected non-monotonic transformation pathways and paving the way for future studies of non-thermal phase transformations.","sentences":["Nonequilibrium phase transitions driven by light pulses represent a rapidly developing field in condensed matter physics.","As one of the archetypal strongly correlated materials, vanadium dioxide (VO2) undergoes a structural phase transition (SPT) from a monoclinic (M1) to rutile (R) structure and an insulator-to-metal transition (IMT) either when heated above 340 K or when excited by an ultrafast laser pulse.","Here, we present a tensor network study of the light-induced phase transitions in VO2 based on a quasi-one-dimensional model with all the important ingredients -- multi-orbital character, electron-lattice coupling, and electron-electron correlations -- being included.","We show that this model qualitatively captures the equilibrium properties of VO2 by calculating the ground state phase diagram and finite-temperature phase transitions.","A hybrid quantum-classical tensor-network method is used to simulate the dynamics following photoexcitation.","We find that the structure can transform faster than the harmonic phonon modes of M1 phase, suggesting lattice nonlinearity is key in the SPT.","We also find separate timescales for the evolution of dimerization and tilt distortions in the lattice dynamics, as well as the loss and subsequent partial restoration behavior of the displacements, which can provide an explanation for the complex dynamics observed in recent experiments [C. Brahms et al., arXiv:XXXX.XXXXX].","Moreover, decoupled SPT and IMT dynamics are observed in the numerical simulations: while the initial M1 structure transforms to the R one in tens of femtoseconds, the IMT occurs quasi-instantaneously, consistent with recent experimental findings.","Our theoretical studies provide insight into the light-induced phase transitions of VO2, revealing unexpected non-monotonic transformation pathways and paving the way for future studies of non-thermal phase transformations."],"url":"http://arxiv.org/abs/2402.01247v1","category":"cond-mat.str-el"}
{"created":"2024-02-02 09:10:40","title":"Quantum simulation of Fermi-Hubbard model based on transmon qudit interaction","abstract":"The Fermi-Hubbard model, a fundamental framework for studying strongly correlated phenomena could significantly benefit from quantum simulations when exploring non-trivial settings. However, simulating this problem requires twice as many qubits as the physical sites, in addition to complicated on-chip connectivities and swap gates required to simulate the physical interactions. In this work, we introduce a novel quantum simulation approach utilizing qudits to overcome such complexities. Leveraging on the symmetries of the Fermi-Hubbard model and their intrinsic relation to Clifford algebras, we first demonstrate a Qudit Fermionic Mapping (QFM) that reduces the encoding cost associated with the qubit-based approach. We then describe the unitary evolution of the mapped Hamiltonian by interpreting the resulting Majorana operators in terms of physical single- and two-qudit gates. While the QFM can be used for any quantum hardware with four accessible energy levels, we demonstrate the specific reduction in overhead resulting from utilizing the native Controlled-SUM gate (equivalent to qubit CNOT) for a fixed-frequency ququart transmon. We further transpile the resulting two transmon-qudit gates by demonstrating a qudit operator Schmidt decomposition using the Controlled-SUM gate. Finally, we demonstrate the efficacy of our proposal by numerical simulation of local observables such as the filling factor and Green's function for various Trotter steps. The compatibility of our approach with different qudit platforms paves the path for achieving quantum advantage in simulating non-trivial quantum many-body systems.","sentences":["The Fermi-Hubbard model, a fundamental framework for studying strongly correlated phenomena could significantly benefit from quantum simulations when exploring non-trivial settings.","However, simulating this problem requires twice as many qubits as the physical sites, in addition to complicated on-chip connectivities and swap gates required to simulate the physical interactions.","In this work, we introduce a novel quantum simulation approach utilizing qudits to overcome such complexities.","Leveraging on the symmetries of the Fermi-Hubbard model and their intrinsic relation to Clifford algebras, we first demonstrate a Qudit Fermionic Mapping (QFM) that reduces the encoding cost associated with the qubit-based approach.","We then describe the unitary evolution of the mapped Hamiltonian by interpreting the resulting Majorana operators in terms of physical single- and two-qudit gates.","While the QFM can be used for any quantum hardware with four accessible energy levels, we demonstrate the specific reduction in overhead resulting from utilizing the native Controlled-SUM gate (equivalent to qubit CNOT) for a fixed-frequency ququart transmon.","We further transpile the resulting two transmon-qudit gates by demonstrating a qudit operator Schmidt decomposition using the Controlled-SUM gate.","Finally, we demonstrate the efficacy of our proposal by numerical simulation of local observables such as the filling factor and Green's function for various Trotter steps.","The compatibility of our approach with different qudit platforms paves the path for achieving quantum advantage in simulating non-trivial quantum many-body systems."],"url":"http://arxiv.org/abs/2402.01243v1","category":"quant-ph"}
{"created":"2024-02-02 09:03:17","title":"Topological Solitons in Su-Schrieffer-Heeger Chain with periodic hopping modulation, domain walls and disorder","abstract":"A chiral symmetric Su-Schrieffer-Heeger (SSH) chain features topological end states in one of its dimerized configurations. Those mid-gap zero energy states show interesting modifications upon a periodic tuning of the hopping modulations. Besides, more and more in-gap end modes appear at nonzero energies for further partitioning of the Brillouin zone due to increased hopping periodicity. The new topological phases are identified with a detailed analysis of the topological invariants namely, winding number and Zak phases. Spectra and topology of these systems with periodically modulated hopping are studied also in presence of a single static domain wall, separating two topologically inequivalent dimerized structures. Lastly, we also study the effect of disorder, particularly the chirality breaking onsite ones, on the edge and domain wall states. The end solitons and domain wall solitons show noteworthy evolutions with the variation of hopping periodicity. Our findings can add important feedback in utilizing topological phases in various fields including quantum computations and the results can be easily verified in a cold atom set up within optical lattices.","sentences":["A chiral symmetric Su-Schrieffer-Heeger (SSH) chain features topological end states in one of its dimerized configurations.","Those mid-gap zero energy states show interesting modifications upon a periodic tuning of the hopping modulations.","Besides, more and more in-gap end modes appear at nonzero energies for further partitioning of the Brillouin zone due to increased hopping periodicity.","The new topological phases are identified with a detailed analysis of the topological invariants namely, winding number and Zak phases.","Spectra and topology of these systems with periodically modulated hopping are studied also in presence of a single static domain wall, separating two topologically inequivalent dimerized structures.","Lastly, we also study the effect of disorder, particularly the chirality breaking onsite ones, on the edge and domain wall states.","The end solitons and domain wall solitons show noteworthy evolutions with the variation of hopping periodicity.","Our findings can add important feedback in utilizing topological phases in various fields including quantum computations and the results can be easily verified in a cold atom set up within optical lattices."],"url":"http://arxiv.org/abs/2402.01236v1","category":"cond-mat.str-el"}
{"created":"2024-02-02 08:58:28","title":"QSpeckleFilter: a Quantum Machine Learning approach for SAR speckle filtering","abstract":"The use of Synthetic Aperture Radar (SAR) has greatly advanced our capacity for comprehensive Earth monitoring, providing detailed insights into terrestrial surface use and cover regardless of weather conditions, and at any time of day or night. However, SAR imagery quality is often compromised by speckle, a granular disturbance that poses challenges in producing accurate results without suitable data processing. In this context, the present paper explores the cutting-edge application of Quantum Machine Learning (QML) in speckle filtering, harnessing quantum algorithms to address computational complexities. We introduce here QSpeckleFilter, a novel QML model for SAR speckle filtering. The proposed method compared to a previous work from the same authors showcases its superior performance in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) on a testing dataset, and it opens new avenues for Earth Observation (EO) applications.","sentences":["The use of Synthetic Aperture Radar (SAR) has greatly advanced our capacity for comprehensive Earth monitoring, providing detailed insights into terrestrial surface use and cover regardless of weather conditions, and at any time of day or night.","However, SAR imagery quality is often compromised by speckle, a granular disturbance that poses challenges in producing accurate results without suitable data processing.","In this context, the present paper explores the cutting-edge application of Quantum Machine Learning (QML) in speckle filtering, harnessing quantum algorithms to address computational complexities.","We introduce here QSpeckleFilter, a novel QML model for SAR speckle filtering.","The proposed method compared to a previous work from the same authors showcases its superior performance in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) on a testing dataset, and it opens new avenues for Earth Observation (EO) applications."],"url":"http://arxiv.org/abs/2402.01235v1","category":"eess.SP"}
{"created":"2024-02-02 08:55:25","title":"Scattering-Passive Structure-Preserving Finite Element Method for the Boundary Controlled Transport Equation with a Moving Mesh","abstract":"A structure-preserving Finite Element Method (FEM) for the transport equation in one- and two-dimensional domains is presented. This Distributed Parameter System (DPS) has non-collocated boundary control and observation, and reveals a scattering-energy preserving structure. We show that the discretized model preserves the aforementioned structure from the original infinite-dimensional system. Moreover, we analyse the case of moving meshes for the one-dimensional case. The moving mesh requires less states than the fixed one to produce solutions with a comparable accuracy, and it can also reduce the overshoot and oscillations of Gibbs phenomenon produced when using the FEM. Numerical simulations are provided for the case of a one-dimensional transport equation with fixed and moving meshes.","sentences":["A structure-preserving Finite Element Method (FEM) for the transport equation in one-","and two-dimensional domains is presented.","This Distributed Parameter System (DPS) has non-collocated boundary control and observation, and reveals a scattering-energy preserving structure.","We show that the discretized model preserves the aforementioned structure from the original infinite-dimensional system.","Moreover, we analyse the case of moving meshes for the one-dimensional case.","The moving mesh requires less states than the fixed one to produce solutions with a comparable accuracy, and it can also reduce the overshoot and oscillations of Gibbs phenomenon produced when using the FEM.","Numerical simulations are provided for the case of a one-dimensional transport equation with fixed and moving meshes."],"url":"http://arxiv.org/abs/2402.01232v1","category":"math.NA"}
{"created":"2024-02-02 08:52:19","title":"Strong solutions of mean-field FBSDEs and their applications to multi-population mean-field games","abstract":"We study the existence of strong solutions for mean-field forward-backward stochastic differential equations (FBSDEs) with measurable coefficients and their implication on the Nash equilibrium of a multi-population mean-field game. More specifically, we allow the coefficients to be discontinuous in the forward process and non-Lipschitz continuous concerning their time-sectional distribution. Using the Pontryagin stochastic maximum principle and the martingale approach, we apply our existence result to a multi-population mean-field game (MPMFG) model where the interacting agents in the system are grouped into multiple populations. Each population shares the same objective function, and we take changes in population sizes into consideration.","sentences":["We study the existence of strong solutions for mean-field forward-backward stochastic differential equations (FBSDEs) with measurable coefficients and their implication on the Nash equilibrium of a multi-population mean-field game.","More specifically, we allow the coefficients to be discontinuous in the forward process and non-Lipschitz continuous concerning their time-sectional distribution.","Using the Pontryagin stochastic maximum principle and the martingale approach, we apply our existence result to a multi-population mean-field game (MPMFG) model where the interacting agents in the system are grouped into multiple populations.","Each population shares the same objective function, and we take changes in population sizes into consideration."],"url":"http://arxiv.org/abs/2402.01229v1","category":"math.PR"}
{"created":"2024-02-02 08:44:57","title":"A new model of variable-length coupled pendulums: from hyperchaos to superintegrability","abstract":"This paper studies the dynamics and integrability of a variable-length coupled pendulum system. The complexity of the model is presented by joining various numerical methods, such as the Poincar\\'e cross-sections, phase-parametric diagrams, and Lyapunov exponents spectra. We show that the presented model is hyperchaotic, which ensures its nonintegrability. We gave analytical proof of this fact analyzing properties of the differential Galois group of variational equations along certain particular solutions of the system. We employ the Kovacic algorithm and its extension to dimension four to analyze the differential Galois group. Amazingly enough, in the absence of the gravitational potential and for certain values of the parameters, the system can exhibit chaotic, integrable, as well as superintegrable dynamics. To the best of our knowledge, this is the first attempt to use the method of Lyapunov exponents in the systematic search for the first integrals of the system. We show how to effectively apply the Lyapunov exponents as an indicator of integrable dynamics. The explicit forms of integrable and superintegrable systems are given.   The article has been published in Nonlinear Dynamics, and the final version is available at this link: https://doi.org/10.1007/s11071-023-09253-5","sentences":["This paper studies the dynamics and integrability of a variable-length coupled pendulum system.","The complexity of the model is presented by joining various numerical methods, such as the Poincar\\'e cross-sections, phase-parametric diagrams, and Lyapunov exponents spectra.","We show that the presented model is hyperchaotic, which ensures its nonintegrability.","We gave analytical proof of this fact analyzing properties of the differential Galois group of variational equations along certain particular solutions of the system.","We employ the Kovacic algorithm and its extension to dimension four to analyze the differential Galois group.","Amazingly enough, in the absence of the gravitational potential and for certain values of the parameters, the system can exhibit chaotic, integrable, as well as superintegrable dynamics.","To the best of our knowledge, this is the first attempt to use the method of Lyapunov exponents in the systematic search for the first integrals of the system.","We show how to effectively apply the Lyapunov exponents as an indicator of integrable dynamics.","The explicit forms of integrable and superintegrable systems are given.   ","The article has been published in Nonlinear Dynamics, and the final version is available at this link: https://doi.org/10.1007/s11071-023-09253-5"],"url":"http://arxiv.org/abs/2402.01224v1","category":"nlin.CD"}
{"created":"2024-02-02 08:43:39","title":"Feedback stabilization and observer design for sterile insect technique model","abstract":"This paper focuses on the feedback global stabilization and observer construction for a sterile insect technique model. The Sterile Insect Technique (SIT) is one of the most ecological methods for controlling insect pests responsible for worldwide crop destruction and disease transmission. In this work, we construct a feedback law that globally asymptotically stabilizes a SIT model at extinction equilibrium. Since the application of this type of control requires the measurement of different states of the target insect population, and in practice, some states are more difficult and very expensive to measure than others, it is important to know how to construct a state estimator which from a few measured states, estimates the other ones as the one we build in the second part of our work. In the last part of our work, we show that we can apply the feedback control with estimated states to stabilize the full system.","sentences":["This paper focuses on the feedback global stabilization and observer construction for a sterile insect technique model.","The Sterile Insect Technique (SIT) is one of the most ecological methods for controlling insect pests responsible for worldwide crop destruction and disease transmission.","In this work, we construct a feedback law that globally asymptotically stabilizes a SIT model at extinction equilibrium.","Since the application of this type of control requires the measurement of different states of the target insect population, and in practice, some states are more difficult and very expensive to measure than others, it is important to know how to construct a state estimator which from a few measured states, estimates the other ones as the one we build in the second part of our work.","In the last part of our work, we show that we can apply the feedback control with estimated states to stabilize the full system."],"url":"http://arxiv.org/abs/2402.01221v1","category":"math.DS"}
{"created":"2024-02-02 08:21:17","title":"The O2 software framework and GPU usage in ALICE online and offline reconstruction in Run 3","abstract":"ALICE has upgraded many of its detectors for LHC Run 3 to operate in continuous readout mode recording Pb--Pb collisions at 50 kHz interaction rate without trigger. This results in the need to process data in real time at rates 100 times higher than during Run 2. In order to tackle such a challenge we introduced O2, a new computing system and the associated infrastructure. Designed and implemented during the LHC long shutdown 2, O2 is now in production taking care of all the data processing needs of the experiment. O2 is designed around the message passing paradigm, enabling resilient, parallel data processing for both the synchronous (to LHC beam) and asynchronous data taking and processing phases. The main purpose of the synchronous online reconstruction is detector calibration and raw data compression. This synchronous processing is dominated by the TPC detector, which produces by far the largest data volume, and TPC reconstruction runs fully on GPUs. When there is no beam in the LHC, the powerful GPU-equipped online computing farm of ALICE is used for the asynchronous reconstruction, which creates the final reconstructed output for analysis from the compressed raw data. Since the majority of the compute performance of the online farm is in the GPUs, and since the asynchronous processing is not dominated by the TPC in the way the synchronous processing is, there is an ongoing effort to offload a significant amount of compute load from other detectors to the GPU as well.","sentences":["ALICE has upgraded many of its detectors for LHC Run 3 to operate in continuous readout mode recording Pb--Pb collisions at 50 kHz interaction rate without trigger.","This results in the need to process data in real time at rates 100 times higher than during Run 2.","In order to tackle such a challenge we introduced O2, a new computing system and the associated infrastructure.","Designed and implemented during the LHC long shutdown 2, O2 is now in production taking care of all the data processing needs of the experiment.","O2 is designed around the message passing paradigm, enabling resilient, parallel data processing for both the synchronous (to LHC beam) and asynchronous data taking and processing phases.","The main purpose of the synchronous online reconstruction is detector calibration and raw data compression.","This synchronous processing is dominated by the TPC detector, which produces by far the largest data volume, and TPC reconstruction runs fully on GPUs.","When there is no beam in the LHC, the powerful GPU-equipped online computing farm of ALICE is used for the asynchronous reconstruction, which creates the final reconstructed output for analysis from the compressed raw data.","Since the majority of the compute performance of the online farm is in the GPUs, and since the asynchronous processing is not dominated by the TPC in the way the synchronous processing is, there is an ongoing effort to offload a significant amount of compute load from other detectors to the GPU as well."],"url":"http://arxiv.org/abs/2402.01205v1","category":"physics.ins-det"}
{"created":"2024-02-02 18:54:18","title":"kNN Algorithm for Conditional Mean and Variance Estimation with Automated Uncertainty Quantification and Variable Selection","abstract":"In this paper, we introduce a kNN-based regression method that synergizes the scalability and adaptability of traditional non-parametric kNN models with a novel variable selection technique. This method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.Our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance. The employment of kNN ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates. Additionally, we introduce a new kNN semi-parametric algorithm for estimating ROC curves, accounting for covariates. For selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.Incorporation of variable selection enhances the performance of the method significantly over conventional kNN techniques in various modeling tasks. We validate the approach through simulations in low, moderate, and high-dimensional covariate spaces. The algorithm's effectiveness is particularly notable in biomedical applications as demonstrated in two case studies. Concluding with a theoretical analysis, we highlight the consistency and convergence rate of our method over traditional kNN models, particularly when the underlying regression model takes values in a low-dimensional space.","sentences":["In this paper, we introduce a kNN-based regression method that synergizes the scalability and adaptability of traditional non-parametric kNN models with a novel variable selection technique.","This method focuses on accurately estimating the conditional mean and variance of random response variables, thereby effectively characterizing conditional distributions across diverse scenarios.","Our approach incorporates a robust uncertainty quantification mechanism, leveraging our prior estimation work on conditional mean and variance.","The employment of kNN ensures scalable computational efficiency in predicting intervals and statistical accuracy in line with optimal non-parametric rates.","Additionally, we introduce a new kNN semi-parametric algorithm for estimating ROC curves, accounting for covariates.","For selecting the smoothing parameter k, we propose an algorithm with theoretical guarantees.","Incorporation of variable selection enhances the performance of the method significantly over conventional kNN techniques in various modeling tasks.","We validate the approach through simulations in low, moderate, and high-dimensional covariate spaces.","The algorithm's effectiveness is particularly notable in biomedical applications as demonstrated in two case studies.","Concluding with a theoretical analysis, we highlight the consistency and convergence rate of our method over traditional kNN models, particularly when the underlying regression model takes values in a low-dimensional space."],"url":"http://arxiv.org/abs/2402.01635v1","category":"stat.ME"}
{"created":"2024-02-02 18:52:16","title":"Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type","abstract":"Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparameters of arbitrary form, and which supports both Bayesian and frequentist settings. Our proof idea is novel and can easily be extended to other variants of Bayesian optimisation. We show this by extending our algorithm to the adversarially robust optimisation setting under unknown hyperparameters. Finally, we empirically evaluate our algorithm on a set of toy problems and show that it can outperform the maximum likelihood estimator.","sentences":["Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known.","The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation.","Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging.","Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case.","We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparameters of arbitrary form, and which supports both Bayesian and frequentist settings.","Our proof idea is novel and can easily be extended to other variants of Bayesian optimisation.","We show this by extending our algorithm to the adversarially robust optimisation setting under unknown hyperparameters.","Finally, we empirically evaluate our algorithm on a set of toy problems and show that it can outperform the maximum likelihood estimator."],"url":"http://arxiv.org/abs/2402.01632v1","category":"cs.LG"}
{"created":"2024-02-02 18:19:30","title":"Duality for weak $\u03c9$-categories","abstract":"We define inductively the opposites of a weak globular $\\omega$-category with respect to a set of dimensions, and we show that the properties of being free on a globular set of a computad are preserved under forming opposites. We then provide a new description of hom $\\omega$-categories, and show that the opposites of a hom $\\omega$-category are hom $\\omega$-category of opposites of the original $\\omega$-category.","sentences":["We define inductively the opposites of a weak globular $\\omega$-category with respect to a set of dimensions, and we show that the properties of being free on a globular set of a computad are preserved under forming opposites.","We then provide a new description of hom $\\omega$-categories, and show that the opposites of a hom $\\omega$-category are hom $\\omega$-category of opposites of the original $\\omega$-category."],"url":"http://arxiv.org/abs/2402.01611v1","category":"math.CT"}
{"created":"2024-02-02 17:44:20","title":"$L^\\infty$ blow-up in the Jordan-Moore-Gibson-Thompson equation","abstract":"The Jordan-Moore-Gibson-Thompson equation   \\[   \\tau u_{ttt} + \\alpha u_{tt} = \\beta \\Delta u_t + \\gamma \\Delta u + (f(u))_{tt} \\] is considered in a smoothly bounded domain $\\Omega \\subset\\mathbb{R}^n$ with $n\\leq 3$, where $\\tau>0,\\beta>0,\\gamma>0$, and $\\alpha\\in\\mathbb{R}$.   Firstly, it is seen that under the assumption that $f\\in C^3([0,\\infty))$ is such that $f(0)=0$, gradient blow-up phenomena cannot occur in the sense that for any appropriately regular initial data, within a suitable framework of strong solvability, an associated Dirichlet type initial-boundary value problem admits a unique solution $u$ on a maximal time interval $(0,T_{max})$ which is such that \\[   \\mbox{if $T_{max}<\\infty$, then } \\limsup_{t\\nearrow T_{max}} \\|u(\\cdot,t)\\|_{L^\\infty(\\Omega)}=\\infty. \\] This is used to, secondly, make sure that if additionally $f$ is convex and grows superlinearly in the sense that \\[   f''\\ge 0   \\mbox{ on $\\mathbb{R}$,}   \\qquad   \\frac{f(\\xi)}{\\xi} \\to +\\infty   \\mbox{ as $\\xi\\to +\\infty$}   \\qquad \\mbox{and} \\qquad   \\int_{\\xi_0}^\\infty \\frac{d\\xi}{f(\\xi)} < \\infty   \\mbox{ for some $\\xi_0>0$,} \\] then for some initial data the above solution must undergo some finite-time $L^\\infty$ blow-up in the style described above.","sentences":["The Jordan-Moore-Gibson-Thompson equation   \\[   \\tau u_{ttt} + \\alpha u_{tt} = \\beta \\Delta u_t","+ \\gamma \\Delta u + (f(u))_{tt} \\] is considered in a smoothly bounded domain $\\Omega \\subset\\mathbb{R}^n$ with $n\\leq 3$, where $\\tau>0,\\beta>0,\\gamma>0$, and $\\alpha\\in\\mathbb{R}$.   Firstly, it is seen that under the assumption that $f\\in C^3([0,\\infty))$ is such that $f(0)=0$, gradient blow-up phenomena cannot occur in the sense that for any appropriately regular initial data, within a suitable framework of strong solvability, an associated Dirichlet type initial-boundary value problem admits a unique solution $u$ on a maximal time interval $(0,T_{max})$ which is such that \\[   \\mbox{if $T_{max}<\\infty$, then } \\limsup_{t\\nearrow T_{max}} \\|u(\\cdot,t)\\|_{L^\\infty(\\Omega)}=\\infty.","\\]","This is used to, secondly, make sure that if additionally $f$ is convex and grows superlinearly in the sense that \\[   f''\\ge 0   \\mbox{ on $\\mathbb{R}$,}   \\qquad   \\frac{f(\\xi)}{\\xi} \\to +\\infty   \\mbox{ as $\\xi\\to +\\infty$}   \\qquad \\mbox{and} \\qquad   \\int_{\\xi_0}^\\infty \\frac{d\\xi}{f(\\xi)}","< \\infty   \\mbox{ for some $\\xi_0>0$,} \\]","then for some initial data the above solution must undergo some finite-time $L^\\infty$ blow-up in the style described above."],"url":"http://arxiv.org/abs/2402.01595v1","category":"math.AP"}
{"created":"2024-02-02 17:42:02","title":"Dark resonance spectra of trapped ions under the influence of micromotion","abstract":"We study the influence of micromotion on the spectrum of trapped ions with a lambda-type level scheme, leading to dark resonances due to coherent population trapping. We work with calcium ions trapped in a ring-shaped Paul trap, in which one can compensate excess micromotion for only one ion of the crystal. We observe that micromotion affects the shapes of the dark resonances and causes the appearance of \"echoes\" separated by intervals given by the drive frequency. We present a theoretical model that provides good fits to the measurements and can be used to estimate the amplitude of the micromotion modulation of the atomic motion. We estimate an effective temperature of the ions from the spectra and observe clear micromotion heating as well as impaired cooling for sufficiently large excess micromotion.","sentences":["We study the influence of micromotion on the spectrum of trapped ions with a lambda-type level scheme, leading to dark resonances due to coherent population trapping.","We work with calcium ions trapped in a ring-shaped Paul trap, in which one can compensate excess micromotion for only one ion of the crystal.","We observe that micromotion affects the shapes of the dark resonances and causes the appearance of \"echoes\" separated by intervals given by the drive frequency.","We present a theoretical model that provides good fits to the measurements and can be used to estimate the amplitude of the micromotion modulation of the atomic motion.","We estimate an effective temperature of the ions from the spectra and observe clear micromotion heating as well as impaired cooling for sufficiently large excess micromotion."],"url":"http://arxiv.org/abs/2402.01594v1","category":"quant-ph"}
{"created":"2024-02-02 17:17:30","title":"Predictive Models based on Deep Learning Algorithms for Tensile Deformation of AlCoCuCrFeNi High-entropy alloy","abstract":"High-entropy alloys (HEAs) stand out between multi-component alloys due to their attractive microstructures and mechanical properties. In this investigation, molecular dynamics (MD) simulation and machine learning were used to ascertain the deformation mechanism of AlCoCuCrFeNi HEAs under the influence of temperature, strain rate, and grain sizes. First, the MD simulation shows that the yield stress decreases significantly as the strain and temperature increase. In other cases, changes in strain rate and grain size have less effect on mechanical properties than changes in strain and temperature. The alloys exhibited superplastic behavior under all test conditions. The deformity mechanism discloses that strain and temperature are the main sources of beginning strain, and the shear bands move along the uniaxial tensile axis inside the workpiece. Furthermore, the fast phase shift of inclusion under mild strain indicates the relative instability of the inclusion phase of HCP. Ultimately, the dislocation evolution mechanism shows that the dislocations are transported to free surfaces under increased strain when they nucleate around the grain boundary. Surprisingly, the ML prediction results also confirm the same characteristics as those confirmed from the MD simulation. Hence, the combination of MD and ML reinforces the confidence in the findings of mechanical characteristics of HEA. Consequently, this combination fills the gaps between MD and ML, which can significantly save time human power and cost to conduct real experiments for testing HEA deformation in practice.","sentences":["High-entropy alloys (HEAs) stand out between multi-component alloys due to their attractive microstructures and mechanical properties.","In this investigation, molecular dynamics (MD) simulation and machine learning were used to ascertain the deformation mechanism of AlCoCuCrFeNi HEAs under the influence of temperature, strain rate, and grain sizes.","First, the MD simulation shows that the yield stress decreases significantly as the strain and temperature increase.","In other cases, changes in strain rate and grain size have less effect on mechanical properties than changes in strain and temperature.","The alloys exhibited superplastic behavior under all test conditions.","The deformity mechanism discloses that strain and temperature are the main sources of beginning strain, and the shear bands move along the uniaxial tensile axis inside the workpiece.","Furthermore, the fast phase shift of inclusion under mild strain indicates the relative instability of the inclusion phase of HCP.","Ultimately, the dislocation evolution mechanism shows that the dislocations are transported to free surfaces under increased strain when they nucleate around the grain boundary.","Surprisingly, the ML prediction results also confirm the same characteristics as those confirmed from the MD simulation.","Hence, the combination of MD and ML reinforces the confidence in the findings of mechanical characteristics of HEA.","Consequently, this combination fills the gaps between MD and ML, which can significantly save time human power and cost to conduct real experiments for testing HEA deformation in practice."],"url":"http://arxiv.org/abs/2402.01578v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 16:43:32","title":"Mapping Acceptance: Assessing Emerging Technologies and Concepts through Micro Scenarios","abstract":"As technology evolves rapidly, understanding public perception becomes increasingly crucial. This article introduces an integrative method for evaluating mental models and social acceptance of various technologies. Our approach utilizes micro scenarios coupled with visual-spatial mapping, offering a comprehensive perspective that contrasts with traditional methods focused on detailed assessments of limited scenarios. This methodology allows for simultaneous quantitative evaluation of multiple technologies on visio-spatial maps, facilitating a comparative ranking based on diverse criteria and an exploration of the interplay between individual factors and technology attributes in shaping public opinion. Our approach provides a framework for researchers and policymakers to gauge critical issues and to identify factors pivotal to acceptance. We illustrate this methodology with examples from our research, offering practical guidelines and R code to enable others in conducting similar studies. This paper aims to bridge the gap between technological advancement and societal perception, offering a tool for more informed decision-making in the realm of technology development and policy.","sentences":["As technology evolves rapidly, understanding public perception becomes increasingly crucial.","This article introduces an integrative method for evaluating mental models and social acceptance of various technologies.","Our approach utilizes micro scenarios coupled with visual-spatial mapping, offering a comprehensive perspective that contrasts with traditional methods focused on detailed assessments of limited scenarios.","This methodology allows for simultaneous quantitative evaluation of multiple technologies on visio-spatial maps, facilitating a comparative ranking based on diverse criteria and an exploration of the interplay between individual factors and technology attributes in shaping public opinion.","Our approach provides a framework for researchers and policymakers to gauge critical issues and to identify factors pivotal to acceptance.","We illustrate this methodology with examples from our research, offering practical guidelines and R code to enable others in conducting similar studies.","This paper aims to bridge the gap between technological advancement and societal perception, offering a tool for more informed decision-making in the realm of technology development and policy."],"url":"http://arxiv.org/abs/2402.01551v1","category":"cs.CY"}
{"created":"2024-02-02 16:30:14","title":"MCXC-II: Second release of the Meta-Catalogue of X-Ray detected Clusters of galaxies","abstract":"We present the second release of the Meta-catalogue of X-Ray detected Clusters of galaxies (hereafter MCXC-II). The MCXC-II has been compiled from publicly available ROSAT All Sky Survey-based (NORAS, REFLEX, BCS, SGP, NEP, MACS, CIZA, RXGCC) and serendipitous (160SD, 400SD, SHARC, WARPS, and EMSS) cluster catalogues. Redshifts were systematically checked and updated if necessary, and additional redshift information (type and origin) added. The X-ray data were standardised to an overdensity of 500 using a new procedure based on the use of the original flux and aperture measurements available in the input catalogues. MCXC-II contains 2221 entries, essentially completing the census of ROSAT cluster detections by including objects from the REFLEX-II and RXGCC surveys, in addition to providing a complete and fully-homogenised sub-catalogue of all published MACS clusters. Duplicate entries from overlaps between the survey areas of the individual input catalogues were carefully handled. For each cluster the MCXC-II provides three identifiers, a redshift, coordinates, membership in the original catalogue, and standardised [0.1-2.4] keV band luminosity $L_{500}$, total mass $M_{500}$, and radius $R_{500}$. Uncertainties on $L_{500}$ were computed from the flux errors in the original catalogues. MCXC-II additionally furnishes information on overlaps between the input catalogues, gives the luminosity and its uncertainty when measurements from different surveys are available, and provides notes on individual objects.","sentences":["We present the second release of the Meta-catalogue of X-Ray detected Clusters of galaxies (hereafter MCXC-II).","The MCXC-II has been compiled from publicly available ROSAT All Sky Survey-based (NORAS, REFLEX, BCS, SGP, NEP, MACS, CIZA, RXGCC) and serendipitous (160SD, 400SD, SHARC, WARPS, and EMSS) cluster catalogues.","Redshifts were systematically checked and updated if necessary, and additional redshift information (type and origin) added.","The X-ray data were standardised to an overdensity of 500 using a new procedure based on the use of the original flux and aperture measurements available in the input catalogues.","MCXC-II contains 2221 entries, essentially completing the census of ROSAT cluster detections by including objects from the REFLEX-II and RXGCC surveys, in addition to providing a complete and fully-homogenised sub-catalogue of all published MACS clusters.","Duplicate entries from overlaps between the survey areas of the individual input catalogues were carefully handled.","For each cluster the MCXC-II provides three identifiers, a redshift, coordinates, membership in the original catalogue, and standardised [0.1-2.4] keV band luminosity $L_{500}$, total mass $M_{500}$, and radius $R_{500}$. Uncertainties on $L_{500}$ were computed from the flux errors in the original catalogues.","MCXC-II additionally furnishes information on overlaps between the input catalogues, gives the luminosity and its uncertainty when measurements from different surveys are available, and provides notes on individual objects."],"url":"http://arxiv.org/abs/2402.01538v1","category":"astro-ph.CO"}
{"created":"2024-02-02 16:24:48","title":"Direct cross-section measurement of the weak r-process 88Sr(\u03b1,n)91Zr reaction in \u03bd-driven winds of core collapse supernovae","abstract":"About half of the heavy elements beyond iron are known to be produced by the rapid neutron capture process, known as r-process. However, the astrophysical site producing the r-process is still uncertain. Chemical abundances observed in several cosmic sites indicate that different mechanisms should be at play. For instance, the abundances around silver measured in a subset of metal-poor stars indicate the presence of a weak r-process. This process may be active in neutrino-driven winds of core collapse supernovae where (${\\alpha}$,n) reactions dominate the synthesis of Z ~ 40 elements in the expelled materials. Scarcely measured, the rates of (${\\alpha}$,n) reactions are determined from statistical Hauser-Feshbach calculations with ${\\alpha}$-optical-model potentials, which are still poorly constrained. The uncertainties of the (${\\alpha}$,n) reaction rates therefore make a significant contribution to the uncertainties of the abundances determined from stellar modeling. In this work, the $^{88}$Sr(${\\alpha}$,n)$^{91}$Zr reaction which impacts the weak r-process abundances has been probed at astrophysics energy for the first time; directly measuring the total cross sections at astrophysical energies of 8.37 - 13.09 MeV in the center of mass (3.8 - 7.5 GK). Two measurements were performed at ATLAS with the electrically-segmented ionization chamber MUSIC, in inverse kinematics, while following the active target technique. The cross sections of this ${\\alpha}$-induced reaction on $^{88}$Sr, located at the shell closure N = 50, have been found to be lower than expected, by a factor of 3, despite recent statistical calculations validated by measurements on neighboring nuclei. This result encourages more experimental investigations of (${\\alpha}$,n) reactions, at N = 50 and towards the neutron-rich side, to further test the predictive power and reliability of such calculations.","sentences":["About half of the heavy elements beyond iron are known to be produced by the rapid neutron capture process, known as r-process.","However, the astrophysical site producing the r-process is still uncertain.","Chemical abundances observed in several cosmic sites indicate that different mechanisms should be at play.","For instance, the abundances around silver measured in a subset of metal-poor stars indicate the presence of a weak r-process.","This process may be active in neutrino-driven winds of core collapse supernovae where (${\\alpha}$,n) reactions dominate the synthesis of Z ~ 40 elements in the expelled materials.","Scarcely measured, the rates of (${\\alpha}$,n) reactions are determined from statistical Hauser-Feshbach calculations with ${\\alpha}$-optical-model potentials, which are still poorly constrained.","The uncertainties of the (${\\alpha}$,n) reaction rates therefore make a significant contribution to the uncertainties of the abundances determined from stellar modeling.","In this work, the $^{88}$Sr(${\\alpha}$,n)$^{91}$Zr reaction which impacts the weak r-process abundances has been probed at astrophysics energy for the first time; directly measuring the total cross sections at astrophysical energies of 8.37 - 13.09 MeV in the center of mass (3.8 - 7.5 GK).","Two measurements were performed at ATLAS with the electrically-segmented ionization chamber MUSIC, in inverse kinematics, while following the active target technique.","The cross sections of this ${\\alpha}$-induced reaction on $^{88}$Sr, located at the shell closure N = 50, have been found to be lower than expected, by a factor of 3, despite recent statistical calculations validated by measurements on neighboring nuclei.","This result encourages more experimental investigations of (${\\alpha}$,n) reactions, at N = 50 and towards the neutron-rich side, to further test the predictive power and reliability of such calculations."],"url":"http://arxiv.org/abs/2402.01534v1","category":"nucl-ex"}
{"created":"2024-02-02 16:03:47","title":"Achieving a combined 15 microns and 60 ps test beam resolution using an RSD 450 microns pitch pixel matrix connected to a FAST2 ASIC","abstract":"This paper reports on the spatial and temporal resolutions of an RSD 450 microns pitch pixels array measured at the DESY test beam facility. RSDs, Resistive Silicon Detectors, also known as AC-LGAD, achieve excellent position and temporal resolution by exploiting charge sharing among neighboring electrodes. The RSD matrix used in this study is part of the second FBK RSD production, RSD2, composed of 450-micron pitch pixels with cross-shaped electrodes. A 7-pixel matrix was read out by the FAST2 ASIC, a 16-channel amplifier fully custom ASIC developed by INFN Torino using the 110 nm CMOS technology. The total area covered by the matrix is about 1.5 mm2. The position resolution reached in this test is 15 microns, about 3.4% of the pitch. The temporal resolution achieved in this work is 60 ps, dominated by the FAST2 resolution. The work also demonstrates that RSD sensors achieve 100% fill factor and homogenous resolutions over the whole matrix surface, making them suitable for 4D tracking applications.","sentences":["This paper reports on the spatial and temporal resolutions of an RSD 450 microns pitch pixels array measured at the DESY test beam facility.","RSDs, Resistive Silicon Detectors, also known as AC-LGAD, achieve excellent position and temporal resolution by exploiting charge sharing among neighboring electrodes.","The RSD matrix used in this study is part of the second FBK RSD production, RSD2, composed of 450-micron pitch pixels with cross-shaped electrodes.","A 7-pixel matrix was read out by the FAST2 ASIC, a 16-channel amplifier fully custom ASIC developed by INFN Torino using the 110 nm CMOS technology.","The total area covered by the matrix is about 1.5 mm2.","The position resolution reached in this test is 15 microns, about 3.4% of the pitch.","The temporal resolution achieved in this work is 60 ps, dominated by the FAST2 resolution.","The work also demonstrates that RSD sensors achieve 100% fill factor and homogenous resolutions over the whole matrix surface, making them suitable for 4D tracking applications."],"url":"http://arxiv.org/abs/2402.01517v1","category":"physics.ins-det"}
{"created":"2024-02-02 15:40:52","title":"Overcoming Blind Spots: Occlusion Considerations for Improved Autonomous Driving Safety","abstract":"Our work introduces a module for assessing the trajectory safety of autonomous vehicles in dynamic environments marked by high uncertainty. We focus on occluded areas and occluded traffic participants with limited information about surrounding obstacles. To address this problem, we propose a software module that handles blind spots (BS) created by static and dynamic obstacles in urban environments. We identify potential occluded traffic participants, predict their movement, and assess the ego vehicle's trajectory using various criticality metrics. The method offers a straightforward and modular integration into motion planner algorithms. We present critical real-world scenarios to evaluate our module and apply our approach to a publicly available trajectory planning algorithm. Our results demonstrate that safe yet efficient driving with occluded road users can be achieved by incorporating safety assessments into the planning process. The code used in this research is publicly available as open-source software and can be accessed at the following link: https://github.com/TUM-AVS/Frenetix-Occlusion.","sentences":["Our work introduces a module for assessing the trajectory safety of autonomous vehicles in dynamic environments marked by high uncertainty.","We focus on occluded areas and occluded traffic participants with limited information about surrounding obstacles.","To address this problem, we propose a software module that handles blind spots (BS) created by static and dynamic obstacles in urban environments.","We identify potential occluded traffic participants, predict their movement, and assess the ego vehicle's trajectory using various criticality metrics.","The method offers a straightforward and modular integration into motion planner algorithms.","We present critical real-world scenarios to evaluate our module and apply our approach to a publicly available trajectory planning algorithm.","Our results demonstrate that safe yet efficient driving with occluded road users can be achieved by incorporating safety assessments into the planning process.","The code used in this research is publicly available as open-source software and can be accessed at the following link: https://github.com/TUM-AVS/Frenetix-Occlusion."],"url":"http://arxiv.org/abs/2402.01507v1","category":"cs.RO"}
{"created":"2024-02-02 15:38:47","title":"Code-Switched Language Identification is Harder Than You Think","abstract":"Code switching (CS) is a very common phenomenon in written and spoken communication but one that is handled poorly by many natural language processing applications. Looking to the application of building CS corpora, we explore CS language identification (LID) for corpus building. We make the task more realistic by scaling it to more languages and considering models with simpler architectures for faster inference. We also reformulate the task as a sentence-level multi-label tagging problem to make it more tractable. Having defined the task, we investigate three reasonable models for this task and define metrics which better reflect desired performance. We present empirical evidence that no current approach is adequate and finally provide recommendations for future work in this area.","sentences":["Code switching (CS) is a very common phenomenon in written and spoken communication but one that is handled poorly by many natural language processing applications.","Looking to the application of building CS corpora, we explore CS language identification (LID) for corpus building.","We make the task more realistic by scaling it to more languages and considering models with simpler architectures for faster inference.","We also reformulate the task as a sentence-level multi-label tagging problem to make it more tractable.","Having defined the task, we investigate three reasonable models for this task and define metrics which better reflect desired performance.","We present empirical evidence that no current approach is adequate and finally provide recommendations for future work in this area."],"url":"http://arxiv.org/abs/2402.01505v1","category":"cs.CL"}
{"created":"2024-02-02 15:15:53","title":"Conformal Inverse Optimization","abstract":"Inverse optimization has been increasingly used to estimate unknown parameters in an optimization model based on decision data. We show that such a point estimation is insufficient in a prescriptive setting where the estimated parameters are used to prescribe new decisions. The prescribed decisions may be low-quality and misaligned with human intuition and thus are unlikely to be adopted. To tackle this challenge, we propose conformal inverse optimization, which seeks to learn an uncertainty set for the unknown parameters and then solve a robust optimization model to prescribe new decisions. Under mild assumptions, we show that the suggested decisions can achieve bounded out-of-sample optimality gaps, as evaluated using both the ground-truth parameters and the decision maker's perception of the unknown parameters. Our method demonstrates strong empirical performance compared to classic inverse optimization.","sentences":["Inverse optimization has been increasingly used to estimate unknown parameters in an optimization model based on decision data.","We show that such a point estimation is insufficient in a prescriptive setting where the estimated parameters are used to prescribe new decisions.","The prescribed decisions may be low-quality and misaligned with human intuition and thus are unlikely to be adopted.","To tackle this challenge, we propose conformal inverse optimization, which seeks to learn an uncertainty set for the unknown parameters and then solve a robust optimization model to prescribe new decisions.","Under mild assumptions, we show that the suggested decisions can achieve bounded out-of-sample optimality gaps, as evaluated using both the ground-truth parameters and the decision maker's perception of the unknown parameters.","Our method demonstrates strong empirical performance compared to classic inverse optimization."],"url":"http://arxiv.org/abs/2402.01489v1","category":"math.OC"}
{"created":"2024-02-02 15:12:16","title":"Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?","abstract":"A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a Bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification.","sentences":["A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks' parameter space.","Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem.","Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis.","As a result, we present a Bayesian deep ensemble approach as an effective solution with competitive performance and uncertainty quantification."],"url":"http://arxiv.org/abs/2402.01484v1","category":"cs.LG"}
{"created":"2024-02-02 15:11:13","title":"Combinatorics of rectangulations: Old and new bijections","abstract":"A rectangulation is a decomposition of a rectangle into finitely many rectangles. Via natural equivalence relations, rectangulations can be seen as combinatorial objects with a rich structure, with links to lattice congruences, flip graphs, polytopes, lattice paths, Hopf algebras, etc. In this paper, we first revisit the structure of the respective equivalence classes: weak rectangulations that preserve rectangle-segment adjacencies, and strong rectangulations that preserve rectangle-rectangle adjacencies. We thoroughly investigate posets defined by adjacency in rectangulations of both kinds, and unify and simplify known bijections between rectangulations and permutation classes. This yields a uniform treatment of mappings between permutations and rectangulations that unifies the results from earlier contributions, and emphasizes parallelism and differences between the weak and the strong cases. Then, we consider the special case of guillotine rectangulations, and prove that they can be characterized - under all known mappings between permutations and rectangulations - by avoidance of two mesh patterns that correspond to \"windmills\" in rectangulations. This yields new permutation classes in bijection with weak guillotine rectangulations, and the first known permutation class in bijection with strong guillotine rectangulations. Finally, we address enumerative issues and prove asymptotic bounds for several families of strong rectangulations.","sentences":["A rectangulation is a decomposition of a rectangle into finitely many rectangles.","Via natural equivalence relations, rectangulations can be seen as combinatorial objects with a rich structure, with links to lattice congruences, flip graphs, polytopes, lattice paths, Hopf algebras, etc.","In this paper, we first revisit the structure of the respective equivalence classes: weak rectangulations that preserve rectangle-segment adjacencies, and strong rectangulations that preserve rectangle-rectangle adjacencies.","We thoroughly investigate posets defined by adjacency in rectangulations of both kinds, and unify and simplify known bijections between rectangulations and permutation classes.","This yields a uniform treatment of mappings between permutations and rectangulations that unifies the results from earlier contributions, and emphasizes parallelism and differences between the weak and the strong cases.","Then, we consider the special case of guillotine rectangulations, and prove that they can be characterized - under all known mappings between permutations and rectangulations - by avoidance of two mesh patterns that correspond to \"windmills\" in rectangulations.","This yields new permutation classes in bijection with weak guillotine rectangulations, and the first known permutation class in bijection with strong guillotine rectangulations.","Finally, we address enumerative issues and prove asymptotic bounds for several families of strong rectangulations."],"url":"http://arxiv.org/abs/2402.01483v1","category":"math.CO"}
{"created":"2024-02-02 14:56:33","title":"A momentum-resolved view of polaron formation in materials","abstract":"A combined experimental and computational methodology for interrogating the phonon contribution to polaron formation in real materials is developed. Using LiF as an example, we show that the recent ab-initio theory of Sio et. al [PRL 122, 246403 (2019)] makes predictions of the momentum- and branch dependent phonon amplitides in polaron quasiparticles that are testable using ultrafast electron diffuse scattering (UEDS) and related techniques. The large electron polaron in LiF has UEDS signatures that are qualitatively similar to those expected from a simple point-defect model, but the small hole polaron exhibits a profoundly anisotropic UEDS pattern that is in poor agreement with a point-defect model. We also show that these polaron diffuse scattering signatures are directly emblematic of the underlying polaron wavefunction. The combination of new time and momentum resolved experimental probes of nonequilibrium phonons with novel computational methods promises to complement the qualitative results obtained via model Hamiltonians with a first principles, material-specific quantiative understanding of polarons and their properties.","sentences":["A combined experimental and computational methodology for interrogating the phonon contribution to polaron formation in real materials is developed.","Using LiF as an example, we show that the recent ab-initio theory of Sio et.","al","[PRL 122, 246403 (2019)] makes predictions of the momentum- and branch dependent phonon amplitides in polaron quasiparticles that are testable using ultrafast electron diffuse scattering (UEDS) and related techniques.","The large electron polaron in LiF has UEDS signatures that are qualitatively similar to those expected from a simple point-defect model, but the small hole polaron exhibits a profoundly anisotropic UEDS pattern that is in poor agreement with a point-defect model.","We also show that these polaron diffuse scattering signatures are directly emblematic of the underlying polaron wavefunction.","The combination of new time and momentum resolved experimental probes of nonequilibrium phonons with novel computational methods promises to complement the qualitative results obtained via model Hamiltonians with a first principles, material-specific quantiative understanding of polarons and their properties."],"url":"http://arxiv.org/abs/2402.01468v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 14:43:49","title":"Self-Correlations of Hurwitz Class Numbers","abstract":"The asymptotic study of class numbers of binary quadratic forms is a foundational problem in arithmetic statistics. Here, we investigate finer statistics of class numbers by studying their self-correlations under additive shifts. Specifically, we produce uniform asymptotics for the shifted convolution sum $\\sum_{n < X} H(n) H(n+\\ell)$ for fixed $\\ell \\in \\mathbb{Z}$, in which $H(n)$ denotes the Hurwitz class number.","sentences":["The asymptotic study of class numbers of binary quadratic forms is a foundational problem in arithmetic statistics.","Here, we investigate finer statistics of class numbers by studying their self-correlations under additive shifts.","Specifically, we produce uniform asymptotics for the shifted convolution sum $\\sum_{n < X} H(n) H(n+\\ell)$ for fixed $\\ell \\in \\mathbb{Z}$, in which $H(n)$ denotes the Hurwitz class number."],"url":"http://arxiv.org/abs/2402.01455v1","category":"math.NT"}
{"created":"2024-02-02 14:40:48","title":"Almost co-K\u00e4hler manifolds and $(m,\u03c1)$-quasi-Einstein solitons","abstract":"The present paper aims to investigate $(m,\\rho)$-quasi-Einstein metrices on almost co-K\\\"ahler manifolds $\\mathcal{M}$. It is proven that if a $(\\kappa,\\mu)$-almost co-K\\\"ahler manifold with $\\kappa<0$ is $(m,\\rho)$-quasi-Einstein manifold, then $\\mathcal{M}$ represents a $N(\\kappa)$-almost co-K\\\"ahler manifold and the manifold is locally isomorphic to a solvable non-nilpotent Lie group. Next, we study the three dimensional case and get the above mentioned result along with the manifold $\\mathcal{M}^3$ becoming an $\\eta$-Einstein manifold. We also show that there does not exist $(m,\\rho)$-quasi-Einstein structure on a compact $(\\kappa,\\mu)$-almost co-K\\\"ahler manifold of dimension greater than three with $\\kappa<0$. Further, we prove that an almost co-K\\\"ahler manifold satisfying $\\eta$-Einstein condition with constant coefficients reduces to a $K$-almost co-K\\\"ahler manifold, provided $ma_{1} \\neq (2n-1)b_{1}$ and $m \\neq 1$. We also characterize perfect fluid spacetime whose Lorentzian metric is equipped with $(m, \\rho)$-quasi Einstein solitons and acquired that the perfect fluid spacetime has vanishing vorticity, or it represents dark energy era under certain restriction on the potential function. Finally, we construct an example of an almost co-K\\\"ahler manifold with $(m,\\rho)$-quasi-Einstein solitons.","sentences":["The present paper aims to investigate $(m,\\rho)$-quasi-Einstein metrices on almost co-K\\\"ahler manifolds $\\mathcal{M}$. It is proven that if a $(\\kappa,\\mu)$-almost co-K\\\"ahler manifold with $\\kappa<0$ is $(m,\\rho)$-quasi-Einstein manifold, then $\\mathcal{M}$ represents a $N(\\kappa)$-almost co-K\\\"ahler manifold and the manifold is locally isomorphic to a solvable non-nilpotent Lie group.","Next, we study the three dimensional case and get the above mentioned result along with the manifold $\\mathcal{M}^3$ becoming an $\\eta$-Einstein manifold.","We also show that there does not exist $(m,\\rho)$-quasi-Einstein structure on a compact $(\\kappa,\\mu)$-almost co-K\\\"ahler manifold of dimension greater than three with $\\kappa<0$. Further, we prove that an almost co-K\\\"ahler manifold satisfying $\\eta$-Einstein condition with constant coefficients reduces to a $K$-almost co-K\\\"ahler manifold, provided $ma_{1} \\neq (2n-1)b_{1}$ and $m \\neq 1$.","We also characterize perfect fluid spacetime whose Lorentzian metric is equipped with $(m, \\rho)$-quasi","Einstein solitons","and","acquired that the perfect fluid spacetime has vanishing vorticity, or it represents dark energy era under certain restriction on the potential function.","Finally, we construct an example of an almost co-K\\\"ahler manifold with $(m,\\rho)$-quasi-Einstein solitons."],"url":"http://arxiv.org/abs/2402.01452v1","category":"math.DG"}
{"created":"2024-02-02 14:15:01","title":"The effect of diversity on group decision-making","abstract":"We explore different aspects of cognitive diversity and its effect on the success of group deliberation. To evaluate this, we use 500 dialogues from small, online groups discussing the Wason Card Selection task - the DeliData corpus. Leveraging the corpus, we perform quantitative analysis evaluating three different measures of cognitive diversity. First, we analyse the effect of group size as a proxy measure for diversity. Second, we evaluate the effect of the size of the initial idea pool. Finally, we look into the content of the discussion by analysing discussed solutions, discussion patterns, and how conversational probing can improve those characteristics.   Despite the reputation of groups for compounding bias, we show that small groups can, through dialogue, overcome intuitive biases and improve individual decision-making. Across a large sample and different operationalisations, we consistently find that greater cognitive diversity is associated with more successful group deliberation. Code and data used for the analysis are available in the anonymised repository: https://anonymous.4open.science/ r/cogsci24-FD6D","sentences":["We explore different aspects of cognitive diversity and its effect on the success of group deliberation.","To evaluate this, we use 500 dialogues from small, online groups discussing the Wason Card Selection task - the DeliData corpus.","Leveraging the corpus, we perform quantitative analysis evaluating three different measures of cognitive diversity.","First, we analyse the effect of group size as a proxy measure for diversity.","Second, we evaluate the effect of the size of the initial idea pool.","Finally, we look into the content of the discussion by analysing discussed solutions, discussion patterns, and how conversational probing can improve those characteristics.   ","Despite the reputation of groups for compounding bias, we show that small groups can, through dialogue, overcome intuitive biases and improve individual decision-making.","Across a large sample and different operationalisations, we consistently find that greater cognitive diversity is associated with more successful group deliberation.","Code and data used for the analysis are available in the anonymised repository: https://anonymous.4open.science/ r/cogsci24-FD6D"],"url":"http://arxiv.org/abs/2402.01427v1","category":"cs.CL"}
{"created":"2024-02-02 13:26:14","title":"A survey on robustness in trajectory prediction for autonomous vehicles","abstract":"Autonomous vehicles rely on accurate trajectory prediction to inform decision-making processes related to navigation and collision avoidance. However, current trajectory prediction models show signs of overfitting, which may lead to unsafe or suboptimal behavior. To address these challenges, this paper presents a comprehensive framework that categorizes and assesses the definitions and strategies used in the literature on evaluating and improving the robustness of trajectory prediction models. This involves a detailed exploration of various approaches, including data slicing methods, perturbation techniques, model architecture changes, and post-training adjustments. In the literature, we see many promising methods for increasing robustness, which are necessary for safe and reliable autonomous driving.","sentences":["Autonomous vehicles rely on accurate trajectory prediction to inform decision-making processes related to navigation and collision avoidance.","However, current trajectory prediction models show signs of overfitting, which may lead to unsafe or suboptimal behavior.","To address these challenges, this paper presents a comprehensive framework that categorizes and assesses the definitions and strategies used in the literature on evaluating and improving the robustness of trajectory prediction models.","This involves a detailed exploration of various approaches, including data slicing methods, perturbation techniques, model architecture changes, and post-training adjustments.","In the literature, we see many promising methods for increasing robustness, which are necessary for safe and reliable autonomous driving."],"url":"http://arxiv.org/abs/2402.01397v1","category":"cs.RO"}
{"created":"2024-02-02 13:14:26","title":"Symmetric Petz-R\u00e9nyi relative entropy uncertainty relation","abstract":"Holevo introduced a fidelity between quantum states that is symmetric and as effective as the trace norm in evaluating their similarity. This fidelity is bounded by a function of the trace norm, a relationship to which we will refer as Holevo's inequality. More broadly, Holevo's fidelity is part of a one-parameter family of symmetric Petz-R\\'enyi relative entropies, which in turn satisfy a Pinsker's-like inequality with respect to the trace norm. Although Holevo's inequality is tight, Pinsker's inequality is loose for this family. We show that the symmetric Petz-R\\'enyi relative entropies satisfy a tight inequality with respect to the trace norm, improving Pinsker's and reproducing Holevo's as a specific case. Additionally, we show how this result emerges from a symmetric Petz-R\\'enyi uncertainty relation, a result that encompasses several relations in quantum and stochastic thermodynamics.","sentences":["Holevo introduced a fidelity between quantum states that is symmetric and as effective as the trace norm in evaluating their similarity.","This fidelity is bounded by a function of the trace norm, a relationship to which we will refer as Holevo's inequality.","More broadly, Holevo's fidelity is part of a one-parameter family of symmetric Petz-R\\'enyi relative entropies, which in turn satisfy a Pinsker's-like inequality with respect to the trace norm.","Although Holevo's inequality is tight, Pinsker's inequality is loose for this family.","We show that the symmetric Petz-R\\'enyi relative entropies satisfy a tight inequality with respect to the trace norm, improving Pinsker's and reproducing Holevo's as a specific case.","Additionally, we show how this result emerges from a symmetric Petz-R\\'enyi uncertainty relation, a result that encompasses several relations in quantum and stochastic thermodynamics."],"url":"http://arxiv.org/abs/2402.01390v1","category":"quant-ph"}
{"created":"2024-02-02 13:06:26","title":"Spatial-Sign based Maxsum Test for High Dimensional Location Parameters","abstract":"In this study, we explore a robust testing procedure for the high-dimensional location parameters testing problem. Initially, we introduce a spatial-sign based max-type test statistic, which exhibits excellent performance for sparse alternatives. Subsequently, we demonstrate the asymptotic independence between this max-type test statistic and the spatial-sign based sum-type test statistic (Feng and Sun, 2016). Building on this, we propose a spatial-sign based max-sum type testing procedure, which shows remarkable performance under varying signal sparsity. Our simulation studies underscore the superior performance of the procedures we propose.","sentences":["In this study, we explore a robust testing procedure for the high-dimensional location parameters testing problem.","Initially, we introduce a spatial-sign based max-type test statistic, which exhibits excellent performance for sparse alternatives.","Subsequently, we demonstrate the asymptotic independence between this max-type test statistic and the spatial-sign based sum-type test statistic (Feng and Sun, 2016).","Building on this, we propose a spatial-sign based max-sum type testing procedure, which shows remarkable performance under varying signal sparsity.","Our simulation studies underscore the superior performance of the procedures we propose."],"url":"http://arxiv.org/abs/2402.01381v1","category":"stat.ME"}
{"created":"2024-02-02 12:55:10","title":"cmaes : A Simple yet Practical Python Library for CMA-ES","abstract":"The covariance matrix adaptation evolution strategy (CMA-ES) has been highly effective in black-box continuous optimization, as demonstrated by its success in both benchmark problems and various real-world applications. To address the need for an accessible yet potent tool in this domain, we developed cmaes, a simple and practical Python library for CMA-ES. cmaes is characterized by its simplicity, offering intuitive use and high code readability. This makes it suitable for quickly using CMA-ES, as well as for educational purposes and seamless integration into other libraries. Despite its simplistic design, cmaes maintains enhanced functionality. It incorporates recent advancements in CMA-ES, such as learning rate adaptation for challenging scenarios, transfer learning, and mixed-integer optimization capabilities. These advanced features are accessible through a user-friendly API, ensuring that cmaes can be easily adopted in practical applications. We regard cmaes as the first choice for a Python CMA-ES library among practitioners. The software is available under the MIT license at https://github.com/CyberAgentAILab/cmaes.","sentences":["The covariance matrix adaptation evolution strategy (CMA-ES) has been highly effective in black-box continuous optimization, as demonstrated by its success in both benchmark problems and various real-world applications.","To address the need for an accessible yet potent tool in this domain, we developed cmaes, a simple and practical Python library for CMA-ES. cmaes is characterized by its simplicity, offering intuitive use and high code readability.","This makes it suitable for quickly using CMA-ES, as well as for educational purposes and seamless integration into other libraries.","Despite its simplistic design, cmaes maintains enhanced functionality.","It incorporates recent advancements in CMA-ES, such as learning rate adaptation for challenging scenarios, transfer learning, and mixed-integer optimization capabilities.","These advanced features are accessible through a user-friendly API, ensuring that cmaes can be easily adopted in practical applications.","We regard cmaes as the first choice for a Python CMA-ES library among practitioners.","The software is available under the MIT license at https://github.com/CyberAgentAILab/cmaes."],"url":"http://arxiv.org/abs/2402.01373v1","category":"cs.NE"}
{"created":"2024-02-02 12:27:58","title":"What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation Properties for Fact Verification","abstract":"Biomedical claim verification fails if no evidence can be discovered. In these cases, the fact-checking verdict remains unknown and the claim is unverifiable. To improve upon this, we have to understand if there are any claim properties that impact its verifiability. In this work we assume that entities and relations define the core variables in a biomedical claim's anatomy and analyze if their properties help us to differentiate verifiable from unverifiable claims. In a study with trained annotation experts we prompt them to find evidence for biomedical claims, and observe how they refine search queries for their evidence search. This leads to the first corpus for scientific fact verification annotated with subject-relation-object triplets, evidence documents, and fact-checking verdicts (the BEAR-Fact corpus). We find (1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is particularly challenging. Further, we see that annotators process queries mostly by adding constraints to the search and by normalizing entities to canonical names. (2) We compare our in-house annotations with a small crowdsourcing setting where we employ medical experts and laypeople. We find that domain expertise does not have a substantial effect on the reliability of annotations. Finally, (3), we demonstrate that it is possible to reliably estimate the success of evidence retrieval purely from the claim text~(.82\\F), whereas identifying unverifiable claims proves more challenging (.27\\F). The dataset is available at http://www.ims.uni-stuttgart.de/data/bioclaim.","sentences":["Biomedical claim verification fails if no evidence can be discovered.","In these cases, the fact-checking verdict remains unknown and the claim is unverifiable.","To improve upon this, we have to understand if there are any claim properties that impact its verifiability.","In this work we assume that entities and relations define the core variables in a biomedical claim's anatomy and analyze if their properties help us to differentiate verifiable from unverifiable claims.","In a study with trained annotation experts we prompt them to find evidence for biomedical claims, and observe how they refine search queries for their evidence search.","This leads to the first corpus for scientific fact verification annotated with subject-relation-object triplets, evidence documents, and fact-checking verdicts (the BEAR-Fact corpus).","We find (1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is particularly challenging.","Further, we see that annotators process queries mostly by adding constraints to the search and by normalizing entities to canonical names.","(2) We compare our in-house annotations with a small crowdsourcing setting where we employ medical experts and laypeople.","We find that domain expertise does not have a substantial effect on the reliability of annotations.","Finally, (3), we demonstrate that it is possible to reliably estimate the success of evidence retrieval purely from the claim text~(.82\\F), whereas identifying unverifiable claims proves more challenging (.27\\F).","The dataset is available at http://www.ims.uni-stuttgart.de/data/bioclaim."],"url":"http://arxiv.org/abs/2402.01360v1","category":"cs.CL"}
{"created":"2024-02-02 11:30:21","title":"AD$^+$ implies that $\u03c9_1$ is a $\u0398$-Berkeley cardinal","abstract":"Following \\cite{bagaria2019large}, given cardinals $\\kappa<\\lambda$, we say $\\kappa$ is a club $\\lambda$-Berkeley cardinal if for every transitive set $N$ of size $<\\lambda$ such that $\\kappa\\subseteq N$, there is a club $C\\subseteq \\kappa$ with the property that for every $\\eta\\in C$ there is an elementary embedding $j: N\\rightarrow N$ with crit$(j)=\\eta$. We say $\\kappa$ is $\\nu$-club $\\lambda$-Berkeley if $C\\subseteq \\kappa$ as above is a $\\nu$-club. We say $\\kappa$ is $\\lambda$-Berkeley if $C$ is unbounded in $\\kappa$. We show that under AD$^+$, (1) every regular Suslin cardinal is $\\omega$-club $\\Theta$-Berkeley (see \\rthm{main theorem}), (2) $\\omega_1$ is club $\\Theta$-Berkeley (see \\rthm{main theorem lr} and \\rthm{main theorem}), and (3) the ${\\tilde\\delta}^1_{2n}$'s are $\\Theta$-Berkeley -- in particular, $\\omega_2$ is $\\Theta$-Berkeley (see \\rrem{omega2}).   Along the way, we represent regular Suslin cardinals in direct limits as cutpoint cardinals (see \\rthm{char extenders}). This topic has been studied in \\cite{MPSC} and \\cite{jackson2022suslin}, albeit from a different point of view. We also show that, assuming $V=L(\\mathbb{R})+{\\mathrm{AD}}$, $\\omega_1$ is not $\\Theta^+$-Berkeley, so the result stated in the title is optimal (see \\rthm{lr optimal} and \\rthm{thetareg optimal}).","sentences":["Following \\cite{bagaria2019large}, given cardinals $\\kappa<\\lambda$, we say $\\kappa$ is a club $\\lambda$-Berkeley cardinal if for every transitive set $N$ of size $<\\lambda$ such that $\\kappa\\subseteq N$, there is a club $C\\subseteq \\kappa$ with the property that for every $\\eta\\in C$ there is an elementary embedding $j: N\\rightarrow N$ with crit$(j)=\\eta$.","We say $\\kappa$ is $\\nu$-club $\\lambda$-Berkeley if $C\\subseteq \\kappa$ as above is a $\\nu$-club.","We say $\\kappa$ is $\\lambda$-Berkeley if $C$ is unbounded in $\\kappa$.","We show that under AD$^+$, (1) every regular Suslin cardinal is $\\omega$-club $\\Theta$-Berkeley (see \\rthm{main theorem}), (2) $\\omega_1$ is club $\\Theta$-Berkeley (see \\rthm{main theorem lr} and \\rthm{main theorem}), and (3) the ${\\tilde\\delta}^1_{2n}$'s are $\\Theta$-Berkeley -- in particular, $\\omega_2$ is $\\Theta$-Berkeley (see \\rrem{omega2}).   ","Along the way, we represent regular Suslin cardinals in direct limits as cutpoint cardinals (see \\rthm{char extenders}).","This topic has been studied in \\cite{MPSC} and \\cite{jackson2022suslin}, albeit from a different point of view.","We also show that, assuming $V=L(\\mathbb{R})+{\\mathrm{AD}}$, $\\omega_1$ is not $\\Theta^+$-Berkeley, so the result stated in the title is optimal (see \\rthm{lr optimal} and \\rthm{thetareg optimal})."],"url":"http://arxiv.org/abs/2402.01329v1","category":"math.LO"}
{"created":"2024-02-02 11:24:07","title":"Adaptive multi-criteria-based load balancing technique for resource allocation in fog-cloud environments","abstract":"Recently, to deliver services directly to the network edge, fog computing, an emerging and developing technology, acts as a layer between the cloud and the IoT worlds. The cloud or fog computing nodes could be selected by IoTs applications to meet their resource needs. Due to the scarce resources of fog devices that are available, as well as the need to meet user demands for low latency and quick reaction times, resource allocation in the fog-cloud environment becomes a difficult problem. In this problem, the load balancing between several fog devices is the most important element in achieving resource efficiency and preventing overload on fog devices. In this paper, a new adaptive resource allocation technique for load balancing in a fog-cloud environment is proposed. The proposed technique ranks each fog device using hybrid multi-criteria decision-making approaches Fuzzy Analytic Hierarchy Process (FAHP) and Fuzzy Technique for Order Performance by Similarity to Ideal Solution (FTOPSIS), then selects the most effective fog device based on the resulting ranking set. The simulation results show that the proposed technique outperforms existing techniques in terms of load balancing, response time, resource utilization, and energy consumption. The proposed technique decreases the number of fog nodes by 11%, load balancing variance by 69% and increases resource utilization to 90% which is comparatively higher than the comparable methods.","sentences":["Recently, to deliver services directly to the network edge, fog computing, an emerging and developing technology, acts as a layer between the cloud and the IoT worlds.","The cloud or fog computing nodes could be selected by IoTs applications to meet their resource needs.","Due to the scarce resources of fog devices that are available, as well as the need to meet user demands for low latency and quick reaction times, resource allocation in the fog-cloud environment becomes a difficult problem.","In this problem, the load balancing between several fog devices is the most important element in achieving resource efficiency and preventing overload on fog devices.","In this paper, a new adaptive resource allocation technique for load balancing in a fog-cloud environment is proposed.","The proposed technique ranks each fog device using hybrid multi-criteria decision-making approaches Fuzzy Analytic Hierarchy Process (FAHP) and Fuzzy Technique for Order Performance by Similarity to Ideal Solution (FTOPSIS), then selects the most effective fog device based on the resulting ranking set.","The simulation results show that the proposed technique outperforms existing techniques in terms of load balancing, response time, resource utilization, and energy consumption.","The proposed technique decreases the number of fog nodes by 11%, load balancing variance by 69% and increases resource utilization to 90% which is comparatively higher than the comparable methods."],"url":"http://arxiv.org/abs/2402.01326v1","category":"cs.DC"}
{"created":"2024-02-02 11:09:37","title":"The first quenched galaxies, when and how?","abstract":"Many quenched galaxies discovered in the early Universe by \\textit{JWST} raise fundemental question s on when and how these galaxies became quiescent. Making use of the latest version of the semi-analytic model GAEA that provides good agreement with the observed quenched fractions up to $z\\sim 3$, we make predictions for the expected fractions of quiescent galaxies up to $z\\sim 7$ and analyze the main quenching mechanism. We find that in a simulated box of $685~{\\rm Mpc}$ on a side, the first quenched massive ($M_{\\star} \\sim 10^{11} {\\rm M}_{\\odot}$), Milky Way mass, and low mass ($M_{\\star} \\sim 10^{9.5} {\\rm M}_{\\odot}$ ) galaxies appear at $z\\sim 4.5$, $z\\sim 6.2$, and before $z = 7$. Most quenched galaxies identified at early redshifts remain quenched for more than 1 Gyr. Independently of galaxy stellar mass, the dominant quenching mechanism at high redshift is accretion disk feedback (quasar winds) from a central massive black hole, which is triggered by mergers in massive and Milky Way mass galaxies, and by disk instabilities in low-mass galaxies. Environmental stripping become increasingly more important at lower redshift.","sentences":["Many quenched galaxies discovered in the early Universe by \\textit{JWST} raise fundemental question s on when and how these galaxies became quiescent.","Making use of the latest version of the semi-analytic model GAEA that provides good agreement with the observed quenched fractions up to $z\\sim 3$, we make predictions for the expected fractions of quiescent galaxies up to $z\\sim 7$ and analyze the main quenching mechanism.","We find that in a simulated box of $685~{\\rm Mpc}$ on a side, the first quenched massive ($M_{\\star} \\sim 10^{11} {\\rm M}_{\\odot}$), Milky Way mass, and low mass ($M_{\\star} \\sim 10^{9.5} {\\rm M}_{\\odot}$ ) galaxies appear at $z\\sim 4.5$, $z\\sim 6.2$, and before $z = 7$. Most quenched galaxies identified at early redshifts remain quenched for more than 1 Gyr.","Independently of galaxy stellar mass, the dominant quenching mechanism at high redshift is accretion disk feedback (quasar winds) from a central massive black hole, which is triggered by mergers in massive and Milky Way mass galaxies, and by disk instabilities in low-mass galaxies.","Environmental stripping become increasingly more important at lower redshift."],"url":"http://arxiv.org/abs/2402.01314v1","category":"astro-ph.GA"}
{"created":"2024-02-02 10:58:26","title":"Bi-Objective Optimization over the Efficient Set of Multi-Objective Integer Quadratic Problem","abstract":"In this paper, we present an exact algorithm for optimizing two linear fractional over the efficient set of a multi-objective integer quadratic problem. This type of problems arises when two decision-makers, such as firms, each have a preference function to optimize over the efficient set of a multi-objective problem. The algorithm employs a branch-and-cut approach, which involves: (1) exploring the solution space using a branch-and-bound strategy in the decision space, and (2) eliminating inefficient solutions using a cutting plane technique with efficient cuts constructed from the non-increasing directions of objective functions. Additionally, integral tests are incorporated to further ensure the efficiency of the obtained solutions.We present a comprehensive example, accompanied by a step-by-step resolution, to demonstrate the functioning of the algorithm.","sentences":["In this paper, we present an exact algorithm for optimizing two linear fractional over the efficient set of a multi-objective integer quadratic problem.","This type of problems arises when two decision-makers, such as firms, each have a preference function to optimize over the efficient set of a multi-objective problem.","The algorithm employs a branch-and-cut approach, which involves: (1) exploring the solution space using a branch-and-bound strategy in the decision space, and (2) eliminating inefficient solutions using a cutting plane technique with efficient cuts constructed from the non-increasing directions of objective functions.","Additionally, integral tests are incorporated to further ensure the efficiency of the obtained solutions.","We present a comprehensive example, accompanied by a step-by-step resolution, to demonstrate the functioning of the algorithm."],"url":"http://arxiv.org/abs/2402.01310v1","category":"math.OC"}
{"created":"2024-02-02 10:14:58","title":"Adiabatic theory of crossing third order resonance and its comparison with experiment at VEPP-4M","abstract":"The universal analytical approach was developed to describe the process of particle slow extraction from a synchrotron using adiabatic crossing the betatron resonance of the third order. The obtained formulas make it possible to calculate the time diagrams for the current of the extracted beam ('spill profile'), taking into account the chromaticity, particle momentum spread and synchrotron oscillations. On this basis, a method for monochromatization of extracted beam is indicated. The theory is compared with some results obtained in the experiment on crossing the third-order resonance at the VEPP-4M storage ring.","sentences":["The universal analytical approach was developed to describe the process of particle slow extraction from a synchrotron using adiabatic crossing the betatron resonance of the third order.","The obtained formulas make it possible to calculate the time diagrams for the current of the extracted beam ('spill profile'), taking into account the chromaticity, particle momentum spread and synchrotron oscillations.","On this basis, a method for monochromatization of extracted beam is indicated.","The theory is compared with some results obtained in the experiment on crossing the third-order resonance at the VEPP-4M storage ring."],"url":"http://arxiv.org/abs/2402.01281v1","category":"physics.acc-ph"}
{"created":"2024-02-02 10:04:29","title":"Parametric-Task MAP-Elites","abstract":"Optimizing a set of functions simultaneously by leveraging their similarity is called multi-task optimization. Current black-box multi-task algorithms only solve a finite set of tasks, even when the tasks originate from a continuous space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel black-box algorithm to solve continuous multi-task optimization problems. This algorithm (1) solves a new task at each iteration, effectively covering the continuous space, and (2) exploits a new variation operator based on local linear regression. The resulting dataset of solutions makes it possible to create a function that maps any task parameter to its optimal solution. We show on two parametric-task toy problems and a more realistic and challenging robotic problem in simulation that PT-ME outperforms all baselines, including the deep reinforcement learning algorithm PPO.","sentences":["Optimizing a set of functions simultaneously by leveraging their similarity is called multi-task optimization.","Current black-box multi-task algorithms only solve a finite set of tasks, even when the tasks originate from a continuous space.","In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel black-box algorithm to solve continuous multi-task optimization problems.","This algorithm (1) solves a new task at each iteration, effectively covering the continuous space, and (2) exploits a new variation operator based on local linear regression.","The resulting dataset of solutions makes it possible to create a function that maps any task parameter to its optimal solution.","We show on two parametric-task toy problems and a more realistic and challenging robotic problem in simulation that PT-ME outperforms all baselines, including the deep reinforcement learning algorithm PPO."],"url":"http://arxiv.org/abs/2402.01275v1","category":"cs.NE"}
{"created":"2024-02-02 09:55:15","title":"An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec","abstract":"Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.","sentences":["Recently, neural networks have proven to be effective in performing speech coding task at low bitrates.","However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality.","To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec).","An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently.","Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise.","CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication.","Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps."],"url":"http://arxiv.org/abs/2402.01271v1","category":"eess.AS"}
{"created":"2024-02-02 08:42:45","title":"Delving into Decision-based Black-box Attacks on Semantic Segmentation","abstract":"Semantic segmentation is a fundamental visual task that finds extensive deployment in applications with security-sensitive considerations. Nonetheless, recent work illustrates the adversarial vulnerability of semantic segmentation models to white-box attacks. However, its adversarial robustness against black-box attacks has not been fully explored. In this paper, we present the first exploration of black-box decision-based attacks on semantic segmentation. First, we analyze the challenges that semantic segmentation brings to decision-based attacks through the case study. Then, to address these challenges, we first propose a decision-based attack on semantic segmentation, called Discrete Linear Attack (DLA). Based on random search and proxy index, we utilize the discrete linear noises for perturbation exploration and calibration to achieve efficient attack efficiency. We conduct adversarial robustness evaluation on 5 models from Cityscapes and ADE20K under 8 attacks. DLA shows its formidable power on Cityscapes by dramatically reducing PSPNet's mIoU from an impressive 77.83% to a mere 2.14% with just 50 queries.","sentences":["Semantic segmentation is a fundamental visual task that finds extensive deployment in applications with security-sensitive considerations.","Nonetheless, recent work illustrates the adversarial vulnerability of semantic segmentation models to white-box attacks.","However, its adversarial robustness against black-box attacks has not been fully explored.","In this paper, we present the first exploration of black-box decision-based attacks on semantic segmentation.","First, we analyze the challenges that semantic segmentation brings to decision-based attacks through the case study.","Then, to address these challenges, we first propose a decision-based attack on semantic segmentation, called Discrete Linear Attack (DLA).","Based on random search and proxy index, we utilize the discrete linear noises for perturbation exploration and calibration to achieve efficient attack efficiency.","We conduct adversarial robustness evaluation on 5 models from Cityscapes and ADE20K under 8 attacks.","DLA shows its formidable power on Cityscapes by dramatically reducing PSPNet's mIoU from an impressive 77.83% to a mere 2.14% with just 50 queries."],"url":"http://arxiv.org/abs/2402.01220v1","category":"cs.CV"}
{"created":"2024-02-02 08:37:17","title":"SPDEs driven by standard symmetric $\u03b1$-stable cylindrical L\u00e9vy processes: existence, Lyapunov functionals and It\u00f4 formula","abstract":"We investigate several aspects of solutions to stochastic evolution equations in Hilbert spaces driven by a standard symmetric $\\alpha$-stable cylindrical noise. Similarly to cylindrical Brownian motion or Gaussian white noise, standard symmetric $\\alpha$-stable noise exists only in a generalised sense in Hilbert spaces. The main results of this work are the existence of a mild solution, long-term regularity of the solutions via Lyapunov functional approach, and an It\\^{o} formula for mild solutions to evolution equations under consideration. The main tools for establishing these results are Yosida approximations and an It\\^{o} formula for Hilbert space-valued semi-martingales where the martingale part is represented as an integral driven by cylindrical $\\alpha$-stable noise. While these tools are standard in stochastic analysis, due to the cylindrical nature of our noise, their application requires completely novel arguments and techniques.","sentences":["We investigate several aspects of solutions to stochastic evolution equations in Hilbert spaces driven by a standard symmetric $\\alpha$-stable cylindrical noise.","Similarly to cylindrical Brownian motion or Gaussian white noise, standard symmetric $\\alpha$-stable noise exists only in a generalised sense in Hilbert spaces.","The main results of this work are the existence of a mild solution, long-term regularity of the solutions via Lyapunov functional approach, and an It\\^{o} formula for mild solutions to evolution equations under consideration.","The main tools for establishing these results are Yosida approximations and an It\\^{o} formula for Hilbert space-valued semi-martingales where the martingale part is represented as an integral driven by cylindrical $\\alpha$-stable noise.","While these tools are standard in stochastic analysis, due to the cylindrical nature of our noise, their application requires completely novel arguments and techniques."],"url":"http://arxiv.org/abs/2402.01211v1","category":"math.PR"}
{"created":"2024-02-02 14:44:50","title":"Convolution kernel adaptation to calibrated fisheye","abstract":"Convolution kernels are the basic structural component of convolutional neural networks (CNNs). In the last years there has been a growing interest in fisheye cameras for many applications. However, the radially symmetric projection model of these cameras produces high distortions that affect the performance of CNNs, especially when the field of view is very large. In this work, we tackle this problem by proposing a method that leverages the calibration of cameras to deform the convolution kernel accordingly and adapt to the distortion. That way, the receptive field of the convolution is similar to standard convolutions in perspective images, allowing us to take advantage of pre-trained networks in large perspective datasets. We show how, with just a brief fine-tuning stage in a small dataset, we improve the performance of the network for the calibrated fisheye with respect to standard convolutions in depth estimation and semantic segmentation.","sentences":["Convolution kernels are the basic structural component of convolutional neural networks (CNNs).","In the last years there has been a growing interest in fisheye cameras for many applications.","However, the radially symmetric projection model of these cameras produces high distortions that affect the performance of CNNs, especially when the field of view is very large.","In this work, we tackle this problem by proposing a method that leverages the calibration of cameras to deform the convolution kernel accordingly and adapt to the distortion.","That way, the receptive field of the convolution is similar to standard convolutions in perspective images, allowing us to take advantage of pre-trained networks in large perspective datasets.","We show how, with just a brief fine-tuning stage in a small dataset, we improve the performance of the network for the calibrated fisheye with respect to standard convolutions in depth estimation and semantic segmentation."],"url":"http://arxiv.org/abs/2402.01456v1","category":"cs.CV"}
{"created":"2024-02-02 13:14:20","title":"SiMA-Hand: Boosting 3D Hand-Mesh Reconstruction by Single-to-Multi-View Adaptation","abstract":"Estimating 3D hand mesh from RGB images is a longstanding track, in which occlusion is one of the most challenging problems. Existing attempts towards this task often fail when the occlusion dominates the image space. In this paper, we propose SiMA-Hand, aiming to boost the mesh reconstruction performance by Single-to-Multi-view Adaptation. First, we design a multi-view hand reconstructor to fuse information across multiple views by holistically adopting feature fusion at image, joint, and vertex levels. Then, we introduce a single-view hand reconstructor equipped with SiMA. Though taking only one view as input at inference, the shape and orientation features in the single-view reconstructor can be enriched by learning non-occluded knowledge from the extra views at training, enhancing the reconstruction precision on the occluded regions. We conduct experiments on the Dex-YCB and HanCo benchmarks with challenging object- and self-caused occlusion cases, manifesting that SiMA-Hand consistently achieves superior performance over the state of the arts. Code will be released on https://github.com/JoyboyWang/SiMA-Hand Pytorch.","sentences":["Estimating 3D hand mesh from RGB images is a longstanding track, in which occlusion is one of the most challenging problems.","Existing attempts towards this task often fail when the occlusion dominates the image space.","In this paper, we propose SiMA-Hand, aiming to boost the mesh reconstruction performance by Single-to-Multi-view Adaptation.","First, we design a multi-view hand reconstructor to fuse information across multiple views by holistically adopting feature fusion at image, joint, and vertex levels.","Then, we introduce a single-view hand reconstructor equipped with SiMA.","Though taking only one view as input at inference, the shape and orientation features in the single-view reconstructor can be enriched by learning non-occluded knowledge from the extra views at training, enhancing the reconstruction precision on the occluded regions.","We conduct experiments on the Dex-YCB and HanCo benchmarks with challenging object- and self-caused occlusion cases, manifesting that SiMA-Hand consistently achieves superior performance over the state of the arts.","Code will be released on https://github.com/JoyboyWang/SiMA-Hand Pytorch."],"url":"http://arxiv.org/abs/2402.01389v1","category":"cs.CV"}
{"created":"2024-02-02 12:39:47","title":"LIR: Efficient Degradation Removal for Lightweight Image Restoration","abstract":"Recently, there have been significant advancements in Image Restoration based on CNN and transformer. However, the inherent characteristics of the Image Restoration task are often overlooked in many works. These works often focus on the basic block design and stack numerous basic blocks to the model, leading to redundant parameters and unnecessary computations and hindering the efficiency of the image restoration. In this paper, we propose a Lightweight Image Restoration network called LIR to efficiently remove degradation (blur, rain, noise, haze, etc.). A key component in LIR is the Efficient Adaptive Attention (EAA) Block, which is mainly composed of Adaptive Filters and Attention Blocks. It is capable of adaptively sharpening contours, removing degradation, and capturing global information in various image restoration scenes in an efficient and computation-friendly manner. In addition, through a simple structural design, LIR addresses the degradations existing in the local and global residual connections that are ignored by modern networks. Extensive experiments demonstrate that our LIR achieves comparable performance to state-of-the-art networks on most benchmarks with fewer parameters and computations. It is worth noting that our LIR produces better visual results than state-of-the-art networks that are more in line with the human aesthetic.","sentences":["Recently, there have been significant advancements in Image Restoration based on CNN and transformer.","However, the inherent characteristics of the Image Restoration task are often overlooked in many works.","These works often focus on the basic block design and stack numerous basic blocks to the model, leading to redundant parameters and unnecessary computations and hindering the efficiency of the image restoration.","In this paper, we propose a Lightweight Image Restoration network called LIR to efficiently remove degradation (blur, rain, noise, haze, etc.).","A key component in LIR is the Efficient Adaptive Attention (EAA) Block, which is mainly composed of Adaptive Filters and Attention Blocks.","It is capable of adaptively sharpening contours, removing degradation, and capturing global information in various image restoration scenes in an efficient and computation-friendly manner.","In addition, through a simple structural design, LIR addresses the degradations existing in the local and global residual connections that are ignored by modern networks.","Extensive experiments demonstrate that our LIR achieves comparable performance to state-of-the-art networks on most benchmarks with fewer parameters and computations.","It is worth noting that our LIR produces better visual results than state-of-the-art networks that are more in line with the human aesthetic."],"url":"http://arxiv.org/abs/2402.01368v1","category":"cs.CV"}
{"created":"2024-02-02 10:23:03","title":"Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection","abstract":"In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.","sentences":["In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing.","Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations.","We propose Spiking CenterNet for object detection on event data.","It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder.","Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy.","Distilling the knowledge of a non-spiking teacher into our SNN further increases performance.","To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection."],"url":"http://arxiv.org/abs/2402.01287v1","category":"cs.CV"}
{"created":"2024-02-02 08:55:23","title":"Unveiling Delay Effects in Traffic Forecasting: A Perspective from Spatial-Temporal Delay Differential Equations","abstract":"Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting. Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed. The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors. 2) Traffic conditions undergo continuous changes. The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements. Most existing discretized models require retraining for each prediction horizon, restricting their applicability. To tackle the above issues, we propose a neural Spatial-Temporal Delay Differential Equation model, namely STDDE. It includes both delay effects and continuity into a unified delay differential equation framework, which explicitly models the time delay in spatial information propagation. Furthermore, theoretical proofs are provided to show its stability. Then we design a learnable traffic-graph time-delay estimator, which utilizes the continuity of the hidden states to achieve the gradient backward process. Finally, we propose a continuous output module, allowing us to accurately predict traffic flow at various frequencies, which provides more flexibility and adaptability to different scenarios. Extensive experiments show the superiority of the proposed STDDE along with competitive computational efficiency.","sentences":["Traffic flow forecasting is a fundamental research issue for transportation planning and management, which serves as a canonical and typical example of spatial-temporal predictions.","In recent years, Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) have achieved great success in capturing spatial-temporal correlations for traffic flow forecasting.","Yet, two non-ignorable issues haven't been well solved: 1) The message passing in GNNs is immediate, while in reality the spatial message interactions among neighboring nodes can be delayed.","The change of traffic flow at one node will take several minutes, i.e., time delay, to influence its connected neighbors.","2) Traffic conditions undergo continuous changes.","The prediction frequency for traffic flow forecasting may vary based on specific scenario requirements.","Most existing discretized models require retraining for each prediction horizon, restricting their applicability.","To tackle the above issues, we propose a neural Spatial-Temporal Delay Differential Equation model, namely STDDE.","It includes both delay effects and continuity into a unified delay differential equation framework, which explicitly models the time delay in spatial information propagation.","Furthermore, theoretical proofs are provided to show its stability.","Then we design a learnable traffic-graph time-delay estimator, which utilizes the continuity of the hidden states to achieve the gradient backward process.","Finally, we propose a continuous output module, allowing us to accurately predict traffic flow at various frequencies, which provides more flexibility and adaptability to different scenarios.","Extensive experiments show the superiority of the proposed STDDE along with competitive computational efficiency."],"url":"http://arxiv.org/abs/2402.01231v1","category":"cs.LG"}
{"created":"2024-02-02 17:49:31","title":"Immersive Video Compression using Implicit Neural Representations","abstract":"Recent work on implicit neural representations (INRs) has evidenced their potential for efficiently representing and encoding conventional video content. In this paper we, for the first time, extend their application to immersive (multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video codec. MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video codec, HiNeRV, which was developed for single-view video compression. We have modified the model to learn a different group of feature grids for each view, and share the learnt network parameters among all views. This enables the model to effectively exploit the spatio-temporal and the inter-view redundancy that exists within multi-view videos. The proposed codec was used to compress multi-view texture and depth video sequences in the MPEG Immersive Video (MIV) Common Test Conditions, and tested against the MIV Test model (TMIV) that uses the VVenC video codec. The results demonstrate the superior performance of MV-HiNeRV, with significant coding gains (up to 72.33%) over TMIV. The implementation of MV-HiNeRV will be published for further development and evaluation.","sentences":["Recent work on implicit neural representations (INRs) has evidenced their potential for efficiently representing and encoding conventional video content.","In this paper we, for the first time, extend their application to immersive (multi-view) videos, by proposing MV-HiNeRV, a new INR-based immersive video codec.","MV-HiNeRV is an enhanced version of a state-of-the-art INR-based video codec, HiNeRV, which was developed for single-view video compression.","We have modified the model to learn a different group of feature grids for each view, and share the learnt network parameters among all views.","This enables the model to effectively exploit the spatio-temporal and the inter-view redundancy that exists within multi-view videos.","The proposed codec was used to compress multi-view texture and depth video sequences in the MPEG Immersive Video (MIV) Common Test Conditions, and tested against the MIV Test model (TMIV) that uses the VVenC video codec.","The results demonstrate the superior performance of MV-HiNeRV, with significant coding gains (up to 72.33%) over TMIV.","The implementation of MV-HiNeRV will be published for further development and evaluation."],"url":"http://arxiv.org/abs/2402.01596v1","category":"eess.IV"}
{"created":"2024-02-02 17:19:44","title":"Kinetic shock profiles for the Landau equation","abstract":"The physical quantities in a gas should vary continuously across a shock. However, the physics inherent in the compressible Euler equations is insufficient to describe the width or structure of the shock. We demonstrate the existence of weak shock profiles to the kinetic Landau equation, that is, traveling wave solutions with Maxwellian asymptotic states whose hydrodynamic quantities satisfy the Rankine-Hugoniot conditions. These solutions serve to capture the structure of weak shocks at the kinetic level. Previous works considered only the Boltzmann equation with hard sphere and angular cut-off potentials.","sentences":["The physical quantities in a gas should vary continuously across a shock.","However, the physics inherent in the compressible Euler equations is insufficient to describe the width or structure of the shock.","We demonstrate the existence of weak shock profiles to the kinetic Landau equation, that is, traveling wave solutions with Maxwellian asymptotic states whose hydrodynamic quantities satisfy the Rankine-Hugoniot conditions.","These solutions serve to capture the structure of weak shocks at the kinetic level.","Previous works considered only the Boltzmann equation with hard sphere and angular cut-off potentials."],"url":"http://arxiv.org/abs/2402.01581v1","category":"math.AP"}
{"created":"2024-02-02 17:08:14","title":"An Actionable Framework for Understanding and Improving Talent Retention as a Competitive Advantage in IT Organizations","abstract":"In the rapidly evolving global business landscape, the demand for software has intensified competition among organizations, leading to challenges in retaining highly qualified IT members in software organizations. One of the problems faced by IT organizations is the retention of these strategic professionals, also known as talent. This work presents an actionable framework for Talent Retention (TR) used in IT organizations. It is based on our findings from interviews performed with 21 IT managers. The TR Framework is our main research outcome. Our framework encompasses a set of factors, contextual characteristics, barriers, strategies, and coping mechanisms.   Our findings indicated that software engineers can be differentiated from other professional groups, and beyond competitive salaries, other elements for retaining talent in IT organizations should be considered, such as psychological safety, work-life balance, a positive work environment, innovative and challenging projects, and flexible work. A better understanding of factors could guide IT managers in improving talent management processes by addressing Software Engineering challenges, identifying important elements, and exploring strategies at the individual, team, and organizational levels.","sentences":["In the rapidly evolving global business landscape, the demand for software has intensified competition among organizations, leading to challenges in retaining highly qualified IT members in software organizations.","One of the problems faced by IT organizations is the retention of these strategic professionals, also known as talent.","This work presents an actionable framework for Talent Retention (TR) used in IT organizations.","It is based on our findings from interviews performed with 21 IT managers.","The TR Framework is our main research outcome.","Our framework encompasses a set of factors, contextual characteristics, barriers, strategies, and coping mechanisms.   ","Our findings indicated that software engineers can be differentiated from other professional groups, and beyond competitive salaries, other elements for retaining talent in IT organizations should be considered, such as psychological safety, work-life balance, a positive work environment, innovative and challenging projects, and flexible work.","A better understanding of factors could guide IT managers in improving talent management processes by addressing Software Engineering challenges, identifying important elements, and exploring strategies at the individual, team, and organizational levels."],"url":"http://arxiv.org/abs/2402.01573v1","category":"cs.SE"}
{"created":"2024-02-02 16:58:57","title":"Efficiency of neural quantum states in light of the quantum geometric tensor","abstract":"Neural quantum state (NQS) ans\\\"atze have shown promise in variational Monte Carlo algorithms by their theoretical capability of representing any quantum state. However, the reason behind the practical improvement in their performance with an increase in the number of parameters is not fully understood. In this work, we systematically study the efficiency of restricted Boltzmann Machines (RBMs) to represent the ground states in different phases of the spin-1 bilinear-biquadratic model, as the hidden layer density $\\alpha$ increases. We train our ansatz by minimizing two different loss functions: 1) energy, and 2) infidelity of the NQS ansatz w.r.t. that of the exact ground state. We observe that the accuracy of our ansatz saturates with $\\alpha$ in both cases. We demonstrate that this can be explained by looking at the spectrum of the quantum geometric tensor (QGT). We find that the rank of the QGT saturates beyond a certain $\\alpha$, and we emphasize that it corresponds to the \\textit{dimension of the relevant manifold} for an optimized NQS. This provides a useful diagnostics for the practical representation power of an NQS ansatz.","sentences":["Neural quantum state (NQS) ans\\\"atze have shown promise in variational Monte Carlo algorithms by their theoretical capability of representing any quantum state.","However, the reason behind the practical improvement in their performance with an increase in the number of parameters is not fully understood.","In this work, we systematically study the efficiency of restricted Boltzmann Machines (RBMs) to represent the ground states in different phases of the spin-1 bilinear-biquadratic model, as the hidden layer density $\\alpha$ increases.","We train our ansatz by minimizing two different loss functions: 1) energy, and 2) infidelity of the NQS ansatz w.r.t.","that of the exact ground state.","We observe that the accuracy of our ansatz saturates with $\\alpha$ in both cases.","We demonstrate that this can be explained by looking at the spectrum of the quantum geometric tensor (QGT).","We find that the rank of the QGT saturates beyond a certain $\\alpha$, and we emphasize that it corresponds to the \\textit{dimension of the relevant manifold} for an optimized NQS.","This provides a useful diagnostics for the practical representation power of an NQS ansatz."],"url":"http://arxiv.org/abs/2402.01565v1","category":"quant-ph"}
{"created":"2024-02-02 16:57:11","title":"First-order planar autoregressive model","abstract":"This paper establishes the conditions of existence of a stationary solution to the first order autoregressive equation on a plane as well as properties of the stationarity solution. The first-order autoregressive model on a plane is defined by the equation   $X_{i,j} = a X_{i-1,j} + b X_{i,j-1} + c X_{i-1,j-1} + \\epsilon_{i,j}.$   A stationary solution $X$ to the equation exists if and only if $(1-a-b-c) (1-a+b+c) (1+a-b+c) (1+a+b-c) > 0$. The stationary solution $X$ satisfies the causality condition with respect to the white noise $\\epsilon$ if and only if $1-a-b-c>0$, $1-a+b+c>0$, $1+a-b+c>0$ and $1+a+b-c>0$. A sufficient condition for X to be purely nondeterministic is provided.   An explicit expression for the autocovariance function of $X$ at some points is provided. With Yule-Walker equations, this allows to compute the autocovariance function everywhere. In addition, all situations are described where different parameters determine the same autocovariance function of $X$.","sentences":["This paper establishes the conditions of existence of a stationary solution to the first order autoregressive equation on a plane as well as properties of the stationarity solution.","The first-order autoregressive model on a plane is defined by the equation   $X_{i,j} = a X_{i-1,j} + b X_{i,j-1} + c X_{i-1,j-1} + \\epsilon_{i,j}.$   ","A stationary solution $X$ to the equation exists if and only if $(1-a-b-c) (1-a+b+c) (1+a-b+c) (1+a+b-c) > 0$.","The stationary solution $X$ satisfies the causality condition with respect to the white noise $\\epsilon$ if and only if $1-a-b-c>0$, $1-a+b+c>0$, $1+a-b+c>0$ and $1+a+b-c>0$. A sufficient condition for X to be purely nondeterministic is provided.   ","An explicit expression for the autocovariance function of $X$ at some points is provided.","With Yule-Walker equations, this allows to compute the autocovariance function everywhere.","In addition, all situations are described where different parameters determine the same autocovariance function of $X$."],"url":"http://arxiv.org/abs/2402.01563v1","category":"math.PR"}
{"created":"2024-02-02 16:55:50","title":"Dynamics of the Korteweg-de Vries equation on a balanced metric graph","abstract":"In this work, we establish local well-posedness for the Korteweg-de Vries model on a balanced star graph with a structure represented by semi-infinite edges, by considering a boundary condition of $\\delta$-type at the {unique} graph-vertex. Also, we extend the linear instability result of Angulo and Cavalcante (2021) to one of nonlinear instability. For the proof of local well posedness theory the principal new ingredient is the utilization of the special solutions by Faminskii in the context of half-lines. As far as we are aware, this approach is being used for the first time in the context of star graphs and can potentially be applied to other boundary classes. In the case of the nonlinear instability result, the principal ingredients are the linearized instability known result and the fact that data-to-solution map determined by the local theory is at least of class $C^2$.","sentences":["In this work, we establish local well-posedness for the Korteweg-de Vries model on a balanced star graph with a structure represented by semi-infinite edges, by considering a boundary condition of $\\delta$-type at the {unique} graph-vertex.","Also, we extend the linear instability result of Angulo and Cavalcante (2021) to one of nonlinear instability.","For the proof of local well posedness theory the principal new ingredient is the utilization of the special solutions by Faminskii in the context of half-lines.","As far as we are aware, this approach is being used for the first time in the context of star graphs and can potentially be applied to other boundary classes.","In the case of the nonlinear instability result, the principal ingredients are the linearized instability known result and the fact that data-to-solution map determined by the local theory is at least of class $C^2$."],"url":"http://arxiv.org/abs/2402.01561v1","category":"math.AP"}
{"created":"2024-02-02 16:46:22","title":"Diastolic and isoperimetric inequalities on surfaces","abstract":"We prove a universal inequality between the diastole, defined using a minimax process on the one-cycle space, and the area of closed Riemannian surfaces. Roughly speaking, we show that any closed Riemannian surface can be swept out by a family of multi-loops whose lengths are bounded in terms of the area of the surface. This diastolic inequality, which relies on an upper bound on Cheeger's constant, yields an effective process to find short closed geodesics on the two-sphere, for instance. We deduce that every Riemannian surface can be decomposed into two domains with the same area such that the length of their boundary is bounded from above in terms of the area of the surface. We also compare various Riemannian invariants on the two-sphere to underline the special role played by the diastole.","sentences":["We prove a universal inequality between the diastole, defined using a minimax process on the one-cycle space, and the area of closed Riemannian surfaces.","Roughly speaking, we show that any closed Riemannian surface can be swept out by a family of multi-loops whose lengths are bounded in terms of the area of the surface.","This diastolic inequality, which relies on an upper bound on Cheeger's constant, yields an effective process to find short closed geodesics on the two-sphere, for instance.","We deduce that every Riemannian surface can be decomposed into two domains with the same area such that the length of their boundary is bounded from above in terms of the area of the surface.","We also compare various Riemannian invariants on the two-sphere to underline the special role played by the diastole."],"url":"http://arxiv.org/abs/2402.01554v1","category":"math.DG"}
{"created":"2024-02-02 14:56:56","title":"Intrinsic orbital fourfold anisotropic magnetoresistance in Dirac materials","abstract":"Fourfold anisotropic magnetoresistance (AMR) have been widely observed in quantum materials, but the underlying mechanisms remain poorly understood. Here we find, in a variety of three-dimensional Dirac materials that can be unifiedly described by the massive Dirac equation, the intrinsic orbital magnetic moment of electrons vary synchronously with the magnetic field and give rise to a {\\pi} periodic correction to its velocity, further leading to unusual fourfold AMR, dubbed intrinsic orbital fourfold AMR. Our theory not only explains the observation of fourfold AMR in bismuth but also uncovers the nature of the dominant fourfold AMR in thin films of antiferromagnetic topological insulator MnBi2Te4, which arises from the near cancellation of the twofold AMR from the surface states and bulk states due to distinct spin-momentum lockings. Our work provides a new mechanism for creation and manipulation of intrinsic fourfold AMR in both conventional conductors and various topological insulators.","sentences":["Fourfold anisotropic magnetoresistance (AMR) have been widely observed in quantum materials, but the underlying mechanisms remain poorly understood.","Here we find, in a variety of three-dimensional Dirac materials that can be unifiedly described by the massive Dirac equation, the intrinsic orbital magnetic moment of electrons vary synchronously with the magnetic field and give rise to a {\\pi} periodic correction to its velocity, further leading to unusual fourfold AMR, dubbed intrinsic orbital fourfold AMR.","Our theory not only explains the observation of fourfold AMR in bismuth but also uncovers the nature of the dominant fourfold AMR in thin films of antiferromagnetic topological insulator MnBi2Te4, which arises from the near cancellation of the twofold AMR from the surface states and bulk states due to distinct spin-momentum lockings.","Our work provides a new mechanism for creation and manipulation of intrinsic fourfold AMR in both conventional conductors and various topological insulators."],"url":"http://arxiv.org/abs/2402.01470v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-02 14:40:36","title":"Divergence conforming finite element methods for flow-transport coupling with osmotic effects","abstract":"We propose a model for the coupling of flow and transport equations with porous membrane-type conditions on part of the boundary. The governing equations consist of the incompressible Navier--Stokes equations coupled with an advection-diffusion equation, and we employ a Lagrange multiplier to enforce the coupling between penetration velocity and transport on the membrane, while mixed boundary conditions are considered in the remainder of the boundary. We show existence and uniqueness of the continuous problem using a fixed-point argument. Next, an H(div)-conforming finite element formulation is proposed, and we address its a priori error analysis. The method uses an upwind approach that provides stability in the convection-dominated regime. We showcase a set of numerical examples validating the theory and illustrating the use of the new methods in the simulation of reverse osmosis processes.","sentences":["We propose a model for the coupling of flow and transport equations with porous membrane-type conditions on part of the boundary.","The governing equations consist of the incompressible Navier--Stokes equations coupled with an advection-diffusion equation, and we employ a Lagrange multiplier to enforce the coupling between penetration velocity and transport on the membrane, while mixed boundary conditions are considered in the remainder of the boundary.","We show existence and uniqueness of the continuous problem using a fixed-point argument.","Next, an H(div)-conforming finite element formulation is proposed, and we address its a priori error analysis.","The method uses an upwind approach that provides stability in the convection-dominated regime.","We showcase a set of numerical examples validating the theory and illustrating the use of the new methods in the simulation of reverse osmosis processes."],"url":"http://arxiv.org/abs/2402.01451v1","category":"math.NA"}
{"created":"2024-02-02 14:12:20","title":"Conformal vector fields on almost Kenmotsu manifolds","abstract":"In this paper, first we consider that the conformal vector field $\\mathbf{X}$ is identical with the Reeb vector field $\\varsigma$ and next, assume that $\\mathbf{X}$ is pointwise collinear with %the Reeb vector field $\\varsigma$, in both cases it is shown that the manifold $\\mathbf{N}^{2m+1}$ becomes a Kenmotsu manifold and $\\mathbf{N}^{2m+1}$ is locally a warped product $\\mathbf{N}' \\times_{f} \\mathbf{M}^{2m}$, where $\\mathbf{M}^{2m}$ is an almost K\\\"ahler manifold, $\\mathbf{N}'$ is an open interval with coordinate t, and $f = ce^{t}$ for some positive constant c. Beside these, we prove that if a $(\\verb\"k\",\\boldsymbol{\\mu})'$-almost Kenmotsu manifold admits a Killing vector field $\\mathbf{X}$, then either it is locally a warped product of an almost K\\\"ahler manifold and an open interval or $\\mathbf{X}$ is a strict infinitesimal contact transformation. Furthermore, we also investigate $\\boldsymbol{\\eta}$-Ricci-Yamabe soliton with conformal vector fields on $(\\verb\"k\",\\boldsymbol{\\mu})'$-almost Kenmotsu manifolds and finally, we construct an example.","sentences":["In this paper, first we consider that the conformal vector field $\\mathbf{X}$ is identical with the Reeb vector field $\\varsigma$ and next, assume that $\\mathbf{X}$ is pointwise collinear with %the Reeb vector field $\\varsigma$, in both cases it is shown that the manifold $\\mathbf{N}^{2m+1}$ becomes a Kenmotsu manifold and $\\mathbf{N}^{2m+1}$ is locally a warped product $\\mathbf{N}' \\times_{f} \\mathbf{M}^{2m}$, where $\\mathbf{M}^{2m}$ is an almost K\\\"ahler manifold, $\\mathbf{N}'$ is an open interval with coordinate t, and $f = ce^{t}$ for some positive constant c. Beside these, we prove that if a $(\\verb\"k\",\\boldsymbol{\\mu})'$-almost Kenmotsu manifold admits a Killing vector field $\\mathbf{X}$, then either it is locally a warped product of an almost K\\\"ahler manifold and an open interval or $\\mathbf{X}$ is a strict infinitesimal contact transformation.","Furthermore, we also investigate $\\boldsymbol{\\eta}$-Ricci-Yamabe soliton with conformal vector fields on $(\\verb\"k\",\\boldsymbol{\\mu})'$-almost Kenmotsu manifolds and finally, we construct an example."],"url":"http://arxiv.org/abs/2402.01425v1","category":"math.DG"}
{"created":"2024-02-02 13:42:39","title":"Analytical solutions of the Schr\u00f6dinger equation for two confined atoms with van der Waals interaction","abstract":"We derive solutions of the Schr\\\"{o}dinger equation for the isotropic van der Waals interaction in a symmetric harmonic trap, with the recent approach [arXiv:2207.09377 (2022)] to handle the multi-scale long-range potential. Asymptotic behaviors of these solutions are then obtained for $r\\rightarrow 0$ and $r\\rightarrow \\infty$. We further deduce the energy spectrum of the two-body relative motion and relate the spectrum to scattering lengths for $s$ wave and $p$ wave. These results can be used to research trapped atom-atom collisions and energy spectra.","sentences":["We derive solutions of the Schr\\\"{o}dinger equation for the isotropic van der Waals interaction in a symmetric harmonic trap, with the recent approach [arXiv:2207.09377 (2022)] to handle the multi-scale long-range potential.","Asymptotic behaviors of these solutions are then obtained for $r\\rightarrow 0$ and $r\\rightarrow \\infty$. We further deduce the energy spectrum of the two-body relative motion and relate the spectrum to scattering lengths for $s$ wave and $p$ wave.","These results can be used to research trapped atom-atom collisions and energy spectra."],"url":"http://arxiv.org/abs/2402.01409v1","category":"physics.atom-ph"}
{"created":"2024-02-02 12:02:42","title":"Monotone, Bi-Lipschitz, and Polyak-\u0141ojasiewicz Networks","abstract":"This paper presents a new \\emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \\emph{Lipschitzness} (output sensitivity to input perturbations) and \\emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computable global minimum.","sentences":["This paper presents a new \\emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \\emph{Lipschitzness} (output sensitivity to input perturbations) and \\emph{inverse Lipschitzness} (input distinguishability from different outputs).","The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks.","The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization.","Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known.","Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\\L{}ojasiewicz condition.","It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computable global minimum."],"url":"http://arxiv.org/abs/2402.01344v1","category":"cs.LG"}
{"created":"2024-02-02 11:16:50","title":"Doubly substituted isotopologues of HCCCN in TMC-1: Detection of D13CCCN, DC13CCN, DCC13CN, DCCC15N, H13C13CCN, H13CC13CN, HC13C13CN, HCC13C15N, and HC13CC15N","abstract":"We report the first detection in space of a complete sample of nine doubly substituted isotopologues of HCCCN towards the cyanopolyyne peak of TMC-1 using observations of the QUIJOTE line survey taken with the Yebes 40 m telescope. We detected D13CCCN, DC13CCN, DCC13CN, DCCC15N, H13C13CCN, H13CC13CN, HC13C13CN, HCC13C15N, and HC13CC15N through their J=4-3 and J=5-4 lines in the 7 mm window. In addition, we present an extensive analysis of the emission of HCCCN and its singly substituted isotopologues through a large velocity gradient model of the lines detected at 7 mm and 3 mm using the Yebes 40 m and the IRAM 30 m telescopes, respectively. The derived column densities for all the isotopologues are consistent in the two spectral bands for an H2 volume density of 1e4 cm-3 and a kinetic temperature of 10 K. Whereas we observed a 13C fractionation for HCC13CN and other double isotopologues with a 13C atom adjacent to the nitrogen atom, we derived similar C/13C abundance ratios for the three 13C substituted species of DCCCN. This suggests additional chemical discrimination for deuterated isotopologues of HCCCN. Finally, we present the spatial distribution of the J=4-3 and J=5-4 lines from the singly substituted species observed with the Yebes 40 m telescope. The emission peak of the spatial distribution of DCCCN appears to be displaced by 40'' with respect to that of HCCCN and the 13C and 15N isotopologues. In addition to a different formation route for the deuterated species, we could also expect that this differentiation owing to the deuterium fractionation is more efficient at low temperatures, and therefore, that deuterated species trace a colder region of the cloud.","sentences":["We report the first detection in space of a complete sample of nine doubly substituted isotopologues of HCCCN towards the cyanopolyyne peak of TMC-1 using observations of the QUIJOTE line survey taken with the Yebes 40 m telescope.","We detected D13CCCN, DC13CCN, DCC13CN, DCCC15N, H13C13CCN, H13CC13CN, HC13C13CN, HCC13C15N, and HC13CC15N through their J=4-3 and J=5-4 lines in the 7 mm window.","In addition, we present an extensive analysis of the emission of HCCCN and its singly substituted isotopologues through a large velocity gradient model of the lines detected at 7 mm and 3 mm using the Yebes 40 m and the IRAM 30 m telescopes, respectively.","The derived column densities for all the isotopologues are consistent in the two spectral bands for an H2 volume density of 1e4 cm-3 and a kinetic temperature of 10 K.","Whereas we observed a 13C fractionation for HCC13CN and other double isotopologues with a 13C atom adjacent to the nitrogen atom, we derived similar C/13C abundance ratios for the three 13C substituted species of DCCCN.","This suggests additional chemical discrimination for deuterated isotopologues of HCCCN.","Finally, we present the spatial distribution of the J=4-3 and J=5-4 lines from the singly substituted species observed with the Yebes 40 m telescope.","The emission peak of the spatial distribution of DCCCN appears to be displaced by 40'' with respect to that of HCCCN and the 13C and 15N isotopologues.","In addition to a different formation route for the deuterated species, we could also expect that this differentiation owing to the deuterium fractionation is more efficient at low temperatures, and therefore, that deuterated species trace a colder region of the cloud."],"url":"http://arxiv.org/abs/2402.01318v1","category":"astro-ph.GA"}
{"created":"2024-02-02 10:42:06","title":"Two Approaches to Diachronic Normalization of Polish Texts","abstract":"This paper discusses two approaches to the diachronic normalization of Polish texts: a rule-based solution that relies on a set of handcrafted patterns, and a neural normalization model based on the text-to-text transfer transformer architecture. The training and evaluation data prepared for the task are discussed in detail, along with experiments conducted to compare the proposed normalization solutions. A quantitative and qualitative analysis is made. It is shown that at the current stage of inquiry into the problem, the rule-based solution outperforms the neural one on 3 out of 4 variants of the prepared dataset, although in practice both approaches have distinct advantages and disadvantages.","sentences":["This paper discusses two approaches to the diachronic normalization of Polish texts: a rule-based solution that relies on a set of handcrafted patterns, and a neural normalization model based on the text-to-text transfer transformer architecture.","The training and evaluation data prepared for the task are discussed in detail, along with experiments conducted to compare the proposed normalization solutions.","A quantitative and qualitative analysis is made.","It is shown that at the current stage of inquiry into the problem, the rule-based solution outperforms the neural one on 3 out of 4 variants of the prepared dataset, although in practice both approaches have distinct advantages and disadvantages."],"url":"http://arxiv.org/abs/2402.01300v1","category":"cs.CL"}
{"created":"2024-02-02 10:16:10","title":"Differentiable and accelerated wavelet transforms on the sphere and ball","abstract":"Directional wavelet dictionaries are hierarchical representations which efficiently capture and segment information across scale, location and orientation. Such representations demonstrate a particular affinity to physical signals, which often exhibit highly anisotropic, localised multiscale structure. Many physically important signals are observed over spherical domains, such as the celestial sky in cosmology. Leveraging recent advances in computational harmonic analysis, we design new highly distributable and automatically differentiable directional wavelet transforms on the $2$-dimensional sphere $\\mathbb{S}^2$ and $3$-dimensional ball $\\mathbb{B}^3 = \\mathbb{R}^+ \\times \\mathbb{S}^2$ (the space formed by augmenting the sphere with the radial half-line). We observe up to a $300$-fold and $21800$-fold acceleration for signals on the sphere and ball, respectively, compared to existing software, whilst maintaining 64-bit machine precision. Not only do these algorithms dramatically accelerate existing spherical wavelet transforms, the gradient information afforded by automatic differentiation unlocks many data-driven analysis techniques previously not possible for these spaces. We publicly release both S2WAV and S2BALL, open-sourced JAX libraries for our transforms that are automatically differentiable and readily deployable both on and over clusters of hardware accelerators (e.g. GPUs & TPUs).","sentences":["Directional wavelet dictionaries are hierarchical representations which efficiently capture and segment information across scale, location and orientation.","Such representations demonstrate a particular affinity to physical signals, which often exhibit highly anisotropic, localised multiscale structure.","Many physically important signals are observed over spherical domains, such as the celestial sky in cosmology.","Leveraging recent advances in computational harmonic analysis, we design new highly distributable and automatically differentiable directional wavelet transforms on the $2$-dimensional sphere $\\mathbb{S}^2$ and $3$-dimensional ball $\\mathbb{B}^3 = \\mathbb{R}^+ \\times \\mathbb{S}^2$ (the space formed by augmenting the sphere with the radial half-line).","We observe up to a $300$-fold and $21800$-fold acceleration for signals on the sphere and ball, respectively, compared to existing software, whilst maintaining 64-bit machine precision.","Not only do these algorithms dramatically accelerate existing spherical wavelet transforms, the gradient information afforded by automatic differentiation unlocks many data-driven analysis techniques previously not possible for these spaces.","We publicly release both S2WAV and S2BALL, open-sourced JAX libraries for our transforms that are automatically differentiable and readily deployable both on and over clusters of hardware accelerators (e.g. GPUs & TPUs)."],"url":"http://arxiv.org/abs/2402.01282v1","category":"astro-ph.IM"}
{"created":"2024-02-02 10:14:55","title":"In-gap states induced by magnetic impurities on wide-band s-wave superconductors: self-consistent calculations","abstract":"The role of self-consistency in Bogoliubov-de Gennes equations is frequently underestimated in the investigation of in-gap states created by magnetic impurities in s-wave superconductors. Our research focuses on the impact of self-consistency on the in-gap states produced by magnetic stuctures on superconductors, specifically evaluating the density of states, the in-gap bands, and their topological attributes. Here, we show results ranging from single impurity to finite chains, and infinite ferromagnetic spin chains in wide-band s-wave superconductors. These results show that the order parameter contains important information regarding quantum phase transitions and their topological nature, underscoring the importance of self-consistency in such studies.","sentences":["The role of self-consistency in Bogoliubov-de Gennes equations is frequently underestimated in the investigation of in-gap states created by magnetic impurities in s-wave superconductors.","Our research focuses on the impact of self-consistency on the in-gap states produced by magnetic stuctures on superconductors, specifically evaluating the density of states, the in-gap bands, and their topological attributes.","Here, we show results ranging from single impurity to finite chains, and infinite ferromagnetic spin chains in wide-band s-wave superconductors.","These results show that the order parameter contains important information regarding quantum phase transitions and their topological nature, underscoring the importance of self-consistency in such studies."],"url":"http://arxiv.org/abs/2402.01280v1","category":"cond-mat.supr-con"}
{"created":"2024-02-02 10:08:09","title":"Degenerate diffusion in porous media with hysteresis-dependent permeability","abstract":"Hysteresis in the pressure-saturation relation in unsaturated porous media, which is due to surface tension on the liquid-gas interface, exhibits strong degeneracy in the resulting mass balance equation. Solutions to such degenerate equations have been recently constructed by the method of convexification. We show here that the convexification argument works even if the permeability coefficient depends on the hysteretic saturation. The problem of uniqueness remains open in this case.","sentences":["Hysteresis in the pressure-saturation relation in unsaturated porous media, which is due to surface tension on the liquid-gas interface, exhibits strong degeneracy in the resulting mass balance equation.","Solutions to such degenerate equations have been recently constructed by the method of convexification.","We show here that the convexification argument works even if the permeability coefficient depends on the hysteretic saturation.","The problem of uniqueness remains open in this case."],"url":"http://arxiv.org/abs/2402.01278v1","category":"math.AP"}
{"created":"2024-02-02 09:33:07","title":"Cascaded Scaling Classifier: class incremental learning with probability scaling","abstract":"Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved by modifying the output of the model with auxiliary scaling functions. We empirically show that our approach performs well on multiple benchmarks against well-established baselines, and we also study each component of our proposal and how the combinations of such components affect the final results.","sentences":["Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting.","The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones.","This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples.","In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier.","The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively.","The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them.","This is achieved by modifying the output of the model with auxiliary scaling functions.","We empirically show that our approach performs well on multiple benchmarks against well-established baselines, and we also study each component of our proposal and how the combinations of such components affect the final results."],"url":"http://arxiv.org/abs/2402.01262v1","category":"cs.LG"}
{"created":"2024-02-02 08:45:38","title":"HW-SW Optimization of DNNs for Privacy-preserving People Counting on Low-resolution Infrared Arrays","abstract":"Low-resolution infrared (IR) array sensors enable people counting applications such as monitoring the occupancy of spaces and people flows while preserving privacy and minimizing energy consumption. Deep Neural Networks (DNNs) have been shown to be well-suited to process these sensor data in an accurate and efficient manner. Nevertheless, the space of DNNs' architectures is huge and its manual exploration is burdensome and often leads to sub-optimal solutions. To overcome this problem, in this work, we propose a highly automated full-stack optimization flow for DNNs that goes from neural architecture search, mixed-precision quantization, and post-processing, down to the realization of a new smart sensor prototype, including a Microcontroller with a customized instruction set. Integrating these cross-layer optimizations, we obtain a large set of Pareto-optimal solutions in the 3D-space of energy, memory, and accuracy. Deploying such solutions on our hardware platform, we improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x code size reduction, and 15.38x energy reduction at iso-accuracy.","sentences":["Low-resolution infrared (IR) array sensors enable people counting applications such as monitoring the occupancy of spaces and people flows while preserving privacy and minimizing energy consumption.","Deep Neural Networks (DNNs) have been shown to be well-suited to process these sensor data in an accurate and efficient manner.","Nevertheless, the space of DNNs' architectures is huge and its manual exploration is burdensome and often leads to sub-optimal solutions.","To overcome this problem, in this work, we propose a highly automated full-stack optimization flow for DNNs that goes from neural architecture search, mixed-precision quantization, and post-processing, down to the realization of a new smart sensor prototype, including a Microcontroller with a customized instruction set.","Integrating these cross-layer optimizations, we obtain a large set of Pareto-optimal solutions in the 3D-space of energy, memory, and accuracy.","Deploying such solutions on our hardware platform, we improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x code size reduction, and 15.38x energy reduction at iso-accuracy."],"url":"http://arxiv.org/abs/2402.01226v1","category":"cs.LG"}
{"created":"2024-02-02 08:25:28","title":"Comparative Evaluation of Weather Forecasting using Machine Learning Models","abstract":"Gaining a deeper understanding of weather and being able to predict its future conduct have always been considered important endeavors for the growth of our society. This research paper explores the advancements in understanding and predicting nature's behavior, particularly in the context of weather forecasting, through the application of machine learning algorithms. By leveraging the power of machine learning, data mining, and data analysis techniques, significant progress has been made in this field. This study focuses on analyzing the contributions of various machine learning algorithms in predicting precipitation and temperature patterns using a 20-year dataset from a single weather station in Dhaka city. Algorithms such as Gradient Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest, Stacking Neural Network, and Stacking KNN are evaluated and compared based on their performance metrics, including Confusion matrix measurements. The findings highlight remarkable achievements and provide valuable insights into their performances and features correlation.","sentences":["Gaining a deeper understanding of weather and being able to predict its future conduct have always been considered important endeavors for the growth of our society.","This research paper explores the advancements in understanding and predicting nature's behavior, particularly in the context of weather forecasting, through the application of machine learning algorithms.","By leveraging the power of machine learning, data mining, and data analysis techniques, significant progress has been made in this field.","This study focuses on analyzing the contributions of various machine learning algorithms in predicting precipitation and temperature patterns using a 20-year dataset from a single weather station in Dhaka city.","Algorithms such as Gradient Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest, Stacking Neural Network, and Stacking KNN are evaluated and compared based on their performance metrics, including Confusion matrix measurements.","The findings highlight remarkable achievements and provide valuable insights into their performances and features correlation."],"url":"http://arxiv.org/abs/2402.01206v1","category":"cs.LG"}
{"created":"2024-02-02 18:55:31","title":"Directional Response of Several Geometries for Reactor-Neutrino Detectors","abstract":"We report simulation studies of six low-energy electron-antineutrino detector designs, with the goal of determining their ability to resolve the direction to an antineutrino source. Such detectors with target masses on the one-ton scale are well-suited to reactor monitoring at distances of 5--25 meters from the core. They can provide accurate measurements of reactor operating power, fuel mix, and burnup, as well as unsurpassed nuclear non-proliferation information in a non-contact cooperating reactor scenario such as those used by IAEA. A number of groups around the world are working on programs to develop detectors similar to some of those in this study. Here, we examine and compare several approaches to detector geometry for their ability not only to detect the inverse beta decay (IBD) reaction, but also to determine the source direction of incident antineutrinos. The information from these detectors provides insight into reactor power and burning profile, which is especially useful in constraining the clandestine production of weapons material. In a live deployment, a non-proliferation detector must be able to isolate the subject reactor, possibly from a field of much-larger power reactors; directional sensitivity can help greatly with this task. We also discuss implications for using such detectors in longer-distance observation of reactors, from a few km to hundreds of km. We have modeled six abstracted detector designs, including two for which we have operational data for validating our computer modeling and analytical processes. We have found that the most promising options, regardless of scale and range, have angular resolutions on the order of a few degrees, which is better than any yet achieved in practice by a factor of at least two.","sentences":["We report simulation studies of six low-energy electron-antineutrino detector designs, with the goal of determining their ability to resolve the direction to an antineutrino source.","Such detectors with target masses on the one-ton scale are well-suited to reactor monitoring at distances of 5--25 meters from the core.","They can provide accurate measurements of reactor operating power, fuel mix, and burnup, as well as unsurpassed nuclear non-proliferation information in a non-contact cooperating reactor scenario such as those used by IAEA.","A number of groups around the world are working on programs to develop detectors similar to some of those in this study.","Here, we examine and compare several approaches to detector geometry for their ability not only to detect the inverse beta decay (IBD) reaction, but also to determine the source direction of incident antineutrinos.","The information from these detectors provides insight into reactor power and burning profile, which is especially useful in constraining the clandestine production of weapons material.","In a live deployment, a non-proliferation detector must be able to isolate the subject reactor, possibly from a field of much-larger power reactors; directional sensitivity can help greatly with this task.","We also discuss implications for using such detectors in longer-distance observation of reactors, from a few km to hundreds of km.","We have modeled six abstracted detector designs, including two for which we have operational data for validating our computer modeling and analytical processes.","We have found that the most promising options, regardless of scale and range, have angular resolutions on the order of a few degrees, which is better than any yet achieved in practice by a factor of at least two."],"url":"http://arxiv.org/abs/2402.01636v1","category":"physics.ins-det"}
{"created":"2024-02-02 17:55:12","title":"Hyperparameter tuning via trajectory predictions: Stochastic prox-linear methods in matrix sensing","abstract":"Motivated by the desire to understand stochastic algorithms for nonconvex optimization that are robust to their hyperparameter choices, we analyze a mini-batched prox-linear iterative algorithm for the problem of recovering an unknown rank-1 matrix from rank-1 Gaussian measurements corrupted by noise. We derive a deterministic recursion that predicts the error of this method and show, using a non-asymptotic framework, that this prediction is accurate for any batch-size and a large range of step-sizes. In particular, our analysis reveals that this method, though stochastic, converges linearly from a local initialization with a fixed step-size to a statistical error floor. Our analysis also exposes how the batch-size, step-size, and noise level affect the (linear) convergence rate and the eventual statistical estimation error, and we demonstrate how to use our deterministic predictions to perform hyperparameter tuning (e.g. step-size and batch-size selection) without ever running the method. On a technical level, our analysis is enabled in part by showing that the fluctuations of the empirical iterates around our deterministic predictions scale with the error of the previous iterate.","sentences":["Motivated by the desire to understand stochastic algorithms for nonconvex optimization that are robust to their hyperparameter choices, we analyze a mini-batched prox-linear iterative algorithm for the problem of recovering an unknown rank-1 matrix from rank-1 Gaussian measurements corrupted by noise.","We derive a deterministic recursion that predicts the error of this method and show, using a non-asymptotic framework, that this prediction is accurate for any batch-size and a large range of step-sizes.","In particular, our analysis reveals that this method, though stochastic, converges linearly from a local initialization with a fixed step-size to a statistical error floor.","Our analysis also exposes how the batch-size, step-size, and noise level affect the (linear) convergence rate and the eventual statistical estimation error, and we demonstrate how to use our deterministic predictions to perform hyperparameter tuning (e.g. step-size and batch-size selection) without ever running the method.","On a technical level, our analysis is enabled in part by showing that the fluctuations of the empirical iterates around our deterministic predictions scale with the error of the previous iterate."],"url":"http://arxiv.org/abs/2402.01599v1","category":"math.OC"}
{"created":"2024-02-02 17:51:49","title":"Learning from Two Decades of Blood Pressure Data: Demography-Specific Patterns Across 75 Million Patient Encounters","abstract":"Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics. This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends. Numerous studies have reported on the relationship between BP variation and various factors. In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender. Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions. Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group. Moreover, our analysis uncovered intriguing similarities in the distribution of BP among some of the racial groups. This comprehensive investigation contributes to the ongoing discourse on hypertension and underscores the importance of considering diverse demographic factors in understanding BP variations. Our results provide valuable insights that may inform personalized healthcare approaches tailored to specific demographic profiles.","sentences":["Hypertension remains a global health concern with a rising prevalence, necessitating effective monitoring and understanding of blood pressure (BP) dynamics.","This study delves into the wealth of information derived from BP measurement, a crucial approach in informing our understanding of hypertensive trends.","Numerous studies have reported on the relationship between BP variation and various factors.","In this research, we leveraged an extensive dataset comprising 75 million records spanning two decades, offering a unique opportunity to explore and analyze BP variations across demographic features such as age, race, and gender.","Our findings revealed that gender-based BP variation was not statistically significant, challenging conventional assumptions.","Interestingly, systolic blood pressure (SBP) consistently increased with age, while diastolic blood pressure (DBP) displayed a distinctive peak in the forties age group.","Moreover, our analysis uncovered intriguing similarities in the distribution of BP among some of the racial groups.","This comprehensive investigation contributes to the ongoing discourse on hypertension and underscores the importance of considering diverse demographic factors in understanding BP variations.","Our results provide valuable insights that may inform personalized healthcare approaches tailored to specific demographic profiles."],"url":"http://arxiv.org/abs/2402.01598v1","category":"q-bio.PE"}
{"created":"2024-02-02 17:00:17","title":"Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise","abstract":"Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.","sentences":["Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited.","In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD.","In this work, we provide a different perspective based on online learning that underscores the importance of Adam's algorithmic components.","Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates, where we choose the updates of an optimizer based on an online learner.","With this framework, the design of a good optimizer is reduced to the design of a good online learner.","Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL).","Building on this observation, we study the benefits of its algorithmic components from the online learning perspective."],"url":"http://arxiv.org/abs/2402.01567v1","category":"cs.LG"}
{"created":"2024-02-02 16:15:55","title":"Big data applications on small quantum computers","abstract":"Current quantum hardware prohibits any direct use of large classical datasets. Coresets allow for a succinct description of these large datasets and their solution in a computational task is competitive with the solution on the original dataset. The method of combining coresets with small quantum computers to solve a given task that requires a large number of data points was first introduced by Harrow [arXiv:2004.00026]. In this paper, we apply the coreset method in three different well-studied classical machine learning problems, namely Divisive Clustering, 3-means Clustering, and Gaussian Mixture Model Clustering. We provide a Hamiltonian formulation of the aforementioned problems for which the number of qubits scales linearly with the size of the coreset. Then, we evaluate how the variational quantum eigensolver (VQE) performs on these problems and demonstrate the practical efficiency of coresets when used along with a small quantum computer. We perform noiseless simulations on instances of sizes up to 25 qubits on CUDA Quantum and show that our approach provides comparable performance to classical solvers.","sentences":["Current quantum hardware prohibits any direct use of large classical datasets.","Coresets allow for a succinct description of these large datasets and their solution in a computational task is competitive with the solution on the original dataset.","The method of combining coresets with small quantum computers to solve a given task that requires a large number of data points was first introduced by Harrow [arXiv:2004.00026].","In this paper, we apply the coreset method in three different well-studied classical machine learning problems, namely Divisive Clustering, 3-means Clustering, and Gaussian Mixture Model Clustering.","We provide a Hamiltonian formulation of the aforementioned problems for which the number of qubits scales linearly with the size of the coreset.","Then, we evaluate how the variational quantum eigensolver (VQE) performs on these problems and demonstrate the practical efficiency of coresets when used along with a small quantum computer.","We perform noiseless simulations on instances of sizes up to 25 qubits on CUDA Quantum and show that our approach provides comparable performance to classical solvers."],"url":"http://arxiv.org/abs/2402.01529v1","category":"quant-ph"}
{"created":"2024-02-02 16:06:24","title":"Low-Resource Cross-Domain Singing Voice Synthesis via Reduced Self-Supervised Speech Representations","abstract":"In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that is trained only on text and speech data as a typical multi-speaker acoustic model. It is a low-resource pipeline that does not utilize any singing data end-to-end, since its vocoder is also trained on speech data. Karaoker-SSL is conditioned by self-supervised speech representations in an unsupervised manner. We preprocess these representations by selecting only a subset of their task-correlated dimensions. The conditioning module is indirectly guided to capture style information during training by multi-tasking. This is achieved with a Conformer-based module, which predicts the pitch from the acoustic model's output. Thus, Karaoker-SSL allows singing voice synthesis without reliance on hand-crafted and domain-specific features. There are also no requirements for text alignments or lyrics timestamps. To refine the voice quality, we employ a U-Net discriminator that is conditioned on the target speaker and follows a Diffusion GAN training scheme.","sentences":["In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that is trained only on text and speech data as a typical multi-speaker acoustic model.","It is a low-resource pipeline that does not utilize any singing data end-to-end, since its vocoder is also trained on speech data.","Karaoker-SSL is conditioned by self-supervised speech representations in an unsupervised manner.","We preprocess these representations by selecting only a subset of their task-correlated dimensions.","The conditioning module is indirectly guided to capture style information during training by multi-tasking.","This is achieved with a Conformer-based module, which predicts the pitch from the acoustic model's output.","Thus, Karaoker-SSL allows singing voice synthesis without reliance on hand-crafted and domain-specific features.","There are also no requirements for text alignments or lyrics timestamps.","To refine the voice quality, we employ a U-Net discriminator that is conditioned on the target speaker and follows a Diffusion GAN training scheme."],"url":"http://arxiv.org/abs/2402.01520v1","category":"cs.SD"}
{"created":"2024-02-02 14:52:24","title":"Visual Gyroscope: Combination of Deep Learning Features and Direct Alignment for Panoramic Stabilization","abstract":"In this article we present a visual gyroscope based on equirectangular panoramas. We propose a new pipeline where we take advantage of combining three different methods to obtain a robust and accurate estimation of the attitude of the camera. We quantitatively and qualitatively validate our method on two image sequences taken with a $360^\\circ$ dual-fisheye camera mounted on different aerial vehicles.","sentences":["In this article we present a visual gyroscope based on equirectangular panoramas.","We propose a new pipeline where we take advantage of combining three different methods to obtain a robust and accurate estimation of the attitude of the camera.","We quantitatively and qualitatively validate our method on two image sequences taken with a $360^\\circ$ dual-fisheye camera mounted on different aerial vehicles."],"url":"http://arxiv.org/abs/2402.01461v1","category":"cs.CV"}
{"created":"2024-02-02 12:29:18","title":"To the Max: Reinventing Reward in Reinforcement Learning","abstract":"In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.","sentences":["In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance.","For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently.","Choosing a good reward function is hence an extremely important yet challenging problem.","In this paper, we explore an alternative approach to using rewards for learning.","We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward.","Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms.","In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL.","The code is publicly available."],"url":"http://arxiv.org/abs/2402.01361v1","category":"cs.LG"}
{"created":"2024-02-02 11:55:57","title":"Fundamental Properties of Causal Entropy and Information Gain","abstract":"Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality.","sentences":["Recent developments enable the quantification of causal control given a structural causal model (SCM).","This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another.","These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role.","They have not yet been properly mathematically studied.","Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules.","Furthermore, we elucidate the relationship between causal entropy and stochastic interventions.","We also propose definitions for causal conditional entropy and causal conditional information gain.","Overall, this exploration paves the way for enhancing causal machine learning tasks through the study of recently-proposed information theoretic quantities grounded in considerations about causality."],"url":"http://arxiv.org/abs/2402.01341v1","category":"cs.LG"}
{"created":"2024-02-02 11:53:27","title":"SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding","abstract":"Distributed learning is an effective approach to accelerate model training using multiple workers. However, substantial communication delays emerge between workers and a parameter server due to massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through one-bit quantization, yet the convergence rates considerably decrease as adversarial workers increase. In this paper, we show that the convergence rate is invariant as the number of adversarial workers increases, provided that the number of adversarial workers is smaller than that of benign workers. The key idea showing this counter-intuitive result is our novel signSGD with federated defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits the gradient information sent by adversarial workers with the proper weights, which are obtained through gradient sign decoding. Experimental results demonstrate signSGD-FD achieves superior convergence rates over traditional algorithms in various adversarial attack scenarios.","sentences":["Distributed learning is an effective approach to accelerate model training using multiple workers.","However, substantial communication delays emerge between workers and a parameter server due to massive costs associated with communicating gradients.","SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through one-bit quantization, yet the convergence rates considerably decrease as adversarial workers increase.","In this paper, we show that the convergence rate is invariant as the number of adversarial workers increases, provided that the number of adversarial workers is smaller than that of benign workers.","The key idea showing this counter-intuitive result is our novel signSGD with federated defense (signSGD-FD).","Unlike the traditional approaches, signSGD-FD exploits the gradient information sent by adversarial workers with the proper weights, which are obtained through gradient sign decoding.","Experimental results demonstrate signSGD-FD achieves superior convergence rates over traditional algorithms in various adversarial attack scenarios."],"url":"http://arxiv.org/abs/2402.01340v1","category":"cs.LG"}
{"created":"2024-02-02 11:52:07","title":"Improving Sequential Recommendations with LLMs","abstract":"The sequential recommendation problem has attracted considerable research attention in the past few years, leading to the rise of numerous recommendation models. In this work, we explore how Large Language Models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches. Specifically, we design three orthogonal approaches and hybrids of those to leverage the power of LLMs in different ways. In addition, we investigate the potential of each approach by focusing on its comprising technical aspects and determining an array of alternative choices for each one. We conduct extensive experiments on three datasets and explore a large variety of configurations, including different language models and baseline recommendation models, to obtain a comprehensive picture of the performance of each approach. Among other observations, we highlight that initializing state-of-the-art sequential recommendation models such as BERT4Rec or SASRec with embeddings obtained from an LLM can lead to substantial performance gains in terms of accuracy. Furthermore, we find that fine-tuning an LLM for recommendation tasks enables it to learn not only the tasks, but also concepts of a domain to some extent. We also show that fine-tuning OpenAI GPT leads to considerably better performance than fine-tuning Google PaLM 2. Overall, our extensive experiments indicate a huge potential value of leveraging LLMs in future recommendation approaches. We publicly share the code and data of our experiments to ensure reproducibility.","sentences":["The sequential recommendation problem has attracted considerable research attention in the past few years, leading to the rise of numerous recommendation models.","In this work, we explore how Large Language Models (LLMs), which are nowadays introducing disruptive effects in many AI-based applications, can be used to build or improve sequential recommendation approaches.","Specifically, we design three orthogonal approaches and hybrids of those to leverage the power of LLMs in different ways.","In addition, we investigate the potential of each approach by focusing on its comprising technical aspects and determining an array of alternative choices for each one.","We conduct extensive experiments on three datasets and explore a large variety of configurations, including different language models and baseline recommendation models, to obtain a comprehensive picture of the performance of each approach.","Among other observations, we highlight that initializing state-of-the-art sequential recommendation models such as BERT4Rec or SASRec with embeddings obtained from an LLM can lead to substantial performance gains in terms of accuracy.","Furthermore, we find that fine-tuning an LLM for recommendation tasks enables it to learn not only the tasks, but also concepts of a domain to some extent.","We also show that fine-tuning OpenAI GPT leads to considerably better performance than fine-tuning Google PaLM 2.","Overall, our extensive experiments indicate a huge potential value of leveraging LLMs in future recommendation approaches.","We publicly share the code and data of our experiments to ensure reproducibility."],"url":"http://arxiv.org/abs/2402.01339v1","category":"cs.IR"}
{"created":"2024-02-02 11:03:33","title":"Deep Multimodal Fusion of Data with Heterogeneous Dimensionality via Projective Networks","abstract":"The use of multimodal imaging has led to significant improvements in the diagnosis and treatment of many diseases. Similar to clinical practice, some works have demonstrated the benefits of multimodal fusion for automatic segmentation and classification using deep learning-based methods. However, current segmentation methods are limited to fusion of modalities with the same dimensionality (e.g., 3D+3D, 2D+2D), which is not always possible, and the fusion strategies implemented by classification methods are incompatible with localization tasks. In this work, we propose a novel deep learning-based framework for the fusion of multimodal data with heterogeneous dimensionality (e.g., 3D+2D) that is compatible with localization tasks. The proposed framework extracts the features of the different modalities and projects them into the common feature subspace. The projected features are then fused and further processed to obtain the final prediction. The framework was validated on the following tasks: segmentation of geographic atrophy (GA), a late-stage manifestation of age-related macular degeneration, and segmentation of retinal blood vessels (RBV) in multimodal retinal imaging. Our results show that the proposed method outperforms the state-of-the-art monomodal methods on GA and RBV segmentation by up to 3.10% and 4.64% Dice, respectively.","sentences":["The use of multimodal imaging has led to significant improvements in the diagnosis and treatment of many diseases.","Similar to clinical practice, some works have demonstrated the benefits of multimodal fusion for automatic segmentation and classification using deep learning-based methods.","However, current segmentation methods are limited to fusion of modalities with the same dimensionality (e.g., 3D+3D, 2D+2D), which is not always possible, and the fusion strategies implemented by classification methods are incompatible with localization tasks.","In this work, we propose a novel deep learning-based framework for the fusion of multimodal data with heterogeneous dimensionality (e.g., 3D+2D) that is compatible with localization tasks.","The proposed framework extracts the features of the different modalities and projects them into the common feature subspace.","The projected features are then fused and further processed to obtain the final prediction.","The framework was validated on the following tasks: segmentation of geographic atrophy (GA), a late-stage manifestation of age-related macular degeneration, and segmentation of retinal blood vessels (RBV) in multimodal retinal imaging.","Our results show that the proposed method outperforms the state-of-the-art monomodal methods on GA and RBV segmentation by up to 3.10% and 4.64% Dice, respectively."],"url":"http://arxiv.org/abs/2402.01311v1","category":"cs.CV"}
{"created":"2024-02-02 10:25:39","title":"UCVC: A Unified Contextual Video Compression Framework with Joint P-frame and B-frame Coding","abstract":"This paper presents a learned video compression method in response to video compression track of the 6th Challenge on Learned Image Compression (CLIC), at DCC 2024.Specifically, we propose a unified contextual video compression framework (UCVC) for joint P-frame and B-frame coding. Each non-intra frame refers to two neighboring decoded frames, which can be either both from the past for P-frame compression, or one from the past and one from the future for B-frame compression. In training stage, the model parameters are jointly optimized with both P-frames and B-frames. Benefiting from the designs, the framework can support both P-frame and B-frame coding and achieve comparable compression efficiency with that specifically designed for P-frame or B-frame.As for challenge submission, we report the optimal compression efficiency by selecting appropriate frame types for each test sequence. Our team name is PKUSZ-LVC.","sentences":["This paper presents a learned video compression method in response to video compression track of the 6th Challenge on Learned Image Compression (CLIC), at DCC 2024.Specifically, we propose a unified contextual video compression framework (UCVC) for joint P-frame and B-frame coding.","Each non-intra frame refers to two neighboring decoded frames, which can be either both from the past for P-frame compression, or one from the past and one from the future for B-frame compression.","In training stage, the model parameters are jointly optimized with both P-frames and B-frames.","Benefiting from the designs, the framework can support both P-frame and B-frame coding and achieve comparable compression efficiency with that specifically designed for P-frame or B-frame.","As for challenge submission, we report the optimal compression efficiency by selecting appropriate frame types for each test sequence.","Our team name is PKUSZ-LVC."],"url":"http://arxiv.org/abs/2402.01289v1","category":"cs.CV"}
{"created":"2024-02-02 10:00:51","title":"On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification","abstract":"In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data. Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning. While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics. This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification. Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks. Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks.","sentences":["In recent years, self-supervised learning has excelled for its capacity to learn robust feature representations from unlabelled data.","Networks pretrained through self-supervision serve as effective feature extractors for downstream tasks, including Few-Shot Learning.","While the evaluation of unsupervised approaches for few-shot learning is well-established in imagery, it is notably absent in acoustics.","This study addresses this gap by assessing large-scale self-supervised models' performance in few-shot audio classification.","Additionally, we explore the relationship between a model's few-shot learning capability and other downstream task benchmarks.","Our findings reveal state-of-the-art performance in some few-shot problems such as SpeechCommandsv2, as well as strong correlations between speech-based few-shot problems and various downstream audio tasks."],"url":"http://arxiv.org/abs/2402.01274v1","category":"cs.SD"}
{"created":"2024-02-02 09:19:30","title":"Amorphous Boron Nitride as a Diffusion Barrier to Cu Atoms","abstract":"This study focuses on amorphous boron nitride ($\\rm \\alpha$-BN) as a novel diffusion barrier for advanced semiconductor technology, particularly addressing the critical challenge of copper diffusion in back-end-of-logic (BEOL) interconnects. Owing to its ultralow dielectric constant and robust barrier properties, $\\rm \\alpha$-BN is examined as an alternative to conventional low-k dielectrics. The investigation primarily employs theoretical modeling, using a Gaussian Approximation Potential, to simulate and understand the atomic-level interactions and barrier mechanisms of $\\rm \\alpha$-BN. This machine learning-based approach allows for realistic simulations of its amorphous structure, enabling the exploration of the impact of different film morphologies on barrier efficacy. Complementing the theoretical study, experimental analyses are conducted on Plasma-Enhanced Chemical Vapor Deposition (PECVD) grown $\\rm \\alpha$-BN films, evaluating their effectiveness in preventing copper diffusion in silicon-based substrates. The results from both the theoretical and experimental investigations highlight the potential of $\\rm \\alpha$-BN as a highly effective diffusion barrier, suitable for integration in nanoelectronics. This research not only proposes $\\rm \\alpha$-BN as a promising candidate for BEOL interconnects but also demonstrates the synergy of advanced computational models and experimental methods in material innovation for semiconductor applications.","sentences":["This study focuses on amorphous boron nitride ($\\rm \\alpha$-BN) as a novel diffusion barrier for advanced semiconductor technology, particularly addressing the critical challenge of copper diffusion in back-end-of-logic (BEOL) interconnects.","Owing to its ultralow dielectric constant and robust barrier properties, $\\rm \\alpha$-BN is examined as an alternative to conventional low-k dielectrics.","The investigation primarily employs theoretical modeling, using a Gaussian Approximation Potential, to simulate and understand the atomic-level interactions and barrier mechanisms of $\\rm \\alpha$-BN.","This machine learning-based approach allows for realistic simulations of its amorphous structure, enabling the exploration of the impact of different film morphologies on barrier efficacy.","Complementing the theoretical study, experimental analyses are conducted on Plasma-Enhanced Chemical Vapor Deposition (PECVD) grown $\\rm \\alpha$-BN films, evaluating their effectiveness in preventing copper diffusion in silicon-based substrates.","The results from both the theoretical and experimental investigations highlight the potential of $\\rm \\alpha$-BN as a highly effective diffusion barrier, suitable for integration in nanoelectronics.","This research not only proposes $\\rm \\alpha$-BN as a promising candidate for BEOL interconnects but also demonstrates the synergy of advanced computational models and experimental methods in material innovation for semiconductor applications."],"url":"http://arxiv.org/abs/2402.01251v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-02 17:22:17","title":"On the efficient computation of smoothness indicators for a class of WENO reconstructions","abstract":"Common smoothness indicators used in Weighted Essentially Non\\--Os\\-cil\\-la\\-to\\-ry (WENO) reconstructions [Jiang, G.S., Shu, C.W.: Efficient implementation of {Weighted} {ENO} schemes, J.\\ Comput.\\ Phys. \\textbf{126}, 202--228 (1996)] have quadratic cost with respect to the order. A set of novel smoothness indicators with linear cost of computation with respect to the order is presented. These smoothness indicators can be used in the context of schemes of the type introduced by Yamaleev and Carpenter [Yamaleev, N.K., Carpenter, M.H.: A systematic methodology to for constructing high-order energy stable WENO schemes. J. Comput. Phys. \\textbf{228}(11), 4248-4272 (2009)]. The accuracy properties of the resulting non-linear weights are the same as those arising from using the traditional Jiang-Shu smoothness indicators in Yamaleev-Carpenter-type reconstructions. The increase of the efficiency and ease of implementation are shown.","sentences":["Common smoothness indicators used in Weighted Essentially Non\\--Os\\-cil\\-la\\-to\\-ry (WENO) reconstructions","[Jiang, G.S., Shu, C.W.: Efficient implementation of {Weighted} {ENO} schemes, J.\\ Comput.\\ Phys. \\textbf{126}, 202--228 (1996)] have quadratic cost with respect to the order.","A set of novel smoothness indicators with linear cost of computation with respect to the order is presented.","These smoothness indicators can be used in the context of schemes of the type introduced by Yamaleev and Carpenter","[Yamaleev, N.K., Carpenter, M.H.:","A systematic methodology to for constructing high-order energy stable WENO schemes.","J. Comput.","Phys. \\textbf{228}(11), 4248-4272 (2009)].","The accuracy properties of the resulting non-linear weights are the same as those arising from using the traditional Jiang-Shu smoothness indicators in Yamaleev-Carpenter-type reconstructions.","The increase of the efficiency and ease of implementation are shown."],"url":"http://arxiv.org/abs/2402.01583v1","category":"math.NA"}
{"created":"2024-02-02 17:17:42","title":"How Paralingual are Paralinguistic Representations? A Case Study in Speech Emotion Recognition","abstract":"Pre-trained Models (PTMs) have facilitated substantial progress in the field of Speech Emotion Recognition (SER). SER is an area with applications ranging from HumanComputer Interaction to Healthcare. Recent studies have leveraged various PTM representations as input features for downstream models for SER. PTM specifically pre-trained for paralinguistic tasks have obtained state-of-the-art (SOTA) performance for SER. However, such PTM haven't been evaluated for SER in multilingual settings and experimented only with English. So, we fill this gap, by performing a comprehensive comparative study of five PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages. Representations from TRILLsson achieved the best performance among all the PTMs. This demonstrates that TRILLsson is able to effectively capture the various paralinguistic features from speech data for better SER. We also show that downstream models using TRILLsson representations achieve SOTA performance in terms of accuracy across various multi-lingual datasets.","sentences":["Pre-trained Models (PTMs) have facilitated substantial progress in the field of Speech Emotion Recognition (SER).","SER is an area with applications ranging from HumanComputer Interaction to Healthcare.","Recent studies have leveraged various PTM representations as input features for downstream models for SER.","PTM specifically pre-trained for paralinguistic tasks have obtained state-of-the-art (SOTA) performance for SER.","However, such PTM haven't been evaluated for SER in multilingual settings and experimented only with English.","So, we fill this gap, by performing a comprehensive comparative study of five PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages.","Representations from TRILLsson achieved the best performance among all the PTMs.","This demonstrates that TRILLsson is able to effectively capture the various paralinguistic features from speech data for better SER.","We also show that downstream models using TRILLsson representations achieve SOTA performance in terms of accuracy across various multi-lingual datasets."],"url":"http://arxiv.org/abs/2402.01579v1","category":"eess.AS"}
{"created":"2024-02-02 17:07:17","title":"Controllable frequency tunability and parabolic-like threshold current behavior in spin Hall nano-oscillators","abstract":"We investigate the individual impacts of critical magnetodynamical parameters-effective magnetization and magnetic damping-on the auto-oscillation characteristics of nano-constriction-based Spin Hall Nano-Oscillators (SHNOs). Our micromagnetic simulations unveil a distinctive non-monotonic relationship between current and auto-oscillation frequency in out-of-plane magnetic fields. The influence of effective magnetization on frequency tunability varies with out-of-plane field strengths. At large out-of-plane fields, the frequency tunability is predominantly governed by effective magnetization, achieving a current tunability of 1 GHz/mA-four times larger than that observed at the lowest effective magnetization. Conversely, at low out-of-plane fields, although a remarkably high-frequency tunability of 4 GHz/mA is observed, the effective magnetization alters the onset of the transition from a linear-like mode to a spin-wave bullet mode. Magnetic damping primarily affects the threshold current with negligible impact on auto-oscillation frequency tunability. The threshold current scales linearly with increased magnetic damping at a constant out-of-plane field but exhibits a parabolic behavior with variations in out-of-plane fields. This behavior is attributed to the qualitatively distinct evolution of the auto-oscillation mode across different out-of-plane field values. Our study not only extends the versatility of SHNOs for oscillator-based neuromorphic computing with controllable frequency tunability but also unveils the intricate auto-oscillation dynamics in out-of-plane fields.","sentences":["We investigate the individual impacts of critical magnetodynamical parameters-effective magnetization and magnetic damping-on the auto-oscillation characteristics of nano-constriction-based Spin Hall Nano-Oscillators (SHNOs).","Our micromagnetic simulations unveil a distinctive non-monotonic relationship between current and auto-oscillation frequency in out-of-plane magnetic fields.","The influence of effective magnetization on frequency tunability varies with out-of-plane field strengths.","At large out-of-plane fields, the frequency tunability is predominantly governed by effective magnetization, achieving a current tunability of 1 GHz/mA-four times larger than that observed at the lowest effective magnetization.","Conversely, at low out-of-plane fields, although a remarkably high-frequency tunability of 4 GHz/mA is observed, the effective magnetization alters the onset of the transition from a linear-like mode to a spin-wave bullet mode.","Magnetic damping primarily affects the threshold current with negligible impact on auto-oscillation frequency tunability.","The threshold current scales linearly with increased magnetic damping at a constant out-of-plane field but exhibits a parabolic behavior with variations in out-of-plane fields.","This behavior is attributed to the qualitatively distinct evolution of the auto-oscillation mode across different out-of-plane field values.","Our study not only extends the versatility of SHNOs for oscillator-based neuromorphic computing with controllable frequency tunability but also unveils the intricate auto-oscillation dynamics in out-of-plane fields."],"url":"http://arxiv.org/abs/2402.01570v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-02 16:50:20","title":"Confinement in the Transverse Field Ising model on the Heavy Hex lattice","abstract":"We study the emergence of confinement in the transverse field Ising model on a decorated hexagonal lattice. Using an infinite tensor network state optimised with belief propagation we show how a quench from a broken symmetry state leads to striking nonthermal behaviour underpinned by persistent oscillations and saturation of the entanglement entropy. We explain this phenomenon by constructing a minimal model based on the confinement of elementary excitations, which take the form of various flavors of hadronic quasiparticles due to the unique structure of the lattice. Our model is in excellent agreement with our numerical results. For quenches to larger values of the transverse field and/or from non-symmetry broken states, our numerical results displays the expected signatures of thermalisation: a linear growth of entanglement entropy in time, propagation of correlations and the saturation of observables to their thermal averages. These results provide a physical explanation for the unexpected simulability of a recent large scale quantum computation.","sentences":["We study the emergence of confinement in the transverse field Ising model on a decorated hexagonal lattice.","Using an infinite tensor network state optimised with belief propagation we show how a quench from a broken symmetry state leads to striking nonthermal behaviour underpinned by persistent oscillations and saturation of the entanglement entropy.","We explain this phenomenon by constructing a minimal model based on the confinement of elementary excitations, which take the form of various flavors of hadronic quasiparticles due to the unique structure of the lattice.","Our model is in excellent agreement with our numerical results.","For quenches to larger values of the transverse field and/or from non-symmetry broken states, our numerical results displays the expected signatures of thermalisation: a linear growth of entanglement entropy in time, propagation of correlations and the saturation of observables to their thermal averages.","These results provide a physical explanation for the unexpected simulability of a recent large scale quantum computation."],"url":"http://arxiv.org/abs/2402.01558v1","category":"quant-ph"}
{"created":"2024-02-02 16:44:52","title":"Hardware Trojans in Quantum Circuits, Their Impacts, and Defense","abstract":"The reliability of the outcome of a quantum circuit in near-term noisy quantum computers depends on the gate count and depth for a given problem. Circuits with a short depth and lower gate count can yield the correct solution more often than the variant with a higher gate count and depth. To work successfully for Noisy Intermediate Scale Quantum (NISQ) computers, quantum circuits need to be optimized efficiently using a compiler that decomposes high-level gates to native gates of the hardware. Many 3rd party compilers are being developed for lower compilation time, reduced circuit depth, and lower gate count for large quantum circuits. Such compilers, or even a specific release version of a compiler that is otherwise trustworthy, may be unreliable and give rise to security risks such as insertion of a quantum trojan during compilation that evades detection due to the lack of a golden/Oracle model in quantum computing. Trojans may corrupt the functionality to give flipped probabilities of basis states, or result in a lower probability of correct basis states in the output. In this paper, we investigate and discuss the impact of a single qubit Trojan (we have chosen a Hadamard gate and a NOT gate) inserted one at a time at various locations in benchmark quantum circuits without changing the the depth of the circuit. Results indicate an average of 16.18% degradation for the Hadamard Trojan without noise, and 7.78% with noise. For the NOT Trojan (with noise) there is 14.6% degradation over all possible inputs. We then discuss the detection of such Trojans in a quantum circuit using CNN-based classifier achieving an accuracy of 90%.","sentences":["The reliability of the outcome of a quantum circuit in near-term noisy quantum computers depends on the gate count and depth for a given problem.","Circuits with a short depth and lower gate count can yield the correct solution more often than the variant with a higher gate count and depth.","To work successfully for Noisy Intermediate Scale Quantum (NISQ) computers, quantum circuits need to be optimized efficiently using a compiler that decomposes high-level gates to native gates of the hardware.","Many 3rd party compilers are being developed for lower compilation time, reduced circuit depth, and lower gate count for large quantum circuits.","Such compilers, or even a specific release version of a compiler that is otherwise trustworthy, may be unreliable and give rise to security risks such as insertion of a quantum trojan during compilation that evades detection due to the lack of a golden/Oracle model in quantum computing.","Trojans may corrupt the functionality to give flipped probabilities of basis states, or result in a lower probability of correct basis states in the output.","In this paper, we investigate and discuss the impact of a single qubit Trojan (we have chosen a Hadamard gate and a NOT gate) inserted one at a time at various locations in benchmark quantum circuits without changing the the depth of the circuit.","Results indicate an average of 16.18% degradation for the Hadamard Trojan without noise, and 7.78% with noise.","For the NOT Trojan (with noise) there is 14.6% degradation over all possible inputs.","We then discuss the detection of such Trojans in a quantum circuit using CNN-based classifier achieving an accuracy of 90%."],"url":"http://arxiv.org/abs/2402.01552v1","category":"cs.CR"}
{"created":"2024-02-02 16:41:36","title":"Quantum advantage in zero-error function computation with side information","abstract":"We consider the problem of zero-error function computation with side information. Alice has a source $X$ and Bob has correlated source $Y$ and they can communicate via either classical or a quantum channel. Bob wants to calculate $f(X,Y)$ with zero error. We aim to characterize the minimum amount of information that Alice needs to send to Bob for this to happen with zero-error. In the classical setting, this quantity depends on the asymptotic growth of $\\chi(G^{(m)})$, the chromatic number of an appropriately defined $m$-instance \"confusion graph\". In this work we present structural characterizations of $G^{(m)}$ and demonstrate two function computation scenarios that have the same single-instance confusion graph. However, in one case there a strict advantage in using quantum transmission as against classical transmission, whereas there is no such advantage in the other case.","sentences":["We consider the problem of zero-error function computation with side information.","Alice has a source $X$ and Bob has correlated source $Y$ and they can communicate via either classical or a quantum channel.","Bob wants to calculate $f(X,Y)$ with zero error.","We aim to characterize the minimum amount of information that Alice needs to send to Bob for this to happen with zero-error.","In the classical setting, this quantity depends on the asymptotic growth of $\\chi(G^{(m)})$, the chromatic number of an appropriately defined $m$-instance \"confusion graph\".","In this work we present structural characterizations of $G^{(m)}$ and demonstrate two function computation scenarios that have the same single-instance confusion graph.","However, in one case there a strict advantage in using quantum transmission as against classical transmission, whereas there is no such advantage in the other case."],"url":"http://arxiv.org/abs/2402.01549v1","category":"quant-ph"}
{"created":"2024-02-02 16:14:50","title":"Continuously Distributing Entanglement in Quantum Networks with Regular Topologies","abstract":"Small interconnected quantum processors can collaborate to tackle quantum computational problems that typically demand more capable devices. These linked processors, referred to as quantum nodes, can use shared entangled states to execute nonlocal operations. As a consequence, understanding how to distribute entangled states among nodes is essential for developing hardware and software. We analyze a protocol where entanglement is continuously distributed among nodes that are physically arranged in a regular pattern: a chain, a honeycomb lattice, a square grid, and a triangular lattice. These regular patterns allow for the modular expansion of networks for large-scale distributed quantum computing. Within the entanglement distribution protocol, nodes can fix the probability of attempting entanglement swaps to trade off multiple entangled states shared with neighboring nodes for fewer entangled states shared with non-neighboring nodes. We evaluate the protocol's performance using the virtual neighborhood size -- a metric indicating the number of other nodes with which a given node shares entangled states. Employing numerical methods, we find that nodes must perform more swaps to maximize the virtual neighborhood size when coherence times are short. In a chain network, the virtual neighborhood size's dependence on swap attempt probability differs for each node based on its distance from the end of the chain. Conversely, all nodes in the square grid exhibit a qualitatively similar dependence of the virtual neighborhood size on the swap probability.","sentences":["Small interconnected quantum processors can collaborate to tackle quantum computational problems that typically demand more capable devices.","These linked processors, referred to as quantum nodes, can use shared entangled states to execute nonlocal operations.","As a consequence, understanding how to distribute entangled states among nodes is essential for developing hardware and software.","We analyze a protocol where entanglement is continuously distributed among nodes that are physically arranged in a regular pattern: a chain, a honeycomb lattice, a square grid, and a triangular lattice.","These regular patterns allow for the modular expansion of networks for large-scale distributed quantum computing.","Within the entanglement distribution protocol, nodes can fix the probability of attempting entanglement swaps to trade off multiple entangled states shared with neighboring nodes for fewer entangled states shared with non-neighboring nodes.","We evaluate the protocol's performance using the virtual neighborhood size -- a metric indicating the number of other nodes with which a given node shares entangled states.","Employing numerical methods, we find that nodes must perform more swaps to maximize the virtual neighborhood size when coherence times are short.","In a chain network, the virtual neighborhood size's dependence on swap attempt probability differs for each node based on its distance from the end of the chain.","Conversely, all nodes in the square grid exhibit a qualitatively similar dependence of the virtual neighborhood size on the swap probability."],"url":"http://arxiv.org/abs/2402.01527v1","category":"quant-ph"}
{"created":"2024-02-02 14:58:48","title":"Eigenvalues of the magnetic Dirichlet Laplacian with constant magnetic field on discs in the strong field limit","abstract":"We consider the magnetic Dirichlet Laplacian with constant magnetic field on domains of finite measure. In the case of a disc, we prove that the eigenvalue branches with respect to the field strength behave asymptotically linear with an exponentially small remainder term as the field strength goes to infinity. We compute the asymptotic expression for this remainder term. Furthermore, we show that for sufficiently large magnetic field strengths, the spectral bound corresponding to the P\\'olya conjecture for the non-magnetic Dirichlet Laplacian is violated up to a sharp excess factor which is independent of the domain.","sentences":["We consider the magnetic Dirichlet Laplacian with constant magnetic field on domains of finite measure.","In the case of a disc, we prove that the eigenvalue branches with respect to the field strength behave asymptotically linear with an exponentially small remainder term as the field strength goes to infinity.","We compute the asymptotic expression for this remainder term.","Furthermore, we show that for sufficiently large magnetic field strengths, the spectral bound corresponding to the P\\'olya conjecture for the non-magnetic Dirichlet Laplacian is violated up to a sharp excess factor which is independent of the domain."],"url":"http://arxiv.org/abs/2402.01474v1","category":"math.SP"}
{"created":"2024-02-02 14:42:09","title":"The Queen of England is not England's Queen: On the Lack of Factual Coherency in PLMs","abstract":"Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases. Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs. In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity. This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them. Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement. This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases.","sentences":["Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches their representations and justifies their use as knowledge bases.","Previous work has focused on probing PLMs for factual knowledge by measuring how often they can correctly predict an object entity given a subject and a relation, and improving fact retrieval by optimizing the prompts used for querying PLMs.","In this work, we consider a complementary aspect, namely the coherency of factual knowledge in PLMs, i.e., how often can PLMs predict the subject entity given its initial prediction of the object entity.","This goes beyond evaluating how much PLMs know, and focuses on the internal state of knowledge inside them.","Our results indicate that PLMs have low coherency using manually written, optimized and paraphrased prompts, but including an evidence paragraph leads to substantial improvement.","This shows that PLMs fail to model inverse relations and need further enhancements to be able to handle retrieving facts from their parameters in a coherent manner, and to be considered as knowledge bases."],"url":"http://arxiv.org/abs/2402.01453v1","category":"cs.CL"}
{"created":"2024-02-02 14:26:35","title":"Theory of Electron Spin Resonance Spectroscopy in Scanning Tunneling Microscope","abstract":"The combination of scanning tunneling microscopy (STM) and electron spin resonance (ESR) has emerged as a powerful and innovative tool for detecting spin excitations and spin-spin interactions in surface-adsorbed atoms and molecules. However, the origin of the STM-ESR signal and the underlying mechanisms behind the important features of the measured spectra have remained elusive. To address this perplexity, we perform precise numerical simulations of STM-ESR spectra for a single Ti adatom and a Ti dimer, achieving excellent agreement with experimental observations. We further develop an analytic theory that elucidates the origin of the signal as well as the essential features in the measured spectra. These theoretical insights establish a solid foundation for the on-demand detection and manipulation of atomic-scale spin states, with promising implications for cutting-edge applications in spin sensing, quantum information, and quantum computing.","sentences":["The combination of scanning tunneling microscopy (STM) and electron spin resonance (ESR) has emerged as a powerful and innovative tool for detecting spin excitations and spin-spin interactions in surface-adsorbed atoms and molecules.","However, the origin of the STM-ESR signal and the underlying mechanisms behind the important features of the measured spectra have remained elusive.","To address this perplexity, we perform precise numerical simulations of STM-ESR spectra for a single Ti adatom and a Ti dimer, achieving excellent agreement with experimental observations.","We further develop an analytic theory that elucidates the origin of the signal as well as the essential features in the measured spectra.","These theoretical insights establish a solid foundation for the on-demand detection and manipulation of atomic-scale spin states, with promising implications for cutting-edge applications in spin sensing, quantum information, and quantum computing."],"url":"http://arxiv.org/abs/2402.01435v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-02 13:03:20","title":"Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate Distortion Optimization","abstract":"Volumetric videos, benefiting from immersive 3D realism and interactivity, hold vast potential for various applications, while the tremendous data volume poses significant challenges for compression. Recently, NeRF has demonstrated remarkable potential in volumetric video compression thanks to its simple representation and powerful 3D modeling capabilities, where a notable work is ReRF. However, ReRF separates the modeling from compression process, resulting in suboptimal compression efficiency. In contrast, in this paper, we propose a volumetric video compression method based on dynamic NeRF in a more compact manner. Specifically, we decompose the NeRF representation into the coefficient fields and the basis fields, incrementally updating the basis fields in the temporal domain to achieve dynamic modeling. Additionally, we perform end-to-end joint optimization on the modeling and compression process to further improve the compression efficiency. Extensive experiments demonstrate that our method achieves higher compression efficiency compared to ReRF on various datasets.","sentences":["Volumetric videos, benefiting from immersive 3D realism and interactivity, hold vast potential for various applications, while the tremendous data volume poses significant challenges for compression.","Recently, NeRF has demonstrated remarkable potential in volumetric video compression thanks to its simple representation and powerful 3D modeling capabilities, where a notable work is ReRF.","However, ReRF separates the modeling from compression process, resulting in suboptimal compression efficiency.","In contrast, in this paper, we propose a volumetric video compression method based on dynamic NeRF in a more compact manner.","Specifically, we decompose the NeRF representation into the coefficient fields and the basis fields, incrementally updating the basis fields in the temporal domain to achieve dynamic modeling.","Additionally, we perform end-to-end joint optimization on the modeling and compression process to further improve the compression efficiency.","Extensive experiments demonstrate that our method achieves higher compression efficiency compared to ReRF on various datasets."],"url":"http://arxiv.org/abs/2402.01380v1","category":"cs.CV"}
{"created":"2024-02-02 12:56:55","title":"A micro-opto-mechanical glass interferometer for megahertz modulation of optical signals","abstract":"Waveguide-based interferometric circuits are widely employed in optical communications, sensing and computing applications. In particular, glass-based devices are appealing due to the transparency and bio-compatibility of this substrate, or where low-loss interfacing with fiber networks is required. However, fast electro-optic phase modulation is hard to achieve in glass materials. Here, we demonstrate an optical phase and intensity modulator in glass, working in the megahertz range. This modulator exploits the elasto-optic effect inside a mechanical microstructure, brought to oscillation at resonance, and is entirely realized by femtosecond laser micromachining. In detail, we demonstrate 23-dB optical intensity modulation at 1.17 MHz, with an internal optical loss of the phase-modulator component as low as 0.04 dB.","sentences":["Waveguide-based interferometric circuits are widely employed in optical communications, sensing and computing applications.","In particular, glass-based devices are appealing due to the transparency and bio-compatibility of this substrate, or where low-loss interfacing with fiber networks is required.","However, fast electro-optic phase modulation is hard to achieve in glass materials.","Here, we demonstrate an optical phase and intensity modulator in glass, working in the megahertz range.","This modulator exploits the elasto-optic effect inside a mechanical microstructure, brought to oscillation at resonance, and is entirely realized by femtosecond laser micromachining.","In detail, we demonstrate 23-dB optical intensity modulation at 1.17 MHz, with an internal optical loss of the phase-modulator component as low as 0.04 dB."],"url":"http://arxiv.org/abs/2402.01374v1","category":"physics.optics"}
{"created":"2024-02-02 11:20:55","title":"A note on some bounds between cubic spline interpolants depending on the boundary conditions: Application to a monotonicity property","abstract":"In the context of cubic splines, the authors have contributed to a recent paper dealing with the computation of nonlinear derivatives at the interior nodes so that monotonicity is enforced while keeping the order of approximation of the spline as high as possible. During the review process of that paper, one of the reviewers raised the question of whether a cubic spline interpolating monotone data could be forced to preserve monotonicity by imposing suitable values of the first derivative at the endpoints. Albeit a negative answer appears to be intuitive, we have found no results regarding this fact. In this short work we prove that the answer to that question is actually negative.","sentences":["In the context of cubic splines, the authors have contributed to a recent paper dealing with the computation of nonlinear derivatives at the interior nodes so that monotonicity is enforced while keeping the order of approximation of the spline as high as possible.","During the review process of that paper, one of the reviewers raised the question of whether a cubic spline interpolating monotone data could be forced to preserve monotonicity by imposing suitable values of the first derivative at the endpoints.","Albeit a negative answer appears to be intuitive, we have found no results regarding this fact.","In this short work we prove that the answer to that question is actually negative."],"url":"http://arxiv.org/abs/2402.01324v1","category":"math.NA"}
{"created":"2024-02-02 11:17:35","title":"A Review of Quantum communication using high-dimensional Hilbert spaces","abstract":"In this project we examine several different quantum key distribution protocols which we divide into ones utilizing qubits whose Hilbert spaces are two dimensional and ones whose Hilbert space dimension is greater than two, these units of data in quantum computers are known as qudits and in the papers we'll examine are implemented using the orbital angular momentum of twisted photons. In sections [3] and [4] the specific procedures of each protocol are briefly described and followed by an examination of the theoretical and experimental merits of each protocol. These merits are measured in the bit error rate tolerance $e_b$, which quantifies the maximum channel noise and the key rate $R$ which quantifies the rate at which data is transferred in the protocol. In section [7] we present a unified view of all the relevant data for the different protocols, and argue for the benefits and drawbacks of the different protocols for different applications.","sentences":["In this project we examine several different quantum key distribution protocols which we divide into ones utilizing qubits whose Hilbert spaces are two dimensional and ones whose Hilbert space dimension is greater than two, these units of data in quantum computers are known as qudits and in the papers we'll examine are implemented using the orbital angular momentum of twisted photons.","In sections [3] and [4] the specific procedures of each protocol are briefly described and followed by an examination of the theoretical and experimental merits of each protocol.","These merits are measured in the bit error rate tolerance $e_b$, which quantifies the maximum channel noise and the key rate $R$ which quantifies the rate at which data is transferred in the protocol.","In section [7] we present a unified view of all the relevant data for the different protocols, and argue for the benefits and drawbacks of the different protocols for different applications."],"url":"http://arxiv.org/abs/2402.01319v1","category":"quant-ph"}
{"created":"2024-02-02 10:20:36","title":"SDSS-IV MaNGA: Calibration of astrophysical line-widths in the H\u03b1 region using HexPak observations","abstract":"We have re-observed $\\rm\\sim$40 low-inclination, star-forming galaxies from the MaNGA survey ($\\upsigma\\sim65$~\\kms) at $\\sim$6.5 times higher spectral resolution ($\\upsigma\\sim10$~\\kms) using the HexPak integral field unit on the WIYN 3.5m telescope. The aim of these observations is to calibrate MaNGA's instrumental resolution and to characterize turbulence in the warm interstellar medium and ionized galactic outflows. Here we report the results for the H$\\rm\\upalpha$ region observations as they pertain to the calibration of MaNGA's spectral resolution. Remarkably, we find that the previously-reported MaNGA line-spread-function (LSF) Gaussian width is systematically underestimated by only 1\\%. The LSF increase modestly reduces the characteristic dispersion of HII regions-dominated spectra sampled at 1-2 kpc spatial scales from 23 to 20 km s$^{-1}$ in our sample, or a 25\\% decrease in the random-motion kinetic energy. This commensurately lowers the dispersion zeropoint in the relation between line-width and star-formation rate surface-density in galaxies sampled on the same spatial scale. This modest zero-point shift does not appear to alter the power-law slope in the relation between line-width and star-formation rate surface-density. We also show that adopting a scheme whereby corrected line-widths are computed as the square root of the median of the difference in the squared measured line width and the squared LSF Gaussian avoids biases and allows for lower SNR data to be used reliably.","sentences":["We have re-observed $\\rm\\sim$40 low-inclination, star-forming galaxies from the MaNGA survey ($\\upsigma\\sim65$~\\kms) at $\\sim$6.5 times higher spectral resolution ($\\upsigma\\sim10$~\\kms) using the HexPak integral field unit on the WIYN 3.5m telescope.","The aim of these observations is to calibrate MaNGA's instrumental resolution and to characterize turbulence in the warm interstellar medium and ionized galactic outflows.","Here we report the results for the H$\\rm\\upalpha$ region observations as they pertain to the calibration of MaNGA's spectral resolution.","Remarkably, we find that the previously-reported MaNGA line-spread-function (LSF) Gaussian width is systematically underestimated by only 1\\%.","The LSF increase modestly reduces the characteristic dispersion of HII regions-dominated spectra sampled at 1-2 kpc spatial scales from 23 to 20 km s$^{-1}$ in our sample, or a 25\\% decrease in the random-motion kinetic energy.","This commensurately lowers the dispersion zeropoint in the relation between line-width and star-formation rate surface-density in galaxies sampled on the same spatial scale.","This modest zero-point shift does not appear to alter the power-law slope in the relation between line-width and star-formation rate surface-density.","We also show that adopting a scheme whereby corrected line-widths are computed as the square root of the median of the difference in the squared measured line width and the squared LSF Gaussian avoids biases and allows for lower SNR data to be used reliably."],"url":"http://arxiv.org/abs/2402.01284v1","category":"astro-ph.GA"}
{"created":"2024-02-02 08:38:34","title":"Forcing with Language Fragments, Extending Namba Forcing, and Models of Theories with Constraints in Interpretation","abstract":"We develop a forcing framework based on the idea of amalgamating language fragments into a theory with a canonical Henkin model. We then demonstrate the usefulness of this framework by applying it to both the extended Namba problem and the analysis of models of certain theories with constraints in interpretation (TCIs). The foundations for a theory of TCIs and their models are laid in parallel to the development of our framework, and are of independent interest.","sentences":["We develop a forcing framework based on the idea of amalgamating language fragments into a theory with a canonical Henkin model.","We then demonstrate the usefulness of this framework by applying it to both the extended Namba problem and the analysis of models of certain theories with constraints in interpretation (TCIs).","The foundations for a theory of TCIs and their models are laid in parallel to the development of our framework, and are of independent interest."],"url":"http://arxiv.org/abs/2402.01213v1","category":"math.LO"}
{"created":"2024-02-02 18:06:31","title":"Convergence Tests of Self-Interacting Dark Matter Simulations","abstract":"Self-interacting dark matter (SIDM) theory predicts that dark matter halos experience core-collapse, a process where the halo's inner region rapidly increases in density and decreases in size. The N-body simulations used to study this process can suffer from numerical errors when simulation parameters are selected incorrectly. Optimal choices for simulation parameters are well studied for cold dark matter (CDM), but are not deeply understood when self-interactions are included. In order to perform reliable N-body simulations and model core-collapse accurately we must understand the potential numerical errors, how to diagnose them, and what parameter selections must be made to reduce them. We use the $\\texttt{Arepo}$ N-body code to perform convergence tests of core-collapsing SIDM halos across a range of halo concentrations and SIDM cross-sections, and quantify potential numerical issues related to mass resolution, timestep size, and gravitational softening length. Our tests discover that halos with fewer than $10^5$ simulation particles, a resolution typically not met by subhalos in N-body simulations, suffer from significant discreteness noise that leads to variation and extreme outliers in the collapse rate. At our lowest resolution of $N=10^4$ particles, this collapse time variation can reach as high as 20%. At this low resolution we also find a bias in collapse times and a small number of extreme outliers. Additionally, we find that simulations which run far beyond the age of the Universe, which have been used to calibrate SIDM gravothermal fluid models in previous work, have a sensitivity to the timestep size that is not present in shorter simulations or simulations using only CDM. Our work shows that choices of simulation parameters that yield converged results for some halo masses and SIDM models do not necessarily yield convergence for others.","sentences":["Self-interacting dark matter (SIDM) theory predicts that dark matter halos experience core-collapse, a process where the halo's inner region rapidly increases in density and decreases in size.","The N-body simulations used to study this process can suffer from numerical errors when simulation parameters are selected incorrectly.","Optimal choices for simulation parameters are well studied for cold dark matter (CDM), but are not deeply understood when self-interactions are included.","In order to perform reliable N-body simulations and model core-collapse accurately we must understand the potential numerical errors, how to diagnose them, and what parameter selections must be made to reduce them.","We use the $\\texttt{Arepo}$ N-body code to perform convergence tests of core-collapsing SIDM halos across a range of halo concentrations and SIDM cross-sections, and quantify potential numerical issues related to mass resolution, timestep size, and gravitational softening length.","Our tests discover that halos with fewer than $10^5$ simulation particles, a resolution typically not met by subhalos in N-body simulations, suffer from significant discreteness noise that leads to variation and extreme outliers in the collapse rate.","At our lowest resolution of $N=10^4$ particles, this collapse time variation can reach as high as 20%.","At this low resolution we also find a bias in collapse times and a small number of extreme outliers.","Additionally, we find that simulations which run far beyond the age of the Universe, which have been used to calibrate SIDM gravothermal fluid models in previous work, have a sensitivity to the timestep size that is not present in shorter simulations or simulations using only CDM.","Our work shows that choices of simulation parameters that yield converged results for some halo masses and SIDM models do not necessarily yield convergence for others."],"url":"http://arxiv.org/abs/2402.01604v1","category":"astro-ph.GA"}
{"created":"2024-02-02 17:55:47","title":"On the return probability of the simple random walk on Galton-Watson trees","abstract":"We consider the simple random walk on Galton-Watson trees with supercritical offspring distribution, conditioned on non-extinction. In case the offspring distribution has finite support, we prove an upper bound for the annealed return probability to the root which decays subexponentially in time with exponent 1/3. This exponent is optimal. Our result improves the previously known subexponential upper bound with exponent 1/5 by Piau [Ann. Probab. 26, 1016-1040 (1998)]. For offspring distributions with unbounded support but sufficiently fast decay, our method also yields improved subexponential upper bounds.","sentences":["We consider the simple random walk on Galton-Watson trees with supercritical offspring distribution, conditioned on non-extinction.","In case the offspring distribution has finite support, we prove an upper bound for the annealed return probability to the root which decays subexponentially in time with exponent 1/3.","This exponent is optimal.","Our result improves the previously known subexponential upper bound with exponent 1/5 by Piau","[Ann. Probab.","26, 1016-1040 (1998)].","For offspring distributions with unbounded support but sufficiently fast decay, our method also yields improved subexponential upper bounds."],"url":"http://arxiv.org/abs/2402.01600v1","category":"math.PR"}
{"created":"2024-02-02 15:27:00","title":"Constructing 100 M\u03a9 and 1 G\u03a9 Resistance Standards via Star-Mesh Transformations","abstract":"A recent mathematical framework for optimizing resistor networks to achieve values in the M{\\Omega} through G{\\Omega} levels was employed for two specific cases. Objectives here include proof of concept and identification of possible apparatus limitations for future experiments involving graphene-based quantum Hall array resistance standards. Using fractal-like, or recursive, features of the framework allows one to calculate and implement network designs with substantially lower-valued resistors. The cases of 100 M{\\Omega} and 1 G{\\Omega} demonstrate that, theoretically, one would not need more than 100 quantum Hall elements to achieve these high resistances.","sentences":["A recent mathematical framework for optimizing resistor networks to achieve values in the M{\\Omega} through G{\\Omega} levels was employed for two specific cases.","Objectives here include proof of concept and identification of possible apparatus limitations for future experiments involving graphene-based quantum Hall array resistance standards.","Using fractal-like, or recursive, features of the framework allows one to calculate and implement network designs with substantially lower-valued resistors.","The cases of 100 M{\\Omega} and 1 G{\\Omega} demonstrate that, theoretically, one would not need more than 100 quantum Hall elements to achieve these high resistances."],"url":"http://arxiv.org/abs/2402.01496v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-02 15:10:17","title":"Binomial-tree approximation for time-inconsistent stopping","abstract":"For time-inconsistent stopping in a one-dimensional diffusion setup, we investigate how to use discrete-time models to approximate the original problem. In particular, we consider the value function $V(\\cdot)$ induced by all mild equilibria in the continuous-time problem, as well as the value $V^h(\\cdot)$ associated with the equilibria in a binomial-tree setting with time step size $h$. We show that $\\lim_{h\\rightarrow 0+} V^h \\leq V$. We provide an example showing that the exact convergence may fail. Then we relax the set of equilibria and consider the value $V^h_{\\varepsilon}(\\cdot)$ induced by $\\varepsilon$-equilibria in the binomial-tree model. We prove that $\\lim_{\\varepsilon \\rightarrow 0+}\\lim_{h \\rightarrow 0+}V^h_{\\varepsilon} = V$.","sentences":["For time-inconsistent stopping in a one-dimensional diffusion setup, we investigate how to use discrete-time models to approximate the original problem.","In particular, we consider the value function $V(\\cdot)$ induced by all mild equilibria in the continuous-time problem, as well as the value $V^h(\\cdot)$ associated with the equilibria in a binomial-tree setting with time step size $h$. We show that $\\lim_{h\\rightarrow 0+} V^h \\leq V$.","We provide an example showing that the exact convergence may fail.","Then we relax the set of equilibria and consider the value $V^h_{\\varepsilon}(\\cdot)$ induced by $\\varepsilon$-equilibria in the binomial-tree model.","We prove that $\\lim_{\\varepsilon \\rightarrow 0+}\\lim_{h","\\rightarrow 0+}V^h_{\\varepsilon} = V$."],"url":"http://arxiv.org/abs/2402.01482v1","category":"math.OC"}
{"created":"2024-02-02 11:42:13","title":"Convergence rates for Backward SDEs driven by L\u00e9vy processes","abstract":"We consider L\\'evy processes that are approximated by compound Poisson processes and, correspondingly, BSDEs driven by L\\'evy processes that are approximated by BSDEs driven by their compound Poisson approximations. We are interested in the rate of convergence of the approximate BSDEs to the ones driven by the L\\'evy processes. The rate of convergence of the L\\'evy processes depends on the Blumenthal--Getoor index of the process. We derive the rate of convergence for the BSDEs in the $\\mathbb L^2$-norm and in the Wasserstein distance, and show that, in both cases, this equals the rate of convergence of the corresponding L\\'evy process, and thus is optimal.","sentences":["We consider L\\'evy processes that are approximated by compound Poisson processes and, correspondingly, BSDEs driven by L\\'evy processes that are approximated by BSDEs driven by their compound Poisson approximations.","We are interested in the rate of convergence of the approximate BSDEs to the ones driven by the L\\'evy processes.","The rate of convergence of the L\\'evy processes depends on the Blumenthal--Getoor index of the process.","We derive the rate of convergence for the BSDEs in the $\\mathbb L^2$-norm and in the Wasserstein distance, and show that, in both cases, this equals the rate of convergence of the corresponding L\\'evy process, and thus is optimal."],"url":"http://arxiv.org/abs/2402.01337v1","category":"math.PR"}
{"created":"2024-02-02 10:07:18","title":"A divergence-based condition to ensure quantile improvement in black-box global optimization","abstract":"Black-box global optimization aims at seeking for the minimizers of an objective function whose analytical form is not known. To do so, many state-of-the-art methods rely on sampling-based strategies, where sampling distributions are built in an iterative fashion, so that their mass concentrate where the objective function is low. Despite empirical success, the convergence of these methods remains difficult to show theoretically. In this work, we introduce a new framework, based on divergence-decrease conditions, to study and design black-box global optimization algorithms. We show that the information-geometric optimization approach fits within our framework, which yields a new proof for its convergence analysis. We also establish a quantile improvement result for two novel algorithms, one related with the cross-entropy approach with mixture models, and another using heavy-tailed sampling distributions.","sentences":["Black-box global optimization aims at seeking for the minimizers of an objective function whose analytical form is not known.","To do so, many state-of-the-art methods rely on sampling-based strategies, where sampling distributions are built in an iterative fashion, so that their mass concentrate where the objective function is low.","Despite empirical success, the convergence of these methods remains difficult to show theoretically.","In this work, we introduce a new framework, based on divergence-decrease conditions, to study and design black-box global optimization algorithms.","We show that the information-geometric optimization approach fits within our framework, which yields a new proof for its convergence analysis.","We also establish a quantile improvement result for two novel algorithms, one related with the cross-entropy approach with mixture models, and another using heavy-tailed sampling distributions."],"url":"http://arxiv.org/abs/2402.01277v1","category":"math.OC"}
