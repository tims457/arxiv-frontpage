{"created":"2024-02-16 18:55:40","title":"Single-photon emitters in WSe$_2$: The critical role of phonons on excitation schemes and indistinguishability","abstract":"Within optical quantum information processing, single-photon sources based on a two-level system in a semiconductor material allow for on-demand generation of single photons. To initiate the spontaneous emission process, it is necessary to efficiently populate the excited state. However, reconciling the requirement for on-demand excitation with both high efficiency and high photon indistinguishability remains a challenge due to the presence of charge noise and phonon-induced decoherence in the solid-state environment. Here, we propose a method for reconstructing the phonon spectral density experienced by WSe$_{2}$ quantum emitters in the emission process. Using the reconstructed phonon spectral density, we analyze the performance of the resonant, phonon-assisted, and SUPER swing-up excitation schemes. Under resonant excitation, we obtain an exciton preparation fidelity limited to $\\sim$0.80 by the strong phonon coupling, which improves to 0.96 for the SUPER scheme (or 0.89, depending on the type of emitter considered). Under near-resonant phonon-assisted excitation, we observe near-unity excitation fidelity up to 0.976 (0.997). Additionally, we demonstrate that, assuming the suppression of the phonon sidebands, residual dephasing mechanisms such as charge/spin fluctuations are the dominating decoherence mechanisms undermining the photon indistinguishability.","sentences":["Within optical quantum information processing, single-photon sources based on a two-level system in a semiconductor material allow for on-demand generation of single photons.","To initiate the spontaneous emission process, it is necessary to efficiently populate the excited state.","However, reconciling the requirement for on-demand excitation with both high efficiency and high photon indistinguishability remains a challenge due to the presence of charge noise and phonon-induced decoherence in the solid-state environment.","Here, we propose a method for reconstructing the phonon spectral density experienced by WSe$_{2}$ quantum emitters in the emission process.","Using the reconstructed phonon spectral density, we analyze the performance of the resonant, phonon-assisted, and SUPER swing-up excitation schemes.","Under resonant excitation, we obtain an exciton preparation fidelity limited to $\\sim$0.80 by the strong phonon coupling, which improves to 0.96 for the SUPER scheme (or 0.89, depending on the type of emitter considered).","Under near-resonant phonon-assisted excitation, we observe near-unity excitation fidelity up to 0.976 (0.997).","Additionally, we demonstrate that, assuming the suppression of the phonon sidebands, residual dephasing mechanisms such as charge/spin fluctuations are the dominating decoherence mechanisms undermining the photon indistinguishability."],"url":"http://arxiv.org/abs/2402.10897v1","category":"quant-ph"}
{"created":"2024-02-16 18:51:42","title":"Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning","abstract":"Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke. The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention. A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification. Supervised contrastive learning is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused multimodal embedding. The network takes as the input DWI and ADC images, and structured health profile data. The outcome is the prediction of the patient needing long-term care at 3 months after the onset of stroke. Trained and evaluated with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80 and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing models that consolidate both imaging and structured data in the medical domain. If trained with comprehensive clinical variables, including NIHSS and comorbidities, the gain from images on making accurate prediction is not considered substantial, but significant. However, diffusion-weighted MRI can replace NIHSS to achieve comparable level of accuracy combining with other readily available clinical variables for better generalization.","sentences":["Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke.","The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention.","A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification.","Supervised contrastive learning is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused multimodal embedding.","The network takes as the input DWI and ADC images, and structured health profile data.","The outcome is the prediction of the patient needing long-term care at 3 months after the onset of stroke.","Trained and evaluated with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80 and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing models that consolidate both imaging and structured data in the medical domain.","If trained with comprehensive clinical variables, including NIHSS and comorbidities, the gain from images on making accurate prediction is not considered substantial, but significant.","However, diffusion-weighted MRI can replace NIHSS to achieve comparable level of accuracy combining with other readily available clinical variables for better generalization."],"url":"http://arxiv.org/abs/2402.10894v1","category":"cs.CV"}
{"created":"2024-02-16 18:50:24","title":"RLVF: Learning from Verbal Feedback without Overgeneralization","abstract":"The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as \"Don't use emojis when drafting emails to my boss.\" However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%.","sentences":["The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences.","A convenient interface to specify such model adjustments is high-level verbal feedback, such as \"Don't use emojis when drafting emails to my boss.\"","However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant.","We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO).","C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should not) be applied.","It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply.","Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts.","For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%."],"url":"http://arxiv.org/abs/2402.10893v1","category":"cs.LG"}
{"created":"2024-02-16 18:47:21","title":"Instruction Diversity Drives Generalization To Unseen Tasks","abstract":"Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \\emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of \"inputs\" and \"instructions\". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalization emerges once a diverse enough set of tasks is provided, even though very few examples are provided for each task. Instruction diversity also ensures robustness with respect to non-uniform distributions of instructions in the training set.","sentences":["Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions.","Its practical success depends on the model learning a broader set of instructions than those it was trained on.","Yet the factors that determine model generalization to such \\emph{unseen tasks} are not well understood.","%To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of \"inputs\" and \"instructions\".","We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization.","Generalization emerges once a diverse enough set of tasks is provided, even though very few examples are provided for each task.","Instruction diversity also ensures robustness with respect to non-uniform distributions of instructions in the training set."],"url":"http://arxiv.org/abs/2402.10891v1","category":"cs.CL"}
{"created":"2024-02-16 18:45:58","title":"When is Tree Search Useful for LLM Planning? It Depends on the Discriminator","abstract":"In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval.","sentences":["In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method.","We investigate the practical utility of two advanced planning methods, iterative correction and tree search.","We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking.","Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency.","For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications.","Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval."],"url":"http://arxiv.org/abs/2402.10890v1","category":"cs.CL"}
{"created":"2024-02-16 18:44:57","title":"Evaluation of EAP Usage for Authenticating Eduroam Users in 5G Networks","abstract":"The fifth generation of the telecommunication networks (5G) established the service-oriented paradigm on the mobile networks. In this new context, the 5G Core component has become extremely flexible so, in addition to serving mobile networks, it can also be used to connect devices from the so-called non-3GPP networks, which contains technologies such as WiFi. The implementation of this connectivity requires specific protocols to ensure authentication and reliability. Given these characteristics and the possibility of convergence, it is necessary to carefully choose the encryption algorithms and authentication methods used by non-3GPP user equipment. In light of the above, this paper highlights key findings resulting from an analysis on the subject conducted through a test environment which could be used in the context of the Eduroam federation.","sentences":["The fifth generation of the telecommunication networks (5G) established the service-oriented paradigm on the mobile networks.","In this new context, the 5G Core component has become extremely flexible so, in addition to serving mobile networks, it can also be used to connect devices from the so-called non-3GPP networks, which contains technologies such as WiFi.","The implementation of this connectivity requires specific protocols to ensure authentication and reliability.","Given these characteristics and the possibility of convergence, it is necessary to carefully choose the encryption algorithms and authentication methods used by non-3GPP user equipment.","In light of the above, this paper highlights key findings resulting from an analysis on the subject conducted through a test environment which could be used in the context of the Eduroam federation."],"url":"http://arxiv.org/abs/2402.10889v1","category":"cs.CR"}
{"created":"2024-02-16 18:44:37","title":"Explainability for Machine Learning Models: From Data Adaptability to User Perception","abstract":"This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure how users perceive their interaction with the model in terms of understanding and trust, depending on the explanations and representations. This research contributes to a better explanation generation, with potential implications for enhancing the transparency, trustworthiness, and usability of deployed AI systems.","sentences":["This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements.","The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   ","The thesis is divided into two parts.","The first enhances a widely used rule-based explanation method.","It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model.","Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other.","The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations.","These experiments measure how users perceive their interaction with the model in terms of understanding and trust, depending on the explanations and representations.","This research contributes to a better explanation generation, with potential implications for enhancing the transparency, trustworthiness, and usability of deployed AI systems."],"url":"http://arxiv.org/abs/2402.10888v1","category":"cs.AI"}
{"created":"2024-02-16 18:43:10","title":"Reviewer2: Optimizing Review Generation Through Prompt Generation","abstract":"Recent developments in LLMs offer new opportunities for assisting authors in improving their work. In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft. While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce. To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2. Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address. We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft. As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a resource for future research.","sentences":["Recent developments in LLMs offer new opportunities for assisting authors in improving their work.","In this paper, we envision a use case where authors can receive LLM-generated reviews that uncover weak points in the current draft.","While initial methods for automated review generation already exist, these methods tend to produce reviews that lack detail, and they do not cover the range of opinions that human reviewers produce.","To address this shortcoming, we propose an efficient two-stage review generation framework called Reviewer2.","Unlike prior work, this approach explicitly models the distribution of possible aspects that the review may address.","We show that this leads to more detailed reviews that better cover the range of aspects that human reviewers identify in the draft.","As part of the research, we generate a large-scale review dataset of 27k papers and 99k reviews that we annotate with aspect prompts, which we make available as a resource for future research."],"url":"http://arxiv.org/abs/2402.10886v1","category":"cs.CL"}
{"created":"2024-02-16 18:43:02","title":"3D Diffuser Actor: Policy Diffusion with 3D Scene Representations","abstract":"We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens. 3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in the setting of zero-shot unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase. It also works in the real world from a handful of demonstrations. We ablate our model's architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization. Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations.","sentences":["We marry diffusion policies and 3D scene representations for robot manipulation.","Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models.","They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods.","3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth.","They have shown to generalize better than their 2D counterparts across camera viewpoints.","We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector.","At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts the 3D translation and rotation error for each of them, by featurizing them using 3D relative attention to other 3D visual and language tokens.","3D Diffuser Actor sets a new state-of-the-art on RLBench with an absolute performance gain of 16.3% over the current SOTA on a multi-view setup and an absolute gain of 13.1% on a single-view setup.","On the CALVIN benchmark, it outperforms the current SOTA in the setting of zero-shot unseen scene generalization by being able to successfully run 0.2 more tasks, a 7% relative increase.","It also works in the real world from a handful of demonstrations.","We ablate our model's architectural design choices, such as 3D scene featurization and 3D relative attentions, and show they all help generalization.","Our results suggest that 3D scene representations and powerful generative modeling are keys to efficient robot learning from demonstrations."],"url":"http://arxiv.org/abs/2402.10885v1","category":"cs.RO"}
{"created":"2024-02-16 18:42:08","title":"Multi-modal preference alignment remedies regression of visual instruction tuning on language model","abstract":"In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This enhancement in textual instruction proficiency correlates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning.","sentences":["In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities.","However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with.","To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM.","Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale.","This enhancement in textual instruction proficiency correlates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to previous RLHF approach.","In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning."],"url":"http://arxiv.org/abs/2402.10884v1","category":"cs.CL"}
{"created":"2024-02-16 18:40:10","title":"Electronic Conductivity Measurements in Solid Electrolytes Using an Ion Blocking Microelectrode: Noise Rejection Based on a Median Filter","abstract":"A method of electronic conductivity measurement is presented. It combines two well known methods of electrochemistry: cyclic voltammetry and chronoamperometry. This DC technique uses the Hebb/Wagner approach to block ionic conduction when steady state conditions are reached and allows electronic conduction of solid electrolytes to be determined. In order to get short diffusion times, a micro contact is used as an ion blocking electrode. However, as the electronic conduction in electrolytes is and should be very low, the current is also very low, typically some tens of nanoamps. Thus, the heating system inevitably generates noise problems that are solved using a median filter. Our system allows the determination of the conductivities without any preliminary smoothing or fitting of the curves. Some results with oxygen ion conductors are also give","sentences":["A method of electronic conductivity measurement is presented.","It combines two well known methods of electrochemistry: cyclic voltammetry and chronoamperometry.","This DC technique uses the Hebb/Wagner approach to block ionic conduction when steady state conditions are reached and allows electronic conduction of solid electrolytes to be determined.","In order to get short diffusion times, a micro contact is used as an ion blocking electrode.","However, as the electronic conduction in electrolytes is and should be very low, the current is also very low, typically some tens of nanoamps.","Thus, the heating system inevitably generates noise problems that are solved using a median filter.","Our system allows the determination of the conductivities without any preliminary smoothing or fitting of the curves.","Some results with oxygen ion conductors are also give"],"url":"http://arxiv.org/abs/2402.10883v1","category":"eess.SY"}
{"created":"2024-02-16 18:36:36","title":"Universal Prompt Optimizer for Safe Text-to-Image Generation","abstract":"Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \\textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance.","sentences":["Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts.","However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images.","Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications.","Hence, \\textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}.","We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo.","To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization.","Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment.","It is also flexible to be combined with methods to achieve better performance."],"url":"http://arxiv.org/abs/2402.10882v1","category":"cs.CV"}
{"created":"2024-02-16 18:29:19","title":"Robust agents learn causal world models","abstract":"It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.","sentences":["It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence.","However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient.","We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents.","We discuss the implications of this result for several research areas including transfer learning and causal inference."],"url":"http://arxiv.org/abs/2402.10877v1","category":"cs.AI"}
{"created":"2024-02-16 18:26:29","title":"Accelerating Sparse DNNs Based on Tiled GEMM","abstract":"Network pruning can reduce the computation cost of deep neural network (DNN) models. However, sparse models often produce randomly-distributed weights to maintain accuracy, leading to irregular computations. Consequently, unstructured sparse models cannot achieve meaningful speedup on commodity hardware built for dense matrix computations. Accelerators are usually modified or designed with structured sparsity-optimized architectures for exploiting sparsity. For example, the Ampere architecture introduces a sparse tensor core, which adopts the 2:4 sparsity pattern.   We propose a pruning method that builds upon the insight that matrix multiplication generally breaks the large matrix into multiple smaller tiles for parallel execution. We present the tile-wise sparsity pattern, which maintains a structured sparsity pattern at the tile level for efficient execution but allows for irregular pruning at the global scale to maintain high accuracy. In addition, the tile-wise sparsity is implemented at the global memory level, and the 2:4 sparsity executes at the register level inside the sparse tensor core. We can combine these two patterns into a tile-vector-wise (TVW) sparsity pattern to explore more fine-grained sparsity and further accelerate the sparse DNN models. We evaluate the TVW on the GPU, achieving averages of $1.85\\times$, $2.75\\times$, and $22.18\\times$ speedups over the dense model, block sparsity, and unstructured sparsity.","sentences":["Network pruning can reduce the computation cost of deep neural network (DNN) models.","However, sparse models often produce randomly-distributed weights to maintain accuracy, leading to irregular computations.","Consequently, unstructured sparse models cannot achieve meaningful speedup on commodity hardware built for dense matrix computations.","Accelerators are usually modified or designed with structured sparsity-optimized architectures for exploiting sparsity.","For example, the Ampere architecture introduces a sparse tensor core, which adopts the 2:4 sparsity pattern.   ","We propose a pruning method that builds upon the insight that matrix multiplication generally breaks the large matrix into multiple smaller tiles for parallel execution.","We present the tile-wise sparsity pattern, which maintains a structured sparsity pattern at the tile level for efficient execution but allows for irregular pruning at the global scale to maintain high accuracy.","In addition, the tile-wise sparsity is implemented at the global memory level, and the 2:4 sparsity executes at the register level inside the sparse tensor core.","We can combine these two patterns into a tile-vector-wise (TVW) sparsity pattern to explore more fine-grained sparsity and further accelerate the sparse DNN models.","We evaluate the TVW on the GPU, achieving averages of $1.85\\times$, $2.75\\times$, and $22.18\\times$ speedups over the dense model, block sparsity, and unstructured sparsity."],"url":"http://arxiv.org/abs/2402.10876v1","category":"cs.DC"}
{"created":"2024-02-16 18:18:32","title":"Fast counter-diabatic Thouless pumping in the Rice-Mele mode","abstract":"Thouless pumping is a transport phenomenon where a periodically varying Hamiltonian can transfer a quantized amount of charge when the time-dependence of the Hamiltonian is quasi-adiabatic. Past proposals to speed up this process involving Floquet techniques lead to a subtle problem of setting the initial state of the system. In this work we apply counter-diabatic driving to the Rice-Mele model, which is one of the simplest models for Thouless pumping, to ensure that the system remains in the ground state for any driving speed. We show that the pumped charge across each bond of the Rice-Mele model is given by a topologically quantized Chern number in this case. However, the counter-diabatic driving in a general case turns out to involve long-range hopping. We show that this can be mitigated either by choosing a very specific example of the Rice-Mele model or by numerical optimization of the Hamiltonian to create experimentally realizable variants of fast pumping in the Rice-Mele model.","sentences":["Thouless pumping is a transport phenomenon where a periodically varying Hamiltonian can transfer a quantized amount of charge when the time-dependence of the Hamiltonian is quasi-adiabatic.","Past proposals to speed up this process involving Floquet techniques lead to a subtle problem of setting the initial state of the system.","In this work we apply counter-diabatic driving to the Rice-Mele model, which is one of the simplest models for Thouless pumping, to ensure that the system remains in the ground state for any driving speed.","We show that the pumped charge across each bond of the Rice-Mele model is given by a topologically quantized Chern number in this case.","However, the counter-diabatic driving in a general case turns out to involve long-range hopping.","We show that this can be mitigated either by choosing a very specific example of the Rice-Mele model or by numerical optimization of the Hamiltonian to create experimentally realizable variants of fast pumping in the Rice-Mele model."],"url":"http://arxiv.org/abs/2402.10872v1","category":"quant-ph"}
{"created":"2024-02-16 18:16:31","title":"Lightweight ciphers based on chaotic Map -- LFSR architectures","abstract":"In this paper, we propose and analyze two different stream ciphers based on a Skew Tent Map and a Modified Logistic Map respectively. In order to improve the randomness of these systems, a single method for increasing the period length of the generated sequences has been applied. The results prove that the randomness of these systems can be severally increased by using this method, making these systems suitable for secure communications.","sentences":["In this paper, we propose and analyze two different stream ciphers based on a Skew Tent Map and a Modified Logistic Map respectively.","In order to improve the randomness of these systems, a single method for increasing the period length of the generated sequences has been applied.","The results prove that the randomness of these systems can be severally increased by using this method, making these systems suitable for secure communications."],"url":"http://arxiv.org/abs/2402.10871v1","category":"cs.CR"}
{"created":"2024-02-16 18:08:47","title":"The broadening of universal relations at the birth and death of a neutron star","abstract":"Certain relations among neutron-star observables that are insensitive to the equation of state are known to exist. Such universal relations have been shown to be valid for cold and stationary neutron stars. Here, we study these relations in more dynamic scenarios: protoneutron stars and hypermassive neutron stars. First, we study protoneutron stars. We use an effective equation of state, extracted from three-dimensional core-collapse supernova simulations, to obtain the structure of spherically symmetric protoneutron stars. We then consider nonradial oscillations to compute their $f$-mode frequency ($f$), as well as slow rotation and small tidal deformation, to compute their moment of inertia ($I$), spin-induced quadrupole moment ($Q$), and Love number. We find that well-established universal relations for cold neutron stars involving these observables ($I$-Love-$Q$ and $f$-Love relations) are approximately valid for protoneutron stars, with a deviation below $\\approx$ 10$\\%$ for a postbounce time above $\\approx$ 0.5 s, considering eight different supernova progenitors and the SFHo equation of state. Next, we study hypermassive neutron stars. We obtain a new universal relation between the $f$-mode frequency and the compactness of cold and nonrotating neutron stars, using bulk quantities. We show that this relation has an equation-of-state-variation of $\\approx$ $3\\%$, considering a set of ten equations of state. Using results from binary neutron star merger simulations, we study the evolution of hypermassive neutron stars on the $f$-$C$ plane, considering two different mass ratios and the SFHo equation of state. We find that the relation between the peak frequency of the gravitational-wave signal and the compactness from these hypermassive neutron stars deviates from the universal $f$-$C$ relation by 70 $-$ 80$\\%$, when the peak frequency is taken directly as a proxy for the $f$-mode.","sentences":["Certain relations among neutron-star observables that are insensitive to the equation of state are known to exist.","Such universal relations have been shown to be valid for cold and stationary neutron stars.","Here, we study these relations in more dynamic scenarios: protoneutron stars and hypermassive neutron stars.","First, we study protoneutron stars.","We use an effective equation of state, extracted from three-dimensional core-collapse supernova simulations, to obtain the structure of spherically symmetric protoneutron stars.","We then consider nonradial oscillations to compute their $f$-mode frequency ($f$), as well as slow rotation and small tidal deformation, to compute their moment of inertia ($I$), spin-induced quadrupole moment ($Q$), and Love number.","We find that well-established universal relations for cold neutron stars involving these observables ($I$-Love-$Q$ and $f$-Love relations) are approximately valid for protoneutron stars, with a deviation below $\\approx$ 10$\\%$ for a postbounce time above $\\approx$ 0.5 s, considering eight different supernova progenitors and the SFHo equation of state.","Next, we study hypermassive neutron stars.","We obtain a new universal relation between the $f$-mode frequency and the compactness of cold and nonrotating neutron stars, using bulk quantities.","We show that this relation has an equation-of-state-variation of $\\approx$ $3\\%$, considering a set of ten equations of state.","Using results from binary neutron star merger simulations, we study the evolution of hypermassive neutron stars on the $f$-$C$ plane, considering two different mass ratios and the SFHo equation of state.","We find that the relation between the peak frequency of the gravitational-wave signal and the compactness from these hypermassive neutron stars deviates from the universal $f$-$C$ relation by 70 $-$ 80$\\%$, when the peak frequency is taken directly as a proxy for the $f$-mode."],"url":"http://arxiv.org/abs/2402.10868v1","category":"astro-ph.HE"}
{"created":"2024-02-16 18:01:43","title":"Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds","abstract":"We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences.","sentences":["We investigate a variation of the 3D registration problem, named multi-model 3D registration.","In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds.","This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene.","Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects.","We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers.","We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth.","We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences."],"url":"http://arxiv.org/abs/2402.10865v1","category":"cs.RO"}
{"created":"2024-02-16 17:58:41","title":"Reactive experimental control of turbulent jets","abstract":"We present an experimental study of reactive control of turbulent jets. We target axisymmetric disturbances associated with coherent structures, which are known to underpin the peak sound radiation of turbulent jets. We first consider a forced jet flow case, such that the coherent structures can be amplified above background levels, which makes it easier to detect them by the sensors. We then consider the more challenging case of a natural jet, i.e., without artificial forcing. The control strategy explores linear convective mechanisms in the initial jet region, which justifies application of linear control theory. The control law is constructed in the frequency domain, based on empirically determined transfer functions. The Wiener-Hopf formalism is used to enforce causality, providing an optimal causal solution, thus, preventing the drop in performance that may be observed in flow control applications that use simpler wave-cancellation methods. With this approach, we could improve the control performance of forced turbulent jets compared to results obtained in previous studies, attaining order-of-magnitude attenuation of power spectra of velocity fluctuations. Furthermore, we could obtain substantial levels of attenuation of natural turbulent jets, of about 60% in power spectra for the most amplified frequencies. These results open new directions for the control of turbulent flows.","sentences":["We present an experimental study of reactive control of turbulent jets.","We target axisymmetric disturbances associated with coherent structures, which are known to underpin the peak sound radiation of turbulent jets.","We first consider a forced jet flow case, such that the coherent structures can be amplified above background levels, which makes it easier to detect them by the sensors.","We then consider the more challenging case of a natural jet, i.e., without artificial forcing.","The control strategy explores linear convective mechanisms in the initial jet region, which justifies application of linear control theory.","The control law is constructed in the frequency domain, based on empirically determined transfer functions.","The Wiener-Hopf formalism is used to enforce causality, providing an optimal causal solution, thus, preventing the drop in performance that may be observed in flow control applications that use simpler wave-cancellation methods.","With this approach, we could improve the control performance of forced turbulent jets compared to results obtained in previous studies, attaining order-of-magnitude attenuation of power spectra of velocity fluctuations.","Furthermore, we could obtain substantial levels of attenuation of natural turbulent jets, of about 60% in power spectra for the most amplified frequencies.","These results open new directions for the control of turbulent flows."],"url":"http://arxiv.org/abs/2402.10860v1","category":"physics.flu-dyn"}
{"created":"2024-02-16 17:54:41","title":"Spatio-temporal point process modelling of fires in Sicily exploring human and environmental factors","abstract":"In 2023, Sicily faced an escalating issue of uncontrolled fires, necessitating a thorough investigation into their spatio-temporal dynamics. Our study addresses this concern through point process theory. Each wildfire is treated as a unique point in both space and time, allowing us to assess the influence of environmental and anthropogenic factors by fitting a spatio-temporal separable Poisson point process model, with a particular focus on the role of land usage. First, a spatial log-linear Poisson model is applied to investigate the influence of land use types on wildfire distribution, controlling for other environmental covariates. The results highlight the significant effect of human activities, altitude, and slope on spatial fire occurrence. Then, a Generalized Additive Model with Poisson-distributed response further explores the temporal dynamics of wildfire occurrences, confirming their dependence on various environmental variables, including the maximum daily temperature, wind speed, surface pressure, and total precipitation.","sentences":["In 2023, Sicily faced an escalating issue of uncontrolled fires, necessitating a thorough investigation into their spatio-temporal dynamics.","Our study addresses this concern through point process theory.","Each wildfire is treated as a unique point in both space and time, allowing us to assess the influence of environmental and anthropogenic factors by fitting a spatio-temporal separable Poisson point process model, with a particular focus on the role of land usage.","First, a spatial log-linear Poisson model is applied to investigate the influence of land use types on wildfire distribution, controlling for other environmental covariates.","The results highlight the significant effect of human activities, altitude, and slope on spatial fire occurrence.","Then, a Generalized Additive Model with Poisson-distributed response further explores the temporal dynamics of wildfire occurrences, confirming their dependence on various environmental variables, including the maximum daily temperature, wind speed, surface pressure, and total precipitation."],"url":"http://arxiv.org/abs/2402.10859v1","category":"stat.AP"}
{"created":"2024-02-16 17:40:47","title":"Schwartz correspondence for real motion groups in low dimensions","abstract":"For a Gelfand pair $(G,K)$ with $G$ a Lie group of polynomial growth and $K$ a compact subgroup, the \"Schwartz correspondence\" states that the spherical transform maps the bi-$K$-invariant Schwartz space ${\\mathcal S}(K\\backslash G/K)$ isomorphically onto the space ${\\mathcal S}(\\Sigma_{\\mathcal D})$, where $\\Sigma_{\\mathcal D}$ is an embedded copy of the Gelfand spectrum in ${\\mathbb R}^\\ell$, canonically associated to a generating system ${\\mathcal D}$ of $G$-invariant differential operators on $G/K$, and ${\\mathcal S}(\\Sigma_{\\mathcal D})$ consists of restrictions to $\\Sigma_{\\mathcal D}$ of Schwartz functions on ${\\mathbb R}^\\ell$.   Schwartz correspondence is known to hold for a large variety of Gelfand pairs of polynomial growth. In this paper we prove that it holds for the strong Gelfand pair $(M_n,SO_n)$ with $n=3,4$. The rather trivial case $n=2$ is included in previous work by the same authors.","sentences":["For a Gelfand pair $(G,K)$ with $G$ a Lie group of polynomial growth and $K$ a compact subgroup, the \"Schwartz correspondence\" states that the spherical transform maps the bi-$K$-invariant Schwartz space ${\\mathcal S}(K\\backslash G/K)$ isomorphically onto the space ${\\mathcal S}(\\Sigma_{\\mathcal D})$, where $\\Sigma_{\\mathcal D}$ is an embedded copy of the Gelfand spectrum in ${\\mathbb R}^\\ell$, canonically associated to a generating system ${\\mathcal D}$ of $G$-invariant differential operators on $G/K$, and ${\\mathcal S}(\\Sigma_{\\mathcal D})$ consists of restrictions to $\\Sigma_{\\mathcal D}$ of Schwartz functions on ${\\mathbb R}^\\ell$.   Schwartz correspondence is known to hold for a large variety of Gelfand pairs of polynomial growth.","In this paper we prove that it holds for the strong Gelfand pair $(M_n,SO_n)$ with $n=3,4$. The rather trivial case $n=2$ is included in previous work by the same authors."],"url":"http://arxiv.org/abs/2402.10848v1","category":"math.FA"}
{"created":"2024-02-16 17:36:51","title":"FedD2S: Personalized Data-Free Federated Knowledge Distillation","abstract":"This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively captures personalized knowledge, resulting in enhanced performance compared to alternative FL models. Moreover, we investigate the impact of key hyperparameters, such as the participation ratio and layer-dropping rate, providing valuable insights into the optimal configuration for FedD2S. The findings demonstrate the efficacy of adaptive layer-dropping in the knowledge distillation process to achieve enhanced personalization and performance across diverse datasets and tasks.","sentences":["This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework.","The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client.","To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation.","FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization.","Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines.","The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients.","The introduced layer-dropping technique effectively captures personalized knowledge, resulting in enhanced performance compared to alternative FL models.","Moreover, we investigate the impact of key hyperparameters, such as the participation ratio and layer-dropping rate, providing valuable insights into the optimal configuration for FedD2S. The findings demonstrate the efficacy of adaptive layer-dropping in the knowledge distillation process to achieve enhanced personalization and performance across diverse datasets and tasks."],"url":"http://arxiv.org/abs/2402.10846v1","category":"cs.LG"}
{"created":"2024-02-16 17:21:21","title":"Dynamics on the SU(2,1)-character variety of the one-holed torus","abstract":"We study the relative SU(2,1)-character varieties of the one-holed torus, and the action of the mapping class group on them. We use an explicit description of the character variety of the free group of rank two in SU(2,1) in terms of traces, which allow us to describe the topology of the character variety. We then combine this description with a generalization of the Farey graph adapted to this new combinatorial setting, using ideas introduced by Bowditch. Using these tools, we can describe an open domain of discontinuity for the action of the mapping class group which strictly contains the set of convex-cocompact characters, and we give several characterizations of representations in this domain.","sentences":["We study the relative SU(2,1)-character varieties of the one-holed torus, and the action of the mapping class group on them.","We use an explicit description of the character variety of the free group of rank two in SU(2,1) in terms of traces, which allow us to describe the topology of the character variety.","We then combine this description with a generalization of the Farey graph adapted to this new combinatorial setting, using ideas introduced by Bowditch.","Using these tools, we can describe an open domain of discontinuity for the action of the mapping class group which strictly contains the set of convex-cocompact characters, and we give several characterizations of representations in this domain."],"url":"http://arxiv.org/abs/2402.10838v1","category":"math.GT"}
{"created":"2024-02-16 17:20:45","title":"Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg","abstract":"Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation - using the legs of a legged robot for manipulation. By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation. By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles. We demonstrate load carrying of more than 2.0 kg at the foot. Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces. Videos of the experiments are available at https://sites.google.com/leggedrobotics.com/pedipulate.","sentences":["Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios.","In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots.","In this work, we explore pedipulation - using the legs of a legged robot for manipulation.","By training a reinforcement learning policy that tracks position targets for one foot, we enable a dedicated pedipulation controller that is robust to disturbances, has a large workspace through whole-body behaviors, and can reach far-away targets with gait emergence, enabling loco-pedipulation.","By deploying our controller on a quadrupedal robot using teleoperation, we demonstrate various real-world tasks such as door opening, sample collection, and pushing obstacles.","We demonstrate load carrying of more than 2.0 kg at the foot.","Additionally, the controller is robust to interaction forces at the foot, disturbances at the base, and slippery contact surfaces.","Videos of the experiments are available at https://sites.google.com/leggedrobotics.com/pedipulate."],"url":"http://arxiv.org/abs/2402.10837v1","category":"cs.RO"}
{"created":"2024-02-16 17:03:08","title":"GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers","abstract":"Inverse scattering problems are inherently challenging, given the fact they are ill-posed and nonlinear. This paper presents a powerful deep learning-based approach that relies on generative adversarial networks to accurately and efficiently reconstruct randomly-shaped two-dimensional dielectric objects from amplitudes of multi-frequency scattered electric fields. An adversarial autoencoder (AAE) is trained to learn to generate the scatterer's geometry from a lower-dimensional latent representation constrained to adhere to the Gaussian distribution. A cohesive inverse neural network (INN) framework is set up comprising a sequence of appropriately designed dense layers, the already-trained generator as well as a separately trained forward neural network. The images reconstructed at the output of the inverse network are validated through comparison with outputs from the forward neural network, addressing the non-uniqueness challenge inherent to electromagnetic (EM) imaging problems. The trained INN demonstrates an enhanced robustness, evidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure similarity index (SSI) of $0.90$. The study not only demonstrates a significant reduction in computational load, but also marks a substantial improvement over traditional objective-function-based methods. It contributes both to the fields of machine learning and EM imaging by offering a real-time quantitative imaging approach. The results obtained with the simulated data, for both training and testing, yield promising results and may open new avenues for radio-frequency inverse imaging.","sentences":["Inverse scattering problems are inherently challenging, given the fact they are ill-posed and nonlinear.","This paper presents a powerful deep learning-based approach that relies on generative adversarial networks to accurately and efficiently reconstruct randomly-shaped two-dimensional dielectric objects from amplitudes of multi-frequency scattered electric fields.","An adversarial autoencoder (AAE) is trained to learn to generate the scatterer's geometry from a lower-dimensional latent representation constrained to adhere to the Gaussian distribution.","A cohesive inverse neural network (INN) framework is set up comprising a sequence of appropriately designed dense layers, the already-trained generator as well as a separately trained forward neural network.","The images reconstructed at the output of the inverse network are validated through comparison with outputs from the forward neural network, addressing the non-uniqueness challenge inherent to electromagnetic (EM) imaging problems.","The trained INN demonstrates an enhanced robustness, evidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure similarity index (SSI) of $0.90$. The study not only demonstrates a significant reduction in computational load, but also marks a substantial improvement over traditional objective-function-based methods.","It contributes both to the fields of machine learning and EM imaging by offering a real-time quantitative imaging approach.","The results obtained with the simulated data, for both training and testing, yield promising results and may open new avenues for radio-frequency inverse imaging."],"url":"http://arxiv.org/abs/2402.10831v1","category":"eess.IV"}
{"created":"2024-02-16 16:57:18","title":"RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model","abstract":"Robots powered by 'blackbox' models need to provide human-understandable explanations which we can trust. Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving. Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations. However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task. Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment. To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving. By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction. More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours.","sentences":["Robots powered by 'blackbox' models need to provide human-understandable explanations which we can trust.","Hence, explainability plays a critical role in trustworthy autonomous decision-making to foster transparency and acceptance among end users, especially in complex autonomous driving.","Recent advancements in Multi-Modal Large Language models (MLLMs) have shown promising potential in enhancing the explainability as a driving agent by producing control predictions along with natural language explanations.","However, severe data scarcity due to expensive annotation costs and significant domain gaps between different datasets makes the development of a robust and generalisable system an extremely challenging task.","Moreover, the prohibitively expensive training requirements of MLLM and the unsolved problem of catastrophic forgetting further limit their generalisability post-deployment.","To address these challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal large language model that leverages in-context learning for high-performance, explainable, and generalisable autonomous driving.","By grounding in retrieved expert demonstration, we empirically validate that RAG-Driver achieves state-of-the-art performance in producing driving action explanations, justifications, and control signal prediction.","More importantly, it exhibits exceptional zero-shot generalisation capabilities to unseen environments without further training endeavours."],"url":"http://arxiv.org/abs/2402.10828v1","category":"cs.RO"}
{"created":"2024-02-16 16:56:46","title":"Mordukhovich derivatives of the set-valued metric projection operator in general Banach spaces","abstract":"In this paper, we investigate the properties and the precise solutions of the Mordukhovich derivatives of the set-valued metric projection operator onto some closed balls in some general Banach spaces. In the Banach space c, we find the properties of Mordukhovich derivatives of the set-valued metric projection operator onto the closed subspace c0. We show that the metric projection from C[0, 1] to polynormal with degree less than or equal to n is a single-valued mapping. We investigate its Mordukhovich derivatives and Gateaux directional derivatives.","sentences":["In this paper, we investigate the properties and the precise solutions of the Mordukhovich derivatives of the set-valued metric projection operator onto some closed balls in some general Banach spaces.","In the Banach space c, we find the properties of Mordukhovich derivatives of the set-valued metric projection operator onto the closed subspace c0.","We show that the metric projection from C[0, 1] to polynormal with degree less than or equal to n is a single-valued mapping.","We investigate its Mordukhovich derivatives and Gateaux directional derivatives."],"url":"http://arxiv.org/abs/2402.10827v1","category":"math.FA"}
{"created":"2024-02-16 16:54:15","title":"Linkage of Pfister forms over semi-global fields","abstract":"We study linkage of $(d+1)$-fold quadratic Pfister forms over function fields in one variable over a henselian valued field of 2-cohomological dimension $d$. Specifically, we characterise this property in terms of linkage of quadratic Pfister forms over function fields over the residue field of the henselian valued field; in full generality in characteristic different from 2, and for most complete discretely valued fields in characteristic 2. As an application, we obtain a proof that $(d+2)$-fold quadratic Pfister forms over function fields in one variable over a $d$-dimensional higher local field are linked.","sentences":["We study linkage of $(d+1)$-fold quadratic Pfister forms over function fields in one variable over a henselian valued field of 2-cohomological dimension $d$. Specifically, we characterise this property in terms of linkage of quadratic Pfister forms over function fields over the residue field of the henselian valued field; in full generality in characteristic different from 2, and for most complete discretely valued fields in characteristic 2.","As an application, we obtain a proof that $(d+2)$-fold quadratic Pfister forms over function fields in one variable over a $d$-dimensional higher local field are linked."],"url":"http://arxiv.org/abs/2402.10826v1","category":"math.NT"}
{"created":"2024-02-16 16:51:32","title":"Kadanoff-Baym equations for interacting systems with dissipative Lindbladian dynamics","abstract":"The extraordinary quantum properties of nonequilibrium systems governed by dissipative dynamics have become a focal point in contemporary scientific inquiry. The Nonequilibrium Green's Functions (NEGF) theory provides a versatile method for addressing driven {\\em non-dissipative} systems, utilizing the powerful diagrammatic technique to incorporate correlation effects. We here present a second-quantization approach to the {\\em dissipative} NEGF theory, reformulating Keldysh ideas to accommodate Lindbladian dynamics and extending the Kadanoff-Baym equations accordingly. Generalizing diagrammatic perturbation theory for many-body Lindblad operators, the formalism enables correlated and dissipative real-time simulations for the exploration of transient and steady-state changes in the electronic, transport, and optical properties of materials.","sentences":["The extraordinary quantum properties of nonequilibrium systems governed by dissipative dynamics have become a focal point in contemporary scientific inquiry.","The Nonequilibrium Green's Functions (NEGF) theory provides a versatile method for addressing driven {\\em non-dissipative} systems, utilizing the powerful diagrammatic technique to incorporate correlation effects.","We here present a second-quantization approach to the {\\em dissipative} NEGF theory, reformulating Keldysh ideas to accommodate Lindbladian dynamics and extending the Kadanoff-Baym equations accordingly.","Generalizing diagrammatic perturbation theory for many-body Lindblad operators, the formalism enables correlated and dissipative real-time simulations for the exploration of transient and steady-state changes in the electronic, transport, and optical properties of materials."],"url":"http://arxiv.org/abs/2402.10824v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-16 16:50:03","title":"Structure results for torus fixed loci","abstract":"Motivated by localization theorems on moduli spaces, we prove a structural classification of Deligne-Mumford stacks with an action of a torus where the induced action on the coarse moduli space is trivial. We also establish a general local structure theorem for morphisms of algebraic stacks.","sentences":["Motivated by localization theorems on moduli spaces, we prove a structural classification of Deligne-Mumford stacks with an action of a torus where the induced action on the coarse moduli space is trivial.","We also establish a general local structure theorem for morphisms of algebraic stacks."],"url":"http://arxiv.org/abs/2402.10823v1","category":"math.AG"}
{"created":"2024-02-16 16:47:21","title":"Training Class-Imbalanced Diffusion Model Via Overlap Optimization","abstract":"Diffusion models have made significant advances recently in high-quality image synthesis and related tasks. However, diffusion models trained on real-world datasets, which often follow long-tailed distributions, yield inferior fidelity for tail classes. Deep generative models, including diffusion models, are biased towards classes with abundant training images. To address the observed appearance overlap between synthesized images of rare classes and tail classes, we propose a method based on contrastive learning to minimize the overlap between distributions of synthetic images for different classes. We show variants of our probabilistic contrastive learning method can be applied to any class conditional diffusion model. We show significant improvement in image synthesis using our loss for multiple datasets with long-tailed distribution. Extensive experimental results demonstrate that the proposed method can effectively handle imbalanced data for diffusion-based generation and classification models. Our code and datasets will be publicly available at https://github.com/yanliang3612/DiffROP.","sentences":["Diffusion models have made significant advances recently in high-quality image synthesis and related tasks.","However, diffusion models trained on real-world datasets, which often follow long-tailed distributions, yield inferior fidelity for tail classes.","Deep generative models, including diffusion models, are biased towards classes with abundant training images.","To address the observed appearance overlap between synthesized images of rare classes and tail classes, we propose a method based on contrastive learning to minimize the overlap between distributions of synthetic images for different classes.","We show variants of our probabilistic contrastive learning method can be applied to any class conditional diffusion model.","We show significant improvement in image synthesis using our loss for multiple datasets with long-tailed distribution.","Extensive experimental results demonstrate that the proposed method can effectively handle imbalanced data for diffusion-based generation and classification models.","Our code and datasets will be publicly available at https://github.com/yanliang3612/DiffROP."],"url":"http://arxiv.org/abs/2402.10821v1","category":"cs.CV"}
{"created":"2024-02-16 16:41:27","title":"Determining classes for generalized $\u03c8$-estimators","abstract":"We prove that the values of a generalized $\\psi$-estimator (introduced by Barczy and P\\'ales in 2022) on samples of arbitrary length but having only two different observations uniquely determine the values of the estimator on any sample of arbitrary length without any restriction on the number of different observations. In other words, samples of arbitrary length but having only two different observations form a determining class for generalized $\\psi$-estimators. We also obtain a similar statement for the comparison of generalized $\\psi$-estimators using comparative functions, and, as a corollary of this result, we derive the Schweitzer's inequality (also called Kantorovich's inequality).","sentences":["We prove that the values of a generalized $\\psi$-estimator (introduced by Barczy and P\\'ales in 2022) on samples of arbitrary length but having only two different observations uniquely determine the values of the estimator on any sample of arbitrary length without any restriction on the number of different observations.","In other words, samples of arbitrary length but having only two different observations form a determining class for generalized $\\psi$-estimators.","We also obtain a similar statement for the comparison of generalized $\\psi$-estimators using comparative functions, and, as a corollary of this result, we derive the Schweitzer's inequality (also called Kantorovich's inequality)."],"url":"http://arxiv.org/abs/2402.10817v1","category":"math.ST"}
{"created":"2024-02-16 16:35:41","title":"Exploring Hybrid Question Answering via Program-based Prompting","abstract":"Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose HProPro, a novel program-based prompting framework for the hybrid question answering task. HProPro follows the code generation and execution paradigm. In addition, HProPro integrates various functions to tackle the hybrid reasoning scenario. Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of HProPro: it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets.","sentences":["Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data.","Various approaches have been proposed to address these challenges.","One approach involves training specialized retrievers to select relevant information, thereby reducing the input length.","Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing.","In this paper, we propose HProPro, a novel program-based prompting framework for the hybrid question answering task.","HProPro follows the code generation and execution paradigm.","In addition, HProPro integrates various functions to tackle the hybrid reasoning scenario.","Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations.","Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of HProPro: it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets."],"url":"http://arxiv.org/abs/2402.10812v1","category":"cs.CL"}
{"created":"2024-02-16 16:34:08","title":"Streaming Algorithms for Connectivity Augmentation","abstract":"We study the $k$-connectivity augmentation problem ($k$-CAP) in the single-pass streaming model. Given a $(k-1)$-edge connected graph $G=(V,E)$ that is stored in memory, and a stream of weighted edges $L$ with weights in $\\{0,1,\\dots,W\\}$, the goal is to choose a minimum weight subset $L'\\subseteq L$ such that $G'=(V,E\\cup L')$ is $k$-edge connected. We give a $(2+\\epsilon)$-approximation algorithm for this problem which requires to store $O(\\epsilon^{-1} n\\log n)$ words. Moreover, we show our result is tight: Any algorithm with better than $2$-approximation for the problem requires $\\Omega(n^2)$ bits of space even when $k=2$. This establishes a gap between the optimal approximation factor one can obtain in the streaming vs the offline setting for $k$-CAP.   We further consider a natural generalization to the fully streaming model where both $E$ and $L$ arrive in the stream in an arbitrary order. We show that this problem has a space lower bound that matches the best possible size of a spanner of the same approximation ratio. Following this, we give improved results for spanners on weighted graphs: We show a streaming algorithm that finds a $(2t-1+\\epsilon)$-approximate weighted spanner of size at most $O(\\epsilon^{-1} n^{1+1/t}\\log n)$ for integer $t$, whereas the best prior streaming algorithm for spanner on weighted graphs had size depending on $\\log W$. Using our spanner result, we provide an optimal $O(t)$-approximation for $k$-CAP in the fully streaming model with $O(nk + n^{1+1/t})$ words of space.   Finally we apply our results to network design problems such as Steiner tree augmentation problem (STAP), $k$-edge connected spanning subgraph ($k$-ECSS), and the general Survivable Network Design problem (SNDP). In particular, we show a single-pass $O(t\\log k)$-approximation for SNDP using $O(kn^{1+1/t})$ words of space, where $k$ is the maximum connectivity requirement.","sentences":["We study the $k$-connectivity augmentation problem ($k$-CAP) in the single-pass streaming model.","Given a $(k-1)$-edge connected graph $G=(V,E)$ that is stored in memory, and a stream of weighted edges $L$ with weights in $\\{0,1,\\dots,W\\}$, the goal is to choose a minimum weight subset $L'\\subseteq L$ such that $G'=(V,E\\cup L')$ is $k$-edge connected.","We give a $(2+\\epsilon)$-approximation algorithm for this problem which requires to store $O(\\epsilon^{-1} n\\log n)$ words.","Moreover, we show our result is tight: Any algorithm with better than $2$-approximation for the problem requires $\\Omega(n^2)$ bits of space even when $k=2$. This establishes a gap between the optimal approximation factor one can obtain in the streaming vs the offline setting for $k$-CAP.   ","We further consider a natural generalization to the fully streaming model where both $E$ and $L$ arrive in the stream in an arbitrary order.","We show that this problem has a space lower bound that matches the best possible size of a spanner of the same approximation ratio.","Following this, we give improved results for spanners on weighted graphs: We show a streaming algorithm that finds a $(2t-1+\\epsilon)$-approximate weighted spanner of size at most $O(\\epsilon^{-1} n^{1+1/t}\\log n)$ for integer $t$, whereas the best prior streaming algorithm for spanner on weighted graphs had size depending on $\\log W$. Using our spanner result, we provide an optimal $O(t)$-approximation for $k$-CAP in the fully streaming model with $O(nk +","n^{1","+1/t})$ words of space.   ","Finally we apply our results to network design problems such as Steiner tree augmentation problem (STAP), $k$-edge connected spanning subgraph ($k$-ECSS), and the general Survivable Network Design problem (SNDP).","In particular, we show a single-pass $O(t\\log k)$-approximation for SNDP using $O(kn^{1+1/t})$ words of space, where $k$ is the maximum connectivity requirement."],"url":"http://arxiv.org/abs/2402.10806v1","category":"cs.DS"}
{"created":"2024-02-16 16:31:46","title":"Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond","abstract":"The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively. Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters. Given a user query for visual content, the MLLM is anticipated to \"recall\" the relevant image from its parameters as the response. Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs. To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve. The first step focuses on training the MLLM to memorize the association between images and their respective identifiers. The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input. By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches. The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets.","sentences":["The recent advancements in generative language models have demonstrated their ability to memorize knowledge from documents and recall knowledge to respond to user queries effectively.","Building upon this capability, we propose to enable multimodal large language models (MLLMs) to memorize and recall images within their parameters.","Given a user query for visual content, the MLLM is anticipated to \"recall\" the relevant image from its parameters as the response.","Achieving this target presents notable challenges, including inbuilt visual memory and visual recall schemes within MLLMs.","To address these challenges, we introduce a generative cross-modal retrieval framework, which assigns unique identifier strings to represent images and involves two training steps: learning to memorize and learning to retrieve.","The first step focuses on training the MLLM to memorize the association between images and their respective identifiers.","The latter step teaches the MLLM to generate the corresponding identifier of the target image, given the textual query input.","By memorizing images in MLLMs, we introduce a new paradigm to cross-modal retrieval, distinct from previous discriminative approaches.","The experiments demonstrate that the generative paradigm performs effectively and efficiently even with large-scale image candidate sets."],"url":"http://arxiv.org/abs/2402.10805v1","category":"cs.MM"}
{"created":"2024-02-16 16:28:58","title":"Modelling crypto markets by multi-agent reinforcement learning","abstract":"Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent reinforcement learning (MARL) model simulating crypto markets, which is calibrated to the Binance's daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022. Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with reinforcement learning (RL) techniques in order to model crypto markets. This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era. A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the market prices themselves, and the approximation of the crypto assets fundamental values beyond what those market prices are. Our MAS calibration against real market data allows for an accurate emulation of crypto markets microstructure and probing key market behaviors, in both the bearish and bullish regimes of that particular time period.","sentences":["Building on a previous foundation work (Lussange et al. 2020), this study introduces a multi-agent reinforcement learning (MARL) model simulating crypto markets, which is calibrated to the Binance's daily closing prices of $153$ cryptocurrencies that were continuously traded between 2018 and 2022.","Unlike previous agent-based models (ABM) or multi-agent systems (MAS) which relied on zero-intelligence agents or single autonomous agent methodologies, our approach relies on endowing agents with reinforcement learning (RL) techniques in order to model crypto markets.","This integration is designed to emulate, with a bottom-up approach to complexity inference, both individual and collective agents, ensuring robustness in the recent volatile conditions of such markets and during the COVID-19 era.","A key feature of our model also lies in the fact that its autonomous agents perform asset price valuation based on two sources of information: the market prices themselves, and the approximation of the crypto assets fundamental values beyond what those market prices are.","Our MAS calibration against real market data allows for an accurate emulation of crypto markets microstructure and probing key market behaviors, in both the bearish and bullish regimes of that particular time period."],"url":"http://arxiv.org/abs/2402.10803v1","category":"q-fin.CP"}
{"created":"2024-02-16 16:21:15","title":"VATr++: Choose Your Words Wisely for Handwritten Text Generation","abstract":"Styled Handwritten Text Generation (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance. This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better. These aspects are validated through extensive analysis on several different settings and datasets. Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol. In particular, we propose a standardization of the evaluation protocol for HTG and conduct a comprehensive benchmarking of existing approaches. By doing so, we aim to establish a foundation for fair and meaningful comparisons between HTG strategies, fostering progress in the field.","sentences":["Styled Handwritten Text Generation (HTG) has received significant attention in recent years, propelled by the success of learning-based solutions employing GANs, Transformers, and, preliminarily, Diffusion Models.","Despite this surge in interest, there remains a critical yet understudied aspect - the impact of the input, both visual and textual, on the HTG model training and its subsequent influence on performance.","This study delves deeper into a cutting-edge Styled-HTG approach, proposing strategies for input preparation and training regularization that allow the model to achieve better performance and generalize better.","These aspects are validated through extensive analysis on several different settings and datasets.","Moreover, in this work, we go beyond performance optimization and address a significant hurdle in HTG research - the lack of a standardized evaluation protocol.","In particular, we propose a standardization of the evaluation protocol for HTG and conduct a comprehensive benchmarking of existing approaches.","By doing so, we aim to establish a foundation for fair and meaningful comparisons between HTG strategies, fostering progress in the field."],"url":"http://arxiv.org/abs/2402.10798v1","category":"cs.CV"}
{"created":"2024-02-16 16:20:54","title":"Parametric instability in a magnomechanical system","abstract":"We study parametric instability in a magnomechanical system, specifically examining magnon tunneling between moving ferromagnetic insulators. Our analysis reveals that quantum fluctuations generate spin currents above a critical velocity threshold, while no spin currents occur below this threshold at low temperatures. The critical velocity depends on magnon stiffness and Zeeman energy. Approaching the threshold, the spin current becomes divergent, linked to the PT-symmetry-breaking transition. This enhanced behavior could offer sensitive measurements and signal amplification in quantum technology.","sentences":["We study parametric instability in a magnomechanical system, specifically examining magnon tunneling between moving ferromagnetic insulators.","Our analysis reveals that quantum fluctuations generate spin currents above a critical velocity threshold, while no spin currents occur below this threshold at low temperatures.","The critical velocity depends on magnon stiffness and Zeeman energy.","Approaching the threshold, the spin current becomes divergent, linked to the PT-symmetry-breaking transition.","This enhanced behavior could offer sensitive measurements and signal amplification in quantum technology."],"url":"http://arxiv.org/abs/2402.10796v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-16 16:20:43","title":"Diversified Ensembling: An Experiment in Crowdsourced Machine Learning","abstract":"Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In arXiv:2201.10408, the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task). Unlike traditional crowdsourcing, this allows for the diversification of participants' efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern). We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data. We provide an empirical analysis of teams' approaches, and discuss the novel system architecture we developed. From here, we give concrete guidance for how best to deploy such a framework.","sentences":["Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models.","Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model.","In arXiv:2201.10408, the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable.","There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness.","Here, we take a broader perspective on this work: we note that within this framework, participants may both specialize in the service of fairness and simply to cater to their particular expertise (e.g., focusing on identifying bird species in an image classification task).","Unlike traditional crowdsourcing, this allows for the diversification of participants' efforts and may provide a participation mechanism to a larger range of individuals (e.g. a machine learning novice who has insight into a specific fairness concern).","We present the first medium-scale experimental evaluation of this framework, with 46 participating teams attempting to generate models to predict income from American Community Survey data.","We provide an empirical analysis of teams' approaches, and discuss the novel system architecture we developed.","From here, we give concrete guidance for how best to deploy such a framework."],"url":"http://arxiv.org/abs/2402.10795v1","category":"cs.LG"}
{"created":"2024-02-16 16:20:11","title":"Masked Attention is All You Need for Graphs","abstract":"Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly better transfer learning capabilities compared to GNNs and comparable or better time and memory scaling. MAG has sub-linear memory scaling in the number of nodes or edges, enabling learning on dense graphs and future-proofing the approach.","sentences":["Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance.","The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators.","Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention.","Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph.","Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks.","We also show significantly better transfer learning capabilities compared to GNNs and comparable or better time and memory scaling.","MAG has sub-linear memory scaling in the number of nodes or edges, enabling learning on dense graphs and future-proofing the approach."],"url":"http://arxiv.org/abs/2402.10793v1","category":"cs.LG"}
{"created":"2024-02-16 16:15:01","title":"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss","abstract":"This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.","sentences":["This paper addresses the challenge of processing long documents using generative transformer models.","To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts.","Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements.","In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7","$ elements.","This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences."],"url":"http://arxiv.org/abs/2402.10790v1","category":"cs.CL"}
{"created":"2024-02-16 16:12:13","title":"Insights into mobile health application market via a content analysis of marketplace data with machine learning","abstract":"Background Despite the benefits offered by an abundance of health applications promoted on app marketplaces (e.g., Google Play Store), the wide adoption of mobile health and e-health apps is yet to come. Objective This study aims to investigate the current landscape of smartphone apps that focus on improving and sustaining health and wellbeing. Understanding the categories that popular apps focus on and the relevant features provided to users, which lead to higher user scores and downloads will offer insights to enable higher adoption in the general populace. This study on 1,000 mobile health applications aims to shed light on the reasons why particular apps are liked and adopted while many are not. Methods User-generated data (i.e. review scores) and company-generated data (i.e. app descriptions) were collected from app marketplaces and manually coded and categorized by two researchers. For analysis, Artificial Neural Networks, Random Forest and Na\\\"ive Bayes Artificial Intelligence algorithms were used. Results The analysis led to features that attracted more download behavior and higher user scores. The findings suggest that apps that mention a privacy policy or provide videos in description lead to higher user scores, whereas free apps with in-app purchase possibilities, social networking and sharing features and feedback mechanisms lead to higher number of downloads. Moreover, differences in user scores and the total number of downloads are detected in distinct subcategories of mobile health apps. Conclusion This study contributes to the current knowledge of m-health application use by reviewing mobile health applications using content analysis and machine learning algorithms. The content analysis adds significant value by providing classification, keywords and factors that influence download behavior and user scores in a m-health context.","sentences":["Background Despite the benefits offered by an abundance of health applications promoted on app marketplaces (e.g., Google Play Store), the wide adoption of mobile health and e-health apps is yet to come.","Objective This study aims to investigate the current landscape of smartphone apps that focus on improving and sustaining health and wellbeing.","Understanding the categories that popular apps focus on and the relevant features provided to users, which lead to higher user scores and downloads will offer insights to enable higher adoption in the general populace.","This study on 1,000 mobile health applications aims to shed light on the reasons why particular apps are liked and adopted while many are not.","Methods User-generated data (i.e. review scores) and company-generated data (i.e. app descriptions) were collected from app marketplaces and manually coded and categorized by two researchers.","For analysis, Artificial Neural Networks, Random Forest and Na\\\"ive Bayes Artificial Intelligence algorithms were used.","Results The analysis led to features that attracted more download behavior and higher user scores.","The findings suggest that apps that mention a privacy policy or provide videos in description lead to higher user scores, whereas free apps with in-app purchase possibilities, social networking and sharing features and feedback mechanisms lead to higher number of downloads.","Moreover, differences in user scores and the total number of downloads are detected in distinct subcategories of mobile health apps.","Conclusion This study contributes to the current knowledge of m-health application use by reviewing mobile health applications using content analysis and machine learning algorithms.","The content analysis adds significant value by providing classification, keywords and factors that influence download behavior and user scores in a m-health context."],"url":"http://arxiv.org/abs/2402.10789v1","category":"cs.HC"}
{"created":"2024-02-16 16:10:38","title":"EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge","abstract":"Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information distortion in quantized attention maps, demonstrated by the different distributions in quantized query and key of the self-attention mechanism. Then, the entropy and distribution guided QAT is proposed to mitigate the information distortion. Moreover, we design a token importance-aware adaptive method to dynamically quantize the tokens with different bit widths for further optimization and acceleration. Our extensive experiments verify the substantial improvements with our framework across various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x compared with its FP16 counterparts across multiple edge devices, signaling a groundbreaking advancement.","sentences":["Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations.","To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference.","However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits.","Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge.","In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices.","We first identify that the performance drop of quantization primarily stems from the information distortion in quantized attention maps, demonstrated by the different distributions in quantized query and key of the self-attention mechanism.","Then, the entropy and distribution guided QAT is proposed to mitigate the information distortion.","Moreover, we design a token importance-aware adaptive method to dynamically quantize the tokens with different bit widths for further optimization and acceleration.","Our extensive experiments verify the substantial improvements with our framework across various datasets.","Furthermore, we achieve an on-device speedup of up to 2.37x compared with its FP16 counterparts across multiple edge devices, signaling a groundbreaking advancement."],"url":"http://arxiv.org/abs/2402.10787v1","category":"cs.LG"}
{"created":"2024-02-16 16:06:08","title":"Simultaneous symmetry breaking in spontaneous Floquet states: Floquet-Nambu-Goldstone modes, Floquet thermodynamics, and the time operator","abstract":"We study simultaneous symmetry-breaking in a spontaneous Floquet state, focusing on the specific case of an atomic condensate. We first describe the quantization procedure of the Nambu-Goldstone (NG) modes for a stationary state simultaneously breaking several symmetries by invoking the generalized Gibbs ensemble, allowing for a thermodynamical description of the problem. The quantization procedure involves a Berry-Gibbs connection, which depends on the macroscopic conserved charges associated to each broken symmetry and is not invariant under generalized gauge transformations. We extend the formalism to a spontaneous Floquet state simultaneously breaking several symmetries, finding that each broken symmetry now has associated a Floquet-Nambu-Goldstone (FNG) mode with zero quasi-energy. Among them, there is a genuine temporal FNG mode arising from the continuous time-translation symmetry breaking, whose quantum amplitude provides a rare realization of a time operator in Quantum Mechanics. Furthermore, as they conserve energy, spontaneous Floquet states admit a thermodynamic description since they have a conserved Floquet charge. Both the temporal FNG mode and the Floquet thermodynamics are the distinctive features of a spontaneous Floquet state, absent in conventional, driven Floquet systems. We apply our formalism to a particular realization of spontaneous Floquet state, the CES state, which breaks $U(1)$ and time-translation symmetries, representing a time supersolid. Using the Truncated Wigner method, we numerically compute its quantum fluctuations, which are theoretically predicted to be dominated by the temporal FNG mode, observing a remarkable agreement between simulation and theory. Based on these results, we propose a feasible experimental method to observe the temporal FNG mode.","sentences":["We study simultaneous symmetry-breaking in a spontaneous Floquet state, focusing on the specific case of an atomic condensate.","We first describe the quantization procedure of the Nambu-Goldstone (NG) modes for a stationary state simultaneously breaking several symmetries by invoking the generalized Gibbs ensemble, allowing for a thermodynamical description of the problem.","The quantization procedure involves a Berry-Gibbs connection, which depends on the macroscopic conserved charges associated to each broken symmetry and is not invariant under generalized gauge transformations.","We extend the formalism to a spontaneous Floquet state simultaneously breaking several symmetries, finding that each broken symmetry now has associated a Floquet-Nambu-Goldstone (FNG) mode with zero quasi-energy.","Among them, there is a genuine temporal FNG mode arising from the continuous time-translation symmetry breaking, whose quantum amplitude provides a rare realization of a time operator in Quantum Mechanics.","Furthermore, as they conserve energy, spontaneous Floquet states admit a thermodynamic description since they have a conserved Floquet charge.","Both the temporal FNG mode and the Floquet thermodynamics are the distinctive features of a spontaneous Floquet state, absent in conventional, driven Floquet systems.","We apply our formalism to a particular realization of spontaneous Floquet state, the CES state, which breaks $U(1)$ and time-translation symmetries, representing a time supersolid.","Using the Truncated Wigner method, we numerically compute its quantum fluctuations, which are theoretically predicted to be dominated by the temporal FNG mode, observing a remarkable agreement between simulation and theory.","Based on these results, we propose a feasible experimental method to observe the temporal FNG mode."],"url":"http://arxiv.org/abs/2402.10784v1","category":"quant-ph"}
{"created":"2024-02-16 16:00:50","title":"AutoGPT+P: Affordance-based Task Planning with Large Language Models","abstract":"Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing objects by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an automatically generated object-affordance-mapping using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors. Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art LLM-based planning method SayCan on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79% on our dataset. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.","sentences":["Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities.","However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem.","To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system.","Affordances encompass the action possibilities of an agent on the environment and objects present in it.","Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects.","AutoGPT+P","leverages this representation to derive and execute a plan for a task specified by the user in natural language.","In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing objects by exploring the scene, suggesting alternatives, or providing a partial plan.","The affordance-based scene representation combines object detection with an automatically generated object-affordance-mapping using ChatGPT.","The core planning tool extends existing work by automatically correcting semantic and syntactic errors.","Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art LLM-based planning method SayCan on the SayCan instruction set.","Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79% on our dataset.","The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone."],"url":"http://arxiv.org/abs/2402.10778v1","category":"cs.RO"}
{"created":"2024-02-16 16:00:42","title":"MultiDimEr: a multi-dimensional bug analyzEr","abstract":"Background: Bugs and bug management consumes a significant amount of time and effort from software development organizations. A reduction in bugs can significantly improve the capacity for new feature development. Aims: We categorize and visualize dimensions of bug reports to identify accruing technical debt. This evidence can serve practitioners and decision makers not only as an argumentative basis for steering improvement efforts, but also as a starting point for root cause analysis, reducing overall bug inflow. Method: We implemented a tool, MultiDimEr, that analyzes and visualizes bug reports. The tool was implemented and evaluated at Ericsson. Results: We present our preliminary findings using the MultiDimEr for bug analysis, where we successfully identified components generating most of the bugs and bug trends within certain components. Conclusions: By analyzing the dimensions provided by MultiDimEr, we show that classifying and visualizing bug reports in different dimensions can stimulate discussions around bug hot spots as well as validating the accuracy of manually entered bug report attributes used in technical debt measurements such as fault slip through.","sentences":["Background: Bugs and bug management consumes a significant amount of time and effort from software development organizations.","A reduction in bugs can significantly improve the capacity for new feature development.","Aims:","We categorize and visualize dimensions of bug reports to identify accruing technical debt.","This evidence can serve practitioners and decision makers not only as an argumentative basis for steering improvement efforts, but also as a starting point for root cause analysis, reducing overall bug inflow.","Method: We implemented a tool, MultiDimEr, that analyzes and visualizes bug reports.","The tool was implemented and evaluated at Ericsson.","Results: We present our preliminary findings using the MultiDimEr for bug analysis, where we successfully identified components generating most of the bugs and bug trends within certain components.","Conclusions: By analyzing the dimensions provided by MultiDimEr, we show that classifying and visualizing bug reports in different dimensions can stimulate discussions around bug hot spots as well as validating the accuracy of manually entered bug report attributes used in technical debt measurements such as fault slip through."],"url":"http://arxiv.org/abs/2402.10777v1","category":"cs.SE"}
{"created":"2024-02-16 15:58:45","title":"In-Vivo Hyperspectral Human Brain Image Database for Brain Cancer Detection","abstract":"The use of hyperspectral imaging for medical applications is becoming more common in recent years. One of the main obstacles that researchers find when developing hyperspectral algorithms for medical applications is the lack of specific, publicly available, and hyperspectral medical data. The work described in this paper was developed within the framework of the European project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations. In this paper, the methodology followed to generate the first hyperspectral database of in-vivo human brain tissues is presented. Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed for the cases where two images of the same scene were captured consecutively. The analysis reveals that the system works more efficiently in the spectral range between 450 and 900 nm. A total of 36 hyperspectral images from 22 different patients were obtained. From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm. Four different classes were defined: normal tissue, tumor tissue, blood vessel, and background elements. All the hyperspectral data has been made available in a public repository.","sentences":["The use of hyperspectral imaging for medical applications is becoming more common in recent years.","One of the main obstacles that researchers find when developing hyperspectral algorithms for medical applications is the lack of specific, publicly available, and hyperspectral medical data.","The work described in this paper was developed within the framework of the European project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main goal the application of hyperspectral imaging to the delineation of brain tumors in real-time during neurosurgical operations.","In this paper, the methodology followed to generate the first hyperspectral database of in-vivo human brain tissues is presented.","Data was acquired employing a customized hyperspectral acquisition system capable of capturing information in the Visual and Near InfraRed (VNIR) range from 400 to 1000 nm.","Repeatability was assessed for the cases where two images of the same scene were captured consecutively.","The analysis reveals that the system works more efficiently in the spectral range between 450 and 900 nm.","A total of 36 hyperspectral images from 22 different patients were obtained.","From these data, more than 300 000 spectral signatures were labeled employing a semi-automatic methodology based on the spectral angle mapper algorithm.","Four different classes were defined: normal tissue, tumor tissue, blood vessel, and background elements.","All the hyperspectral data has been made available in a public repository."],"url":"http://arxiv.org/abs/2402.10776v1","category":"eess.IV"}
{"created":"2024-02-16 15:55:59","title":"Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants","abstract":"Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in heterogeneous data regimes. We take the reader on a journey of our discovery process. Starting with the idea of applying EF21 to an equivalent reformulation of the underlying problem which (unfortunately) requires (often impractical) machine cloning, we continue to the discovery of a new weighted version of EF21 which can (fortunately) be executed without any cloning, and finally circle back to an improved analysis of the original EF21 method. While this development applies to the simplest form of EF21, our approach naturally extends to more elaborate variants involving stochastic gradients and partial participation. Further, our technique improves the best-known theory of EF21 in the rare features regime (Richtarik et al., 2023). Finally, we validate our theoretical findings with suitable experiments.","sentences":["Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore.","In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice.","In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in heterogeneous data regimes.","We take the reader on a journey of our discovery process.","Starting with the idea of applying EF21 to an equivalent reformulation of the underlying problem which (unfortunately) requires (often impractical) machine cloning, we continue to the discovery of a new weighted version of EF21 which can (fortunately) be executed without any cloning, and finally circle back to an improved analysis of the original EF21 method.","While this development applies to the simplest form of EF21, our approach naturally extends to more elaborate variants involving stochastic gradients and partial participation.","Further, our technique improves the best-known theory of EF21 in the rare features regime (Richtarik et al., 2023).","Finally, we validate our theoretical findings with suitable experiments."],"url":"http://arxiv.org/abs/2402.10774v1","category":"cs.LG"}
{"created":"2024-02-16 15:54:58","title":"AIM: Automated Input Set Minimization for Metamorphic Security Testing","abstract":"For Web systems, which are accessible to any machine connected to internet, security is a critical concern. Although security testing can be automated by generating crafted inputs as an attacker would do, solutions to automate the test oracle, i.e., distinguishing correct from incorrect outputs for a given input, remain preliminary. Specifically, previous work has demonstrated the potential of metamorphic testing; indeed, security failures can be determined by metamorphic relations that turn valid inputs into malicious inputs and compare their outputs. However, without further guidance, metamorphic relations should be executed on a very large set of valid inputs, which is time consuming and makes metamorphic testing impractical. Hence, in this study, we propose AIM, an approach that automatically selects inputs to reduce testing costs while preserving vulnerability detection capabilities. AIM includes a clustering-based black box approach, identifying similar inputs based on their security properties. It also presents a novel genetic algorithm able to efficiently select diverse inputs while minimizing their total cost. Further, it contains a problem reduction component to reduce the search space and speed up the minimization process. We evaluated the effectiveness of AIM on two well-known web systems, Jenkins and Joomla. We compared AIM's results with four baselines in security testing. Overall, AIM reduced MRs execution time by 84 percent for Jenkins and 82 percent for Joomla while preserving full vulnerability detection. Furthermore, AIM outperformed all the considered baselines regarding vulnerability coverage. Although it has been tuned to work with Web system inputs, AIM could be applied to minimize metamorphic testing cost in other contexts.","sentences":["For Web systems, which are accessible to any machine connected to internet, security is a critical concern.","Although security testing can be automated by generating crafted inputs as an attacker would do, solutions to automate the test oracle, i.e., distinguishing correct from incorrect outputs for a given input, remain preliminary.","Specifically, previous work has demonstrated the potential of metamorphic testing; indeed, security failures can be determined by metamorphic relations that turn valid inputs into malicious inputs and compare their outputs.","However, without further guidance, metamorphic relations should be executed on a very large set of valid inputs, which is time consuming and makes metamorphic testing impractical.","Hence, in this study, we propose AIM, an approach that automatically selects inputs to reduce testing costs while preserving vulnerability detection capabilities.","AIM includes a clustering-based black box approach, identifying similar inputs based on their security properties.","It also presents a novel genetic algorithm able to efficiently select diverse inputs while minimizing their total cost.","Further, it contains a problem reduction component to reduce the search space and speed up the minimization process.","We evaluated the effectiveness of AIM on two well-known web systems, Jenkins and Joomla.","We compared AIM's results with four baselines in security testing.","Overall, AIM reduced MRs execution time by 84 percent for Jenkins and 82 percent for Joomla while preserving full vulnerability detection.","Furthermore, AIM outperformed all the considered baselines regarding vulnerability coverage.","Although it has been tuned to work with Web system inputs, AIM could be applied to minimize metamorphic testing cost in other contexts."],"url":"http://arxiv.org/abs/2402.10773v1","category":"cs.CR"}
{"created":"2024-02-16 15:49:47","title":"Generalizing Geometric Nonwindowed Scattering Transforms on Compact Riemannian Manifolds","abstract":"Let $\\mathcal{M}$ be a compact, smooth, $n$-dimensional Riemannian manifold without boundary. In this paper, we generalize nonwindowed geometric scattering transforms, which we formulate as $\\mathbf{L}^q(\\mathcal{M})$ norms of a cascade of geometric wavelet transforms and modulus operators. We then provide weighted measures for these operators, prove that these operators are well-defined under specific conditions on the manifold, invariant to the action of isometries, and stable to diffeomorphisms for $\\lambda$-bandlimited functions.","sentences":["Let $\\mathcal{M}$ be a compact, smooth, $n$-dimensional Riemannian manifold without boundary.","In this paper, we generalize nonwindowed geometric scattering transforms, which we formulate as $\\mathbf{L}^q(\\mathcal{M})$ norms of a cascade of geometric wavelet transforms and modulus operators.","We then provide weighted measures for these operators, prove that these operators are well-defined under specific conditions on the manifold, invariant to the action of isometries, and stable to diffeomorphisms for $\\lambda$-bandlimited functions."],"url":"http://arxiv.org/abs/2402.10771v1","category":"math.FA"}
{"created":"2024-02-16 15:48:33","title":"How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?","abstract":"Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements under specific conditions, their reliability is highly context-dependent. Our findings enhance the understanding of how automatic methods should be applied and interpreted when developing and evaluating instruction-tuned LLMs.","sentences":["Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation.","In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting.","In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type.","Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer.","The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks.","In summary, we find that, while automatic evaluation methods can approximate human judgements under specific conditions, their reliability is highly context-dependent.","Our findings enhance the understanding of how automatic methods should be applied and interpreted when developing and evaluating instruction-tuned LLMs."],"url":"http://arxiv.org/abs/2402.10770v1","category":"cs.CL"}
{"created":"2024-02-16 15:48:24","title":"Distillation Enhanced Generative Retrieval","abstract":"Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden to the inference stage. We conduct experiments on four public datasets, and the results indicate that DGR achieves state-of-the-art performance among the generative retrieval methods. Additionally, DGR demonstrates exceptional robustness and generalizability with various teacher models and distillation losses.","sentences":["Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target.","This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods.","In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR.","DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels.","This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden to the inference stage.","We conduct experiments on four public datasets, and the results indicate that DGR achieves state-of-the-art performance among the generative retrieval methods.","Additionally, DGR demonstrates exceptional robustness and generalizability with various teacher models and distillation losses."],"url":"http://arxiv.org/abs/2402.10769v1","category":"cs.CL"}
{"created":"2024-02-16 15:47:55","title":"Optimal Savings and Value of Population in A Stochastic Environment: Transient Behavior","abstract":"We extend the work on optimal investment and consumption of a population considered in [2] to a general stochastic setting over a finite time horizon. We incorporate the Cobb-Douglas production function in the capital dynamics while the consumption utility function and the drift rate in the population dynamics can be general, in contrast with [2, 30, 31]. The dynamic programming formulation yields an unconventional nonlinear Hamilton-Jacobi-Bellman (HJB) equation, in which the Cobb-Douglas production function as the coefficient of the gradient of the value function induces the mismatching of power rates between capital and population. Moreover, the equation has a very singular term, essentially a very negative power of the partial derivative of the value function with respect to the capital, coming from the optimization of control, and their resolution turns out to be a complex problem not amenable to classical analysis. To show that this singular term, which has not been studied in any physical systems yet, does not actually blow up, we establish new pointwise generalized power laws for the partial derivative of the value function. Our contribution lies in providing a theoretical treatment that combines both the probabilistic approach and theory of partial differential equations to derive the pointwise upper and lower bounds as well as energy estimates in weighted Sobolev spaces. By then, we accomplish showing the well-posedness of classical solutions to a non-canonical parabolic equation arising from a long-lasting problem in macroeconomics.","sentences":["We extend the work on optimal investment and consumption of a population considered in [2] to a general stochastic setting over a finite time horizon.","We incorporate the Cobb-Douglas production function in the capital dynamics while the consumption utility function and the drift rate in the population dynamics can be general, in contrast with [2, 30, 31].","The dynamic programming formulation yields an unconventional nonlinear Hamilton-Jacobi-Bellman (HJB) equation, in which the Cobb-Douglas production function as the coefficient of the gradient of the value function induces the mismatching of power rates between capital and population.","Moreover, the equation has a very singular term, essentially a very negative power of the partial derivative of the value function with respect to the capital, coming from the optimization of control, and their resolution turns out to be a complex problem not amenable to classical analysis.","To show that this singular term, which has not been studied in any physical systems yet, does not actually blow up, we establish new pointwise generalized power laws for the partial derivative of the value function.","Our contribution lies in providing a theoretical treatment that combines both the probabilistic approach and theory of partial differential equations to derive the pointwise upper and lower bounds as well as energy estimates in weighted Sobolev spaces.","By then, we accomplish showing the well-posedness of classical solutions to a non-canonical parabolic equation arising from a long-lasting problem in macroeconomics."],"url":"http://arxiv.org/abs/2402.10768v1","category":"math.AP"}
{"created":"2024-02-16 15:41:23","title":"Inference to the Best Explanation in Large Language Models","abstract":"While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.","sentences":["While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood.","This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations.","IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty.","Extensive experiments are conducted on Causal Question Answering (CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2).","The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically more efficient and interpretable.","Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools."],"url":"http://arxiv.org/abs/2402.10767v1","category":"cs.CL"}
{"created":"2024-02-16 15:39:51","title":"Policy Learning for Off-Dynamics RL with Deficient Support","abstract":"Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addressing large dynamics mismatch adaptation. We move away from the stringent full support condition of earlier research, focusing instead on crafting an effective policy for the target domain. Our proposed approach is simple but effective. It is anchored in the central concepts of the skewing and extension of source support towards target support to mitigate support deficiencies. Through comprehensive testing on a varied set of benchmarks, our method's efficacy stands out, showcasing notable improvements over previous techniques.","sentences":["Reinforcement Learning (RL) can effectively learn complex policies.","However, learning these policies often demands extensive trial-and-error interactions with the environment.","In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns.","As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment.","However, this process poses challenges.","Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments.","Past research posited that the source domain must encompass all possible target transitions, a condition we term full support.","However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise.","In this paper, our emphasis shifts to addressing large dynamics mismatch adaptation.","We move away from the stringent full support condition of earlier research, focusing instead on crafting an effective policy for the target domain.","Our proposed approach is simple but effective.","It is anchored in the central concepts of the skewing and extension of source support towards target support to mitigate support deficiencies.","Through comprehensive testing on a varied set of benchmarks, our method's efficacy stands out, showcasing notable improvements over previous techniques."],"url":"http://arxiv.org/abs/2402.10765v1","category":"cs.LG"}
{"created":"2024-02-16 15:38:00","title":"On Explaining Unfairness: An Overview","abstract":"Algorithmic fairness and explainability are foundational elements for achieving responsible AI. In this paper, we focus on their interplay, a research area that is recently receiving increasing attention. To this end, we first present two comprehensive taxonomies, each representing one of the two complementary fields of study: fairness and explanations. Then, we categorize explanations for fairness into three types: (a) Explanations to enhance fairness metrics, (b) Explanations to help us understand the causes of (un)fairness, and (c) Explanations to assist us in designing methods for mitigating unfairness. Finally, based on our fairness and explanation taxonomies, we present undiscovered literature paths revealing gaps that can serve as valuable insights for future research.","sentences":["Algorithmic fairness and explainability are foundational elements for achieving responsible AI.","In this paper, we focus on their interplay, a research area that is recently receiving increasing attention.","To this end, we first present two comprehensive taxonomies, each representing one of the two complementary fields of study: fairness and explanations.","Then, we categorize explanations for fairness into three types: (a) Explanations to enhance fairness metrics, (b) Explanations to help us understand the causes of (un)fairness, and (c) Explanations to assist us in designing methods for mitigating unfairness.","Finally, based on our fairness and explanation taxonomies, we present undiscovered literature paths revealing gaps that can serve as valuable insights for future research."],"url":"http://arxiv.org/abs/2402.10762v1","category":"cs.AI"}
{"created":"2024-02-16 15:34:07","title":"RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction","abstract":"Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose a novel model, RAGIC, which introduces sequence generation for stock interval prediction to quantify uncertainty more effectively. Our approach leverages a Generative Adversarial Network (GAN) to produce future price sequences infused with randomness inherent in financial markets. RAGIC's generator includes a risk module, capturing the risk perception of informed investors, and a temporal module, accounting for historical price trends and seasonality. This multi-faceted generator informs the creation of risk-sensitive intervals through statistical inference, incorporating horizon-wise insights. The interval's width is carefully adjusted to reflect market volatility. Importantly, our approach relies solely on publicly available data and incurs only low computational overhead. RAGIC's evaluation across globally recognized broad-based indices demonstrates its balanced performance, offering both accuracy and informativeness. Achieving a consistent 95% coverage, RAGIC maintains a narrow interval width. This promising outcome suggests that our approach effectively addresses the challenges of stock market prediction while incorporating vital risk considerations.","sentences":["Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors.","Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk.","To bridge this gap, we propose a novel model, RAGIC, which introduces sequence generation for stock interval prediction to quantify uncertainty more effectively.","Our approach leverages a Generative Adversarial Network (GAN) to produce future price sequences infused with randomness inherent in financial markets.","RAGIC's generator includes a risk module, capturing the risk perception of informed investors, and a temporal module, accounting for historical price trends and seasonality.","This multi-faceted generator informs the creation of risk-sensitive intervals through statistical inference, incorporating horizon-wise insights.","The interval's width is carefully adjusted to reflect market volatility.","Importantly, our approach relies solely on publicly available data and incurs only low computational overhead.","RAGIC's evaluation across globally recognized broad-based indices demonstrates its balanced performance, offering both accuracy and informativeness.","Achieving a consistent 95% coverage, RAGIC maintains a narrow interval width.","This promising outcome suggests that our approach effectively addresses the challenges of stock market prediction while incorporating vital risk considerations."],"url":"http://arxiv.org/abs/2402.10760v1","category":"q-fin.ST"}
{"created":"2024-02-16 15:28:41","title":"Stochastic Localization via Iterative Posterior Sampling","abstract":"Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics.","sentences":["Building upon score-based learning, new interest in stochastic localization techniques has recently emerged.","In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics.","Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively.","This work contributes to fill this gap.","We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules.","We provide a complete methodology, $\\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution.","Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines.","We illustrate the benefits and applicability of SLIPS on several benchmarks, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics."],"url":"http://arxiv.org/abs/2402.10758v1","category":"stat.ML"}
{"created":"2024-02-16 15:25:56","title":"Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering","abstract":"Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters. By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance.","sentences":["Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability.","To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters.","By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization.","Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance."],"url":"http://arxiv.org/abs/2402.10756v1","category":"cs.LG"}
{"created":"2024-02-16 15:25:38","title":"Gravitational Waves: Echoes of the Biggest Bangs since the Big Bang and/or BSM Physics?","abstract":"\"If one could ever prove the existence of gravitational waves, the processes responsible for their generation would probably be much more curious and interesting than even the waves themselves.\" (Gustav Mie, 1868 - 1957) The discovery of gravitational waves has opened new windows on astrophysics, cosmology and physics beyond the Standard Model (BSM). Measurements by the LIGO, Virgo and KAGRA Collaborations of stellar-mass binaries and neutron star mergers have shown that gravitational waves travel at close to the velocity of light, and also constrain BSM possibilities such as a graviton mass and Lorentz violation in gravitational wave propagation. Follow-up measurements of neutron star mergers have provided evidence for the production of heavy elements, possibly including some essential for human life. The gravitational waves in the nanoHz range observed by Pulsar Timing Arrays (PTAs) may have been emitted by supermassive black hole binaries, but might also have originated from BSM cosmological scenarios such as cosmic strings, or phase transitions in the early Universe. The answer to the question in the title may be provided by gravitational-wave detectors at higher frequencies, such as LISA and atom interferometers.","sentences":["\"If one could ever prove the existence of gravitational waves, the processes responsible for their generation would probably be much more curious and interesting than even the waves themselves.\"","(Gustav Mie, 1868 - 1957)","The discovery of gravitational waves has opened new windows on astrophysics, cosmology and physics beyond the Standard Model (BSM).","Measurements by the LIGO, Virgo and KAGRA Collaborations of stellar-mass binaries and neutron star mergers have shown that gravitational waves travel at close to the velocity of light, and also constrain BSM possibilities such as a graviton mass and Lorentz violation in gravitational wave propagation.","Follow-up measurements of neutron star mergers have provided evidence for the production of heavy elements, possibly including some essential for human life.","The gravitational waves in the nanoHz range observed by Pulsar Timing Arrays (PTAs) may have been emitted by supermassive black hole binaries, but might also have originated from BSM cosmological scenarios such as cosmic strings, or phase transitions in the early Universe.","The answer to the question in the title may be provided by gravitational-wave detectors at higher frequencies, such as LISA and atom interferometers."],"url":"http://arxiv.org/abs/2402.10755v1","category":"hep-ph"}
{"created":"2024-02-16 15:19:46","title":"ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages","abstract":"Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in https://github.com/Junjie-Ye/ToolSword.","sentences":["Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios.","While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application.","To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning.","Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage.","Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to.","Moreover, we conduct further studies with the aim of fostering research on tool learning safety.","The data is released in https://github.com/Junjie-Ye/ToolSword."],"url":"http://arxiv.org/abs/2402.10753v1","category":"cs.CL"}
{"created":"2024-02-16 15:16:33","title":"Towards benchmarking of Solidity verification tools","abstract":"Formal verification of smart contracts has become a hot topic in academic and industrial research, given the growing value of assets managed by decentralized applications and the consequent incentive for adversaries to tamper with them. Most of the current research on the verification of contracts revolves around Solidity, the main high-level language supported by Ethereum and other leading blockchains. Although bug detection tools for Solidity have been proliferating almost since the inception of Ethereum, only in the last few years we have seen verification tools capable of proving that a contract respects some desirable properties. An open issue is how to evaluate and compare the effectiveness of these tools: indeed, the existing benchmarks for general-purpose programming languages cannot be adapted to Solidity, given substantial differences in the programming model and in the desirable properties. We address this problem by proposing an open benchmark for Solidity verification tools. By exploiting our benchmark, we compare two leading tools, SolCMC and Certora, discussing their completeness, soundness and expressiveness limitations.","sentences":["Formal verification of smart contracts has become a hot topic in academic and industrial research, given the growing value of assets managed by decentralized applications and the consequent incentive for adversaries to tamper with them.","Most of the current research on the verification of contracts revolves around Solidity, the main high-level language supported by Ethereum and other leading blockchains.","Although bug detection tools for Solidity have been proliferating almost since the inception of Ethereum, only in the last few years we have seen verification tools capable of proving that a contract respects some desirable properties.","An open issue is how to evaluate and compare the effectiveness of these tools: indeed, the existing benchmarks for general-purpose programming languages cannot be adapted to Solidity, given substantial differences in the programming model and in the desirable properties.","We address this problem by proposing an open benchmark for Solidity verification tools.","By exploiting our benchmark, we compare two leading tools, SolCMC and Certora, discussing their completeness, soundness and expressiveness limitations."],"url":"http://arxiv.org/abs/2402.10750v1","category":"cs.LO"}
{"created":"2024-02-16 15:15:00","title":"Inertia-gravity waves in geophysical vortices","abstract":"Pancake-like vortices are often generated by turbulence in geophysical flows. Here, we study the inertia-gravity oscillations that can exist within such geophysical vortices, due to the combined action of rotation and gravity. We consider a fluid enclosed within a triaxial ellipsoid, which is stratified in density with a constant Brunt-V\\\"ais\\\"al\\\"a frequency (using the Boussinesq approximation) and uniformly rotating along a (possibly) tilted axis with respect to gravity. The wave problem is then governed by a mixed hyperbolic-elliptic equation for the velocity. As in the rotating non-stratified case considered by Vantieghem (2014, Proc. R. Soc. A, 470, 20140093, doi:10.1098/rspa.2014.0093), we find that the spectrum is pure point in ellipsoids (i.e. only consists of eigenvalues) with smooth polynomial eigenvectors. Then, we characterise the spectrum using numerical computations (obtained with a bespoke Galerkin method) and asymptotic spectral theory. Finally, the results are discussed in light of natural applications (e.g. for Mediterranean eddies or Jupiter's vortices).","sentences":["Pancake-like vortices are often generated by turbulence in geophysical flows.","Here, we study the inertia-gravity oscillations that can exist within such geophysical vortices, due to the combined action of rotation and gravity.","We consider a fluid enclosed within a triaxial ellipsoid, which is stratified in density with a constant Brunt-V\\\"ais\\\"al\\\"a frequency (using the Boussinesq approximation) and uniformly rotating along a (possibly) tilted axis with respect to gravity.","The wave problem is then governed by a mixed hyperbolic-elliptic equation for the velocity.","As in the rotating non-stratified case considered by Vantieghem (2014, Proc. R. Soc.","A, 470, 20140093, doi:10.1098/rspa.2014.0093), we find that the spectrum is pure point in ellipsoids (i.e. only consists of eigenvalues) with smooth polynomial eigenvectors.","Then, we characterise the spectrum using numerical computations (obtained with a bespoke Galerkin method) and asymptotic spectral theory.","Finally, the results are discussed in light of natural applications (e.g. for Mediterranean eddies or Jupiter's vortices)."],"url":"http://arxiv.org/abs/2402.10749v1","category":"physics.flu-dyn"}
{"created":"2024-02-16 15:13:30","title":"Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting","abstract":"This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.","sentences":["This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge.","We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference.","Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models."],"url":"http://arxiv.org/abs/2402.10747v1","category":"cs.LG"}
{"created":"2024-02-16 15:01:24","title":"GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models","abstract":"The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE","sentences":["The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs).","However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods.","This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references.","To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results.","With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations.","Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality.","Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE"],"url":"http://arxiv.org/abs/2402.10744v1","category":"cs.CL"}
{"created":"2024-02-16 14:59:55","title":"Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research","abstract":"Entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis. Most of the information of the Yishui school of traditional Chinese Medicine (TCM) is stored in the form of unstructured classical Chinese text. The key information extraction of TCM texts plays an important role in mining and studying the academic schools of TCM. In order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional Chinese medicine texts, and uses the common weighting technology of TF-IDF information retrieval and data mining to extract important key entity information in different ancient books. The dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the knowledge graph of Yishui school and the use of artificial intelligence methods to carry out the research of TCM academic schools.","sentences":["Entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis.","Most of the information of the Yishui school of traditional Chinese Medicine (TCM) is stored in the form of unstructured classical Chinese text.","The key information extraction of TCM texts plays an important role in mining and studying the academic schools of TCM.","In order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional Chinese medicine texts, and uses the common weighting technology of TF-IDF information retrieval and data mining to extract important key entity information in different ancient books.","The dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the knowledge graph of Yishui school and the use of artificial intelligence methods to carry out the research of TCM academic schools."],"url":"http://arxiv.org/abs/2402.10743v1","category":"cs.CL"}
{"created":"2024-02-16 14:54:55","title":"Triples and quadruples of consecutive squares or non-squares in a finite field","abstract":"\\begin{abstract} Let $\\F$ be the finite field of odd prime power order $q$, We find explicit expressions for the number of triples $\\{\\al-1,\\al,\\al+1 \\}$ of consecutive non-zero squares in $\\F$ and similarly for the number of triples of consecutive non-square elements. A key ingredient is the evaluation of Jacobsthal sums over general finite fields by Katre and Rajwade. This extends results of Monzingo(1985) to non-prime fields. Curiously, the same machinery alows the evaluation of the number of consecutive quadruples $\\{\\al -1, \\al,\\al+1, \\al +2\\}$ of square and non-squares over $\\F$, when $q$ is a power of 5. \\end{abstract}","sentences":["\\begin{abstract} Let $\\F$ be the finite field of odd prime power order $q$, We find explicit expressions for the number of triples $\\{\\al-1,\\al,\\al+1 \\}$ of consecutive non-zero squares in $\\F$ and similarly for the number of triples of consecutive non-square elements.","A key ingredient is the evaluation of Jacobsthal sums over general finite fields by Katre and Rajwade.","This extends results of Monzingo(1985) to non-prime fields.","Curiously, the same machinery alows the evaluation of the number of consecutive quadruples $\\{\\al -1, \\al,\\al+1, \\al +2\\}$ of square and non-squares over $\\F$, when $q$ is a power of 5. \\end{abstract}"],"url":"http://arxiv.org/abs/2402.10737v1","category":"math.NT"}
{"created":"2024-02-16 14:48:18","title":"${\\tt maria}$: A novel simulator for forecasting (sub-)mm observations","abstract":"Submillimeter single-dish telescopes offer two key advantages compared to interferometers: they can efficiently map larger portions of the sky and recover larger spatial scales. Nonetheless, fluctuations in the atmosphere limit the accurate retrieval of signals from astronomical sources. Therefore, we introduce a user-friendly simulator named ${\\tt maria}$ to optimize scanning strategies and instrument designs to efficiently reduce atmospheric noise and filtering effects. We further use this tool to produce synthetic time streams and maps from hydrodynamical simulations, enabling a fair comparison between theory and reality. ${\\tt maria}$ has implemented a suite of telescope and instrument designs intended to mimic current and future facilities. To generate synthetic time-ordered data, each mock observatory scans through the atmosphere in a configurable pattern over the celestial object. We generate evolving and location-and-time-specific weather for each of the fiducial sites using a combination of satellite and ground-based measurements. While ${\\tt maria}$ is a generic virtual telescope, this study specifically focuses on mimicking broadband bolometers observing at 100 GHz. To validate our virtual telescope, we compare the mock time streams with real MUSTANG-2 observations and find that they are quantitatively similar by conducting a k-sample Anderson-Darling test resulting in p<0.001. Subsequently, we image the time-ordered data to create noise maps and mock observations of clusters of galaxies for both MUSTANG-2 and an instrument concept for the 50m Atacama Large Aperture Submillimeter Telescope (AtLAST). Furthermore, using ${\\tt maria}$, we find that a 50m dish provides the highest levels of correlation of atmospheric signals across adjacent detectors compared to smaller apertures (e.g., 42-cm and 6-m survey experiments), facilitating removal of atmospheric signal on large scales.","sentences":["Submillimeter single-dish telescopes offer two key advantages compared to interferometers: they can efficiently map larger portions of the sky and recover larger spatial scales.","Nonetheless, fluctuations in the atmosphere limit the accurate retrieval of signals from astronomical sources.","Therefore, we introduce a user-friendly simulator named ${\\tt maria}$ to optimize scanning strategies and instrument designs to efficiently reduce atmospheric noise and filtering effects.","We further use this tool to produce synthetic time streams and maps from hydrodynamical simulations, enabling a fair comparison between theory and reality.","${\\tt maria}$ has implemented a suite of telescope and instrument designs intended to mimic current and future facilities.","To generate synthetic time-ordered data, each mock observatory scans through the atmosphere in a configurable pattern over the celestial object.","We generate evolving and location-and-time-specific weather for each of the fiducial sites using a combination of satellite and ground-based measurements.","While ${\\tt maria}$ is a generic virtual telescope, this study specifically focuses on mimicking broadband bolometers observing at 100 GHz.","To validate our virtual telescope, we compare the mock time streams with real MUSTANG-2 observations and find that they are quantitatively similar by conducting a k-sample Anderson-Darling test resulting in p<0.001.","Subsequently, we image the time-ordered data to create noise maps and mock observations of clusters of galaxies for both MUSTANG-2 and an instrument concept for the 50m Atacama Large Aperture Submillimeter Telescope (AtLAST).","Furthermore, using ${\\tt maria}$, we find that a 50m dish provides the highest levels of correlation of atmospheric signals across adjacent detectors compared to smaller apertures (e.g., 42-cm and 6-m survey experiments), facilitating removal of atmospheric signal on large scales."],"url":"http://arxiv.org/abs/2402.10731v1","category":"astro-ph.IM"}
{"created":"2024-02-16 14:47:41","title":"Quantum switch as a thermodynamic resource in the context of passive states","abstract":"In recent years many works have explored possible advantages of indefinite causal order, with the main focus on its controlled implementation known as quantum switch. In this paper, we tackle advantages in quantum thermodynamics, studying whether quantum switch is capable of activating a passive state: either alone or with extra resources (active control state) and/or operations (measurement of the control system). By disproving the first possibility and confirming the second one, we show that quantum switch is not a thermodynamic resource in the discussed context, though, it can facilitate work extraction given external resources. We discuss our findings by considering specific examples: a qubit system subject to rotations around the x and y axes in the Bloch sphere, as well as general unitaries from the U(2) group; and the system as a quantum harmonic oscillator with displacement operators, and with a combination of displacement and squeeze operators.","sentences":["In recent years many works have explored possible advantages of indefinite causal order, with the main focus on its controlled implementation known as quantum switch.","In this paper, we tackle advantages in quantum thermodynamics, studying whether quantum switch is capable of activating a passive state: either alone or with extra resources (active control state) and/or operations (measurement of the control system).","By disproving the first possibility and confirming the second one, we show that quantum switch is not a thermodynamic resource in the discussed context, though, it can facilitate work extraction given external resources.","We discuss our findings by considering specific examples: a qubit system subject to rotations around the x and y axes in the Bloch sphere, as well as general unitaries from the U(2) group; and the system as a quantum harmonic oscillator with displacement operators, and with a combination of displacement and squeeze operators."],"url":"http://arxiv.org/abs/2402.10730v1","category":"quant-ph"}
{"created":"2024-02-16 14:40:22","title":"Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules","abstract":"Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation.","sentences":["Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains.","Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them.","Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear.","In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations.","Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them.","To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation."],"url":"http://arxiv.org/abs/2402.10727v1","category":"stat.ML"}
{"created":"2024-02-16 14:36:58","title":"Learning Planning Action Models from State Traces","abstract":"Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the state-of-the-art learning tool FAMA on a large collection of IPC benchmarks. The evaluation shows that our new algorithms are faster, can handle larger inputs and provide better results in terms of learning action models more similar to reference models.","sentences":["Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned.","Therefore their only task is to deduce the preconditions and effects of the given actions.","In this work, we explore learning in situations when the parameters of learned actions are not provided.","We define two levels of trace quality based on which information is provided and present an algorithm for each.","In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters.","In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions.","Here we still need to deduce the types of the parameters in the learned actions.","We experimentally evaluate the proposed algorithms and compare them with the state-of-the-art learning tool FAMA on a large collection of IPC benchmarks.","The evaluation shows that our new algorithms are faster, can handle larger inputs and provide better results in terms of learning action models more similar to reference models."],"url":"http://arxiv.org/abs/2402.10726v1","category":"cs.AI"}
{"created":"2024-02-16 14:31:33","title":"Cloud Kitchen: Using Planning-based Composite AI to Optimize Food Delivery Process","abstract":"The global food delivery market provides many opportunities for AI-based services that can improve the efficiency of feeding the world. This paper presents the Cloud Kitchen platform as a decision-making tool for restaurants with food delivery and a simulator to evaluate the impact of the decisions. The platform consists of a Technology-Specific Bridge (TSB) that provides an interface for communicating with restaurants or the simulator. TSB uses a PDDL model to represent decisions embedded in the Unified Planning Framework (UPF). Decision-making, which concerns allocating customers' orders to vehicles and deciding in which order the customers will be served (for each vehicle), is done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool for this problem. We show that decisions made by our platform can improve customer satisfaction by reducing the number of delayed deliveries using a real-world historical dataset.","sentences":["The global food delivery market provides many opportunities for AI-based services that can improve the efficiency of feeding the world.","This paper presents the Cloud Kitchen platform as a decision-making tool for restaurants with food delivery and a simulator to evaluate the impact of the decisions.","The platform consists of a Technology-Specific Bridge (TSB) that provides an interface for communicating with restaurants or the simulator.","TSB uses a PDDL model to represent decisions embedded in the Unified Planning Framework (UPF).","Decision-making, which concerns allocating customers' orders to vehicles and deciding in which order the customers will be served (for each vehicle), is done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool for this problem.","We show that decisions made by our platform can improve customer satisfaction by reducing the number of delayed deliveries using a real-world historical dataset."],"url":"http://arxiv.org/abs/2402.10725v1","category":"cs.AI"}
{"created":"2024-02-16 14:23:10","title":"Assessment of random-phase approximation and second order M\u00f8ller-Plesset perturbation theory for many-body interactions in solid ethane, ethylene, and acetylene","abstract":"The relative energies of different phases or polymorphs of molecular solids can be small, less than a kiloJoule/mol. Reliable description of such energy differences requires high quality treatment of electron correlations, typically beyond that achievable by routinely applicable density functional theory approximations (DFT). At the same time, high-level wave function theory is currently too computationally expensive. Methods employing intermediate level of approximations, such as M{\\o}ller-Plesset (MP) perturbation theory and the random-phase approximation (RPA) are potentially useful. However, their development and application for molecular solids has been impeded by the scarcity of necessary benchmark data for these systems. In this work we employ the coupled-clusters method with singles, doubles and perturbative triples (CCSD(T)) to obtain a reference-quality many-body expansion of the binding energy of four crystalline hydrocarbons with a varying $\\pi$-electron character: ethane, ethene, and cubic and orthorhombic forms of acetylene. The binding energy is resolved into explicit dimer, trimer, and tetramer contributions, which facilitates the analysis of errors in the approximate approaches. With the newly generated benchmark data we test the accuracy of MP2 and non-self-consistent RPA. We find that both of the methods poorly describe the non-additive many-body interactions in closely packed clusters. Using different DFT input states for RPA leads to similar total binding energies, but the many-body components strongly depend on the choice of the exchange-correlation functional.","sentences":["The relative energies of different phases or polymorphs of molecular solids can be small, less than a kiloJoule/mol.","Reliable description of such energy differences requires high quality treatment of electron correlations, typically beyond that achievable by routinely applicable density functional theory approximations (DFT).","At the same time, high-level wave function theory is currently too computationally expensive.","Methods employing intermediate level of approximations, such as M{\\o}ller-Plesset (MP) perturbation theory and the random-phase approximation (RPA) are potentially useful.","However, their development and application for molecular solids has been impeded by the scarcity of necessary benchmark data for these systems.","In this work we employ the coupled-clusters method with singles, doubles and perturbative triples (CCSD(T)) to obtain a reference-quality many-body expansion of the binding energy of four crystalline hydrocarbons with a varying $\\pi$-electron character: ethane, ethene, and cubic and orthorhombic forms of acetylene.","The binding energy is resolved into explicit dimer, trimer, and tetramer contributions, which facilitates the analysis of errors in the approximate approaches.","With the newly generated benchmark data we test the accuracy of MP2 and non-self-consistent RPA.","We find that both of the methods poorly describe the non-additive many-body interactions in closely packed clusters.","Using different DFT input states for RPA leads to similar total binding energies, but the many-body components strongly depend on the choice of the exchange-correlation functional."],"url":"http://arxiv.org/abs/2402.10720v1","category":"physics.chem-ph"}
{"created":"2024-02-16 14:19:33","title":"BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+ Breast Cancer Through Multifeature and Multimodal Data Fusion","abstract":"Breast cancer is a significant health concern affecting millions of women worldwide. Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes. Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features. We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to capture the interplay between them. Additionally, clinical data is incorporated using a feed-forward network (FFN), further enhancing predictive performance and achieving comprehensive multimodal feature integration. Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge in the field. The proposed model achieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with prognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88--4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80--4.68, p<0.005). The proposed method not only improves model performance but also addresses a critical gap in handling imbalanced data.","sentences":["Breast cancer is a significant health concern affecting millions of women worldwide.","Accurate survival risk stratification plays a crucial role in guiding personalised treatment decisions and improving patient outcomes.","Here we present BioFusionNet, a deep learning framework that fuses image-derived features with genetic and clinical data to achieve a holistic patient profile and perform survival risk stratification of ER+ breast cancer patients.","We employ multiple self-supervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches to capture detailed histopathological image features.","We then utilise a variational autoencoder (VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention network, generating patient-level features.","Next, we develop a co-dual-cross-attention mechanism to combine the histopathological features with genetic data, enabling the model to capture the interplay between them.","Additionally, clinical data is incorporated using a feed-forward network (FFN), further enhancing predictive performance and achieving comprehensive multimodal feature integration.","Furthermore, we introduce a weighted Cox loss function, specifically designed to handle imbalanced survival data, which is a common challenge in the field.","The proposed model achieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84, outperforming state-of-the-art methods.","It predicts risk (high versus low) with prognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88--4.78, p<0.005), and maintains independent significance in multivariate analysis incorporating standard clinicopathological variables (HR=2.91, 95% CI: 1.80--4.68, p<0.005).","The proposed method not only improves model performance but also addresses a critical gap in handling imbalanced data."],"url":"http://arxiv.org/abs/2402.10717v1","category":"cs.CV"}
{"created":"2024-02-16 14:15:15","title":"An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference","abstract":"The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find that cross-lingual vocabulary adaptation substantially contributes to LLM inference speedups of up to 271.5%. We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models.","sentences":["The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data.","Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English.","This results in increased inference time and costs.","Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance.","However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored.","In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks.","We find that cross-lingual vocabulary adaptation substantially contributes to LLM inference speedups of up to 271.5%.","We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models."],"url":"http://arxiv.org/abs/2402.10712v1","category":"cs.CL"}
{"created":"2024-02-16 14:14:23","title":"StableLego: Stability Analysis of Block Stacking Assembly","abstract":"Recent advancements in robotics enable robots to accomplish complex assembly tasks. However, designing an assembly requires a non-trivial effort since a slight variation in the design could significantly affect the task feasibility. It is critical to ensure the physical feasibility of the assembly design so that the assembly task can be successfully executed. To address the challenge, this paper studies the physical stability of assembly structures, in particular, block stacking assembly, where people use cubic blocks to build 3D structures (e.g., Lego constructions). The paper proposes a new optimization formulation, which optimizes over force balancing equations, for inferring the structural stability of 3D block-stacking structures. The proposed stability analysis is tested and verified on hand-crafted Lego examples. The experiment results demonstrate that the proposed stability analysis can correctly predict whether the structure is stable. In addition, it outperforms the existing methods since it can locate the weakest parts in the design, and more importantly, solve any given assembly structure. To further validate the proposed analysis formulation, we provide StableLego: a comprehensive dataset including more than 50k 3D objects with their Lego layouts. We test the proposed stability analysis and include the stability inference for each corresponding object in StableLego. Our code and the dataset are available at https://github.com/intelligent-control-lab/StableLego.","sentences":["Recent advancements in robotics enable robots to accomplish complex assembly tasks.","However, designing an assembly requires a non-trivial effort since a slight variation in the design could significantly affect the task feasibility.","It is critical to ensure the physical feasibility of the assembly design so that the assembly task can be successfully executed.","To address the challenge, this paper studies the physical stability of assembly structures, in particular, block stacking assembly, where people use cubic blocks to build 3D structures (e.g., Lego constructions).","The paper proposes a new optimization formulation, which optimizes over force balancing equations, for inferring the structural stability of 3D block-stacking structures.","The proposed stability analysis is tested and verified on hand-crafted Lego examples.","The experiment results demonstrate that the proposed stability analysis can correctly predict whether the structure is stable.","In addition, it outperforms the existing methods since it can locate the weakest parts in the design, and more importantly, solve any given assembly structure.","To further validate the proposed analysis formulation, we provide StableLego: a comprehensive dataset including more than 50k 3D objects with their Lego layouts.","We test the proposed stability analysis and include the stability inference for each corresponding object in StableLego.","Our code and the dataset are available at https://github.com/intelligent-control-lab/StableLego."],"url":"http://arxiv.org/abs/2402.10711v1","category":"cs.RO"}
{"created":"2024-02-16 14:06:28","title":"Semi-algebraic Geometry and generic Hamiltonian stability","abstract":"The steepness property is a local geometric transversality condition on the gradient of a $C^2$-function which is fundamental in order to ensure the stability of sufficiently-regular nearly-integrable Hamiltonian systems over long timespans. Steep functions were originally introduced by Nekhoroshev, who also proved their genericity. Namely, given a pair of positive integers $r,n$, with $r$ high enough, and a point $I_0\\in \\mathbb{R}^n$, the Taylor polynomials of those $C^{2r-1}$ functions which are not steep around $I_0$ are contained in a semi-algebraic set of positive codimension in the space of polynomials of $n$ variables and degree bounded by $r$. The demonstration of this result was originally published in 1973 and has been hardly studied ever since, probably due to the fact that it involves no arguments of dynamical systems: it makes use of quantitative reasonings of real-algebraic geometry and complex analysis. The aim of the present work is two-fold. In the first part, the original proof of the genericity of steepness is rewritten by making use of modern tools of real-algebraic geometry: this allows to clarify the original reasonings, that were obscure or sketchy in many parts. In particular, Yomdin's Lemma on the analytic reparametrization of semi-algebraic sets, together with non trivial estimates on the codimension of certain algebraic varieties, turn out to be the fundamental ingredients to prove the genericity of steepness. The second part of this work is entirely new and is devoted to the formulation of explicit algebraic criteria to check steepness of any given sufficiently regular function, which constitutes a very important result for applications, as the original definition of steepness is not constructive. These criteria involve both the derivatives of the studied function up to any given order and external real parameters that, generically, belong to compact sets.","sentences":["The steepness property is a local geometric transversality condition on the gradient of a $C^2$-function which is fundamental in order to ensure the stability of sufficiently-regular nearly-integrable Hamiltonian systems over long timespans.","Steep functions were originally introduced by Nekhoroshev, who also proved their genericity.","Namely, given a pair of positive integers $r,n$, with $r$ high enough, and a point $I_0\\in \\mathbb{R}^n$, the Taylor polynomials of those $C^{2r-1}$ functions which are not steep around $I_0$ are contained in a semi-algebraic set of positive codimension in the space of polynomials of $n$ variables and degree bounded by $r$. The demonstration of this result was originally published in 1973 and has been hardly studied ever since, probably due to the fact that it involves no arguments of dynamical systems: it makes use of quantitative reasonings of real-algebraic geometry and complex analysis.","The aim of the present work is two-fold.","In the first part, the original proof of the genericity of steepness is rewritten by making use of modern tools of real-algebraic geometry: this allows to clarify the original reasonings, that were obscure or sketchy in many parts.","In particular, Yomdin's Lemma on the analytic reparametrization of semi-algebraic sets, together with non trivial estimates on the codimension of certain algebraic varieties, turn out to be the fundamental ingredients to prove the genericity of steepness.","The second part of this work is entirely new and is devoted to the formulation of explicit algebraic criteria to check steepness of any given sufficiently regular function, which constitutes a very important result for applications, as the original definition of steepness is not constructive.","These criteria involve both the derivatives of the studied function up to any given order and external real parameters that, generically, belong to compact sets."],"url":"http://arxiv.org/abs/2402.10707v1","category":"math.AG"}
{"created":"2024-02-16 14:05:25","title":"Elementary considerations on gravitational waves from hyperbolic encounters","abstract":"We examine the main properties of gravitational waves (GWs) emitted by transient hyperbolic encounters of black holes. We begin by building the set of basic variables most relevant to setting our problem. After exposing the ranges of masses and eccentricities accessible at a given GW frequency, we analyze the dependence of the gravitational strain on those parameters and determine the trajectories resulting in the most sizeable strains. Some non-trivial behaviors are unveiled, showing that highly eccentric events can be more easily detectable than parabolic ones. In particular, we underline the correct way to extend formulas from hyperbolic to parabolic orbits. Our reasonings are as general as possible, and we make a point of explaining our considerations pedagogically.","sentences":["We examine the main properties of gravitational waves (GWs) emitted by transient hyperbolic encounters of black holes.","We begin by building the set of basic variables most relevant to setting our problem.","After exposing the ranges of masses and eccentricities accessible at a given GW frequency, we analyze the dependence of the gravitational strain on those parameters and determine the trajectories resulting in the most sizeable strains.","Some non-trivial behaviors are unveiled, showing that highly eccentric events can be more easily detectable than parabolic ones.","In particular, we underline the correct way to extend formulas from hyperbolic to parabolic orbits.","Our reasonings are as general as possible, and we make a point of explaining our considerations pedagogically."],"url":"http://arxiv.org/abs/2402.10706v1","category":"gr-qc"}
{"created":"2024-02-16 14:04:56","title":"AutoSAT: Automatically Optimize SAT Solvers via Large Language Models","abstract":"Heuristics are crucial in SAT solvers, while no heuristic rules are suitable for all problem instances. Therefore, it typically requires to refine specific solvers for specific problem instances. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on Large Large Models (LLMs) which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive preliminary setup and model training, and fosters a Chain of Thought collaborative process with fault-tolerance, ensuring robust heuristic optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL) solver demonstrates the overall superior performance of AutoSAT, especially in solving some specific SAT problem instances.","sentences":["Heuristics are crucial in SAT solvers, while no heuristic rules are suitable for all problem instances.","Therefore, it typically requires to refine specific solvers for specific problem instances.","In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers.","AutoSAT is based on Large Large Models (LLMs) which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities.","AutoSAT operates on a plug-and-play basis, eliminating the need for extensive preliminary setup and model training, and fosters a Chain of Thought collaborative process with fault-tolerance, ensuring robust heuristic optimization.","Extensive experiments on a Conflict-Driven Clause Learning (CDCL) solver demonstrates the overall superior performance of AutoSAT, especially in solving some specific SAT problem instances."],"url":"http://arxiv.org/abs/2402.10705v1","category":"cs.AI"}
{"created":"2024-02-16 14:03:07","title":"A theorem of Strichartz for multipliers on homogeneous trees","abstract":"A theorem of Strichartz states that if a uniformly bounded bi-infinite sequence of functions on the Euclidean spaces, satisfies the condition that the Laplacian acting on a function in this sequence yields the next one, then each function in this sequence is an eigenfunction of the Laplacian. We consider a generalization of this result for homogeneous trees, where we replace bounded functions by tempered distributions and the Laplacian by multiplier operators acting on the tempered distributions. After establishing the result in this general context, we narrow our focus to specific cases, which includes important examples of multiplier operators such as heat and Schr\\\"odinger operator, ball and sphere averages of function.","sentences":["A theorem of Strichartz states that if a uniformly bounded bi-infinite sequence of functions on the Euclidean spaces, satisfies the condition that the Laplacian acting on a function in this sequence yields the next one, then each function in this sequence is an eigenfunction of the Laplacian.","We consider a generalization of this result for homogeneous trees, where we replace bounded functions by tempered distributions and the Laplacian by multiplier operators acting on the tempered distributions.","After establishing the result in this general context, we narrow our focus to specific cases, which includes important examples of multiplier operators such as heat and Schr\\\"odinger operator, ball and sphere averages of function."],"url":"http://arxiv.org/abs/2402.10703v1","category":"math.CA"}
{"created":"2024-02-16 14:02:28","title":"Does Twinning Vehicular Networks Enhance Their Performance in Dense Areas?","abstract":"This paper investigates the potential of Digital Twins (DTs) to enhance network performance in densely populated urban areas, specifically focusing on vehicular networks. The study comprises two phases. In Phase I, we utilize traffic data and AI clustering to identify critical locations, particularly in crowded urban areas with high accident rates. In Phase II, we evaluate the advantages of twinning vehicular networks through three deployment scenarios: edge-based twin, cloud-based twin, and hybrid-based twin. Our analysis demonstrates that twinning significantly reduces network delays, with virtual twins outperforming physical networks. Virtual twins maintain low delays even with increased vehicle density, such as 15.05 seconds for 300 vehicles. Moreover, they exhibit faster computational speeds, with cloud-based twins being 1.7 times faster than edge twins in certain scenarios. These findings provide insights for efficient vehicular communication and underscore the potential of virtual twins in enhancing vehicular networks in crowded areas while emphasizing the importance of considering real-world factors when making deployment decisions.","sentences":["This paper investigates the potential of Digital Twins (DTs) to enhance network performance in densely populated urban areas, specifically focusing on vehicular networks.","The study comprises two phases.","In Phase I, we utilize traffic data and AI clustering to identify critical locations, particularly in crowded urban areas with high accident rates.","In Phase II, we evaluate the advantages of twinning vehicular networks through three deployment scenarios: edge-based twin, cloud-based twin, and hybrid-based twin.","Our analysis demonstrates that twinning significantly reduces network delays, with virtual twins outperforming physical networks.","Virtual twins maintain low delays even with increased vehicle density, such as 15.05 seconds for 300 vehicles.","Moreover, they exhibit faster computational speeds, with cloud-based twins being 1.7 times faster than edge twins in certain scenarios.","These findings provide insights for efficient vehicular communication and underscore the potential of virtual twins in enhancing vehicular networks in crowded areas while emphasizing the importance of considering real-world factors when making deployment decisions."],"url":"http://arxiv.org/abs/2402.10701v1","category":"cs.NI"}
{"created":"2024-02-16 14:01:52","title":"Resonance of Gravitational Axions-like Particles","abstract":"The motion of gravitational axion-like particles (ALP) around a Kerr black hole is analyzed, paying attention to resonance and distribution of spectral radiation. We first discuss the computation of $\\sqrt{g}{\\tilde R}_{\\mu \\nu \\rho \\rho \\sigma}R^{\\mu \\nu \\rho \\sigma}$ and its implications with Pontryagin's theorem and a detailed analysis of Teukolsky's master equation is done. After carefully analyzing the Teukolsky master equation, we show that this system exhibits resonance when $\\omega \\gtrsim \\mu$ where $\\mu$ is the mass of the ALP. A skew-normal distribution can approximate the energy distribution, and we can calculate the mean lifetime of the resonance for black holes with masses between 100 to 1000 $M_{\\odot}$. This range corresponds to a duration between $10^{-1}$s and $10^{41}$s, the observation range used in LIGO data.","sentences":["The motion of gravitational axion-like particles (ALP) around a Kerr black hole is analyzed, paying attention to resonance and distribution of spectral radiation.","We first discuss the computation of $\\sqrt{g}{\\tilde R}_{\\mu \\nu \\rho \\rho \\sigma}R^{\\mu","\\nu \\rho \\sigma}$ and its implications with Pontryagin's theorem and a detailed analysis of Teukolsky's master equation is done.","After carefully analyzing the Teukolsky master equation, we show that this system exhibits resonance when $\\omega \\gtrsim \\mu$ where $\\mu$ is the mass of the ALP.","A skew-normal distribution can approximate the energy distribution, and we can calculate the mean lifetime of the resonance for black holes with masses between 100 to 1000 $M_{\\odot}$. This range corresponds to a duration between $10^{-1}$s and $10^{41}$s, the observation range used in LIGO data."],"url":"http://arxiv.org/abs/2402.10700v1","category":"hep-th"}
{"created":"2024-02-16 13:58:23","title":"Unlink to Unlearn: Simplifying Edge Unlearning in GNNs","abstract":"As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. Furthermore, our analysis also suggests that loss functions may not be essential for effective edge unlearning. Building on these insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from graph structure. Our extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks. Specifically, UtU upholds over 97.3% of the retrained model's privacy protection capabilities and 99.8% of its link prediction accuracy. Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution.","sentences":["As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia.","This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request.","Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability.","Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting.","It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges.","To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon.","Furthermore, our analysis also suggests that loss functions may not be essential for effective edge unlearning.","Building on these insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a novel method that facilitates unlearning exclusively through unlinking the forget edges from graph structure.","Our extensive experiments demonstrate that UtU delivers privacy protection on par with that of a retrained model while preserving high accuracy in downstream tasks.","Specifically, UtU upholds over 97.3% of the retrained model's privacy protection capabilities and 99.8% of its link prediction accuracy.","Meanwhile, UtU requires only constant computational demands, underscoring its advantage as a highly lightweight and practical edge unlearning solution."],"url":"http://arxiv.org/abs/2402.10695v1","category":"cs.LG"}
{"created":"2024-02-16 13:57:24","title":"Exact dg categories I : Foundations","abstract":"We introduce the notion of exact dg category, which provides a differential graded enhancement of Nakaoka--Palu's notion of extriangulated category. We give a definition in complete analogy with Quillen's but where the category of kernel-cokernel pairs is replaced with a more sophisticated homotopy category. We introduce the notion of stable dg category, and prove that the $H^0$-category of an exact dg category $\\mathcal{A}$ is triangulated if and only if $\\mathcal{A}$ is stable. We illustrate our theory with several examples including the homotopy category of two-term complexes and Amiot's fundamental domain for generalized cluster categories.","sentences":["We introduce the notion of exact dg category, which provides a differential graded enhancement of Nakaoka--Palu's notion of extriangulated category.","We give a definition in complete analogy with Quillen's but where the category of kernel-cokernel pairs is replaced with a more sophisticated homotopy category.","We introduce the notion of stable dg category, and prove that the $H^0$-category of an exact dg category $\\mathcal{A}$ is triangulated if and only if $\\mathcal{A}$ is stable.","We illustrate our theory with several examples including the homotopy category of two-term complexes and Amiot's fundamental domain for generalized cluster categories."],"url":"http://arxiv.org/abs/2402.10694v1","category":"math.RT"}
{"created":"2024-02-16 13:53:26","title":"Exploring Precision and Recall to assess the quality and diversity of LLMs","abstract":"This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.","sentences":["This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation.","This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora.","By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks.","The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback.","This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text."],"url":"http://arxiv.org/abs/2402.10693v1","category":"cs.CL"}
{"created":"2024-02-16 13:46:38","title":"Multi-Cultural Commonsense Knowledge Distillation","abstract":"Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin. For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions. We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.","sentences":["Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions.","This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge.","We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures.","Outputs are consolidated via clustering and generative summarization.","Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin.","For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions.","We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators.","Data and code are available for download."],"url":"http://arxiv.org/abs/2402.10689v1","category":"cs.CL"}
{"created":"2024-02-16 13:41:28","title":"Beamforming Optimization for Active RIS-Aided Multiuser Communications With Hardware Impairments","abstract":"In this paper, we consider an active reconfigurable intelligent surface (RIS) to assist the multiuser downlink transmission in the presence of practical hardware impairments (HWIs), including the HWIs at the transceivers and the phase noise at the active RIS. The active RIS is deployed to amplify the incident signals to alleviate the multiplicative fading effect, which is a limitation in the conventional passive RIS-aided wireless systems. We aim to maximize the sum rate through jointly designing the transmit beamforming at the base station (BS), the amplification factors and the phase shifts at the active RIS. To tackle this challenging optimization problem effectively, we decouple it into two tractable subproblems. Subsequently, each subproblem is transformed into a second order cone programming problem. The block coordinate descent framework is applied to tackle them, where the transmit beamforming and the reflection coefficients are alternately designed. In addition, another efficient algorithm is presented to reduce the computational complexity. Specifically, by exploiting the majorization-minimization approach, each subproblem is reformulated into a tractable surrogate problem, whose closed-form solutions are obtained by Lagrange dual decomposition approach and element-wise alternating sequential optimization method. Simulation results validate the effectiveness of our developed algorithms, and reveal that the HWIs significantly limit the system performance of active RIS-empowered wireless communications. Furthermore, the active RIS noticeably boosts the sum rate under the same total power budget, compared with the passive RIS.","sentences":["In this paper, we consider an active reconfigurable intelligent surface (RIS) to assist the multiuser downlink transmission in the presence of practical hardware impairments (HWIs), including the HWIs at the transceivers and the phase noise at the active RIS.","The active RIS is deployed to amplify the incident signals to alleviate the multiplicative fading effect, which is a limitation in the conventional passive RIS-aided wireless systems.","We aim to maximize the sum rate through jointly designing the transmit beamforming at the base station (BS), the amplification factors and the phase shifts at the active RIS.","To tackle this challenging optimization problem effectively, we decouple it into two tractable subproblems.","Subsequently, each subproblem is transformed into a second order cone programming problem.","The block coordinate descent framework is applied to tackle them, where the transmit beamforming and the reflection coefficients are alternately designed.","In addition, another efficient algorithm is presented to reduce the computational complexity.","Specifically, by exploiting the majorization-minimization approach, each subproblem is reformulated into a tractable surrogate problem, whose closed-form solutions are obtained by Lagrange dual decomposition approach and element-wise alternating sequential optimization method.","Simulation results validate the effectiveness of our developed algorithms, and reveal that the HWIs significantly limit the system performance of active RIS-empowered wireless communications.","Furthermore, the active RIS noticeably boosts the sum rate under the same total power budget, compared with the passive RIS."],"url":"http://arxiv.org/abs/2402.10687v1","category":"eess.SP"}
{"created":"2024-02-16 13:41:18","title":"Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective","abstract":"In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in which an adaptive prediction set is produced as in conformal prediction. We derive bounds on the advantage of an MIA adversary with the aim of offering insights into the impact of uncertainty and calibration on the effectiveness of MIAs. Simulation results demonstrate that the derived analytical bounds predict well the effectiveness of MIAs.","sentences":["In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model.","In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model.","We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in which an adaptive prediction set is produced as in conformal prediction.","We derive bounds on the advantage of an MIA adversary with the aim of offering insights into the impact of uncertainty and calibration on the effectiveness of MIAs.","Simulation results demonstrate that the derived analytical bounds predict well the effectiveness of MIAs."],"url":"http://arxiv.org/abs/2402.10686v1","category":"cs.IT"}
{"created":"2024-02-16 13:39:34","title":"LongHeads: Multi-Head Attention is Secretly a Long Context Processor","abstract":"Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads. In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts. LongHeads works efficiently in linear time, fits seamlessly with many LLMs that use relative positional encoding. Our extensive empirical analyses verify LongHeads's efficacy in extending the usable context window for existing models, showcasing its promise for enhancing long text understanding.","sentences":["Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands.","Many sought to mitigate this by restricting the attention window within the pre-trained length.","However, these methods introduce new issues such as ignoring the middle context and requiring additional training.","To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential.","Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks.","To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads.","In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts.","LongHeads works efficiently in linear time, fits seamlessly with many LLMs that use relative positional encoding.","Our extensive empirical analyses verify LongHeads's efficacy in extending the usable context window for existing models, showcasing its promise for enhancing long text understanding."],"url":"http://arxiv.org/abs/2402.10685v1","category":"cs.CL"}
{"created":"2024-02-16 13:37:57","title":"Language-Driven Engineering An Interdisciplinary Software Development Paradigm","abstract":"We illustrate how purpose-specific, graphical modeling enables application experts with different levels of expertise to collaboratively design and then produce complex applications using their individual, purpose-specific modeling language. Our illustration includes seven graphical Integrated Modeling Environments (IMEs) that support full code generation, as well as four browser-based applications that were modeled and then fully automatically generated and produced using DIME, our most complex graphical IME. While the seven IMEs were chosen to illustrate the types of languages we support with our Language-Driven Engineering (LDE) approach, the four DIME products were chosen to give an impression of the power of our LDE-generated IMEs. In fact, Equinocs, Springer Nature's future editorial system for proceedings, is also being fully automatically generated and then deployed at their Dordrecht site using a deployment pipeline generated with Rig, one of the IMEs presented. Our technology is open source and the products presented are currently in use.","sentences":["We illustrate how purpose-specific, graphical modeling enables application experts with different levels of expertise to collaboratively design and then produce complex applications using their individual, purpose-specific modeling language.","Our illustration includes seven graphical Integrated Modeling Environments (IMEs) that support full code generation, as well as four browser-based applications that were modeled and then fully automatically generated and produced using DIME, our most complex graphical IME.","While the seven IMEs were chosen to illustrate the types of languages we support with our Language-Driven Engineering (LDE) approach, the four DIME products were chosen to give an impression of the power of our LDE-generated IMEs.","In fact, Equinocs, Springer Nature's future editorial system for proceedings, is also being fully automatically generated and then deployed at their Dordrecht site using a deployment pipeline generated with Rig, one of the IMEs presented.","Our technology is open source and the products presented are currently in use."],"url":"http://arxiv.org/abs/2402.10684v1","category":"cs.SE"}
{"created":"2024-02-16 13:37:28","title":"Energy-saving fast-forward scaling","abstract":"We propose energy-saving fast-forward scaling. Fast-forward scaling is a method which enables us to speedup (or slowdown) given dynamics in a certain measurement basis. We introduce energy costs of fast-forward scaling, and find possibility of energy-saving speedup for time-independent measurement bases. As concrete examples, we show such energy-saving fast-forward scaling in a two-level system and quantum annealing of a general Ising spin glass. We also discuss influence of a time-dependent measurement basis, and give a remedy for unwanted energy costs. The present results pave the way for realization of energy-efficient quantum technologies.","sentences":["We propose energy-saving fast-forward scaling.","Fast-forward scaling is a method which enables us to speedup (or slowdown) given dynamics in a certain measurement basis.","We introduce energy costs of fast-forward scaling, and find possibility of energy-saving speedup for time-independent measurement bases.","As concrete examples, we show such energy-saving fast-forward scaling in a two-level system and quantum annealing of a general Ising spin glass.","We also discuss influence of a time-dependent measurement basis, and give a remedy for unwanted energy costs.","The present results pave the way for realization of energy-efficient quantum technologies."],"url":"http://arxiv.org/abs/2402.10683v1","category":"quant-ph"}
{"created":"2024-02-16 13:36:53","title":"Considerations on the relaxation time in shear-driven jamming","abstract":"We study the jamming transition in a model of elastic particles under shear at zero temperature, with a focus on the relaxation time $\\tau_1$. This relaxation time is from two-step simulations where the first step is the ordinary shearing simulation and the second step is the relaxation of the energy after stopping the shearing. $\\tau_1$ is determined from the final exponential decay of the energy. Such relaxations are done with many different starting configuration generated by a long shearing simulation in which the shear varible $\\gamma$ slowly increases. We study the correlations of both $\\tau_1$, determined from the decay, and the pressure, $p_1$, from the starting configurations as a function of the difference in $\\gamma$. We find that the correlations of $p_1$ are more long lived than the ones of $\\tau_1$ and find that the reason for this is that the individual $\\tau_1$ is controlled both by $p_1$ of the starting configuration and a random contribution which depends on the relaxation path length -- the average distance moved by the particles during the relaxation. We further conclude that it is $\\gammatau$, determined from the correlations of $\\tau_1$, which is the relevant one when the aim is to generate data that may be used for determining the critical exponent that characterizes the jamming transition.","sentences":["We study the jamming transition in a model of elastic particles under shear at zero temperature, with a focus on the relaxation time $\\tau_1$. This relaxation time is from two-step simulations where the first step is the ordinary shearing simulation and the second step is the relaxation of the energy after stopping the shearing.","$\\tau_1$ is determined from the final exponential decay of the energy.","Such relaxations are done with many different starting configuration generated by a long shearing simulation in which the shear varible $\\gamma$ slowly increases.","We study the correlations of both $\\tau_1$, determined from the decay, and the pressure, $p_1$, from the starting configurations as a function of the difference in $\\gamma$. We find that the correlations of $p_1$ are more long lived than the ones of $\\tau_1$ and find that the reason for this is that the individual $\\tau_1$ is controlled both by $p_1$ of the starting configuration and a random contribution which depends on the relaxation path length -- the average distance moved by the particles during the relaxation.","We further conclude that it is $\\gammatau$, determined from the correlations of $\\tau_1$, which is the relevant one when the aim is to generate data that may be used for determining the critical exponent that characterizes the jamming transition."],"url":"http://arxiv.org/abs/2402.10682v1","category":"cond-mat.soft"}
{"created":"2024-02-16 13:34:51","title":"Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes","abstract":"Engineering components must meet increasing technological demands in ever shorter development cycles. To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process. Current approaches employ numerical simulations, which however quickly becomes computation-intensive, especially for iterative optimization. Data-driven machine learning methods can be used to replace time- and resource-intensive numerical simulations. In particular, MeshGraphNets (MGNs) have shown promising results. They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization. However, these models rely on large amounts of expensive training data, such as numerical simulations. Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled data, but have not been extended yet to handle time-dependent simulations of arbitrary meshes. This work introduces PI-MGNs, a hybrid approach that combines PINNs and MGNs to quickly and accurately solve non-stationary and nonlinear partial differential equations (PDEs) on arbitrary meshes. The method is exemplified for thermal process simulations of unseen parts with inhomogeneous material distribution. Further results show that the model scales well to large and complex meshes, although it is trained on small generic meshes only.","sentences":["Engineering components must meet increasing technological demands in ever shorter development cycles.","To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process.","Current approaches employ numerical simulations, which however quickly becomes computation-intensive, especially for iterative optimization.","Data-driven machine learning methods can be used to replace time- and resource-intensive numerical simulations.","In particular, MeshGraphNets (MGNs) have shown promising results.","They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization.","However, these models rely on large amounts of expensive training data, such as numerical simulations.","Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled data, but have not been extended yet to handle time-dependent simulations of arbitrary meshes.","This work introduces PI-MGNs, a hybrid approach that combines PINNs and MGNs to quickly and accurately solve non-stationary and nonlinear partial differential equations (PDEs) on arbitrary meshes.","The method is exemplified for thermal process simulations of unseen parts with inhomogeneous material distribution.","Further results show that the model scales well to large and complex meshes, although it is trained on small generic meshes only."],"url":"http://arxiv.org/abs/2402.10681v1","category":"cs.LG"}
{"created":"2024-02-16 13:34:14","title":"Discovery of the first olivine-dominated A-type asteroid family","abstract":"The classical theory of differentiation states that due to the heat generated by the decay of radioactive elements, some asteroids form an iron core, an olivine-rich mantle, and a crust. The collisional breakup of these differentiated bodies is expected to lead to exposed mantle fragments, creating families of newly-formed asteroids. Among these new objects, some are expected to show an olivine-rich composition in spectroscopic observations. However, several years of spectrophotometric surveys have led to the conclusion that olivine-rich asteroids are rare in the asteroid main belt, and no significant concentration of olivine-rich bodies in any asteroid family has been detected to date. Using ESA's Gaia DR3 reflectance spectra, we show that the family (36256) 1999 XT17 presents a prominence of objects that are likely to present an olivine-rich composition (A-type spectroscopic class). If S-complex asteroids as the second most prominent spectroscopic class in the family are real family members, then arguably the 1999 XT17 family has originated from the break-up of a partially differentiated parent body. Alternatively, if the S-complex asteroids are interlopers, then the 1999 XT17 family could have originated from the breakup of an olivine-rich body. This body could have been part of the mantle of a differentiated planetesimal, which may have broken up in a different region of the Solar System, and one of its fragments (i.e. the parent body of the 1999 XT17 family) could have been dynamically implanted in the main belt.","sentences":["The classical theory of differentiation states that due to the heat generated by the decay of radioactive elements, some asteroids form an iron core, an olivine-rich mantle, and a crust.","The collisional breakup of these differentiated bodies is expected to lead to exposed mantle fragments, creating families of newly-formed asteroids.","Among these new objects, some are expected to show an olivine-rich composition in spectroscopic observations.","However, several years of spectrophotometric surveys have led to the conclusion that olivine-rich asteroids are rare in the asteroid main belt, and no significant concentration of olivine-rich bodies in any asteroid family has been detected to date.","Using ESA's Gaia DR3 reflectance spectra, we show that the family (36256) 1999","XT17 presents a prominence of objects that are likely to present an olivine-rich composition (A-type spectroscopic class).","If S-complex asteroids as the second most prominent spectroscopic class in the family are real family members, then arguably the 1999 XT17 family has originated from the break-up of a partially differentiated parent body.","Alternatively, if the S-complex asteroids are interlopers, then the 1999 XT17 family could have originated from the breakup of an olivine-rich body.","This body could have been part of the mantle of a differentiated planetesimal, which may have broken up in a different region of the Solar System, and one of its fragments (i.e. the parent body of the 1999 XT17 family) could have been dynamically implanted in the main belt."],"url":"http://arxiv.org/abs/2402.10679v1","category":"astro-ph.EP"}
{"created":"2024-02-16 13:28:49","title":"Generating Plane Quadrangulations and Symmetry-preserving Operations on Maps","abstract":"Lopsp-operations are operations on maps that are applied locally and are guaranteed to preserve all the orientation-preserving symmetries of maps. Well-known examples of such operations are dual, ambo, truncation, and leapfrog. They are described by plane 3-coloured triangulations with specific properties. We developed and implemented a program that can generate all lopsp-operations of a given size by reducing the problem of generating lopsp-operations to generating all plane quadrangulations that are not necessarily simple. We extended the program plantri to generate these quadrangulations.","sentences":["Lopsp-operations are operations on maps that are applied locally and are guaranteed to preserve all the orientation-preserving symmetries of maps.","Well-known examples of such operations are dual, ambo, truncation, and leapfrog.","They are described by plane 3-coloured triangulations with specific properties.","We developed and implemented a program that can generate all lopsp-operations of a given size by reducing the problem of generating lopsp-operations to generating all plane quadrangulations that are not necessarily simple.","We extended the program plantri to generate these quadrangulations."],"url":"http://arxiv.org/abs/2402.10676v1","category":"math.CO"}
{"created":"2024-02-16 13:28:44","title":"German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data","abstract":"This study pioneers the use of synthetically generated data for training generative models in document-level text simplification of German texts. We demonstrate the effectiveness of our approach with real-world online texts. Addressing the challenge of data scarcity in language simplification, we crawled professionally simplified German texts and synthesized a corpus using GPT-4. We finetune Large Language Models with up to 13 billion parameters on this data and evaluate their performance. This paper employs various methodologies for evaluation and demonstrates the limitations of currently used rule-based metrics. Both automatic and manual evaluations reveal that our models can significantly simplify real-world online texts, indicating the potential of synthetic data in improving text simplification.","sentences":["This study pioneers the use of synthetically generated data for training generative models in document-level text simplification of German texts.","We demonstrate the effectiveness of our approach with real-world online texts.","Addressing the challenge of data scarcity in language simplification, we crawled professionally simplified German texts and synthesized a corpus using GPT-4.","We finetune Large Language Models with up to 13 billion parameters on this data and evaluate their performance.","This paper employs various methodologies for evaluation and demonstrates the limitations of currently used rule-based metrics.","Both automatic and manual evaluations reveal that our models can significantly simplify real-world online texts, indicating the potential of synthetic data in improving text simplification."],"url":"http://arxiv.org/abs/2402.10675v1","category":"cs.CL"}
{"created":"2024-02-16 13:25:49","title":"Border subrank via a generalised Hilbert-Mumford criterion","abstract":"We show that the border subrank of a sufficiently general tensor in $(\\mathbb{C}^n)^{\\otimes d}$ is $\\mathcal{O}(n^{1/(d-1)})$ for $n \\to \\infty$. Since this matches the growth rate $\\Theta(n^{1/(d-1)})$ for the generic (non-border) subrank recently established by Derksen-Makam-Zuiddam, we find that the generic border subrank has the same growth rate. In our proof, we use a generalisation of the Hilbert-Mumford criterion that we believe will be of independent interest.","sentences":["We show that the border subrank of a sufficiently general tensor in $(\\mathbb{C}^n)^{\\otimes d}$ is $\\mathcal{O}(n^{1/(d-1)})$ for $n \\to \\infty$. Since this matches the growth rate $\\Theta(n^{1/(d-1)})$ for the generic (non-border) subrank recently established by Derksen-Makam-Zuiddam, we find that the generic border subrank has the same growth rate.","In our proof, we use a generalisation of the Hilbert-Mumford criterion that we believe will be of independent interest."],"url":"http://arxiv.org/abs/2402.10674v1","category":"math.AG"}
{"created":"2024-02-16 13:21:33","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models","abstract":"Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.","sentences":["Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects.","Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects.","However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner.","Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation.","We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand.","We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM).","Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal.","By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments.","Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness.","Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments."],"url":"http://arxiv.org/abs/2402.10670v1","category":"cs.CL"}
{"created":"2024-02-16 13:15:32","title":"Binary linear codes with a fixed point free permutation automorphism of order three","abstract":"We investigate the structural properties of binary linear codes whose permutation automorphism group has a fixed point free automorphism of order $3$. We prove that up to dimension or codimension $4$, there is no binary linear code whose permutation automorphism group is generated by a fixed point free permutation of order $3$. We also prove that there is no binary $5$-dimensional linear code whose length is at least $30$ and whose permutation automorphism group is generated by a fixed point free permutation of order $3$.","sentences":["We investigate the structural properties of binary linear codes whose permutation automorphism group has a fixed point free automorphism of order $3$. We prove that up to dimension or codimension $4$, there is no binary linear code whose permutation automorphism group is generated by a fixed point free permutation of order $3$. We also prove that there is no binary $5$-dimensional linear code whose length is at least $30$ and whose permutation automorphism group is generated by a fixed point free permutation of order $3$."],"url":"http://arxiv.org/abs/2402.10667v1","category":"cs.IT"}
{"created":"2024-02-16 13:14:35","title":"Multi-Hop Table Retrieval for Open-Domain Text-to-SQL","abstract":"Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with an average improvement of 6.38%.","sentences":["Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL.","However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity.","Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre).","To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search.","To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables.","We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with an average improvement of 6.38%."],"url":"http://arxiv.org/abs/2402.10666v1","category":"cs.CL"}
{"created":"2024-02-16 13:13:57","title":"Generative AI and Attentive User Interfaces: Five Strategies to Enhance Take-Over Quality in Automated Driving","abstract":"As the automotive world moves toward higher levels of driving automation, Level 3 automated driving represents a critical juncture. In Level 3 driving, vehicles can drive alone under limited conditions, but drivers are expected to be ready to take over when the system requests. Assisting the driver to maintain an appropriate level of Situation Awareness (SA) in such contexts becomes a critical task. This position paper explores the potential of Attentive User Interfaces (AUIs) powered by generative Artificial Intelligence (AI) to address this need. Rather than relying on overt notifications, we argue that AUIs based on novel AI technologies such as large language models or diffusion models can be used to improve SA in an unconscious and subtle way without negative effects on drivers overall workload. Accordingly, we propose 5 strategies how generative AI s can be used to improve the quality of takeovers and, ultimately, road safety.","sentences":["As the automotive world moves toward higher levels of driving automation, Level 3 automated driving represents a critical juncture.","In Level 3 driving, vehicles can drive alone under limited conditions, but drivers are expected to be ready to take over when the system requests.","Assisting the driver to maintain an appropriate level of Situation Awareness (SA) in such contexts becomes a critical task.","This position paper explores the potential of Attentive User Interfaces (AUIs) powered by generative Artificial Intelligence (AI) to address this need.","Rather than relying on overt notifications, we argue that AUIs based on novel AI technologies such as large language models or diffusion models can be used to improve SA in an unconscious and subtle way without negative effects on drivers overall workload.","Accordingly, we propose 5 strategies how generative AI s can be used to improve the quality of takeovers and, ultimately, road safety."],"url":"http://arxiv.org/abs/2402.10664v1","category":"cs.HC"}
{"created":"2024-02-16 13:11:13","title":"Fine Tuning Named Entity Extraction Models for the Fantasy Domain","abstract":"Named Entity Recognition (NER) is a sequence classification Natural Language Processing task where entities are identified in the text and classified into predefined categories. It acts as a foundation for most information extraction systems. Dungeons and Dragons (D&D) is an open-ended tabletop fantasy game with its own diverse lore. DnD entities are domain-specific and are thus unrecognizable by even the state-of-the-art off-the-shelf NER systems as the NER systems are trained on general data for pre-defined categories such as: person (PERS), location (LOC), organization (ORG), and miscellaneous (MISC). For meaningful extraction of information from fantasy text, the entities need to be classified into domain-specific entity categories as well as the models be fine-tuned on a domain-relevant corpus. This work uses available lore of monsters in the D&D domain to fine-tune Trankit, which is a prolific NER framework that uses a pre-trained model for NER. Upon this training, the system acquires the ability to extract monster names from relevant domain documents under a novel NER tag. This work compares the accuracy of the monster name identification against; the zero-shot Trankit model and two FLAIR models. The fine-tuned Trankit model achieves an 87.86% F1 score surpassing all the other considered models.","sentences":["Named Entity Recognition (NER) is a sequence classification Natural Language Processing task where entities are identified in the text and classified into predefined categories.","It acts as a foundation for most information extraction systems.","Dungeons and Dragons (D&D) is an open-ended tabletop fantasy game with its own diverse lore.","DnD entities are domain-specific and are thus unrecognizable by even the state-of-the-art off-the-shelf NER systems as the NER systems are trained on general data for pre-defined categories such as: person (PERS), location (LOC), organization (ORG), and miscellaneous (MISC).","For meaningful extraction of information from fantasy text, the entities need to be classified into domain-specific entity categories as well as the models be fine-tuned on a domain-relevant corpus.","This work uses available lore of monsters in the D&D domain to fine-tune Trankit, which is a prolific NER framework that uses a pre-trained model for NER.","Upon this training, the system acquires the ability to extract monster names from relevant domain documents under a novel NER tag.","This work compares the accuracy of the monster name identification against; the zero-shot Trankit model and two FLAIR models.","The fine-tuned Trankit model achieves an 87.86% F1 score surpassing all the other considered models."],"url":"http://arxiv.org/abs/2402.10662v1","category":"cs.CL"}
{"created":"2024-02-16 13:10:17","title":"Power Allocation Scheme for Device-Free Localization in 6G ISAC Networks","abstract":"Integrated Sensing and Communication (ISAC) is considered one of the crucial technologies in the upcoming sixth-generation (6G) mobile communication systems that could facilitate ultra-precise positioning of passive and active targets and extremely high data rates through spectrum coexistence and hardware sharing. Such an ISAC network offers a lot of benefits, but comes with the challenge of managing the mutual interference between the sensing and communication services. In this paper, we investigate the problem of localization accuracy in a monostatic ISAC network under consideration of inter-BS interference due to communication signal and sensing echoes, and self-interference at the respective BS. We propose a power allocation algorithm that minimizes BS's maximum range estimate error while considering minimum communication signal-to-interference-plus-noise ratio (SINR) and total power constraint. Our numerical results demonstrate the effectiveness of the proposed algorithm and indicate that it can enhance the sensing performance when the self-interference is effectively suppressed.","sentences":["Integrated Sensing and Communication (ISAC) is considered one of the crucial technologies in the upcoming sixth-generation (6G) mobile communication systems that could facilitate ultra-precise positioning of passive and active targets and extremely high data rates through spectrum coexistence and hardware sharing.","Such an ISAC network offers a lot of benefits, but comes with the challenge of managing the mutual interference between the sensing and communication services.","In this paper, we investigate the problem of localization accuracy in a monostatic ISAC network under consideration of inter-BS interference due to communication signal and sensing echoes, and self-interference at the respective BS.","We propose a power allocation algorithm that minimizes BS's maximum range estimate error while considering minimum communication signal-to-interference-plus-noise ratio (SINR) and total power constraint.","Our numerical results demonstrate the effectiveness of the proposed algorithm and indicate that it can enhance the sensing performance when the self-interference is effectively suppressed."],"url":"http://arxiv.org/abs/2402.10660v1","category":"eess.SP"}
{"created":"2024-02-16 13:10:14","title":"Network Formation and Dynamics Among Multi-LLMs","abstract":"Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furthermore, we investigate LLMs' decision-making based on real-world networks to compare the strengths of these principles. Our results reveal that triadic closure and homophily have a stronger influence than preferential attachment and that LLMs substantially exceed random guessing in the task of network formation predictions. Overall, our study contributes to the development of socially aware LLMs by shedding light on LLMs' network formation behaviors and exploring their impacts on social dynamics and norms.","sentences":["Social networks influence behaviors, preferences, and relationships","and","play a crucial role in the dissemination of information and norms within human societies.","As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential.","Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics.","We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon.","Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation.","Furthermore, we investigate LLMs' decision-making based on real-world networks to compare the strengths of these principles.","Our results reveal that triadic closure and homophily have a stronger influence than preferential attachment and that LLMs substantially exceed random guessing in the task of network formation predictions.","Overall, our study contributes to the development of socially aware LLMs by shedding light on LLMs' network formation behaviors and exploring their impacts on social dynamics and norms."],"url":"http://arxiv.org/abs/2402.10659v1","category":"cs.SI"}
{"created":"2024-02-16 13:07:50","title":"Existence of a minimizer to the particle number-Casimir functional for the Einstein-Vlasov system","abstract":"In 2001 Wolansky \\cite{Wol} introduced a particle number-Casimir functional for the Einstein-Vlasov system. Two open questions are associated with this functional. First, a meaningful variational problem should be formulated and the existence of a minimizer to this problem should be established. The second issue is to show that a minimizer, for some choice of the parameters, is a static solution of the Einstein-Vlasov system. In the present work we solve the first problem by proving the existence of a minimizer to the particle number-Casimir functional. On the technical side, it is a main achievement that we are able to bypass the non-compactness of minimizing sequences by new arguments in both $v$-space and $x$-space, which might have several further applications. We note that such compactness results for the Einstein-Vlasov system have been absent in the literature, whereas similar results have been known in the Newtonian case. We also provide arguments which give strong support that minimizers corresponding to small masses are static solutions of the Einstein-Vlasov system. Furthermore, our analysis leads us to propose a new stability criterion for static solutions: We conjecture that a static solution for which the Casimir-binding energy is positive is stable for mass-preserving perturbations.","sentences":["In 2001 Wolansky \\cite{Wol} introduced a particle number-Casimir functional for the Einstein-Vlasov system.","Two open questions are associated with this functional.","First, a meaningful variational problem should be formulated and the existence of a minimizer to this problem should be established.","The second issue is to show that a minimizer, for some choice of the parameters, is a static solution of the Einstein-Vlasov system.","In the present work we solve the first problem by proving the existence of a minimizer to the particle number-Casimir functional.","On the technical side, it is a main achievement that we are able to bypass the non-compactness of minimizing sequences by new arguments in both $v$-space and $x$-space, which might have several further applications.","We note that such compactness results for the Einstein-Vlasov system have been absent in the literature, whereas similar results have been known in the Newtonian case.","We also provide arguments which give strong support that minimizers corresponding to small masses are static solutions of the Einstein-Vlasov system.","Furthermore, our analysis leads us to propose a new stability criterion for static solutions: We conjecture that a static solution for which the Casimir-binding energy is positive is stable for mass-preserving perturbations."],"url":"http://arxiv.org/abs/2402.10657v1","category":"math.AP"}
{"created":"2024-02-16 13:05:47","title":"Free-discontinuity problems generated by higher-order singular perturbations","abstract":"We carry on an asymptotic analysis for a class of non-convex integral functionals with a singular perturbation by derivatives of order $k\\ge 2$. In the case $k=2$ these functional have been already proved to converge to a free-discontinuity functional whose jump part energy density is a constant $m_2$ times the square root of the jump size by Alicandro et al.~and by Bouchitt\\'e et al. We show that the $k$-th order perturbation gives a constant, $m_k$, times the $k$-th root of the jump size as the jump part energy density. The constant $m_k$ is characterized by an optimal-profile problem with zero derivatives up to order $k-1$ at the boundary. Contrary to the case $k=2$, the set of points where these boundary conditions hold are not naturally selected from the energy, and the main technical part of the paper is the analysis of the properties of such sets, carried on by a combined use of interpolation inequalities and energy estimates. The result is proven for truncated quadratic energy densities and in the one-dimensional case, from which the general higher-dimensional case can be obtained by slicing techniques and a wide class of non-convex energies can be studied as an envelope of the particular ones. Finally, we remark that an approximation of the Mumford-Shah functional can be obtained by letting $k\\to+\\infty$.","sentences":["We carry on an asymptotic analysis for a class of non-convex integral functionals with a singular perturbation by derivatives of order $k\\ge 2$.","In the case $k=2$ these functional have been already proved to converge to a free-discontinuity functional whose jump part energy density is a constant $m_2$ times the square root of the jump size by Alicandro et al.~and by Bouchitt\\'e et al.","We show that the $k$-th order perturbation gives a constant, $m_k$, times the $k$-th root of the jump size as the jump part energy density.","The constant $m_k$ is characterized by an optimal-profile problem with zero derivatives up to order $k-1$ at the boundary.","Contrary to the case $k=2$, the set of points where these boundary conditions hold are not naturally selected from the energy, and the main technical part of the paper is the analysis of the properties of such sets, carried on by a combined use of interpolation inequalities and energy estimates.","The result is proven for truncated quadratic energy densities and in the one-dimensional case, from which the general higher-dimensional case can be obtained by slicing techniques and a wide class of non-convex energies can be studied as an envelope of the particular ones.","Finally, we remark that an approximation of the Mumford-Shah functional can be obtained by letting $k\\to+\\infty$."],"url":"http://arxiv.org/abs/2402.10656v1","category":"math.AP"}
{"created":"2024-02-16 13:02:11","title":"Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes","abstract":"Numerical reasoning is an essential ability for NLP systems to handle numeric information. Recent research indicates that fine-tuning a small-scale model to learn generating reasoning processes alongside answers can significantly enhance performance. However, current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are \"unreliable\" since such processes could contain information unrelated to the answer. To address this limitation, we introduce Enhancing NumeriCal reasOning with Reliable procEsses (Encore), which derives the reliable reasoning process by decomposing the answer formula, ensuring which fully supports the answer. Nevertheless, models could lack enough data to learn the reasoning process generation adequately, since our method generates only one single reasoning process for one formula. To overcome this difficulty, we present a series of pre-training tasks to help models learn the reasoning process generation with synthesized data. The experiments show that Encore yields improvement on all five experimental datasets with an average of 1.8%, proving the effectiveness of our method.","sentences":["Numerical reasoning is an essential ability for NLP systems to handle numeric information.","Recent research indicates that fine-tuning a small-scale model to learn generating reasoning processes alongside answers can significantly enhance performance.","However, current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are \"unreliable\" since such processes could contain information unrelated to the answer.","To address this limitation, we introduce Enhancing NumeriCal reasOning with Reliable procEsses (Encore), which derives the reliable reasoning process by decomposing the answer formula, ensuring which fully supports the answer.","Nevertheless, models could lack enough data to learn the reasoning process generation adequately, since our method generates only one single reasoning process for one formula.","To overcome this difficulty, we present a series of pre-training tasks to help models learn the reasoning process generation with synthesized data.","The experiments show that Encore yields improvement on all five experimental datasets with an average of 1.8%, proving the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.10654v1","category":"cs.CL"}
{"created":"2024-02-16 12:58:35","title":"Hilbert transforms and maximal functions along flat curves on the Heisenberg group","abstract":"We establish the $L^p$ boundedness of Hilbert transforms and maximal functions along flat curves in the Heisenberg group. This generalizes the $\\mathbb{R}^n$ result by Carbery, Christ, Vance, Wainger, and Watson. What is new about our result compared to the Heisenberg group generalization by Carbery, Wainger, and Wright is that we allow all three components of the curves to vary independently, we keep the original form of the conditions required in the $\\mathbb{R}^n$ case, and our method is likely to be generalized to other stratified nilpotent groups.","sentences":["We establish the $L^p$ boundedness of Hilbert transforms and maximal functions along flat curves in the Heisenberg group.","This generalizes the $\\mathbb{R}^n$ result by Carbery, Christ, Vance, Wainger, and Watson.","What is new about our result compared to the Heisenberg group generalization by Carbery, Wainger, and Wright is that we allow all three components of the curves to vary independently, we keep the original form of the conditions required in the $\\mathbb{R}^n$ case, and our method is likely to be generalized to other stratified nilpotent groups."],"url":"http://arxiv.org/abs/2402.10653v1","category":"math.CA"}
{"created":"2024-02-16 12:57:44","title":"A Gauss-Newton method for iterative optimization of memory kernels for generalized Langevin thermostats in coarse-grained molecular dynamics simulations","abstract":"In molecular dynamics simulations, dynamically consistent coarse-grained (CG) models commonly use stochastic thermostats to model friction and fluctuations that are lost in a CG description. While Markovian, i.e., time-local, formulations of such thermostats allow for an accurate representation of diffusivities/long-time dynamics, a correct description of the dynamics on all time scales generally requires non-Markovian, i.e., non-time-local, thermostats. These thermostats are typically of the form of a Generalized Langevin Equation (GLE) determined by a memory kernel. In this work, we use a Markovian embedded formulation of a position-independent GLE thermostat acting independently on each CG degree of freedom. Extracting the memory kernel of this CG model from atomistic reference data requires several approximations. Therefore, this task is easiest understood as an inverse problem. While our recently proposed approximate Newton scheme (IOMK) allows for the iterative optimization of a memory kernel, Markovian embedding remained potentially error-prone and computationally expensive. In this work, we present a Gauss-Newton scheme (IOMK-GN) based on IOMK, which allows for a direct parameterization of a Markovian embedded model.","sentences":["In molecular dynamics simulations, dynamically consistent coarse-grained (CG) models commonly use stochastic thermostats to model friction and fluctuations that are lost in a CG description.","While Markovian, i.e., time-local, formulations of such thermostats allow for an accurate representation of diffusivities/long-time dynamics, a correct description of the dynamics on all time scales generally requires non-Markovian, i.e., non-time-local, thermostats.","These thermostats are typically of the form of a Generalized Langevin Equation (GLE) determined by a memory kernel.","In this work, we use a Markovian embedded formulation of a position-independent GLE thermostat acting independently on each CG degree of freedom.","Extracting the memory kernel of this CG model from atomistic reference data requires several approximations.","Therefore, this task is easiest understood as an inverse problem.","While our recently proposed approximate Newton scheme (IOMK) allows for the iterative optimization of a memory kernel, Markovian embedding remained potentially error-prone and computationally expensive.","In this work, we present a Gauss-Newton scheme (IOMK-GN) based on IOMK, which allows for a direct parameterization of a Markovian embedded model."],"url":"http://arxiv.org/abs/2402.10652v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-16 12:54:06","title":"Quantum dimer models with Rydberg gadgets","abstract":"The Rydberg blockade mechanism is an important ingredient in quantum simulators based on neutral atom arrays. It enables the emergence of a rich variety of quantum phases of matter, such as topological spin liquids. The typically isotropic nature of the blockade effect, however, restricts the range of natively accessible models and quantum states. In this work, we propose a method to systematically overcome this limitation, by developing gadgets, i.e., specific arrangements of atoms, that transform the underlying Rydberg blockade into more general constraints. We apply this technique to realize dimer models on square and triangular geometries. In these setups, we study the role of the quantum fluctuations induced by a coherent drive of the atoms and find signatures of $U(1)$ and $\\mathbb{Z}_2$ quantum spin liquid states in the respective ground states. Finally, we show that these states can be dynamically prepared with high fidelity, paving the way for the quantum simulation of a broader class of constrained models and topological matter in experiments with Rydberg atom arrays.","sentences":["The Rydberg blockade mechanism is an important ingredient in quantum simulators based on neutral atom arrays.","It enables the emergence of a rich variety of quantum phases of matter, such as topological spin liquids.","The typically isotropic nature of the blockade effect, however, restricts the range of natively accessible models and quantum states.","In this work, we propose a method to systematically overcome this limitation, by developing gadgets, i.e., specific arrangements of atoms, that transform the underlying Rydberg blockade into more general constraints.","We apply this technique to realize dimer models on square and triangular geometries.","In these setups, we study the role of the quantum fluctuations induced by a coherent drive of the atoms and find signatures of $U(1)$ and $\\mathbb{Z}_2$ quantum spin liquid states in the respective ground states.","Finally, we show that these states can be dynamically prepared with high fidelity, paving the way for the quantum simulation of a broader class of constrained models and topological matter in experiments with Rydberg atom arrays."],"url":"http://arxiv.org/abs/2402.10651v1","category":"quant-ph"}
{"created":"2024-02-16 12:49:44","title":"Polynomial functors on flags","abstract":"We study generalizations of Schur functors from categories consisting of flags of vector spaces. We give different descriptions of the category of such functors in terms of representations of certain combinatorial categories and infinite rank groups, and we apply these descriptions to study polynomial representations and representation stability of parabolic subgroups of general linear groups.","sentences":["We study generalizations of Schur functors from categories consisting of flags of vector spaces.","We give different descriptions of the category of such functors in terms of representations of certain combinatorial categories and infinite rank groups, and we apply these descriptions to study polynomial representations and representation stability of parabolic subgroups of general linear groups."],"url":"http://arxiv.org/abs/2402.10648v1","category":"math.RT"}
{"created":"2024-02-16 12:47:11","title":"AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation","abstract":"Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.","sentences":["Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study.","Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored.","In this work, we design the framework AbsInstruct to enhance LLMs' abstraction ability through instruction tuning.","The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction.","Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned.","Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset.","Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities."],"url":"http://arxiv.org/abs/2402.10646v1","category":"cs.CL"}
{"created":"2024-02-16 12:46:16","title":"Can Separators Improve Chain-of-Thought Prompting?","abstract":"Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7B. Interestingly, the type/location of separators should be chosen appropriately to boost the reasoning capability of CoT.","sentences":["Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs).","The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt.","However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs.","Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting.","These separators are designed to help the LLMs understand their thought processes better while reasoning.","It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators.","We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7B. Interestingly, the type/location of separators should be chosen appropriately to boost the reasoning capability of CoT."],"url":"http://arxiv.org/abs/2402.10645v1","category":"cs.CL"}
{"created":"2024-02-16 12:43:26","title":"`Keep it Together': Enforcing Cohesion in Extractive Summaries by Simulating Human Memory","abstract":"Extractive summaries are usually presented as lists of sentences with no expected cohesion between them. In this paper, we aim to enforce cohesion whilst controlling for informativeness and redundancy in summaries, in cases where the input exhibits high redundancy. The pipeline controls for redundancy in long inputs as it is consumed, and balances informativeness and cohesion during sentence selection. Our sentence selector simulates human memory to keep track of topics --modeled as lexical chains--, enforcing cohesive ties between noun phrases. Across a variety of domains, our experiments revealed that it is possible to extract highly cohesive summaries that nevertheless read as informative to humans as summaries extracted by only accounting for informativeness or redundancy. The extracted summaries exhibit smooth topic transitions between sentences as signaled by lexical chains, with chains spanning adjacent or near-adjacent sentences.","sentences":["Extractive summaries are usually presented as lists of sentences with no expected cohesion between them.","In this paper, we aim to enforce cohesion whilst controlling for informativeness and redundancy in summaries, in cases where the input exhibits high redundancy.","The pipeline controls for redundancy in long inputs as it is consumed, and balances informativeness and cohesion during sentence selection.","Our sentence selector simulates human memory to keep track of topics --modeled as lexical chains--, enforcing cohesive ties between noun phrases.","Across a variety of domains, our experiments revealed that it is possible to extract highly cohesive summaries that nevertheless read as informative to humans as summaries extracted by only accounting for informativeness or redundancy.","The extracted summaries exhibit smooth topic transitions between sentences as signaled by lexical chains, with chains spanning adjacent or near-adjacent sentences."],"url":"http://arxiv.org/abs/2402.10643v1","category":"cs.CL"}
{"created":"2024-02-16 12:43:01","title":"Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model","abstract":"Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.","sentences":["Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks.","However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment.","Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability.","To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself?","In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain.","This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility.","By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement."],"url":"http://arxiv.org/abs/2402.10642v1","category":"eess.AS"}
{"created":"2024-02-16 12:41:31","title":"A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface","abstract":"This paper aims to comprehensively investigate the efficacy of various Model Order Reduction (MOR) and deep learning techniques in predicting heat transfer in a pulsed jet impinging on a concave surface. Expanding on the previous experimental and numerical research involving pulsed circular jets, this investigation extends to evaluate Predictive Surrogate Models (PSM) for heat transfer across various jet characteristics. To this end, this work introduces two predictive approaches, one employing a Fast Fourier Transformation augmented Artificial Neural Network (FFT-ANN) for predicting the average Nusselt number under constant-frequency scenarios. Moreover, the investigation introduces the Proper Orthogonal Decomposition and Long Short-Term Memory (POD-LSTM) approach for random-frequency impingement jets. The POD-LSTM method proves to be a robust solution for predicting the local heat transfer rate under random-frequency impingement scenarios, capturing both the trend and value of temporal modes. The comparison of these approaches highlights the versatility and efficacy of advanced machine learning techniques in modelling complex heat transfer phenomena.","sentences":["This paper aims to comprehensively investigate the efficacy of various Model Order Reduction (MOR) and deep learning techniques in predicting heat transfer in a pulsed jet impinging on a concave surface.","Expanding on the previous experimental and numerical research involving pulsed circular jets, this investigation extends to evaluate Predictive Surrogate Models (PSM) for heat transfer across various jet characteristics.","To this end, this work introduces two predictive approaches, one employing a Fast Fourier Transformation augmented Artificial Neural Network (FFT-ANN) for predicting the average Nusselt number under constant-frequency scenarios.","Moreover, the investigation introduces the Proper Orthogonal Decomposition and Long Short-Term Memory (POD-LSTM) approach for random-frequency impingement jets.","The POD-LSTM method proves to be a robust solution for predicting the local heat transfer rate under random-frequency impingement scenarios, capturing both the trend and value of temporal modes.","The comparison of these approaches highlights the versatility and efficacy of advanced machine learning techniques in modelling complex heat transfer phenomena."],"url":"http://arxiv.org/abs/2402.10641v1","category":"math.NA"}
{"created":"2024-02-16 12:38:03","title":"Relaxation and noise-driven oscillations in a model of mitotic spindle dynamics","abstract":"During cell division, the mitotic spindle moves dynamically through the cell to position the chromosomes and determine the ultimate spatial position of the two daughter cells. These movements have been attributed to the action of cortical force generators which pull on the astral microtubules to position the spindle, as well as pushing events by these same microtubules against the cell cortex and membrane. Attachment and detachment of cortical force generators working antagonistically against centring forces of microtubules have been modelled previously (Grill et al. 2005, Phys. Rev. Lett. 94:108104) via stochastic simulations and Fokker-Planck equations to predict oscillations of a spindle pole in one spatial dimension. Using systematic asymptotic methods, we reduce the Fokker-Planck system to a set of ordinary differential equations (ODEs), consistent with a set proposed by Grill et al., which provide accurate predictions of the conditions for the Fokker-Planck system to exhibit oscillations. In the limit of small restoring forces, we derive an algebraic prediction of the amplitude of spindle-pole oscillations and demonstrate the relaxation structure of nonlinear oscillations. We also show how noise-induced oscillations can arise in stochastic simulations for conditions in which the Fokker-Planck system predicts stability, but for which the period can be estimated directly by the ODE model.","sentences":["During cell division, the mitotic spindle moves dynamically through the cell to position the chromosomes and determine the ultimate spatial position of the two daughter cells.","These movements have been attributed to the action of cortical force generators which pull on the astral microtubules to position the spindle, as well as pushing events by these same microtubules against the cell cortex and membrane.","Attachment and detachment of cortical force generators working antagonistically against centring forces of microtubules have been modelled previously (Grill et al. 2005, Phys.","Rev. Lett.","94:108104) via stochastic simulations and Fokker-Planck equations to predict oscillations of a spindle pole in one spatial dimension.","Using systematic asymptotic methods, we reduce the Fokker-Planck system to a set of ordinary differential equations (ODEs), consistent with a set proposed by Grill et al., which provide accurate predictions of the conditions for the Fokker-Planck system to exhibit oscillations.","In the limit of small restoring forces, we derive an algebraic prediction of the amplitude of spindle-pole oscillations and demonstrate the relaxation structure of nonlinear oscillations.","We also show how noise-induced oscillations can arise in stochastic simulations for conditions in which the Fokker-Planck system predicts stability, but for which the period can be estimated directly by the ODE model."],"url":"http://arxiv.org/abs/2402.10638v1","category":"q-bio.SC"}
{"created":"2024-02-16 12:35:35","title":"PEGASUS: Personalized Generative 3D Avatars with Composable Attributes","abstract":"We present, PEGASUS, a method for constructing personalized generative 3D face avatars from monocular video sources. As a compositional generative model, our model enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) of the target individual, while preserving the identity. We present two key approaches to achieve this goal. First, we present a method to construct a person-specific generative 3D avatar by building a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing parts from diverse individuals from other monocular videos. Through several experiments, we demonstrate the superior performance of our approach by generating unseen attributes with high realism. Subsequently, we introduce a zero-shot approach to achieve the same generative modeling more efficiently by leveraging a previously constructed personalized generative model.","sentences":["We present, PEGASUS, a method for constructing personalized generative 3D face avatars from monocular video sources.","As a compositional generative model, our model enables disentangled controls to selectively alter the facial attributes (e.g., hair or nose) of the target individual, while preserving the identity.","We present two key approaches to achieve this goal.","First, we present a method to construct a person-specific generative 3D avatar by building a synthetic video collection of the target identity with varying facial attributes, where the videos are synthesized by borrowing parts from diverse individuals from other monocular videos.","Through several experiments, we demonstrate the superior performance of our approach by generating unseen attributes with high realism.","Subsequently, we introduce a zero-shot approach to achieve the same generative modeling more efficiently by leveraging a previously constructed personalized generative model."],"url":"http://arxiv.org/abs/2402.10636v1","category":"cs.CV"}
{"created":"2024-02-16 12:34:38","title":"ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling","abstract":"Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of Transformers. We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer. A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data. The project link is https://seqml.github.io/contiformer/.","sentences":["Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously.","Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns.","However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms.","Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences.","It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system.","To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, which explicitly incorporates the modeling abilities of continuous dynamics of Neural ODEs with the attention mechanism of Transformers.","We mathematically characterize the expressive power of ContiFormer and illustrate that, by curated designs of function hypothesis, many Transformer variants specialized in irregular time series modeling can be covered as a special case of ContiFormer.","A wide range of experiments on both synthetic and real-world datasets have illustrated the superior modeling capacities and prediction performance of ContiFormer on irregular time series data.","The project link is https://seqml.github.io/contiformer/."],"url":"http://arxiv.org/abs/2402.10635v1","category":"cs.LG"}
{"created":"2024-02-16 12:33:31","title":"Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling","abstract":"Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts. Our approach outperforms state-of-the-art methods on synthetic and real-world benchmarks under different missing data distributions, particularly in the presence of contiguous blocks of missing values.","sentences":["Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point.","Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph.","Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing.","In this work, we tackle this problem through hierarchical spatiotemporal downsampling.","The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics.","Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate the forecasts.","Our approach outperforms state-of-the-art methods on synthetic and real-world benchmarks under different missing data distributions, particularly in the presence of contiguous blocks of missing values."],"url":"http://arxiv.org/abs/2402.10634v1","category":"cs.LG"}
{"created":"2024-02-16 12:31:40","title":"Crossing number of graphs and $\\mathsf{\u0394Y}$-move","abstract":"The crossing number of a graph is the minimum number of double points over all generic immersions of the graph into the plane. In this paper we investigate the behavior of crossing number under a graph transformation, called $\\mathsf{\\Delta Y}$-move, on the complete graph $K_n$. Concretely it is shown that for any $k\\in \\mathbb{N}$, there exist a natural number $n$ and a sequence of $\\mathsf{\\Delta Y}$-moves $K_n\\rightarrow G^{(1)}\\rightarrow \\cdots \\rightarrow G^{(k)}$ which is decreasing with respect to the crossing number. We also discuss the decrease of crossing number for relatively small $n$.","sentences":["The crossing number of a graph is the minimum number of double points over all generic immersions of the graph into the plane.","In this paper we investigate the behavior of crossing number under a graph transformation, called $\\mathsf{\\Delta Y}$-move, on the complete graph $K_n$. Concretely it is shown that for any $k\\in \\mathbb{N}$, there exist a natural number $n$ and a sequence of $\\mathsf{\\Delta Y}$-moves $K_n\\rightarrow G^{(1)}\\rightarrow \\cdots \\rightarrow G^{(k)}$ which is decreasing with respect to the crossing number.","We also discuss the decrease of crossing number for relatively small $n$."],"url":"http://arxiv.org/abs/2402.10633v1","category":"math.CO"}
{"created":"2024-02-16 12:28:15","title":"A reduction theorem for the Character Triple Conjecture","abstract":"In this paper, we show that the Character Triple Conjecture holds for all finite groups once assumed for all quasi-simple groups. This answers the question on the existence of a self-reducing form of Dade's conjecture, a problem that was long investigated by Dade in the 1990s. Our result shows that this role is played by the Character Triple Conjecture, recently introduced by Sp\\\"ath, that we present here in a general form free of all previously imposed restrictions.","sentences":["In this paper, we show that the Character Triple Conjecture holds for all finite groups once assumed for all quasi-simple groups.","This answers the question on the existence of a self-reducing form of Dade's conjecture, a problem that was long investigated by Dade in the 1990s.","Our result shows that this role is played by the Character Triple Conjecture, recently introduced by Sp\\\"ath, that we present here in a general form free of all previously imposed restrictions."],"url":"http://arxiv.org/abs/2402.10632v1","category":"math.RT"}
{"created":"2024-02-16 12:27:15","title":"BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation","abstract":"The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.","sentences":["The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges.","Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands.","This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit).","Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance.","Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks.","Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources.","The code is available at https://github.com/DD-DuDa/BitDistiller."],"url":"http://arxiv.org/abs/2402.10631v1","category":"cs.CL"}
{"created":"2024-02-16 12:27:02","title":"Reduced inequalities for vector-valued functions","abstract":"Building on the notion of convex body domination introduced by Nazarov, Petermichl, Treil, and Volberg, we provide a general principle of bootstrapping bilinear estimates for scalar-valued functions into vector-valued versions with a reduced right-hand side involving iterated norms of a pointwise dot product $\\vec f(x)\\cdot\\vec g(y)$ instead of the product of lengths $|\\vec f(x)| |\\vec g(y)|$ that would result from a na\\\"ive extension of the scalar inequality. On the way, we study connections between convex body domination and tensor norms. In order to cover the full regime of $L^p$ norms, also with $p<1$, that naturally arise in bilinear harmonic analysis, we develop a framework in general quasi-normed spaces. A key application is a vector-valued Kato-Ponce inequality (or fractional Leibnitz rule) with a reduced right-hand side, which we obtain as a soft corollary of the known scalar-valued version and our general bootstrapping method.","sentences":["Building on the notion of convex body domination introduced by Nazarov, Petermichl, Treil, and Volberg, we provide a general principle of bootstrapping bilinear estimates for scalar-valued functions into vector-valued versions with a reduced right-hand side involving iterated norms of a pointwise dot product $\\vec f(x)\\cdot\\vec g(y)$ instead of the product of lengths $|\\vec f(x)| |\\vec g(y)|$ that would result from a na\\\"ive extension of the scalar inequality.","On the way, we study connections between convex body domination and tensor norms.","In order to cover the full regime of $L^p$ norms, also with $p<1$, that naturally arise in bilinear harmonic analysis, we develop a framework in general quasi-normed spaces.","A key application is a vector-valued Kato-Ponce inequality (or fractional Leibnitz rule) with a reduced right-hand side, which we obtain as a soft corollary of the known scalar-valued version and our general bootstrapping method."],"url":"http://arxiv.org/abs/2402.10630v1","category":"math.FA"}
{"created":"2024-02-16 12:26:47","title":"Robust Beamforming for RIS-aided Communications: Gradient-based Manifold Meta Learning","abstract":"Reconfigurable intelligent surface (RIS) has become a promising technology to realize the programmable wireless environment via steering the incident signal in fully customizable ways. However, a major challenge in RIS-aided communication systems is the simultaneous design of the precoding matrix at the base station (BS) and the phase shifting matrix of the RIS elements. This is mainly attributed to the highly non-convex optimization space of variables at both the BS and the RIS, and the diversity of communication environments. Generally, traditional optimization methods for this problem suffer from the high complexity, while existing deep learning based methods are lack of robustness in various scenarios. To address these issues, we introduce a gradient-based manifold meta learning method (GMML), which works without pre-training and has strong robustness for RIS-aided communications. Specifically, the proposed method fuses meta learning and manifold learning to improve the overall spectral efficiency, and reduce the overhead of the high-dimensional signal process. Unlike traditional deep learning based methods which directly take channel state information as input, GMML feeds the gradients of the precoding matrix and phase shifting matrix into neural networks. Coherently, we design a differential regulator to constrain the phase shifting matrix of the RIS. Numerical results show that the proposed GMML can improve the spectral efficiency by up to 7.31\\%, and speed up the convergence by 23 times faster compared to traditional approaches. Moreover, they also demonstrate remarkable robustness and adaptability in dynamic settings.","sentences":["Reconfigurable intelligent surface (RIS) has become a promising technology to realize the programmable wireless environment via steering the incident signal in fully customizable ways.","However, a major challenge in RIS-aided communication systems is the simultaneous design of the precoding matrix at the base station (BS) and the phase shifting matrix of the RIS elements.","This is mainly attributed to the highly non-convex optimization space of variables at both the BS and the RIS, and the diversity of communication environments.","Generally, traditional optimization methods for this problem suffer from the high complexity, while existing deep learning based methods are lack of robustness in various scenarios.","To address these issues, we introduce a gradient-based manifold meta learning method (GMML), which works without pre-training and has strong robustness for RIS-aided communications.","Specifically, the proposed method fuses meta learning and manifold learning to improve the overall spectral efficiency, and reduce the overhead of the high-dimensional signal process.","Unlike traditional deep learning based methods which directly take channel state information as input, GMML feeds the gradients of the precoding matrix and phase shifting matrix into neural networks.","Coherently, we design a differential regulator to constrain the phase shifting matrix of the RIS.","Numerical results show that the proposed GMML can improve the spectral efficiency by up to 7.31\\%, and speed up the convergence by 23 times faster compared to traditional approaches.","Moreover, they also demonstrate remarkable robustness and adaptability in dynamic settings."],"url":"http://arxiv.org/abs/2402.10626v1","category":"cs.IT"}
{"created":"2024-02-16 12:16:04","title":"Towards quantum gravity with neural networks: Solving the quantum Hamilton constraint of U(1) BF theory","abstract":"In the canonical approach of loop quantum gravity, arguably the most important outstanding problem is finding and interpreting solutions to the Hamiltonian constraint. In this work, we demonstrate that methods of machine learning are in principle applicable to this problem. We consider U(1) BF theory in 3 dimensions, quantized with loop quantum gravity methods. In particular, we formulate a master constraint corresponding to Hamilton and Gauss constraints using loop quantum gravity methods. To make the problem amenable for numerical simulation we fix a graph and introduce a cutoff on the kinematical degrees of freedom, effectively considering U$_q$(1) BF theory at a root of unity. We show that the Neural Network Quantum State (NNQS) ansatz can be used to numerically solve the constraints efficiently and accurately. We compute expectation values and fluctuations of certain observables and compare them with exact results or exact numerical methods where possible. We also study the dependence on the cutoff.","sentences":["In the canonical approach of loop quantum gravity, arguably the most important outstanding problem is finding and interpreting solutions to the Hamiltonian constraint.","In this work, we demonstrate that methods of machine learning are in principle applicable to this problem.","We consider U(1) BF theory in 3 dimensions, quantized with loop quantum gravity methods.","In particular, we formulate a master constraint corresponding to Hamilton and Gauss constraints using loop quantum gravity methods.","To make the problem amenable for numerical simulation we fix a graph and introduce a cutoff on the kinematical degrees of freedom, effectively considering U$_q$(1) BF theory at a root of unity.","We show that the Neural Network Quantum State (NNQS) ansatz can be used to numerically solve the constraints efficiently and accurately.","We compute expectation values and fluctuations of certain observables and compare them with exact results or exact numerical methods where possible.","We also study the dependence on the cutoff."],"url":"http://arxiv.org/abs/2402.10622v1","category":"gr-qc"}
{"created":"2024-02-16 12:13:03","title":"Testing Higher Derivative Gravity Through Tunnelling","abstract":"Higher derivative terms in the gravitational action are natural from the perspective of quantum gravity, but are perceived as leading to a lack of well-posedness. The Gauss Bonnet term has second-order equations of motion, but does not impact gravitational dynamics in 4D, so one might expect that it is not physically relevant. We discuss how signatures can show up in tunnelling processes and whether these will likely be physically accessible in Higgs vacuum decay.","sentences":["Higher derivative terms in the gravitational action are natural from the perspective of quantum gravity, but are perceived as leading to a lack of well-posedness.","The Gauss Bonnet term has second-order equations of motion, but does not impact gravitational dynamics in 4D, so one might expect that it is not physically relevant.","We discuss how signatures can show up in tunnelling processes and whether these will likely be physically accessible in Higgs vacuum decay."],"url":"http://arxiv.org/abs/2402.10620v1","category":"hep-th"}
{"created":"2024-02-16 12:12:05","title":"Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement","abstract":"The advent of Large Language Models (LLMs) has propelled dialogue generation into new realms, particularly in the field of role-playing systems (RPSs). While enhanced with ordinary role-relevant training dialogues, existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios. In this paper, we design the Modular ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve the role-playing LLMs' performance. MORTISE can produce highly role-relevant aggressive queries through the collaborative effort of multiple LLM-based modules, and formulate corresponding responses to create an adversarial training dataset via a consistent response generator. We select 190 Chinese and English roles to construct aggressive queries to benchmark existing role-playing LLMs. Through comprehensive evaluation, we find that existing models exhibit a general deficiency in role alignment capabilities. We further select 180 of the roles to collect an adversarial training dataset (named RoleAD) and retain the other 10 roles for testing. Experiments on models improved by RoleAD indicate that our adversarial dataset ameliorates this deficiency, with the improvements demonstrating a degree of generalizability in ordinary scenarios.","sentences":["The advent of Large Language Models (LLMs) has propelled dialogue generation into new realms, particularly in the field of role-playing systems (RPSs).","While enhanced with ordinary role-relevant training dialogues, existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios.","In this paper, we design the Modular ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve the role-playing LLMs' performance.","MORTISE can produce highly role-relevant aggressive queries through the collaborative effort of multiple LLM-based modules, and formulate corresponding responses to create an adversarial training dataset via a consistent response generator.","We select 190 Chinese and English roles to construct aggressive queries to benchmark existing role-playing LLMs.","Through comprehensive evaluation, we find that existing models exhibit a general deficiency in role alignment capabilities.","We further select 180 of the roles to collect an adversarial training dataset (named RoleAD) and retain the other 10 roles for testing.","Experiments on models improved by RoleAD indicate that our adversarial dataset ameliorates this deficiency, with the improvements demonstrating a degree of generalizability in ordinary scenarios."],"url":"http://arxiv.org/abs/2402.10618v1","category":"cs.CL"}
{"created":"2024-02-16 12:11:34","title":"Multitask Kernel-based Learning with Logic Constraints","abstract":"This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised examples, a regularization term, and a penalty term that enforces the constraints on both supervised and unsupervised examples. The proposed semi-supervised learning framework is particularly suited for learning in high dimensionality feature spaces, where the supervised training examples tend to be sparse and generalization difficult. Unlike for standard kernel machines, the cost function to optimize is not generally guaranteed to be convex. However, the experimental results show that it is still possible to find good solutions using a two stage learning schema, in which first the supervised examples are learned until convergence and then the logic constraints are forced. Some promising experimental results on artificial multi-task learning tasks are reported, showing how the classification accuracy can be effectively improved by exploiting the a priori rules and the unsupervised examples.","sentences":["This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines.","The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples.","In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input.","A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates.","The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised examples, a regularization term, and a penalty term that enforces the constraints on both supervised and unsupervised examples.","The proposed semi-supervised learning framework is particularly suited for learning in high dimensionality feature spaces, where the supervised training examples tend to be sparse and generalization difficult.","Unlike for standard kernel machines, the cost function to optimize is not generally guaranteed to be convex.","However, the experimental results show that it is still possible to find good solutions using a two stage learning schema, in which first the supervised examples are learned until convergence and then the logic constraints are forced.","Some promising experimental results on artificial multi-task learning tasks are reported, showing how the classification accuracy can be effectively improved by exploiting the a priori rules and the unsupervised examples."],"url":"http://arxiv.org/abs/2402.10617v1","category":"cs.LG"}
{"created":"2024-02-16 12:00:34","title":"Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements","abstract":"Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate & tuning (\"DEBATunE\") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic. Evaluations by the GPT-4 judge with a novel controversy controllability metric show that LLMs' capability of expressing diverse perspectives is significantly improved by DEBATunE. Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments. Our codes, models, and data will be released at https://github.com/tianyi-lab/DEBATunE.","sentences":["Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment.","However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements.","In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt.","We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs.","Motivated by this, we develop a novel debate & tuning (\"DEBATunE\") pipeline finetuning LLMs to generate the statements obtained via debate.","To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 controversial topics and corresponding arguments for each topic.","Evaluations by the GPT-4 judge with a novel controversy controllability metric show that LLMs' capability of expressing diverse perspectives is significantly improved by DEBATunE.","Moreover, such controllability can be generalized to unseen topics, generating high-quality statements supporting controversial arguments.","Our codes, models, and data will be released at https://github.com/tianyi-lab/DEBATunE."],"url":"http://arxiv.org/abs/2402.10614v1","category":"cs.CL"}
{"created":"2024-02-16 11:56:45","title":"Advancements and challenges in plasmon-exciton quantum emitters based on colloidal quantum dots","abstract":"The Nobel Prizes in Physics (2022) and Chemistry (2023) heralded the recognition of quantum information science and the synthesis of quantum dots, respectively. This acknowledgment has propelled colloidal quantum dots and perovskite nanocrystals to the forefront of quantum technologies. Their distinct emission properties, facilitating the efficient generation of both single photons and photon pairs, render them particularly captivating. Moreover, their adaptability to diverse structures, ranging from traditional electronics to nanopatterned frameworks, underscores their pivotal role in shaping quantum technologies. Despite notable strides in synthesis, certain properties require refinement for enhanced applicability in quantum information, encompassing emission brightness, stability, single photon indistinguishability, and entanglement fidelity of photon pairs. Here we offer an overview of recent achievements in plasmon-exciton quantum emitters based on luminescent semiconductor nanocrystals. Emphasizing the utilization of the light-matter coupling phenomenon, we explore how this interaction enables the manipulation of quantum properties without altering the chemical structure of the emitters. This approach addresses critical aspects for quantum information applications, offering precise control over emission rate, intensity, and energy. The development of these hybrid systems represents a significant stride forward, demonstrating their potential to overcome existing challenges and advance the integration of quantum emitters into cutting-edge quantum technology applications.","sentences":["The Nobel Prizes in Physics (2022) and Chemistry (2023) heralded the recognition of quantum information science and the synthesis of quantum dots, respectively.","This acknowledgment has propelled colloidal quantum dots and perovskite nanocrystals to the forefront of quantum technologies.","Their distinct emission properties, facilitating the efficient generation of both single photons and photon pairs, render them particularly captivating.","Moreover, their adaptability to diverse structures, ranging from traditional electronics to nanopatterned frameworks, underscores their pivotal role in shaping quantum technologies.","Despite notable strides in synthesis, certain properties require refinement for enhanced applicability in quantum information, encompassing emission brightness, stability, single photon indistinguishability, and entanglement fidelity of photon pairs.","Here we offer an overview of recent achievements in plasmon-exciton quantum emitters based on luminescent semiconductor nanocrystals.","Emphasizing the utilization of the light-matter coupling phenomenon, we explore how this interaction enables the manipulation of quantum properties without altering the chemical structure of the emitters.","This approach addresses critical aspects for quantum information applications, offering precise control over emission rate, intensity, and energy.","The development of these hybrid systems represents a significant stride forward, demonstrating their potential to overcome existing challenges and advance the integration of quantum emitters into cutting-edge quantum technology applications."],"url":"http://arxiv.org/abs/2402.10613v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-16 11:55:40","title":"Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models","abstract":"Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Upon detecting inconsistencies indicative of hallucinations, Rowen activates the retrieval of external information to rectify the model outputs. Rowen adeptly harmonizes the intrinsic parameters in LLMs with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal reasoning and external evidence. Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.","sentences":["Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs).","The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations.","While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations.","A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations.","In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs.","This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries.","Upon detecting inconsistencies indicative of hallucinations, Rowen activates the retrieval of external information to rectify the model outputs.","Rowen adeptly harmonizes the intrinsic parameters in LLMs with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal reasoning and external evidence.","Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs."],"url":"http://arxiv.org/abs/2402.10612v1","category":"cs.CL"}
{"created":"2024-02-16 11:55:39","title":"Spectral variations within solar flare ribbons","abstract":"Solar flare ribbons are intense brightenings of principally chromospheric material that are responsible for a large fraction of the chromospheric emission in solar and stellar flares. We present an on-disc observation of flare ribbon substructures in an X9.3-class flare observed by the Swedish 1-m Solar Telescope. We identify categories of ribbon substructures seen in the Ca II 8542 \\AA, H$\\alpha$, and Ca II K lines, focusing on their spatial locations and their (spectro-)polarimetric properties. Color Collapsed Plotting (COCOPLOT) software is used to assist in identifying areas of interest. We present five categories of spectral profiles within the general body of the flare ribbon: (1) Extremely broadened spectral line profiles, where the standard Fabry-Perot interferometer wavelength windows ($\\approx 70$ km s$^{-1}$) are insufficiently wide to allow for a complete analysis of the dynamics and atmospheric conditions. The mechanisms causing this degree of this broadening are not yet clearly understood. (2) Long-lived, dense kernels that manifest as more saturated chromospheric line profiles with lower signal in both Stokes parameters. They are interpreted as footpoints of bunched magnetic field loops, whose chromospheric lines form at greater heights than the nearby areas. (3) Doppler-shifted leading edges of the flare ribbon in regions that transiently display lower Stokes signals due to the emission dominating at greater heights in the atmosphere. (4) Condensed coronal rain overlapping the flare ribbons in the line of sight, producing exceptionally high Doppler shifts near the footpoints...","sentences":["Solar flare ribbons are intense brightenings of principally chromospheric material that are responsible for a large fraction of the chromospheric emission in solar and stellar flares.","We present an on-disc observation of flare ribbon substructures in an X9.3-class flare observed by the Swedish 1-m Solar Telescope.","We identify categories of ribbon substructures seen in the Ca II 8542 \\AA, H$\\alpha$, and Ca II K lines, focusing on their spatial locations and their (spectro-)polarimetric properties.","Color Collapsed Plotting (COCOPLOT) software is used to assist in identifying areas of interest.","We present five categories of spectral profiles within the general body of the flare ribbon: (1) Extremely broadened spectral line profiles, where the standard Fabry-Perot interferometer wavelength windows ($\\approx 70$ km s$^{-1}$) are insufficiently wide to allow for a complete analysis of the dynamics and atmospheric conditions.","The mechanisms causing this degree of this broadening are not yet clearly understood.","(2) Long-lived, dense kernels that manifest as more saturated chromospheric line profiles with lower signal in both Stokes parameters.","They are interpreted as footpoints of bunched magnetic field loops, whose chromospheric lines form at greater heights than the nearby areas.","(3) Doppler-shifted leading edges of the flare ribbon in regions that transiently display lower Stokes signals due to the emission dominating at greater heights in the atmosphere.","(4) Condensed coronal rain overlapping the flare ribbons in the line of sight, producing exceptionally high Doppler shifts near the footpoints..."],"url":"http://arxiv.org/abs/2402.10611v1","category":"astro-ph.SR"}
{"created":"2024-02-16 11:54:34","title":"U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model","abstract":"Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion methods on in-domain datasets while demonstrating the best generalizability on out-of-domain datasets. To the best of our knowledge, U$^2$MRPD is the {\\bf first} unsupervised method that demonstrates the universal prowess of a LLDM, %trained on magnitude-only natural images in medical imaging, attaining the best adaptability for both MRI database-free and database-available scenarios and generalizability towards out-of-domain data.","sentences":["Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images.","To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD).","Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images.","With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact.","Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion methods on in-domain datasets while demonstrating the best generalizability on out-of-domain datasets.","To the best of our knowledge, U$^2$MRPD is the {\\bf first} unsupervised method that demonstrates the universal prowess of a LLDM, %trained on magnitude-only natural images in medical imaging, attaining the best adaptability for both MRI database-free and database-available scenarios and generalizability towards out-of-domain data."],"url":"http://arxiv.org/abs/2402.10609v1","category":"eess.IV"}
{"created":"2024-02-16 11:48:58","title":"Combination of searches for heavy spin-1 resonances using 139 fb$^{-1}$ of proton-proton collision data at $\\sqrt{s} = 13$ TeV with the ATLAS detector","abstract":"A combination of searches for new heavy spin-1 resonances decaying into different pairings of $W$, $Z$, or Higgs bosons, as well as directly into leptons or quarks, is presented. The data sample used corresponds to 139 fb$^{-1}$ of proton-proton collisions at $\\sqrt{s}$ = 13 TeV collected during 2015-2018 with the ATLAS detector at the CERN Large Hadron Collider. Analyses selecting quark pairs ($qq$, $bb$, $t\\bar{t}$, and $tb$) or third-generation leptons ($\\tau\\nu$ and $\\tau\\tau$) are included in this kind of combination for the first time. A simplified model predicting a spin-1 heavy vector-boson triplet is used. Cross-section limits are set at the 95% confidence level and are compared with predictions for the benchmark model. These limits are also expressed in terms of constraints on couplings of the heavy vector-boson triplet to quarks, leptons, and the Higgs boson. The complementarity of the various analyses increases the sensitivity to new physics, and the resulting constraints are stronger than those from any individual analysis considered. The data exclude a heavy vector-boson triplet with mass below 5.8 TeV in a weakly coupled scenario, below 4.4 TeV in a strongly coupled scenario, and up to 1.5 TeV in the case of production via vector-boson fusion.","sentences":["A combination of searches for new heavy spin-1 resonances decaying into different pairings of $W$, $Z$, or Higgs bosons, as well as directly into leptons or quarks, is presented.","The data sample used corresponds to 139 fb$^{-1}$ of proton-proton collisions at $\\sqrt{s}$ = 13 TeV collected during 2015-2018 with the ATLAS detector at the CERN Large Hadron Collider.","Analyses selecting quark pairs ($qq$, $bb$, $t\\bar{t}$, and $tb$) or third-generation leptons ($\\tau\\nu$ and $\\tau\\tau$) are included in this kind of combination for the first time.","A simplified model predicting a spin-1 heavy vector-boson triplet is used.","Cross-section limits are set at the 95% confidence level and are compared with predictions for the benchmark model.","These limits are also expressed in terms of constraints on couplings of the heavy vector-boson triplet to quarks, leptons, and the Higgs boson.","The complementarity of the various analyses increases the sensitivity to new physics, and the resulting constraints are stronger than those from any individual analysis considered.","The data exclude a heavy vector-boson triplet with mass below 5.8 TeV in a weakly coupled scenario, below 4.4 TeV in a strongly coupled scenario, and up to 1.5 TeV in the case of production via vector-boson fusion."],"url":"http://arxiv.org/abs/2402.10607v1","category":"hep-ex"}
{"created":"2024-02-16 11:37:05","title":"Jailbreaking Proprietary Large Language Models using Word Substitution Cipher","abstract":"Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbreaking approach on state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro. Additionally, we discuss the over-defensiveness of these models. We believe that our work will encourage further research in making these LLMs more robust while maintaining their decoding capabilities.","sentences":["Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process.","However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves.","In this paper, we present jailbreaking prompts encoded using cryptographic techniques.","We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively.","Motivated by this result, we use this encoding technique for writing jailbreaking prompts.","We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words.","Experimental results show an attack success rate (up to 59.42%) of our proposed jailbreaking approach on state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro.","Additionally, we discuss the over-defensiveness of these models.","We believe that our work will encourage further research in making these LLMs more robust while maintaining their decoding capabilities."],"url":"http://arxiv.org/abs/2402.10601v1","category":"cs.CL"}
{"created":"2024-02-16 11:30:20","title":"Quantum second harmonic generation in terms of elementary processes","abstract":"We address the quantum dynamics of second harmonic generation with a perturbative approach. By inspecting the Taylor expansion of the unitary evolution, we identify the subsequent application of annihilation and creation operators as elementary processes and find out how the expansion of the second-harmonic photon-number probability distribution can be expressed in terms of the interplay of these processes. We show that overlaps between the output states of different elementary processes contribute to the expansion of the probability distribution and provide a diagrammatic technique to analytically retrieve terms of the distribution expansion at any order.","sentences":["We address the quantum dynamics of second harmonic generation with a perturbative approach.","By inspecting the Taylor expansion of the unitary evolution, we identify the subsequent application of annihilation and creation operators as elementary processes and find out how the expansion of the second-harmonic photon-number probability distribution can be expressed in terms of the interplay of these processes.","We show that overlaps between the output states of different elementary processes contribute to the expansion of the probability distribution and provide a diagrammatic technique to analytically retrieve terms of the distribution expansion at any order."],"url":"http://arxiv.org/abs/2402.10598v1","category":"quant-ph"}
{"created":"2024-02-16 11:30:11","title":"Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks","abstract":"The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as $25$ million parameters.   Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can operate on low-cost, in-house computing infrastructure. The advantages of these models, in terms of speed and reduced training costs, dramatically outweighs any performance gain from large foundation LLMs. Furthermore, we highlight how domain-specific pre-training interacts with PEFT methods and model size, and discuss how these factors interplay to provide the best efficiency-performance trade-off. Full code available at: tbd.","sentences":["The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability, followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods.","We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as $25$ million parameters.   ","Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance.","The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can operate on low-cost, in-house computing infrastructure.","The advantages of these models, in terms of speed and reduced training costs, dramatically outweighs any performance gain from large foundation LLMs.","Furthermore, we highlight how domain-specific pre-training interacts with PEFT methods and model size, and discuss how these factors interplay to provide the best efficiency-performance trade-off.","Full code available at: tbd."],"url":"http://arxiv.org/abs/2402.10597v1","category":"cs.CL"}
{"created":"2024-02-16 11:28:09","title":"Bayesian Learning for Double-RIS Aided ISAC Systems with Superimposed Pilots and Data","abstract":"Reconfigurable intelligent surface (RIS) has great potential to improve the performance of integrated sensing and communication (ISAC) systems, especially in scenarios where line-of-sight paths between the base station and users are blocked. However, the spectral efficiency (SE) of RIS-aided ISAC uplink transmissions may be drastically reduced by the heavy burden of pilot overhead for realizing sensing capabilities. In this paper, we tackle this bottleneck by proposing a superimposed symbol scheme, which superimposes sensing pilots onto data symbols over the same time-frequency resources. Specifically, we develop a structure-aware sparse Bayesian learning framework, where decoded data symbols serve as side information to enhance sensing performance and increase SE. To meet the low-latency requirements of emerging ISAC applications, we further propose a low-complexity simultaneous communication and localization algorithm for multiple users. This algorithm employs the unitary approximate message passing in the Bayesian learning framework for initial angle estimate, followed by iterative refinements through reduced-dimension matrix calculations. Moreover, the sparse code multiple access technology is incorporated into this iterative framework for accurate data detection which also facilitates localization. Numerical results show that the proposed superimposed symbol-based scheme empowered by the developed algorithm can achieve centimeter-level localization while attaining up to $96\\%$ of the SE of conventional communications without sensing capabilities. Moreover, compared to other typical ISAC schemes, the proposed superimposed symbol scheme can provide an effective throughput improvement over $133\\%$.","sentences":["Reconfigurable intelligent surface (RIS) has great potential to improve the performance of integrated sensing and communication (ISAC) systems, especially in scenarios where line-of-sight paths between the base station and users are blocked.","However, the spectral efficiency (SE) of RIS-aided ISAC uplink transmissions may be drastically reduced by the heavy burden of pilot overhead for realizing sensing capabilities.","In this paper, we tackle this bottleneck by proposing a superimposed symbol scheme, which superimposes sensing pilots onto data symbols over the same time-frequency resources.","Specifically, we develop a structure-aware sparse Bayesian learning framework, where decoded data symbols serve as side information to enhance sensing performance and increase SE.","To meet the low-latency requirements of emerging ISAC applications, we further propose a low-complexity simultaneous communication and localization algorithm for multiple users.","This algorithm employs the unitary approximate message passing in the Bayesian learning framework for initial angle estimate, followed by iterative refinements through reduced-dimension matrix calculations.","Moreover, the sparse code multiple access technology is incorporated into this iterative framework for accurate data detection which also facilitates localization.","Numerical results show that the proposed superimposed symbol-based scheme empowered by the developed algorithm can achieve centimeter-level localization while attaining up to $96\\%$ of the SE of conventional communications without sensing capabilities.","Moreover, compared to other typical ISAC schemes, the proposed superimposed symbol scheme can provide an effective throughput improvement over $133\\%$."],"url":"http://arxiv.org/abs/2402.10593v1","category":"cs.IT"}
{"created":"2024-02-16 11:20:30","title":"Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs","abstract":"With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset will be available at [TBA].","sentences":["With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred.","This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures.","Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans.","Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains.","Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples.","This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns.","The code and dataset will be available at [TBA]."],"url":"http://arxiv.org/abs/2402.10586v1","category":"cs.CL"}
{"created":"2024-02-16 11:09:16","title":"Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation","abstract":"Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular depth estimation as well as efficient multi-task uncertainty quantification. By implicitly leveraging the predictive uncertainties of the teacher, EMUFormer achieves new state-of-the-art results on Cityscapes and NYUv2 and additionally estimates high-quality predictive uncertainties for both tasks that are comparable or superior to a Deep Ensemble despite being an order of magnitude more efficient.","sentences":["Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive.","Many real-world applications are multi-modal in nature and hence benefit from multi-task learning.","In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable.","In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other.","Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately.","Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular depth estimation as well as efficient multi-task uncertainty quantification.","By implicitly leveraging the predictive uncertainties of the teacher, EMUFormer achieves new state-of-the-art results on Cityscapes and NYUv2 and additionally estimates high-quality predictive uncertainties for both tasks that are comparable or superior to a Deep Ensemble despite being an order of magnitude more efficient."],"url":"http://arxiv.org/abs/2402.10580v1","category":"cs.CV"}
{"created":"2024-02-16 11:09:07","title":"Conjugate points along spherical harmonics","abstract":"Utilizing structure constants, we present a version of the Misiolek criterion for identifying conjugate points. We propose an approach that enables us to locate these points along solutions of the quasi-geostrophic equations on the sphere $\\Sph^2$. We demonstrate that for any spherical harmonics $Y_{lm}$ with $1 \\leq |m| \\leq l$, except for $Y_{1\\pm1}$ and $Y_{2\\pm 1}$, conjugate points can be determined along the solution generated by the velocity field $e_{lm}=\\nabla^\\perp Y_{lm}$. Subsequently, we investigate the impact of the Coriolis force on the occurrence of conjugate points. Moreover, for any zonal flow generated by the velocity field $\\nabla^\\perp Y_{l_1~0}$, we demonstrate that varying the rotation rate can lead to the appearance of conjugate points along the corresponding solution, where $l_1 = 2k+1. \\in \\mathbb{N}$ Additionally, we prove the existence of conjugate points along (complex) Rossby-Haurwitz waves and explore the effect of the Coriolis force on their stability.","sentences":["Utilizing structure constants, we present a version of the Misiolek criterion for identifying conjugate points.","We propose an approach that enables us to locate these points along solutions of the quasi-geostrophic equations on the sphere $\\Sph^2$. We demonstrate that for any spherical harmonics $Y_{lm}$ with $1 \\leq |m| \\leq l$, except for $Y_{1\\pm1}$ and $Y_{2\\pm 1}$, conjugate points can be determined along the solution generated by the velocity field $e_{lm}=\\nabla^\\perp Y_{lm}$. Subsequently, we investigate the impact of the Coriolis force on the occurrence of conjugate points.","Moreover, for any zonal flow generated by the velocity field $\\nabla^\\perp Y_{l_1~0}$, we demonstrate that varying the rotation rate can lead to the appearance of conjugate points along the corresponding solution, where $l_1 = 2k+1.","\\in \\mathbb{N}$","Additionally, we prove the existence of conjugate points along (complex) Rossby-Haurwitz waves and explore the effect of the Coriolis force on their stability."],"url":"http://arxiv.org/abs/2402.10578v1","category":"math.DG"}
{"created":"2024-02-16 11:04:31","title":"Symbolic Autoencoding for Self-Supervised Sequence Learning","abstract":"Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \\textit{symbolic autoencoding} ($\\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\\Sigma$AE significantly enhances performance on transduction tasks, even with minimal parallel data, offering a promising solution for weakly supervised learning scenarios.","sentences":["Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce.","Addressing this issue, we introduce \\textit{symbolic autoencoding} ($\\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data.","$\\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence.","We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck.","Our results demonstrate that $\\Sigma$AE significantly enhances performance on transduction tasks, even with minimal parallel data, offering a promising solution for weakly supervised learning scenarios."],"url":"http://arxiv.org/abs/2402.10575v1","category":"cs.LG"}
{"created":"2024-02-16 10:57:34","title":"Laser-Dressed States on Riemannian Manifolds: A Generalization of the Kramers-Henneberger Transformation","abstract":"Quantum particles under geometric constraints are sensitive to the geometry and topology of the underlying space. We analytically study the laser-driven nonlinear dynamics of a quantum particle whose motion is constrained to a two-dimensional Riemannian manifold embedded in a three-dimensional hyperspace. The geometry of space results in a potential-like term that supports bound states on the manifold. In the presence of a laser field, we derive expressions for a generalized Kramers-Henneberger-type unitary transformation which is shown to be generally space- and time-dependent, and deduce a Schr\\\"odinger-like equation in the Kramers-Henneberger frame. Compared to a flat (geometrically trivial) space, new time-averaged coefficients of differential operators and operator-valued perturbation terms appear which determine the geometry-dependent laser-dressed states on Riemannian manifolds.","sentences":["Quantum particles under geometric constraints are sensitive to the geometry and topology of the underlying space.","We analytically study the laser-driven nonlinear dynamics of a quantum particle whose motion is constrained to a two-dimensional Riemannian manifold embedded in a three-dimensional hyperspace.","The geometry of space results in a potential-like term that supports bound states on the manifold.","In the presence of a laser field, we derive expressions for a generalized Kramers-Henneberger-type unitary transformation which is shown to be generally space- and time-dependent, and deduce a Schr\\\"odinger-like equation in the Kramers-Henneberger frame.","Compared to a flat (geometrically trivial) space, new time-averaged coefficients of differential operators and operator-valued perturbation terms appear which determine the geometry-dependent laser-dressed states on Riemannian manifolds."],"url":"http://arxiv.org/abs/2402.10572v1","category":"quant-ph"}
{"created":"2024-02-16 10:55:38","title":"Direct Preference Optimization with an Offset","abstract":"Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.","sentences":["Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning.","DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response.","However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content.","In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning.","Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value.","The offset is determined based on the extent to which one response is preferred over another.","Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited."],"url":"http://arxiv.org/abs/2402.10571v1","category":"cs.CL"}
{"created":"2024-02-16 10:55:32","title":"Optimisation--Based Coupling of Finite Element Model and Reduced Order Model for Computational Fluid Dynamics","abstract":"With the increased interest in complex problems, such as multiphysics and multiscale models, as well as real-time computations, there is a strong need for domain-decomposition (DD) segregated solvers and reduced-order models (ROMs). Segregated models decouple the subcomponents of the problems at hand and use already existing state-of-the-art numerical codes in each component. In this manuscript, starting with a DD algorithm on non-overlapping domains, we aim at the comparison of couplings of different discretisation models, such as Finite Element (FEM) and ROM for separate subcomponents. In particular, we consider an optimisation-based DD model on two non-overlapping subdomains where the coupling on the common interface is performed by introducing a control variable representing a normal flux. Gradient-based optimisation algorithms are used to construct an iterative procedure to fully decouple the subdomain state solutions as well as to locally generate ROMs on each subdomain. Then, we consider FEM or ROM discretisation models for each of the DD problem components, namely, the triplet state1-state2-control. We perform numerical tests on the backward-facing step Navier-Stokes problem to investigate the efficacy of the presented couplings in terms of optimisation iterations and relative errors.","sentences":["With the increased interest in complex problems, such as multiphysics and multiscale models, as well as real-time computations, there is a strong need for domain-decomposition (DD) segregated solvers and reduced-order models (ROMs).","Segregated models decouple the subcomponents of the problems at hand and use already existing state-of-the-art numerical codes in each component.","In this manuscript, starting with a DD algorithm on non-overlapping domains, we aim at the comparison of couplings of different discretisation models, such as Finite Element (FEM) and ROM for separate subcomponents.","In particular, we consider an optimisation-based DD model on two non-overlapping subdomains where the coupling on the common interface is performed by introducing a control variable representing a normal flux.","Gradient-based optimisation algorithms are used to construct an iterative procedure to fully decouple the subdomain state solutions as well as to locally generate ROMs on each subdomain.","Then, we consider FEM or ROM discretisation models for each of the DD problem components, namely, the triplet state1-state2-control.","We perform numerical tests on the backward-facing step Navier-Stokes problem to investigate the efficacy of the presented couplings in terms of optimisation iterations and relative errors."],"url":"http://arxiv.org/abs/2402.10570v1","category":"math.NA"}
{"created":"2024-02-16 10:55:09","title":"Asymptotic spectral properties and preconditioning of an approximated nonlocal Helmholtz equation with Caputo fractional Laplacian and variable coefficient wave number $\u03bc$","abstract":"The current study investigates the asymptotic spectral properties of a finite difference approximation of nonlocal Helmholtz equations with a Caputo fractional Laplacian and a variable coefficient wave number $\\mu$, as it occurs when considering a wave propagation in complex media, characterized by nonlocal interactions and spatially varying wave speeds. More specifically, by using tools from Toeplitz and generalized locally Toeplitz theory, the present research delves into the spectral analysis of nonpreconditioned and preconditioned matrix-sequences. We report numerical evidences supporting the theoretical findings. Finally, open problems and potential extensions in various directions are presented and briefly discussed.","sentences":["The current study investigates the asymptotic spectral properties of a finite difference approximation of nonlocal Helmholtz equations with a Caputo fractional Laplacian and a variable coefficient wave number $\\mu$, as it occurs when considering a wave propagation in complex media, characterized by nonlocal interactions and spatially varying wave speeds.","More specifically, by using tools from Toeplitz and generalized locally Toeplitz theory, the present research delves into the spectral analysis of nonpreconditioned and preconditioned matrix-sequences.","We report numerical evidences supporting the theoretical findings.","Finally, open problems and potential extensions in various directions are presented and briefly discussed."],"url":"http://arxiv.org/abs/2402.10569v1","category":"math.NA"}
{"created":"2024-02-16 10:54:31","title":"Examples and cofibrant generation of effective Kan fibrations","abstract":"We will show make two contributions to the theory of effective Kan fibrations, which are a more explicit version of the notion of a Kan fibration, a notion which plays a fundamental role in simplicial homotopy theory. We will show that simplicial Malcev algebras are effective Kan complexes and that the effective Kan fibrations can be seen as the right class in an algebraic weak factorisation system. In addition, we will introduce two strengthenings of the notion of an effective Kan fibration, the symmetric effective and degenerate-preferring Kan fibrations, and show that the previous results holds for these strengthenings as well.","sentences":["We will show make two contributions to the theory of effective Kan fibrations, which are a more explicit version of the notion of a Kan fibration, a notion which plays a fundamental role in simplicial homotopy theory.","We will show that simplicial Malcev algebras are effective Kan complexes and that the effective Kan fibrations can be seen as the right class in an algebraic weak factorisation system.","In addition, we will introduce two strengthenings of the notion of an effective Kan fibration, the symmetric effective and degenerate-preferring Kan fibrations, and show that the previous results holds for these strengthenings as well."],"url":"http://arxiv.org/abs/2402.10568v1","category":"math.AT"}
{"created":"2024-02-16 10:54:10","title":"InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?","abstract":"Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate that the proposed $LSS_{\\beta}$ metric can effectively determine the readiness of a model for safe usage in the legal sector. We also propose finetuning pipelines, utilising specialised legal datasets, as a potential method to mitigate bias and improve model safety. The finetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\\beta}$, improving their usability in the Indian legal domain. Our code is publicly released.","sentences":["Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries.","Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions.","In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved.","We present a novel metric, $\\beta$-weighted $\\textit{Legal Safety Score ($LSS_{\\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM.","We assess LLMs' safety by considering its performance in the $\\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society.","Task performance and fairness scores of LLaMA and LLaMA--2 models indicate that the proposed $LSS_{\\beta}$ metric can effectively determine the readiness of a model for safe usage in the legal sector.","We also propose finetuning pipelines, utilising specialised legal datasets, as a potential method to mitigate bias and improve model safety.","The finetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\\beta}$, improving their usability in the Indian legal domain.","Our code is publicly released."],"url":"http://arxiv.org/abs/2402.10567v1","category":"cs.CL"}
{"created":"2024-02-16 10:46:35","title":"Space-time-matter gravity as the origin of rotation in 4-D stationary and axisymmetric vacuum solutions","abstract":"The standard theory of General Relativity (GR) currently provides the most reliable description of all gravitational events in Astrophysics and Cosmology. However, current Astronomy allows measurements that contradict the predictions of GR in some gravitational scenarios such as the accelerated expansion of the Universe, the fast rotation of cluster of galaxies of the so strongly deflection of light by gravitational lenses. This has led the scientific community to propose modifications of the theory, and in particular to introduce the existence of dark matter and dark energy. One of these modified theories of gravitation proposes, from the same scheme that the geometrical theory of GR, a so called space-time-matter (STM) theory within a manifold of five dimensions where the resulting metric were able to measure adequately and adjusted to the current observations the most massive gravitational events. Here we show that the gravitational features of a 4D metric in the standard space-time can be understood as an induced effect provided by a geometrical generalization of GR into a 5D manifold. In particular the rotation corresponding to an stationary and axisymmetric 4D metric is explained as a manifestation of the existence of a new additional dimension of the manifold which can be related with the rest mass. Furthermore the equivalence between a description of gravity in 4 dimensions and the static vacuum solution constructed in a 5D manifold could allow us to obtain stationary and axially symmetric solutions not known so far. These solutions can provide metrics that lead to determine those small observational corrections that are currently related to the existence of dark matter, but that in reality could be gravitational effects well explained within a geometrical theory generalizing Einstein gravity.","sentences":["The standard theory of General Relativity (GR) currently provides the most reliable description of all gravitational events in Astrophysics and Cosmology.","However, current Astronomy allows measurements that contradict the predictions of GR in some gravitational scenarios such as the accelerated expansion of the Universe, the fast rotation of cluster of galaxies of the so strongly deflection of light by gravitational lenses.","This has led the scientific community to propose modifications of the theory, and in particular to introduce the existence of dark matter and dark energy.","One of these modified theories of gravitation proposes, from the same scheme that the geometrical theory of GR, a so called space-time-matter (STM) theory within a manifold of five dimensions where the resulting metric were able to measure adequately and adjusted to the current observations the most massive gravitational events.","Here we show that the gravitational features of a 4D metric in the standard space-time can be understood as an induced effect provided by a geometrical generalization of GR into a 5D manifold.","In particular the rotation corresponding to an stationary and axisymmetric 4D metric is explained as a manifestation of the existence of a new additional dimension of the manifold which can be related with the rest mass.","Furthermore the equivalence between a description of gravity in 4 dimensions and the static vacuum solution constructed in a 5D manifold could allow us to obtain stationary and axially symmetric solutions not known so far.","These solutions can provide metrics that lead to determine those small observational corrections that are currently related to the existence of dark matter, but that in reality could be gravitational effects well explained within a geometrical theory generalizing Einstein gravity."],"url":"http://arxiv.org/abs/2402.10560v1","category":"gr-qc"}
{"created":"2024-02-16 10:42:25","title":"Discovery of an exchange-only gate sequence for CNOT with record-low gate time using reinforcement learning","abstract":"Exchange-only quantum computation is a version of spin-based quantum computation that entirely avoids the difficulty of controlling individual spins by a magnetic field and instead functions by sequences of exchange pulses. The challenge for exchange-only quantum computation is to find short sequences that generate the required logical quantum gates. A reduction of the total gate time of such synthesized quantum gates can help to minimize the effects of decoherence and control errors during the gate operation and thus increase the total gate fidelity. We apply reinforcement learning to the optimization of exchange-gate sequences realizing the CNOT and CZ two-qubit gates which lend themselves to the construction of universal gate sets for quantum computation. We obtain a significant improvement regarding the total gate time compared to previously known results.","sentences":["Exchange-only quantum computation is a version of spin-based quantum computation that entirely avoids the difficulty of controlling individual spins by a magnetic field and instead functions by sequences of exchange pulses.","The challenge for exchange-only quantum computation is to find short sequences that generate the required logical quantum gates.","A reduction of the total gate time of such synthesized quantum gates can help to minimize the effects of decoherence and control errors during the gate operation and thus increase the total gate fidelity.","We apply reinforcement learning to the optimization of exchange-gate sequences realizing the CNOT and CZ two-qubit gates which lend themselves to the construction of universal gate sets for quantum computation.","We obtain a significant improvement regarding the total gate time compared to previously known results."],"url":"http://arxiv.org/abs/2402.10559v1","category":"quant-ph"}
{"created":"2024-02-16 10:40:38","title":"Neural paraphrasing by automatically crawled and aligned sentence pairs","abstract":"Paraphrasing is the task of re-writing an input text using other words, without altering the meaning of the original content. Conversational systems can exploit automatic paraphrasing to make the conversation more natural, e.g., talking about a certain topic using different paraphrases in different time instants. Recently, the task of automatically generating paraphrases has been approached in the context of Natural Language Generation (NLG). While many existing systems simply consist in rule-based models, the recent success of the Deep Neural Networks in several NLG tasks naturally suggests the possibility of exploiting such networks for generating paraphrases. However, the main obstacle toward neural-network-based paraphrasing is the lack of large datasets with aligned pairs of sentences and paraphrases, that are needed to efficiently train the neural models. In this paper we present a method for the automatic generation of large aligned corpora, that is based on the assumption that news and blog websites talk about the same events using different narrative styles. We propose a similarity search procedure with linguistic constraints that, given a reference sentence, is able to locate the most similar candidate paraphrases out from millions of indexed sentences. The data generation process is evaluated in the case of the Italian language, performing experiments using pointer-based deep neural architectures.","sentences":["Paraphrasing is the task of re-writing an input text using other words, without altering the meaning of the original content.","Conversational systems can exploit automatic paraphrasing to make the conversation more natural, e.g., talking about a certain topic using different paraphrases in different time instants.","Recently, the task of automatically generating paraphrases has been approached in the context of Natural Language Generation (NLG).","While many existing systems simply consist in rule-based models, the recent success of the Deep Neural Networks in several NLG tasks naturally suggests the possibility of exploiting such networks for generating paraphrases.","However, the main obstacle toward neural-network-based paraphrasing is the lack of large datasets with aligned pairs of sentences and paraphrases, that are needed to efficiently train the neural models.","In this paper we present a method for the automatic generation of large aligned corpora, that is based on the assumption that news and blog websites talk about the same events using different narrative styles.","We propose a similarity search procedure with linguistic constraints that, given a reference sentence, is able to locate the most similar candidate paraphrases out from millions of indexed sentences.","The data generation process is evaluated in the case of the Italian language, performing experiments using pointer-based deep neural architectures."],"url":"http://arxiv.org/abs/2402.10558v1","category":"cs.CL"}
{"created":"2024-02-16 10:38:23","title":"On the spectrum of generalized H-join operation constrained by indexing maps -- I","abstract":"Fix $m \\in \\mathbb N$. A new generalization of the $H$-join operation of a family of graphs $\\{G_1, G_2, \\dots, G_k\\}$ constrained by indexing maps $I_1,I_2,\\dots,I_k$ is introduced as $H_m$-join of graphs, where the maps $I_i:V(G_i)$ to $[m]$. Various spectra, including adjacency, Laplacian, and signless Laplacian spectra, of any graph $G$, which is a $H_m$-join of graphs is obtained by introducing the concept of $E$-main eigenvalues. More precisely, we deduce that in the case of adjacency spectra, there is an associated matrix $E_i$ of the graph $G_i$ such that a $E_i$-non-main eigenvalue of multiplicity $m_i$ of $A(G_i)$ carry forward as an eigenvalue for $A(G)$ with the same multiplicity $m_i$, while an $E_i$-main eigenvalue of multiplicity $m_i$ carry forward as an eigenvalue of $G$ with multiplicity at least $m_i - m$. As a corollary, the universal adjacency spectra of some families of graphs is obtained by realizing them as $H_m$-joins of graphs. As an application, infinite families of cospectral families of graphs are found.","sentences":["Fix $m \\in \\mathbb N$.","A new generalization of the $H$-join operation of a family of graphs $\\{G_1, G_2, \\dots, G_k\\}$ constrained by indexing maps $I_1,I_2,\\dots,I_k$ is introduced as $H_m$-join of graphs, where the maps $I_i:V(G_i)$ to $[m]$. Various spectra, including adjacency, Laplacian, and signless Laplacian spectra, of any graph $G$, which is a $H_m$-join of graphs is obtained by introducing the concept of $E$-main eigenvalues.","More precisely, we deduce that in the case of adjacency spectra, there is an associated matrix $E_i$ of the graph $G_i$ such that a $E_i$-non-main eigenvalue of multiplicity $m_i$ of $A(G_i)$ carry forward as an eigenvalue for $A(G)$ with the same multiplicity $m_i$, while an $E_i$-main eigenvalue of multiplicity $m_i$ carry forward as an eigenvalue of $G$ with multiplicity at least $m_i - m$. As a corollary, the universal adjacency spectra of some families of graphs is obtained by realizing them as $H_m$-joins of graphs.","As an application, infinite families of cospectral families of graphs are found."],"url":"http://arxiv.org/abs/2402.10557v1","category":"math.CO"}
{"created":"2024-02-16 10:23:58","title":"High-order reliable numerical methods for epidemic models with non-constant recruitment rate","abstract":"The mathematical modeling of the propagation of illnesses has an important role from both mathematical and biological points of view. In this article, we observe an SEIR-type model with a general incidence rate and a non-constant recruitment rate function. First, we observe the qualitative properties of different methods: first-order and higher-order strong stability preserving Runge-Kutta methods \\cite{shu}. We give different conditions under which the numerical schemes behave as expected. Then, the theoretical results are demonstrated by some numerical experiments. \\keywords{positivity preservation, general SEIR model, SSP Runge-Kutta methods}","sentences":["The mathematical modeling of the propagation of illnesses has an important role from both mathematical and biological points of view.","In this article, we observe an SEIR-type model with a general incidence rate and a non-constant recruitment rate function.","First, we observe the qualitative properties of different methods: first-order and higher-order strong stability preserving Runge-Kutta methods \\cite{shu}.","We give different conditions under which the numerical schemes behave as expected.","Then, the theoretical results are demonstrated by some numerical experiments.","\\keywords{positivity preservation, general SEIR model, SSP Runge-Kutta methods}"],"url":"http://arxiv.org/abs/2402.10549v1","category":"math.NA"}
{"created":"2024-02-16 10:11:20","title":"Strong hallucinations from negation and how to fix them","abstract":"Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \\textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \\textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.","sentences":["Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence.","We call such responses \\textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations.","Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \\textit{an operation over an LM's latent representations that constrains how they may evolve}.","We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data."],"url":"http://arxiv.org/abs/2402.10543v1","category":"cs.CL"}
{"created":"2024-02-16 10:10:39","title":"WDVV-based recursion for open Gromov-Witten invariants","abstract":"We give a computability result for open Gromov-Witten invariants based on open WDVV equations. This is analogous to the result of Kontsevich-Manin for closed Gromov-Witten invariants. For greater generality, we base the argument on a formal object, the Frobenius superpotential, that generalizes several different definitions of open Gromov-Witten invariants. As an application, we show that open Gromov-Witten invariants for some products of projective spaces vanish.","sentences":["We give a computability result for open Gromov-Witten invariants based on open WDVV equations.","This is analogous to the result of Kontsevich-Manin for closed Gromov-Witten invariants.","For greater generality, we base the argument on a formal object, the Frobenius superpotential, that generalizes several different definitions of open Gromov-Witten invariants.","As an application, we show that open Gromov-Witten invariants for some products of projective spaces vanish."],"url":"http://arxiv.org/abs/2402.10542v1","category":"math.SG"}
{"created":"2024-02-16 10:01:06","title":"Cousin complexes in motivic homotopy theory","abstract":"We investigate Cousin (bi-)complexes in the setting of motives. Over essentially smooth local schemes, the columns of the Cousin bicomplex with coefficients in any stable motivic homotopy type are shown to be acyclic. On the other hand, we also construct a family of non-acyclic Cousin complexes over any positive dimensional base scheme. Our method of proof employs the notion of extended compactified framed correspondences.   Three major motivations for this study are to further our understanding of strict homotopy invariance, motivic infinite loop spaces, and connectivity in stable motivic homotopy theory. As applications of our main results on motivic Cousin complexes, we generalize several fundamental results in these topics to finite dimensional base schemes.","sentences":["We investigate Cousin (bi-)complexes in the setting of motives.","Over essentially smooth local schemes, the columns of the Cousin bicomplex with coefficients in any stable motivic homotopy type are shown to be acyclic.","On the other hand, we also construct a family of non-acyclic Cousin complexes over any positive dimensional base scheme.","Our method of proof employs the notion of extended compactified framed correspondences.   ","Three major motivations for this study are to further our understanding of strict homotopy invariance, motivic infinite loop spaces, and connectivity in stable motivic homotopy theory.","As applications of our main results on motivic Cousin complexes, we generalize several fundamental results in these topics to finite dimensional base schemes."],"url":"http://arxiv.org/abs/2402.10541v1","category":"math.AG"}
{"created":"2024-02-16 09:46:44","title":"On a Question of J.D. Meiss Concerning Anti-integrability for Three-Dimensional Quadratic Diffeomorphisms","abstract":"Three-dimensional quadratic diffeomorphisms with quadratic inverse generically have five independent parameters. When some parameters approach infinity, the diffeomorphisms may exhibit the so-called anti-integrability in the traditional sense of Aubry and Abramovici. That is, the dynamics of the diffeomorphisms reduce to symbolic dynamics on finite number of symbols. However, the diffeomorphisms may reduce to quadratic correspondences when parameters approach infinity, and the traditional anti-integrable limit does not deal with this situation. Meiss asked what about an anti-integrable limit for it. A remarkable progress was achieved very recently by Meiss himself and his student Hampton [SIAM J. Appl. Dyn. Syst. 21 (2022), pp. 650--675]. Under some conditions, using the contraction mapping theorem, they showed there is a bijection between the anti-integrable states and the sequences of branches of a quadratic correspondence. They also showed that an anti-integrable state can be continued to a genuine orbit of the three-dimensional diffeomorphism. This paper aims to contribute the progress, by means of the implicit function theorem. We shall show that the bijection indeed is a topological conjugacy and establish the uniform hyperbolicity of the continued genuine orbits.","sentences":["Three-dimensional quadratic diffeomorphisms with quadratic inverse generically have five independent parameters.","When some parameters approach infinity, the diffeomorphisms may exhibit the so-called anti-integrability in the traditional sense of Aubry and Abramovici.","That is, the dynamics of the diffeomorphisms reduce to symbolic dynamics on finite number of symbols.","However, the diffeomorphisms may reduce to quadratic correspondences when parameters approach infinity, and the traditional anti-integrable limit does not deal with this situation.","Meiss asked what about an anti-integrable limit for it.","A remarkable progress was achieved very recently by Meiss himself and his student Hampton [SIAM J. Appl.","Dyn.","Syst. 21 (2022), pp.","650--675].","Under some conditions, using the contraction mapping theorem, they showed there is a bijection between the anti-integrable states and the sequences of branches of a quadratic correspondence.","They also showed that an anti-integrable state can be continued to a genuine orbit of the three-dimensional diffeomorphism.","This paper aims to contribute the progress, by means of the implicit function theorem.","We shall show that the bijection indeed is a topological conjugacy and establish the uniform hyperbolicity of the continued genuine orbits."],"url":"http://arxiv.org/abs/2402.10536v1","category":"math.DS"}
{"created":"2024-02-16 09:38:16","title":"APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding","abstract":"This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal convolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as SoundStream, Encodec, HiFi-Codec and AudioDec.","sentences":["This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs.","The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs.","It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism.","The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution.","This code is subsequently quantized by the quantizer.","Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform.","To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec.","To support low-latency streamable inference, we employ feed-forward layers and causal convolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio.","Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio.","At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as SoundStream, Encodec, HiFi-Codec and AudioDec."],"url":"http://arxiv.org/abs/2402.10533v1","category":"cs.SD"}
{"created":"2024-02-16 09:37:54","title":"Properties and Challenges of LLM-Generated Explanations","abstract":"The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations \"in the wild\", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.","sentences":["The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets.","However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs.","The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning.","As the pre-training corpus includes a large amount of human-written explanations \"in the wild\", we hypothesise that LLMs adopt common properties of human explanations.","By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading.","We discuss reasons and consequences of the properties' presence or absence.","In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system."],"url":"http://arxiv.org/abs/2402.10532v1","category":"cs.CL"}
{"created":"2024-02-16 09:34:29","title":"On the torsion in a group $\\bf F/[M,N]$ in the case of combinatorial asphericity of groups $\\bf F/M$ and $\\bf F/N$","abstract":"Let $F$ be a non-Abelian free group with basis $A$, $M$ and $N$ be the normal closures of sets $R_M$ and $R_N$ of words in the alphabet $A^{\\pm 1}$. As is known, the group $F/[N, N]$ is torsion-free, but, in general, torsion in $F/[M, N]$ is possible.   In the paper of Hartley and Kuz'min (1991), it was proved that if $R_M=\\{v\\}$, $R_N=\\{w\\}$ and words $v$ and $w$ are not a proper power in $F$, then $F/[M,N]$ is torsion-free.   In the present paper a sufficient condition for the absence of torsion in $F/[M,N]$ is obtained, which allows to generalize the result of Hartley and Kuz'min to arbitrary words $v$ and $w$.","sentences":["Let $F$ be a non-Abelian free group with basis $A$, $M$ and $N$ be the normal closures of sets $R_M$ and $R_N$ of words in the alphabet $A^{\\pm 1}$.","As is known, the group $F/[N, N]$ is torsion-free, but, in general, torsion in $F/[M, N]$ is possible.   ","In the paper of Hartley and Kuz'min (1991), it was proved that if $R_M=\\{v\\}$, $R_N=\\{w\\}$ and words $v$ and $w$ are not a proper power in $F$, then $F/[M,N]$ is torsion-free.   ","In the present paper a sufficient condition for the absence of torsion in $F/[M,N]$ is obtained, which allows to generalize the result of Hartley and Kuz'min to arbitrary words $v$ and $w$."],"url":"http://arxiv.org/abs/2402.10531v1","category":"math.GR"}
{"created":"2024-02-16 09:32:39","title":"Strong collapsibility of the arc complexes of orientable and non-orientable crowns","abstract":"We prove that the arc complex of a polygon with a marked point in its interior is a strongly collapsible combinatorial ball. We also show that the arc complex of a M\\\"{o}bius strip, with finitely many marked points on its boundary, is a simplicially collapsible combinatorial ball but is not strongly collapsible.","sentences":["We prove that the arc complex of a polygon with a marked point in its interior is a strongly collapsible combinatorial ball.","We also show that the arc complex of a M\\\"{o}bius strip, with finitely many marked points on its boundary, is a simplicially collapsible combinatorial ball but is not strongly collapsible."],"url":"http://arxiv.org/abs/2402.10530v1","category":"math.GN"}
{"created":"2024-02-16 09:29:50","title":"Can We Verify Step by Step for Incorrect Answer Detection?","abstract":"Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of 5.1% increase in the F1 score across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy. Data and code are available at https://github.com/XinXU-USTC/R2PE.","sentences":["Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs).","Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance.","In addition, there has been research on assessing the quality of reasoning chains in CoT.","This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate?","To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains.","This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps.","To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin.","Concretely, this resulted in an average of 5.1% increase in the F1 score across all 45 subsets within R2PE.","We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy.","Data and code are available at https://github.com/XinXU-USTC/R2PE."],"url":"http://arxiv.org/abs/2402.10528v1","category":"cs.CL"}
{"created":"2024-02-16 09:19:11","title":"How People Prompt to Create Interactive VR Scenes","abstract":"Generative AI tools can provide people with the ability to create virtual environments and scenes with natural language prompts. Yet, how people will formulate such prompts is unclear -- particularly when they inhabit the environment that they are designing. For instance, it is likely that a person might say, \"Put a chair here\", while pointing at a location. If such linguistic features are common to people's prompts, we need to tune models to accommodate them. In this work, we present a wizard-of-oz elicitation study with 22 participants, where we studied people's implicit expectations when verbally prompting such programming agents to create interactive VR scenes. Our findings show that people prompt with several implicit expectations: (1) that agents have an embodied knowledge of the environment; (2) that agents understand embodied prompts by users; (3) that the agents can recall previous states of the scene and the conversation, and that (4) agents have a commonsense understanding of objects in the scene. Further, we found that participants prompt differently when they are prompting in situ (i.e. within the VR environment) versus ex situ (i.e. viewing the VR environment from the outside). To explore how our could be applied, we designed and built Oastaad, a conversational programming agent that allows non-programmers to design interactive VR experiences that they inhabit. Based on these explorations, we outline new opportunities and challenges for conversational programming agents that create VR environments.","sentences":["Generative AI tools can provide people with the ability to create virtual environments and scenes with natural language prompts.","Yet, how people will formulate such prompts is unclear -- particularly when they inhabit the environment that they are designing.","For instance, it is likely that a person might say, \"Put a chair here\", while pointing at a location.","If such linguistic features are common to people's prompts, we need to tune models to accommodate them.","In this work, we present a wizard-of-oz elicitation study with 22 participants, where we studied people's implicit expectations when verbally prompting such programming agents to create interactive VR scenes.","Our findings show that people prompt with several implicit expectations: (1) that agents have an embodied knowledge of the environment; (2) that agents understand embodied prompts by users; (3) that the agents can recall previous states of the scene and the conversation, and that (4) agents have a commonsense understanding of objects in the scene.","Further, we found that participants prompt differently when they are prompting in situ (i.e. within the VR environment) versus ex situ (i.e. viewing the VR environment from the outside).","To explore how our could be applied, we designed and built Oastaad, a conversational programming agent that allows non-programmers to design interactive VR experiences that they inhabit.","Based on these explorations, we outline new opportunities and challenges for conversational programming agents that create VR environments."],"url":"http://arxiv.org/abs/2402.10525v1","category":"cs.HC"}
{"created":"2024-02-16 09:14:49","title":"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models","abstract":"Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.","sentences":["Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs).","However, analyzing the results from this evaluation approach raises scalability and interpretability challenges.","In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation.","The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different.","We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company.","This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models."],"url":"http://arxiv.org/abs/2402.10524v1","category":"cs.HC"}
{"created":"2024-02-16 09:08:35","title":"Nonlinear optics driven magnetism reorientation in semiconductors","abstract":"Based on nonlinear optics, we develop a band theory to elucidate how light could manipulate magnetization, which is rooted by the quantum geometric structure and topological nature of electronic wavefunctions. Their existence are determined by the light polarization and specific material symmetry, based on the magnetic group theory. In general, both circularly and linearly polarized light could exert an effective magnetic field and torque effect, to reorient the magnetization. They are contributed by spin and orbital angular momenta simultaneously. Aided by group theory and first-principles calculations, we illustrate this theory using a showcase example of monolayer NiCl2, showing that light irradiation effectively generates an out-of-plane effective magnetic torque, which lifts its in-plane easy magnetization. According to magnetic dynamic simulations, the in-plane magnetization could be switched to the out-of-plane direction in a few nanoseconds under a modest light intensity, demonstrating its ultrafast nature desirable for quantum manipulation.","sentences":["Based on nonlinear optics, we develop a band theory to elucidate how light could manipulate magnetization, which is rooted by the quantum geometric structure and topological nature of electronic wavefunctions.","Their existence are determined by the light polarization and specific material symmetry, based on the magnetic group theory.","In general, both circularly and linearly polarized light could exert an effective magnetic field and torque effect, to reorient the magnetization.","They are contributed by spin and orbital angular momenta simultaneously.","Aided by group theory and first-principles calculations, we illustrate this theory using a showcase example of monolayer NiCl2, showing that light irradiation effectively generates an out-of-plane effective magnetic torque, which lifts its in-plane easy magnetization.","According to magnetic dynamic simulations, the in-plane magnetization could be switched to the out-of-plane direction in a few nanoseconds under a modest light intensity, demonstrating its ultrafast nature desirable for quantum manipulation."],"url":"http://arxiv.org/abs/2402.10518v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-16 09:05:02","title":"Generative AI for Controllable Protein Sequence Design: A Survey","abstract":"The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finally, we identify the unresolved challenges and highlight research opportunities that merit deeper exploration.","sentences":["The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering.","However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints.","This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution.","In this survey, we systematically review recent advances in generative AI for controllable protein sequence design.","To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms.","We then offer in-depth reviews of each design task and discuss the pertinent applications.","Finally, we identify the unresolved challenges and highlight research opportunities that merit deeper exploration."],"url":"http://arxiv.org/abs/2402.10516v1","category":"q-bio.BM"}
{"created":"2024-02-16 09:04:04","title":"Power-Efficient Indoor Localization Using Adaptive Channel-aware Ultra-wideband DL-TDOA","abstract":"Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink communication or centralized computation makes downlink time-difference-of-arrival (DL-TDOA) localization the most suitable for large-scale industrial deployments. However, temporary or permanent obstacles in the deployment region often lead to non-line-of-sight (NLOS) channel path and signal outage effects, which result in localization errors. Prior research has addressed this problem by increasing the ranging frequency, which leads to a heavy increase in the user device power consumption. It also does not contribute to any increase in localization accuracy under line-of-sight (LOS) conditions. In this paper, we propose and implement a novel low-power channel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS probability predictor based on a convolutional neural network (CNN), a dynamic ranging frequency control module, and an IMU sensor-based ranging filter. Based on the conducted experiments, we show that the proposed algorithm achieves 50% higher accuracy in NLOS conditions while having 46% lower power consumption in LOS conditions compared to baseline methods from prior research.","sentences":["Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink communication or centralized computation makes downlink time-difference-of-arrival (DL-TDOA) localization the most suitable for large-scale industrial deployments.","However, temporary or permanent obstacles in the deployment region often lead to non-line-of-sight (NLOS) channel path and signal outage effects, which result in localization errors.","Prior research has addressed this problem by increasing the ranging frequency, which leads to a heavy increase in the user device power consumption.","It also does not contribute to any increase in localization accuracy under line-of-sight (LOS) conditions.","In this paper, we propose and implement a novel low-power channel-aware dynamic frequency DL-TDOA ranging algorithm.","It comprises NLOS probability predictor based on a convolutional neural network (CNN), a dynamic ranging frequency control module, and an IMU sensor-based ranging filter.","Based on the conducted experiments, we show that the proposed algorithm achieves 50% higher accuracy in NLOS conditions while having 46% lower power consumption in LOS conditions compared to baseline methods from prior research."],"url":"http://arxiv.org/abs/2402.10515v1","category":"eess.SP"}
{"created":"2024-02-16 08:59:17","title":"Understanding Delays in AF\\_XDP-based Applications","abstract":"Packet processing on Linux can be slow due to its complex network stack. To solve this problem, there are two main solutions: eXpress Data Path (XDP) and Data Plane Development Kit (DPDK). XDP and the AF XDP socket offer full interoperability with the legacy system and is being adopted by major internet players like Open vSwitch or Facebook. While the performance evaluation of AF XDP against the legacy protocol stack in the kernel or against DPDK has been studied in the literature, the impact of the multiple socket parameters and the system configuration on its latency has been left aside. To address this, we conduct an experimental study to understand the XDP/AF XDP ecosystem and detect microseconds delays to better architect future latency-sensitive applications. Since the performance of AF XDP depends on multiple parameters found in different layers, finding the configuration minimizing its latency is a challenging task. We rely on a classification algorithm to group the performance results, allowing us to easily identify parameters with the biggest impact on performance at different loads. Last, but not least, we show that some configurations can significantly decrease the benefits of AF XDP, leading to undesirable behaviors, while other configurations are able to reduce such round trip delays to an impressive value of 6.5 $\\mu$s in the best case, including the tracing overhead. In summary, AF XDP is a promising solution, and careful selection of both application and socket parameters can significantly improve performance.","sentences":["Packet processing on Linux can be slow due to its complex network stack.","To solve this problem, there are two main solutions: eXpress Data Path (XDP) and Data Plane Development Kit (DPDK).","XDP and the AF XDP socket offer full interoperability with the legacy system and is being adopted by major internet players like Open vSwitch or Facebook.","While the performance evaluation of AF XDP against the legacy protocol stack in the kernel or against DPDK has been studied in the literature, the impact of the multiple socket parameters and the system configuration on its latency has been left aside.","To address this, we conduct an experimental study to understand the XDP/AF XDP ecosystem and detect microseconds delays to better architect future latency-sensitive applications.","Since the performance of AF XDP depends on multiple parameters found in different layers, finding the configuration minimizing its latency is a challenging task.","We rely on a classification algorithm to group the performance results, allowing us to easily identify parameters with the biggest impact on performance at different loads.","Last, but not least, we show that some configurations can significantly decrease the benefits of AF XDP, leading to undesirable behaviors, while other configurations are able to reduce such round trip delays to an impressive value of 6.5 $\\mu$s in the best case, including the tracing overhead.","In summary, AF XDP is a promising solution, and careful selection of both application and socket parameters can significantly improve performance."],"url":"http://arxiv.org/abs/2402.10513v1","category":"cs.NI"}
{"created":"2024-02-16 08:56:22","title":"Can Transformers Predict Vibrations?","abstract":"Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs). EVs often experience vibrations when driving on rough terrains, known as torsional resonance. This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle's drive shaft. However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection. In this study, we propose a novel approach to address this issue by introducing Resoformer, a transformer-based model for predicting torsional resonance. Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series. By calculating the attention between recursive and convolutional features extracted from the measured data points, Resoformer improves the accuracy of vibration forecasting. To evaluate the model, we use a vibration dataset called VIBES (Dataset for Forecasting Vibration Transition in EVs), consisting of 2,600 simulator-generated vibration sequences. Our experiments, conducted on strong baselines built on the VIBES dataset, demonstrate that Resoformer achieves state-of-the-art results. In conclusion, our study answers the question \"Can Transformers Forecast Vibrations?\" While traditional transformer architectures show low performance in forecasting torsional resonance waves, our findings indicate that combining recurrent neural network and temporal convolutional network using the transformer architecture improves the accuracy of long-term vibration forecasting.","sentences":["Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs).","EVs often experience vibrations when driving on rough terrains, known as torsional resonance.","This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle's drive shaft.","However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection.","In this study, we propose a novel approach to address this issue by introducing Resoformer, a transformer-based model for predicting torsional resonance.","Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series.","By calculating the attention between recursive and convolutional features extracted from the measured data points, Resoformer improves the accuracy of vibration forecasting.","To evaluate the model, we use a vibration dataset called VIBES (Dataset for Forecasting Vibration Transition in EVs), consisting of 2,600 simulator-generated vibration sequences.","Our experiments, conducted on strong baselines built on the VIBES dataset, demonstrate that Resoformer achieves state-of-the-art results.","In conclusion, our study answers the question \"Can Transformers Forecast Vibrations?\"","While traditional transformer architectures show low performance in forecasting torsional resonance waves, our findings indicate that combining recurrent neural network and temporal convolutional network using the transformer architecture improves the accuracy of long-term vibration forecasting."],"url":"http://arxiv.org/abs/2402.10511v1","category":"cs.LG"}
{"created":"2024-02-16 08:55:23","title":"Human Goal Recognition as Bayesian Inference: Investigating the Impact of Actions, Timing, and Goal Solvability","abstract":"Goal recognition is a fundamental cognitive process that enables individuals to infer intentions based on available cues. Current goal recognition algorithms often take only observed actions as input, but here we use a Bayesian framework to explore the role of actions, timing, and goal solvability in goal recognition. We analyze human responses to goal-recognition problems in the Sokoban domain, and find that actions are assigned most importance, but that timing and solvability also influence goal recognition in some cases, especially when actions are uninformative. We leverage these findings to develop a goal recognition model that matches human inferences more closely than do existing algorithms. Our work provides new insight into human goal recognition and takes a step towards more human-like AI models.","sentences":["Goal recognition is a fundamental cognitive process that enables individuals to infer intentions based on available cues.","Current goal recognition algorithms often take only observed actions as input, but here we use a Bayesian framework to explore the role of actions, timing, and goal solvability in goal recognition.","We analyze human responses to goal-recognition problems in the Sokoban domain, and find that actions are assigned most importance, but that timing and solvability also influence goal recognition in some cases, especially when actions are uninformative.","We leverage these findings to develop a goal recognition model that matches human inferences more closely than do existing algorithms.","Our work provides new insight into human goal recognition and takes a step towards more human-like AI models."],"url":"http://arxiv.org/abs/2402.10510v1","category":"cs.HC"}
{"created":"2024-02-16 08:21:43","title":"Late-time transition of $M_B$ inferred via neural networks","abstract":"The strengthening of tensions in the cosmological parameters has led to a reconsideration of fundamental aspects of standard cosmology. The tension in the Hubble constant can also be viewed as a tension between local and early Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In this work, we reconsider the possibility of a variation of this parameter in a model-independent way. We employ neural networks to agnostically constrain the value of the absolute magnitude as well as assess the impact and statistical significance of a variation in $M_B$ with redshift from the Pantheon+ compilation, together with a thorough analysis of the neural network architecture. We find an indication for a transition redshift at the $z\\approx 1$ region.","sentences":["The strengthening of tensions in the cosmological parameters has led to a reconsideration of fundamental aspects of standard cosmology.","The tension in the Hubble constant can also be viewed as a tension between local and early Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova.","In this work, we reconsider the possibility of a variation of this parameter in a model-independent way.","We employ neural networks to agnostically constrain the value of the absolute magnitude as well as assess the impact and statistical significance of a variation in $M_B$ with redshift from the Pantheon+ compilation, together with a thorough analysis of the neural network architecture.","We find an indication for a transition redshift at the $z\\approx 1$ region."],"url":"http://arxiv.org/abs/2402.10502v1","category":"astro-ph.CO"}
{"created":"2024-02-16 08:20:31","title":"NUTs, Bolts and Stokes Phenomena in the No-Boundary Wave Function","abstract":"In this note, we revisit and extend the analysis of the no-boundary wave function for the minisuperspace model in which the universe is described by a biaxial Bianchi IX metric. As matter content, we simply assume a positive cosmological constant. We find that two Stokes phenomena occur, at large squashing parameters of the spatial section of the universe. These Stokes phenomena eliminate potentially dominant Taub-Bolt-de Sitter saddle point geometries and are crucial for the consistency of the model. They also imply that phase transitions occur at certain levels of squashing, where NUT and Bolt saddle points exchange dominance.","sentences":["In this note, we revisit and extend the analysis of the no-boundary wave function for the minisuperspace model in which the universe is described by a biaxial Bianchi IX metric.","As matter content, we simply assume a positive cosmological constant.","We find that two Stokes phenomena occur, at large squashing parameters of the spatial section of the universe.","These Stokes phenomena eliminate potentially dominant Taub-Bolt-de Sitter saddle point geometries and are crucial for the consistency of the model.","They also imply that phase transitions occur at certain levels of squashing, where NUT and Bolt saddle points exchange dominance."],"url":"http://arxiv.org/abs/2402.10501v1","category":"gr-qc"}
{"created":"2024-02-16 08:19:34","title":"Provably Sample Efficient RLHF via Active Preference Optimization","abstract":"Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\\Omega(1)$ suboptimality gap in rewards. Then we propose $\\textit{Active Preference Optimization}$ ($\\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \\texttt{APO} achieves sample efficiency without compromising on policy performance. We show that given a sample budget of $T$, the suboptimality gap of a policy learned via $\\texttt{APO}$ scales as $O(1/\\sqrt{T})$. Next, we propose a compute-efficient batch version of $\\texttt{APO}$ with minor modification and evaluate its performance in practice. Experimental evaluations on a human preference dataset validate \\texttt{APO}'s efficacy as a sample-efficient and practical solution to data collection for RLHF, facilitating alignment of LLMs with human preferences in a cost-effective and scalable manner.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences.","While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF.","Hence better and adaptive strategies for data collection is needed.","To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\\Omega(1)$ suboptimality gap in rewards.","Then we propose $\\textit{Active Preference Optimization}$ ($\\texttt{APO}$), an algorithm that actively selects prompts to collect preference data.","Under the Bradley-Terry-Luce (BTL) preference model, \\texttt{APO} achieves sample efficiency without compromising on policy performance.","We show that given a sample budget of $T$, the suboptimality gap of a policy learned via $\\texttt{APO}$ scales as $O(1/\\sqrt{T})$. Next, we propose a compute-efficient batch version of $\\texttt{APO}$ with minor modification and evaluate its performance in practice.","Experimental evaluations on a human preference dataset validate \\texttt{APO}'s efficacy as a sample-efficient and practical solution to data collection for RLHF, facilitating alignment of LLMs with human preferences in a cost-effective and scalable manner."],"url":"http://arxiv.org/abs/2402.10500v1","category":"cs.LG"}
{"created":"2024-02-16 17:47:11","title":"Discovering and exploring cases of educational source code plagiarism with Dolos","abstract":"Source code plagiarism is a significant issue in educational practice, and educators need user-friendly tools to cope with such academic dishonesty. This article introduces the latest version of Dolos, a state-of-the-art ecosystem of tools for detecting and preventing plagiarism in educational source code. In this new version, the primary focus has been on enhancing the user experience. Educators can now run the entire plagiarism detection pipeline from a new web app in their browser, eliminating the need for any installation or configuration. Completely redesigned analytics dashboards provide an instant assessment of whether a collection of source files contains suspected cases of plagiarism and how widespread plagiarism is within the collection. The dashboards support hierarchically structured navigation to facilitate zooming in and out of suspect cases. Clusters are an essential new component of the dashboard design, reflecting the observation that plagiarism can occur among larger groups of students. To meet various user needs, the Dolos software stack for source code plagiarism detections now includes a web interface, a JSON application programming interface (API), a command line interface (CLI), a JavaScript library and a preconfigured Docker container. Clear documentation and a free-to-use instance of the web app can be found at https://dolos.ugent.be. The source code is also available on GitHub.","sentences":["Source code plagiarism is a significant issue in educational practice, and educators need user-friendly tools to cope with such academic dishonesty.","This article introduces the latest version of Dolos, a state-of-the-art ecosystem of tools for detecting and preventing plagiarism in educational source code.","In this new version, the primary focus has been on enhancing the user experience.","Educators can now run the entire plagiarism detection pipeline from a new web app in their browser, eliminating the need for any installation or configuration.","Completely redesigned analytics dashboards provide an instant assessment of whether a collection of source files contains suspected cases of plagiarism and how widespread plagiarism is within the collection.","The dashboards support hierarchically structured navigation to facilitate zooming in and out of suspect cases.","Clusters are an essential new component of the dashboard design, reflecting the observation that plagiarism can occur among larger groups of students.","To meet various user needs, the Dolos software stack for source code plagiarism detections now includes a web interface, a JSON application programming interface (API), a command line interface (CLI), a JavaScript library and a preconfigured Docker container.","Clear documentation and a free-to-use instance of the web app can be found at https://dolos.ugent.be.","The source code is also available on GitHub."],"url":"http://arxiv.org/abs/2402.10853v1","category":"cs.CY"}
{"created":"2024-02-16 17:06:45","title":"Understanding perturbative and non-perturbative contributions to jets at RHIC","abstract":"Jets, as collections of multi-scale objects, allow for insight into perturbative (high-momentum) processes, but gaining an understanding of the non-perturbative structure within jets such as hadronization effects and the underlying event has been more difficult. In observables sensitive to these non-perturbative effects, a large discrepancy is observed between partonic calculations and experimental data at RHIC. Grooming techniques such as SoftDrop reduce the impact of these non-perturbative effects and isolate the hard radiation associated with the hard scattering for more direct comparison to perturbative calculations. By analogy, CollinearDrop can be used to isolate non-perturbative contributions from a different region of the emission phase space, or the Lund plane, although no measurement of a CollinearDrop observable has yet been published. In these proceedings, we review recent progress on experimentally accessing the perturbative and non-perturbative contributions to jets and their substructure, and discuss potential future measurements at RHIC as a road map toward precision QCD at the EIC.","sentences":["Jets, as collections of multi-scale objects, allow for insight into perturbative (high-momentum) processes, but gaining an understanding of the non-perturbative structure within jets such as hadronization effects and the underlying event has been more difficult.","In observables sensitive to these non-perturbative effects, a large discrepancy is observed between partonic calculations and experimental data at RHIC.","Grooming techniques such as SoftDrop reduce the impact of these non-perturbative effects and isolate the hard radiation associated with the hard scattering for more direct comparison to perturbative calculations.","By analogy, CollinearDrop can be used to isolate non-perturbative contributions from a different region of the emission phase space, or the Lund plane, although no measurement of a CollinearDrop observable has yet been published.","In these proceedings, we review recent progress on experimentally accessing the perturbative and non-perturbative contributions to jets and their substructure, and discuss potential future measurements at RHIC as a road map toward precision QCD at the EIC."],"url":"http://arxiv.org/abs/2402.10832v1","category":"hep-ph"}
{"created":"2024-02-16 16:21:02","title":"BlackJAX: Composable Bayesian inference in JAX","abstract":"BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.","sentences":["BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation.","It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation.","BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs.","The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function.","BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use.","It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work."],"url":"http://arxiv.org/abs/2402.10797v1","category":"cs.MS"}
{"created":"2024-02-16 16:02:33","title":"A Condensed Transition Graph Framework for Zero-shot Link Prediction with Large Language Models","abstract":"Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically identifying relations between given entities. Existing methods primarily employ auxiliary information to predict tail entity given head entity and its relation, yet face challenges due to the occasional unavailability of such detailed information and the inherent simplicity of predicting tail entities based on semantic similarities. Even though Large Language Models (LLMs) offer a promising solution to predict unobserved relations between the head and tail entity in a zero-shot manner, their performance is still restricted due to the inability to leverage all the (exponentially many) paths' information between two entities, which are critical in collectively indicating their relation types. To address this, in this work, we introduce a Condensed Transition Graph Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths' information in linear time complexity to predict unseen relations between entities, attaining both efficiency and information preservation. Specifically, we design a condensed transition graph encoder with theoretical guarantees on its coverage, expressiveness, and efficiency. It is learned by a transition graph contrastive learning strategy. Subsequently, we design a soft instruction tuning to learn and map the all-path embedding to the input of LLMs. Experimental results show that our proposed CTLP method achieves state-of-the-art performance on three standard ZSLP datasets","sentences":["Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically identifying relations between given entities.","Existing methods primarily employ auxiliary information to predict tail entity given head entity and its relation, yet face challenges due to the occasional unavailability of such detailed information and the inherent simplicity of predicting tail entities based on semantic similarities.","Even though Large Language Models (LLMs) offer a promising solution to predict unobserved relations between the head and tail entity in a zero-shot manner, their performance is still restricted due to the inability to leverage all the (exponentially many) paths' information between two entities, which are critical in collectively indicating their relation types.","To address this, in this work, we introduce a Condensed Transition Graph Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths' information in linear time complexity to predict unseen relations between entities, attaining both efficiency and information preservation.","Specifically, we design a condensed transition graph encoder with theoretical guarantees on its coverage, expressiveness, and efficiency.","It is learned by a transition graph contrastive learning strategy.","Subsequently, we design a soft instruction tuning to learn and map the all-path embedding to the input of LLMs.","Experimental results show that our proposed CTLP method achieves state-of-the-art performance on three standard ZSLP datasets"],"url":"http://arxiv.org/abs/2402.10779v1","category":"cs.CL"}
{"created":"2024-02-16 14:52:05","title":"Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification","abstract":"The reasoning capabilities of LLMs are currently hotly debated. We examine the issue from the perspective of claim/rumour verification. We propose the first logical reasoning framework designed to break down any claim or rumor paired with evidence into the atomic reasoning steps necessary for verification. Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set stemming from rumours circulating on Twitter. We use them to evaluate the reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as ChatGPT) within the context of our framework, providing a thorough analysis. Our results show that ChatGPT struggles in abductive reasoning, although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated in order to distinguish between hype and actual capabilities, especially in high stake real-world tasks such as claim verification.","sentences":["The reasoning capabilities of LLMs are currently hotly debated.","We examine the issue from the perspective of claim/rumour verification.","We propose the first logical reasoning framework designed to break down any claim or rumor paired with evidence into the atomic reasoning steps necessary for verification.","Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set stemming from rumours circulating on Twitter.","We use them to evaluate the reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as ChatGPT) within the context of our framework, providing a thorough analysis.","Our results show that ChatGPT struggles in abductive reasoning, although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero Shot (ZS) and ZS CoT approaches.","Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated in order to distinguish between hype and actual capabilities, especially in high stake real-world tasks such as claim verification."],"url":"http://arxiv.org/abs/2402.10735v1","category":"cs.CL"}
{"created":"2024-02-16 14:51:08","title":"New Chinese Facilities for Short-Range Correlation Physics","abstract":"This article explores the significant advancements in Short-Range Correlation (SRC) research enabled by the latest Chinese nuclear physics facilities- CSR at HIRFL, HIAF, SHINE, and the upcoming EicC. These facilities introduce cutting-edge technologies and methodologies, addressing existing challenges and broadening the scope for SRC studies. By providing detailed insights into the capabilities and expected contributions of each facility, the paper highlights China's emerging role in the global nuclear physics landscape. The collaborative potential, alongside complementary global efforts, positions these facilities to deeply influence our understanding of nuclear matter's fundamental properties and interactions.","sentences":["This article explores the significant advancements in Short-Range Correlation (SRC) research enabled by the latest Chinese nuclear physics facilities- CSR at HIRFL, HIAF, SHINE, and the upcoming EicC.","These facilities introduce cutting-edge technologies and methodologies, addressing existing challenges and broadening the scope for SRC studies.","By providing detailed insights into the capabilities and expected contributions of each facility, the paper highlights China's emerging role in the global nuclear physics landscape.","The collaborative potential, alongside complementary global efforts, positions these facilities to deeply influence our understanding of nuclear matter's fundamental properties and interactions."],"url":"http://arxiv.org/abs/2402.10733v1","category":"nucl-ex"}
{"created":"2024-02-16 14:18:15","title":"Construction of weak solutions to a pressureless viscous model driven by nonlocal attraction-repulsion","abstract":"We analyze the pressureless Navier-Stokes system with nonlocal attraction-repulsion forces. Such systems appear in the context of models of collective behavior. We prove the existence of weak solutions on the whole space $\\mathbb{R}^3$ in the case of density-dependent degenerate viscosity. For the nonlocal term it is assumed that the interaction kernel has the quadratic growth at infinity and almost quadratic singularity at zero. Under these assumptions, we derive the analog of the Bresch-Desjardins and Mellet-Vasseur estimates for the nonlocal system. In particular, we are able to adapt the approach of Vasseur and Yu to construct a weak solution.","sentences":["We analyze the pressureless Navier-Stokes system with nonlocal attraction-repulsion forces.","Such systems appear in the context of models of collective behavior.","We prove the existence of weak solutions on the whole space $\\mathbb{R}^3$ in the case of density-dependent degenerate viscosity.","For the nonlocal term it is assumed that the interaction kernel has the quadratic growth at infinity and almost quadratic singularity at zero.","Under these assumptions, we derive the analog of the Bresch-Desjardins and Mellet-Vasseur estimates for the nonlocal system.","In particular, we are able to adapt the approach of Vasseur and Yu to construct a weak solution."],"url":"http://arxiv.org/abs/2402.10716v1","category":"math.AP"}
{"created":"2024-02-16 12:26:18","title":"Enhanced Long Wavelength Mermin-Wagner Fluctuations in Two-Dimensional Active Crystals and Glasses","abstract":"In the realm of two-dimensional (2D) systems, the renowned Mermin-Wagner effect plays a significant role, giving rise to striking dimensionality effects marked by far-reaching density fluctuations and the divergence of various dynamic properties. This effect also unequivocally negates the possibility of stable crystalline phases in 2D particulate systems characterized by continuous degrees of freedom. This effect has been recently discerned in glass-forming liquids, displaying characteristic signatures like the logarithmic divergence of mean squared displacement in the plateau regime. We explored these long-wavelength fluctuations in crystalline solids and in glass-forming liquids in the presence of non-equilibrium active forces. These systems are known in the literature as active crystals and glasses, and they can be thought of as a minimalistic model for understanding various non-equilibrium systems where the constituent particles dynamics are controlled by both temperature and other active forces, which can be external or internal. Such models often offer valuable insights into the dynamical behavior of biological systems, such as collections of cells, bacteria, ant colonies, or even synthetic self-propelled particles like Janus colloids. Our study reveals that fluctuations stemming from active forces get strongly coupled with long wavelength fluctuations arising from thermal effects, resulting in dramatic dynamical effects, particularly in 2D systems. We also shed light on how these fluctuations impact dynamical heterogeneity, a defining characteristic of glassy dynamics.","sentences":["In the realm of two-dimensional (2D) systems, the renowned Mermin-Wagner effect plays a significant role, giving rise to striking dimensionality effects marked by far-reaching density fluctuations and the divergence of various dynamic properties.","This effect also unequivocally negates the possibility of stable crystalline phases in 2D particulate systems characterized by continuous degrees of freedom.","This effect has been recently discerned in glass-forming liquids, displaying characteristic signatures like the logarithmic divergence of mean squared displacement in the plateau regime.","We explored these long-wavelength fluctuations in crystalline solids and in glass-forming liquids in the presence of non-equilibrium active forces.","These systems are known in the literature as active crystals and glasses, and they can be thought of as a minimalistic model for understanding various non-equilibrium systems where the constituent particles dynamics are controlled by both temperature and other active forces, which can be external or internal.","Such models often offer valuable insights into the dynamical behavior of biological systems, such as collections of cells, bacteria, ant colonies, or even synthetic self-propelled particles like Janus colloids.","Our study reveals that fluctuations stemming from active forces get strongly coupled with long wavelength fluctuations arising from thermal effects, resulting in dramatic dynamical effects, particularly in 2D systems.","We also shed light on how these fluctuations impact dynamical heterogeneity, a defining characteristic of glassy dynamics."],"url":"http://arxiv.org/abs/2402.10625v1","category":"cond-mat.soft"}
{"created":"2024-02-16 18:33:48","title":"Spin-Orbit Synchronization and Singular Perturbation Theory","abstract":"In this study, we formulate a set of differential equations for a binary system to describe the secular-tidal evolution of orbital elements, rotational dynamics, and deformation (flattening), under the assumption that one body remains spherical while the other is slightly aspherical throughout the analysis. By applying singular perturbation theory, we analyze the dynamics of both the original and secular equations. Our findings indicate that the secular equations serve as a robust approximation for the entire system, often representing a slow-fast dynamical system. Additionally, we explore the geometric aspects of spin-orbit resonance capture, interpreting it as a manifestation of relaxation oscillations within singularly perturbed systems.","sentences":["In this study, we formulate a set of differential equations for a binary system to describe the secular-tidal evolution of orbital elements, rotational dynamics, and deformation (flattening), under the assumption that one body remains spherical while the other is slightly aspherical throughout the analysis.","By applying singular perturbation theory, we analyze the dynamics of both the original and secular equations.","Our findings indicate that the secular equations serve as a robust approximation for the entire system, often representing a slow-fast dynamical system.","Additionally, we explore the geometric aspects of spin-orbit resonance capture, interpreting it as a manifestation of relaxation oscillations within singularly perturbed systems."],"url":"http://arxiv.org/abs/2402.10880v1","category":"astro-ph.EP"}
{"created":"2024-02-16 18:31:56","title":"Avoiding decoherence with giant atoms in a two-dimensional structured environment","abstract":"Giant atoms are quantum emitters that can couple to light at multiple discrete points. Such atoms have been shown to interact without decohering via a one-dimensional waveguide. Here, we study how giant atoms behave when coupled to a two-dimensional square lattice of coupled cavities, an environment characterized by a finite energy band and band gaps. In particular, we describe the role that bound states in the continuum (BICs) play in how giant atoms avoid decoherence. By developing numerical methods, we are able to investigate the dynamics of the system and show the appearance of interfering BICs within a single giant atom, as well as oscillating BICs between many giant atoms. In this way, we find the geometric arrangements of atomic coupling points that yield protection from decoherence in the two-dimensional lattice. These results on engineering the interaction between light and matter may find applications in quantum simulation and quantum information processing.","sentences":["Giant atoms are quantum emitters that can couple to light at multiple discrete points.","Such atoms have been shown to interact without decohering via a one-dimensional waveguide.","Here, we study how giant atoms behave when coupled to a two-dimensional square lattice of coupled cavities, an environment characterized by a finite energy band and band gaps.","In particular, we describe the role that bound states in the continuum (BICs) play in how giant atoms avoid decoherence.","By developing numerical methods, we are able to investigate the dynamics of the system and show the appearance of interfering BICs within a single giant atom, as well as oscillating BICs between many giant atoms.","In this way, we find the geometric arrangements of atomic coupling points that yield protection from decoherence in the two-dimensional lattice.","These results on engineering the interaction between light and matter may find applications in quantum simulation and quantum information processing."],"url":"http://arxiv.org/abs/2402.10879v1","category":"quant-ph"}
{"created":"2024-02-16 18:21:39","title":"Tidal Evolution and Spin-Orbit Dynamics: The Critical Role of Rheology","abstract":"This study analyzes secular dynamics using averaged equations that detail tidal effects on the motion of two extended bodies in Keplerian orbits. It introduces formulas for energy dissipation within each body of a binary system. The equations, particularly in contexts like the Sun-Mercury system, can be delineated into a fast-slow system. A significant contribution of this work is the demonstration of the crucial role complex rheological models play in the capture by spin-orbit resonances. This is particularly evident in the notable enlargement of the basin of attraction for Mercury's current state when transitioning from a single characteristic time rheology to a dual characteristic time model, under the constraint that both models comply with the same estimate of the complex Love number at orbital frequency. The study also underscores the importance of Mercury's elastic rigidity on secular timescales.","sentences":["This study analyzes secular dynamics using averaged equations that detail tidal effects on the motion of two extended bodies in Keplerian orbits.","It introduces formulas for energy dissipation within each body of a binary system.","The equations, particularly in contexts like the Sun-Mercury system, can be delineated into a fast-slow system.","A significant contribution of this work is the demonstration of the crucial role complex rheological models play in the capture by spin-orbit resonances.","This is particularly evident in the notable enlargement of the basin of attraction for Mercury's current state when transitioning from a single characteristic time rheology to a dual characteristic time model, under the constraint that both models comply with the same estimate of the complex Love number at orbital frequency.","The study also underscores the importance of Mercury's elastic rigidity on secular timescales."],"url":"http://arxiv.org/abs/2402.10875v1","category":"astro-ph.EP"}
{"created":"2024-02-16 18:20:33","title":"Design of 2D Skyrmionic Metamaterial Through Controlled Assembly","abstract":"Despite extensive research on magnetic skyrmions and antiskyrmions, a significant challenge remains in crafting nontrivial high-order skyrmionic textures with varying, or even tailor-made, topologies. We address this challenge, by focusing on a construction pathway of skyrmionics metamaterial within a monolayer thin film and suggest several promising lattice-like, flakes-like, and cell-like skyrmionic metamaterials that are surprisingly stable. Central to our approach is the concept of 'simulated controlled assembly', in short, a protocol inspired by 'click chemistry' that allows for positioning topological magnetic structures where one likes, and then allowing for energy minimization to elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium (Q=0). These entities serve as foundational 'skyrmionic building blocks' to forming reported intricate textures. In this work, two key contributions are introduced to the field of skyrmionic systems. First, we present a novel method for integrating control assembly protocols for the stabilization and investigation of topological magnets, which marks a significant advancement in the ability to explore new skyrmionic textures. Second, we report on the discovery of skyrmionic metamaterials, which shows a plethora of complex topologies that are possible to investigate theoretically and experimentally.","sentences":["Despite extensive research on magnetic skyrmions and antiskyrmions, a significant challenge remains in crafting nontrivial high-order skyrmionic textures with varying, or even tailor-made, topologies.","We address this challenge, by focusing on a construction pathway of skyrmionics metamaterial within a monolayer thin film and suggest several promising lattice-like, flakes-like, and cell-like skyrmionic metamaterials that are surprisingly stable.","Central to our approach is the concept of 'simulated controlled assembly', in short, a protocol inspired by 'click chemistry' that allows for positioning topological magnetic structures where one likes, and then allowing for energy minimization to elucidate the stability.","Utilizing high-throughput atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium (Q=0).","These entities serve as foundational 'skyrmionic building blocks' to forming reported intricate textures.","In this work, two key contributions are introduced to the field of skyrmionic systems.","First, we present a novel method for integrating control assembly protocols for the stabilization and investigation of topological magnets, which marks a significant advancement in the ability to explore new skyrmionic textures.","Second, we report on the discovery of skyrmionic metamaterials, which shows a plethora of complex topologies that are possible to investigate theoretically and experimentally."],"url":"http://arxiv.org/abs/2402.10874v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-16 18:13:35","title":"Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice","abstract":"Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.","sentences":["Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods.","However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings.","This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings.","We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment."],"url":"http://arxiv.org/abs/2402.10870v1","category":"cs.LG"}
{"created":"2024-02-16 18:00:52","title":"Estimating thresholds for asynchronous susceptible-infected-removed model on complex networks","abstract":"We use the pair heterogeneous mean-field (PHMF) approximation for an asynchronous version of the susceptible-infected-removed (SIR) model to estimate the epidemic thresholds on complex quenched networks. Our results indicate an improvement compared to the heuristic heterogeneous mean-field theory developed for one vertex (HMF) when the dynamic evolves on top random regular and power-law networks. However, there is a slight overestimation of the transition point for the later network type. We also analyze scaling for random regular networks near the thresholds. For this region, collapses were shown at the subcritical and supercritical phases.","sentences":["We use the pair heterogeneous mean-field (PHMF) approximation for an asynchronous version of the susceptible-infected-removed (SIR) model to estimate the epidemic thresholds on complex quenched networks.","Our results indicate an improvement compared to the heuristic heterogeneous mean-field theory developed for one vertex (HMF) when the dynamic evolves on top random regular and power-law networks.","However, there is a slight overestimation of the transition point for the later network type.","We also analyze scaling for random regular networks near the thresholds.","For this region, collapses were shown at the subcritical and supercritical phases."],"url":"http://arxiv.org/abs/2402.10863v1","category":"physics.soc-ph"}
{"created":"2024-02-16 17:52:46","title":"Transverse forces and glassy liquids in infinite dimensions","abstract":"We explore the dynamics of a simple liquid whose particles, in addition to standard potential-based interactions, are also subjected to transverse forces preserving the Boltzmann distribution. We derive the effective dynamics of one and two tracer particles in the infinite-dimensional limit. We determine the amount of acceleration of the dynamics caused by the transverse forces, in particular in the vicinity of the glass transition. We analyze the emergence and evolution of odd transport phenomena induced by the transverse forces.","sentences":["We explore the dynamics of a simple liquid whose particles, in addition to standard potential-based interactions, are also subjected to transverse forces preserving the Boltzmann distribution.","We derive the effective dynamics of one and two tracer particles in the infinite-dimensional limit.","We determine the amount of acceleration of the dynamics caused by the transverse forces, in particular in the vicinity of the glass transition.","We analyze the emergence and evolution of odd transport phenomena induced by the transverse forces."],"url":"http://arxiv.org/abs/2402.10856v1","category":"cond-mat.soft"}
{"created":"2024-02-16 17:44:11","title":"HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images","abstract":"Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric.","sentences":["Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues.","However, large histology slides with numerous microscopic fields pose challenges for visual search.","To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions.","This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application.","The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies.","The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis.","Experimental results show that the proposed model outperforms traditional methods in terms of accuracy and the mean Intersection-over-Union (mIoU) metric."],"url":"http://arxiv.org/abs/2402.10851v1","category":"eess.IV"}
{"created":"2024-02-16 17:43:54","title":"Error Checking for Sparse Systolic Tensor Arrays","abstract":"Structured sparsity is an efficient way to prune the complexity of modern Machine Learning (ML) applications and to simplify the handling of sparse data in hardware. In such cases, the acceleration of structured-sparse ML models is handled by sparse systolic tensor arrays. The increasing prevalence of ML in safety-critical systems requires enhancing the sparse tensor arrays with online error detection for managing random hardware failures. Algorithm-based fault tolerance has been proposed as a low-cost mechanism to check online the result of computations against random hardware failures. In this work, we address a key architectural challenge with structured-sparse tensor arrays: how to provide online error checking for a range of structured sparsity levels while maintaining high utilization of the hardware. Experimental results highlight the minimum hardware overhead incurred by the proposed checking logic and its error detection properties after injecting random hardware faults on sparse tensor arrays that execute layers of ResNet50 CNN.","sentences":["Structured sparsity is an efficient way to prune the complexity of modern Machine Learning (ML) applications and to simplify the handling of sparse data in hardware.","In such cases, the acceleration of structured-sparse ML models is handled by sparse systolic tensor arrays.","The increasing prevalence of ML in safety-critical systems requires enhancing the sparse tensor arrays with online error detection for managing random hardware failures.","Algorithm-based fault tolerance has been proposed as a low-cost mechanism to check online the result of computations against random hardware failures.","In this work, we address a key architectural challenge with structured-sparse tensor arrays: how to provide online error checking for a range of structured sparsity levels while maintaining high utilization of the hardware.","Experimental results highlight the minimum hardware overhead incurred by the proposed checking logic and its error detection properties after injecting random hardware faults on sparse tensor arrays that execute layers of ResNet50 CNN."],"url":"http://arxiv.org/abs/2402.10850v1","category":"cs.AR"}
{"created":"2024-02-16 17:42:47","title":"Fractional Spin Quantum Hall Effect in Weakly Coupled Spin Chain Arrays","abstract":"Topological magnetic insulators host chiral gapless edge modes. In the presence of strong interaction effects, the spin of these modes may fractionalize. Studying a 2D array of coupled insulating spin-1/2 chains, we show how spatially modulated magnetic fields and Dzyaloshinskii-Moriya interactions can be exploited to realize chiral spin liquids or integer and fractional spin quantum Hall effect phases. These are characterized by a gapped bulk spectrum and gapless chiral edge modes with fractional spin. The spin fractionalization is manifested in the quantized spin conductance, which can be used to probe the fractional spin quantum Hall effect. We analyze the system via bosonization and perturbative renormalization group techniques that allow us to identify the most relevant terms induced by the spin-spin interactions that open gaps and render the system topological under well-specified resonance conditions. We show explicitly that the emerging phase is a genuine chiral spin liquid. We suggest that the phases can be realized experimentally in synthetic spin chains and ultracold atom systems.","sentences":["Topological magnetic insulators host chiral gapless edge modes.","In the presence of strong interaction effects, the spin of these modes may fractionalize.","Studying a 2D array of coupled insulating spin-1/2 chains, we show how spatially modulated magnetic fields and Dzyaloshinskii-Moriya interactions can be exploited to realize chiral spin liquids or integer and fractional spin quantum Hall effect phases.","These are characterized by a gapped bulk spectrum and gapless chiral edge modes with fractional spin.","The spin fractionalization is manifested in the quantized spin conductance, which can be used to probe the fractional spin quantum Hall effect.","We analyze the system via bosonization and perturbative renormalization group techniques that allow us to identify the most relevant terms induced by the spin-spin interactions that open gaps and render the system topological under well-specified resonance conditions.","We show explicitly that the emerging phase is a genuine chiral spin liquid.","We suggest that the phases can be realized experimentally in synthetic spin chains and ultracold atom systems."],"url":"http://arxiv.org/abs/2402.10849v1","category":"cond-mat.str-el"}
{"created":"2024-02-16 17:09:49","title":"Agent-based Simulation Evaluation of CBD Tolling: A Case Study from New York City","abstract":"Congestion tollings have been widely developed and adopted as an effective tool to mitigate urban traffic congestion and enhance transportation system sustainability. Nevertheless, these tolling schemes are often tailored on a city-by-city or even area-by-area basis, and the cost of conducting field experiments often makes the design and evaluation process challenging. In this work, we leverage MATSim, a simulation platform that provides microscopic behaviors at the agent level, to evaluate performance on tolling schemes. Specifically, we conduct a case study of the Manhattan Central Business District (CBD) in New York City (NYC) using a fine-granularity traffic network model in the large-scale agent behavior setting. The flexibility of MATSim enables the implementation of a customized tolling policy proposed yet not deployed by the NYC agency while providing detailed interpretations. The quantitative and qualitative results indicate that the tested tolling program can regulate the personal vehicle volume in the CBD area and encourage the usage of public transportation, which proves to be a practical move towards sustainable transportation systems. More importantly, our work demonstrates that agent-based simulation helps better understand the travel pattern change subject to tollings in dense and complex urban environments, and it has the potential to facilitate efficient decision-making for the devotion to sustainable traffic management.","sentences":["Congestion tollings have been widely developed and adopted as an effective tool to mitigate urban traffic congestion and enhance transportation system sustainability.","Nevertheless, these tolling schemes are often tailored on a city-by-city or even area-by-area basis, and the cost of conducting field experiments often makes the design and evaluation process challenging.","In this work, we leverage MATSim, a simulation platform that provides microscopic behaviors at the agent level, to evaluate performance on tolling schemes.","Specifically, we conduct a case study of the Manhattan Central Business District (CBD) in New York City (NYC) using a fine-granularity traffic network model in the large-scale agent behavior setting.","The flexibility of MATSim enables the implementation of a customized tolling policy proposed yet not deployed by the NYC agency while providing detailed interpretations.","The quantitative and qualitative results indicate that the tested tolling program can regulate the personal vehicle volume in the CBD area and encourage the usage of public transportation, which proves to be a practical move towards sustainable transportation systems.","More importantly, our work demonstrates that agent-based simulation helps better understand the travel pattern change subject to tollings in dense and complex urban environments, and it has the potential to facilitate efficient decision-making for the devotion to sustainable traffic management."],"url":"http://arxiv.org/abs/2402.10834v1","category":"stat.AP"}
{"created":"2024-02-16 17:08:08","title":"Observation of the two-photon Landau-Zener-St\u00fcckelberg-Majorana effect","abstract":"Second-order processes introduce nonlinearities in quantum dynamics, unlocking a totally unexpected area of control operations. Here we show that the well-known Landau-Zener-St\\\"uckelberg-Majorana (LZSM) transition can be driven by a virtual process in a three-level system whereby two photons from a drive with linearly-modulated phase create excitations onto the third level while avoiding completely the first level. We implement this experimentally in a transmon qubit achieving a population transfer of $98\\%$, limited by relaxation. We predict and observe experimentally the doubling of the LZSM velocity. The observation of this effect is made possible by the nearly-exact cancellation of the two-photon ac Stark shift when the third transition is included. Furthermore, we demonstrate considerable robustness to offsets in frequency and amplitude, both in theory and experimentally.","sentences":["Second-order processes introduce nonlinearities in quantum dynamics, unlocking a totally unexpected area of control operations.","Here we show that the well-known Landau-Zener-St\\\"uckelberg-Majorana (LZSM) transition can be driven by a virtual process in a three-level system whereby two photons from a drive with linearly-modulated phase create excitations onto the third level while avoiding completely the first level.","We implement this experimentally in a transmon qubit achieving a population transfer of $98\\%$, limited by relaxation.","We predict and observe experimentally the doubling of the LZSM velocity.","The observation of this effect is made possible by the nearly-exact cancellation of the two-photon ac Stark shift when the third transition is included.","Furthermore, we demonstrate considerable robustness to offsets in frequency and amplitude, both in theory and experimentally."],"url":"http://arxiv.org/abs/2402.10833v1","category":"quant-ph"}
{"created":"2024-02-16 16:54:07","title":"Nash Equilibrium and Learning Dynamics in Three-Player Matching $m$-Action Games","abstract":"Learning in games discusses the processes where multiple players learn their optimal strategies through the repetition of game plays. The dynamics of learning between two players in zero-sum games, such as matching pennies, where their benefits are competitive, have already been well analyzed. However, it is still unexplored and challenging to analyze the dynamics of learning among three players. In this study, we formulate a minimalistic game where three players compete to match their actions with one another. Although interaction among three players diversifies and complicates the Nash equilibria, we fully analyze the equilibria. We also discuss the dynamics of learning based on some famous algorithms categorized into Follow the Regularized Leader. From both theoretical and experimental aspects, we characterize the dynamics by categorizing three-player interactions into three forces to synchronize their actions, switch their actions rotationally, and seek competition.","sentences":["Learning in games discusses the processes where multiple players learn their optimal strategies through the repetition of game plays.","The dynamics of learning between two players in zero-sum games, such as matching pennies, where their benefits are competitive, have already been well analyzed.","However, it is still unexplored and challenging to analyze the dynamics of learning among three players.","In this study, we formulate a minimalistic game where three players compete to match their actions with one another.","Although interaction among three players diversifies and complicates the Nash equilibria, we fully analyze the equilibria.","We also discuss the dynamics of learning based on some famous algorithms categorized into Follow the Regularized Leader.","From both theoretical and experimental aspects, we characterize the dynamics by categorizing three-player interactions into three forces to synchronize their actions, switch their actions rotationally, and seek competition."],"url":"http://arxiv.org/abs/2402.10825v1","category":"cs.GT"}
{"created":"2024-02-16 16:39:14","title":"Core Stability in Additively Separable Hedonic Games of Low Treewidth","abstract":"Additively Separable Hedonic Game (ASHG) are coalition-formation games where we are given a graph whose vertices represent $n$ selfish agents and the weight of each edge $uv$ denotes how much agent $u$ gains (or loses) when she is placed in the same coalition as agent $v$. We revisit the computational complexity of the well-known notion of core stability of ASHGs, where the goal is to construct a partition of the agents into coalitions such that no group of agents would prefer to diverge from the given partition and form a new (blocking) coalition. Since both finding a core stable partition and verifying that a given partition is core stable are intractable problems ($\\Sigma_2^p$-complete and coNP-complete respectively) we study their complexity from the point of view of structural parameterized complexity, using standard graph-theoretic parameters, such as treewidth.","sentences":["Additively Separable Hedonic Game (ASHG) are coalition-formation games where we are given a graph whose vertices represent $n$ selfish agents and the weight of each edge $uv$ denotes how much agent $u$ gains (or loses) when she is placed in the same coalition as agent $v$. We revisit the computational complexity of the well-known notion of core stability of ASHGs, where the goal is to construct a partition of the agents into coalitions such that no group of agents would prefer to diverge from the given partition and form a new (blocking) coalition.","Since both finding a core stable partition and verifying that a given partition is core stable are intractable problems ($\\Sigma_2^p$-complete and coNP-complete respectively) we study their complexity from the point of view of structural parameterized complexity, using standard graph-theoretic parameters, such as treewidth."],"url":"http://arxiv.org/abs/2402.10815v1","category":"cs.DS"}
{"created":"2024-02-16 16:37:48","title":"Associative Memories in the Feature Space","abstract":"An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing \\emph{similarities} in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as CIFAR10 and STL10. An additional drawback of current models is the need of storing the whole dataset in the pixel space, which is often extremely large. We relax this condition and propose a class of memory models that only stores low-dimensional semantic embeddings, and uses them to retrieve similar, but not identical, memories. We demonstrate a proof of concept of this method on a simple task on the MNIST dataset.","sentences":["An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set.","However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator.","This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images.","This problem can be easily solved by computing \\emph{similarities} in an embedding space instead of the pixel space.","We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss.","As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores.","We test this method on complex datasets such as CIFAR10 and STL10.","An additional drawback of current models is the need of storing the whole dataset in the pixel space, which is often extremely large.","We relax this condition and propose a class of memory models that only stores low-dimensional semantic embeddings, and uses them to retrieve similar, but not identical, memories.","We demonstrate a proof of concept of this method on a simple task on the MNIST dataset."],"url":"http://arxiv.org/abs/2402.10814v1","category":"cs.LG"}
{"created":"2024-02-16 16:37:38","title":"Iterative embedding and reweighting of complex networks reveals community structure","abstract":"Graph embeddings learn the structure of networks and represent it in low-dimensional vector spaces. Community structure is one of the features that are recognized and reproduced by embeddings. We show that an iterative procedure, in which a graph is repeatedly embedded and its links are reweighted based on the geometric proximity between the nodes, reinforces intra-community links and weakens inter-community links, making the clusters of the initial network more visible and more easily detectable. The geometric separation between the communities can become so strong that even a very simple parsing of the links may recover the communities as isolated components with surprisingly high precision. Furthermore, when used as a pre-processing step, our embedding and reweighting procedure can improve the performance of traditional community detection algorithms.","sentences":["Graph embeddings learn the structure of networks and represent it in low-dimensional vector spaces.","Community structure is one of the features that are recognized and reproduced by embeddings.","We show that an iterative procedure, in which a graph is repeatedly embedded and its links are reweighted based on the geometric proximity between the nodes, reinforces intra-community links and weakens inter-community links, making the clusters of the initial network more visible and more easily detectable.","The geometric separation between the communities can become so strong that even a very simple parsing of the links may recover the communities as isolated components with surprisingly high precision.","Furthermore, when used as a pre-processing step, our embedding and reweighting procedure can improve the performance of traditional community detection algorithms."],"url":"http://arxiv.org/abs/2402.10813v1","category":"physics.soc-ph"}
{"created":"2024-02-16 16:34:26","title":"Non-collinear first-principles studies of the spin-electric coupling in frustrated triangular molecular magnets","abstract":"Frustrated triangular molecular magnets (MMs) with anti-ferromagnetic ground states (GS) are an important class of magnetic systems with potential applications in quantum information processing. The two-fold degenerate GS of these molecules, characterized by spin chirality, can be utilized to encode qubits for quantum computing. Furthermore, because of the lack of inversion symmetry in these molecules, an electric field couples directly states of opposite chirality, allowing a very efficient and fast control of the qubits. In this work we present a theoretical method to calculate the spin-electric coupling for triangular MMs with effective {\\it local} spins $s$ larger than 1/2, which is amenable to a first-principles implementation based on density functional theory (DFT). In contrast to MMs where the net magnetization at the magnetic atoms is $\\mu_{\\rm B}/2$ ($\\mu_{\\rm B} $ is the Bohr magneton), the DFT treatment of frustrated triangular MMs with larger local magnetizations requires a fully non-collinear approach, which we have implemented in the NRLMOL DFT code. As an example, we have used these methods to evaluate the spin-electric coupling for a spin $s = 5/2$ $\\{\\mathrm{Fe_3}\\}$ triangular MM, where this effect has been observed experimentally for the first time quite recently. Our theoretical and computational methods will help elucidate and further guide ongoing experimental work in the field of quantum molecular spintronics.","sentences":["Frustrated triangular molecular magnets (MMs) with anti-ferromagnetic ground states (GS) are an important class of magnetic systems with potential applications in quantum information processing.","The two-fold degenerate GS of these molecules, characterized by spin chirality, can be utilized to encode qubits for quantum computing.","Furthermore, because of the lack of inversion symmetry in these molecules, an electric field couples directly states of opposite chirality, allowing a very efficient and fast control of the qubits.","In this work we present a theoretical method to calculate the spin-electric coupling for triangular MMs with effective {\\it local} spins $s$ larger than 1/2, which is amenable to a first-principles implementation based on density functional theory (DFT).","In contrast to MMs where the net magnetization at the magnetic atoms is $\\mu_{\\rm B}/2$ ($\\mu_{\\rm B} $ is the Bohr magneton), the DFT treatment of frustrated triangular MMs with larger local magnetizations requires a fully non-collinear approach, which we have implemented in the NRLMOL DFT code.","As an example, we have used these methods to evaluate the spin-electric coupling for a spin $s = 5/2$ $\\{\\mathrm{Fe_3}\\}$ triangular MM, where this effect has been observed experimentally for the first time quite recently.","Our theoretical and computational methods will help elucidate and further guide ongoing experimental work in the field of quantum molecular spintronics."],"url":"http://arxiv.org/abs/2402.10807v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-16 16:25:20","title":"TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models","abstract":"Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance of testing newly incoming unseen time series on current TSAD algorithms remains unknown. Lastly, although some papers have conducted detailed surveys, the absence of an online evaluation platform prevents answering questions like \"Who is the best at anomaly detection at the current stage?\" In this paper, we propose TimeSeriesBench, an industrial-grade benchmark that we continuously maintain as a leaderboard. On this leaderboard, we assess the performance of existing algorithms across more than 168 evaluation settings combining different training and testing paradigms, evaluation metrics and datasets. Through our comprehensive analysis of the results, we provide recommendations for the future design of anomaly detection algorithms. To address known issues with existing public datasets, we release an industrial dataset to the public together with TimeSeriesBench. All code, data, and the online leaderboard have been made publicly available.","sentences":["Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest.","However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems.","Firstly, current algorithms typically train a specific model for each individual time series.","In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical.","The performance of using merely one single unified model to detect anomalies remains unknown.","Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment.","In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily.","The performance of testing newly incoming unseen time series on current TSAD algorithms remains unknown.","Lastly, although some papers have conducted detailed surveys, the absence of an online evaluation platform prevents answering questions like \"Who is the best at anomaly detection at the current stage?\"","In this paper, we propose TimeSeriesBench, an industrial-grade benchmark that we continuously maintain as a leaderboard.","On this leaderboard, we assess the performance of existing algorithms across more than 168 evaluation settings combining different training and testing paradigms, evaluation metrics and datasets.","Through our comprehensive analysis of the results, we provide recommendations for the future design of anomaly detection algorithms.","To address known issues with existing public datasets, we release an industrial dataset to the public together with TimeSeriesBench.","All code, data, and the online leaderboard have been made publicly available."],"url":"http://arxiv.org/abs/2402.10802v1","category":"cs.LG"}
{"created":"2024-02-16 16:25:08","title":"Complexity Results and Active-Set Identification of a Derivative-Free Method for Bound-Constrained Problems","abstract":"In this paper, we analyze a derivative-free linesearch method designed for bound-constrained problems. Our analysis demonstrates that this method exhibits a worst-case complexity comparable to other derivative-free methods for unconstrained problems. In particular, we prove that the total number of iterations where the norm of the reduced gradient (vanishing at stationary points) exceeds a predefined threshold $\\epsilon$ can be bounded in the worst case by ${\\cal O}(\\epsilon^{-2})$. Moreover, we investigate the method capability to identify active constraints at the final solution. We show that, after a finite number of iterations, all the active constraints satisfying the strict complementarity condition are correctly identified.","sentences":["In this paper, we analyze a derivative-free linesearch method designed for bound-constrained problems.","Our analysis demonstrates that this method exhibits a worst-case complexity comparable to other derivative-free methods for unconstrained problems.","In particular, we prove that the total number of iterations where the norm of the reduced gradient (vanishing at stationary points) exceeds a predefined threshold $\\epsilon$ can be bounded in the worst case by ${\\cal O}(\\epsilon^{-2})$.","Moreover, we investigate the method capability to identify active constraints at the final solution.","We show that, after a finite number of iterations, all the active constraints satisfying the strict complementarity condition are correctly identified."],"url":"http://arxiv.org/abs/2402.10801v1","category":"math.OC"}
{"created":"2024-02-16 16:24:00","title":"A Second Look at the Impact of Passive Voice Requirements on Domain Modeling: Bayesian Reanalysis of an Experiment","abstract":"The quality of requirements specifications may impact subsequent, dependent software engineering (SE) activities. However, empirical evidence of this impact remains scarce and too often superficial as studies abstract from the phenomena under investigation too much. Two of these abstractions are caused by the lack of frameworks for causal inference and frequentist methods which reduce complex data to binary results. In this study, we aim to demonstrate (1) the use of a causal framework and (2) contrast frequentist methods with more sophisticated Bayesian statistics for causal inference. To this end, we reanalyze the only known controlled experiment investigating the impact of passive voice on the subsequent activity of domain modeling. We follow a framework for statistical causal inference and employ Bayesian data analysis methods to re-investigate the hypotheses of the original study. Our results reveal that the effects observed by the original authors turned out to be much less significant than previously assumed. This study supports the recent call to action in SE research to adopt Bayesian data analysis, including causal frameworks and Bayesian statistics, for more sophisticated causal inference.","sentences":["The quality of requirements specifications may impact subsequent, dependent software engineering (SE) activities.","However, empirical evidence of this impact remains scarce and too often superficial as studies abstract from the phenomena under investigation too much.","Two of these abstractions are caused by the lack of frameworks for causal inference and frequentist methods which reduce complex data to binary results.","In this study, we aim to demonstrate (1) the use of a causal framework and (2) contrast frequentist methods with more sophisticated Bayesian statistics for causal inference.","To this end, we reanalyze the only known controlled experiment investigating the impact of passive voice on the subsequent activity of domain modeling.","We follow a framework for statistical causal inference and employ Bayesian data analysis methods to re-investigate the hypotheses of the original study.","Our results reveal that the effects observed by the original authors turned out to be much less significant than previously assumed.","This study supports the recent call to action in SE research to adopt Bayesian data analysis, including causal frameworks and Bayesian statistics, for more sophisticated causal inference."],"url":"http://arxiv.org/abs/2402.10800v1","category":"cs.SE"}
{"created":"2024-02-16 16:17:50","title":"Discrete scaling in non-integer dimensions","abstract":"We explore the effect of a finite two-body energy in the discrete scale symmetry regime of two heavy bosonic impurities immersed in a light bosonic system. By means of the Born-Oppenheimer approximation in non-integer dimensions $(D)$, we discuss the effective potential of the heavy-particles Schrodinger equation. We study how including the two-body energy in the effective potential changes the light-particles wave function and the ratio between successive Efimov states. We present the limit cycles associated with correlation between the energy of successive levels for the three and four-body systems. Our study is exemplified by considering a system composed of $N$-bosons, namely two Rubidium atoms interacting with $N-2$ Lithium ones ($^7$Li$_{N-2}-^{87}$Rb$_2$), which represent compounds of current experimental interest.","sentences":["We explore the effect of a finite two-body energy in the discrete scale symmetry regime of two heavy bosonic impurities immersed in a light bosonic system.","By means of the Born-Oppenheimer approximation in non-integer dimensions $(D)$, we discuss the effective potential of the heavy-particles Schrodinger equation.","We study how including the two-body energy in the effective potential changes the light-particles wave function and the ratio between successive Efimov states.","We present the limit cycles associated with correlation between the energy of successive levels for the three and four-body systems.","Our study is exemplified by considering a system composed of $N$-bosons, namely two Rubidium atoms interacting with $N-2$ Lithium ones ($^7$Li$_{N-2}-^{87}$Rb$_2$), which represent compounds of current experimental interest."],"url":"http://arxiv.org/abs/2402.10792v1","category":"physics.atom-ph"}
{"created":"2024-02-16 16:12:08","title":"Hard confinement of a two-particle quantum system using the variational method","abstract":"The variational method is used to study the hard confinement of a two-particle quantum system in two potential models, the Cornell potential and the global potential, with Dirichlet-type boundary conditions at various cut-off radii. The trial wavefunction is constructed as the product of the $1S$ free hydrogen atom wavefunction or $1S$ free harmonic oscillator wavefunction times a cut-off function of the form $(r-z)$ to ensure hard entrapment within a sphere of radius $z$. The behavior of $|\\psi|^2$, the wavefunction at the origin (WFO), and the mean radius $\\ev{r}$ are computed for different situations and compared for the two potential models.","sentences":["The variational method is used to study the hard confinement of a two-particle quantum system in two potential models, the Cornell potential and the global potential, with Dirichlet-type boundary conditions at various cut-off radii.","The trial wavefunction is constructed as the product of the $1S$ free hydrogen atom wavefunction or $1S$ free harmonic oscillator wavefunction times a cut-off function of the form $(r-z)$ to ensure hard entrapment within a sphere of radius $z$. The behavior of $|\\psi|^2$, the wavefunction at the origin (WFO), and the mean radius $\\ev{r}$ are computed for different situations and compared for the two potential models."],"url":"http://arxiv.org/abs/2402.10788v1","category":"quant-ph"}
{"created":"2024-02-16 16:05:47","title":"On Permutation Selectors and their Applications in Ad-Hoc Radio Networks Protocols","abstract":"Selective families of sets, or selectors, are combinatorial tools used to \"isolate\" individual members of sets from some set family. Given a set $X$ and an element $x\\in X$, to isolate $x$ from $X$, at least one of the sets in the selector must intersect $X$ on exactly $x$. We study (k,N)-permutation selectors which have the property that they can isolate each element of each $k$-element subset of $\\{0,1,...,N-1\\}$ in each possible order. These selectors can be used in protocols for ad-hoc radio networks to more efficiently disseminate information along multiple hops. In 2004, Gasieniec, Radzik and Xin gave a construction of a (k,N)-permutation selector of size $O(k^2\\log^3 N)$. This paper improves this by providing a probabilistic construction of a (k,N)-permutation selector of size $O(k^2\\log N)$. Remarkably, this matches the asymptotic bound for standard strong (k,N)-selectors, that isolate each element of each set of size $k$, but with no restriction on the order. We then show that the use of our (k,N)-permutation selector improves the best running time for gossiping in ad-hoc radio networks by a poly-logarithmic factor.","sentences":["Selective families of sets, or selectors, are combinatorial tools used to \"isolate\" individual members of sets from some set family.","Given a set $X$ and an element $x\\in X$, to isolate $x$ from $X$, at least one of the sets in the selector must intersect $X$ on exactly $x$.","We study (k,N)-permutation selectors which have the property that they can isolate each element of each $k$-element subset of $\\{0,1,...,N-1\\}$ in each possible order.","These selectors can be used in protocols for ad-hoc radio networks to more efficiently disseminate information along multiple hops.","In 2004, Gasieniec, Radzik and Xin gave a construction of a (k,N)-permutation selector of size $","O(k^2\\log^3 N)$.","This paper improves this by providing a probabilistic construction of a (k,N)-permutation selector of size $O(k^2\\log N)$.","Remarkably, this matches the asymptotic bound for standard strong (k,N)-selectors, that isolate each element of each set of size $k$, but with no restriction on the order.","We then show that the use of our (k,N)-permutation selector improves the best running time for gossiping in ad-hoc radio networks by a poly-logarithmic factor."],"url":"http://arxiv.org/abs/2402.10783v1","category":"cs.DS"}
{"created":"2024-02-16 15:57:57","title":"Intermodulation spectroscopy and the nonlinear response of two-level systems in superconducting coplanar waveguide resonators","abstract":"Two-level system (TLS) loss is typically limiting the coherence of superconducting quantum circuits. The loss induced by TLS defects is nonlinear, resulting in quality factors with a strong dependence on the circulating microwave power. We observe frequency mixing due to this nonlinearity by applying a two-tone drive to a coplanar waveguide resonator and measuring the intermodulation products using a multifrequency lock-in technique. This intermodulation spectroscopy method provides an efficient approach to characterizing TLS loss in superconducting circuits. Using harmonic balance reconstruction, we recover the nonlinear parameters of the device-TLS interaction, which are in good agreement with the standard tunnelling model for TLSs.","sentences":["Two-level system (TLS) loss is typically limiting the coherence of superconducting quantum circuits.","The loss induced by TLS defects is nonlinear, resulting in quality factors with a strong dependence on the circulating microwave power.","We observe frequency mixing due to this nonlinearity by applying a two-tone drive to a coplanar waveguide resonator and measuring the intermodulation products using a multifrequency lock-in technique.","This intermodulation spectroscopy method provides an efficient approach to characterizing TLS loss in superconducting circuits.","Using harmonic balance reconstruction, we recover the nonlinear parameters of the device-TLS interaction, which are in good agreement with the standard tunnelling model for TLSs."],"url":"http://arxiv.org/abs/2402.10775v1","category":"quant-ph"}
{"created":"2024-02-16 15:54:24","title":"Enhancing ESG Impact Type Identification through Early Fusion and Multilingual Models","abstract":"In the evolving landscape of Environmental, Social, and Corporate Governance (ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG impact types. To address this challenge, we present a comprehensive system leveraging ensemble learning techniques, capitalizing on early and late fusion approaches. Our approach employs four distinct models: mBERT, FlauBERT-base, ALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent Semantic Analysis (LSA) and Term Frequency-Inverse Document Frequency (TF-IDF) features. Through extensive experimentation, we find that our early fusion ensemble approach, featuring the integration of LSA, TF-IDF, mBERT, FlauBERT-base, and ALBERT-base-v2, delivers the best performance. Our system offers a comprehensive ESG impact type identification solution, contributing to the responsible and sustainable decision-making processes vital in today's financial and corporate governance landscape.","sentences":["In the evolving landscape of Environmental, Social, and Corporate Governance (ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG impact types.","To address this challenge, we present a comprehensive system leveraging ensemble learning techniques, capitalizing on early and late fusion approaches.","Our approach employs four distinct models: mBERT, FlauBERT-base, ALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent Semantic Analysis (LSA) and Term Frequency-Inverse Document Frequency (TF-IDF) features.","Through extensive experimentation, we find that our early fusion ensemble approach, featuring the integration of LSA, TF-IDF, mBERT, FlauBERT-base, and ALBERT-base-v2, delivers the best performance.","Our system offers a comprehensive ESG impact type identification solution, contributing to the responsible and sustainable decision-making processes vital in today's financial and corporate governance landscape."],"url":"http://arxiv.org/abs/2402.10772v1","category":"cs.CL"}
{"created":"2024-02-16 15:41:02","title":"Live magnetic observation of parahydrogen hyperpolarization dynamics","abstract":"Nuclear spin hyperpolarization is used in physics, chemistry, and medicine to produce strong magnetization unachievable by equilibrium polarization techniques. Hyperpolarization enables magnetic resonance spectroscopy and imaging with minute samples, and is used to produce MRI spin-tracers and polarized physics targets. Although widely used, the dynamics of the hyperpolarization process have never been studied `live' due to the extremely low (Hz-band) frequencies involved, and/or detector saturation by the driving fields used. Here, we use an atomic magnetometer with sub-pT sensitivity to observe, in real time, the complex dynamics of hyperpolarization, without disturbing or disrupting the process. We start by examining parahydrogen-induced $^1$H and $^{13}$C magnetization build-up during adiabatic eigenbasis transformations in the $\\mu$T-field avoided state crossings at the heart of the process; we see live hyperpolarization dynamics including coherent oscillations, leakage mechanisms and dipolar shifts that would be challenging or impossible to observe by post hoc measurement. We then extend the methods to observe the chemical-exchange-driven $^{13}$C hyperpolarization of [1-$^{13}$C]-pyruvate -- the most important spin tracer for clinical metabolic imaging. Beyond the interests of hyperpolarization, the observation of adiabatic transitions in real-time is a fundamentally new approach to NMR, reveals previously hidden nuclear spin dynamics and enables quantum control and live process optimization in a variety of chemical scenarios.","sentences":["Nuclear spin hyperpolarization is used in physics, chemistry, and medicine to produce strong magnetization unachievable by equilibrium polarization techniques.","Hyperpolarization enables magnetic resonance spectroscopy and imaging with minute samples, and is used to produce MRI spin-tracers and polarized physics targets.","Although widely used, the dynamics of the hyperpolarization process have never been studied `live' due to the extremely low (Hz-band) frequencies involved, and/or detector saturation by the driving fields used.","Here, we use an atomic magnetometer with sub-pT sensitivity to observe, in real time, the complex dynamics of hyperpolarization, without disturbing or disrupting the process.","We start by examining parahydrogen-induced $^1$H and $^{13}$C magnetization build-up during adiabatic eigenbasis transformations in the $\\mu$T-field avoided state crossings at the heart of the process; we see live hyperpolarization dynamics including coherent oscillations, leakage mechanisms and dipolar shifts that would be challenging or impossible to observe by post hoc measurement.","We then extend the methods to observe the chemical-exchange-driven $^{13}$C hyperpolarization of [1-$^{13}$C]-pyruvate -- the most important spin tracer for clinical metabolic imaging.","Beyond the interests of hyperpolarization, the observation of adiabatic transitions in real-time is a fundamentally new approach to NMR, reveals previously hidden nuclear spin dynamics and enables quantum control and live process optimization in a variety of chemical scenarios."],"url":"http://arxiv.org/abs/2402.10766v1","category":"physics.chem-ph"}
{"created":"2024-02-16 15:39:26","title":"Nearly-optimal effective stability estimates around Diophantine tori of H\u00f6lder Hamiltonians","abstract":"We prove that the solutions of H\\\"older-differentiable Hamiltonian systems, associated to initial conditions in a small ball of radius $\\rho>0$ around a Lagrangian, $(\\gamma,\\tau)-$Diophantine, quasi-periodic torus, are stable over a time $t^{\\text{stab}}\\simeq 1/(|\\rho|^{1+\\frac{\\ell-1}{\\tau+1}}|\\ln \\rho|^{\\ell-1})$, where $\\ell>2d+1, \\ell \\in \\mathbb R$, is the regularity, and $d$ is the number of degrees of freedom. In the finitely differentiable case (for integer $\\ell$), this result improves the previously known effective stability bounds around Diophantine tori. Moreover, by a previous work based on the Anosov-Katok construction, it is known that for any $\\varepsilon>0$ there exists a $C^\\ell$-Hamiltonian, with $ \\ell\\ge 3$, admitting a sequence of solutions starting at distance $\\rho_n \\to 0$ from a $(\\gamma,\\tau)$-Diophantine torus that diffuse in a time of order $t^{\\text{diff}}_n\\simeq 1/(|\\rho_n|^{1+\\frac{\\ell-1}{\\tau+1}+\\varepsilon})$.   Therefore the stability estimates that we show are optimal up to an arbitrarily small polynomial correction.","sentences":["We prove that the solutions of H\\\"older-differentiable Hamiltonian systems, associated to initial conditions in a small ball of radius $\\rho>0$ around a Lagrangian, $(\\gamma,\\tau)-$Diophantine, quasi-periodic torus, are stable over a time $t^{\\text{stab}}\\simeq 1/(|\\rho|^{1+\\frac{\\ell-1}{\\tau+1}}|\\ln \\rho|^{\\ell-1})$, where $\\ell>2d+1, \\ell \\in \\mathbb R$, is the regularity, and $d$ is the number of degrees of freedom.","In the finitely differentiable case (for integer $\\ell$), this result improves the previously known effective stability bounds around Diophantine tori.","Moreover, by a previous work based on the Anosov-Katok construction, it is known that for any $\\varepsilon>0$ there exists a $C^\\ell$-Hamiltonian, with $ \\ell\\ge 3$, admitting a sequence of solutions starting at distance $\\rho_n \\to 0$ from a $(\\gamma,\\tau)$-Diophantine torus that diffuse in a time of order $t^{\\text{diff}}_n\\simeq 1/(|\\rho_n|^{1+\\frac{\\ell-1}{\\tau+1}+\\varepsilon})$.   ","Therefore the stability estimates that we show are optimal up to an arbitrarily small polynomial correction."],"url":"http://arxiv.org/abs/2402.10764v1","category":"math.DS"}
{"created":"2024-02-16 15:34:46","title":"Autonomous Emergency Braking With Driver-In-The-Loop: Torque Vectoring for Active Learning","abstract":"Autonomous Emergency Braking (AEB) potentially brings significant improvements in automotive safety due to its ability to autonomously prevent collisions in situations where the driver may not be able to do so. Driven by the poor performance of the state of the art in recent testing, this work provides an online solution to identify critical parameters such as the current and maximum friction coefficients. The method introduced here, namely Torque Vectoring for Active Learning (TVAL), can perform state and parameter estimation whilst following the driver's input. Importantly with less power requirements than normal driving. Our method is designed with a crucial focus on ensuring minimal disruption to the driver, allowing them to maintain full control of the vehicle. Additionally, we exploit a rain/light sensor to drive the observer resampling to maintain estimation certainty across prolonged operation. Then a scheme to modulate TVAL is introduced that considers powertrain efficiency, safety, and availability in an online fashion. Using a high-fidelity vehicle model and drive cycle we demonstrate the functionality of TVAL controller across changing road surfaces where we successfully identify the road surface whenever possible.","sentences":["Autonomous Emergency Braking (AEB) potentially brings significant improvements in automotive safety due to its ability to autonomously prevent collisions in situations where the driver may not be able to do so.","Driven by the poor performance of the state of the art in recent testing, this work provides an online solution to identify critical parameters such as the current and maximum friction coefficients.","The method introduced here, namely Torque Vectoring for Active Learning (TVAL), can perform state and parameter estimation whilst following the driver's input.","Importantly with less power requirements than normal driving.","Our method is designed with a crucial focus on ensuring minimal disruption to the driver, allowing them to maintain full control of the vehicle.","Additionally, we exploit a rain/light sensor to drive the observer resampling to maintain estimation certainty across prolonged operation.","Then a scheme to modulate TVAL is introduced that considers powertrain efficiency, safety, and availability in an online fashion.","Using a high-fidelity vehicle model and drive cycle we demonstrate the functionality of TVAL controller across changing road surfaces where we successfully identify the road surface whenever possible."],"url":"http://arxiv.org/abs/2402.10761v1","category":"eess.SY"}
{"created":"2024-02-16 15:34:04","title":"A class of symbols that induce bounded composition operators for Dirichlet-type spaces on the disc","abstract":"In this note we study the problem of determining the holomorphic self maps of the unit disc that induce a bounded composition operator on Dirichlet-type spaces. We find a class of symbols $\\varphi$ that induce a bounded composition operator on the Dirichlet-type spaces, by applying results of the multidimensional theory of composition operators for the weighted Bergman spaces of the bi-disc.","sentences":["In this note we study the problem of determining the holomorphic self maps of the unit disc that induce a bounded composition operator on Dirichlet-type spaces.","We find a class of symbols $\\varphi$ that induce a bounded composition operator on the Dirichlet-type spaces, by applying results of the multidimensional theory of composition operators for the weighted Bergman spaces of the bi-disc."],"url":"http://arxiv.org/abs/2402.10759v1","category":"math.CV"}
{"created":"2024-02-16 15:14:16","title":"A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers","abstract":"Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy. A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the transformer machine learning model. Transformers are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity. In this work, we present a tiny transformer model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices. We explored an augmentation-based training approach for improving the robustness against electrode motion artifacts noise, resulting in a worst-case post-deployment performance assessment of 98.36% accuracy. Suitability for wearable monitoring solutions is finally demonstrated through efficient deployment on the parallel ultra-low-power GAP9 processor, where inference execution requires 4.28ms and 0.09mJ.","sentences":["Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy.","A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the transformer machine learning model.","Transformers are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity.","In this work, we present a tiny transformer model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices.","We explored an augmentation-based training approach for improving the robustness against electrode motion artifacts noise, resulting in a worst-case post-deployment performance assessment of 98.36% accuracy.","Suitability for wearable monitoring solutions is finally demonstrated through efficient deployment on the parallel ultra-low-power GAP9 processor, where inference execution requires 4.28ms and 0.09mJ."],"url":"http://arxiv.org/abs/2402.10748v1","category":"eess.SP"}
{"created":"2024-02-16 14:59:26","title":"Chirality enhancement using topology-designed 3D nanophotonic antennas","abstract":"We explore chiroptical phenomena in 3D chiral nano-gap antennas using topology optimization. The characteristic helical geometries of the topology-designed antennas exhibit giant chiral dissymmetry (g=-1.70) considering the gap intensity, circular-to-linear polarization conversion, and circularly polarized light emission from a linear dipole coupled with the antenna. We observed that the spin angular momentum of light, flowing into the nanogap with opposite signs, locally amplifies optical chirality. These findings carry profound implications for the nanoscale control of complex light-matter interactions with structured light.","sentences":["We explore chiroptical phenomena in 3D chiral nano-gap antennas using topology optimization.","The characteristic helical geometries of the topology-designed antennas exhibit giant chiral dissymmetry (g=-1.70) considering the gap intensity, circular-to-linear polarization conversion, and circularly polarized light emission from a linear dipole coupled with the antenna.","We observed that the spin angular momentum of light, flowing into the nanogap with opposite signs, locally amplifies optical chirality.","These findings carry profound implications for the nanoscale control of complex light-matter interactions with structured light."],"url":"http://arxiv.org/abs/2402.10742v1","category":"physics.optics"}
{"created":"2024-02-16 14:57:37","title":"Identifying heterogeneous micromechanical properties of biological tissues via physics-informed neural networks","abstract":"The heterogeneous micromechanical properties of biological tissues have profound implications across diverse medical and engineering domains. However, identifying the full-field heterogeneous elastic properties of soft materials using traditional computational and engineering approaches is fundamentally challenging due to difficulties in estimating local stress fields. Recently, there has been a growing interest in using data-driven models to learn full-field mechanical responses such as displacement and strain from experimental or synthetic data. However, research studies on inferring the full-field elastic properties of materials, a more challenging problem, are scarce, particularly for large deformation, hyperelastic materials. Here, we propose a novel approach to identify the elastic modulus distribution in nonlinear, large deformation hyperelastic materials utilizing physics-informed neural networks (PINNs). We evaluate the prediction accuracies and computational efficiency of PINNs, informed by mechanic features and principles, across three synthetic materials with structural complexity that closely resemble real tissue patterns, such as brain tissue and tricuspid valve tissue. Our improved PINN architecture accurately estimates the full-field elastic properties, with relative errors of less than 5% across all examples. This research has significant potential for advancing our understanding of micromechanical behaviors in biological materials, impacting future innovations in engineering and medicine.","sentences":["The heterogeneous micromechanical properties of biological tissues have profound implications across diverse medical and engineering domains.","However, identifying the full-field heterogeneous elastic properties of soft materials using traditional computational and engineering approaches is fundamentally challenging due to difficulties in estimating local stress fields.","Recently, there has been a growing interest in using data-driven models to learn full-field mechanical responses such as displacement and strain from experimental or synthetic data.","However, research studies on inferring the full-field elastic properties of materials, a more challenging problem, are scarce, particularly for large deformation, hyperelastic materials.","Here, we propose a novel approach to identify the elastic modulus distribution in nonlinear, large deformation hyperelastic materials utilizing physics-informed neural networks (PINNs).","We evaluate the prediction accuracies and computational efficiency of PINNs, informed by mechanic features and principles, across three synthetic materials with structural complexity that closely resemble real tissue patterns, such as brain tissue and tricuspid valve tissue.","Our improved PINN architecture accurately estimates the full-field elastic properties, with relative errors of less than 5% across all examples.","This research has significant potential for advancing our understanding of micromechanical behaviors in biological materials, impacting future innovations in engineering and medicine."],"url":"http://arxiv.org/abs/2402.10741v1","category":"math.NA"}
{"created":"2024-02-16 14:56:13","title":"PointMamba: A Simple State Space Model for Point Cloud Analysis","abstract":"Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability. However, the attention mechanism has quadratic complexity and is difficult to extend to long sequence modeling due to limited computational resources and so on. Recently, state space models (SSM), a new family of deep sequence models, have presented great potential for sequence modeling in NLP tasks. In this paper, taking inspiration from the success of SSM in NLP, we propose PointMamba, a framework with global modeling and linear complexity. Specifically, by taking embedded point patches as input, we proposed a reordering strategy to enhance SSM's global modeling ability by providing a more logical geometric scanning order. The reordered point tokens are then sent to a series of Mamba blocks to causally capture the point cloud structure. Experimental results show our proposed PointMamba outperforms the transformer-based counterparts on different point cloud analysis datasets, while significantly saving about 44.3% parameters and 25% FLOPs, demonstrating the potential option for constructing foundational 3D vision models. We hope our PointMamba can provide a new perspective for point cloud analysis. The code is available at https://github.com/LMD0311/PointMamba.","sentences":["Transformers have become one of the foundational architectures in point cloud analysis tasks due to their excellent global modeling ability.","However, the attention mechanism has quadratic complexity and is difficult to extend to long sequence modeling due to limited computational resources and so on.","Recently, state space models (SSM), a new family of deep sequence models, have presented great potential for sequence modeling in NLP tasks.","In this paper, taking inspiration from the success of SSM in NLP, we propose PointMamba, a framework with global modeling and linear complexity.","Specifically, by taking embedded point patches as input, we proposed a reordering strategy to enhance SSM's global modeling ability by providing a more logical geometric scanning order.","The reordered point tokens are then sent to a series of Mamba blocks to causally capture the point cloud structure.","Experimental results show our proposed PointMamba outperforms the transformer-based counterparts on different point cloud analysis datasets, while significantly saving about 44.3% parameters and 25% FLOPs, demonstrating the potential option for constructing foundational 3D vision models.","We hope our PointMamba can provide a new perspective for point cloud analysis.","The code is available at https://github.com/LMD0311/PointMamba."],"url":"http://arxiv.org/abs/2402.10739v1","category":"cs.CV"}
{"created":"2024-02-16 14:55:33","title":"Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning","abstract":"Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require additional knowledge and similarity calculation. We advocate the few-shot in-context curriculum learning (ICCL), a simple but effective demonstration ordering method for ICL, which implies gradually increasing the complexity of prompt demonstrations during the inference process. Then we design three experiments to discuss the effectiveness of ICCL, the formation mechanism of LLM's ICCL capability, and the impact of ordering subjects. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacity compared to humans in discerning the difficulty levels of demonstrations. We release our code at https://github.com/61peng/curri_learning.","sentences":["Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs).","However, most of the current approaches of ordering require additional knowledge and similarity calculation.","We advocate the few-shot in-context curriculum learning (ICCL), a simple but effective demonstration ordering method for ICL, which implies gradually increasing the complexity of prompt demonstrations during the inference process.","Then we design three experiments to discuss the effectiveness of ICCL, the formation mechanism of LLM's ICCL capability, and the impact of ordering subjects.","Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for open-source LLMs.","Moreover, LLMs exhibit a weaker capacity compared to humans in discerning the difficulty levels of demonstrations.","We release our code at https://github.com/61peng/curri_learning."],"url":"http://arxiv.org/abs/2402.10738v1","category":"cs.CL"}
{"created":"2024-02-16 14:48:49","title":"Lattice realization of complex CFTs: Two-dimensional Potts model with $Q>4$ states","abstract":"The two-dimensional $Q$-state Potts model with real couplings has a first-order transition for $Q>4$. We study a loop-model realization in which $Q$ is a continuous parameter. This model allows for the collision of a critical and a tricritical fixed point at $Q=4$, which then emerge as complex conformally invariant theories at $Q>4$, or even complex $Q$, for suitable complex coupling constants. All critical exponents can be obtained as analytic continuation of known exact results for $Q \\le 4$. We verify this scenario in detail for $Q=5$ using transfer-matrix computations.","sentences":["The two-dimensional $Q$-state Potts model with real couplings has a first-order transition for $Q>4$. We study a loop-model realization in which $Q$ is a continuous parameter.","This model allows for the collision of a critical and a tricritical fixed point at $Q=4$, which then emerge as complex conformally invariant theories at $Q>4$, or even complex $Q$, for suitable complex coupling constants.","All critical exponents can be obtained as analytic continuation of known exact results for $Q \\le 4$.","We verify this scenario in detail for $Q=5$ using transfer-matrix computations."],"url":"http://arxiv.org/abs/2402.10732v1","category":"hep-th"}
{"created":"2024-02-16 14:47:39","title":"A CBF-Adaptive Control Architecture for Visual Navigation for UAV in the Presence of Uncertainties","abstract":"In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances. The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner. The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which is tracked by an adaptive controller to overcome modeling uncertainties and external disturbances. The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera.","sentences":["In this article, we propose a control solution for the safe transfer of a quadrotor UAV between two surface robots positioning itself only using the visual features on the surface robots, which enforces safety constraints for precise landing and visual locking, in the presence of modeling uncertainties and external disturbances.","The controller handles the ascending and descending phases of the navigation using a visual locking control barrier function (VCBF) and a parametrizable switching descending CBF (DCBF) respectively, eliminating the need for an external planner.","The control scheme has a backstepping approach for the position controller with the CBF filter acting on the position kinematics to produce a filtered virtual velocity control input, which is tracked by an adaptive controller to overcome modeling uncertainties and external disturbances.","The experimental validation is carried out with a UAV that navigates from the base to the target using an RGB camera."],"url":"http://arxiv.org/abs/2402.10729v1","category":"cs.RO"}
{"created":"2024-02-16 14:15:42","title":"Molecular Weight Distribution Modeling in Step Growth Polymerization of Oligomers","abstract":"An approach to modeling the oligomer composition distribution function in the irreversible step growth homopolymerization process based on a mixture of oligomers of arbitrary composition is developed. The approach is based on consideration of probabilities of processes in the system, proceeding on the principle of P. Flory and obtaining on this basis an infinite system of differential equations linking mole fractions of each oligomer of a finite mixture with the degree of transformation.","sentences":["An approach to modeling the oligomer composition distribution function in the irreversible step growth homopolymerization process based on a mixture of oligomers of arbitrary composition is developed.","The approach is based on consideration of probabilities of processes in the system, proceeding on the principle of P. Flory and obtaining on this basis an infinite system of differential equations linking mole fractions of each oligomer of a finite mixture with the degree of transformation."],"url":"http://arxiv.org/abs/2402.10713v1","category":"physics.chem-ph"}
{"created":"2024-02-16 14:02:57","title":"The Quantum Ratio","abstract":"The concept of {\\it quantum ratio} emerged in the recent efforts to understand how Newton's equations appear for the center of mass (CM) of an isolated macroscopic body at finite body-temperatures, as the first approximation to quantum-mechanical equations. It is defined as $Q\\equiv R_q/L_0$, where the quantum fluctuation range $R_q$ is the spatial extension of the pure-state CM wave function, whereas $L_0$ stands for the body's linear size (the space support of the internal, bound-state wave function). The two cases $R_q /L_0 \\lesssim 1$ or $R_q/ L_0 \\gg 1$, roughly correspond to the body's CM behaving classically or quantum mechanically, respectively. In the present note we elaborate more on this concept, illustrating it in several examples. An important notion following from introduction of the quantum ratio is that the elementary particles (thus the electron and the photon) are quantum mechanical, even when the environment-induced decoherence turns them into a mixed state. Decoherence and classical state should not be identified. This simple observation, further illustrated by the consideration of a few atomic or molecular processes, may have significant implications on the way quantum mechanics works in biological systems.","sentences":["The concept of {\\it quantum ratio} emerged in the recent efforts to understand how Newton's equations appear for the center of mass (CM) of an isolated macroscopic body at finite body-temperatures, as the first approximation to quantum-mechanical equations.","It is defined as $Q\\equiv R_q/L_0$, where the quantum fluctuation range $R_q$ is the spatial extension of the pure-state CM wave function, whereas $L_0$ stands for the body's linear size (the space support of the internal, bound-state wave function).","The two cases $R_q /L_0","\\lesssim 1$ or $R_q/ L_0 \\gg 1$, roughly correspond to the body's CM behaving classically or quantum mechanically, respectively.","In the present note we elaborate more on this concept, illustrating it in several examples.","An important notion following from introduction of the quantum ratio is that the elementary particles (thus the electron and the photon) are quantum mechanical, even when the environment-induced decoherence turns them into a mixed state.","Decoherence and classical state should not be identified.","This simple observation, further illustrated by the consideration of a few atomic or molecular processes, may have significant implications on the way quantum mechanics works in biological systems."],"url":"http://arxiv.org/abs/2402.10702v1","category":"quant-ph"}
{"created":"2024-02-16 13:59:07","title":"Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering","abstract":"We present Q-ViD, a simple approach for video question answering (video QA), that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like GPTs, Q-ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle videoQA using frame descriptions. Specifically, we create captioning instruction prompts that rely on the target questions about the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand. Subsequently, we form descriptions of the whole video using the question-dependent frame captions, and feed that information, along with a question-answering prompt, to a large language model (LLM). The LLM is our reasoning module, and performs the final step of multiple-choice QA. Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA benchmarks, including NExT-QA, STAR, How2QA, TVQA and IntentQA.","sentences":["We present Q-ViD, a simple approach for video question answering (video QA), that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like GPTs, Q-ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle videoQA using frame descriptions.","Specifically, we create captioning instruction prompts that rely on the target questions about the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand.","Subsequently, we form descriptions of the whole video using the question-dependent frame captions, and feed that information, along with a question-answering prompt, to a large language model (LLM).","The LLM is our reasoning module, and performs the final step of multiple-choice QA.","Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA benchmarks, including NExT-QA, STAR, How2QA, TVQA and IntentQA."],"url":"http://arxiv.org/abs/2402.10698v1","category":"cs.CV"}
{"created":"2024-02-16 13:24:05","title":"Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm","abstract":"In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin. About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev and Spider-Realistic datasets and new SOTA results on the Spider Test dataset are achieved. Our code is available on GitHub: \\url{https://github.com/FlyingFeather/DEA-SQL}.","sentences":["In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL.","To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition.","Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention.","Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches.","Extensive experiments conducted on three datasets demonstrate that our approach outperforms other methods by a significant margin.","About 2-3 percentage point improvements compared to the existing baseline on the Spider Dev and Spider-Realistic datasets and new SOTA results on the Spider Test dataset are achieved.","Our code is available on GitHub: \\url{https://github.com/FlyingFeather/DEA-SQL}."],"url":"http://arxiv.org/abs/2402.10671v1","category":"cs.CL"}
{"created":"2024-02-16 13:21:06","title":"Humans or LLMs as the Judge? A Study on Judgement Biases","abstract":"Adopting human and large language models (LLM) as judges (\\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of developing robust evaluation systems.","sentences":["Adopting human and large language models (LLM) as judges (\\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention.","Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results.","In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges.","We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations.","Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases.","We further exploit their weakness and conduct attacks on LLM judges.","We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of developing robust evaluation systems."],"url":"http://arxiv.org/abs/2402.10669v1","category":"cs.CL"}
{"created":"2024-02-16 13:19:04","title":"Data-Driven Abstractions for Control Systems","abstract":"At the intersection of dynamical systems, control theory, and formal methods lies the construction of symbolic abstractions: these typically represent simpler, finite-state models whose behaviour mimics the one of an underlying concrete system but are easier to analyse. Building an abstraction usually requires an accurate knowledge of the underlying model: this knowledge may be costly to gather, especially in real-life applications. We aim to bridge this gap by building abstractions based on sampling finite length trajectories. Adding the controller degrees of freedom, we newly define the notion of probabilistic alternating simulation, and provide probably approximately correct (PAC) guarantees that the constructed abstraction includes all behaviours of the concrete system and that it is suitable for control design, for arbitrarily long time horizons, leveraging the scenario theory. Our method is then tested on several numerical benchmarks.","sentences":["At the intersection of dynamical systems, control theory, and formal methods lies the construction of symbolic abstractions: these typically represent simpler, finite-state models whose behaviour mimics the one of an underlying concrete system but are easier to analyse.","Building an abstraction usually requires an accurate knowledge of the underlying model: this knowledge may be costly to gather, especially in real-life applications.","We aim to bridge this gap by building abstractions based on sampling finite length trajectories.","Adding the controller degrees of freedom, we newly define the notion of probabilistic alternating simulation, and provide probably approximately correct (PAC) guarantees that the constructed abstraction includes all behaviours of the concrete system and that it is suitable for control design, for arbitrarily long time horizons, leveraging the scenario theory.","Our method is then tested on several numerical benchmarks."],"url":"http://arxiv.org/abs/2402.10668v1","category":"eess.SY"}
{"created":"2024-02-16 13:09:15","title":"Human-machine collaboration: ordering mechanism of rank$-2$ spin liquid on breathing pyrochlore lattice","abstract":"Machine learning algorithms thrive on large data sets of good quality. Here we show that they can also excel in a typical research setting with little data of limited quality, through an interplay of insights coming from machine, and human researchers. The question we address is the unsolved problem of ordering out of a spin-liquid phase described by an emergent rank-2 $U(1)$ gauge theory, as described by [H. Yan it et al., Phys. Rev. Lett. 124, 127203 (2020)]. Published Monte Carlo simulations for this problem are consistent with a strong first-order phase transition, out of the R2-U1 spin liquid, but were too noisy for the form of low-temperature order to be identified. Using a highly-interpretable machine learning approach based on a support vector machine with a tensorial kernel (TKSVM), we re-analyze this Monte Carlo data, gaining new information about the form of order that could in turn be interpreted by traditionally-trained physicists. We find that the low-temperature ordered phase is a form of hybrid nematic order with emergent $Z_2$ symmetry, which allows for a sub-extensive set of domain walls at zero energy. This complex form of order arises due to a subtle thermal order-by-disorder mechanism, that can be understood from the fluctuations of the tensor electric field of the parent rank-2 gauge theory. These results were obtained by a back-and-forth process which closely resembles a collaboration between human researchers and machines. We argue that this \"collaborative\" approach may provide a blueprint for solving other problems that have not yielded to human insights alone.","sentences":["Machine learning algorithms thrive on large data sets of good quality.","Here we show that they can also excel in a typical research setting with little data of limited quality, through an interplay of insights coming from machine, and human researchers.","The question we address is the unsolved problem of ordering out of a spin-liquid phase described by an emergent rank-2 $U(1)$ gauge theory, as described by [H. Yan it et al., Phys.","Rev. Lett.","124, 127203 (2020)].","Published Monte Carlo simulations for this problem are consistent with a strong first-order phase transition, out of the R2-U1 spin liquid, but were too noisy for the form of low-temperature order to be identified.","Using a highly-interpretable machine learning approach based on a support vector machine with a tensorial kernel (TKSVM), we re-analyze this Monte Carlo data, gaining new information about the form of order that could in turn be interpreted by traditionally-trained physicists.","We find that the low-temperature ordered phase is a form of hybrid nematic order with emergent $Z_2","$ symmetry, which allows for a sub-extensive set of domain walls at zero energy.","This complex form of order arises due to a subtle thermal order-by-disorder mechanism, that can be understood from the fluctuations of the tensor electric field of the parent rank-2 gauge theory.","These results were obtained by a back-and-forth process which closely resembles a collaboration between human researchers and machines.","We argue that this \"collaborative\" approach may provide a blueprint for solving other problems that have not yielded to human insights alone."],"url":"http://arxiv.org/abs/2402.10658v1","category":"cond-mat.str-el"}
{"created":"2024-02-16 13:03:18","title":"An energy-based material model for the simulation of shape memory alloys under complex boundary value problems","abstract":"Shape memory alloys are remarkable 'smart' materials used in a broad spectrum of applications, ranging from aerospace to robotics, thanks to their unique thermomechanical coupling capabilities. Given the complex properties of shape memory alloys, which are largely influenced by thermal and mechanical loads, as well as their loading history, predicting their behavior can be challenging. Consequently, there exists a pronounced demand for an efficient material model to simulate the behavior of these alloys. This paper introduces a material model rooted in Hamilton's principle. The key advantages of the presented material model encompass a more accurate depiction of the internal variable evolution and heightened robustness. As such, the proposed material model signifies an advancement in the realistic and efficient simulation of shape memory alloys.","sentences":["Shape memory alloys are remarkable 'smart' materials used in a broad spectrum of applications, ranging from aerospace to robotics, thanks to their unique thermomechanical coupling capabilities.","Given the complex properties of shape memory alloys, which are largely influenced by thermal and mechanical loads, as well as their loading history, predicting their behavior can be challenging.","Consequently, there exists a pronounced demand for an efficient material model to simulate the behavior of these alloys.","This paper introduces a material model rooted in Hamilton's principle.","The key advantages of the presented material model encompass a more accurate depiction of the internal variable evolution and heightened robustness.","As such, the proposed material model signifies an advancement in the realistic and efficient simulation of shape memory alloys."],"url":"http://arxiv.org/abs/2402.10655v1","category":"cs.CE"}
{"created":"2024-02-16 12:52:56","title":"Temperature dependence of the bandgap of Eu doped {ZnCdO/ZnO}30 multilayer structures","abstract":"In situ Eu-doped {ZnCdO/ZnO}30 multilayer systems were grown on p-type Si-substrates and on quartz substrates by plasma-assisted molecular beam epitaxy. Various Eu concentrations in the samples were achieved by controlling temperature of the europium effusion cell. The properties of as-grown and annealed {ZnCdO/ZnO}30:Eu multilayers were investigated using secondary ion mass spectrometry (SIMS) and X-ray diffraction methods. SIMS measurements showed that annealing at 700{\\deg}C and 900{\\deg}C practically did not change the Eu concentration and the rare earth depth profiles are uniform. It was found that the band gap depends on the concentration of Eu and it was changed by rapid thermal annealing. Varshni and Bose-Einstein equations were used to describe the temperature dependence of the band gap of {ZnCdO/ZnO}30:Eu multilayer structures and Debye and Einstein temperatures were obtained.","sentences":["In situ Eu-doped {ZnCdO/ZnO}30 multilayer systems were grown on p-type Si-substrates and on quartz substrates by plasma-assisted molecular beam epitaxy.","Various Eu concentrations in the samples were achieved by controlling temperature of the europium effusion cell.","The properties of as-grown and annealed {ZnCdO/ZnO}30:","Eu multilayers were investigated using secondary ion mass spectrometry (SIMS) and X-ray diffraction methods.","SIMS measurements showed that annealing at 700{\\deg}C and 900{\\deg}C practically did not change the Eu concentration and the rare earth depth profiles are uniform.","It was found that the band gap depends on the concentration of Eu and it was changed by rapid thermal annealing.","Varshni and Bose-Einstein equations were used to describe the temperature dependence of the band gap of {ZnCdO/ZnO}30:","Eu multilayer structures and Debye and Einstein temperatures were obtained."],"url":"http://arxiv.org/abs/2402.10650v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-16 12:51:25","title":"Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation","abstract":"The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB's Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method.","sentences":["The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system.","It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics.","In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions.","Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution.","The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision.","Finally, the proposed method was simulated using MATLAB's Simulink tool.","The results were then compared with those obtained using Physics-informed neural networks and the presented method."],"url":"http://arxiv.org/abs/2402.10649v1","category":"math.NA"}
{"created":"2024-02-16 12:47:56","title":"X-ray Linear Dichroic Tomography of Crystallographic and Topological Defects","abstract":"The functionality of materials is determined by their composition and microstructure, that is, the distribution and orientation of crystalline grains, grain boundaries and the defects within them. The characterisation of the material's microstructure is therefore critical for materials applications such as catalysis, energy storage and buildings. Until now, characterization techniques that map the distribution of grains, their orientation, and the presence of defects have either been limited to surface investigations, to spatial resolutions of a few hundred nanometres, or to systems of thickness around one hundred nanometres, thus requiring destructive sample preparation for measurements and preventing the study of system-representative volumes or the investigation of materials under operational conditions. Here, we present X-ray linear dichroic orientation tomography, a quantitative, non-invasive technique that allows for an intra- and inter-granular characterisation of extended polycrystalline and amorphous materials in three dimensions (3D). We present the detailed characterisation of a polycrystalline sample of vanadium pentoxide (V2O5), a key catalyst in the production of sulfuric acid. In addition to determining the nanoscale composition, we map the crystal orientation throughout the polycrystalline sample with 73 nm spatial resolution. We identify grains, as well as twist, tilt, and twin grain boundaries. We further observe the creation and annihilation of topological defects promoted by the presence of volume crystallographic defects in 3D. Our method's non-destructive and spectroscopic nature opens the door to in-operando combined chemical and microstructural investigations of functional materials, including energy and mechanical materials in existing industries, as well as quantum materials for future technologies.","sentences":["The functionality of materials is determined by their composition and microstructure, that is, the distribution and orientation of crystalline grains, grain boundaries and the defects within them.","The characterisation of the material's microstructure is therefore critical for materials applications such as catalysis, energy storage and buildings.","Until now, characterization techniques that map the distribution of grains, their orientation, and the presence of defects have either been limited to surface investigations, to spatial resolutions of a few hundred nanometres, or to systems of thickness around one hundred nanometres, thus requiring destructive sample preparation for measurements and preventing the study of system-representative volumes or the investigation of materials under operational conditions.","Here, we present X-ray linear dichroic orientation tomography, a quantitative, non-invasive technique that allows for an intra- and inter-granular characterisation of extended polycrystalline and amorphous materials in three dimensions (3D).","We present the detailed characterisation of a polycrystalline sample of vanadium pentoxide (V2O5), a key catalyst in the production of sulfuric acid.","In addition to determining the nanoscale composition, we map the crystal orientation throughout the polycrystalline sample with 73 nm spatial resolution.","We identify grains, as well as twist, tilt, and twin grain boundaries.","We further observe the creation and annihilation of topological defects promoted by the presence of volume crystallographic defects in 3D.","Our method's non-destructive and spectroscopic nature opens the door to in-operando combined chemical and microstructural investigations of functional materials, including energy and mechanical materials in existing industries, as well as quantum materials for future technologies."],"url":"http://arxiv.org/abs/2402.10647v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-16 12:26:59","title":"FairSync: Ensuring Amortized Group Exposure in Distributed Recommendation Retrieval","abstract":"In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period. For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs. Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1. Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task. Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups. Previous fairness-aware works designed for stage-2 typically require accessing and traversing all items. In stage-1, however, millions of items are distributively stored in servers, making it infeasible to traverse all of them. How to ensure group exposures in the distributed retrieval process is a challenging question. To address this issue, we introduce a model named FairSync, which transforms the problem into a constrained distributed optimization problem. Specifically, FairSync resolves the issue by moving it to the dual space, where a central node aggregates historical fairness data into a vector and distributes it to all servers. To trade off the efficiency and accuracy, the gradient descent technique is used to periodically update the parameter of the dual vector. The experiment results on two public recommender retrieval datasets showcased that FairSync outperformed all the baselines, achieving the desired minimum level of exposures while maintaining a high level of retrieval accuracy.","sentences":["In pursuit of fairness and balanced development, recommender systems (RS) often prioritize group fairness, ensuring that specific groups maintain a minimum level of exposure over a given period.","For example, RS platforms aim to ensure adequate exposure for new providers or specific categories of items according to their needs.","Modern industry RS usually adopts a two-stage pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from millions of items distributed across various servers, and stage-2 (ranking stage) focuses on presenting a small-size but accurate selection from items chosen in stage-1.","Existing efforts for ensuring amortized group exposures focus on stage-2, however, stage-1 is also critical for the task.","Without a high-quality set of candidates, the stage-2 ranker cannot ensure the required exposure of groups.","Previous fairness-aware works designed for stage-2 typically require accessing and traversing all items.","In stage-1, however, millions of items are distributively stored in servers, making it infeasible to traverse all of them.","How to ensure group exposures in the distributed retrieval process is a challenging question.","To address this issue, we introduce a model named FairSync, which transforms the problem into a constrained distributed optimization problem.","Specifically, FairSync resolves the issue by moving it to the dual space, where a central node aggregates historical fairness data into a vector and distributes it to all servers.","To trade off the efficiency and accuracy, the gradient descent technique is used to periodically update the parameter of the dual vector.","The experiment results on two public recommender retrieval datasets showcased that FairSync outperformed all the baselines, achieving the desired minimum level of exposures while maintaining a high level of retrieval accuracy."],"url":"http://arxiv.org/abs/2402.10628v1","category":"cs.IR"}
{"created":"2024-02-16 12:26:57","title":"Alphabet Reduction for Reconfiguration Problems","abstract":"We present a reconfiguration analogue of alphabet reduction \\`a la Dinur (J. ACM, 2007) and its applications. Given a binary constraint graph $G$ and its two satisfying assignments $\\psi^\\mathsf{ini}$ and $\\psi^\\mathsf{tar}$, the Maxmin Binary CSP Reconfiguration problem requests to transform $\\psi^\\mathsf{ini}$ into $\\psi^\\mathsf{tar}$ by repeatedly changing the value of a single vertex so that the minimum fraction of satisfied edges is maximized. We demonstrate a polynomial-time reduction from Maxmin Binary CSP Reconfiguration with arbitrarily large alphabet size $W \\in \\mathbb{N}$ to itself with universal alphabet size $W_0 \\in \\mathbb{N}$ such that   1. the perfect completeness is preserved, and   2. if any reconfiguration for the former violates $\\varepsilon$-fraction of edges, then $\\Omega(\\varepsilon)$-fraction of edges must be unsatisfied during any reconfiguration for the latter.   The crux of its construction is the reconfigurability of Hadamard codes, which enables to reconfigure between a pair of codewords, while avoiding getting too close to the other codewords. Combining this alphabet reduction with gap amplification due to Ohsaka (SODA 2024), we are able to amplify the $1$ vs. $1-\\varepsilon$ gap for arbitrarily small $\\varepsilon \\in (0,1)$ up to the $1$ vs. $1-\\varepsilon_0$ for some universal $\\varepsilon_0 \\in (0,1)$ without blowing up the alphabet size. In particular, a $1$ vs. $1-\\varepsilon_0$ gap version of Maxmin Binary CSP Reconfiguration with alphabet size $W_0$ is PSPACE-hard only assuming the Reconfiguration Inapproximability Hypothesis posed by Ohsaka (STACS 2023), whose gap parameter can be arbitrarily small. This may not be achieved only by gap amplification of Ohsaka, which makes the alphabet size gigantic depending on the gap value of the hypothesis.","sentences":["We present a reconfiguration analogue of alphabet reduction \\`a la Dinur (J. ACM, 2007) and its applications.","Given a binary constraint graph $G$ and its two satisfying assignments $\\psi^\\mathsf{ini}$ and $\\psi^\\mathsf{tar}$, the Maxmin Binary CSP Reconfiguration problem requests to transform $\\psi^\\mathsf{ini}$ into $\\psi^\\mathsf{tar}$ by repeatedly changing the value of a single vertex so that the minimum fraction of satisfied edges is maximized.","We demonstrate a polynomial-time reduction from Maxmin Binary CSP Reconfiguration with arbitrarily large alphabet size $W \\in \\mathbb{N}$ to itself with universal alphabet size $W_0 \\in \\mathbb{N}$ such that   1.","the perfect completeness is preserved, and   2.","if any reconfiguration for the former violates $\\varepsilon$-fraction of edges, then $\\Omega(\\varepsilon)$-fraction of edges must be unsatisfied during any reconfiguration for the latter.   ","The crux of its construction is the reconfigurability of Hadamard codes, which enables to reconfigure between a pair of codewords, while avoiding getting too close to the other codewords.","Combining this alphabet reduction with gap amplification due to Ohsaka (SODA 2024), we are able to amplify the $1$ vs. $1-\\varepsilon$ gap for arbitrarily small $\\varepsilon \\in (0,1)$ up to the $1$ vs. $1-\\varepsilon_0$ for some universal $\\varepsilon_0 \\in (0,1)$ without blowing up the alphabet size.","In particular, a $1$ vs. $1-\\varepsilon_0$ gap version of Maxmin Binary CSP Reconfiguration with alphabet size $W_0$ is PSPACE-hard only assuming the Reconfiguration Inapproximability Hypothesis posed by Ohsaka (STACS 2023), whose gap parameter can be arbitrarily small.","This may not be achieved only by gap amplification of Ohsaka, which makes the alphabet size gigantic depending on the gap value of the hypothesis."],"url":"http://arxiv.org/abs/2402.10627v1","category":"cs.CC"}
{"created":"2024-02-16 12:00:54","title":"A mortar method for the coupled Stokes-Darcy problem using the MAC scheme for Stokes and mixed finite elements for Darcy","abstract":"A discretization method with non-matching grids is proposed for the coupled Stokes-Darcy problem that uses a mortar variable at the interface to couple the marker and cell (MAC) method in the Stokes domain with the Raviart-Thomas mixed finite element pair in the Darcy domain. Due to this choice, the method conserves linear momentum and mass locally in the Stokes domain and exhibits local mass conservation in the Darcy domain. The MAC scheme is reformulated as a mixed finite element method on a staggered grid, which allows for the proposed scheme to be analyzed as a mortar mixed finite element method. We show that the discrete system is well-posed and derive a priori error estimates that indicate first order convergence in all variables. The system can be reduced to an interface problem concerning only the mortar variables, leading to a non-overlapping domain decomposition method. Numerical examples are presented to illustrate the theoretical results and the applicability of the method.","sentences":["A discretization method with non-matching grids is proposed for the coupled Stokes-Darcy problem that uses a mortar variable at the interface to couple the marker and cell (MAC) method in the Stokes domain with the Raviart-Thomas mixed finite element pair in the Darcy domain.","Due to this choice, the method conserves linear momentum and mass locally in the Stokes domain and exhibits local mass conservation in the Darcy domain.","The MAC scheme is reformulated as a mixed finite element method on a staggered grid, which allows for the proposed scheme to be analyzed as a mortar mixed finite element method.","We show that the discrete system is well-posed and derive a priori error estimates that indicate first order convergence in all variables.","The system can be reduced to an interface problem concerning only the mortar variables, leading to a non-overlapping domain decomposition method.","Numerical examples are presented to illustrate the theoretical results and the applicability of the method."],"url":"http://arxiv.org/abs/2402.10615v1","category":"math.NA"}
{"created":"2024-02-16 11:49:46","title":"A maximum likelihood estimation of L\u00e9vy-driven stochastic systems for univariate and multivariate time series of observations","abstract":"Literature is full of inference techniques developed to estimate the parameters of stochastic dynamical systems driven by the well-known Brownian noise. Such diffusion models are often inappropriate models to properly describe the dynamics reflected in many real-world data which are dominated by jump discontinuities of various sizes and frequencies. To account for the presence of jumps, jump-diffusion models are introduced and some inference techniques are developed. Jump-diffusion models are also inadequate models since they fail to reflect the frequent occurrence as well as the continuous spectrum of natural jumps. It is, therefore, crucial to depart from the classical stochastic systems like diffusion and jump-diffusion models and resort to stochastic systems where the regime of stochasticity is governed by the stochastic fluctuations of L\\'evy type. Reconstruction of L\\'evy-driven dynamical systems, however, has been a major challenge. The literature on the reconstruction of L\\'evy-driven systems is rather poor: there are few reconstruction algorithms developed which suffer from one or several problems such as being data-hungry, failing to provide a full reconstruction of noise parameters, tackling only some specific systems, failing to cope with multivariate data in practice, lacking proper validation mechanisms, and many more. This letter introduces a maximum likelihood estimation procedure which grants a full reconstruction of the system, requires less data, and its implementation for multivariate data is quite straightforward. To the best of our knowledge this contribution is the first to tackle all the mentioned shortcomings. We apply our algorithm to simulated data as well as an ice-core dataset spanning the last glaciation. In particular, we find new insights about the dynamics of the climate in the curse of the last glaciation which was not found in previous studies.","sentences":["Literature is full of inference techniques developed to estimate the parameters of stochastic dynamical systems driven by the well-known Brownian noise.","Such diffusion models are often inappropriate models to properly describe the dynamics reflected in many real-world data which are dominated by jump discontinuities of various sizes and frequencies.","To account for the presence of jumps, jump-diffusion models are introduced and some inference techniques are developed.","Jump-diffusion models are also inadequate models since they fail to reflect the frequent occurrence as well as the continuous spectrum of natural jumps.","It is, therefore, crucial to depart from the classical stochastic systems like diffusion and jump-diffusion models and resort to stochastic systems where the regime of stochasticity is governed by the stochastic fluctuations of L\\'evy type.","Reconstruction of L\\'evy-driven dynamical systems, however, has been a major challenge.","The literature on the reconstruction of L\\'evy-driven systems is rather poor: there are few reconstruction algorithms developed which suffer from one or several problems such as being data-hungry, failing to provide a full reconstruction of noise parameters, tackling only some specific systems, failing to cope with multivariate data in practice, lacking proper validation mechanisms, and many more.","This letter introduces a maximum likelihood estimation procedure which grants a full reconstruction of the system, requires less data, and its implementation for multivariate data is quite straightforward.","To the best of our knowledge this contribution is the first to tackle all the mentioned shortcomings.","We apply our algorithm to simulated data as well as an ice-core dataset spanning the last glaciation.","In particular, we find new insights about the dynamics of the climate in the curse of the last glaciation which was not found in previous studies."],"url":"http://arxiv.org/abs/2402.10608v1","category":"math.DS"}
{"created":"2024-02-16 11:48:40","title":"On the spectrum of $2\\times 2$ Dirac operator with degenerate boundary conditions","abstract":"We study the spectral problem for the Dirac operator with degenerate boundary conditions and a complex-valued summable potential. Sufficient conditions are found under which the spectrum of the problem under consideration coincides with the spectrum of the corresponding unperturbed operator.","sentences":["We study the spectral problem for the Dirac operator with degenerate boundary conditions and a complex-valued summable potential.","Sufficient conditions are found under which the spectrum of the problem under consideration coincides with the spectrum of the corresponding unperturbed operator."],"url":"http://arxiv.org/abs/2402.10606v1","category":"math.SP"}
{"created":"2024-02-16 11:42:18","title":"Automatic Circular Take-off and Landing of Tethered Motorized Aircraft","abstract":"We consider a motorized aircraft tethered to a central anchorage point in a configuration similar to a control line model airplane. For this system, we address the problem of automatic take-off and landing (ATOL) with a circular path, whose center and radius are defined by the anchorage point and the tether length, respectively. We propose a hierarchical control architecture for ATOL and discuss the controllers designed for each control layer and for each of the flight phases. Simulation results are reported, showing the viability of the approach, but also showing the limitations on the maximum altitude attainable with a fixed-tether length. The tethered aircraft and the proposed ATOL control architecture are to be used in an Airborne Wind Energy System.","sentences":["We consider a motorized aircraft tethered to a central anchorage point in a configuration similar to a control line model airplane.","For this system, we address the problem of automatic take-off and landing (ATOL) with a circular path, whose center and radius are defined by the anchorage point and the tether length, respectively.","We propose a hierarchical control architecture for ATOL and discuss the controllers designed for each control layer and for each of the flight phases.","Simulation results are reported, showing the viability of the approach, but also showing the limitations on the maximum altitude attainable with a fixed-tether length.","The tethered aircraft and the proposed ATOL control architecture are to be used in an Airborne Wind Energy System."],"url":"http://arxiv.org/abs/2402.10603v1","category":"math.OC"}
{"created":"2024-02-16 11:23:40","title":"Investigating black hole accretion disks as potential polluter sources for the formation of enriched stars in globular clusters","abstract":"Accretion disks surrounding stellar mass black holes (BHs) have been suggested as potential locations for the nucleosynthesis of light elements, which are our primary observational discriminant of multiple stellar populations within globular clusters. The population of enriched stars in globular clusters are enhanced in N14, Na23, and sometimes in Al27 and/or in K39. In this study, our aim is to investigate the feasibility of initiating nucleosynthesis for these four elements in BH accretion disks, considering various internal parameters such as the temperature of the gas and timescale of the accretion. To achieve this, we employed a 132-species reaction network. We used the slim disk model, suitable for the Super-Eddington mass accretion rate and for geometrically and optically thick disks. We explored the conditions related to the mass, mass accretion rate, viscosity, and radius of the BH-accretion disk system that would allow for the creation of N14, Na23, Al27, and K39 before the gas is accreted onto the central object. Our findings reveal that there is no region in the parameter space where the formation of Na23 can occur and only a very limited region where the formation of N14, Al27, and K39 is plausible. Specifically, this occurs for BHs with masses lower than 10 solar masses, with a preference toward even lower mass values and extremely low viscosity parameters ($\\alpha <10^{-3}$). Such values are highly unlikely based on current observations of stellar mass BHs. However, such low mass BHs could actually exist in the early universe, as so-called primordial BHs. In conclusion, our study suggests that the nucleosynthesis within BH accretion disks of four elements of interest for the multiple stellar populations is improbable, but not impossible, using the slim disk model.","sentences":["Accretion disks surrounding stellar mass black holes (BHs) have been suggested as potential locations for the nucleosynthesis of light elements, which are our primary observational discriminant of multiple stellar populations within globular clusters.","The population of enriched stars in globular clusters are enhanced in N14, Na23, and sometimes in Al27 and/or in K39.","In this study, our aim is to investigate the feasibility of initiating nucleosynthesis for these four elements in BH accretion disks, considering various internal parameters such as the temperature of the gas and timescale of the accretion.","To achieve this, we employed a 132-species reaction network.","We used the slim disk model, suitable for the Super-Eddington mass accretion rate and for geometrically and optically thick disks.","We explored the conditions related to the mass, mass accretion rate, viscosity, and radius of the BH-accretion disk system that would allow for the creation of N14, Na23, Al27, and K39 before the gas is accreted onto the central object.","Our findings reveal that there is no region in the parameter space where the formation of Na23 can occur and only a very limited region where the formation of N14, Al27, and K39 is plausible.","Specifically, this occurs for BHs with masses lower than 10 solar masses, with a preference toward even lower mass values and extremely low viscosity parameters ($\\alpha <10^{-3}$).","Such values are highly unlikely based on current observations of stellar mass BHs.","However, such low mass BHs could actually exist in the early universe, as so-called primordial BHs.","In conclusion, our study suggests that the nucleosynthesis within BH accretion disks of four elements of interest for the multiple stellar populations is improbable, but not impossible, using the slim disk model."],"url":"http://arxiv.org/abs/2402.10590v1","category":"astro-ph.GA"}
{"created":"2024-02-16 11:20:22","title":"Coherent X-ray Imaging of Stochastic Dynamics","abstract":"Condensed phase systems often exhibit a mixture of deterministic and stochastic dynamics at the nanoscale which are essential to understanding their function, but can be challenging to study directly using conventional imaging methods. Coherent X-ray imaging has emerged as a powerful tool for studying both nanoscale structures and dynamics in condensed phase systems, including stochastic dynamics, but the requirement to obtain single-shot images in order to obtain freeze-frame images of the stochastic dynamics means the X-ray fluxes used must be very high, potentially destroying the samples. This prevents coherent imaging from being applied to complex systems like tracking the motion of charge carriers or domain fluctuations in quantum materials. Here we show that, by leveraging the coherence intrinsic to these methods, we can separate out the stochastic and deterministic contributions to a coherent X-ray scattering pattern, returning real space images of the deterministic contributions and the momentum spectrum of the stochastic contributions. We further show that, for several typical and important classes of fluctuations, we can return real space images of the mean fluctuations. We demonstrate this approach by numerically simulating the imaging of stochastic polaron separation following photoexcitation and by recovering the spectral properties of fluctuating domain walls. Our versatile approach will enable the direct recovery of the spatial, spectral and temporal properties of stochastic material dynamics in a wide variety of systems currently unobtainable with existing methods.","sentences":["Condensed phase systems often exhibit a mixture of deterministic and stochastic dynamics at the nanoscale which are essential to understanding their function, but can be challenging to study directly using conventional imaging methods.","Coherent X-ray imaging has emerged as a powerful tool for studying both nanoscale structures and dynamics in condensed phase systems, including stochastic dynamics, but the requirement to obtain single-shot images in order to obtain freeze-frame images of the stochastic dynamics means the X-ray fluxes used must be very high, potentially destroying the samples.","This prevents coherent imaging from being applied to complex systems like tracking the motion of charge carriers or domain fluctuations in quantum materials.","Here we show that, by leveraging the coherence intrinsic to these methods, we can separate out the stochastic and deterministic contributions to a coherent X-ray scattering pattern, returning real space images of the deterministic contributions and the momentum spectrum of the stochastic contributions.","We further show that, for several typical and important classes of fluctuations, we can return real space images of the mean fluctuations.","We demonstrate this approach by numerically simulating the imaging of stochastic polaron separation following photoexcitation and by recovering the spectral properties of fluctuating domain walls.","Our versatile approach will enable the direct recovery of the spatial, spectral and temporal properties of stochastic material dynamics in a wide variety of systems currently unobtainable with existing methods."],"url":"http://arxiv.org/abs/2402.10585v1","category":"physics.optics"}
{"created":"2024-02-16 11:20:05","title":"Order-by-disorder and long-range interactions in the antiferromagnetic transverse-field Ising model on the triangular lattice -- A perturbative point of view","abstract":"We study the low-field ground-state (GS) properties of the antiferromagnetic transverse-field Ising model with long-range interactions (afLRTFIM) on the triangular lattice. We use the method of perturbative continuous unitary transformations (pCUT) to derive an effective model for the degenerate GS space of the antiferromagnetic nearest-neighbour (NN) Ising model on a finite system, by treating the transverse-field (TF) and the long-range interactions (LRI) as a perturbation. We determine a level-crossing between the plain stripe phase at small TF and the clock-ordered phase at intermediate TF at $h\\cong0.129$ for $\\alpha=6$, $N=36$ spins in order three perturbation theory. We discuss the qualitative layout of the quantum phase diagram of the afLRTFIM on the triangular lattice.","sentences":["We study the low-field ground-state (GS) properties of the antiferromagnetic transverse-field Ising model with long-range interactions (afLRTFIM) on the triangular lattice.","We use the method of perturbative continuous unitary transformations (pCUT) to derive an effective model for the degenerate GS space of the antiferromagnetic nearest-neighbour (NN) Ising model on a finite system, by treating the transverse-field (TF) and the long-range interactions (LRI) as a perturbation.","We determine a level-crossing between the plain stripe phase at small TF and the clock-ordered phase at intermediate TF at $h\\cong0.129$ for $\\alpha=6$, $N=36$ spins in order three perturbation theory.","We discuss the qualitative layout of the quantum phase diagram of the afLRTFIM on the triangular lattice."],"url":"http://arxiv.org/abs/2402.10584v1","category":"cond-mat.str-el"}
{"created":"2024-02-16 11:15:12","title":"On the importance of parallel magnetic-field fluctuations for electromagnetic instabilities in STEP","abstract":"This paper discusses the importance of parallel perturbations of the magnetic-field in gyrokinetic simulations of electromagnetic instabilities and turbulence at mid-radius in the burning plasma phase of the conceptual high-${\\beta}$, reactor-scale, tight-aspect-ratio tokamak STEP. Previous studies have revealed the presence of unstable hybrid kinetic ballooning modes (hKBMs) and subdominant microtearing modes (MTMs) at binormal scales approaching the ion Larmor radius. It was found that the hKBM requires the inclusion of parallel magnetic-field perturbations to be linearly unstable. Here, the extent to which the inclusion of fluctuations in the parallel magnetic-field can be relaxed is explored through gyrokinetic simulations. In particular, the frequently used MHD approximation (dropping ${\\delta}\\! B_\\parallel$ and setting the ${\\nabla}B$ drift frequency equal to the curvature drift frequency) is discussed and simulations explore whether this approximation is useful for modelling STEP plasmas. It is shown that the MHD approximation can reproduce some of the linear properties of the full STEP gyrokinetic system, but nonlinear simulations using the MHD approximation result in very different transport states. It is demonstrated that it is difficult to improve the agreement even in conditions that appear to be more compatible with the assumptions that underlie the MHD approximation. Furthermore, it is shown that the sensitivity of STEP to ${\\delta}B_{\\parallel}$ fluctuations is primarily because the plasma sits close to marginality and it is shown that in slightly more strongly driven conditions the hKBM is unstable without ${\\delta}\\! B_{\\parallel}$. Crucially, it is demonstrated that the state of large transport typically predicted by local electromagnetic gyrokinetic simulations of STEP plasmas is not solely due to ${\\delta}\\! B_{\\parallel}$ physics","sentences":["This paper discusses the importance of parallel perturbations of the magnetic-field in gyrokinetic simulations of electromagnetic instabilities and turbulence at mid-radius in the burning plasma phase of the conceptual high-${\\beta}$, reactor-scale, tight-aspect-ratio tokamak STEP.","Previous studies have revealed the presence of unstable hybrid kinetic ballooning modes (hKBMs) and subdominant microtearing modes (MTMs) at binormal scales approaching the ion Larmor radius.","It was found that the hKBM requires the inclusion of parallel magnetic-field perturbations to be linearly unstable.","Here, the extent to which the inclusion of fluctuations in the parallel magnetic-field can be relaxed is explored through gyrokinetic simulations.","In particular, the frequently used MHD approximation (dropping ${\\delta}\\! B_\\parallel$ and setting the ${\\nabla}B$ drift frequency equal to the curvature drift frequency) is discussed and simulations explore whether this approximation is useful for modelling STEP plasmas.","It is shown that the MHD approximation can reproduce some of the linear properties of the full STEP gyrokinetic system, but nonlinear simulations using the MHD approximation result in very different transport states.","It is demonstrated that it is difficult to improve the agreement even in conditions that appear to be more compatible with the assumptions that underlie the MHD approximation.","Furthermore, it is shown that the sensitivity of STEP to ${\\delta}B_{\\parallel}$ fluctuations is primarily because the plasma sits close to marginality and it is shown that in slightly more strongly driven conditions the hKBM is unstable without ${\\delta}\\! B_{\\parallel}$. Crucially, it is demonstrated that the state of large transport typically predicted by local electromagnetic gyrokinetic simulations of STEP plasmas is not solely due to","${\\delta}\\! B_{\\parallel}$ physics"],"url":"http://arxiv.org/abs/2402.10583v1","category":"physics.plasm-ph"}
{"created":"2024-02-16 11:14:22","title":"Deterministic Leader Election for Stationary Programmable Matter with Common Direction","abstract":"Leader Election is an important primitive for programmable matter, since it is often an intermediate step for the solution of more complex problems. Although the leader election problem itself is well studied even in the specific context of programmable matter systems, research on fault tolerant approaches is more limited. We consider the problem in the previously studied Amoebot model on a triangular grid, when the configuration is connected but contains nodes the particles cannot move to (e.g., obstacles). We assume that particles agree on a common direction (i.e., the horizontal axis) but do not have chirality (i.e., they do not agree on the other two directions of the triangular grid). We begin by showing that an election algorithm with explicit termination is not possible in this case, but we provide an implicitly terminating algorithm that elects a unique leader without requiring any movement. These results are in contrast to those in the more common model with chirality but no agreement on directions, where explicit termination is always possible but the number of elected leaders depends on the symmetry of the initial configuration. Solving the problem under the assumption of one common direction allows for a unique leader to be elected in a stationary and deterministic way, which until now was only possible for simply connected configurations under a sequential scheduler.","sentences":["Leader Election is an important primitive for programmable matter, since it is often an intermediate step for the solution of more complex problems.","Although the leader election problem itself is well studied even in the specific context of programmable matter systems, research on fault tolerant approaches is more limited.","We consider the problem in the previously studied Amoebot model on a triangular grid, when the configuration is connected but contains nodes the particles cannot move to (e.g., obstacles).","We assume that particles agree on a common direction (i.e., the horizontal axis) but do not have chirality (i.e., they do not agree on the other two directions of the triangular grid).","We begin by showing that an election algorithm with explicit termination is not possible in this case, but we provide an implicitly terminating algorithm that elects a unique leader without requiring any movement.","These results are in contrast to those in the more common model with chirality but no agreement on directions, where explicit termination is always possible but the number of elected leaders depends on the symmetry of the initial configuration.","Solving the problem under the assumption of one common direction allows for a unique leader to be elected in a stationary and deterministic way, which until now was only possible for simply connected configurations under a sequential scheduler."],"url":"http://arxiv.org/abs/2402.10582v1","category":"cs.DC"}
{"created":"2024-02-16 11:09:13","title":"Nonequilibrium dynamics and entropy production of a trapped colloidal particle in a complex nonreciprocal medium","abstract":"We discuss the two-dimensional motion of a Brownian particle that is confined to a harmonic trap and driven by a shear flow. The surrounding medium induces memory effects modelled by a linear, typically nonreciprocal coupling of the particle coordinates to an auxiliary (hidden) variable. The system's behavior resulting from the microscopic Langevin equations for the three variables is analyzed by means of exact moment equations derived from the Fokker-Planck representation, and numerical Brownian Dynamics (BD) simulations. Increasing the shear rate beyond a critical value we observe, for suitable coupling scenarios with nonreciprocal elements, a transition from a stationary to an instationary state, corresponding to an escape from the trap. We analyze this behavior, analytically and numerically, in terms of the associated moments of the probability distribution, and from the perspective of nonequilibrium thermodynamics. Intriguingly, the entropy production rate remains finite when crossing the stability threshold.","sentences":["We discuss the two-dimensional motion of a Brownian particle that is confined to a harmonic trap and driven by a shear flow.","The surrounding medium induces memory effects modelled by a linear, typically nonreciprocal coupling of the particle coordinates to an auxiliary (hidden) variable.","The system's behavior resulting from the microscopic Langevin equations for the three variables is analyzed by means of exact moment equations derived from the Fokker-Planck representation, and numerical Brownian Dynamics (BD) simulations.","Increasing the shear rate beyond a critical value we observe, for suitable coupling scenarios with nonreciprocal elements, a transition from a stationary to an instationary state, corresponding to an escape from the trap.","We analyze this behavior, analytically and numerically, in terms of the associated moments of the probability distribution, and from the perspective of nonequilibrium thermodynamics.","Intriguingly, the entropy production rate remains finite when crossing the stability threshold."],"url":"http://arxiv.org/abs/2402.10579v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-16 11:04:36","title":"Post-Quantum Cryptography","abstract":"In this survey we propose to cover the prose of post-quantum cryptography over classical cryptography. We talk about the various cryptographic methods that are being practiced to safeguard our information. The future of secure communication is expected to be the implementation of quantum-safe cryptographic systems, and that in the post-quantum era, the development of post-quantum cryptography is essential for ensuring the security of sensitive data.","sentences":["In this survey we propose to cover the prose of post-quantum cryptography over classical cryptography.","We talk about the various cryptographic methods that are being practiced to safeguard our information.","The future of secure communication is expected to be the implementation of quantum-safe cryptographic systems, and that in the post-quantum era, the development of post-quantum cryptography is essential for ensuring the security of sensitive data."],"url":"http://arxiv.org/abs/2402.10576v1","category":"cs.CR"}
{"created":"2024-02-16 11:02:29","title":"LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty","abstract":"Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, achieving better performance. We experiment with both standard NER test sets and noisy social media datasets. LinkNER enhances NER task performance, notably surpassing SOTA models in robustness tests. We also quantitatively analyze the influence of key components like uncertainty estimation methods, LLMs, and in-context learning on diverse NER tasks, offering specific web-related recommendations.","sentences":["Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems.","Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks.","However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition.","As a result, the usability and reliability of NER models in web-related applications are compromised.","Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks.","Furthermore, non-public and large-scale weights make tuning LLMs difficult.","To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, achieving better performance.","We experiment with both standard NER test sets and noisy social media datasets.","LinkNER enhances NER task performance, notably surpassing SOTA models in robustness tests.","We also quantitatively analyze the influence of key components like uncertainty estimation methods, LLMs, and in-context learning on diverse NER tasks, offering specific web-related recommendations."],"url":"http://arxiv.org/abs/2402.10573v1","category":"cs.CL"}
{"created":"2024-02-16 10:52:36","title":"Distribution of centrality measures on undirected random networks via cavity method","abstract":"The Katz centrality of a node in a complex network is a measure of the node's importance as far as the flow of information across the network is concerned. For ensembles of locally tree-like and undirected random graphs, this observable is a random variable. Its full probability distribution is of interest but difficult to handle analytically because of its \"global\" character and its definition in terms of a matrix inverse. Leveraging a fast Gaussian Belief Propagation-cavity algorithm to solve linear systems on a tree-like structure, we show that (i) the Katz centrality of a single instance can be computed recursively in a very fast way, and (ii) the probability $P(K)$ that a random node in the ensemble of undirected random graphs has centrality $K$ satisfies a set of recursive distributional equations, which can be analytically characterized and efficiently solved using a population dynamics algorithm. We test our solution on ensembles of Erd\\H{o}s-R\\'enyi and scale-free networks in the locally tree-like regime, with excellent agreement. The distributions display a crossover between multimodality and unimodality as the mean degree increases, where distinct peaks correspond to the contribution to the centrality coming from nodes of different degrees. We also provide an approximate formula based on a rank-$1$ projection that works well if the network is not too sparse, and we argue that an extension of our method could be efficiently extended to tackle analytical distributions of other centrality measures such as PageRank for directed networks in a transparent and user-friendly way.","sentences":["The Katz centrality of a node in a complex network is a measure of the node's importance as far as the flow of information across the network is concerned.","For ensembles of locally tree-like and undirected random graphs, this observable is a random variable.","Its full probability distribution is of interest but difficult to handle analytically because of its \"global\" character and its definition in terms of a matrix inverse.","Leveraging a fast Gaussian Belief Propagation-cavity algorithm to solve linear systems on a tree-like structure, we show that (i) the Katz centrality of a single instance can be computed recursively in a very fast way, and (ii) the probability $P(K)$ that a random node in the ensemble of undirected random graphs has centrality $K$ satisfies a set of recursive distributional equations, which can be analytically characterized and efficiently solved using a population dynamics algorithm.","We test our solution on ensembles of Erd\\H{o}s-R\\'enyi and scale-free networks in the locally tree-like regime, with excellent agreement.","The distributions display a crossover between multimodality and unimodality as the mean degree increases, where distinct peaks correspond to the contribution to the centrality coming from nodes of different degrees.","We also provide an approximate formula based on a rank-$1$ projection that works well if the network is not too sparse, and we argue that an extension of our method could be efficiently extended to tackle analytical distributions of other centrality measures such as PageRank for directed networks in a transparent and user-friendly way."],"url":"http://arxiv.org/abs/2402.10566v1","category":"physics.soc-ph"}
{"created":"2024-02-16 10:35:18","title":"Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts","abstract":"Aspect-based summarization has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.","sentences":["Aspect-based summarization has seen significant advancements, especially in structured text.","Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge.","Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments.","Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text.","Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5."],"url":"http://arxiv.org/abs/2402.10554v1","category":"cs.CL"}
{"created":"2024-02-16 10:29:25","title":"Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information","abstract":"Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. We also present the design of a treatment recommendation system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial.","sentences":["Cancer remains a global challenge due to its growing clinical and economic burden.","Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies.","Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels.","Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data.","Previous methods to address this problem have used various forms of transfer learning.","However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels.","Further, they do not utilize auxiliary information (like patient survival) for model training.","We address these limitations through a novel transformer based method, which surpasses the performance of state-of-the-art DRP models on benchmark data.","We also present the design of a treatment recommendation system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial."],"url":"http://arxiv.org/abs/2402.10551v1","category":"cs.LG"}
{"created":"2024-02-16 10:11:44","title":"Accretion of primordial H-He atmospheres in mini-Neptunes: the importance of envelope enrichment","abstract":"Out of the more than 5,000 detected exoplanets a considerable number belongs to a category called 'mini-Neptunes'. Interior models of these planets suggest that they have some primordial, H-He dominated atmosphere. As this type of planet does not occur in the solar system, understanding their formation is a key challenge in planet formation theory. Unfortunately, quantifying the H-He, based on their observed mass and radius, is impossible due to the degeneracy of interior models. We explore the effects that different assumptions on planet formation have on the nebular gas accretion rate, particularly by exploring the way in which solid material interacts with the envelope. This allows us to estimate the range of possible post-formation primordial envelopes. Thereby we demonstrate the importance of envelope enrichment on the initial primordial envelope which can be used in evolution models. We apply formation models that include different solid accretion rate prescriptions. Our assumption is that mini-Neptunes form beyond the ice-line and migrate inward after formation, thus we form planets in-situ at 3 and 5 au. We consider that the envelope can be enriched by the accreted solids in the form of water. We study how different assumptions and parameters influence the ratio between the planet's total mass and the fraction of primordial gas. The primordial envelope fractions for small- and intermediate-mass planets (total mass below 15 M$_{\\oplus}$) can range from 0.1% to 50%. Envelope enrichment can lead to higher primordial mass fractions. We find that the solid accretion rate timescale has the largest influence on the primordial envelope size. Primordial gas accretion rates can span many orders of magnitude. Planet formation models need to use a self-consistent gas accretion prescription.","sentences":["Out of the more than 5,000 detected exoplanets a considerable number belongs to a category called 'mini-Neptunes'.","Interior models of these planets suggest that they have some primordial, H-He dominated atmosphere.","As this type of planet does not occur in the solar system, understanding their formation is a key challenge in planet formation theory.","Unfortunately, quantifying the H-He, based on their observed mass and radius, is impossible due to the degeneracy of interior models.","We explore the effects that different assumptions on planet formation have on the nebular gas accretion rate, particularly by exploring the way in which solid material interacts with the envelope.","This allows us to estimate the range of possible post-formation primordial envelopes.","Thereby we demonstrate the importance of envelope enrichment on the initial primordial envelope which can be used in evolution models.","We apply formation models that include different solid accretion rate prescriptions.","Our assumption is that mini-Neptunes form beyond the ice-line and migrate inward after formation, thus we form planets in-situ at 3 and 5 au.","We consider that the envelope can be enriched by the accreted solids in the form of water.","We study how different assumptions and parameters influence the ratio between the planet's total mass and the fraction of primordial gas.","The primordial envelope fractions for small- and intermediate-mass planets (total mass below 15 M$_{\\oplus}$) can range from 0.1% to 50%.","Envelope enrichment can lead to higher primordial mass fractions.","We find that the solid accretion rate timescale has the largest influence on the primordial envelope size.","Primordial gas accretion rates can span many orders of magnitude.","Planet formation models need to use a self-consistent gas accretion prescription."],"url":"http://arxiv.org/abs/2402.10544v1","category":"astro-ph.EP"}
{"created":"2024-02-16 09:54:50","title":"Minimal Constraint Violation Probability in Model Predictive Control for Linear Systems","abstract":"Handling uncertainty in model predictive control comes with various challenges, especially when considering state constraints under uncertainty. Most methods focus on either the conservative approach of robustly accounting for uncertainty or allowing a small probability of constraint violation. In this work, we propose a linear model predictive control approach that minimizes the probability that linear state constraints are violated in the presence of additive uncertainty. This is achieved by first determining a set of inputs that minimize the probability of constraint violation. Then, this resulting set is used to define admissible inputs for the optimal control problem. Recursive feasibility is guaranteed and input-to-state stability is proved under assumptions. Numerical results illustrate the benefits of the proposed model predictive control approach.","sentences":["Handling uncertainty in model predictive control comes with various challenges, especially when considering state constraints under uncertainty.","Most methods focus on either the conservative approach of robustly accounting for uncertainty or allowing a small probability of constraint violation.","In this work, we propose a linear model predictive control approach that minimizes the probability that linear state constraints are violated in the presence of additive uncertainty.","This is achieved by first determining a set of inputs that minimize the probability of constraint violation.","Then, this resulting set is used to define admissible inputs for the optimal control problem.","Recursive feasibility is guaranteed and input-to-state stability is proved under assumptions.","Numerical results illustrate the benefits of the proposed model predictive control approach."],"url":"http://arxiv.org/abs/2402.10538v1","category":"eess.SY"}
{"created":"2024-02-16 09:46:40","title":"Quantifying and combining uncertainty for improving the behavior of Digital Twin Systems","abstract":"Uncertainty is an inherent property of any complex system, especially those that integrate physical parts or operate in real environments. In this paper, we focus on the Digital Twins of adaptive systems, which are particularly complex to design, verify, and optimize. One of the problems of having two systems (the physical one and its digital replica) is that their behavior may not always be consistent. In addition, both twins are normally subject to different types of uncertainties, which complicates their comparison. In this paper we propose the explicit representation and treatment of the uncertainty of both twins, and show how this enables a more accurate comparison of their behaviors. Furthermore, this allows us to reduce the overall system uncertainty and improve its behavior by properly averaging the individual uncertainties of the two twins. An exemplary incubator system is used to illustrate and validate our proposal.","sentences":["Uncertainty is an inherent property of any complex system, especially those that integrate physical parts or operate in real environments.","In this paper, we focus on the Digital Twins of adaptive systems, which are particularly complex to design, verify, and optimize.","One of the problems of having two systems (the physical one and its digital replica) is that their behavior may not always be consistent.","In addition, both twins are normally subject to different types of uncertainties, which complicates their comparison.","In this paper we propose the explicit representation and treatment of the uncertainty of both twins, and show how this enables a more accurate comparison of their behaviors.","Furthermore, this allows us to reduce the overall system uncertainty and improve its behavior by properly averaging the individual uncertainties of the two twins.","An exemplary incubator system is used to illustrate and validate our proposal."],"url":"http://arxiv.org/abs/2402.10535v1","category":"eess.SY"}
{"created":"2024-02-16 09:46:20","title":"Using Left and Right Brains Together: Towards Vision and Language Planning","abstract":"Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks. However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability. In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process. Therefore, we introduce a novel vision-language planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form. Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system. We evaluate the effectiveness of our framework across vision-language tasks, vision-only tasks, and language-only tasks. The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution.","sentences":["Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have demonstrated remarkable decision masking capabilities on a variety of tasks.","However, they inherently operate planning within the language space, lacking the vision and spatial imagination ability.","In contrast, humans utilize both left and right hemispheres of the brain for language and visual planning during the thinking process.","Therefore, we introduce a novel vision-language planning framework in this work to perform concurrent visual and language planning for tasks with inputs of any form.","Our framework incorporates visual planning to capture intricate environmental details, while language planning enhances the logical coherence of the overall system.","We evaluate the effectiveness of our framework across vision-language tasks, vision-only tasks, and language-only tasks.","The results demonstrate the superior performance of our approach, indicating that the integration of visual and language planning yields better contextually aware task execution."],"url":"http://arxiv.org/abs/2402.10534v1","category":"cs.CV"}
{"created":"2024-02-16 09:09:16","title":"Real-Time Model-Based Quantitative Ultrasound and Radar","abstract":"Ultrasound and radar signals are highly beneficial for medical imaging as they are non-invasive and non-ionizing. Traditional imaging techniques have limitations in terms of contrast and physical interpretation. Quantitative medical imaging can display various physical properties such as speed of sound, density, conductivity, and relative permittivity. This makes it useful for a wider range of applications, including improving cancer detection, diagnosing fatty liver, and fast stroke imaging. However, current quantitative imaging techniques that estimate physical properties from received signals, such as Full Waveform Inversion, are time-consuming and tend to converge to local minima, making them unsuitable for medical imaging. To address these challenges, we propose a neural network based on the physical model of wave propagation, which defines the relationship between the received signals and physical properties. Our network can reconstruct multiple physical properties in less than one second for complex and realistic scenarios, using data from only eight elements. We demonstrate the effectiveness of our approach for both radar and ultrasound signals.","sentences":["Ultrasound and radar signals are highly beneficial for medical imaging as they are non-invasive and non-ionizing.","Traditional imaging techniques have limitations in terms of contrast and physical interpretation.","Quantitative medical imaging can display various physical properties such as speed of sound, density, conductivity, and relative permittivity.","This makes it useful for a wider range of applications, including improving cancer detection, diagnosing fatty liver, and fast stroke imaging.","However, current quantitative imaging techniques that estimate physical properties from received signals, such as Full Waveform Inversion, are time-consuming and tend to converge to local minima, making them unsuitable for medical imaging.","To address these challenges, we propose a neural network based on the physical model of wave propagation, which defines the relationship between the received signals and physical properties.","Our network can reconstruct multiple physical properties in less than one second for complex and realistic scenarios, using data from only eight elements.","We demonstrate the effectiveness of our approach for both radar and ultrasound signals."],"url":"http://arxiv.org/abs/2402.10520v1","category":"cs.CV"}
{"created":"2024-02-16 09:03:28","title":"Flat-band engineering of quasi-one-dimensional systems via supersymmetric transformations","abstract":"We introduce a systematic method to spectrally design quasi-one-dimensional crystal models described by the Dirac equation in the low-energy regime. The method is based on the supersymmetric transformation applied to an initially known pseudo-spin-1/2 model. This allows extending the corresponding susy partner so that the new model describes a pseudo-spin-1 system. The spectral design allows the introduction of a flat-band and discrete energies at will into the new model. The results are illustrated in two examples where the Su-Schriefer-Heeger chain is locally converted into a stub lattice.","sentences":["We introduce a systematic method to spectrally design quasi-one-dimensional crystal models described by the Dirac equation in the low-energy regime.","The method is based on the supersymmetric transformation applied to an initially known pseudo-spin-1/2 model.","This allows extending the corresponding susy partner so that the new model describes a pseudo-spin-1 system.","The spectral design allows the introduction of a flat-band and discrete energies at will into the new model.","The results are illustrated in two examples where the Su-Schriefer-Heeger chain is locally converted into a stub lattice."],"url":"http://arxiv.org/abs/2402.10514v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-16 08:41:31","title":"Multiple localized-itinerant dualities in magnetism of 5f electron systems. The case of UPt$_2$Si$_2$","abstract":"The paper deals with the U based compound UPt$_2$Si$_2$ (UPS). The material was first treated as a localized 5f-electron system. Later, an opposite opinion of a predominantly itinerant nature of the system was put forward. The most recent publications treat UPS as a dual material. We suggest a material specific theoretical model based on the density functional theory plus Hubbard $U$ (DFT+$U$) calculations that describes the set of fundamental ground-state properties and high magnetic field experiment. The ground state properties include antiferromagnetic magnetic structure, magnetic easy axis, and the value of the U atomic moment. The in-field experiment shows the presence of a strong metamagnetic transition for the field parallel to the easy axis in contrast to the hard field direction where such a feature is absent. On the other hand, comparable induced magnetization values are obtained for both easy and hard field directions. Within the framework of the suggested model we show that the compound possesses well-formed atomic moments built by electrons treated as delocalized. To understand the experimental high-field properties we estimate exchange energy, magnetic anisotropy energy, and Zeeman energy. All three energies are shown to have comparable values what is crucial for the interpretation of the experiment. At all steps of the study we devote special attention to revealing and emphasizing the dual itinerant-localized properties of the material. The obtained forms of the duality are different: well defined atomic moments formed by the itinerant electrons, interplay of the single-site and two-site anisotropies, strong localization of two of the 5f electrons in contrast to the itinerant nature of the 5f electrons contributing to the states around the Fermi level.","sentences":["The paper deals with the U based compound UPt$_2$Si$_2$ (UPS).","The material was first treated as a localized 5f-electron system.","Later, an opposite opinion of a predominantly itinerant nature of the system was put forward.","The most recent publications treat UPS as a dual material.","We suggest a material specific theoretical model based on the density functional theory plus Hubbard $U$ (DFT+$U$) calculations that describes the set of fundamental ground-state properties and high magnetic field experiment.","The ground state properties include antiferromagnetic magnetic structure, magnetic easy axis, and the value of the U atomic moment.","The in-field experiment shows the presence of a strong metamagnetic transition for the field parallel to the easy axis in contrast to the hard field direction where such a feature is absent.","On the other hand, comparable induced magnetization values are obtained for both easy and hard field directions.","Within the framework of the suggested model we show that the compound possesses well-formed atomic moments built by electrons treated as delocalized.","To understand the experimental high-field properties we estimate exchange energy, magnetic anisotropy energy, and Zeeman energy.","All three energies are shown to have comparable values what is crucial for the interpretation of the experiment.","At all steps of the study we devote special attention to revealing and emphasizing the dual itinerant-localized properties of the material.","The obtained forms of the duality are different: well defined atomic moments formed by the itinerant electrons, interplay of the single-site and two-site anisotropies, strong localization of two of the 5f electrons in contrast to the itinerant nature of the 5f electrons contributing to the states around the Fermi level."],"url":"http://arxiv.org/abs/2402.10507v1","category":"cond-mat.str-el"}
{"created":"2024-02-16 08:34:13","title":"A Survey of Resilient Coordination for Cyber-Physical Systems Against Malicious Attacks","abstract":"Cyber-physical systems (CPSs) facilitate the integration of physical entities and cyber infrastructures through the utilization of pervasive computational resources and communication units, leading to improved efficiency, automation, and practical viability in both academia and industry. Due to its openness and distributed characteristics, a critical issue prevalent in CPSs is to guarantee resilience in presence of malicious attacks. This paper conducts a comprehensive survey of recent advances on resilient coordination for CPSs. Different from existing survey papers, we focus on the node injection attack and propose a novel taxonomy according to the multi-layered framework of CPS. Furthermore, miscellaneous resilient coordination problems are discussed in this survey. Specifically, some preliminaries and the fundamental problem settings are given at the beginning. Subsequently, based on a multi-layered framework of CPSs, promising results of resilient consensus are classified and reviewed from three perspectives: physical structure, communication mechanism, and network topology. Next, two typical application scenarios, i.e., multi-robot systems and smart grids are exemplified to extend resilient consensus to other coordination tasks. Particularly, we examine resilient containment and resilient distributed optimization problems, both of which demonstrate the applicability of resilient coordination approaches. Finally, potential avenues are highlighted for future research.","sentences":["Cyber-physical systems (CPSs) facilitate the integration of physical entities and cyber infrastructures through the utilization of pervasive computational resources and communication units, leading to improved efficiency, automation, and practical viability in both academia and industry.","Due to its openness and distributed characteristics, a critical issue prevalent in CPSs is to guarantee resilience in presence of malicious attacks.","This paper conducts a comprehensive survey of recent advances on resilient coordination for CPSs.","Different from existing survey papers, we focus on the node injection attack and propose a novel taxonomy according to the multi-layered framework of CPS.","Furthermore, miscellaneous resilient coordination problems are discussed in this survey.","Specifically, some preliminaries and the fundamental problem settings are given at the beginning.","Subsequently, based on a multi-layered framework of CPSs, promising results of resilient consensus are classified and reviewed from three perspectives: physical structure, communication mechanism, and network topology.","Next, two typical application scenarios, i.e., multi-robot systems and smart grids are exemplified to extend resilient consensus to other coordination tasks.","Particularly, we examine resilient containment and resilient distributed optimization problems, both of which demonstrate the applicability of resilient coordination approaches.","Finally, potential avenues are highlighted for future research."],"url":"http://arxiv.org/abs/2402.10505v1","category":"eess.SY"}
{"created":"2024-02-16 18:56:41","title":"The Price of Adaptivity in Stochastic Convex Optimization","abstract":"We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a \"price of adaptivity\" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.","sentences":["We prove impossibility results for adaptivity in non-smooth stochastic convex optimization.","Given a set of problem parameters we wish to adapt to, we define a \"price of adaptivity\" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters.","When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality.","When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty.","Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch."],"url":"http://arxiv.org/abs/2402.10898v1","category":"math.OC"}
{"created":"2024-02-16 18:54:47","title":"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter","abstract":"This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance, and stronger scalability. Extensive experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large vision-language models, marking a significant efficiency improvement.","sentences":["This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language models (LLMs).","While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training strategy of vision-language adapters vary significantly across recent works.","Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline.","However, we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision.","To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the vision-language adapter.","Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance, and stronger scalability.","Extensive experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities.","Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large vision-language models, marking a significant efficiency improvement."],"url":"http://arxiv.org/abs/2402.10896v1","category":"cs.CV"}
{"created":"2024-02-16 18:49:27","title":"Proving membership in LLM pretraining data via data watermarks","abstract":"Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B's training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.","sentences":["Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem.","This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release.","By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate.","We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes.","We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test.","Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases.","Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B's training data, as long as they occurred at least 90 times.","Together, our results point towards a promising future for data watermarks in real world use."],"url":"http://arxiv.org/abs/2402.10892v1","category":"cs.CR"}
{"created":"2024-02-16 18:43:39","title":"Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation","abstract":"Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.","sentences":["Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs.","This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations.","The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling.","The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks.","The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet.","This highlights its potential in scenarios with sparse or imprecise annotations.","The source code is made publicly accessible."],"url":"http://arxiv.org/abs/2402.10887v1","category":"eess.IV"}
{"created":"2024-02-16 18:03:42","title":"EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models","abstract":"Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking. This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens. We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits. We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs. Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware supervised and unsupervised baselines.","sentences":["Large Language Models (LLMs) have achieved state-of-the-art performance in text re-ranking.","This process includes queries and candidate passages in the prompts, utilizing pointwise, listwise, and pairwise prompting strategies.","A limitation of these ranking strategies with LLMs is their cost: the process can become expensive due to API charges, which are based on the number of input and output tokens.","We study how to maximize the re-ranking performance given a budget, by navigating the vast search spaces of prompt choices, LLM APIs, and budget splits.","We propose a suite of budget-constrained methods to perform text re-ranking using a set of LLM APIs.","Our most efficient method, called EcoRank, is a two-layered pipeline that jointly optimizes decisions regarding budget allocation across prompt strategies and LLM APIs.","Our experimental results on four popular QA and passage reranking datasets show that EcoRank outperforms other budget-aware supervised and unsupervised baselines."],"url":"http://arxiv.org/abs/2402.10866v1","category":"cs.CL"}
{"created":"2024-02-16 17:28:26","title":"Stressed and sliding ice surfaces liquefy without much heating","abstract":"The low kinetic friction observed between ice or snow and numerous counterbodies is commonly attributed to a thin interfacial water layer [1-3], which is believed to exist because of pressure melting [4], surface melting [5, 6], or friction-induced heating [7]. However, even the currently leading theory of frictional melting keeps being challenged, for example, due to the lack of detectable warming of snow surfaces under a rotating slider at -7{\\deg}C temperature and 1 m/s sliding velocity despite high temporospatial resolution [8]. Here we present molecular simulations of ice interfaces that reveal that ice surfaces liquefy readily without melting thermally but rather by displacement driven amorphization, normal-stress gradients, and tensile in-plane stress. Yet, friction coefficients below 0.01, as observed during the sliding of hydrophobic solids over ice [9], appear possible only when the counterfaces are smooth and allow water to slip past them. Our findings provide fundamental guidelines on how to optimize ice friction and challenge experimentalists to measure the surface temperature of ice and snow at minute scales and with unprecedented speed.","sentences":["The low kinetic friction observed between ice or snow and numerous counterbodies is commonly attributed to a thin interfacial water layer [1-3], which is believed to exist because of pressure melting [4], surface melting [5, 6], or friction-induced heating [7].","However, even the currently leading theory of frictional melting keeps being challenged, for example, due to the lack of detectable warming of snow surfaces under a rotating slider at -7{\\deg}C temperature and 1 m/s sliding velocity despite high temporospatial resolution [8].","Here we present molecular simulations of ice interfaces that reveal that ice surfaces liquefy readily without melting thermally but rather by displacement driven amorphization, normal-stress gradients, and tensile in-plane stress.","Yet, friction coefficients below 0.01, as observed during the sliding of hydrophobic solids over ice","[9], appear possible only when the counterfaces are smooth and allow water to slip past them.","Our findings provide fundamental guidelines on how to optimize ice friction and challenge experimentalists to measure the surface temperature of ice and snow at minute scales and with unprecedented speed."],"url":"http://arxiv.org/abs/2402.10843v1","category":"cond-mat.soft"}
{"created":"2024-02-16 17:22:22","title":"Graded polynomial identities of the infinite-dimensional upper triangular matrices over an arbitrary field","abstract":"We compute the graded polynomial identities of the infinite dimensional upper triangular matrix algebra over an arbitrary field. If the grading group is finite, we prove that the set of graded polynomial identities admits a finite basis. We find conditions under which a grading on such an algebra satisfies a nontrivial graded polynomial identity. Finally, we provide examples showing that two nonisomorphic gradings can have the same set of graded polynomial identities.","sentences":["We compute the graded polynomial identities of the infinite dimensional upper triangular matrix algebra over an arbitrary field.","If the grading group is finite, we prove that the set of graded polynomial identities admits a finite basis.","We find conditions under which a grading on such an algebra satisfies a nontrivial graded polynomial identity.","Finally, we provide examples showing that two nonisomorphic gradings can have the same set of graded polynomial identities."],"url":"http://arxiv.org/abs/2402.10839v1","category":"math.RA"}
{"created":"2024-02-16 17:15:28","title":"Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities","abstract":"Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions.","sentences":["Large language models (LLMs) have been applied in many fields with rapid development in recent years.","As a classic machine learning task, time series forecasting has recently received a boost from LLMs.","However, there is a research gap in the LLMs' preferences in this field.","In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found.","For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity.","We explain our findings through designing prompts to require LLMs to tell the period of the datasets.","In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series.","Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions."],"url":"http://arxiv.org/abs/2402.10835v1","category":"cs.CL"}
{"created":"2024-02-16 17:03:04","title":"Designing a Taxonomy for Smart Tourism Tools","abstract":"Smart tourism (ST) stems from the concepts of e-tourism - focused on the digitalization of processes within the tourism industry, and digital tourism - also considering the digitalization within the tourist experience. The earlier ST references found regard ST Destinations and emerge from the development of Smart Cities.   Our initial literature review on the ST concept and Smart Tourism Tools (STT) revealed significant research uncertainties: ST is poorly defined and frequently linked to the concept of Smart Cities; different authors have different, sometimes contradictory, views on the goals of ST; STT claims are often only based on technological aspects, and their \"smartness\" is difficult to evaluate; often the term \"Smart\" describes developments fueled by cutting-edge technologies, which lose that status after a few years.   This chapter is part of the ongoing initiative to build an online observatory that provides a comprehensive view of STTs' offerings in Europe, known as the European STT Observatory. To achieve this, the observatory requires methodologies and tools to evaluate \"smartness\" based on a sound definition of ST and STT, while also being able to adapt to technological advancements. In this chapter, we present the results of a participatory approach where we invited ST experts from around the world to help us achieve this level of soundness. Our goal is to make a valuable contribution to the discussion on the definition of ST and STT.","sentences":["Smart tourism (ST) stems from the concepts of e-tourism - focused on the digitalization of processes within the tourism industry, and digital tourism - also considering the digitalization within the tourist experience.","The earlier ST references found regard ST Destinations and emerge from the development of Smart Cities.   ","Our initial literature review on the ST concept and Smart Tourism Tools (STT) revealed significant research uncertainties: ST is poorly defined and frequently linked to the concept of Smart Cities; different authors have different, sometimes contradictory, views on the goals of ST; STT claims are often only based on technological aspects, and their \"smartness\" is difficult to evaluate; often the term \"Smart\" describes developments fueled by cutting-edge technologies, which lose that status after a few years.   ","This chapter is part of the ongoing initiative to build an online observatory that provides a comprehensive view of STTs' offerings in Europe, known as the European STT Observatory.","To achieve this, the observatory requires methodologies and tools to evaluate \"smartness\" based on a sound definition of ST and STT, while also being able to adapt to technological advancements.","In this chapter, we present the results of a participatory approach where we invited ST experts from around the world to help us achieve this level of soundness.","Our goal is to make a valuable contribution to the discussion on the definition of ST and STT."],"url":"http://arxiv.org/abs/2402.10830v1","category":"cs.CY"}
{"created":"2024-02-16 16:46:53","title":"Goal-Conditioned Offline Reinforcement Learning via Metric Learning","abstract":"In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.","sentences":["In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning.","To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions.","We study a property for representations to recover optimality and propose a new optimization objective that leads to such property.","We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL.","Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets.","Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks."],"url":"http://arxiv.org/abs/2402.10820v1","category":"cs.LG"}
{"created":"2024-02-16 16:42:09","title":"Trading off Consistency and Dimensionality of Convex Surrogates for the Mode","abstract":"In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the \"correct\" classification, regardless of the data distribution. For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomenon called hallucination occurs, which is when the optimal report under the surrogate loss is an outcome with zero probability. Looking towards application, we derive a result to check if consistency holds under a given polytope embedding and low-noise assumption, providing insight into when to use a particular embedding. We provide examples of embedding $n = 2^{d}$ outcomes into the $d$-dimensional unit cube and $n = d!$ outcomes into the $d$-dimensional permutahedron under low-noise assumptions. Finally, we demonstrate that with multiple problem instances, we can learn the mode with $\\frac{n}{2}$ dimensions over the whole simplex.","sentences":["In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the \"correct\" classification, regardless of the data distribution.","For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable.","We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification.","Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space.","We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomenon called hallucination occurs, which is when the optimal report under the surrogate loss is an outcome with zero probability.","Looking towards application, we derive a result to check if consistency holds under a given polytope embedding and low-noise assumption, providing insight into when to use a particular embedding.","We provide examples of embedding $n = 2^{d}$ outcomes into the $d$-dimensional unit cube and $n = d!$ outcomes into the $d$-dimensional permutahedron under low-noise assumptions.","Finally, we demonstrate that with multiple problem instances, we can learn the mode with $\\frac{n}{2}$ dimensions over the whole simplex."],"url":"http://arxiv.org/abs/2402.10818v1","category":"cs.LG"}
{"created":"2024-02-16 16:35:18","title":"Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning","abstract":"We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover, by embedding the visitation measure into a finite-dimensional space, we can handle large state spaces by incorporating function approximation. Two notable examples are (1) Kernelized Nonlinear Regulators and (2) Low-rank MDPs. We prove that with an optimistic planning oracle, our algorithm achieves sublinear regret and constraint violation in both cases and can attain the globally optimal policy of the original constrained problem.","sentences":["We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint.","Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure.","In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization.","Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent.","Moreover, by embedding the visitation measure into a finite-dimensional space, we can handle large state spaces by incorporating function approximation.","Two notable examples are (1) Kernelized Nonlinear Regulators and (2) Low-rank MDPs.","We prove that with an optimistic planning oracle, our algorithm achieves sublinear regret and constraint violation in both cases and can attain the globally optimal policy of the original constrained problem."],"url":"http://arxiv.org/abs/2402.10810v1","category":"cs.LG"}
{"created":"2024-02-16 16:07:57","title":"An X-ray Synchrotron Shell and a Pulsar: The Peculiar Supernova Remnant G32.4+0.1","abstract":"We present a deep Chandra observation of the shell supernova remnant G32.4+0.1, whose featureless X-ray spectrum has led to its classification as an X-ray synchrotron-dominated supernova remnant (SNR). We find a partial shell morphology whose outline is quite circular, with a radius of about 11 pc at an assumed distance of 11 kpc. Thermal and power-law spectral models for three relatively bright regions provided equally good fits, but the absence of spectral lines required ionization timescales from thermal fits that are inconsistent with mean densities derived from emission measures. We thus confirm the nonthermal, i.e., synchrotron, origin of X-rays from G32.4+0.1. Shock velocities needed to accelerate electrons to the required TeV energies are >~1000 km/s, giving remnant ages <~5,000 -- 9,000 yr. There is no obvious X-ray counterpart to the radio pulsar PSR J1850--0026, but its position adjoins a region of X-ray emission whose spectrum is somewhat harder than that of other regions of the shell, and which may be a pulsar-wind nebula (PWN), though its spectrum is steeper than almost all known X-ray PWNe. The distance of the pulsar from the center of symmetry of the shell disfavors a birth in a supernova event at that location only a few thousand years before: either the pulsar (and putative PWN) are not associated with the shell SNR, requiring a coincidence of both position and (roughly) absorbing column density, or the SNR is much older, making the origin of nonthermal emission problematic.","sentences":["We present a deep Chandra observation of the shell supernova remnant G32.4+0.1, whose featureless X-ray spectrum has led to its classification as an X-ray synchrotron-dominated supernova remnant (SNR).","We find a partial shell morphology whose outline is quite circular, with a radius of about 11 pc at an assumed distance of 11 kpc.","Thermal and power-law spectral models for three relatively bright regions provided equally good fits, but the absence of spectral lines required ionization timescales from thermal fits that are inconsistent with mean densities derived from emission measures.","We thus confirm the nonthermal, i.e., synchrotron, origin of X-rays from G32.4+0.1.","Shock velocities needed to accelerate electrons to the required TeV energies are >~1000 km/s, giving remnant ages <~5,000 -- 9,000 yr.","There is no obvious X-ray counterpart to the radio pulsar PSR J1850--0026, but its position adjoins a region of X-ray emission whose spectrum is somewhat harder than that of other regions of the shell, and which may be a pulsar-wind nebula (PWN), though its spectrum is steeper than almost all known X-ray PWNe.","The distance of the pulsar from the center of symmetry of the shell disfavors a birth in a supernova event at that location only a few thousand years before: either the pulsar (and putative PWN) are not associated with the shell SNR, requiring a coincidence of both position and (roughly) absorbing column density, or the SNR is much older, making the origin of nonthermal emission problematic."],"url":"http://arxiv.org/abs/2402.10785v1","category":"astro-ph.HE"}
{"created":"2024-02-16 15:19:35","title":"Another Body in the World: Flusserian Freedom in Mixed Reality","abstract":"In Flusserian view of media history, humans often misperceive the world projected by media to be the world itself, leading to a loss of freedom. This paper examines Flusserian Freedom in the context of Mixed Reality (MR) and explores how humans can recognize the obscuration of the world within the media (i.e., MR) and understand their relationship. The authors investigate the concept of playing against apparatus and deliberately alienating the perception of the projected world through an artwork titled \"Surrealism Me.\" This artwork enables the user to have another body within MR through interactive and immersive experiences based on the definition of Sense of Embodiment. The purpose of this work is to raise awareness of the domination of media and to approach Flusserian freedom within contemporary technical arrangements.","sentences":["In Flusserian view of media history, humans often misperceive the world projected by media to be the world itself, leading to a loss of freedom.","This paper examines Flusserian Freedom in the context of Mixed Reality (MR) and explores how humans can recognize the obscuration of the world within the media (i.e., MR) and understand their relationship.","The authors investigate the concept of playing against apparatus and deliberately alienating the perception of the projected world through an artwork titled \"Surrealism Me.\"","This artwork enables the user to have another body within MR through interactive and immersive experiences based on the definition of Sense of Embodiment.","The purpose of this work is to raise awareness of the domination of media and to approach Flusserian freedom within contemporary technical arrangements."],"url":"http://arxiv.org/abs/2402.10751v1","category":"cs.CY"}
{"created":"2024-02-16 14:30:12","title":"Conformalized Credal Set Predictors","abstract":"Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method to natural language inference, a highly ambiguous natural language task where it is common to obtain multiple annotations per example.","sentences":["Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution.","In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction.","However, the design of methods for learning credal set predictors remains a challenging problem.","In this paper, we make use of conformal prediction for this purpose.","More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions.","Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution).","We demonstrate the applicability of our method to natural language inference, a highly ambiguous natural language task where it is common to obtain multiple annotations per example."],"url":"http://arxiv.org/abs/2402.10723v1","category":"stat.ML"}
{"created":"2024-02-16 14:00:56","title":"Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion Model with Large Language Models for Machine Translation","abstract":"Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.","sentences":["Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation.","However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators.","In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue.","We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources.","We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios.","We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method."],"url":"http://arxiv.org/abs/2402.10699v1","category":"cs.CL"}
{"created":"2024-02-16 13:49:32","title":"Understanding inner-shell excitations in molecules through spectroscopy of the 4f hole states of YbF","abstract":"Molecules containing a lanthanide atom have sets of electronic states arising from excitation of an inner-shell electron. These states have received little attention, but are thought to play an important role in laser cooling of such molecules and may be a useful resource for testing fundamental physics. We study a series of inner-shell excited states in YbF using resonance-enhanced multi-photon ionisation spectroscopy. We investigate the excited states of lowest energy, 8474, 9013 and 9090 cm$^{-1}$ above the ground state, all corresponding to the configuration 4f$^{13}$6s$^{2}$ ${}^{2}F_{7/2}$ of the Yb$^+$ ion. They are metastable, since they have no electric dipole allowed transitions to the ground state. We also characterize a state at 31050 cm$^{-1}$ that is easily excited from both the ground and metastable states, which makes it especially useful for this spectroscopic study. Finally, we study a state at 48729 cm$^{-1}$, which is above the ionization limit and features strong auto-ionizing resonances that prove useful for efficient detection of the molecules and for identifying the rotational quantum number of each line in the spectrum. We resolve the rotational structures of all these states and find that they can all be described by a very simple model based on Hund's case (c). Our study provides information necessary for laser slowing and magneto-optical trapping of YbF, which is an important species for testing fundamental physics. The metastable states may themselves be important for this application. They are long-lived states in a laser-coolable molecule featuring closely-spaced levels of opposite parity, all of which are desirable properties for measuring the electric dipole moments of electrons and protons, investigating parity violation, and developing molecular lattice clocks as frequency standards and probes of varying fundamental constants.","sentences":["Molecules containing a lanthanide atom have sets of electronic states arising from excitation of an inner-shell electron.","These states have received little attention, but are thought to play an important role in laser cooling of such molecules and may be a useful resource for testing fundamental physics.","We study a series of inner-shell excited states in YbF using resonance-enhanced multi-photon ionisation spectroscopy.","We investigate the excited states of lowest energy, 8474, 9013 and 9090 cm$^{-1}$","above the ground state, all corresponding to the configuration 4f$^{13}$6s$^{2}$","${}^{2}F_{7/2}$ of the Yb$^+$ ion.","They are metastable, since they have no electric dipole allowed transitions to the ground state.","We also characterize a state at 31050 cm$^{-1}$","that is easily excited from both the ground and metastable states, which makes it especially useful for this spectroscopic study.","Finally, we study a state at 48729 cm$^{-1}$, which is above the ionization limit and features strong auto-ionizing resonances that prove useful for efficient detection of the molecules and for identifying the rotational quantum number of each line in the spectrum.","We resolve the rotational structures of all these states and find that they can all be described by a very simple model based on Hund's case (c).","Our study provides information necessary for laser slowing and magneto-optical trapping of YbF, which is an important species for testing fundamental physics.","The metastable states may themselves be important for this application.","They are long-lived states in a laser-coolable molecule featuring closely-spaced levels of opposite parity, all of which are desirable properties for measuring the electric dipole moments of electrons and protons, investigating parity violation, and developing molecular lattice clocks as frequency standards and probes of varying fundamental constants."],"url":"http://arxiv.org/abs/2402.10692v1","category":"physics.atom-ph"}
{"created":"2024-02-16 13:48:06","title":"MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages","abstract":"Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6\\% improvement on average on both Starcoder and ChatGPT (gpt-3.5-turbo).","sentences":["Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the reasoning process.","Currently, PoT primarily uses Python.","However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages.","In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models.","The effectiveness of each language varies depending on the specific scenarios.","Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages.","Experimental results reveal that it significantly outperforms Python Self-Consistency.","Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models.","In particular, MultiPoT achieves more than 4.6\\% improvement on average on both Starcoder and ChatGPT (gpt-3.5-turbo)."],"url":"http://arxiv.org/abs/2402.10691v1","category":"cs.CL"}
{"created":"2024-02-16 13:32:03","title":"Covering a Graph with Minimal Local Sets","abstract":"Local sets, a graph structure invariant under local complementation, have been originally introduced in the context of quantum computing for the study of quantum entanglement within the so-called graph state formalism. A local set in a graph is made of a non-empty set of vertices together with its odd neighborhood. We show that any graph can be covered by minimal local sets, i.e. that every vertex is contained in at least one local set that is minimal by inclusion. More precisely, we introduce an algorithm for finding a minimal local set cover in polynomial time. This result is proved by exploring the link between local sets and cut-rank. We prove some additional results on minimal local sets: we give tight bounds on their size, and we show that there can be exponentially many of them in a graph. Finally, we provide an extension of our definitions and our main result to $q$-multigraphs, the graphical counterpart of quantum qudit graph states.","sentences":["Local sets, a graph structure invariant under local complementation, have been originally introduced in the context of quantum computing for the study of quantum entanglement within the so-called graph state formalism.","A local set in a graph is made of a non-empty set of vertices together with its odd neighborhood.","We show that any graph can be covered by minimal local sets, i.e. that every vertex is contained in at least one local set that is minimal by inclusion.","More precisely, we introduce an algorithm for finding a minimal local set cover in polynomial time.","This result is proved by exploring the link between local sets and cut-rank.","We prove some additional results on minimal local sets: we give tight bounds on their size, and we show that there can be exponentially many of them in a graph.","Finally, we provide an extension of our definitions and our main result to $q$-multigraphs, the graphical counterpart of quantum qudit graph states."],"url":"http://arxiv.org/abs/2402.10678v1","category":"quant-ph"}
{"created":"2024-02-16 13:31:43","title":"Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model","abstract":"We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a BBP-type transition behavior. This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in handling structured tensor data.","sentences":["We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering.","Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard.","A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown.","We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a BBP-type transition behavior.","This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in handling structured tensor data."],"url":"http://arxiv.org/abs/2402.10677v1","category":"stat.ML"}
{"created":"2024-02-16 13:24:33","title":"A note on weak compactness of occupation measures for an absorbing Markov decision process","abstract":"We consider an absorbing Markov decision process with a discrete time parameter with Borel state and action spaces. We prove that the set of occupation measures is compact for the weak topology if and only if the control model is uniformly absorbing.","sentences":["We consider an absorbing Markov decision process with a discrete time parameter with Borel state and action spaces.","We prove that the set of occupation measures is compact for the weak topology if and only if the control model is uniformly absorbing."],"url":"http://arxiv.org/abs/2402.10672v1","category":"math.PR"}
{"created":"2024-02-16 13:14:12","title":"Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift","abstract":"Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through experiments on three medical imaging tasks. Our findings show that post-hoc confidence estimators offer a cost-effective approach to reducing the impacts of distribution shift.","sentences":["Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data.","To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets.","This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest.","In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision.","This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift.","We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through experiments on three medical imaging tasks.","Our findings show that post-hoc confidence estimators offer a cost-effective approach to reducing the impacts of distribution shift."],"url":"http://arxiv.org/abs/2402.10665v1","category":"cs.LG"}
{"created":"2024-02-16 12:27:02","title":"M1 Radiative and spin-nonflip $\u03c0\u03c0$ transitions of $B_c$ states in the Cornell potential model","abstract":"In this paper, we mainly predict the rates of M1 radiative and spin-nonflip $\\pi\\pi$ transitions of $B_{c}$-meson under the non-relativistic Cornell potential model with a screening potential effect. We employ the numerical wave function to determine the M1 radiative transition widths of $B_c$ excited states and utilize the Kuang-Yan proposed method for the spin-nonflip $\\pi\\pi$ transitions among $B_c$ states. Our theoretical results are valuable for studying the M1 radiative and spin-nonflip $\\pi\\pi$ transition processes of $B_c$ states in experiments.","sentences":["In this paper, we mainly predict the rates of M1 radiative and spin-nonflip $\\pi\\pi$ transitions of $B_{c}$-meson under the non-relativistic Cornell potential model with a screening potential effect.","We employ the numerical wave function to determine the M1 radiative transition widths of $B_c$ excited states and utilize the Kuang-Yan proposed method for the spin-nonflip $\\pi\\pi$ transitions among $B_c$ states.","Our theoretical results are valuable for studying the M1 radiative and spin-nonflip $\\pi\\pi$ transition processes of $B_c$ states in experiments."],"url":"http://arxiv.org/abs/2402.10629v1","category":"hep-ph"}
{"created":"2024-02-16 12:21:46","title":"Functional principal component analysis as an alternative to mixed-effect models for describing sparse repeated measures in presence of missing data","abstract":"Analyzing longitudinal data in health studies is challenging due to sparse and error-prone measurements, strong within-individual correlation, missing data and various trajectory shapes. While mixed-effect models (MM) effectively address these challenges, they remain parametric models and may incur computational costs. In contrast, Functional Principal Component Analysis (FPCA) is a non-parametric approach developed for regular and dense functional data that flexibly describes temporal trajectories at a lower computational cost. This paper presents an empirical simulation study evaluating the behaviour of FPCA with sparse and error-prone repeated measures and its robustness under different missing data schemes in comparison with MM. The results show that FPCA is well-suited in the presence of missing at random data caused by dropout, except in scenarios involving most frequent and systematic dropout. Like MM, FPCA fails under missing not at random mechanism. The FPCA was applied to describe the trajectories of four cognitive functions before clinical dementia and contrast them with those of matched controls in a case-control study nested in a population-based aging cohort. The average cognitive declines of future dementia cases showed a sudden divergence from those of their matched controls with a sharp acceleration 5 to 2.5 years prior to diagnosis.","sentences":["Analyzing longitudinal data in health studies is challenging due to sparse and error-prone measurements, strong within-individual correlation, missing data and various trajectory shapes.","While mixed-effect models (MM) effectively address these challenges, they remain parametric models and may incur computational costs.","In contrast, Functional Principal Component Analysis (FPCA) is a non-parametric approach developed for regular and dense functional data that flexibly describes temporal trajectories at a lower computational cost.","This paper presents an empirical simulation study evaluating the behaviour of FPCA with sparse and error-prone repeated measures and its robustness under different missing data schemes in comparison with MM.","The results show that FPCA is well-suited in the presence of missing at random data caused by dropout, except in scenarios involving most frequent and systematic dropout.","Like MM, FPCA fails under missing not at random mechanism.","The FPCA was applied to describe the trajectories of four cognitive functions before clinical dementia and contrast them with those of matched controls in a case-control study nested in a population-based aging cohort.","The average cognitive declines of future dementia cases showed a sudden divergence from those of their matched controls with a sharp acceleration 5 to 2.5 years prior to diagnosis."],"url":"http://arxiv.org/abs/2402.10624v1","category":"stat.ME"}
{"created":"2024-02-16 12:13:19","title":"The Elusive member of the Ti-Al-C MAX family- Ti4AlC3","abstract":"We report here perhaps the first successful synthesis and structural characterization of the n=3 family member of Tin+1AlCn, i. e. Ti4AlC3. X-ray Powder diffraction (XRD) data shows characteristic reflections of from corresponding to reflections from the (002), (004), (006), (008), (100), (102), (104), (0010), (105), (106), (0012), (1011) and (1012) planes at 2{\\theta} =7.640, 15.170, 22.760, 30.50, 350, 37.40 38.30, 39.20, 41.30, 46.220, 55.240, 58.620 and 60.780 (double structure) respectively. Rietveld refinement of the XRD data reveals a phase purity of about 79 % for alpha-Ti4AlC3, 15 % for beta-Ti4AlC3 and the rest mostly that of cubic TiC (6 %). The primary crystal symmetry of the two dominant phases is the hexagonal P63/mmc. The precursors chosen were TiH2, Al metal powder and Carbon powder in a molar ratio of 3:1.2:2, which build the case for an Al-deficient condition. We adopted the pressureless sintering technique at 13500 C with a dwelling time of 4 hours under ultra-high vacuum of 10-7 mbar. The co-existence of trace amount of Ti2AlC at 1350 deg C is proven by the small structure at 2{\\theta}=13.130. No trace of oxides like Al2O3 or TiO2 was found in the end product. The line profile width of XRD data indicates average grain size of the order of micro meter. The Scanning Electron Microscopy images show highly lamellar stacked growth of almost a pure MAX (alpha or beta) phase and grain size of micron order, agreeing well with the XRD data.","sentences":["We report here perhaps the first successful synthesis and structural characterization of the n=3 family member of Tin+1AlCn, i. e. Ti4AlC3. X-ray Powder diffraction (XRD) data shows characteristic reflections of from corresponding to reflections from the (002), (004), (006), (008), (100), (102), (104), (0010), (105), (106), (0012), (1011) and (1012) planes at 2{\\theta} =7.640, 15.170, 22.760, 30.50, 350, 37.40 38.30, 39.20, 41.30, 46.220, 55.240, 58.620 and 60.780 (double structure) respectively.","Rietveld refinement of the XRD data reveals a phase purity of about 79 % for alpha-Ti4AlC3, 15 % for beta-Ti4AlC3 and the rest mostly that of cubic TiC (6 %).","The primary crystal symmetry of the two dominant phases is the hexagonal P63/mmc.","The precursors chosen were TiH2, Al metal powder and Carbon powder in a molar ratio of 3:1.2:2, which build the case for an Al-deficient condition.","We adopted the pressureless sintering technique at 13500 C with a dwelling time of 4 hours under ultra-high vacuum of 10-7 mbar.","The co-existence of trace amount of Ti2AlC at 1350 deg C is proven by the small structure at 2{\\theta}=13.130.","No trace of oxides like Al2O3 or TiO2 was found in the end product.","The line profile width of XRD data indicates average grain size of the order of micro meter.","The Scanning Electron Microscopy images show highly lamellar stacked growth of almost a pure MAX (alpha or beta) phase and grain size of micron order, agreeing well with the XRD data."],"url":"http://arxiv.org/abs/2402.10621v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-16 11:54:34","title":"Spanning Matrices via Satisfiability Solving","abstract":"We propose a new encoding of the first-order connection method as a Boolean satisfiability problem. The encoding eschews tree-like presentations of the connection method in favour of matrices, as we show that tree-like calculi have a number of drawbacks in the context of satisfiability solving. The matrix setting permits numerous global refinements of the basic connection calculus. We also show that a suitably-refined calculus is a decision procedure for the Bernays-Sch\\\"onfinkel class.","sentences":["We propose a new encoding of the first-order connection method as a Boolean satisfiability problem.","The encoding eschews tree-like presentations of the connection method in favour of matrices, as we show that tree-like calculi have a number of drawbacks in the context of satisfiability solving.","The matrix setting permits numerous global refinements of the basic connection calculus.","We also show that a suitably-refined calculus is a decision procedure for the Bernays-Sch\\\"onfinkel class."],"url":"http://arxiv.org/abs/2402.10610v1","category":"cs.LO"}
{"created":"2024-02-16 11:43:17","title":"Explicit formula and quasicrystal definition","abstract":"We show that the Riemann hypothesis is true if and only if the measure $$\\mu=-\\sum_{n=1}^\\infty\\frac{\\Lambda(n)}{\\sqrt{n}}(\\delta_{\\log n}+\\delta_{-\\log n})+2\\cosh(x/2)\\,dx$$ is a tempered distribution. In this case it is the Fourier transform of another measure $$\\mathcal{F}\\Bigl(\\sum_{\\gamma}\\delta_{\\gamma/2\\pi}-2\\vartheta'(2\\pi t)\\,dt\\Bigr)=\\mu.$$ We propose a definition of Fourier quasi-crystal to make sense of Dyson suggestion.","sentences":["We show that the Riemann hypothesis is true if and only if the measure $$\\mu=-\\sum_{n=1}^\\infty\\frac{\\Lambda(n)}{\\sqrt{n}}(\\delta_{\\log n}+\\delta_{-\\log n})+2\\cosh(x/2)\\,dx$$ is a tempered distribution.","In this case it is the Fourier transform of another measure $$\\mathcal{F}\\Bigl(\\sum_{\\gamma}\\delta_{\\gamma/2\\pi}-2\\vartheta'(2\\pi t)\\,dt\\Bigr)=\\mu.$$ We propose a definition of Fourier quasi-crystal to make sense of Dyson suggestion."],"url":"http://arxiv.org/abs/2402.10604v1","category":"math.NT"}
{"created":"2024-02-16 10:51:07","title":"Advanced Receiver Autonomous Integrity Monitoring: Impact of Time-Correlated Pseudorange Measurement Noise","abstract":"The paper deals with the allocation of the probability of false alert within the advanced receiver integrity monitoring method. Namely, the stress is laid on the correct computation of the probability of false alert per sample under assumption of time-correlated pseudorange noise. Detailed analysis of the dependence of the probability of false alert per sample on the measurement noise time constant is given and a numerical algorithm for the correct computation of the probability is proposed. The algorithm is illustrated using a numerical example.","sentences":["The paper deals with the allocation of the probability of false alert within the advanced receiver integrity monitoring method.","Namely, the stress is laid on the correct computation of the probability of false alert per sample under assumption of time-correlated pseudorange noise.","Detailed analysis of the dependence of the probability of false alert per sample on the measurement noise time constant is given and a numerical algorithm for the correct computation of the probability is proposed.","The algorithm is illustrated using a numerical example."],"url":"http://arxiv.org/abs/2402.10565v1","category":"eess.SP"}
{"created":"2024-02-16 10:36:38","title":"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention","abstract":"Leveraging users' long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance user profiling by exploiting large language model (LLM) to extract global interests from user engagement history. Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing state-of-the-art (SoTA) methods.","sentences":["Leveraging users' long engagement histories is essential for personalized content recommendations.","The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks.","However, existing works still struggle with processing very long user historical text and insufficient user-item interaction.","In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history.","It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user's history in a session-based manner.","The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment.","Moreover, we enhance user profiling by exploiting large language model (LLM) to extract global interests from user engagement history.","Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing state-of-the-art (SoTA) methods."],"url":"http://arxiv.org/abs/2402.10555v1","category":"cs.IR"}
{"created":"2024-02-16 10:35:01","title":"A novel integrated industrial approach with cobots in the age of industry 4.0 through conversational interaction and computer vision","abstract":"From robots that replace workers to robots that serve as helpful colleagues, the field of robotic automation is experiencing a new trend that represents a huge challenge for component manufacturers. The contribution starts from an innovative vision that sees an ever closer collaboration between Cobot, able to do a specific physical job with precision, the AI world, able to analyze information and support the decision-making process, and the man able to have a strategic vision of the future.","sentences":["From robots that replace workers to robots that serve as helpful colleagues, the field of robotic automation is experiencing a new trend that represents a huge challenge for component manufacturers.","The contribution starts from an innovative vision that sees an ever closer collaboration between Cobot, able to do a specific physical job with precision, the AI world, able to analyze information and support the decision-making process, and the man able to have a strategic vision of the future."],"url":"http://arxiv.org/abs/2402.10553v1","category":"cs.RO"}
{"created":"2024-02-16 10:21:47","title":"Cognitive Personalized Search Integrating Large Language Models with an Efficient Memory Mechanism","abstract":"Traditional search engines usually provide identical search results for all users, overlooking individual preferences. To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs. Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges. This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition. CoPS employs LLMs to enhance user modeling and user search experience. The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions. CoPS handles new queries using a three-step approach: identifying re-finding behaviors, constructing user profiles with relevant historical information, and ranking documents based on personalized query intent. Experiments show that CoPS outperforms baseline models in zero-shot scenarios.","sentences":["Traditional search engines usually provide identical search results for all users, overlooking individual preferences.","To counter this limitation, personalized search has been developed to re-rank results based on user preferences derived from query logs.","Deep learning-based personalized search methods have shown promise, but they rely heavily on abundant training data, making them susceptible to data sparsity challenges.","This paper proposes a Cognitive Personalized Search (CoPS) model, which integrates Large Language Models (LLMs) with a cognitive memory mechanism inspired by human cognition.","CoPS employs LLMs to enhance user modeling and user search experience.","The cognitive memory mechanism comprises sensory memory for quick sensory responses, working memory for sophisticated cognitive responses, and long-term memory for storing historical interactions.","CoPS handles new queries using a three-step approach: identifying re-finding behaviors, constructing user profiles with relevant historical information, and ranking documents based on personalized query intent.","Experiments show that CoPS outperforms baseline models in zero-shot scenarios."],"url":"http://arxiv.org/abs/2402.10548v1","category":"cs.IR"}
{"created":"2024-02-16 09:47:55","title":"Quantifying Individual Risk for Binary Outcome: Bounds and Inference","abstract":"Understanding treatment heterogeneity is crucial for reliable decision-making in treatment evaluation and selection. While the conditional average treatment effect (CATE) is commonly used to capture treatment heterogeneity induced by covariates and design individualized treatment policies, it remains an averaging metric within subpopulations. This limitation prevents it from unveiling individual-level risks, potentially leading to misleading results. This article addresses this gap by examining individual risk for binary outcomes, specifically focusing on the fraction negatively affected (FNA) conditional on covariates -- a metric assessing the percentage of individuals experiencing worse outcomes with treatment compared to control. Under the strong ignorability assumption, FNA is unidentifiable, and we find that previous bounds are wide and practically unattainable except in certain degenerate cases. By introducing a plausible positive correlation assumption for the potential outcomes, we obtain significantly improved bounds compared to previous studies. We show that even with a positive and statistically significant CATE, the lower bound on FNA can be positive, i.e., in the best-case scenario many units will be harmed if receiving treatment. We establish a nonparametric sensitivity analysis framework for FNA using the Pearson correlation coefficient as the sensitivity parameter, thereby exploring the relationships among the correlation coefficient, FNA, and CATE. We also present a practical and tractable method for selecting the range of correlation coefficients. Furthermore, we propose flexible estimators for refined FNA bounds and prove their consistency and asymptotic normality.","sentences":["Understanding treatment heterogeneity is crucial for reliable decision-making in treatment evaluation and selection.","While the conditional average treatment effect (CATE) is commonly used to capture treatment heterogeneity induced by covariates and design individualized treatment policies, it remains an averaging metric within subpopulations.","This limitation prevents it from unveiling individual-level risks, potentially leading to misleading results.","This article addresses this gap by examining individual risk for binary outcomes, specifically focusing on the fraction negatively affected (FNA) conditional on covariates -- a metric assessing the percentage of individuals experiencing worse outcomes with treatment compared to control.","Under the strong ignorability assumption, FNA is unidentifiable, and we find that previous bounds are wide and practically unattainable except in certain degenerate cases.","By introducing a plausible positive correlation assumption for the potential outcomes, we obtain significantly improved bounds compared to previous studies.","We show that even with a positive and statistically significant CATE, the lower bound on FNA can be positive, i.e., in the best-case scenario many units will be harmed if receiving treatment.","We establish a nonparametric sensitivity analysis framework for FNA using the Pearson correlation coefficient as the sensitivity parameter, thereby exploring the relationships among the correlation coefficient, FNA, and CATE.","We also present a practical and tractable method for selecting the range of correlation coefficients.","Furthermore, we propose flexible estimators for refined FNA bounds and prove their consistency and asymptotic normality."],"url":"http://arxiv.org/abs/2402.10537v1","category":"stat.ME"}
{"created":"2024-02-16 09:29:38","title":"Zero-shot sampling of adversarial entities in biomedical question answering","abstract":"The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks successfully manipulate token-wise Shapley value explanations, which become deceptive in the adversarial setting. Our investigations illustrate the brittleness of domain knowledge in LLMs and reveal a shortcoming of standard evaluations for high-capacity models.","sentences":["The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications.","In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use.","The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings.","Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors.","We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics.","Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics.","Moreover, we show that the attacks successfully manipulate token-wise Shapley value explanations, which become deceptive in the adversarial setting.","Our investigations illustrate the brittleness of domain knowledge in LLMs and reveal a shortcoming of standard evaluations for high-capacity models."],"url":"http://arxiv.org/abs/2402.10527v1","category":"cs.CL"}
{"created":"2024-02-16 16:35:02","title":"A lattice Boltzmann method for non-Newtonian blood flow in coiled intracranial aneurysms","abstract":"Intracranial aneurysms are the leading cause of stroke. One of the established treatment approaches is the embolization induced by coil insertion. However, the prediction of treatment and subsequent changed flow characteristics in the aneurysm, is still an open problem. In this work, we present an approach based on patient specific geometry and parameters including a coil representation as inhomogeneous porous medium. The model consists of the volume-averaged Navier-Stokes equations including the non-Newtonian blood rheology. We solve these equations using a problem-adapted lattice Boltzmann method and present a comparison between fully-resolved and volume-averaged simulations. The results indicate the validity of the model. Overall, this workflow allows for patient specific assessment of the flow due to potential treatment.","sentences":["Intracranial aneurysms are the leading cause of stroke.","One of the established treatment approaches is the embolization induced by coil insertion.","However, the prediction of treatment and subsequent changed flow characteristics in the aneurysm, is still an open problem.","In this work, we present an approach based on patient specific geometry and parameters including a coil representation as inhomogeneous porous medium.","The model consists of the volume-averaged Navier-Stokes equations including the non-Newtonian blood rheology.","We solve these equations using a problem-adapted lattice Boltzmann method and present a comparison between fully-resolved and volume-averaged simulations.","The results indicate the validity of the model.","Overall, this workflow allows for patient specific assessment of the flow due to potential treatment."],"url":"http://arxiv.org/abs/2402.10809v1","category":"math.NA"}
{"created":"2024-02-16 14:07:00","title":"Application of an adaptive model hierarchy to parametrized optimal control problems","abstract":"In this contribution we apply an adaptive model hierarchy, consisting of a full-order model, a reduced basis reduced order model, and a machine learning surrogate, to parametrized linear-quadratic optimal control problems. The involved reduced order models are constructed adaptively and are called in such a way that the model hierarchy returns an approximate solution of given accuracy for every parameter value. At the same time, the fastest model of the hierarchy is evaluated whenever possible and slower models are only queried if the faster ones are not sufficiently accurate. The performance of the model hierarchy is studied for a parametrized heat equation example with boundary value control.","sentences":["In this contribution we apply an adaptive model hierarchy, consisting of a full-order model, a reduced basis reduced order model, and a machine learning surrogate, to parametrized linear-quadratic optimal control problems.","The involved reduced order models are constructed adaptively and are called in such a way that the model hierarchy returns an approximate solution of given accuracy for every parameter value.","At the same time, the fastest model of the hierarchy is evaluated whenever possible and slower models are only queried if the faster ones are not sufficiently accurate.","The performance of the model hierarchy is studied for a parametrized heat equation example with boundary value control."],"url":"http://arxiv.org/abs/2402.10708v1","category":"math.OC"}
{"created":"2024-02-16 14:03:19","title":"Asymptotic analysis of mixing in stratified turbulent flows, and the conditions for an inertial sub-range","abstract":"In an important study, Maffioli et al. (J. Fluid Mech., Vol. 794 , 2016) used a scaling analysis to predict that in the weakly stratified flow regime $Fr_h\\gg1$ ($Fr_h$ is the horizontal Froude number), the mixing coefficient $\\Gamma$ (defined as the ratio of the dissipation rates of potential to kinetic energy) scales as $\\Gamma\\sim O(Fr_h^{-2})$. Direct numerical simulations confirmed this result, and also indicated that for the strongly stratified regime $Fr_h\\ll 1$, $\\Gamma\\sim O(1)$. Furthermore, the study argued that $\\Gamma$ does not depend on the buoyancy Reynolds number $Re_b$, but only on $Fr_h$. We present an asymptotic analysis to predict theoretically how $\\Gamma$ should behave for $Fr_h\\ll1$ and $Fr_h\\gg1$ in the limit $Re_b\\to\\infty$. To correctly handle the singular limit $Re_b\\to\\infty$ we perform the asymptotic analysis on the filtered Boussinesq-Navier-Stokes equations, and demonstrate the precise sense in which the inviscid scaling analysis of Billant \\& Chomaz (Phys. Fluids, vol. 13, 1645-1651, 2001) applies to viscous flows with $Re_b\\to\\infty$. The analysis yields $\\Gamma\\sim O(Fr_h^{-2}(1+Fr_h^{-2}))$ for $Fr_h\\gg1$ and $\\Gamma\\sim O(1+Fr_h^{2})$ for $Fr_h\\ll 1$, providing a theoretical basis for the numerical observation made by Maffioli et al, as well as predicting the sub-leading behavior. Our analysis also shows that the Ozmidov scale $L_O$ does not describe the scale below which buoyancy forces are sub-leading, which is instead given by $O(Fr_h^{1/2} L_O)$, and that the condition for there to be an inertial sub-range when $Fr_h\\ll 1$ is not $Re_b\\gg1$, but the more restrictive condition $Re_b\\gg Fr_h^{-4/3}$.","sentences":["In an important study, Maffioli et al.","(J. Fluid Mech., Vol. 794 , 2016) used a scaling analysis to predict that in the weakly stratified flow regime $Fr_h\\gg1$ ($Fr_h$ is the horizontal Froude number), the mixing coefficient $\\Gamma$ (defined as the ratio of the dissipation rates of potential to kinetic energy) scales as $\\Gamma\\sim O(Fr_h^{-2})$. Direct numerical simulations confirmed this result, and also indicated that for the strongly stratified regime $Fr_h\\ll 1$, $\\Gamma\\sim O(1)$.","Furthermore, the study argued that $\\Gamma$ does not depend on the buoyancy Reynolds number $Re_b$, but only on $Fr_h$. We present an asymptotic analysis to predict theoretically how $\\Gamma$ should behave for $Fr_h\\ll1$ and $Fr_h\\gg1$ in the limit $Re_b\\to\\infty$. To correctly handle the singular limit $Re_b\\to\\infty$ we perform the asymptotic analysis on the filtered Boussinesq-Navier-Stokes equations, and demonstrate the precise sense in which the inviscid scaling analysis of Billant \\& Chomaz (Phys. Fluids, vol.","13, 1645-1651, 2001) applies to viscous flows with $Re_b\\to\\infty$. The analysis yields $\\Gamma\\sim O(Fr_h^{-2}(1+Fr_h^{-2}))$ for $Fr_h\\gg1$ and $\\Gamma\\sim O(1+Fr_h^{2})$ for $Fr_h\\ll 1$, providing a theoretical basis for the numerical observation made by Maffioli et al, as well as predicting the sub-leading behavior.","Our analysis also shows that the Ozmidov scale $L_O$ does not describe the scale below which buoyancy forces are sub-leading, which is instead given by $O(Fr_h^{1/2} L_O)$, and that the condition for there to be an inertial sub-range when","$Fr_h\\ll 1$ is not $Re_b\\gg1$, but the more restrictive condition $Re_b\\gg Fr_h^{-4/3}$."],"url":"http://arxiv.org/abs/2402.10704v1","category":"physics.flu-dyn"}
{"created":"2024-02-16 12:39:10","title":"Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning","abstract":"Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures' generalizability. All source code will be published.","sentences":["Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once.","Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency.","However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored.","Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation.","We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures' generalizability.","All source code will be published."],"url":"http://arxiv.org/abs/2402.10639v1","category":"cs.CL"}
{"created":"2024-02-16 12:12:17","title":"The Very Early Soft X-ray Plateau of GRB 230307A: Signature of an Evolving Radiative Efficiency in Magnetar Wind Dissipation?","abstract":"Very recently, a particularly long gamma-ray burst (GRB) 230307A was reported and proposed to originate from a compact binary merger based on its host galaxy property, kilonova, and heavy elements. More intriguingly, a very early plateau followed by a rapid decline in soft X-ray band was detected in its light curve by the Lobster Eye Imager for Astronomy, indicating strong evidence of the existence of a magnetar as the merger product. This work explores that the Magnetar Wind Internal Gradual MAgnetic Dissipation (MIGMAD) model, in which the radiative efficiency evolves over time, successfully fits it to the observed data. Our results reinforce the notion that the X-ray plateau serves as a powerful indicator of a magnetar and imply that an evolving efficiency is likely to be a common feature in X-ray plateaus of GRB afterglows. In addition, we also discuss the explanations for the prompt emission, GRB afterglows, as well as kilonova, and predict possible kilonova afterglows in a magnetar central engine.","sentences":["Very recently, a particularly long gamma-ray burst (GRB) 230307A was reported and proposed to originate from a compact binary merger based on its host galaxy property, kilonova, and heavy elements.","More intriguingly, a very early plateau followed by a rapid decline in soft X-ray band was detected in its light curve by the Lobster Eye Imager for Astronomy, indicating strong evidence of the existence of a magnetar as the merger product.","This work explores that the Magnetar Wind Internal Gradual MAgnetic Dissipation (MIGMAD) model, in which the radiative efficiency evolves over time, successfully fits it to the observed data.","Our results reinforce the notion that the X-ray plateau serves as a powerful indicator of a magnetar and imply that an evolving efficiency is likely to be a common feature in X-ray plateaus of GRB afterglows.","In addition, we also discuss the explanations for the prompt emission, GRB afterglows, as well as kilonova, and predict possible kilonova afterglows in a magnetar central engine."],"url":"http://arxiv.org/abs/2402.10619v1","category":"astro-ph.HE"}
{"created":"2024-02-16 12:04:39","title":"Credential Control Balance: A Universal Blockchain Account Model Abstract From Bank to Bitcoin, Ethereum External Owned Account and Account Abstraction","abstract":"Blockchain market value peaked at $3 trillion, fell to $1 trillion, then recovered to $1.5 trillion and is rising again. Blockchain accounts secure most on-chain assets in this huge market (Web-12). This paper initiates a universal blockchain account model from a comprehensive review of blockchain account development, encompassing both academic and industry perspectives. This paper uses a model analysis method to analysis the account progress and create high level new account model. And it uses systematic literature review method to search, filter, analysis and evaluate the papers about account models and analyzes related technology trade-offs. Searching with key words: blockchain, account, private key and security in WOS, Scopus and Bitcoin and Ethereum community repositories, this research provides in-depth insights into the design and evaluation of account models, from traditional bank accounts to Bitcoin, EVM-adaptable, and abstraction accounts. Through data-driven comparisons of account models (security, cost, adoption), this study also explores future directions and provides an overview of cross-model account theory, guiding further blockchain research. This paper leaves deeper dives into model change drivers, application technology advancements.","sentences":["Blockchain market value peaked at $3 trillion, fell to $1 trillion, then recovered to $1.5 trillion and is rising again.","Blockchain accounts secure most on-chain assets in this huge market (Web-12).","This paper initiates a universal blockchain account model from a comprehensive review of blockchain account development, encompassing both academic and industry perspectives.","This paper uses a model analysis method to analysis the account progress and create high level new account model.","And it uses systematic literature review method to search, filter, analysis and evaluate the papers about account models and analyzes related technology trade-offs.","Searching with key words: blockchain, account, private key and security in WOS, Scopus and Bitcoin and Ethereum community repositories, this research provides in-depth insights into the design and evaluation of account models, from traditional bank accounts to Bitcoin, EVM-adaptable, and abstraction accounts.","Through data-driven comparisons of account models (security, cost, adoption), this study also explores future directions and provides an overview of cross-model account theory, guiding further blockchain research.","This paper leaves deeper dives into model change drivers, application technology advancements."],"url":"http://arxiv.org/abs/2402.10616v1","category":"cs.CR"}
{"created":"2024-02-16 11:27:48","title":"Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification","abstract":"Practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population. Currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter. This paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes. We then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature. This unification also uncovers novel insights. For example, the theory reveals that familiar algorithms, like the recently proposed top-two Thompson sampling algorithm, can be adapted to optimize a broad class of objectives by simply adjusting a single scalar parameter. In addition, the theory reveals that enormous reductions in experiment duration can sometimes be achieved with minimal impact on both within-experiment and post-experiment regret.","sentences":["Practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population.","Currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter.","This paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes.","We then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature.","This unification also uncovers novel insights.","For example, the theory reveals that familiar algorithms, like the recently proposed top-two Thompson sampling algorithm, can be adapted to optimize a broad class of objectives by simply adjusting a single scalar parameter.","In addition, the theory reveals that enormous reductions in experiment duration can sometimes be achieved with minimal impact on both within-experiment and post-experiment regret."],"url":"http://arxiv.org/abs/2402.10592v1","category":"cs.LG"}
{"created":"2024-02-16 18:12:05","title":"Hyperbolic groups and spherical minimal surfaces","abstract":"Let $M$ be a closed, oriented, negatively curved, $n$-dimensional manifold with fundamental group $\\Gamma$. Let $S^\\infty$ be the unit sphere in $\\ell^2(\\Gamma)$, on which $\\Gamma$ acts by the regular representation. The spherical volume of $M$ is a topological invariant introduced by Besson-Courtois-Gallot. We show that it is equal to the area of an $n$-dimensional area-minimizing minimal surface inside the ultralimit of $S^\\infty/\\Gamma$, in the sense of Ambrosio-Kirchheim. Our proof combines the theory of metric currents with a study of limits of the regular representation of torsion-free hyperbolic groups.","sentences":["Let $M$ be a closed, oriented, negatively curved, $n$-dimensional manifold with fundamental group $\\Gamma$. Let $S^\\infty$ be the unit sphere in $\\ell^2(\\Gamma)$, on which $\\Gamma$ acts by the regular representation.","The spherical volume of $M$ is a topological invariant introduced by Besson-Courtois-Gallot.","We show that it is equal to the area of an $n$-dimensional area-minimizing minimal surface inside the ultralimit of $S^\\infty/\\Gamma$, in the sense of Ambrosio-Kirchheim.","Our proof combines the theory of metric currents with a study of limits of the regular representation of torsion-free hyperbolic groups."],"url":"http://arxiv.org/abs/2402.10869v1","category":"math.DG"}
{"created":"2024-02-16 18:03:44","title":"On the quantum differential equations for a family of non-K\u00e4hler monotone symplectic manifolds","abstract":"In this paper we prove Gamma Conjecture $1$ for twistor bundles of hyperbolic $6$ manifolds, which are monotone symplectic manifolds which admit no K\\\"ahler structure. The proof involves a direct computation of the $J$-function, and a version of Laplace's method for estimating power series (as opposed to integrals). This method allows us to rephrase Gamma Conjecture $1$ in certain situations to an Ap\\'ery-like discrete limit. We use this to give a simple proof of Gamma Conjecture $1$ for projective spaces. Additionally we show that the quantum connections of the twistor bundles we consider have unramified exponential type.","sentences":["In this paper we prove Gamma Conjecture $1$ for twistor bundles of hyperbolic $6$ manifolds, which are monotone symplectic manifolds which admit no K\\\"ahler structure.","The proof involves a direct computation of the $J$-function, and a version of Laplace's method for estimating power series (as opposed to integrals).","This method allows us to rephrase Gamma Conjecture $1$ in certain situations to an Ap\\'ery-like discrete limit.","We use this to give a simple proof of Gamma Conjecture $1$ for projective spaces.","Additionally we show that the quantum connections of the twistor bundles we consider have unramified exponential type."],"url":"http://arxiv.org/abs/2402.10867v1","category":"math.SG"}
{"created":"2024-02-16 18:00:04","title":"Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection","abstract":"Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality. The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data. Despite strides in federated learning for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications. In this paper, we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency. To accomplish this, we integrate federated learning with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) transfer learning, employing a pre-trained universal model to adeptly address issues of data imbalance and insufficiency. We evaluate the framework by a case study on stress detection, employing a dataset of physiological and contextual data from a longitudinal study. Our finding show that the proposed approach can attain a 10% boost in accuracy and a 21% enhancement in recall, while ensuring privacy protection.","sentences":["Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality.","The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data.","Despite strides in federated learning for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications.","In this paper, we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency.","To accomplish this, we integrate federated learning with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) transfer learning, employing a pre-trained universal model to adeptly address issues of data imbalance and insufficiency.","We evaluate the framework by a case study on stress detection, employing a dataset of physiological and contextual data from a longitudinal study.","Our finding show that the proposed approach can attain a 10% boost in accuracy and a 21% enhancement in recall, while ensuring privacy protection."],"url":"http://arxiv.org/abs/2402.10862v1","category":"cs.LG"}
{"created":"2024-02-16 17:44:47","title":"Testing analytical methods to derive the cosmic-ray ionisation rate in cold regions via synthetic observations","abstract":"Cosmic rays (CRs) heavily impact the chemistry and physics of cold and dense star-forming regions. However, characterising their ionisation rate is still challenging from an observational point of view. In the past, a few analytical formulas have been proposed to infer the cosmic-ray ionization rate $\\zeta_2$ from molecular line observations. These have been derived from the chemical kinetics of the involved species, but they have not been validated using synthetic data processed with a standard observative pipeline. We aim to bridge this gap. We perform the radiative transfer on a set of three-dimensional magneto-hydrodynamical simulations of prestellar cores, exploring different initial $\\zeta_2$, evolutionary stages, types of radiative transfer (e.g. assuming local-thermodynamic-equilibrium conditions), and telescope responses. We then compute the column densities of the involved tracers to determine $\\zeta_2$, using, in particular, the equation proposed by Bovino et. al (2020) and by Caselli et al. (1998) both used nowadays. Our results confirm that the method of Bovino et al. (2020) accurately retrieves the actual $\\zeta_2$ within a factor of $2-3$, in the physical conditions explored in our tests. Since we also explore a non-local thermodynamic equilibrium radiative transfer, this work indirectly offers insights into the excitation temperatures of common transitions at moderate volume densities ($n\\approx 10^5 \\, \\rm cm^{-3}$). We have also performed a few tests using the formula proposed by Caselli et al. (1998), which overestimates the actual $\\zeta_2$ by at least two orders of magnitudes. We also consider a new derivation of this method, which, however, still leads to large overestimates.","sentences":["Cosmic rays (CRs) heavily impact the chemistry and physics of cold and dense star-forming regions.","However, characterising their ionisation rate is still challenging from an observational point of view.","In the past, a few analytical formulas have been proposed to infer the cosmic-ray ionization rate $\\zeta_2$ from molecular line observations.","These have been derived from the chemical kinetics of the involved species, but they have not been validated using synthetic data processed with a standard observative pipeline.","We aim to bridge this gap.","We perform the radiative transfer on a set of three-dimensional magneto-hydrodynamical simulations of prestellar cores, exploring different initial $\\zeta_2$, evolutionary stages, types of radiative transfer (e.g. assuming local-thermodynamic-equilibrium conditions), and telescope responses.","We then compute the column densities of the involved tracers to determine $\\zeta_2$, using, in particular, the equation proposed by Bovino et.","al (2020) and by Caselli et al.","(1998) both used nowadays.","Our results confirm that the method of Bovino et al.","(2020) accurately retrieves the actual $\\zeta_2$ within a factor of $2-3$, in the physical conditions explored in our tests.","Since we also explore a non-local thermodynamic equilibrium radiative transfer, this work indirectly offers insights into the excitation temperatures of common transitions at moderate volume densities ($n\\approx 10^5 \\, \\rm cm^{-3}$).","We have also performed a few tests using the formula proposed by Caselli et al.","(1998), which overestimates the actual $\\zeta_2$ by at least two orders of magnitudes.","We also consider a new derivation of this method, which, however, still leads to large overestimates."],"url":"http://arxiv.org/abs/2402.10852v1","category":"astro-ph.GA"}
{"created":"2024-02-16 17:36:46","title":"Cyclic Lie-Rinehart algebras","abstract":"We study Lie-Rinehart algebra structures in the framework provided by a duality pairing of modules over a unital commutative associative algebra. Thus, we construct examples of Lie brackets corresponding to a fixed anchor map whose image is a cyclic submodule of the derivation module, and therefore we call them cyclic Lie-Rinehart algebras. In a very special case of our construction, these brackets turn out to be related to certain differential operators that occur in mathematical physics.","sentences":["We study Lie-Rinehart algebra structures in the framework provided by a duality pairing of modules over a unital commutative associative algebra.","Thus, we construct examples of Lie brackets corresponding to a fixed anchor map whose image is a cyclic submodule of the derivation module, and therefore we call them cyclic Lie-Rinehart algebras.","In a very special case of our construction, these brackets turn out to be related to certain differential operators that occur in mathematical physics."],"url":"http://arxiv.org/abs/2402.10845v1","category":"math.DG"}
{"created":"2024-02-16 16:41:14","title":"TernaryVote: Differentially Private, Communication Efficient, and Byzantine Resilient Distributed Optimization on Heterogeneous Data","abstract":"Distributed training of deep neural networks faces three critical challenges: privacy preservation, communication efficiency, and robustness to fault and adversarial behaviors. Although significant research efforts have been devoted to addressing these challenges independently, their synthesis remains less explored. In this paper, we propose TernaryVote, which combines a ternary compressor and the majority vote mechanism to realize differential privacy, gradient compression, and Byzantine resilience simultaneously. We theoretically quantify the privacy guarantee through the lens of the emerging f-differential privacy (DP) and the Byzantine resilience of the proposed algorithm. Particularly, in terms of privacy guarantees, compared to the existing sign-based approach StoSign, the proposed method improves the dimension dependence on the gradient size and enjoys privacy amplification by mini-batch sampling while ensuring a comparable convergence rate. We also prove that TernaryVote is robust when less than 50% of workers are blind attackers, which matches that of SIGNSGD with majority vote. Extensive experimental results validate the effectiveness of the proposed algorithm.","sentences":["Distributed training of deep neural networks faces three critical challenges: privacy preservation, communication efficiency, and robustness to fault and adversarial behaviors.","Although significant research efforts have been devoted to addressing these challenges independently, their synthesis remains less explored.","In this paper, we propose TernaryVote, which combines a ternary compressor and the majority vote mechanism to realize differential privacy, gradient compression, and Byzantine resilience simultaneously.","We theoretically quantify the privacy guarantee through the lens of the emerging f-differential privacy (DP) and the Byzantine resilience of the proposed algorithm.","Particularly, in terms of privacy guarantees, compared to the existing sign-based approach StoSign, the proposed method improves the dimension dependence on the gradient size and enjoys privacy amplification by mini-batch sampling while ensuring a comparable convergence rate.","We also prove that TernaryVote is robust when less than 50% of workers are blind attackers, which matches that of SIGNSGD with majority vote.","Extensive experimental results validate the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2402.10816v1","category":"cs.LG"}
{"created":"2024-02-16 16:29:25","title":"Self-consistent equilibrium of a helical magnetic flux rope in a finite-pressure plasma","abstract":"We present an analytical model of the self-consistent equilibrium of a magnetic flux rope which is obtained in cylindrical geometry. The equilibrium azimuthal magnetic field and plasma pressure are determined in a self-consistent way through the current density which is derived as a solution of a nonlinear equation. By minimizing the energy functional, it was shown that the constrained equilibrium state is stable. The obtained results are also applicable to the cylindrical tokamak magnetic configurations. It is shown that the analytically predicted radial profiles of equilibrium quantities are in good agreement with the experimental data.","sentences":["We present an analytical model of the self-consistent equilibrium of a magnetic flux rope which is obtained in cylindrical geometry.","The equilibrium azimuthal magnetic field and plasma pressure are determined in a self-consistent way through the current density which is derived as a solution of a nonlinear equation.","By minimizing the energy functional, it was shown that the constrained equilibrium state is stable.","The obtained results are also applicable to the cylindrical tokamak magnetic configurations.","It is shown that the analytically predicted radial profiles of equilibrium quantities are in good agreement with the experimental data."],"url":"http://arxiv.org/abs/2402.10804v1","category":"nlin.PS"}
{"created":"2024-02-16 15:28:27","title":"Fitness-based Linkage Learning and Maximum-Clique Conditional Linkage Modelling for Gray-box Optimization with RV-GOMEA","abstract":"For many real-world optimization problems it is possible to perform partial evaluations, meaning that the impact of changing a few variables on a solution's fitness can be computed very efficiently. It has been shown that such partial evaluations can be excellently leveraged by the Real-Valued GOMEA (RV-GOMEA) that uses a linkage model to capture dependencies between problem variables. Recently, conditional linkage models were introduced for RV-GOMEA, expanding its state-of-the-art performance even to problems with overlapping dependencies. However, that work assumed that the dependency structure is known a priori. Fitness-based linkage learning techniques have previously been used to detect dependencies during optimization, but only for non-conditional linkage models. In this work, we combine fitness-based linkage learning and conditional linkage modelling in RV-GOMEA. In addition, we propose a new way to model overlapping dependencies in conditional linkage models to maximize the joint sampling of fully interdependent groups of variables. We compare the resulting novel variant of RV-GOMEA to other variants of RV-GOMEA and VkD-CMA on 12 problems with varying degree of overlapping dependencies. We find that the new RV-GOMEA not only performs best on most problems, also the overhead of learning the conditional linkage models during optimization is often negligible.","sentences":["For many real-world optimization problems it is possible to perform partial evaluations, meaning that the impact of changing a few variables on a solution's fitness can be computed very efficiently.","It has been shown that such partial evaluations can be excellently leveraged by the Real-Valued GOMEA (RV-GOMEA) that uses a linkage model to capture dependencies between problem variables.","Recently, conditional linkage models were introduced for RV-GOMEA, expanding its state-of-the-art performance even to problems with overlapping dependencies.","However, that work assumed that the dependency structure is known a priori.","Fitness-based linkage learning techniques have previously been used to detect dependencies during optimization, but only for non-conditional linkage models.","In this work, we combine fitness-based linkage learning and conditional linkage modelling in RV-GOMEA.","In addition, we propose a new way to model overlapping dependencies in conditional linkage models to maximize the joint sampling of fully interdependent groups of variables.","We compare the resulting novel variant of RV-GOMEA to other variants of RV-GOMEA and VkD-CMA on 12 problems with varying degree of overlapping dependencies.","We find that the new RV-GOMEA not only performs best on most problems, also the overhead of learning the conditional linkage models during optimization is often negligible."],"url":"http://arxiv.org/abs/2402.10757v1","category":"cs.NE"}
{"created":"2024-02-16 15:19:39","title":"STF: Spatio-Temporal Fusion Module for Improving Video Object Detection","abstract":"Consecutive frames in a video contain redundancy, but they may also contain relevant complementary information for the detection task. The objective of our work is to leverage this complementary information to improve detection. Therefore, we propose a spatio-temporal fusion framework (STF). We first introduce multi-frame and single-frame attention modules that allow a neural network to share feature maps between nearby frames to obtain more robust object representations. Second, we introduce a dual-frame fusion module that merges feature maps in a learnable manner to improve them. Our evaluation is conducted on three different benchmarks including video sequences of moving road users. The performed experiments demonstrate that the proposed spatio-temporal fusion module leads to improved detection performance compared to baseline object detectors. Code is available at https://github.com/noreenanwar/STF-module","sentences":["Consecutive frames in a video contain redundancy, but they may also contain relevant complementary information for the detection task.","The objective of our work is to leverage this complementary information to improve detection.","Therefore, we propose a spatio-temporal fusion framework (STF).","We first introduce multi-frame and single-frame attention modules that allow a neural network to share feature maps between nearby frames to obtain more robust object representations.","Second, we introduce a dual-frame fusion module that merges feature maps in a learnable manner to improve them.","Our evaluation is conducted on three different benchmarks including video sequences of moving road users.","The performed experiments demonstrate that the proposed spatio-temporal fusion module leads to improved detection performance compared to baseline object detectors.","Code is available at https://github.com/noreenanwar/STF-module"],"url":"http://arxiv.org/abs/2402.10752v1","category":"cs.CV"}
{"created":"2024-02-16 14:44:40","title":"Semi-weakly-supervised neural network training for medical image registration","abstract":"For training registration networks, weak supervision from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing unsupervised methods, and (b) being used independently in registration tasks in which unsupervised losses are unavailable or ineffective. This correspondence-informing supervision entails cost in annotation that requires significant specialised effort. This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs. We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based unsupervised losses can be applied on unlabelled data. The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation. Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies. Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics.","sentences":["For training registration networks, weak supervision from segmented corresponding regions-of-interest (ROIs) have been proven effective for (a) supplementing unsupervised methods, and (b) being used independently in registration tasks in which unsupervised losses are unavailable or ineffective.","This correspondence-informing supervision entails cost in annotation that requires significant specialised effort.","This paper describes a semi-weakly-supervised registration pipeline that improves the model performance, when only a small corresponding-ROI-labelled dataset is available, by exploiting unlabelled image pairs.","We examine two types of augmentation methods by perturbation on network weights and image resampling, such that consistency-based unsupervised losses can be applied on unlabelled data.","The novel WarpDDF and RegCut approaches are proposed to allow commutative perturbation between an image pair and the predicted spatial transformation (i.e. respective input and output of registration networks), distinct from existing perturbation methods for classification or segmentation.","Experiments using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the improvement in registration performance and the ablated contributions from the individual strategies.","Furthermore, this study attempts to construct one of the first computational atlases for pelvic structures, enabled by registering inter-subject MRs, and quantifies the significant differences due to the proposed semi-weak supervision with a discussion on the potential clinical use of example atlas-derived statistics."],"url":"http://arxiv.org/abs/2402.10728v1","category":"eess.IV"}
{"created":"2024-02-16 14:20:58","title":"A current based approach for the uniqueness of the continuity equation","abstract":"We consider the problem of proving uniqueness of the solution of the continuity equation with a vector field $u \\in [L^1 (0,T; W^{1,p}(\\mathbb{T}^d)) \\cap L^\\infty ((0,T) \\times \\mathbb{T}^d)]^d$ with $\\operatorname{div}(u) ^- \\in L^1 (0,T; L^\\infty (\\mathbb{T}^d))$ and an initial datum $\\rho_0 \\in L^q (\\mathbb{T}^d)$, where $\\mathbb{T}^d$ is the $d$-dimensional torus and $ 1 \\leq p,q \\leq +\\infty$ such that $1/p + 1/q =1$ without using the theory of renormalized solutions. We propose a more geometric approach which will however still rely on a strong $L^1$ estimate on the commutator (which is the key technical tool when using renormalized solutions, too), but other than that will be based on the theory of currents.","sentences":["We consider the problem of proving uniqueness of the solution of the continuity equation with a vector field $u \\in [L^1 (0,T; W^{1,p}(\\mathbb{T}^d))","\\cap L^\\infty ((0,T) \\times \\mathbb{T}^d)]^d$ with $\\operatorname{div}(u) ^-","\\in L^1 (0,T; L^\\infty (\\mathbb{T}^d))$ and an initial datum $\\rho_0 \\in L^q (\\mathbb{T}^d)$, where $\\mathbb{T}^d$ is the $d$-dimensional torus and $ 1 \\leq p,q \\leq +\\infty$ such that $1/p + 1/q =1$ without using the theory of renormalized solutions.","We propose a more geometric approach which will however still rely on a strong $L^1$ estimate on the commutator (which is the key technical tool when using renormalized solutions, too), but other than that will be based on the theory of currents."],"url":"http://arxiv.org/abs/2402.10719v1","category":"math.AP"}
{"created":"2024-02-16 14:13:17","title":"Prediction of Photodynamics of 200 nm Excited Cyclobutanone with Linear Response Electronic Structure and Ab Initio Multiple Spawning","abstract":"Simulations of photochemical reaction dynamics have been a challenge to the theoretical chemistry community for some time. In an effort to determine the predictive character of current approaches, we predict the results of an upcoming ultrafast diffraction experiment on the photodynamics of cyclobutanone after excitation to the lowest lying Rydberg state (S$_2$). A picosecond of nonadiabatic dynamics is described with ab initio multiple spawning. We use both time dependent density functional theory and equation-of-motion coupled cluster for the underlying electronic structure theory. We find that the lifetime of the S$_2$ state is more than a picosecond (with both TDDFT and EOM-CCSD). The predicted UED spectrum exhibits numerous structural features, but weak time dependence over the course of the simulations.","sentences":["Simulations of photochemical reaction dynamics have been a challenge to the theoretical chemistry community for some time.","In an effort to determine the predictive character of current approaches, we predict the results of an upcoming ultrafast diffraction experiment on the photodynamics of cyclobutanone after excitation to the lowest lying Rydberg state (S$_2$).","A picosecond of nonadiabatic dynamics is described with ab initio multiple spawning.","We use both time dependent density functional theory and equation-of-motion coupled cluster for the underlying electronic structure theory.","We find that the lifetime of the S$_2$ state is more than a picosecond (with both TDDFT and EOM-CCSD).","The predicted UED spectrum exhibits numerous structural features, but weak time dependence over the course of the simulations."],"url":"http://arxiv.org/abs/2402.10710v1","category":"physics.chem-ph"}
{"created":"2024-02-16 13:34:28","title":"Gauss-Newton Natural Gradient Descent for Physics-Informed Computational Fluid Dynamics","abstract":"We propose Gauss-Newton's method in function space for the solution of the Navier-Stokes equations in the physics-informed neural network (PINN) framework. Upon discretization, this yields a natural gradient method that provably mimics the function space dynamics. Our computational results demonstrate close to single-precision accuracy measured in relative $L^2$ norm on a number of benchmark problems. To the best of our knowledge, this constitutes the first contribution in the PINN literature that solves the Navier-Stokes equations to this degree of accuracy. Finally, we show that given a suitable integral discretization, the proposed optimization algorithm agrees with Gauss-Newton's method in parameter space. This allows a matrix-free formulation enabling efficient scalability to large network sizes.","sentences":["We propose Gauss-Newton's method in function space for the solution of the Navier-Stokes equations in the physics-informed neural network (PINN) framework.","Upon discretization, this yields a natural gradient method that provably mimics the function space dynamics.","Our computational results demonstrate close to single-precision accuracy measured in relative $L^2$ norm on a number of benchmark problems.","To the best of our knowledge, this constitutes the first contribution in the PINN literature that solves the Navier-Stokes equations to this degree of accuracy.","Finally, we show that given a suitable integral discretization, the proposed optimization algorithm agrees with Gauss-Newton's method in parameter space.","This allows a matrix-free formulation enabling efficient scalability to large network sizes."],"url":"http://arxiv.org/abs/2402.10680v1","category":"math.OC"}
{"created":"2024-02-16 11:44:25","title":"Studying the Impact of Quantum-Specific Hyperparameters on Hybrid Quantum-Classical Neural Networks","abstract":"In current noisy intermediate-scale quantum devices, hybrid quantum-classical neural networks (HQNNs) represent a promising solution that combines the strengths of classical machine learning with quantum computing capabilities. Compared to classical deep neural networks (DNNs), HQNNs present an additional set of hyperparameters, which are specific to quantum circuits. These quantum-specific hyperparameters, such as quantum layers depth, number of qubits, type of entanglement, type of encoding, number of shots, and measurement observables, can be tuned to modify the behavior of the HQNNs and their capabilities to learn the given task. In this paper, we investigate the impact of these variations on different HQNN models for image classification tasks, implemented on both Qiskit and PennyLane frameworks. We aim to uncover intuitive and counter-intuitive learning patterns of HQNN models within granular levels of controlled quantum perturbations, to form a sound basis for their correlation to accuracy. The outcome of our study opens new avenues for designing efficient HQNN algorithms and builds a foundational base for comprehending and identifying tunable hyperparameters of HQNN models that can lead to useful implementation and usage.","sentences":["In current noisy intermediate-scale quantum devices, hybrid quantum-classical neural networks (HQNNs) represent a promising solution that combines the strengths of classical machine learning with quantum computing capabilities.","Compared to classical deep neural networks (DNNs), HQNNs present an additional set of hyperparameters, which are specific to quantum circuits.","These quantum-specific hyperparameters, such as quantum layers depth, number of qubits, type of entanglement, type of encoding, number of shots, and measurement observables, can be tuned to modify the behavior of the HQNNs and their capabilities to learn the given task.","In this paper, we investigate the impact of these variations on different HQNN models for image classification tasks, implemented on both Qiskit and PennyLane frameworks.","We aim to uncover intuitive and counter-intuitive learning patterns of HQNN models within granular levels of controlled quantum perturbations, to form a sound basis for their correlation to accuracy.","The outcome of our study opens new avenues for designing efficient HQNN algorithms and builds a foundational base for comprehending and identifying tunable hyperparameters of HQNN models that can lead to useful implementation and usage."],"url":"http://arxiv.org/abs/2402.10605v1","category":"quant-ph"}
{"created":"2024-02-16 11:42:08","title":"Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for Effective Sequential Recommendation","abstract":"Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance. Despite their effectiveness, the expressive power of text features in these models remains largely unexplored. While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings. Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items. We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance. To address this issue, we propose to employ a pre-processing step known as whitening transformation, which transforms the anisotropic text feature distribution into an isotropic Gaussian distribution. Our experiments show that whitening pre-trained text embeddings in the sequential model can significantly improve recommendation performance. However, the full whitening operation might break the potential manifold of items with similar text semantics. To preserve the original semantics while benefiting from the isotropy of the whitened text features, we introduce WhitenRec+, an ensemble approach that leverages both fully whitened and relaxed whitened item representations for effective recommendations. We further discuss and analyze the benefits of our design through experiments and proofs. Experimental results on three public benchmark datasets demonstrate that WhitenRec+ outperforms state-of-the-art methods for sequential recommendation.","sentences":["Recent sequential recommendation models have combined pre-trained text embeddings of items with item ID embeddings to achieve superior recommendation performance.","Despite their effectiveness, the expressive power of text features in these models remains largely unexplored.","While most existing models emphasize the importance of ID embeddings in recommendations, our study takes a step further by studying sequential recommendation models that only rely on text features and do not necessitate ID embeddings.","Upon examining pretrained text embeddings experimentally, we discover that they reside in an anisotropic semantic space, with an average cosine similarity of over 0.8 between items.","We also demonstrate that this anisotropic nature hinders recommendation models from effectively differentiating between item representations and leads to degenerated performance.","To address this issue, we propose to employ a pre-processing step known as whitening transformation, which transforms the anisotropic text feature distribution into an isotropic Gaussian distribution.","Our experiments show that whitening pre-trained text embeddings in the sequential model can significantly improve recommendation performance.","However, the full whitening operation might break the potential manifold of items with similar text semantics.","To preserve the original semantics while benefiting from the isotropy of the whitened text features, we introduce WhitenRec+, an ensemble approach that leverages both fully whitened and relaxed whitened item representations for effective recommendations.","We further discuss and analyze the benefits of our design through experiments and proofs.","Experimental results on three public benchmark datasets demonstrate that WhitenRec+ outperforms state-of-the-art methods for sequential recommendation."],"url":"http://arxiv.org/abs/2402.10602v1","category":"cs.IR"}
{"created":"2024-02-16 11:04:45","title":"On Molchanov's criterion for compactness of the resolvent for a non self-adjoint Sturm-Liouville operator","abstract":"The Molchanov's condition is a necessary condition for the compactness of the resolvent for a wide class of ordinary differential operators of arbitrary order, but for the Sturm-Liouville operator it is not sufficient, even if the real part of the potential is non-negative. Molchanov's criterion remains valid in the formulation closest to the original one for potentials that take values in a narrower sector than the half-plane, separated from the negative half-axis. This work is devoted to these questions.","sentences":["The Molchanov's condition is a necessary condition for the compactness of the resolvent for a wide class of ordinary differential operators of arbitrary order, but for the Sturm-Liouville operator it is not sufficient, even if the real part of the potential is non-negative.","Molchanov's criterion remains valid in the formulation closest to the original one for potentials that take values in a narrower sector than the half-plane, separated from the negative half-axis.","This work is devoted to these questions."],"url":"http://arxiv.org/abs/2402.10577v1","category":"math.SP"}
{"created":"2024-02-16 09:59:44","title":"A Comparative Analysis of Hybrid-Quantum Classical Neural Networks","abstract":"Hybrid Quantum-Classical Machine Learning (ML) is an emerging field, amalgamating the strengths of both classical neural networks and quantum variational circuits on the current noisy intermediate-scale quantum devices. This paper performs an extensive comparative analysis between different hybrid quantum-classical machine learning algorithms, namely Quantum Convolution Neural Network, Quanvolutional Neural Network and Quantum ResNet, for image classification. The experiments designed in this paper focus on different Quantum ML (QML) algorithms to better understand the accuracy variation across the different quantum architectures by implementing interchangeable quantum circuit layers, varying the repetition of such layers and their efficient placement. Such variations enable us to compare the accuracy across different architectural permutations of a given hybrid QML algorithm. The performance comparison of the hybrid models, based on the accuracy, provides us with an understanding of hybrid quantum-classical convergence in correlation with the quantum layer count and the qubit count variations in the circuit.","sentences":["Hybrid Quantum-Classical Machine Learning (ML) is an emerging field, amalgamating the strengths of both classical neural networks and quantum variational circuits on the current noisy intermediate-scale quantum devices.","This paper performs an extensive comparative analysis between different hybrid quantum-classical machine learning algorithms, namely Quantum Convolution Neural Network, Quanvolutional Neural Network and Quantum ResNet, for image classification.","The experiments designed in this paper focus on different Quantum ML (QML) algorithms to better understand the accuracy variation across the different quantum architectures by implementing interchangeable quantum circuit layers, varying the repetition of such layers and their efficient placement.","Such variations enable us to compare the accuracy across different architectural permutations of a given hybrid QML algorithm.","The performance comparison of the hybrid models, based on the accuracy, provides us with an understanding of hybrid quantum-classical convergence in correlation with the quantum layer count and the qubit count variations in the circuit."],"url":"http://arxiv.org/abs/2402.10540v1","category":"quant-ph"}
{"created":"2024-02-16 09:22:46","title":"New multivariable mean from nonlinear matrix equation associated to the harmonic mean","abstract":"Various multivariable means have been defined for positive definite matrices, such as the Cartan mean, Wasserstein mean, and R\\'{e}nyi power mean. These multivariable means have corresponding matrix equations. In this paper, we consider the following non-linear matrix equation: $$ X = \\left[ \\sum_{i=1}^{n} w_{i} [ (1-t) X + t A_{i} ]^{-1} \\right]^{-1}, $$ where $t \\in (0,1]$. We prove that this equation has a unique solution and define a new mean, which we denote as $G_{t}(\\omega; \\mathbb{A})$. We explore important properties of the mean $G_{t}(\\omega; \\mathbb{A})$ including the relationship with matrix power mean, and show that the mean $G_{t}(\\omega; \\mathbb{A})$ is monotone in the parameter $t$. Finally, we connect the mean $G_{t}(\\omega; \\mathbb{A})$ to a barycenter for the log-determinant divergence.","sentences":["Various multivariable means have been defined for positive definite matrices, such as the Cartan mean, Wasserstein mean, and R\\'{e}nyi power mean.","These multivariable means have corresponding matrix equations.","In this paper, we consider the following non-linear matrix equation: $$ X = \\left[ \\sum_{i=1}^{n} w_{i} [ (1-t) X + t A_{i} ]^{-1} \\right]^{-1}, $$ where $t \\in (0,1]$. We prove that this equation has a unique solution and define a new mean, which we denote as $G_{t}(\\omega; \\mathbb{A})$. We explore important properties of the mean $G_{t}(\\omega; \\mathbb{A})$ including the relationship with matrix power mean, and show that the mean $G_{t}(\\omega; \\mathbb{A})$ is monotone in the parameter $t$. Finally, we connect the mean $G_{t}(\\omega; \\mathbb{A})$ to a barycenter for the log-determinant divergence."],"url":"http://arxiv.org/abs/2402.10526v1","category":"math.FA"}
{"created":"2024-02-16 08:57:31","title":"A Novel Computing Paradigm for MobileNetV3 using Memristor","abstract":"The advancement in the field of machine learning is inextricably linked with the concurrent progress in domain-specific hardware accelerators such as GPUs and TPUs. However, the rapidly growing computational demands necessitated by larger models and increased data have become a primary bottleneck in further advancing machine learning, especially in mobile and edge devices. Currently, the neuromorphic computing paradigm based on memristors presents a promising solution. In this study, we introduce a memristor-based MobileNetV3 neural network computing paradigm and provide an end-to-end framework for validation. The results demonstrate that this computing paradigm achieves over 90\\% accuracy on the CIFAR-10 dataset while saving inference time and reducing energy consumption. With the successful development and verification of MobileNetV3, the potential for realizing more memristor-based neural networks using this computing paradigm and open-source framework has significantly increased. This progress sets a groundbreaking pathway for future deployment initiatives.","sentences":["The advancement in the field of machine learning is inextricably linked with the concurrent progress in domain-specific hardware accelerators such as GPUs and TPUs.","However, the rapidly growing computational demands necessitated by larger models and increased data have become a primary bottleneck in further advancing machine learning, especially in mobile and edge devices.","Currently, the neuromorphic computing paradigm based on memristors presents a promising solution.","In this study, we introduce a memristor-based MobileNetV3 neural network computing paradigm and provide an end-to-end framework for validation.","The results demonstrate that this computing paradigm achieves over 90\\% accuracy on the CIFAR-10 dataset while saving inference time and reducing energy consumption.","With the successful development and verification of MobileNetV3, the potential for realizing more memristor-based neural networks using this computing paradigm and open-source framework has significantly increased.","This progress sets a groundbreaking pathway for future deployment initiatives."],"url":"http://arxiv.org/abs/2402.10512v1","category":"cs.AR"}
{"created":"2024-02-16 17:53:08","title":"JetTrain: IDE-Native Machine Learning Experiments","abstract":"Integrated development environments (IDEs) are prevalent code-writing and debugging tools. However, they have yet to be widely adopted for launching machine learning (ML) experiments. This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources. A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware. We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput.","sentences":["Integrated development environments (IDEs) are prevalent code-writing and debugging tools.","However, they have yet to be widely adopted for launching machine learning (ML) experiments.","This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources.","A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware.","We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput."],"url":"http://arxiv.org/abs/2402.10857v1","category":"cs.SE"}
{"created":"2024-02-16 17:36:56","title":"Enhancement-Driven Pretraining for Robust Fingerprint Representation Learning","abstract":"Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools. In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training. Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a self-supervised manner. We further refine these representations, aiming to enhance the verification capabilities. Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established self-supervised training techniques. Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements. Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples.","sentences":["Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools.","In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training.","Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a self-supervised manner.","We further refine these representations, aiming to enhance the verification capabilities.","Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established self-supervised training techniques.","Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements.","Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples."],"url":"http://arxiv.org/abs/2402.10847v1","category":"cs.CV"}
{"created":"2024-02-16 15:21:35","title":"When Dataflow Analysis Meets Large Language Models","abstract":"Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark programs, outperforming state-of-the-art (classic) tools, including a very recent industrial analyzer.","sentences":["Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection.","Existing approaches require the successful compilation of the subject program and customizations for downstream applications.","This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications.","Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis.","Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark programs, outperforming state-of-the-art (classic) tools, including a very recent industrial analyzer."],"url":"http://arxiv.org/abs/2402.10754v1","category":"cs.PL"}
{"created":"2024-02-16 14:30:46","title":"Machine Learning based Prediction of Ditching Loads","abstract":"We present approaches to predict dynamic ditching loads on aircraft fuselages using machine learning. The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a convolutional autoencoder (CAE) and the transient evolution of these loads in a subsequent part. Different CAE strategies are assessed and combined with either long short-term memory (LSTM) networks or Koopman-operator based methods to predict the transient behaviour. The training data is compiled by an extension of the momentum method of von-Karman and Wagner and the rationale of the training approach is briefly summarised. The application included refers to a full-scale fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach velocities at 6{\\deg} incidence. Results indicate a satisfactory level of predictive agreement for all four investigated surrogate models examined, with the combination of an LSTM and a deep decoder CAE showing the best performance.","sentences":["We present approaches to predict dynamic ditching loads on aircraft fuselages using machine learning.","The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a convolutional autoencoder (CAE) and the transient evolution of these loads in a subsequent part.","Different CAE strategies are assessed and combined with either long short-term memory (LSTM) networks or Koopman-operator based methods to predict the transient behaviour.","The training data is compiled by an extension of the momentum method of von-Karman and Wagner and the rationale of the training approach is briefly summarised.","The application included refers to a full-scale fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach velocities at 6{\\deg} incidence.","Results indicate a satisfactory level of predictive agreement for all four investigated surrogate models examined, with the combination of an LSTM and a deep decoder CAE showing the best performance."],"url":"http://arxiv.org/abs/2402.10724v1","category":"cs.LG"}
{"created":"2024-02-16 13:13:18","title":"Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL","abstract":"Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused.","sentences":["Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research.","Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool.","However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead.","Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL.","We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments.","Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost.","Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused."],"url":"http://arxiv.org/abs/2402.10663v1","category":"cs.CL"}
{"created":"2024-02-16 12:44:15","title":"Linear Transformers with Learnable Kernel Functions are Better In-Context Models","abstract":"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.","sentences":["Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing.","Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks.","However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines.","The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks.","Mirroring the Transformer's in-context adeptness, it became a strong contender in the field.","In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset."],"url":"http://arxiv.org/abs/2402.10644v1","category":"cs.LG"}
{"created":"2024-02-16 11:28:50","title":"Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification","abstract":"Whole-slide image (WSI) classification is a challenging task because 1) patches from WSI lack annotation, and 2) WSI possesses unnecessary variability, e.g., stain protocol. Recently, Multiple-Instance Learning (MIL) has made significant progress, allowing for classification based on slide-level, rather than patch-level, annotations. However, existing MIL methods ignore that all patches from normal slides are normal. Using this free annotation, we introduce a semi-supervision signal to de-bias the inter-slide variability and to capture the common factors of variation within normal patches. Because our method is orthogonal to the MIL algorithm, we evaluate our method on top of the recently proposed MIL algorithms and also compare the performance with other semi-supervised approaches. We evaluate our method on two public WSI datasets including Camelyon-16 and TCGA lung cancer and demonstrate that our approach significantly improves the predictive performance of existing MIL algorithms and outperforms other semi-supervised algorithms. We release our code at https://github.com/AITRICS/pathology_mil.","sentences":["Whole-slide image (WSI) classification is a challenging task because 1) patches from WSI lack annotation, and 2) WSI possesses unnecessary variability, e.g., stain protocol.","Recently, Multiple-Instance Learning (MIL) has made significant progress, allowing for classification based on slide-level, rather than patch-level, annotations.","However, existing MIL methods ignore that all patches from normal slides are normal.","Using this free annotation, we introduce a semi-supervision signal to de-bias the inter-slide variability and to capture the common factors of variation within normal patches.","Because our method is orthogonal to the MIL algorithm, we evaluate our method on top of the recently proposed MIL algorithms and also compare the performance with other semi-supervised approaches.","We evaluate our method on two public WSI datasets including Camelyon-16 and TCGA lung cancer and demonstrate that our approach significantly improves the predictive performance of existing MIL algorithms and outperforms other semi-supervised algorithms.","We release our code at https://github.com/AITRICS/pathology_mil."],"url":"http://arxiv.org/abs/2402.10595v1","category":"cs.CV"}
{"created":"2024-02-16 11:03:07","title":"Nowcasting with mixed frequency data using Gaussian processes","abstract":"We propose and discuss Bayesian machine learning methods for mixed data sampling (MIDAS) regressions. This involves handling frequency mismatches with restricted and unrestricted MIDAS variants and specifying functional relationships between many predictors and the dependent variable. We use Gaussian processes (GP) and Bayesian additive regression trees (BART) as flexible extensions to linear penalized estimation. In a nowcasting and forecasting exercise we focus on quarterly US output growth and inflation in the GDP deflator. The new models leverage macroeconomic Big Data in a computationally efficient way and offer gains in predictive accuracy along several dimensions.","sentences":["We propose and discuss Bayesian machine learning methods for mixed data sampling (MIDAS) regressions.","This involves handling frequency mismatches with restricted and unrestricted MIDAS variants and specifying functional relationships between many predictors and the dependent variable.","We use Gaussian processes (GP) and Bayesian additive regression trees (BART) as flexible extensions to linear penalized estimation.","In a nowcasting and forecasting exercise we focus on quarterly US output growth and inflation in the GDP deflator.","The new models leverage macroeconomic Big Data in a computationally efficient way and offer gains in predictive accuracy along several dimensions."],"url":"http://arxiv.org/abs/2402.10574v1","category":"econ.EM"}
{"created":"2024-02-16 10:20:42","title":"Learning Disentangled Audio Representations through Controlled Synthesis","abstract":"This paper tackles the scarcity of benchmarking data in disentangled auditory representation learning. We introduce SynTone, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques. Benchmarking state-of-the-art methods on SynTone highlights its utility for method evaluation. Our results underscore strengths and limitations in audio disentanglement, motivating future research.","sentences":["This paper tackles the scarcity of benchmarking data in disentangled auditory representation learning.","We introduce SynTone, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques.","Benchmarking state-of-the-art methods on SynTone highlights its utility for method evaluation.","Our results underscore strengths and limitations in audio disentanglement, motivating future research."],"url":"http://arxiv.org/abs/2402.10547v1","category":"cs.SD"}
{"created":"2024-02-16 09:10:37","title":"tsdataleaks: An R Package to Detect Potential Data Leaks in Forecasting Competitions","abstract":"Forecasting competitions are of increasing importance as a means to learn best practices and gain knowledge. Data leakage is one of the most common issues that can often be found in competitions. Data leaks can happen when the training data contains information about the test data. There are a variety of different ways that data leaks can occur with time series data. For example: i) randomly chosen blocks of time series are concatenated to form a new time series; ii) scale-shifts; iii) repeating patterns in time series; iv) white noise is added to the original time series to form a new time series, etc. This work introduces a novel tool to detect these data leaks. The tsdataleaks package provides a simple and computationally efficient algorithm to exploit data leaks in time series data. This paper demonstrates the package design and its power to detect data leakages with an application to forecasting competition data.","sentences":["Forecasting competitions are of increasing importance as a means to learn best practices and gain knowledge.","Data leakage is one of the most common issues that can often be found in competitions.","Data leaks can happen when the training data contains information about the test data.","There are a variety of different ways that data leaks can occur with time series data.","For example: i) randomly chosen blocks of time series are concatenated to form a new time series; ii) scale-shifts; iii) repeating patterns in time series; iv) white noise is added to the original time series to form a new time series, etc.","This work introduces a novel tool to detect these data leaks.","The tsdataleaks package provides a simple and computationally efficient algorithm to exploit data leaks in time series data.","This paper demonstrates the package design and its power to detect data leakages with an application to forecasting competition data."],"url":"http://arxiv.org/abs/2402.10522v1","category":"stat.AP"}
{"created":"2024-02-16 09:06:06","title":"Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs","abstract":"Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \\emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs. The source code will be publicly available soon.","sentences":["Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes.","Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance.","Thus, this paper introduces \\emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs.","Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving.","As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM.","All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs.","The source code will be publicly available soon."],"url":"http://arxiv.org/abs/2402.10517v1","category":"cs.LG"}
{"created":"2024-02-16 08:27:55","title":"Resilience of the quadratic Littlewood-Offord problem","abstract":"We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\\boldsymbol{\\xi}^{\\mathsf{T}} M \\boldsymbol{\\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\\boldsymbol{\\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\\boldsymbol{\\xi}$ sustain without \"inflating\" $\\sup_{x\\in \\mathbb{R}} \\mathbb{P} \\left\\{\\boldsymbol{\\xi}^{\\mathsf{T}} M \\boldsymbol{\\xi} = x\\right\\}$ and thus \"de-smooth\" the original distribution resulting in a more \"grainy\" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.","sentences":["We study the statistical resilience of high-dimensional data.","Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\\boldsymbol{\\xi}^{\\mathsf{T}} M \\boldsymbol{\\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\\boldsymbol{\\xi}$ is a conformal Rademacher vector.","Specifically, we pursue the question of how many adversarial sign-flips can $\\boldsymbol{\\xi}$ sustain without \"inflating\" $\\sup_{x\\in \\mathbb{R}} \\mathbb{P} \\left\\{\\boldsymbol{\\xi}^{\\mathsf{T}} M \\boldsymbol{\\xi} = x\\right\\}$ and thus \"de-smooth\" the original distribution resulting in a more \"grainy\" and adversarially biased distribution.","Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes."],"url":"http://arxiv.org/abs/2402.10504v1","category":"math.PR"}
{"created":"2024-02-16 13:58:31","title":"Dark Energy Survey: Galaxy Sample for the Baryonic Acoustic Oscillation Measurement from the Final Dataset","abstract":"In this paper we present and validate the galaxy sample used for the analysis of the baryon acoustic oscillation (BAO) signal in the Dark Energy Survey (DES) Y6 data. The definition is based on a color and redshift-dependent magnitude cut optimized to select galaxies at redshifts higher than 0.6, while ensuring a high-quality photo-$z$ determination. The optimization is performed using a Fisher forecast algorithm, finding the optimal $i$-magnitude cut to be given by $i$<19.64+2.894$z_{\\rm ph}$. For the optimal sample, we forecast an increase in precision in the BAO measurement of $\\sim$25% with respect to the Y3 analysis. Our BAO sample has a total of 15,937,556 galaxies in the redshift range 0.6<$z_{\\rm ph}$<1.2, and its angular mask covers 4,273.42 deg${}^2$ to a depth of $i$=22.5. We validate its redshift distributions with three different methods: directional neighborhood fitting algorithm (DNF), which is our primary photo-$z$ estimation; direct calibration with spectroscopic redshifts from VIPERS; and clustering redshift using SDSS galaxies. The fiducial redshift distribution is a combination of these three techniques performed by modifying the mean and width of the DNF distributions to match those of VIPERS and clustering redshift. In this paper we also describe the methodology used to mitigate the effect of observational systematics, which is analogous to the one used in the Y3 analysis. This paper is one of the two dedicated to the analysis of the BAO signal in DES Y6. In its companion paper, we present the angular diameter distance constraints obtained through the fitting to the BAO scale.","sentences":["In this paper we present and validate the galaxy sample used for the analysis of the baryon acoustic oscillation (BAO) signal in the Dark Energy Survey (DES) Y6 data.","The definition is based on a color and redshift-dependent magnitude cut optimized to select galaxies at redshifts higher than 0.6, while ensuring a high-quality photo-$z$ determination.","The optimization is performed using a Fisher forecast algorithm, finding the optimal $i$-magnitude cut to be given by $i$<19.64+2.894$z_{\\rm ph}$. For the optimal sample, we forecast an increase in precision in the BAO measurement of $\\sim$25% with respect to the Y3 analysis.","Our BAO sample has a total of 15,937,556 galaxies in the redshift range 0.6<$z_{\\rm ph}$<1.2, and its angular mask covers 4,273.42 deg${}^2$ to a depth of $i$=22.5.","We validate its redshift distributions with three different methods: directional neighborhood fitting algorithm (DNF), which is our primary photo-$z$ estimation; direct calibration with spectroscopic redshifts from VIPERS; and clustering redshift using SDSS galaxies.","The fiducial redshift distribution is a combination of these three techniques performed by modifying the mean and width of the DNF distributions to match those of VIPERS and clustering redshift.","In this paper we also describe the methodology used to mitigate the effect of observational systematics, which is analogous to the one used in the Y3 analysis.","This paper is one of the two dedicated to the analysis of the BAO signal in DES Y6.","In its companion paper, we present the angular diameter distance constraints obtained through the fitting to the BAO scale."],"url":"http://arxiv.org/abs/2402.10697v1","category":"astro-ph.CO"}
{"created":"2024-02-16 13:58:30","title":"Dark Energy Survey: A 2.1% measurement of the angular Baryonic Acoustic Oscillation scale at redshift $z_{\\rm eff}$=0.85 from the final dataset","abstract":"We present the angular diameter distance measurement obtained with the Baryonic Acoustic Oscillation feature from galaxy clustering in the completed Dark Energy Survey, consisting of six years (Y6) of observations. We use the Y6 BAO galaxy sample, optimized for BAO science in the redshift range 0.6<$z$<1.2, with an effective redshift at $z_{\\rm eff}$=0.85 and split into six tomographic bins. The sample has nearly 16 million galaxies over 4,273 square degrees. Our consensus measurement constrains the ratio of the angular distance to sound horizon scale to $D_M(z_{\\rm eff})/r_d$ = 19.51$\\pm$0.41 (at 68.3% confidence interval), resulting from comparing the BAO position in our data to that predicted by Planck $\\Lambda$CDM via the BAO shift parameter $\\alpha=(D_M/r_d)/(D_M/r_d)_{\\rm Planck}$. To achieve this, the BAO shift is measured with three different methods, Angular Correlation Function (ACF), Angular Power Spectrum (APS), and Projected Correlation Function (PCF) obtaining $\\alpha=$ 0.952$\\pm$0.023, 0.962$\\pm$0.022, and 0.955$\\pm$0.020, respectively, which we combine to $\\alpha=$ 0.957$\\pm$0.020, including systematic errors. When compared with the $\\Lambda$CDM model that best fits Planck data, this measurement is found to be 4.3% and 2.1$\\sigma$ below the angular BAO scale predicted. To date, it represents the most precise angular BAO measurement at $z$>0.75 from any survey and the most precise measurement at any redshift from photometric surveys. The analysis was performed blinded to the BAO position and it is shown to be robust against analysis choices, data removal, redshift calibrations and observational systematics.","sentences":["We present the angular diameter distance measurement obtained with the Baryonic Acoustic Oscillation feature from galaxy clustering in the completed Dark Energy Survey, consisting of six years (Y6) of observations.","We use the Y6 BAO galaxy sample, optimized for BAO science in the redshift range 0.6<$z$<1.2, with an effective redshift at $z_{\\rm eff}$=0.85 and split into six tomographic bins.","The sample has nearly 16 million galaxies over 4,273 square degrees.","Our consensus measurement constrains the ratio of the angular distance to sound horizon scale to $D_M(z_{\\rm eff})/r_d$ = 19.51$\\pm$0.41 (at 68.3% confidence interval), resulting from comparing the BAO position in our data to that predicted by Planck $\\Lambda$CDM via the BAO shift parameter $\\alpha=(D_M/r_d)/(D_M/r_d)_{\\rm Planck}$. To achieve this, the BAO shift is measured with three different methods, Angular Correlation Function (ACF), Angular Power Spectrum (APS), and Projected Correlation Function (PCF) obtaining $\\alpha=$ 0.952$\\pm$0.023, 0.962$\\pm$0.022, and 0.955$\\pm$0.020, respectively, which we combine to $\\alpha=$ 0.957$\\pm$0.020, including systematic errors.","When compared with the $\\Lambda$CDM model that best fits Planck data, this measurement is found to be 4.3% and 2.1$\\sigma$ below the angular BAO scale predicted.","To date, it represents the most precise angular BAO measurement at $z$>0.75 from any survey and the most precise measurement at any redshift from photometric surveys.","The analysis was performed blinded to the BAO position and it is shown to be robust against analysis choices, data removal, redshift calibrations and observational systematics."],"url":"http://arxiv.org/abs/2402.10696v1","category":"astro-ph.CO"}
{"created":"2024-02-16 11:23:54","title":"On Simplices with a Given Barycenter That Are Enclosed by the Standard Simplex","abstract":"We present an optimization model defined on the manifold of the set of stochastic matrices. Geometrically, the model is akin to identifying a maximum-volume $n$-dimensional simplex that has a given barycenter and is enclosed by the $n$-dimensional standard simplex. Maximizing the volume of a simplex is equivalent to maximizing the determinant of its corresponding matrix. In our model, we employ trace maximization as a linear alternative to determinant maximization. We identify the analytical form of a solution to this model. We prove the solution is optimal and present necessary and sufficient conditions for it to be the unique optimal solution. Additionally, we show the identified optimal solution is an inverse $M$-matrix, and that its eigenvalues are the same as its diagonal entries. We demonstrate how the model and its solutions apply to the task of synthesizing conditional cumulative distribution functions (CDFs) that, in tandem with a given discrete marginal distribution, coherently preserve a given CDF.","sentences":["We present an optimization model defined on the manifold of the set of stochastic matrices.","Geometrically, the model is akin to identifying a maximum-volume $n$-dimensional simplex that has a given barycenter and is enclosed by the $n$-dimensional standard simplex.","Maximizing the volume of a simplex is equivalent to maximizing the determinant of its corresponding matrix.","In our model, we employ trace maximization as a linear alternative to determinant maximization.","We identify the analytical form of a solution to this model.","We prove the solution is optimal and present necessary and sufficient conditions for it to be the unique optimal solution.","Additionally, we show the identified optimal solution is an inverse $M$-matrix, and that its eigenvalues are the same as its diagonal entries.","We demonstrate how the model and its solutions apply to the task of synthesizing conditional cumulative distribution functions (CDFs) that, in tandem with a given discrete marginal distribution, coherently preserve a given CDF."],"url":"http://arxiv.org/abs/2402.10591v1","category":"math.OC"}
{"created":"2024-02-16 09:31:51","title":"Energy-aware Multi-UAV Coverage Mission Planning with Optimal Speed of Flight","abstract":"This paper tackles the problem of planning minimum-energy coverage paths for multiple UAVs. The addressed Multi-UAV Coverage Path Planning (mCPP) is a crucial problem for many UAV applications such as inspection and aerial survey. However, the typical path-length objective of existing approaches does not directly minimize the energy consumption, nor allows for constraining energy of individual paths by the battery capacity. To this end, we propose a novel mCPP method that uses the optimal flight speed for minimizing energy consumption per traveled distance and a simple yet precise energy consumption estimation algorithm that is utilized during the mCPP planning phase. The method decomposes a given area with boustrophedon decomposition and represents the mCPP as an instance of Multiple Set Traveling Salesman Problem with a minimum energy objective and energy consumption constraint. The proposed method is shown to outperform state-of-the-art methods in terms of computational time and energy efficiency of produced paths. The experimental results show that the accuracy of the energy consumption estimation is on average 97% compared to real flight consumption. The feasibility of the proposed method was verified in a real-world coverage experiment with two UAVs.","sentences":["This paper tackles the problem of planning minimum-energy coverage paths for multiple UAVs.","The addressed Multi-UAV Coverage Path Planning (mCPP) is a crucial problem for many UAV applications such as inspection and aerial survey.","However, the typical path-length objective of existing approaches does not directly minimize the energy consumption, nor allows for constraining energy of individual paths by the battery capacity.","To this end, we propose a novel mCPP method that uses the optimal flight speed for minimizing energy consumption per traveled distance and a simple yet precise energy consumption estimation algorithm that is utilized during the mCPP planning phase.","The method decomposes a given area with boustrophedon decomposition and represents the mCPP as an instance of Multiple Set Traveling Salesman Problem with a minimum energy objective and energy consumption constraint.","The proposed method is shown to outperform state-of-the-art methods in terms of computational time and energy efficiency of produced paths.","The experimental results show that the accuracy of the energy consumption estimation is on average 97% compared to real flight consumption.","The feasibility of the proposed method was verified in a real-world coverage experiment with two UAVs."],"url":"http://arxiv.org/abs/2402.10529v1","category":"cs.RO"}
