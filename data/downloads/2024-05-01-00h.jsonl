{"created":"2024-04-29 17:59:41","title":"Hallucination of Multimodal Large Language Models: A Survey","abstract":"This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.","sentences":["This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks.","Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications.","This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies.","We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue.","Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research.","By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field.","Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike.","Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination."],"url":"http://arxiv.org/abs/2404.18930v1","category":"cs.CV"}
{"created":"2024-04-29 17:59:30","title":"DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing","abstract":"We consider the problem of editing 3D objects and scenes based on open-ended language instructions. The established paradigm to solve this problem is to use a 2D image generator or editor to guide the 3D editing process. However, this is often slow as it requires do update a computationally expensive 3D representations such as a neural radiance field, and to do so by using contradictory guidance from a 2D model which is inherently not multi-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two ways. First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent. We do so by utilizing a training-free approach which integrates cues from the underlying 3D geometry of the scene. Second, given a multi-view consistent edited sequence of images of the object, we directly and efficiently optimize the 3D object representation, which is based on 3D Gaussian Splatting. Because it does not require to apply edits incrementally and iteratively, DGE is significantly more efficient than existing approaches, and comes with other perks such as allowing selective editing of parts of the scene.","sentences":["We consider the problem of editing 3D objects and scenes based on open-ended language instructions.","The established paradigm to solve this problem is to use a 2D image generator or editor to guide the 3D editing process.","However, this is often slow as it requires do update a computationally expensive 3D representations such as a neural radiance field, and to do so by using contradictory guidance from a 2D model which is inherently not multi-view consistent.","We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two ways.","First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent.","We do so by utilizing a training-free approach which integrates cues from the underlying 3D geometry of the scene.","Second, given a multi-view consistent edited sequence of images of the object, we directly and efficiently optimize the 3D object representation, which is based on 3D Gaussian Splatting.","Because it does not require to apply edits incrementally and iteratively, DGE is significantly more efficient than existing approaches, and comes with other perks such as allowing selective editing of parts of the scene."],"url":"http://arxiv.org/abs/2404.18929v1","category":"cs.CV"}
{"created":"2024-04-29 17:59:16","title":"Symmetry defect of $n$-dimensional complete intersections in $\\mathbb{C}^{2n-1}$","abstract":"Let $X, Y \\subset \\mathbb{C}^{2n-1}$ be $n$-dimensional strong complete intersections in a general position. In this note, we consider the set of midpoints of chords connecting a point $x \\in X$ to a point $y \\in Y$. This set is defined as the image of the map $\\Phi(x,y)=\\frac{x+y}{2}.$ Under geometric conditions on $X$ and $Y$, we prove that the symmetry defect of $X$ and $Y$, which is the bifurcation set $B(X,Y)$ of the mapping $\\Phi$, is an algebraic variety, characterized by a topological invariant. We introduce a hypersurface that approximates the set $B(X,Y)$ and we present an estimate for its degree. Moreover, for any two $n$-dimensional strong complete intersections $X,Y\\subset \\mathbb{C}^{2n-1}$ (including the case $X=Y$) we introduce a generic symmetry defect set $\\tilde{B}(X,Y)$ of $X$ and $Y$, which is defined up to homeomorphism.","sentences":["Let $X, Y \\subset \\mathbb{C}^{2n-1}$ be $n$-dimensional strong complete intersections in a general position.","In this note, we consider the set of midpoints of chords connecting a point $x \\in X$ to a point $y \\in Y$.","This set is defined as the image of the map $\\Phi(x,y)=\\frac{x+y}{2}.$ Under geometric conditions on $X$ and $Y$, we prove that the symmetry defect of $X$ and $Y$, which is the bifurcation set $B(X,Y)$ of the mapping $\\Phi$, is an algebraic variety, characterized by a topological invariant.","We introduce a hypersurface that approximates the set $B(X,Y)$ and we present an estimate for its degree.","Moreover, for any two $n$-dimensional strong complete intersections $X,Y\\subset \\mathbb{C}^{2n-1}$ (including the case $X=Y$) we introduce a generic symmetry defect set $\\tilde{B}(X,Y)$ of $X$ and $Y$, which is defined up to homeomorphism."],"url":"http://arxiv.org/abs/2404.18927v1","category":"math.AG"}
{"created":"2024-04-29 17:59:16","title":"Stylus: Automatic Adapter Selection for Diffusion Models","abstract":"Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions. This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model. See stylus-diffusion.github.io for more.","sentences":["Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs.","As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters-most of which are highly customized with insufficient descriptions.","This paper explores the problem of matching the prompt to a set of relevant adapters, built on recent work that highlight the performance gains of composing adapters.","We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords.","Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt.","To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings.","In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model.","See stylus-diffusion.github.io for more."],"url":"http://arxiv.org/abs/2404.18928v1","category":"cs.CV"}
{"created":"2024-04-29 17:58:30","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","abstract":"In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning. Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies. To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information. Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation. Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal. Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently. For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage. Extensive real-world alignment experiments verify the effectiveness of the proposed approach.","sentences":["In the classical Reinforcement Learning from Human Feedback (RLHF) framework, Proximal Policy Optimization (PPO) is employed to learn from sparse, sentence-level rewards -- a challenging scenario in traditional deep reinforcement learning.","Despite the great successes of PPO in the alignment of state-of-the-art closed-source large language models (LLMs), its open-source implementation is still largely sub-optimal, as widely reported by numerous research studies.","To address these issues, we introduce a framework that models RLHF problems as a Markov decision process (MDP), enabling the capture of fine-grained token-wise information.","Furthermore, we provide theoretical insights that demonstrate the superiority of our MDP framework over the previous sentence-level bandit formulation.","Under this framework, we introduce an algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which learns the token-wise reward function from preference data and performs policy optimization based on this learned token-wise reward signal.","Theoretically, \\texttt{RTO} is proven to have the capability of finding the near-optimal policy sample-efficiently.","For its practical implementation, \\texttt{RTO} innovatively integrates Direct Preference Optimization (DPO) and PPO.","DPO, originally derived from sparse sentence rewards, surprisingly provides us with a token-wise characterization of response quality, which is seamlessly incorporated into our subsequent PPO training stage.","Extensive real-world alignment experiments verify the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2404.18922v1","category":"cs.LG"}
{"created":"2024-04-29 17:58:28","title":"Anomaly and invertible field theory with higher-form symmetry: Extended group cohomology","abstract":"In the realm of invertible symmetry, the topological approach based on classifying spaces dominates the classification of 't Hooft anomalies and symmetry protected topological phases. In contrast, except for discrete 0-form symmetry, a systematic algebraic approach based on cochains remains poorly explored. In this work, we investigate the systematic algebraic approach for discrete higher-form symmetry with a trivial higher-group structure. Studying the discrete formulation of invertible field theories in arbitrary dimension, we extract a purely algebraic structure that we call extended group cohomology, which directly characterizes and classifies the lattice lagrangian of invertible field theories and the behavior of anomalous topological operators. Using techniques from simplicial homotopy theory, we prove the isomorphism between extended group cohomology and cohomology of classifying spaces. The proof is based on an explicit construction of Eilenberg-MacLane spaces and their products. Our findings also clarify the discrete formulation of a class of generalized Dijkgraaf-Witten-Yetter models.","sentences":["In the realm of invertible symmetry, the topological approach based on classifying spaces dominates the classification of 't Hooft anomalies and symmetry protected topological phases.","In contrast, except for discrete 0-form symmetry, a systematic algebraic approach based on cochains remains poorly explored.","In this work, we investigate the systematic algebraic approach for discrete higher-form symmetry with a trivial higher-group structure.","Studying the discrete formulation of invertible field theories in arbitrary dimension, we extract a purely algebraic structure that we call extended group cohomology, which directly characterizes and classifies the lattice lagrangian of invertible field theories and the behavior of anomalous topological operators.","Using techniques from simplicial homotopy theory, we prove the isomorphism between extended group cohomology and cohomology of classifying spaces.","The proof is based on an explicit construction of Eilenberg-MacLane spaces and their products.","Our findings also clarify the discrete formulation of a class of generalized Dijkgraaf-Witten-Yetter models."],"url":"http://arxiv.org/abs/2404.18921v1","category":"hep-th"}
{"created":"2024-04-29 17:58:14","title":"TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation","abstract":"Recent advances in diffusion models can generate high-quality and stunning images from text. However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns. To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation. Within this framework, LLMs, acting as a \"Screenwriter\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image. Based on these, Theatergen generate a list of character images and extract guidance information, akin to the \"Rehearsal\". Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the \"Final Performance\". With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images. Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions. Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance. Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation. Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly. It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity.","sentences":["Recent advances in diffusion models can generate high-quality and stunning images from text.","However, multi-turn image generation, which is of high demand in real-world scenarios, still faces challenges in maintaining semantic consistency between images and texts, as well as contextual consistency of the same subject across multiple interactive turns.","To address this issue, we introduce TheaterGen, a training-free framework that integrates large language models (LLMs) and text-to-image (T2I) models to provide the capability of multi-turn image generation.","Within this framework, LLMs, acting as a \"Screenwriter\", engage in multi-turn interaction, generating and managing a standardized prompt book that encompasses prompts and layout designs for each character in the target image.","Based on these, Theatergen generate a list of character images and extract guidance information, akin to the \"Rehearsal\".","Subsequently, through incorporating the prompt book and guidance information into the reverse denoising process of T2I diffusion models, Theatergen generate the final image, as conducting the \"Final Performance\".","With the effective management of prompt books and character images, TheaterGen significantly improves semantic and contextual consistency in synthesized images.","Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions.","Different from previous multi-turn benchmarks, CMIGBench does not define characters in advance.","Both the tasks of story generation and multi-turn editing are included on CMIGBench for comprehensive evaluation.","Extensive experimental results show that TheaterGen outperforms state-of-the-art methods significantly.","It raises the performance bar of the cutting-edge Mini DALLE 3 model by 21% in average character-character similarity and 19% in average text-image similarity."],"url":"http://arxiv.org/abs/2404.18919v1","category":"cs.CV"}
{"created":"2024-04-29 17:56:40","title":"Observation of Generalized t-J Spin Dynamics with Tunable Dipolar Interactions","abstract":"Long-range and anisotropic dipolar interactions profoundly modify the dynamics of particles hopping in a periodic lattice potential. Here, we report the realization of a generalized t-J model with dipolar interactions using a system of ultracold fermionic molecules with spin encoded in the two lowest rotational states. We systematically explore the role of dipolar Ising and spin-exchange couplings and the effect of motion on spin dynamics. The model parameters can be controlled independently, with dipolar couplings tuned by electric fields and motion regulated by optical lattices. Using Ramsey spectroscopy, we observed interaction-driven contrast decay that depends strongly both on the strength of the anisotropy between Ising and spin-exchange couplings and on motion. These observations are supported by theory models established in different motional regimes that provide intuitive pictures of the underlying physics. This study paves the way for future exploration of kinetic spin dynamics and quantum magnetism with highly tunable molecular platforms in regimes challenging for existing numerical and analytical methods, and it could shed light on the complex behaviors observed in real materials.","sentences":["Long-range and anisotropic dipolar interactions profoundly modify the dynamics of particles hopping in a periodic lattice potential.","Here, we report the realization of a generalized t-J model with dipolar interactions using a system of ultracold fermionic molecules with spin encoded in the two lowest rotational states.","We systematically explore the role of dipolar Ising and spin-exchange couplings and the effect of motion on spin dynamics.","The model parameters can be controlled independently, with dipolar couplings tuned by electric fields and motion regulated by optical lattices.","Using Ramsey spectroscopy, we observed interaction-driven contrast decay that depends strongly both on the strength of the anisotropy between Ising and spin-exchange couplings and on motion.","These observations are supported by theory models established in different motional regimes that provide intuitive pictures of the underlying physics.","This study paves the way for future exploration of kinetic spin dynamics and quantum magnetism with highly tunable molecular platforms in regimes challenging for existing numerical and analytical methods, and it could shed light on the complex behaviors observed in real materials."],"url":"http://arxiv.org/abs/2404.18916v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 17:54:06","title":"Two-axis twisting using Floquet-engineered XYZ spin models with polar molecules","abstract":"Polar molecules confined in an optical lattice are a versatile platform to explore spin-motion dynamics based on strong, long-range dipolar interactions. The precise tunability of Ising and spin-exchange interactions with both microwave and dc electric fields makes the molecular system particularly suitable for engineering complex many-body dynamics. Here, we used Floquet engineering to realize interesting quantum many-body systems of polar molecules. Using a spin encoded in the two lowest rotational states of ultracold KRb molecules, we mutually validated XXZ spin models tuned by a Floquet microwave pulse sequence against those tuned by a dc electric field through observations of Ramsey contrast dynamics, setting the stage for the realization of Hamiltonians inaccessible with static fields. In particular, we observed two-axis twisting mean-field dynamics, generated by a Floquet-engineered XYZ model using itinerant molecules in 2D layers. In the future, Floquet-engineered Hamiltonians could generate entangled states for molecule-based precision measurement or could take advantage of the rich molecular structure for quantum simulation of multi-level systems.","sentences":["Polar molecules confined in an optical lattice are a versatile platform to explore spin-motion dynamics based on strong, long-range dipolar interactions.","The precise tunability of Ising and spin-exchange interactions with both microwave and dc electric fields makes the molecular system particularly suitable for engineering complex many-body dynamics.","Here, we used Floquet engineering to realize interesting quantum many-body systems of polar molecules.","Using a spin encoded in the two lowest rotational states of ultracold KRb molecules, we mutually validated XXZ spin models tuned by a Floquet microwave pulse sequence against those tuned by a dc electric field through observations of Ramsey contrast dynamics, setting the stage for the realization of Hamiltonians inaccessible with static fields.","In particular, we observed two-axis twisting mean-field dynamics, generated by a Floquet-engineered XYZ model using itinerant molecules in 2D layers.","In the future, Floquet-engineered Hamiltonians could generate entangled states for molecule-based precision measurement or could take advantage of the rich molecular structure for quantum simulation of multi-level systems."],"url":"http://arxiv.org/abs/2404.18913v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 17:53:54","title":"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting","abstract":"Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M). The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.","sentences":["Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution.","However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly.","Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model.","We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability.","It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model.","To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens.","Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold.","Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo.","Under single-sequence verification, Kangaroo achieves speedups up to $1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional parameters (67M compared to 591M).","The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo."],"url":"http://arxiv.org/abs/2404.18911v1","category":"cs.CL"}
{"created":"2024-04-29 17:51:47","title":"Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty","abstract":"To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.","sentences":["To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties.","While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions.","This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set.","This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria.","Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria.","We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length."],"url":"http://arxiv.org/abs/2404.18909v1","category":"cs.LG"}
{"created":"2024-04-29 17:50:32","title":"On the uncommonness of minimal rank-2 systems of linear equations","abstract":"We prove that suitably generic pairs of linear equations on an even number of variables are uncommon. This verifies a conjecture of Kam\\v{c}ev, Morrison and the second author. Moreover, we prove that any large system containing such a $(2\\times k)$-system as a minimal subsystem is uncommon.","sentences":["We prove that suitably generic pairs of linear equations on an even number of variables are uncommon.","This verifies a conjecture of Kam\\v{c}ev, Morrison and the second author.","Moreover, we prove that any large system containing such a $(2\\times k)$-system as a minimal subsystem is uncommon."],"url":"http://arxiv.org/abs/2404.18908v1","category":"math.CO"}
{"created":"2024-04-29 17:44:32","title":"On Clustering Induced Voronoi Diagrams","abstract":"In this paper, we study a generalization of the classical Voronoi diagram, called clustering induced Voronoi diagram (CIVD). Different from the traditional model, CIVD takes as its sites the power set $U$ of an input set $P$ of objects. For each subset $C$ of $P$, CIVD uses an influence function $F(C,q)$ to measure the total (or joint) influence of all objects in $C$ on an arbitrary point $q$ in the space $\\mathbb{R}^d$, and determines the influence-based Voronoi cell in $\\mathbb{R}^d$ for $C$. This generalized model offers a number of new features (e.g., simultaneous clustering and space partition) to Voronoi diagram which are useful in various new applications. We investigate the general conditions for the influence function which ensure the existence of a small-size (e.g., nearly linear) approximate CIVD for a set $P$ of $n$ points in $\\mathbb{R}^d$ for some fixed $d$. To construct CIVD, we first present a standalone new technique, called approximate influence (AI) decomposition, for the general CIVD problem. With only $O(n\\log n)$ time, the AI decomposition partitions the space $\\mathbb{R}^{d}$ into a nearly linear number of cells so that all points in each cell receive their approximate maximum influence from the same (possibly unknown) site (i.e., a subset of $P$). Based on this technique, we develop assignment algorithms to determine a proper site for each cell in the decomposition and form various $(1-\\epsilon)$-approximate CIVDs for some small fixed $\\epsilon>0$. Particularly, we consider two representative CIVD problems, vector CIVD and density-based CIVD, and show that both of them admit fast assignment algorithms; consequently, their $(1-\\epsilon)$-approximate CIVDs can be built in $O(n \\log^{\\max\\{3,d+1\\}}n)$ and $O(n \\log^{2} n)$ time, respectively.","sentences":["In this paper, we study a generalization of the classical Voronoi diagram, called clustering induced Voronoi diagram (CIVD).","Different from the traditional model, CIVD takes as its sites the power set $U$ of an input set $P$ of objects.","For each subset $C$ of $P$, CIVD uses an influence function $F(C,q)$ to measure the total (or joint) influence of all objects in $C$ on an arbitrary point $q$ in the space $\\mathbb{R}^d$, and determines the influence-based Voronoi cell in $\\mathbb{R}^d$ for $C$.","This generalized model offers a number of new features (e.g., simultaneous clustering and space partition) to Voronoi diagram which are useful in various new applications.","We investigate the general conditions for the influence function which ensure the existence of a small-size (e.g., nearly linear) approximate CIVD for a set $P$ of $n$ points in $\\mathbb{R}^d$ for some fixed $d$. To construct CIVD, we first present a standalone new technique, called approximate influence (AI) decomposition, for the general CIVD problem.","With only $O(n\\log n)$ time, the AI decomposition partitions the space $\\mathbb{R}^{d}$ into a nearly linear number of cells so that all points in each cell receive their approximate maximum influence from the same (possibly unknown) site (i.e., a subset of $P$).","Based on this technique, we develop assignment algorithms to determine a proper site for each cell in the decomposition and form various $(1-\\epsilon)$-approximate CIVDs for some small fixed $\\epsilon>0$. Particularly, we consider two representative CIVD problems, vector CIVD and density-based CIVD, and show that both of them admit fast assignment algorithms; consequently, their $(1-\\epsilon)$-approximate CIVDs can be built in $O(n \\log^{\\max\\{3,d+1\\}}n)$ and $O(n \\log^{2} n)$ time, respectively."],"url":"http://arxiv.org/abs/2404.18906v1","category":"cs.CG"}
{"created":"2024-04-29 17:44:28","title":"Detecting critical treatment effect bias in small subgroups","abstract":"Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.","sentences":["Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice.","Observational studies, on the other hand, cover a broader patient population but are prone to various biases.","Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial.","We propose a novel strategy to benchmark observational studies beyond the average treatment effect.","First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance.","We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study.","Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge."],"url":"http://arxiv.org/abs/2404.18905v1","category":"stat.ME"}
{"created":"2024-04-29 17:43:38","title":"On classes of bounded tree rank, their interpretations, and efficient sparsification","abstract":"Graph classes of bounded tree rank were introduced recently in the context of the model checking problem for first-order logic of graphs. These graph classes are a common generalization of graph classes of bounded degree and bounded treedepth, and they are a special case of graph classes of bounded expansion. We introduce a notion of decomposition for these classes and show that these decompositions can be efficiently computed. Also, a natural extension of our decomposition leads to a new characterization and decomposition for graph classes of bounded expansion (and an efficient algorithm computing this decomposition).   We then focus on interpretations of graph classes of bounded tree rank. We give a characterization of graph classes interpretable in graph classes of tree rank $2$. Importantly, our characterization leads to an efficient sparsification procedure: For any graph class $C$ interpretable in a efficiently bounded graph class of tree rank at most $2$, there is a polynomial time algorithm that to any $G \\in C$ computes a (sparse) graph $H$ from a fixed graph class of tree rank at most $2$ such that $G = I(H)$ for a fixed interpretation $I$. To the best of our knowledge, this is the first efficient \"interpretation reversal\" result that generalizes the result of Gajarsk\\'y et al. [LICS 2016], who showed an analogous result for graph classes interpretable in classes of graphs of bounded degree.","sentences":["Graph classes of bounded tree rank were introduced recently in the context of the model checking problem for first-order logic of graphs.","These graph classes are a common generalization of graph classes of bounded degree and bounded treedepth, and they are a special case of graph classes of bounded expansion.","We introduce a notion of decomposition for these classes and show that these decompositions can be efficiently computed.","Also, a natural extension of our decomposition leads to a new characterization and decomposition for graph classes of bounded expansion (and an efficient algorithm computing this decomposition).   ","We then focus on interpretations of graph classes of bounded tree rank.","We give a characterization of graph classes interpretable in graph classes of tree rank $2$. Importantly, our characterization leads to an efficient sparsification procedure: For any graph class $C$ interpretable in a efficiently bounded graph class of tree rank at most $2$, there is a polynomial time algorithm that to any $G \\in C$ computes a (sparse) graph $H$ from a fixed graph class of tree rank at most $2$ such that $G = I(H)$ for a fixed interpretation $I$. To the best of our knowledge, this is the first efficient \"interpretation reversal\" result that generalizes the result of Gajarsk\\'y et al.","[LICS 2016], who showed an analogous result for graph classes interpretable in classes of graphs of bounded degree."],"url":"http://arxiv.org/abs/2404.18904v1","category":"cs.DM"}
{"created":"2024-04-29 17:30:36","title":"Learning general Gaussian mixtures with efficient score matching","abstract":"We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions. We make no separation assumptions on the underlying mixture components: we only require that the covariance matrices have bounded condition number and that the means and covariances lie in a ball of bounded radius. We give an algorithm that draws $d^{\\mathrm{poly}(k/\\varepsilon)}$ samples from the target mixture, runs in sample-polynomial time, and constructs a sampler whose output distribution is $\\varepsilon$-far from the unknown mixture in total variation. Prior works for this problem either (i) required exponential runtime in the dimension $d$, (ii) placed strong assumptions on the instance (e.g., spherical covariances or clusterability), or (iii) had doubly exponential dependence on the number of components $k$.   Our approach departs from commonly used techniques for this problem like the method of moments. Instead, we leverage a recently developed reduction, based on diffusion models, from distribution learning to a supervised learning task called score matching. We give an algorithm for the latter by proving a structural result showing that the score function of a Gaussian mixture can be approximated by a piecewise-polynomial function, and there is an efficient algorithm for finding it. To our knowledge, this is the first example of diffusion models achieving a state-of-the-art theoretical guarantee for an unsupervised learning task.","sentences":["We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions.","We make no separation assumptions on the underlying mixture components: we only require that the covariance matrices have bounded condition number and that the means and covariances lie in a ball of bounded radius.","We give an algorithm that draws $d^{\\mathrm{poly}(k/\\varepsilon)}$ samples from the target mixture, runs in sample-polynomial time, and constructs a sampler whose output distribution is $\\varepsilon$-far from the unknown mixture in total variation.","Prior works for this problem either (i) required exponential runtime in the dimension $d$, (ii) placed strong assumptions on the instance (e.g., spherical covariances or clusterability), or (iii) had doubly exponential dependence on the number of components $k$.   Our approach departs from commonly used techniques for this problem like the method of moments.","Instead, we leverage a recently developed reduction, based on diffusion models, from distribution learning to a supervised learning task called score matching.","We give an algorithm for the latter by proving a structural result showing that the score function of a Gaussian mixture can be approximated by a piecewise-polynomial function, and there is an efficient algorithm for finding it.","To our knowledge, this is the first example of diffusion models achieving a state-of-the-art theoretical guarantee for an unsupervised learning task."],"url":"http://arxiv.org/abs/2404.18893v1","category":"cs.DS"}
{"created":"2024-04-29 17:27:37","title":"IPixMatch: Boost Semi-supervised Semantic Segmentation with Inter-Pixel Relation","abstract":"The scarcity of labeled data in real-world scenarios is a critical bottleneck of deep learning's effectiveness. Semi-supervised semantic segmentation has been a typical solution to achieve a desirable tradeoff between annotation cost and segmentation performance. However, previous approaches, whether based on consistency regularization or self-training, tend to neglect the contextual knowledge embedded within inter-pixel relations. This negligence leads to suboptimal performance and limited generalization. In this paper, we propose a novel approach IPixMatch designed to mine the neglected but valuable Inter-Pixel information for semi-supervised learning. Specifically, IPixMatch is constructed as an extension of the standard teacher-student network, incorporating additional loss terms to capture inter-pixel relations. It shines in low-data regimes by efficiently leveraging the limited labeled data and extracting maximum utility from the available unlabeled data. Furthermore, IPixMatch can be integrated seamlessly into most teacher-student frameworks without the need of model modification or adding additional components. Our straightforward IPixMatch method demonstrates consistent performance improvements across various benchmark datasets under different partitioning protocols.","sentences":["The scarcity of labeled data in real-world scenarios is a critical bottleneck of deep learning's effectiveness.","Semi-supervised semantic segmentation has been a typical solution to achieve a desirable tradeoff between annotation cost and segmentation performance.","However, previous approaches, whether based on consistency regularization or self-training, tend to neglect the contextual knowledge embedded within inter-pixel relations.","This negligence leads to suboptimal performance and limited generalization.","In this paper, we propose a novel approach IPixMatch designed to mine the neglected but valuable Inter-Pixel information for semi-supervised learning.","Specifically, IPixMatch is constructed as an extension of the standard teacher-student network, incorporating additional loss terms to capture inter-pixel relations.","It shines in low-data regimes by efficiently leveraging the limited labeled data and extracting maximum utility from the available unlabeled data.","Furthermore, IPixMatch can be integrated seamlessly into most teacher-student frameworks without the need of model modification or adding additional components.","Our straightforward IPixMatch method demonstrates consistent performance improvements across various benchmark datasets under different partitioning protocols."],"url":"http://arxiv.org/abs/2404.18891v1","category":"cs.CV"}
{"created":"2024-04-29 17:27:08","title":"Hide and Seek: How Does Watermarking Impact Face Recognition?","abstract":"The recent progress in generative models has revolutionized the synthesis of highly realistic images, including face images. This technological development has undoubtedly helped face recognition, such as training data augmentation for higher recognition accuracy and data privacy. However, it has also introduced novel challenges concerning the responsible use and proper attribution of computer generated images. We investigate the impact of digital watermarking, a technique for embedding ownership signatures into images, on the effectiveness of face recognition models. We propose a comprehensive pipeline that integrates face image generation, watermarking, and face recognition to systematically examine this question. The proposed watermarking scheme, based on an encoder-decoder architecture, successfully embeds and recovers signatures from both real and synthetic face images while preserving their visual fidelity. Through extensive experiments, we unveil that while watermarking enables robust image attribution, it results in a slight decline in face recognition accuracy, particularly evident for face images with challenging poses and expressions. Additionally, we find that directly training face recognition models on watermarked images offers only a limited alleviation of this performance decline. Our findings underscore the intricate trade off between watermarking and face recognition accuracy. This work represents a pivotal step towards the responsible utilization of generative models in face recognition and serves to initiate discussions regarding the broader implications of watermarking in biometrics.","sentences":["The recent progress in generative models has revolutionized the synthesis of highly realistic images, including face images.","This technological development has undoubtedly helped face recognition, such as training data augmentation for higher recognition accuracy and data privacy.","However, it has also introduced novel challenges concerning the responsible use and proper attribution of computer generated images.","We investigate the impact of digital watermarking, a technique for embedding ownership signatures into images, on the effectiveness of face recognition models.","We propose a comprehensive pipeline that integrates face image generation, watermarking, and face recognition to systematically examine this question.","The proposed watermarking scheme, based on an encoder-decoder architecture, successfully embeds and recovers signatures from both real and synthetic face images while preserving their visual fidelity.","Through extensive experiments, we unveil that while watermarking enables robust image attribution, it results in a slight decline in face recognition accuracy, particularly evident for face images with challenging poses and expressions.","Additionally, we find that directly training face recognition models on watermarked images offers only a limited alleviation of this performance decline.","Our findings underscore the intricate trade off between watermarking and face recognition accuracy.","This work represents a pivotal step towards the responsible utilization of generative models in face recognition and serves to initiate discussions regarding the broader implications of watermarking in biometrics."],"url":"http://arxiv.org/abs/2404.18890v1","category":"cs.CV"}
{"created":"2024-04-29 17:19:40","title":"A Survey on Diffusion Models for Time Series and Spatio-Temporal Data","abstract":"The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.","sentences":["The study of time series data is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors.","Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions.","Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining.","Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks.","In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain.","In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series data and spatio-temporal data separately.","Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation.","Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks.","Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data.","Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework."],"url":"http://arxiv.org/abs/2404.18886v1","category":"cs.LG"}
{"created":"2024-04-29 17:16:47","title":"The $\u03a9_c \\to \u03c0^+ \\, (\u03c0^0,\\, \u03b7)\\, \u03c0\u039e^*$ reactions and the two $\u039e(1820)$ states","abstract":"We have studied the $\\Omega_c \\to \\pi^+ (\\pi^0, \\eta) \\pi \\Xi^*$ decays, where the final $\\pi \\Xi^*$ comes from the decay of two resonances around the nominal $\\Xi(1820)$, which are generated from the interaction of coupled channels made of a pseudoscalar and a baryon of the decuplet. The $\\pi \\Xi^*$ mass distributions obtained in the six different reactions studied are quite different and we single out four of them, which are free of a tree level contribution, showing more clearly the effect of the resonances. The lower mass resonance is clearly seen as a sharp peak, but the higher mass resonance manifests itself through an interference with the lower one that leads to a dip in the mass distribution around $1850 \\, {\\rm MeV}$. Such a feature is similar to the dip observed in the $s$-wave $\\pi \\pi$ cross section around the $980 \\, {\\rm MeV}$ coming from the interference of the $f_0(500)$ and $f_0(980)$ resonances. Its observation in coming upgrades of present facilities will shed light on the existence of these two resonances and their nature.","sentences":["We have studied the $\\Omega_c \\to \\pi^+ (\\pi^0, \\eta) \\pi \\Xi^*$ decays, where the final $\\pi \\Xi^*$ comes from the decay of two resonances around the nominal $\\Xi(1820)$, which are generated from the interaction of coupled channels made of a pseudoscalar and a baryon of the decuplet.","The $\\pi \\Xi^*$ mass distributions obtained in the six different reactions studied are quite different and we single out four of them, which are free of a tree level contribution, showing more clearly the effect of the resonances.","The lower mass resonance is clearly seen as a sharp peak, but the higher mass resonance manifests itself through an interference with the lower one that leads to a dip in the mass distribution around $1850 \\, {\\rm MeV}$. Such a feature is similar to the dip observed in the $s$-wave $\\pi \\pi$ cross section around the $980 \\, {\\rm MeV}$ coming from the interference of the $f_0(500)$ and $f_0(980)$ resonances.","Its observation in coming upgrades of present facilities will shed light on the existence of these two resonances and their nature."],"url":"http://arxiv.org/abs/2404.18882v1","category":"hep-ph"}
{"created":"2024-04-29 17:16:27","title":"Human-in-the-Loop Synthetic Text Data Inspection with Provenance Tracking","abstract":"Data augmentation techniques apply transformations to existing texts to generate additional data. The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension. Analyzing the synthetically generated texts and their corresponding labels is slow and demanding. To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique. INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling. INSPECTOR allows users to group related texts by their transformation provenance, i.e., the transformations applied to the original text, or feature provenance, the linguistic features of the original text. For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model. In a user study, INSPECTOR increases the number of texts with correct labels identified by 3X on a sentiment analysis task and by 4X on a hate speech detection task. The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique. Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful. Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort. This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort.","sentences":["Data augmentation techniques apply transformations to existing texts to generate additional data.","The transformations may produce low-quality texts, where the meaning of the text is changed and the text may even be mangled beyond human comprehension.","Analyzing the synthetically generated texts and their corresponding labels is slow and demanding.","To winnow out texts with incorrect labels, we develop INSPECTOR, a human-in-the-loop data inspection technique.","INSPECTOR combines the strengths of provenance tracking techniques with assistive labeling.","INSPECTOR allows users to group related texts by their transformation provenance, i.e., the transformations applied to the original text, or feature provenance, the linguistic features of the original text.","For assistive labeling, INSPECTOR computes metrics that approximate data quality, and allows users to compare the corresponding label of each text against the predictions of a large language model.","In a user study, INSPECTOR increases the number of texts with correct labels identified by 3X on a sentiment analysis task and by 4X on a hate speech detection task.","The participants found grouping the synthetically generated texts by their common transformation to be the most useful technique.","Surprisingly, grouping texts by common linguistic features was perceived to be unhelpful.","Contrary to prior work, our study finds that no single technique obviates the need for human inspection effort.","This validates the design of INSPECTOR which combines both analysis of data provenance and assistive labeling to reduce human inspection effort."],"url":"http://arxiv.org/abs/2404.18881v1","category":"cs.HC"}
{"created":"2024-04-29 17:09:15","title":"Mapping eccentricity evolutions between numerical relativity and effective-one-body gravitational waveforms","abstract":"Orbital eccentricity in compact binaries is considered to be a key tracer of their astrophysical origin, and can be inferred from gravitational-wave observations due to its imprint on the emitted signal. For a robust measurement, accurate waveform models are needed. However, ambiguities in the definition of eccentricity can obfuscate the physical meaning and result in seemingly discrepant measurements. In this work we present a suite of 28 new numerical relativity simulations of eccentric, aligned-spin binary black holes with mass ratios between 1 and 6 and initial post-Newtonian eccentricities between 0.05 and 0.3. We then develop a robust pipeline for measuring the eccentricity evolution as a function of frequency from gravitational-wave observables that is applicable even to signals that span at least $\\gtrsim 7$ orbits. We assess the reliability of our procedure and quantify its robustness under different assumptions on the data. Using the eccentricity measured at the first apastron, we initialise effective-one-body waveforms and quantify how the precision in the eccentricity measurement, and therefore the choice of the initial conditions, impacts the agreement with the numerical data. We find that even small deviations in the initial eccentricity can lead to non-negligible differences in the phase and amplitude of the waveforms. However, we demonstrate that we can reliably map the eccentricities between the simulation data and analytic models, which is crucial for robustly building eccentric hybrid waveforms, and to improve the accuracy of eccentric waveform models in the strong-field regime.","sentences":["Orbital eccentricity in compact binaries is considered to be a key tracer of their astrophysical origin, and can be inferred from gravitational-wave observations due to its imprint on the emitted signal.","For a robust measurement, accurate waveform models are needed.","However, ambiguities in the definition of eccentricity can obfuscate the physical meaning and result in seemingly discrepant measurements.","In this work we present a suite of 28 new numerical relativity simulations of eccentric, aligned-spin binary black holes with mass ratios between 1 and 6 and initial post-Newtonian eccentricities between 0.05 and 0.3.","We then develop a robust pipeline for measuring the eccentricity evolution as a function of frequency from gravitational-wave observables that is applicable even to signals that span at least $\\gtrsim 7$ orbits.","We assess the reliability of our procedure and quantify its robustness under different assumptions on the data.","Using the eccentricity measured at the first apastron, we initialise effective-one-body waveforms and quantify how the precision in the eccentricity measurement, and therefore the choice of the initial conditions, impacts the agreement with the numerical data.","We find that even small deviations in the initial eccentricity can lead to non-negligible differences in the phase and amplitude of the waveforms.","However, we demonstrate that we can reliably map the eccentricities between the simulation data and analytic models, which is crucial for robustly building eccentric hybrid waveforms, and to improve the accuracy of eccentric waveform models in the strong-field regime."],"url":"http://arxiv.org/abs/2404.18875v1","category":"gr-qc"}
{"created":"2024-04-29 17:06:44","title":"OpenStreetView-5M: The Many Roads to Global Visual Geolocation","abstract":"Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To address this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and training strategies. All associated codes and models can be found at https://github.com/gastruc/osv5m.","sentences":["Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms.","Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential.","To address this issue, we introduce OpenStreetView-5M, a large-scale, open-access dataset comprising over 5.1 million geo-referenced street view images, covering 225 countries and territories.","In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization.","To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the-art image encoders, spatial representations, and training strategies.","All associated codes and models can be found at https://github.com/gastruc/osv5m."],"url":"http://arxiv.org/abs/2404.18873v1","category":"cs.CV"}
{"created":"2024-04-29 17:00:53","title":"More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness","abstract":"The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power. Despite the effectiveness of preference learning algorithms like Reinforcement Learning From Human Feedback (RLHF) in aligning human preferences, their assumed improvements on model trustworthiness haven't been thoroughly testified. Toward this end, this study investigates how models that have been aligned with general-purpose preference data on helpfulness and harmlessness perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. For model alignment, we focus on three widely used RLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO). Through extensive empirical investigations, we discover that the improvement in trustworthiness by RLHF is far from guaranteed, and there exists a complex interplay between preference data, alignment algorithms, and specific trustworthiness aspects. Together, our results underscore the need for more nuanced approaches for model alignment. By shedding light on the intricate dynamics of these components within model alignment, we hope this research will guide the community towards developing language models that are both capable and trustworthy.","sentences":["The surge in Large Language Models (LLMs) development has led to improved performance on cognitive tasks as well as an urgent need to align these models with human values in order to safely exploit their power.","Despite the effectiveness of preference learning algorithms like Reinforcement Learning From Human Feedback (RLHF) in aligning human preferences, their assumed improvements on model trustworthiness haven't been thoroughly testified.","Toward this end, this study investigates how models that have been aligned with general-purpose preference data on helpfulness and harmlessness perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy.","For model alignment, we focus on three widely used RLHF variants: Supervised Finetuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO).","Through extensive empirical investigations, we discover that the improvement in trustworthiness by RLHF is far from guaranteed, and there exists a complex interplay between preference data, alignment algorithms, and specific trustworthiness aspects.","Together, our results underscore the need for more nuanced approaches for model alignment.","By shedding light on the intricate dynamics of these components within model alignment, we hope this research will guide the community towards developing language models that are both capable and trustworthy."],"url":"http://arxiv.org/abs/2404.18870v1","category":"cs.CL"}
{"created":"2024-04-29 17:00:20","title":"Learning Mixtures of Gaussians Using Diffusion Models","abstract":"We give a new algorithm for learning mixtures of $k$ Gaussians (with identity covariance in $\\mathbb{R}^n$) to TV error $\\varepsilon$, with quasi-polynomial ($O(n^{\\text{poly log}\\left(\\frac{n+k}{\\varepsilon}\\right)})$) time and sample complexity, under a minimum weight assumption. Unlike previous approaches, most of which are algebraic in nature, our approach is analytic and relies on the framework of diffusion models. Diffusion models are a modern paradigm for generative modeling, which typically rely on learning the score function (gradient log-pdf) along a process transforming a pure noise distribution, in our case a Gaussian, to the data distribution. Despite their dazzling performance in tasks such as image generation, there are few end-to-end theoretical guarantees that they can efficiently learn nontrivial families of distributions; we give some of the first such guarantees. We proceed by deriving higher-order Gaussian noise sensitivity bounds for the score functions for a Gaussian mixture to show that that they can be inductively learned using piecewise polynomial regression (up to poly-logarithmic degree), and combine this with known convergence results for diffusion models. Our results extend to continuous mixtures of Gaussians where the mixing distribution is supported on a union of $k$ balls of constant radius. In particular, this applies to the case of Gaussian convolutions of distributions on low-dimensional manifolds, or more generally sets with small covering number.","sentences":["We give a new algorithm for learning mixtures of $k$ Gaussians (with identity covariance in $\\mathbb{R}^n$) to TV error $\\varepsilon$, with quasi-polynomial ($O(n^{\\text{poly log}\\left(\\frac{n+k}{\\varepsilon}\\right)})$) time and sample complexity, under a minimum weight assumption.","Unlike previous approaches, most of which are algebraic in nature, our approach is analytic and relies on the framework of diffusion models.","Diffusion models are a modern paradigm for generative modeling, which typically rely on learning the score function (gradient log-pdf) along a process transforming a pure noise distribution, in our case a Gaussian, to the data distribution.","Despite their dazzling performance in tasks such as image generation, there are few end-to-end theoretical guarantees that they can efficiently learn nontrivial families of distributions; we give some of the first such guarantees.","We proceed by deriving higher-order Gaussian noise sensitivity bounds for the score functions for a Gaussian mixture to show that that they can be inductively learned using piecewise polynomial regression (up to poly-logarithmic degree), and combine this with known convergence results for diffusion models.","Our results extend to continuous mixtures of Gaussians where the mixing distribution is supported on a union of $k$ balls of constant radius.","In particular, this applies to the case of Gaussian convolutions of distributions on low-dimensional manifolds, or more generally sets with small covering number."],"url":"http://arxiv.org/abs/2404.18869v1","category":"cs.LG"}
{"created":"2024-04-29 16:56:33","title":"Feminist Interaction Techniques: Deterring Non-Consensual Screenshots with Interaction Techniques","abstract":"Non-consensual Intimate Media (NCIM) refers to the distribution of sexual or intimate content without consent. NCIM is common and causes significant emotional, financial, and reputational harm. We developed Hands-Off, an interaction technique for messaging applications that deters non-consensual screenshots. Hands-Off requires recipients to perform a hand gesture in the air, above the device, to unlock media -- which makes simultaneous screenshotting difficult. A lab study shows that Hands-Off gestures are easy to perform and reduce non-consensual screenshots by 67 percent. We conclude by generalizing this approach and introduce the idea of Feminist Interaction Techniques (FIT), interaction techniques that encode feminist values and speak to societal problems, and reflect on FIT's opportunities and limitations.","sentences":["Non-consensual Intimate Media (NCIM) refers to the distribution of sexual or intimate content without consent.","NCIM is common and causes significant emotional, financial, and reputational harm.","We developed Hands-Off, an interaction technique for messaging applications that deters non-consensual screenshots.","Hands-Off requires recipients to perform a hand gesture in the air, above the device, to unlock media -- which makes simultaneous screenshotting difficult.","A lab study shows that Hands-Off gestures are easy to perform and reduce non-consensual screenshots by 67 percent.","We conclude by generalizing this approach and introduce the idea of Feminist Interaction Techniques (FIT), interaction techniques that encode feminist values and speak to societal problems, and reflect on FIT's opportunities and limitations."],"url":"http://arxiv.org/abs/2404.18867v1","category":"cs.HC"}
{"created":"2024-04-29 16:54:21","title":"K\u00e4hler Soliton Surfaces Are Generically Toric","abstract":"Let $(M, g, \\omega, f, \\lambda)$ be a K\\\"{a}hler gradient Ricci soliton in real dimension four. The first theorem states that it is an integrable Hamiltonian system in a classical sense. Furthermore, it is either of cohomogeneity one or the integrals of motion are given by the potential function $f$ and the scalar curvature $\\text{S}$. The second theorem states that it must be toric under a generic assumption. That is, one assumes that the system is non-degenerate and the potential function $f$ is proper. In case $\\lambda=0$, one further assumes that $f$ is bounded below or above. Then there is an effective, completely integrable Hamiltonian toric $\\mathbb{T}^2$- action on $(M, \\omega)$.","sentences":["Let $(M, g, \\omega, f, \\lambda)$ be a K\\\"{a}hler gradient Ricci soliton in real dimension four.","The first theorem states that it is an integrable Hamiltonian system in a classical sense.","Furthermore, it is either of cohomogeneity one or the integrals of motion are given by the potential function $f$ and the scalar curvature $\\text{S}$. The second theorem states that it must be toric under a generic assumption.","That is, one assumes that the system is non-degenerate and the potential function $f$ is proper.","In case $\\lambda=0$, one further assumes that $f$ is bounded below or above.","Then there is an effective, completely integrable Hamiltonian toric $\\mathbb{T}^2$- action on $(M, \\omega)$."],"url":"http://arxiv.org/abs/2404.18866v1","category":"math.DG"}
{"created":"2024-04-29 16:52:57","title":"Truth-value judgment in language models: belief directions are context sensitive","abstract":"Recent work has demonstrated that the latent spaces of large language models (LLMs) contain directions predictive of the truth of sentences. Multiple methods recover such directions and build probes that are described as getting at a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon, looking closely at the impact of context on the probes. Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences. Specifically, we quantify the responsiveness of the probes to the presence of (negated) supporting and contradicting sentences, and score the probes on their consistency. We also perform a causal intervention experiment, investigating whether moving the representation of a premise along these belief directions influences the position of the hypothesis along that same direction. We find that the probes we test are generally context sensitive, but that contexts which should not affect the truth often still impact the probe outputs. Our experiments show that the type of errors depend on the layer, the (type of) model, and the kind of data. Finally, our results suggest that belief directions are (one of the) causal mediators in the inference process that incorporates in-context information.","sentences":["Recent work has demonstrated that the latent spaces of large language models (LLMs) contain directions predictive of the truth of sentences.","Multiple methods recover such directions and build probes that are described as getting at a model's \"knowledge\" or \"beliefs\".","We investigate this phenomenon, looking closely at the impact of context on the probes.","Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences.","Specifically, we quantify the responsiveness of the probes to the presence of (negated) supporting and contradicting sentences, and score the probes on their consistency.","We also perform a causal intervention experiment, investigating whether moving the representation of a premise along these belief directions influences the position of the hypothesis along that same direction.","We find that the probes we test are generally context sensitive, but that contexts which should not affect the truth often still impact the probe outputs.","Our experiments show that the type of errors depend on the layer, the (type of) model, and the kind of data.","Finally, our results suggest that belief directions are (one of the) causal mediators in the inference process that incorporates in-context information."],"url":"http://arxiv.org/abs/2404.18865v1","category":"cs.CL"}
{"created":"2024-04-29 16:52:38","title":"Performance-Aligned LLMs for Generating Fast Code","abstract":"Optimizing scientific software is a difficult task because codebases are often large and complex, and performance can depend upon several factors including the algorithm, its implementation, and hardware among others. Causes of poor performance can originate from disparate sources and be difficult to diagnose. Recent years have seen a multitude of work that use large language models (LLMs) to assist in software development tasks. However, these tools are trained to model the distribution of code as text, and are not specifically designed to understand performance aspects of code. In this work, we introduce a reinforcement learning based methodology to align the outputs of code LLMs with performance. This allows us to build upon the current code modeling capabilities of LLMs and extend them to generate better performing code. We demonstrate that our fine-tuned model improves the expected speedup of generated code over base models for a set of benchmark tasks from 0.9 to 1.6 for serial code and 1.9 to 4.5 for OpenMP code.","sentences":["Optimizing scientific software is a difficult task because codebases are often large and complex, and performance can depend upon several factors including the algorithm, its implementation, and hardware among others.","Causes of poor performance can originate from disparate sources and be difficult to diagnose.","Recent years have seen a multitude of work that use large language models (LLMs) to assist in software development tasks.","However, these tools are trained to model the distribution of code as text, and are not specifically designed to understand performance aspects of code.","In this work, we introduce a reinforcement learning based methodology to align the outputs of code LLMs with performance.","This allows us to build upon the current code modeling capabilities of LLMs and extend them to generate better performing code.","We demonstrate that our fine-tuned model improves the expected speedup of generated code over base models for a set of benchmark tasks from 0.9 to 1.6 for serial code and 1.9 to 4.5 for OpenMP code."],"url":"http://arxiv.org/abs/2404.18864v1","category":"cs.DC"}
{"created":"2024-04-29 16:51:25","title":"Constructive Recognition of Special Linear Groups","abstract":"We introduce a new constructive recognition algorithm for finite special linear groups in their natural representation. Given a group $G$ generated by a set of $d\\times d$ matrices over a finite field $\\mathbb{F}_q$, known to be isomorphic to the special linear group $\\mathrm{SL}(d,q)$, the algorithm computes a special generating set $S$ for $G$. These generators enable efficient computations with the input group, including solving the word problem. Implemented in the computer algebra system GAP, our algorithm outperforms existing state-of-the-art algorithms by a significant margin. A detailed complexity analysis of the algorithm will be presented in an upcoming publication.","sentences":["We introduce a new constructive recognition algorithm for finite special linear groups in their natural representation.","Given a group $G$ generated by a set of $d\\times d$ matrices over a finite field $\\mathbb{F}_q$, known to be isomorphic to the special linear group $\\mathrm{SL}(d,q)$, the algorithm computes a special generating set $S$ for $G$. These generators enable efficient computations with the input group, including solving the word problem.","Implemented in the computer algebra system GAP, our algorithm outperforms existing state-of-the-art algorithms by a significant margin.","A detailed complexity analysis of the algorithm will be presented in an upcoming publication."],"url":"http://arxiv.org/abs/2404.18860v1","category":"math.GR"}
{"created":"2024-04-29 16:50:15","title":"Leading terms of generalized Pl\u00fccker formulas","abstract":"Generalized Pl\\\"ucker numbers are defined to count certain types of tangent lines of generic degree $d$ complex projective hypersurfaces. They can be computed by identifying them as coefficients of GL(2)-equivariant cohomology classes of certain invariant subspaces, the so-called coincident root strata, of the vector space of homogeneous degree $d$ complex polynomials in two variables. In an earlier paper L\\'aszl\\'o M. Feh\\'er and the author gave a new, recursive method for calculating these classes. Using this method, we showed that -- similarly to the classical Pl\\\"ucker formulas counting the bitangents and flex lines of a degree $d$ plane curve -- generalized Pl\\\"ucker numbers are polynomials in the degree $d$.   In this paper, by further analyzing our recursive formula, we determine the leading terms of all the generalized Pl\\\"ucker formulas.","sentences":["Generalized Pl\\\"ucker numbers are defined to count certain types of tangent lines of generic degree $d$ complex projective hypersurfaces.","They can be computed by identifying them as coefficients of GL(2)-equivariant cohomology classes of certain invariant subspaces, the so-called coincident root strata, of the vector space of homogeneous degree $d$ complex polynomials in two variables.","In an earlier paper L\\'aszl\\'o M. Feh\\'er and the author gave a new, recursive method for calculating these classes.","Using this method, we showed that -- similarly to the classical Pl\\\"ucker formulas counting the bitangents and flex lines of a degree $d$ plane curve -- generalized Pl\\\"ucker numbers are polynomials in the degree $d$.   ","In this paper, by further analyzing our recursive formula, we determine the leading terms of all the generalized Pl\\\"ucker formulas."],"url":"http://arxiv.org/abs/2404.18859v1","category":"math.AG"}
{"created":"2024-04-29 16:48:41","title":"Generating self-similar membrane solutions","abstract":"Several ways to reduce to a first order ODE the non-linear PDE's governing the relativistic motion of an axially symmetric membrane in 4 space time dimensions, as well as examples for a previously found non-trivial transformation generating solutions, are given.","sentences":["Several ways to reduce to a first order ODE the non-linear PDE's governing the relativistic motion of an axially symmetric membrane in 4 space time dimensions, as well as examples for a previously found non-trivial transformation generating solutions, are given."],"url":"http://arxiv.org/abs/2404.18856v1","category":"math-ph"}
{"created":"2024-04-29 16:48:33","title":"Exceptional sets to Shallit's law of leap years in Pierce expansions","abstract":"In his 1994 work, Shallit introduced a rule for determining leap years that generalizes both the historically used Julian calendar and the contemporary Gregorian calendar. This rule depends on a so-called intercalation sequence. According to what we term Shallit's law of leap years, almost every point of the interval $[0,1]$ with respect to Lebesgue measure has the same limsup and liminf, respectively, of a quotient defined in terms of the number of leap years determined by the rule using the Pierce expansion digit sequence as an intercalation sequence. In this paper, we show that the set of exceptions to this law is dense and has full Hausdorff dimension in $[0,1]$, and that the exceptional set intersected with any non-empty open subset of $[0,1]$ has full Hausdorff dimension in $[0,1]$. As a more general result, we establish that for certain subsets of $[0,1]$ concerning the limiting behavior of Pierce expansion digits, intersecting with a non-empty open subset of $[0,1]$ preserves the Hausdorff dimension.","sentences":["In his 1994 work, Shallit introduced a rule for determining leap years that generalizes both the historically used Julian calendar and the contemporary Gregorian calendar.","This rule depends on a so-called intercalation sequence.","According to what we term Shallit's law of leap years, almost every point of the interval $[0,1]$ with respect to Lebesgue measure has the same limsup and liminf, respectively, of a quotient defined in terms of the number of leap years determined by the rule using the Pierce expansion digit sequence as an intercalation sequence.","In this paper, we show that the set of exceptions to this law is dense and has full Hausdorff dimension in $[0,1]$, and that the exceptional set intersected with any non-empty open subset of $[0,1]$ has full Hausdorff dimension in $[0,1]$. As a more general result, we establish that for certain subsets of $[0,1]$ concerning the limiting behavior of Pierce expansion digits, intersecting with a non-empty open subset of $[0,1]$ preserves the Hausdorff dimension."],"url":"http://arxiv.org/abs/2404.18855v1","category":"math.NT"}
{"created":"2024-04-29 16:47:55","title":"Switching Models of Oscillatory Networks Greatly Improve Inference of Dynamic Functional Connectivity","abstract":"Functional brain networks can change rapidly as a function of stimuli or cognitive shifts. Tracking dynamic functional connectivity is particularly challenging as it requires estimating the structure of the network at each moment as well as how it is shifting through time. In this paper, we describe a general modeling framework and a set of specific models that provides substantially increased statistical power for estimating rhythmic dynamic networks, based on the assumption that for a particular experiment or task, the network state at any moment is chosen from a discrete set of possible network modes. Each model is comprised of three components: (1) a set of latent switching states that represent transitions between the expression of each network mode; (2) a set of latent oscillators, each characterized by an estimated mean oscillation frequency and an instantaneous phase and amplitude at each time point; and (3) an observation model that relates the observed activity at each electrode to a linear combination of the latent oscillators. We develop an expectation-maximization procedure to estimate the network structure for each switching state and the probability of each state being expressed at each moment. We conduct a set of simulation studies to illustrate the application of these models and quantify their statistical power, even in the face of model misspecification.","sentences":["Functional brain networks can change rapidly as a function of stimuli or cognitive shifts.","Tracking dynamic functional connectivity is particularly challenging as it requires estimating the structure of the network at each moment as well as how it is shifting through time.","In this paper, we describe a general modeling framework and a set of specific models that provides substantially increased statistical power for estimating rhythmic dynamic networks, based on the assumption that for a particular experiment or task, the network state at any moment is chosen from a discrete set of possible network modes.","Each model is comprised of three components: (1) a set of latent switching states that represent transitions between the expression of each network mode; (2) a set of latent oscillators, each characterized by an estimated mean oscillation frequency and an instantaneous phase and amplitude at each time point; and (3) an observation model that relates the observed activity at each electrode to a linear combination of the latent oscillators.","We develop an expectation-maximization procedure to estimate the network structure for each switching state and the probability of each state being expressed at each moment.","We conduct a set of simulation studies to illustrate the application of these models and quantify their statistical power, even in the face of model misspecification."],"url":"http://arxiv.org/abs/2404.18854v1","category":"stat.ME"}
{"created":"2024-04-29 16:45:03","title":"VERT: Verified Equivalent Rust Transpilation with Few-Shot Learning","abstract":"Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default. Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust. Existing work falls into two categories: rule-based and large language model (LLM)-based. While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language. On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness. In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness. VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages. VERT first uses the Web Assembly compiler to obtain an oracle Rust program. In parallel, VERT uses an LLM to generate a readable candidate Rust program. This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds. We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks. Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone. In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers. Our results provide insights into the limitations of LLMs to write safe Rust.","sentences":["Rust is a programming language that combines memory safety and low-level control, providing C-like performance while guaranteeing the absence of undefined behaviors by default.","Rust's growing popularity has prompted research on safe and correct transpiling of existing code-bases to Rust.","Existing work falls into two categories: rule-based and large language model (LLM)-based.","While rule-based approaches can theoretically produce correct transpilations that maintain input-output equivalence to the original, they often yield unreadable Rust code that uses unsafe subsets of the Rust language.","On the other hand, while LLM-based approaches typically produce more readable, maintainable, and safe code, they do not provide any guarantees about correctness.","In this work, we present VERT, a tool that can produce readable Rust transpilations with formal guarantees of correctness.","VERT's only requirement is that there is Web Assembly compiler for the source language, which is true for most major languages.","VERT first uses the Web Assembly compiler to obtain an oracle Rust program.","In parallel, VERT uses an LLM to generate a readable candidate Rust program.","This candidate is verified against the oracle, and if verification fails, we regenerate a new candidate transpilation until verification succeeds.","We evaluate VERT by transpiling a suite of 1,394 programs taken from competitive programming style benchmarks.","Combining Anthropic's Claude-2 and VERT increases Rust transpilations passing property-based testing from 31% to 54% and bounded model-checking from 1% to 42% compared to using Claude alone.","In addition, we evaluate VERT's ability to generate non-trivial safe Rust on programs taken from real-world C projects that make significant use of pointers.","Our results provide insights into the limitations of LLMs to write safe Rust."],"url":"http://arxiv.org/abs/2404.18852v1","category":"cs.PL"}
{"created":"2024-04-29 16:43:45","title":"Sparse Sampling in Fractional Fourier Domain: Recovery Guarantees and Cram\u00e9r-Rao Bounds","abstract":"Sampling theory in fractional Fourier Transform (FrFT) domain has been studied extensively in the last decades. This interest stems from the ability of the FrFT to generalize the traditional Fourier Transform, broadening the traditional concept of bandwidth and accommodating a wider range of functions that may not be bandlimited in the Fourier sense. Beyond bandlimited functions, sampling and recovery of sparse signals has also been studied in the FrFT domain. Existing methods for sparse recovery typically operate in the transform domain, capitalizing on the spectral features of spikes in the FrFT domain. Our paper contributes two new theoretical advancements in this area. First, we introduce a novel time-domain sparse recovery method that avoids the typical bottlenecks of transform domain methods, such as spectral leakage. This method is backed by a sparse sampling theorem applicable to arbitrary FrFT-bandlimited kernels and is validated through a hardware experiment. Second, we present Cram\\'er-Rao Bounds for the sparse sampling problem, addressing a gap in existing literature.","sentences":["Sampling theory in fractional Fourier Transform (FrFT) domain has been studied extensively in the last decades.","This interest stems from the ability of the FrFT to generalize the traditional Fourier Transform, broadening the traditional concept of bandwidth and accommodating a wider range of functions that may not be bandlimited in the Fourier sense.","Beyond bandlimited functions, sampling and recovery of sparse signals has also been studied in the FrFT domain.","Existing methods for sparse recovery typically operate in the transform domain, capitalizing on the spectral features of spikes in the FrFT domain.","Our paper contributes two new theoretical advancements in this area.","First, we introduce a novel time-domain sparse recovery method that avoids the typical bottlenecks of transform domain methods, such as spectral leakage.","This method is backed by a sparse sampling theorem applicable to arbitrary FrFT-bandlimited kernels and is validated through a hardware experiment.","Second, we present Cram\\'er-Rao Bounds for the sparse sampling problem, addressing a gap in existing literature."],"url":"http://arxiv.org/abs/2404.18850v1","category":"cs.IT"}
{"created":"2024-04-29 16:42:26","title":"FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition","abstract":"Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning. Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training reliant on extensive data collection. Federated learning(FL), which only requires training on the clients and aggregates weights on the server without sharing data, has emerged as a solution. However, the substantial parameter size of PLMs places a significant burden on the computational resources of client devices, while also leading to costly communication expenses. Introducing Parameter-Efficient Fine-Tuning(PEFT) into FL can effectively address this problem. However, we observe that the non-IID data in federated learning leads to a gap in performance between the PEFT method and full parameter fine-tuning(FT). To overcome this, we propose FeDeRA, an improvement over the LoRA method in FL. FeDeRA uses the same adapter module as LoRA. However, the difference lies in FeDeRA's initialization of the adapter module by performing Singular Value Decomposition (SVD) on the pre-trained matrix and selecting its principal components. We conducted extensive experiments, using RoBERTa and DeBERTaV3, on three tasks and six datasets, comparing the methods including FT and the other three different PEFT methods. FeDeRA outperforms all other PEFT methods and is comparable to or even surpasses the performance of FT methods. We also deployed federated learning on Jetson AGX Orin and compared the time required by different methods to achieve the target accuracy on specific tasks. Compared to FT, FeDeRA reduces the training time by 95.9%, 97.9%, 96.9%, and 97.3%, 96.5%, and 96.5% respectively on three tasks using RoBERTa and DeBERTaV3. The overall experiments indicate that FeDeRA achieves good performance while also maintaining efficiency.","sentences":["Pre-trained Language Models (PLMs) have shown excellent performance on various downstream tasks after fine-tuning.","Nevertheless, the escalating concerns surrounding user privacy have posed significant challenges to centralized training reliant on extensive data collection.","Federated learning(FL), which only requires training on the clients and aggregates weights on the server without sharing data, has emerged as a solution.","However, the substantial parameter size of PLMs places a significant burden on the computational resources of client devices, while also leading to costly communication expenses.","Introducing Parameter-Efficient Fine-Tuning(PEFT) into FL can effectively address this problem.","However, we observe that the non-IID data in federated learning leads to a gap in performance between the PEFT method and full parameter fine-tuning(FT).","To overcome this, we propose FeDeRA, an improvement over the LoRA method in FL. FeDeRA uses the same adapter module as LoRA.","However, the difference lies in FeDeRA's initialization of the adapter module by performing Singular Value Decomposition (SVD) on the pre-trained matrix and selecting its principal components.","We conducted extensive experiments, using RoBERTa and DeBERTaV3, on three tasks and six datasets, comparing the methods including FT and the other three different PEFT methods.","FeDeRA outperforms all other PEFT methods and is comparable to or even surpasses the performance of FT methods.","We also deployed federated learning on Jetson AGX Orin and compared the time required by different methods to achieve the target accuracy on specific tasks.","Compared to FT, FeDeRA reduces the training time by 95.9%, 97.9%, 96.9%, and 97.3%, 96.5%, and 96.5% respectively on three tasks using RoBERTa and DeBERTaV3.","The overall experiments indicate that FeDeRA achieves good performance while also maintaining efficiency."],"url":"http://arxiv.org/abs/2404.18848v1","category":"cs.LG"}
{"created":"2024-04-29 16:35:42","title":"Phantom matter: a challenging solution to the cosmological tensions","abstract":"The idea of composite dark energy (DE) is quite natural since on general grounds we expect that the vacuum energy (associated to the cosmological term $\\Lambda$) may appear in combination with other effective forms of DE, collectively denoted $X$. This was indeed the old proposal from 2006, (cf. Ref.[41]) called the `$\\Lambda$XCDM model', which was primordially designed to explain the cosmic coincidence problem. We now find that it can also have far reaching consequences on the current cosmological tensions on $H_0$ and the growth of large scale structure (LSS). The $\\Lambda$XCDM may involve both a phantom-like component $X$ and a constant cosmological term $\\Lambda$ (positive or negative) or even a running one, $\\Lambda=\\Lambda(H)$. In the current work, we deal with a simplified version of the model and exploit the possibility that $X$ behaves as `phantom matter' (PM). The latter appears in stringy versions of the running vacuum model (RVM). Unlike phantom DE, it satisfies the strong energy condition like usual matter, hence bringing to bear positive pressure at the expense of negative energy. Bubbles of PM may appear in the manner of a transitory `phantom vacuum' tunneled into the late universe before it heads towards a new de Sitter era, thereby offering a crop field for the growing of structures earlier than expected. Using SNIa, cosmic chronometers, transversal BAO, LSS data and the full CMB likelihood from Planck 2018, we find that the tensions virtually disappear in this stringy RVM scenario characterized by axionic dark matter. The value of $H_0$ emerging from our analysis proves compatible with SH0ES to within less than $0.25\\sigma$ and the LSS growth tension is nonexistent. The statistical information criteria point to very strong evidence in favor of the PM solution.","sentences":["The idea of composite dark energy (DE) is quite natural since on general grounds we expect that the vacuum energy (associated to the cosmological term $\\Lambda$) may appear in combination with other effective forms of DE, collectively denoted $X$. This was indeed the old proposal from 2006, (cf.","Ref.[41]) called the `$\\Lambda$XCDM model', which was primordially designed to explain the cosmic coincidence problem.","We now find that it can also have far reaching consequences on the current cosmological tensions on $H_0$ and the growth of large scale structure (LSS).","The $\\Lambda$XCDM may involve both a phantom-like component $X$ and a constant cosmological term $\\Lambda$ (positive or negative) or even a running one, $\\Lambda=\\Lambda(H)$. In the current work, we deal with a simplified version of the model and exploit the possibility that $X$ behaves as `phantom matter' (PM).","The latter appears in stringy versions of the running vacuum model (RVM).","Unlike phantom DE, it satisfies the strong energy condition like usual matter, hence bringing to bear positive pressure at the expense of negative energy.","Bubbles of PM may appear in the manner of a transitory `phantom vacuum' tunneled into the late universe before it heads towards a new de Sitter era, thereby offering a crop field for the growing of structures earlier than expected.","Using SNIa, cosmic chronometers, transversal BAO, LSS data and the full CMB likelihood from Planck 2018, we find that the tensions virtually disappear in this stringy RVM scenario characterized by axionic dark matter.","The value of $H_0$ emerging from our analysis proves compatible with SH0ES to within less than $0.25\\sigma$ and the LSS growth tension is nonexistent.","The statistical information criteria point to very strong evidence in favor of the PM solution."],"url":"http://arxiv.org/abs/2404.18845v1","category":"astro-ph.CO"}
{"created":"2024-04-29 16:24:46","title":"Degeneration in discrimintal arrangements","abstract":"Discriminantal arrangements are hyperplane arrangements, which are generalized braid ones. They are constructed from given hyperplane arrangements, but their combinatorics are not invariant under combinatorial equivalence. However, it is known that the combinatorics of the discriminantal arrangement are constant on a Zariski open set of the space of hyperplane arrangements. In the present paper, we introduce non-very generic varieties in the space of hyperplane arrangements to classify discriminantal arrangements and show that the Zariski open set is the complement of non-very generic varieties. We study their basic properties and construction and provide examples, including infinite families of non-very generic varieties. In particular, the construction we call degeneration is a powerful tool for constructing non-very generic varieties. As an application, we provide lists of non-very generic varieties for spaces of small line arrangements.","sentences":["Discriminantal arrangements are hyperplane arrangements, which are generalized braid ones.","They are constructed from given hyperplane arrangements, but their combinatorics are not invariant under combinatorial equivalence.","However, it is known that the combinatorics of the discriminantal arrangement are constant on a Zariski open set of the space of hyperplane arrangements.","In the present paper, we introduce non-very generic varieties in the space of hyperplane arrangements to classify discriminantal arrangements and show that the Zariski open set is the complement of non-very generic varieties.","We study their basic properties and construction and provide examples, including infinite families of non-very generic varieties.","In particular, the construction we call degeneration is a powerful tool for constructing non-very generic varieties.","As an application, we provide lists of non-very generic varieties for spaces of small line arrangements."],"url":"http://arxiv.org/abs/2404.18835v1","category":"math.CO"}
{"created":"2024-04-29 16:21:59","title":"Interpolating between Optimal Transport and KL regularized Optimal Transport using R\u00e9nyi Divergences","abstract":"Regularized optimal transport (OT) has received much attention in recent years starting from Cuturi's paper with Kullback-Leibler (KL) divergence regularized OT. In this paper, we propose to regularize the OT problem using the family of $\\alpha$-R\\'enyi divergences for $\\alpha \\in (0, 1)$. R\\'enyi divergences are neither $f$-divergences nor Bregman distances, but they recover the KL divergence in the limit $\\alpha \\nearrow 1$. The advantage of introducing the additional parameter $\\alpha$ is that for $\\alpha \\searrow 0$ we obtain convergence to the unregularized OT problem. For the KL regularized OT problem, this was achieved by letting the regularization parameter tend to zero, which causes numerical instabilities. We present two different ways to obtain premetrics on probability measures, namely by R\\'enyi divergence constraints and penalization. The latter premetric interpolates between the unregularized and KL regularized OT problem with weak convergence of the minimizer, generalizing the interpolating property of KL regularized OT. We use a nested mirror descent algorithm for solving the primal formulation. Both on real and synthetic data sets R\\'enyi regularized OT plans outperform their KL and Tsallis counterparts in terms of being closer to the unregularized transport plans and recovering the ground truth in inference tasks better.","sentences":["Regularized optimal transport (OT) has received much attention in recent years starting from Cuturi's paper with Kullback-Leibler (KL) divergence regularized OT.","In this paper, we propose to regularize the OT problem using the family of $\\alpha$-R\\'enyi divergences for $\\alpha \\in (0, 1)$. R\\'enyi divergences are neither $f$-divergences nor Bregman distances, but they recover the KL divergence in the limit $\\alpha \\nearrow 1$.","The advantage of introducing the additional parameter $\\alpha$ is that for $\\alpha \\searrow 0$ we obtain convergence to the unregularized OT problem.","For the KL regularized OT problem, this was achieved by letting the regularization parameter tend to zero, which causes numerical instabilities.","We present two different ways to obtain premetrics on probability measures, namely by R\\'enyi divergence constraints and penalization.","The latter premetric interpolates between the unregularized and KL regularized OT problem with weak convergence of the minimizer, generalizing the interpolating property of KL regularized OT.","We use a nested mirror descent algorithm for solving the primal formulation.","Both on real and synthetic data sets R\\'enyi regularized OT plans outperform their KL and Tsallis counterparts in terms of being closer to the unregularized transport plans and recovering the ground truth in inference tasks better."],"url":"http://arxiv.org/abs/2404.18834v1","category":"math.OC"}
{"created":"2024-04-29 16:16:42","title":"ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization","abstract":"Understanding the severity of conditions shown in images in medical diagnosis is crucial, serving as a key guide for clinical assessment, treatment, as well as evaluating longitudinal progression. This paper proposes Con- PrO: a novel representation learning method for severity assessment in medical images using Contrastive learningintegrated Preference Optimization. Different from conventional contrastive learning methods that maximize the distance between classes, ConPrO injects into the latent vector the distance preference knowledge between various severity classes and the normal class. We systematically examine the key components of our framework to illuminate how contrastive prediction tasks acquire valuable representations. We show that our representation learning framework offers valuable severity ordering in the feature space while outperforming previous state-of-the-art methods on classification tasks. We achieve a 6% and 20% relative improvement compared to a supervised and a self-supervised baseline, respectively. In addition, we derived discussions on severity indicators and related applications of preference comparison in the medical domain.","sentences":["Understanding the severity of conditions shown in images in medical diagnosis is crucial, serving as a key guide for clinical assessment, treatment, as well as evaluating longitudinal progression.","This paper proposes Con- PrO: a novel representation learning method for severity assessment in medical images using Contrastive learningintegrated Preference Optimization.","Different from conventional contrastive learning methods that maximize the distance between classes, ConPrO injects into the latent vector the distance preference knowledge between various severity classes and the normal class.","We systematically examine the key components of our framework to illuminate how contrastive prediction tasks acquire valuable representations.","We show that our representation learning framework offers valuable severity ordering in the feature space while outperforming previous state-of-the-art methods on classification tasks.","We achieve a 6% and 20% relative improvement compared to a supervised and a self-supervised baseline, respectively.","In addition, we derived discussions on severity indicators and related applications of preference comparison in the medical domain."],"url":"http://arxiv.org/abs/2404.18831v1","category":"cs.CV"}
{"created":"2024-04-29 16:16:33","title":"Solution of a Problem in Monoidal Categorification by Additive Categorification","abstract":"In 2021, Kashiwara-Kim-Oh-Park constructed cluster algebra structures on the Grothendieck rings of certain monoidal subcategories of the category of finite-dimensional representations of a quantum loop algebra, generalizing Hernandez-Leclerc's pioneering work from 2010. They stated the problem of finding explicit quivers for the seeds they used. We provide a solution by using Palu's generalized mutation rule applied to the cluster categories associated with certain algebras of global dimension at most 2, for example tensor products of path algebras of representation-finite quivers. Thus, our method is based on (and contributes to) the bridge, provided by cluster combinatorics, between the representation theory of quantum groups and that of quivers with relations.","sentences":["In 2021, Kashiwara-Kim-Oh-Park constructed cluster algebra structures on the Grothendieck rings of certain monoidal subcategories of the category of finite-dimensional representations of a quantum loop algebra, generalizing Hernandez-Leclerc's pioneering work from 2010.","They stated the problem of finding explicit quivers for the seeds they used.","We provide a solution by using Palu's generalized mutation rule applied to the cluster categories associated with certain algebras of global dimension at most 2, for example tensor products of path algebras of representation-finite quivers.","Thus, our method is based on (and contributes to) the bridge, provided by cluster combinatorics, between the representation theory of quantum groups and that of quivers with relations."],"url":"http://arxiv.org/abs/2404.18830v1","category":"math.RT"}
{"created":"2024-04-29 16:15:32","title":"Disentangling the development of collective flow in high energy proton proton collisions with a multiphase transport model","abstract":"In this work, we investigate the collective flow development in high energy proton proton (pp) collisions with a multiphase transport model (AMPT) based on PYTHIA8 initial conditions with a sub-nucleon structure. It is found that the PYTHIA8 based AMPT model can reasonably describe both the charged hadron productions and elliptic flow experimental data measured in pp collisions at $\\sqrt{s}=13$ TeV. By turning on the parton and hadron rescatterings in AMPT separately, we find that the observed collective flow in pp collisions is largely developed during the parton evolutions, while no significant flow effect can be generated with the pure hadronic rescatterings. It is also shown that the parton escape mechanism is important for describing both the magnitude of the two-particle cumulant and the sign of the four-particle cumulants. We emphasize that the strong mass ordering of the elliptic flow results from the coalescence process in the transport model can thus be regarded as unique evidence related to the creation of deconfined parton matter in high energy pp collisions.","sentences":["In this work, we investigate the collective flow development in high energy proton proton (pp) collisions with a multiphase transport model (AMPT) based on PYTHIA8 initial conditions with a sub-nucleon structure.","It is found that the PYTHIA8 based AMPT model can reasonably describe both the charged hadron productions and elliptic flow experimental data measured in pp collisions at $\\sqrt{s}=13$ TeV. By turning on the parton and hadron rescatterings in AMPT separately, we find that the observed collective flow in pp collisions is largely developed during the parton evolutions, while no significant flow effect can be generated with the pure hadronic rescatterings.","It is also shown that the parton escape mechanism is important for describing both the magnitude of the two-particle cumulant and the sign of the four-particle cumulants.","We emphasize that the strong mass ordering of the elliptic flow results from the coalescence process in the transport model can thus be regarded as unique evidence related to the creation of deconfined parton matter in high energy pp collisions."],"url":"http://arxiv.org/abs/2404.18829v1","category":"nucl-th"}
{"created":"2024-04-29 16:07:36","title":"Harmonic Machine Learning Models are Robust","abstract":"We introduce Harmonic Robustness, a powerful and intuitive method to test the robustness of any machine-learning model either during training or in black-box real-time inference monitoring without ground-truth labels. It is based on functional deviation from the harmonic mean value property, indicating instability and lack of explainability. We show implementation examples in low-dimensional trees and feedforward NNs, where the method reliably identifies overfitting, as well as in more complex high-dimensional models such as ResNet-50 and Vision Transformer where it efficiently measures adversarial vulnerability across image classes.","sentences":["We introduce Harmonic Robustness, a powerful and intuitive method to test the robustness of any machine-learning model either during training or in black-box real-time inference monitoring without ground-truth labels.","It is based on functional deviation from the harmonic mean value property, indicating instability and lack of explainability.","We show implementation examples in low-dimensional trees and feedforward NNs, where the method reliably identifies overfitting, as well as in more complex high-dimensional models such as ResNet-50 and Vision Transformer where it efficiently measures adversarial vulnerability across image classes."],"url":"http://arxiv.org/abs/2404.18825v1","category":"cs.LG"}
{"created":"2024-04-29 16:05:36","title":"Benchmarking Benchmark Leakage in Large Language Models","abstract":"Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs). This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development. To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages. By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons. These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations. Notably, we propose the \"Benchmark Transparency Card\" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs. we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research.","sentences":["Amid the expanding use of pre-training data, the phenomenon of benchmark dataset leakage has become increasingly prominent, exacerbated by opaque training processes and the often undisclosed inclusion of supervised data in contemporary Large Language Models (LLMs).","This issue skews benchmark effectiveness and fosters potentially unfair comparisons, impeding the field's healthy development.","To address this, we introduce a detection pipeline utilizing Perplexity and N-gram accuracy, two simple and scalable metrics that gauge a model's prediction precision on benchmark, to identify potential data leakages.","By analyzing 31 LLMs under the context of mathematical reasoning, we reveal substantial instances of training even test set misuse, resulting in potentially unfair comparisons.","These findings prompt us to offer several recommendations regarding model documentation, benchmark setup, and future evaluations.","Notably, we propose the \"Benchmark Transparency Card\" to encourage clear documentation of benchmark utilization, promoting transparency and healthy developments of LLMs.","we have made our leaderboard, pipeline implementation, and model predictions publicly available, fostering future research."],"url":"http://arxiv.org/abs/2404.18824v1","category":"cs.CL"}
{"created":"2024-04-29 16:04:25","title":"A Multi-Period Black-Litterman Model","abstract":"The Black-Litterman model is a framework for incorporating forward-looking expert views in a portfolio optimization problem. Existing work focuses almost exclusively on single-period problems and assumes that the horizon of expert forecasts matches that of the investor. We consider a multi-period generalization where the horizon of expert views may differ from that of a dynamically-trading investor. By exploiting an underlying graphical structure relating the asset prices and views, we derive the conditional distribution of asset returns when the price process is geometric Brownian motion. We also show that it can be written in terms of a multi-dimensional Brownian bridge. The new price process is an affine factor model with the conditional log-price process playing the role of a vector of factors. We derive an explicit expression for the optimal dynamic investment policy and analyze the hedging demand associated with the new covariate. More generally, the paper shows that Bayesian graphical models are a natural framework for incorporating complex information structures in the Black-Litterman model.","sentences":["The Black-Litterman model is a framework for incorporating forward-looking expert views in a portfolio optimization problem.","Existing work focuses almost exclusively on single-period problems and assumes that the horizon of expert forecasts matches that of the investor.","We consider a multi-period generalization where the horizon of expert views may differ from that of a dynamically-trading investor.","By exploiting an underlying graphical structure relating the asset prices and views, we derive the conditional distribution of asset returns when the price process is geometric Brownian motion.","We also show that it can be written in terms of a multi-dimensional Brownian bridge.","The new price process is an affine factor model with the conditional log-price process playing the role of a vector of factors.","We derive an explicit expression for the optimal dynamic investment policy and analyze the hedging demand associated with the new covariate.","More generally, the paper shows that Bayesian graphical models are a natural framework for incorporating complex information structures in the Black-Litterman model."],"url":"http://arxiv.org/abs/2404.18822v1","category":"q-fin.PM"}
{"created":"2024-04-29 16:03:21","title":"Control Policy Correction Framework for Reinforcement Learning-based Energy Arbitrage Strategies","abstract":"A continuous rise in the penetration of renewable energy sources, along with the use of the single imbalance pricing, provides a new opportunity for balance responsible parties to reduce their cost through energy arbitrage in the imbalance settlement mechanism. Model-free reinforcement learning (RL) methods are an appropriate choice for solving the energy arbitrage problem due to their outstanding performance in solving complex stochastic sequential problems. However, RL is rarely deployed in real-world applications since its learned policy does not necessarily guarantee safety during the execution phase. In this paper, we propose a new RL-based control framework for batteries to obtain a safe energy arbitrage strategy in the imbalance settlement mechanism. In our proposed control framework, the agent initially aims to optimize the arbitrage revenue. Subsequently, in the post-processing step, we correct (constrain) the learned policy following a knowledge distillation process based on properties that follow human intuition. Our post-processing step is a generic method and is not restricted to the energy arbitrage domain. We use the Belgian imbalance price of 2023 to evaluate the performance of our proposed framework. Furthermore, we deploy our proposed control framework on a real battery to show its capability in the real world.","sentences":["A continuous rise in the penetration of renewable energy sources, along with the use of the single imbalance pricing, provides a new opportunity for balance responsible parties to reduce their cost through energy arbitrage in the imbalance settlement mechanism.","Model-free reinforcement learning (RL) methods are an appropriate choice for solving the energy arbitrage problem due to their outstanding performance in solving complex stochastic sequential problems.","However, RL is rarely deployed in real-world applications since its learned policy does not necessarily guarantee safety during the execution phase.","In this paper, we propose a new RL-based control framework for batteries to obtain a safe energy arbitrage strategy in the imbalance settlement mechanism.","In our proposed control framework, the agent initially aims to optimize the arbitrage revenue.","Subsequently, in the post-processing step, we correct (constrain) the learned policy following a knowledge distillation process based on properties that follow human intuition.","Our post-processing step is a generic method and is not restricted to the energy arbitrage domain.","We use the Belgian imbalance price of 2023 to evaluate the performance of our proposed framework.","Furthermore, we deploy our proposed control framework on a real battery to show its capability in the real world."],"url":"http://arxiv.org/abs/2404.18821v2","category":"eess.SY"}
{"created":"2024-04-29 16:02:38","title":"Towards Extreme Image Compression with Latent Feature Guidance and Diffusion Prior","abstract":"Compressing images at extremely low bitrates (below 0.1 bits per pixel (bpp)) is a significant challenge due to substantial information loss. Existing extreme image compression methods generally suffer from heavy compression artifacts or low-fidelity reconstructions. To address this problem, we propose a novel extreme image compression framework that combines compressive VAEs and pre-trained text-to-image diffusion models in an end-to-end manner. Specifically, we introduce a latent feature-guided compression module based on compressive VAEs. This module compresses images and initially decodes the compressed information into content variables. To enhance the alignment between content variables and the diffusion space, we introduce external guidance to modulate intermediate feature maps. Subsequently, we develop a conditional diffusion decoding module that leverages pre-trained diffusion models to further decode these content variables. To preserve the generative capability of pre-trained diffusion models, we keep their parameters fixed and use a control module to inject content information. We also design a space alignment loss to provide sufficient constraints for the latent feature-guided compression module. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in terms of both visual performance and image fidelity at extremely low bitrates.","sentences":["Compressing images at extremely low bitrates (below 0.1 bits per pixel (bpp)) is a significant challenge due to substantial information loss.","Existing extreme image compression methods generally suffer from heavy compression artifacts or low-fidelity reconstructions.","To address this problem, we propose a novel extreme image compression framework that combines compressive VAEs and pre-trained text-to-image diffusion models in an end-to-end manner.","Specifically, we introduce a latent feature-guided compression module based on compressive VAEs.","This module compresses images and initially decodes the compressed information into content variables.","To enhance the alignment between content variables and the diffusion space, we introduce external guidance to modulate intermediate feature maps.","Subsequently, we develop a conditional diffusion decoding module that leverages pre-trained diffusion models to further decode these content variables.","To preserve the generative capability of pre-trained diffusion models, we keep their parameters fixed and use a control module to inject content information.","We also design a space alignment loss to provide sufficient constraints for the latent feature-guided compression module.","Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in terms of both visual performance and image fidelity at extremely low bitrates."],"url":"http://arxiv.org/abs/2404.18820v1","category":"eess.IV"}
{"created":"2024-04-29 15:56:04","title":"Calculating the Capacity Region of a Quantum Switch","abstract":"Quantum repeaters are necessary to fully realize the capabilities of the emerging quantum internet, especially applications involving distributing entanglement across long distances. A more general notion of this can be called a quantum switch, which connects to many users and can act as a repeater to create end-to-end entanglement between different subsets of these users. Here we present a method of calculating the capacity region of both discrete- and continuous-variable quantum switches that in general support mixed-partite entanglement generation. The method uses tools from convex analysis to generate the boundaries of the capacity region. We show example calculations with illustrative topologies and perform simulations to support the analytical results.","sentences":["Quantum repeaters are necessary to fully realize the capabilities of the emerging quantum internet, especially applications involving distributing entanglement across long distances.","A more general notion of this can be called a quantum switch, which connects to many users and can act as a repeater to create end-to-end entanglement between different subsets of these users.","Here we present a method of calculating the capacity region of both discrete- and continuous-variable quantum switches that in general support mixed-partite entanglement generation.","The method uses tools from convex analysis to generate the boundaries of the capacity region.","We show example calculations with illustrative topologies and perform simulations to support the analytical results."],"url":"http://arxiv.org/abs/2404.18818v1","category":"quant-ph"}
{"created":"2024-04-29 15:52:45","title":"AppPoet: Large Language Model based Android malware detection via multi-view prompt engineering","abstract":"Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions. However, numerous feature engineering based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection. Besides, the majority of existing feature engineering based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports. Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection. Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views. Subsequently, it steers the LLM to produce function descriptions and behavioral summaries for views via our meticulously devised multi-view prompt engineering technique to realize the deep mining of view semantics. Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the heuristic diagnostic reports. Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline method Drebin and its variant. Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports.","sentences":["Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions.","However, numerous feature engineering based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection.","Besides, the majority of existing feature engineering based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports.","Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection.","Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views.","Subsequently, it steers the LLM to produce function descriptions and behavioral summaries for views via our meticulously devised multi-view prompt engineering technique to realize the deep mining of view semantics.","Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the heuristic diagnostic reports.","Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline method Drebin and its variant.","Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports."],"url":"http://arxiv.org/abs/2404.18816v1","category":"cs.CR"}
{"created":"2024-04-29 15:49:37","title":"Safe Reach Set Computation via Neural Barrier Certificates","abstract":"We present a novel technique for online safety verification of autonomous systems, which performs reachability analysis efficiently for both bounded and unbounded horizons by employing neural barrier certificates. Our approach uses barrier certificates given by parameterized neural networks that depend on a given initial set, unsafe sets, and time horizon. Such networks are trained efficiently offline using system simulations sampled from regions of the state space. We then employ a meta-neural network to generalize the barrier certificates to state space regions that are outside the training set. These certificates are generated and validated online as sound over-approximations of the reachable states, thus either ensuring system safety or activating appropriate alternative actions in unsafe scenarios. We demonstrate our technique on case studies from linear models to nonlinear control-dependent models for online autonomous driving scenarios.","sentences":["We present a novel technique for online safety verification of autonomous systems, which performs reachability analysis efficiently for both bounded and unbounded horizons by employing neural barrier certificates.","Our approach uses barrier certificates given by parameterized neural networks that depend on a given initial set, unsafe sets, and time horizon.","Such networks are trained efficiently offline using system simulations sampled from regions of the state space.","We then employ a meta-neural network to generalize the barrier certificates to state space regions that are outside the training set.","These certificates are generated and validated online as sound over-approximations of the reachable states, thus either ensuring system safety or activating appropriate alternative actions in unsafe scenarios.","We demonstrate our technique on case studies from linear models to nonlinear control-dependent models for online autonomous driving scenarios."],"url":"http://arxiv.org/abs/2404.18813v1","category":"eess.SY"}
{"created":"2024-04-29 15:48:49","title":"Weak equivalence principle and nonrelativistic limit of general dispersion relations","abstract":"We study the weak equivalence principle in the context of modified dispersion relations, a prevalent approach to quantum gravity phenomenology. We find that generic modified dispersion relations violate the weak equivalence principle. The acceleration in general depends on the mass of the test body, unless the Hamiltonian is either two-homogeneous in the test particles' 4-momenta or the corresponding Lagrangian differs from the homogeneous case by a total derivative only. The key ingredient of this calculation is a $3+1$ decomposition of the parametrization invariant relativistic test particle action derived from the dispersion relation. Additionally, we apply a perturbative expansion in the test particle's spatial velocity and the inverse speed of light. To quantify our result, we provide a general formula for the E\\\"otv\\'os factor of modified dispersion relations. As a specific example, we study the $\\kappa$-Poincar\\'e dispersion relation. Comparing the ensuing non-vanishing E\\\"otv\\'os factor to recent data from the MICROSCOPE experiment, we obtain a bound of $\\kappa^{-1}\\geq10^{15}{\\rm GeV}/c^2.$.","sentences":["We study the weak equivalence principle in the context of modified dispersion relations, a prevalent approach to quantum gravity phenomenology.","We find that generic modified dispersion relations violate the weak equivalence principle.","The acceleration in general depends on the mass of the test body, unless the Hamiltonian is either two-homogeneous in the test particles' 4-momenta or the corresponding Lagrangian differs from the homogeneous case by a total derivative only.","The key ingredient of this calculation is a $3+1$ decomposition of the parametrization invariant relativistic test particle action derived from the dispersion relation.","Additionally, we apply a perturbative expansion in the test particle's spatial velocity and the inverse speed of light.","To quantify our result, we provide a general formula for the E\\\"otv\\'os factor of modified dispersion relations.","As a specific example, we study the $\\kappa$-Poincar\\'e dispersion relation.","Comparing the ensuing non-vanishing E\\\"otv\\'os factor to recent data from the MICROSCOPE experiment, we obtain a bound of $\\kappa^{-1}\\geq10^{15}{\\rm GeV}/c^2.$."],"url":"http://arxiv.org/abs/2404.18811v1","category":"gr-qc"}
{"created":"2024-04-29 15:43:27","title":"Bivariate Generating Functions Enumerating Non-Bonding Dominoes on Rectangular Boards","abstract":"The manuscript studies configurations of non-overlapping non-bonding dominoes on finite rectangular boards of unit squares characterized by row and column number. The non-bonding dominoes are defined here by the requirement that any domino on the board shares at most one point (one of its four corner points) with any other domino, but no edge. With the Transfer Matrix Method, rational generating functions are derived that solve the enumeration problem entirely, here evaluated for boards with up to six rows or columns.","sentences":["The manuscript studies configurations of non-overlapping non-bonding dominoes on finite rectangular boards of unit squares characterized by row and column number.","The non-bonding dominoes are defined here by the requirement that any domino on the board shares at most one point (one of its four corner points) with any other domino, but no edge.","With the Transfer Matrix Method, rational generating functions are derived that solve the enumeration problem entirely, here evaluated for boards with up to six rows or columns."],"url":"http://arxiv.org/abs/2404.18806v1","category":"math.CO"}
{"created":"2024-04-29 15:43:26","title":"NEOMOD 3: The Debiased Size Distribution of Near Earth Objects","abstract":"Our previous model (NEOMOD2) for the orbital and absolute magnitude distribution of Near Earth Objects (NEOs) was calibrated on the Catalina Sky Survey observations between 2013 and 2022. Here we extend NEOMOD2 to include visible albedo information from the Wide-Field Infrared Survey Explorer. The debiased albedo distribution of NEOs can be approximated by the sum of two Rayleigh distributions with the scale parameters p_V,dark=0.03 and p_V,bright=0.17. We find evidence for smaller NEOs having (on average) higher albedos than larger NEOs; this is likely a consequence of the size-dependent sampling of different main belt sources. These inferences and the absolute magnitude distribution from NEOMOD2 are used to construct the debiased size distribution of NEOs. We estimate 830+/-60 NEOs with diameters D>1 km and 20,000+/-2,000 NEOs with D>140 m. The new model, NEOMOD3, is available via the NEOMOD Simulator -- an easy-to-operate code that can be used to generate user-defined samples (orbits, sizes and albedos) from the model.","sentences":["Our previous model (NEOMOD2) for the orbital and absolute magnitude distribution of Near Earth Objects (NEOs) was calibrated on the Catalina Sky Survey observations between 2013 and 2022.","Here we extend NEOMOD2 to include visible albedo information from the Wide-Field Infrared Survey Explorer.","The debiased albedo distribution of NEOs can be approximated by the sum of two Rayleigh distributions with the scale parameters p_V,dark=0.03 and p_V,bright=0.17.","We find evidence for smaller NEOs having (on average) higher albedos than larger NEOs; this is likely a consequence of the size-dependent sampling of different main belt sources.","These inferences and the absolute magnitude distribution from NEOMOD2 are used to construct the debiased size distribution of NEOs.","We estimate 830+/-60 NEOs with diameters D>1 km and 20,000+/-2,000 NEOs with D>140 m. The new model, NEOMOD3, is available via the NEOMOD Simulator -- an easy-to-operate code that can be used to generate user-defined samples (orbits, sizes and albedos) from the model."],"url":"http://arxiv.org/abs/2404.18805v1","category":"astro-ph.EP"}
{"created":"2024-04-29 15:42:54","title":"Thermodynamics of the quantum Schwarzschild black hole","abstract":"We discuss some thermodynamic properties as well as the stability of a quantum Schwarzschild black hole, comparing the results with those obtained within a bumblebee gravity model. In particular, the Hawking temperature, $T_H$, the entropy, $S$, the heat capacity, $C$, and the Gibbs free energy, $G$, are computed for both cases. In addition to that, we compute the Brown-York quasilocal energy and compare the solution with the Schwarzschild case. We find that in both cases (quantum Schwarzschild and bumblebee gravity model) the temperature, the entropy, and the heat capacity show the same functional form, under the replacement $\\lambda^2 \\rightarrow \\ell$ and vice versa. Specifically, the temperature is found to be lower compared to the classical (Schwarzschild) solution, whereas the entropy is computed to be larger. Moreover, the heat capacity becomes more negative. Notably, a distinct contrast emerges in obtaining the Gibbs free energy between these two cases, and this distinction appears to stem from the ADM mass.","sentences":["We discuss some thermodynamic properties as well as the stability of a quantum Schwarzschild black hole, comparing the results with those obtained within a bumblebee gravity model.","In particular, the Hawking temperature, $T_H$, the entropy, $S$, the heat capacity, $C$, and the Gibbs free energy, $G$, are computed for both cases.","In addition to that, we compute the Brown-York quasilocal energy and compare the solution with the Schwarzschild case.","We find that in both cases (quantum Schwarzschild and bumblebee gravity model) the temperature, the entropy, and the heat capacity show the same functional form, under the replacement $\\lambda^2 \\rightarrow \\ell$ and vice versa.","Specifically, the temperature is found to be lower compared to the classical (Schwarzschild) solution, whereas the entropy is computed to be larger.","Moreover, the heat capacity becomes more negative.","Notably, a distinct contrast emerges in obtaining the Gibbs free energy between these two cases, and this distinction appears to stem from the ADM mass."],"url":"http://arxiv.org/abs/2404.18804v1","category":"gr-qc"}
{"created":"2024-04-29 15:42:36","title":"Convergence of dynamical stationary fluctuations","abstract":"We present a general black box theorem that ensures convergence of a sequence of stationary Markov processes, provided a few assumptions are satisfied. This theorem relies on a control of the resolvents of the sequence of Markov processes, and on a suitable characterization of the resolvents of the limit. One major advantage of this approach is that it circumvents the use of the Boltzmann-Gibbs principle: in particular, we deduce in a rather simple way that the stationary fluctuations of the one-dimensional zero-range process converge to the stochastic heat equation. It also allows to establish results that were probably out of reach of existing methods: using the black box result, we are able to prove that the stationary fluctuations of a discrete model of ordered interfaces, that was considered previously in the statistical physics literature, converge to a system of reflected stochastic PDEs.","sentences":["We present a general black box theorem that ensures convergence of a sequence of stationary Markov processes, provided a few assumptions are satisfied.","This theorem relies on a control of the resolvents of the sequence of Markov processes, and on a suitable characterization of the resolvents of the limit.","One major advantage of this approach is that it circumvents the use of the Boltzmann-Gibbs principle: in particular, we deduce in a rather simple way that the stationary fluctuations of the one-dimensional zero-range process converge to the stochastic heat equation.","It also allows to establish results that were probably out of reach of existing methods: using the black box result, we are able to prove that the stationary fluctuations of a discrete model of ordered interfaces, that was considered previously in the statistical physics literature, converge to a system of reflected stochastic PDEs."],"url":"http://arxiv.org/abs/2404.18803v1","category":"math.PR"}
{"created":"2024-04-29 15:38:14","title":"Extending h adaptivity with refinement patterns","abstract":"This contribution introduces the idea of refinement patterns for the generation of optimal meshes in the context of the Finite Element Method. The main idea is to generate a library of possible patterns on which elements can be refined and use this library to inform an h adaptive code on how to handle complex refinements in regions of interest. There are no restrictions on the type of elements that can be refined, and the patterns can be generated for any element type. The main advantage of this approach is that it allows for the generation of optimal meshes in a systematic way where, even if a certain pattern is not available, it can easily be included through a simple text file with nodes and sub-elements. The contribution presents a detailed methodology for incorporating refinement patterns into h adaptive Finite Element Method codes and demonstrates the effectiveness of the approach through mesh refinement of problems with complex geometries.","sentences":["This contribution introduces the idea of refinement patterns for the generation of optimal meshes in the context of the Finite Element Method.","The main idea is to generate a library of possible patterns on which elements can be refined and use this library to inform an h adaptive code on how to handle complex refinements in regions of interest.","There are no restrictions on the type of elements that can be refined, and the patterns can be generated for any element type.","The main advantage of this approach is that it allows for the generation of optimal meshes in a systematic way where, even if a certain pattern is not available, it can easily be included through a simple text file with nodes and sub-elements.","The contribution presents a detailed methodology for incorporating refinement patterns into h adaptive Finite Element Method codes and demonstrates the effectiveness of the approach through mesh refinement of problems with complex geometries."],"url":"http://arxiv.org/abs/2404.18800v1","category":"math.NA"}
{"created":"2024-04-29 15:33:23","title":"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models","abstract":"As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality. Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge. To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs. Evaluations most commonly use a single large model like GPT4. While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary. We propose instead to evaluate models using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.","sentences":["As Large Language Models (LLMs) have become more advanced, they have outpaced our abilities to accurately evaluate their quality.","Not only is finding data to adequately probe particular model properties difficult, but evaluating the correctness of a model's freeform generation alone is a challenge.","To address this, many evaluations now rely on using LLMs themselves as judges to score the quality of outputs from other LLMs.","Evaluations most commonly use a single large model like GPT4.","While this method has grown in popularity, it is costly, has been shown to introduce intramodel bias, and in this work, we find that very large models are often unnecessary.","We propose instead to evaluate models using a Panel of LLm evaluators (PoLL).","Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive."],"url":"http://arxiv.org/abs/2404.18796v1","category":"cs.CL"}
{"created":"2024-04-29 15:27:09","title":"A real-time digital twin of azimuthal thermoacoustic instabilities","abstract":"When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.","sentences":["When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines.","We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor.","The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations.","First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic.","Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously.","This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem.","Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations.","Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios.","We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables.","The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models.","This work opens new opportunities for real-time digital twinning of multi-physics problems."],"url":"http://arxiv.org/abs/2404.18793v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 15:23:26","title":"Certification of Speaker Recognition Models to Additive Perturbations","abstract":"Speaker recognition technology is applied in various tasks ranging from personal virtual assistants to secure access systems. However, the robustness of these systems against adversarial attacks, particularly to additive perturbations, remains a significant challenge. In this paper, we pioneer applying robustness certification techniques to speaker recognition, originally developed for the image domain. In our work, we cover this gap by transferring and improving randomized smoothing certification techniques against norm-bounded additive perturbations for classification and few-shot learning tasks to speaker recognition. We demonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for several models. We expect this work to improve voice-biometry robustness, establish a new certification benchmark, and accelerate research of certification methods in the audio domain.","sentences":["Speaker recognition technology is applied in various tasks ranging from personal virtual assistants to secure access systems.","However, the robustness of these systems against adversarial attacks, particularly to additive perturbations, remains a significant challenge.","In this paper, we pioneer applying robustness certification techniques to speaker recognition, originally developed for the image domain.","In our work, we cover this gap by transferring and improving randomized smoothing certification techniques against norm-bounded additive perturbations for classification and few-shot learning tasks to speaker recognition.","We demonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for several models.","We expect this work to improve voice-biometry robustness, establish a new certification benchmark, and accelerate research of certification methods in the audio domain."],"url":"http://arxiv.org/abs/2404.18791v1","category":"cs.SD"}
{"created":"2024-04-29 15:19:28","title":"Randomization-based confidence intervals for the local average treatment effect","abstract":"We consider the problem of generating confidence intervals in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure.","sentences":["We consider the problem of generating confidence intervals in randomized experiments with noncompliance.","We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties.","Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous.","We provide a uniform analysis of this procedure."],"url":"http://arxiv.org/abs/2404.18786v1","category":"math.ST"}
{"created":"2024-04-29 15:18:33","title":"Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy Multilingual User Input","abstract":"Geo-entity linking is the task of linking a location mention to the real-world geographic location. In this paper we explore the challenging task of geo-entity linking for noisy, multilingual social media data. There are few open-source multilingual geo-entity linking tools available and existing ones are often rule-based, which break easily in social media settings, or LLM-based, which are too expensive for large-scale datasets. We present a method which represents real-world locations as averaged embeddings from labeled user-input location names and allows for selective prediction via an interpretable confidence score. We show that our approach improves geo-entity linking on a global and multilingual social media dataset, and discuss progress and problems with evaluating at different geographic granularities.","sentences":["Geo-entity linking is the task of linking a location mention to the real-world geographic location.","In this paper we explore the challenging task of geo-entity linking for noisy, multilingual social media data.","There are few open-source multilingual geo-entity linking tools available and existing ones are often rule-based, which break easily in social media settings, or LLM-based, which are too expensive for large-scale datasets.","We present a method which represents real-world locations as averaged embeddings from labeled user-input location names and allows for selective prediction via an interpretable confidence score.","We show that our approach improves geo-entity linking on a global and multilingual social media dataset, and discuss progress and problems with evaluating at different geographic granularities."],"url":"http://arxiv.org/abs/2404.18784v1","category":"cs.CL"}
{"created":"2024-04-29 15:18:07","title":"Improved bounds for group testing in arbitrary hypergraphs","abstract":"Recent papers initiated the study of a generalization of group testing where the potentially contaminated sets are the members of a given hypergraph F=(V,E). This generalization finds application in contexts where contaminations can be conditioned by some kinds of social and geographical clusterings. The paper focuses on few-stage group testing algorithms, i.e., slightly adaptive algorithms where tests are performed in stages and all tests performed in the same stage should be decided at the very beginning of the stage. In particular, the paper presents the first two-stage algorithm that uses o(dlog|E|) tests for general hypergraphs with hyperedges of size at most d, and a three-stage algorithm that improves by a d^{1/6} factor on the number of tests of the best known three-stage algorithm. These algorithms are special cases of an s-stage algorithm designed for an arbitrary positive integer s<= d. The design of this algorithm resort to a new non-adaptive algorithm (one-stage algorithm), i.e., an algorithm where all tests must be decided beforehand. Further, we derive a lower bound for non-adaptive group testing. For E sufficiently large, the lower bound is very close to the upper bound on the number of tests of the best non-adaptive group testing algorithm known in the literature, and it is the first lower bound that improves on the information theoretic lower bound Omega(log |E|).","sentences":["Recent papers initiated the study of a generalization of group testing where the potentially contaminated sets are the members of a given hypergraph F=(V,E).","This generalization finds application in contexts where contaminations can be conditioned by some kinds of social and geographical clusterings.","The paper focuses on few-stage group testing algorithms, i.e., slightly adaptive algorithms where tests are performed in stages and all tests performed in the same stage should be decided at the very beginning of the stage.","In particular, the paper presents the first two-stage algorithm that uses o(dlog|E|) tests for general hypergraphs with hyperedges of size at most d, and a three-stage algorithm that improves by a d^{1/6} factor on the number of tests of the best known three-stage algorithm.","These algorithms are special cases of an s-stage algorithm designed for an arbitrary positive integer s<= d.","The design of this algorithm resort to a new non-adaptive algorithm (one-stage algorithm), i.e., an algorithm where all tests must be decided beforehand.","Further, we derive a lower bound for non-adaptive group testing.","For E sufficiently large, the lower bound is very close to the upper bound on the number of tests of the best non-adaptive group testing algorithm known in the literature, and it is the first lower bound that improves on the information theoretic lower bound Omega(log |E|)."],"url":"http://arxiv.org/abs/2404.18783v1","category":"cs.DS"}
{"created":"2024-04-29 15:17:30","title":"Wavelet-based tools to analyze, filter, and reconstruct transient gravitational-wave signals","abstract":"The analysis of gravitational-wave (GW) signals is one of the most challenging application areas of signal processing. Wavelet transforms are specially helpful in detecting and analyzing GW transients and several analysis pipelines are based on these transforms, both continuous and discrete. While discrete wavelet transforms have distinct advantages in terms of computing efficiency, continuous wavelet transforms (CWT) produce smooth and visually stunning time-frequency maps. In addition to wavelets the Q-transform is also used, which is a Morlet wavelet-like transform where the width of the Gaussian envelope is parameterized by a parameter denoted by Q. To date, the use of CWTs in GW data analysis has been limited by the higher computational load when compared with discrete wavelets, and also by the lack of an inversion formula for wavelet families that do not satisfy the admissibility condition. In this paper we consider Morlet wavelets parameterized in the same way as the Q-transform (hence the name wavelet Q-transform) which have all the advantages of the Morlet wavelets and where the wavelet transform can be inverted with a computationally efficient specialization of the non-standard inversion formula of Lebedeva and Postnikov [Lebedeva and Postnikov, Royal Society Open Science, 1 (2014) 140124]. We also introduce a two-parameter extension (the wavelet Qp-transform) which is well-adapted to chirping signals like those originating from compact binary coalescences (CBC), and show that it is also invertible just like the wavelet Q-transform. The inversion formulas of both transforms allow for effective noise filtering and produce very clean reconstructions of GW signals. Our preliminary results indicate that the method could be well suited to perform accurate tests of General Relativity by comparing modeled and unmodeled reconstructions of CBC GW signals.","sentences":["The analysis of gravitational-wave (GW) signals is one of the most challenging application areas of signal processing.","Wavelet transforms are specially helpful in detecting and analyzing GW transients and several analysis pipelines are based on these transforms, both continuous and discrete.","While discrete wavelet transforms have distinct advantages in terms of computing efficiency, continuous wavelet transforms (CWT) produce smooth and visually stunning time-frequency maps.","In addition to wavelets the Q-transform is also used, which is a Morlet wavelet-like transform where the width of the Gaussian envelope is parameterized by a parameter denoted by Q. To date, the use of CWTs in GW data analysis has been limited by the higher computational load when compared with discrete wavelets, and also by the lack of an inversion formula for wavelet families that do not satisfy the admissibility condition.","In this paper we consider Morlet wavelets parameterized in the same way as the Q-transform (hence the name wavelet Q-transform) which have all the advantages of the Morlet wavelets and where the wavelet transform can be inverted with a computationally efficient specialization of the non-standard inversion formula of Lebedeva and Postnikov [Lebedeva and Postnikov, Royal Society Open Science, 1 (2014) 140124].","We also introduce a two-parameter extension (the wavelet Qp-transform) which is well-adapted to chirping signals like those originating from compact binary coalescences (CBC), and show that it is also invertible just like the wavelet Q-transform.","The inversion formulas of both transforms allow for effective noise filtering and produce very clean reconstructions of GW signals.","Our preliminary results indicate that the method could be well suited to perform accurate tests of General Relativity by comparing modeled and unmodeled reconstructions of CBC GW signals."],"url":"http://arxiv.org/abs/2404.18781v1","category":"gr-qc"}
{"created":"2024-04-29 15:14:03","title":"On Approximating the Potts Model with Contracting Glauber Dynamics","abstract":"We show that the Potts model on a graph can be approximated by a sequence of independent and identically distributed spins in terms of Wasserstein distance at high temperatures. We prove a similar result for the Curie-Weiss-Potts model on the complete graph, conditioned on being close enough to any of its equilibrium macrostates, in the low-temperature regime. Our proof technique is based on Stein's method for comparing the stationary distributions of two Glauber dynamics with similar updates, one of which is rapid mixing and contracting on a subset of the state space. Along the way, we obtain new upper bounds on the mixing times of the Glauber dynamics for the Potts model on a general bounded-degree graph, and for the conditional measure of the Curie-Weiss-Potts model near an equilibrium macrostate.","sentences":["We show that the Potts model on a graph can be approximated by a sequence of independent and identically distributed spins in terms of Wasserstein distance at high temperatures.","We prove a similar result for the Curie-Weiss-Potts model on the complete graph, conditioned on being close enough to any of its equilibrium macrostates, in the low-temperature regime.","Our proof technique is based on Stein's method for comparing the stationary distributions of two Glauber dynamics with similar updates, one of which is rapid mixing and contracting on a subset of the state space.","Along the way, we obtain new upper bounds on the mixing times of the Glauber dynamics for the Potts model on a general bounded-degree graph, and for the conditional measure of the Curie-Weiss-Potts model near an equilibrium macrostate."],"url":"http://arxiv.org/abs/2404.18778v1","category":"math.PR"}
{"created":"2024-04-29 15:11:38","title":"Quantum key distribution with displaced thermal states","abstract":"Secret key exchange relies on the creation of correlated signals, serving as the raw resource for secure communication. Thermal states, exhibit Hanbury Brown and Twiss correlations, which offer a promising avenue for generating such signals. In this paper, we present an experimental implementation of a central broadcast thermal state quantum key distribution (QKD) protocol in the microwave region. Our objective is to showcase a straightforward method of QKD utilizing readily available broadcasting equipment. Unlike conventional approaches to thermal state QKD, we leverage displaced thermal states. These states enable us to share the output of a thermal source among Alice, Bob, and Eve via both waveguide channels and free space. Through measurement and conversion into bit strings, our protocol produces key-ready bit strings without the need for specialized equipment. By harnessing the inherent noise in thermal broadcasts, our setup facilitates the recovery of distinct bit strings by all parties involved.","sentences":["Secret key exchange relies on the creation of correlated signals, serving as the raw resource for secure communication.","Thermal states, exhibit Hanbury Brown and Twiss correlations, which offer a promising avenue for generating such signals.","In this paper, we present an experimental implementation of a central broadcast thermal state quantum key distribution (QKD) protocol in the microwave region.","Our objective is to showcase a straightforward method of QKD utilizing readily available broadcasting equipment.","Unlike conventional approaches to thermal state QKD, we leverage displaced thermal states.","These states enable us to share the output of a thermal source among Alice, Bob, and Eve via both waveguide channels and free space.","Through measurement and conversion into bit strings, our protocol produces key-ready bit strings without the need for specialized equipment.","By harnessing the inherent noise in thermal broadcasts, our setup facilitates the recovery of distinct bit strings by all parties involved."],"url":"http://arxiv.org/abs/2404.18777v1","category":"quant-ph"}
{"created":"2024-04-29 15:10:02","title":"Resource-rational reinforcement learning and sensorimotor causal states","abstract":"We propose a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement learning, the study of learning with feedback via rewards; rate-distortion theory, a branch of information theory that deals with compressing signals to retain relevant information; and computational mechanics, the study of minimal sufficient statistics of prediction also known as causal states. We highlight why this proposal is likely only an approximation, but is likely to be an interesting one, and propose a new algorithm for evaluating it to obtain the newly-coined \"reward-rate manifold\". The performance of real and artificial agents in partially observable environments can be newly benchmarked using these reward-rate manifolds. Finally, we describe experiments that can probe whether or not biological organisms are resource-rational reinforcement learners.","sentences":["We propose a new computational-level objective function for theoretical biology and theoretical neuroscience that combines: reinforcement learning, the study of learning with feedback via rewards; rate-distortion theory, a branch of information theory that deals with compressing signals to retain relevant information; and computational mechanics, the study of minimal sufficient statistics of prediction also known as causal states.","We highlight why this proposal is likely only an approximation, but is likely to be an interesting one, and propose a new algorithm for evaluating it to obtain the newly-coined \"reward-rate manifold\".","The performance of real and artificial agents in partially observable environments can be newly benchmarked using these reward-rate manifolds.","Finally, we describe experiments that can probe whether or not biological organisms are resource-rational reinforcement learners."],"url":"http://arxiv.org/abs/2404.18775v1","category":"q-bio.NC"}
{"created":"2024-04-29 15:09:00","title":"Self-training superconducting neuromorphic circuits using reinforcement learning rules","abstract":"Reinforcement learning algorithms are used in a wide range of applications, from gaming and robotics to autonomous vehicles. In this paper we describe a set of reinforcement learning-based local weight update rules and their implementation in superconducting hardware. Using SPICE circuit simulations, we implement a small-scale neural network with a learning time of order one nanosecond. This network can be trained to learn new functions simply by changing the target output for a given set of inputs, without the need for any external adjustments to the network. In this implementation the weights are adjusted based on the current state of the overall network response and locally stored information about the previous action. This removes the need to program explicit weight values in these networks, which is one of the primary challenges that analog hardware implementations of neural networks face. The adjustment of weights is based on a global reinforcement signal that obviates the need for circuitry to back-propagate errors.","sentences":["Reinforcement learning algorithms are used in a wide range of applications, from gaming and robotics to autonomous vehicles.","In this paper we describe a set of reinforcement learning-based local weight update rules and their implementation in superconducting hardware.","Using SPICE circuit simulations, we implement a small-scale neural network with a learning time of order one nanosecond.","This network can be trained to learn new functions simply by changing the target output for a given set of inputs, without the need for any external adjustments to the network.","In this implementation the weights are adjusted based on the current state of the overall network response and locally stored information about the previous action.","This removes the need to program explicit weight values in these networks, which is one of the primary challenges that analog hardware implementations of neural networks face.","The adjustment of weights is based on a global reinforcement signal that obviates the need for circuitry to back-propagate errors."],"url":"http://arxiv.org/abs/2404.18774v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 15:05:42","title":"Saliency Suppressed, Semantics Surfaced: Visual Transformations in Neural Networks and the Brain","abstract":"Deep learning algorithms lack human-interpretable accounts of how they transform raw visual input into a robust semantic understanding, which impedes comparisons between different architectures, training objectives, and the human brain. In this work, we take inspiration from neuroscience and employ representational approaches to shed light on how neural networks encode information at low (visual saliency) and high (semantic similarity) levels of abstraction. Moreover, we introduce a custom image dataset where we systematically manipulate salient and semantic information. We find that ResNets are more sensitive to saliency information than ViTs, when trained with object classification objectives. We uncover that networks suppress saliency in early layers, a process enhanced by natural language supervision (CLIP) in ResNets. CLIP also enhances semantic encoding in both architectures. Finally, we show that semantic encoding is a key factor in aligning AI with human visual perception, while saliency suppression is a non-brain-like strategy.","sentences":["Deep learning algorithms lack human-interpretable accounts of how they transform raw visual input into a robust semantic understanding, which impedes comparisons between different architectures, training objectives, and the human brain.","In this work, we take inspiration from neuroscience and employ representational approaches to shed light on how neural networks encode information at low (visual saliency) and high (semantic similarity) levels of abstraction.","Moreover, we introduce a custom image dataset where we systematically manipulate salient and semantic information.","We find that ResNets are more sensitive to saliency information than ViTs, when trained with object classification objectives.","We uncover that networks suppress saliency in early layers, a process enhanced by natural language supervision (CLIP) in ResNets.","CLIP also enhances semantic encoding in both architectures.","Finally, we show that semantic encoding is a key factor in aligning AI with human visual perception, while saliency suppression is a non-brain-like strategy."],"url":"http://arxiv.org/abs/2404.18772v1","category":"cs.CV"}
{"created":"2024-04-29 15:05:27","title":"KBX: Verified Model Synchronization via Formal Bidirectional Transformation","abstract":"Complex safety-critical systems require multiple models for a comprehensive description, resulting in error-prone development and laborious verification. Bidirectional transformation (BX) is an approach to automatically synchronizing these models. However, existing BX frameworks lack formal verification to enforce these models' consistency rigorously. This paper introduces KBX, a formal bidirectional transformation framework for verified model synchronization. First, we present a matching logic-based BX model, providing a logical foundation for constructing BX definitions within the $\\mathbb{K}$ framework. Second, we propose algorithms to synthesize formal BX definitions from unidirectional ones, which allows developers to focus on crafting the unidirectional definitions while disregarding the reverse direction and missing information recovery for synchronization. Afterward, we harness $\\mathbb{K}$ to generate a formal synchronizer from the synthesized definitions for consistency maintenance and verification. To evaluate the effectiveness of KBX, we conduct a comparative analysis against existing BX frameworks. Furthermore, we demonstrate the application of KBX in constructing a BX between UML and HCSP for real-world scenarios, showcasing an 82.8\\% reduction in BX development effort compared to manual specification writing in $\\mathbb{K}$.","sentences":["Complex safety-critical systems require multiple models for a comprehensive description, resulting in error-prone development and laborious verification.","Bidirectional transformation (BX) is an approach to automatically synchronizing these models.","However, existing BX frameworks lack formal verification to enforce these models' consistency rigorously.","This paper introduces KBX, a formal bidirectional transformation framework for verified model synchronization.","First, we present a matching logic-based BX model, providing a logical foundation for constructing BX definitions within the $\\mathbb{K}$ framework.","Second, we propose algorithms to synthesize formal BX definitions from unidirectional ones, which allows developers to focus on crafting the unidirectional definitions while disregarding the reverse direction and missing information recovery for synchronization.","Afterward, we harness $\\mathbb{K}$ to generate a formal synchronizer from the synthesized definitions for consistency maintenance and verification.","To evaluate the effectiveness of KBX, we conduct a comparative analysis against existing BX frameworks.","Furthermore, we demonstrate the application of KBX in constructing a BX between UML and HCSP for real-world scenarios, showcasing an 82.8\\% reduction in BX development effort compared to manual specification writing in $\\mathbb{K}$."],"url":"http://arxiv.org/abs/2404.18771v1","category":"cs.SE"}
{"created":"2024-04-29 15:04:25","title":"Generalizing Space Logistics Network Optimization with Integrated Machine Learning and Mathematical Programming","abstract":"Recent growing complexity in space missions has led to an active research field of space logistics and mission design. This research field leverages the key ideas and methods used to handle complex terrestrial logistics to tackle space logistics design problems. A typical goal in space logistics is to optimize the commodity flow to satisfy some mission objectives with the lowest cost. One of the successful space logistics approaches is network flow modeling and optimization using mixed-integer linear programming (MILP). A caveat of the conventional MILP-based network approach for space logistics is its incapability of handling nonlinearity. For example, in the MILP formulation, the spacecraft structure mass and fuel/payload capacity are approximated by a linear relationship. However, this oversimplified relationship cannot characterize a realistic spacecraft design. Other types of nonlinearity can appear when a nonlinear time-dependent trajectory model is considered in an event-driven network, where the time step of each event itself is a variable. In response to this challenge, this Note develops a new systematic general framework to handle nonlinearity in the MILP-based space logistics formulation using machine learning (ML). Specifically, we replace the nonlinear constraints in the space logistics formulation with trained ML models that are compatible with MILP. The MILP-compatible ML model includes linear regression, PWL approximations, neural networks (NN) with Rectified Linear Unit (ReLU) activations, decision tree regression, and random forest regression, among others; these models can be translated into MILP formulations with a definition of additional variables and constraints while maintaining the linearity. This Note provides the first demonstration of using such trained ML models directly in a MILP-based space logistics optimization formulation.","sentences":["Recent growing complexity in space missions has led to an active research field of space logistics and mission design.","This research field leverages the key ideas and methods used to handle complex terrestrial logistics to tackle space logistics design problems.","A typical goal in space logistics is to optimize the commodity flow to satisfy some mission objectives with the lowest cost.","One of the successful space logistics approaches is network flow modeling and optimization using mixed-integer linear programming (MILP).","A caveat of the conventional MILP-based network approach for space logistics is its incapability of handling nonlinearity.","For example, in the MILP formulation, the spacecraft structure mass and fuel/payload capacity are approximated by a linear relationship.","However, this oversimplified relationship cannot characterize a realistic spacecraft design.","Other types of nonlinearity can appear when a nonlinear time-dependent trajectory model is considered in an event-driven network, where the time step of each event itself is a variable.","In response to this challenge, this Note develops a new systematic general framework to handle nonlinearity in the MILP-based space logistics formulation using machine learning (ML).","Specifically, we replace the nonlinear constraints in the space logistics formulation with trained ML models that are compatible with MILP.","The MILP-compatible ML model includes linear regression, PWL approximations, neural networks (NN) with Rectified Linear Unit (ReLU) activations, decision tree regression, and random forest regression, among others; these models can be translated into MILP formulations with a definition of additional variables and constraints while maintaining the linearity.","This Note provides the first demonstration of using such trained ML models directly in a MILP-based space logistics optimization formulation."],"url":"http://arxiv.org/abs/2404.18770v1","category":"math.OC"}
{"created":"2024-04-29 15:04:07","title":"Learning with Norm Constrained, Over-parameterized, Two-layer Neural Networks","abstract":"Recent studies show that a reproducing kernel Hilbert space (RKHS) is not a suitable space to model functions by neural networks as the curse of dimensionality (CoD) cannot be evaded when trying to approximate even a single ReLU neuron (Bach, 2017). In this paper, we study a suitable function space for over-parameterized two-layer neural networks with bounded norms (e.g., the path norm, the Barron norm) in the perspective of sample complexity and generalization properties. First, we show that the path norm (as well as the Barron norm) is able to obtain width-independence sample complexity bounds, which allows for uniform convergence guarantees. Based on this result, we derive the improved result of metric entropy for $\\epsilon$-covering up to $\\mathcal{O}(\\epsilon^{-\\frac{2d}{d+2}})$ ($d$ is the input dimension and the depending constant is at most polynomial order of $d$) via the convex hull technique, which demonstrates the separation with kernel methods with $\\Omega(\\epsilon^{-d})$ to learn the target function in a Barron space. Second, this metric entropy result allows for building a sharper generalization bound under a general moment hypothesis setting, achieving the rate at $\\mathcal{O}(n^{-\\frac{d+2}{2d+2}})$. Our analysis is novel in that it offers a sharper and refined estimation for metric entropy (with a clear dependence relationship on the dimension $d$) and unbounded sampling in the estimation of the sample error and the output error.","sentences":["Recent studies show that a reproducing kernel Hilbert space (RKHS) is not a suitable space to model functions by neural networks as the curse of dimensionality (CoD) cannot be evaded when trying to approximate even a single ReLU neuron (Bach, 2017).","In this paper, we study a suitable function space for over-parameterized two-layer neural networks with bounded norms (e.g., the path norm, the Barron norm) in the perspective of sample complexity and generalization properties.","First, we show that the path norm (as well as the Barron norm) is able to obtain width-independence sample complexity bounds, which allows for uniform convergence guarantees.","Based on this result, we derive the improved result of metric entropy for $\\epsilon$-covering up to $\\mathcal{O}(\\epsilon^{-\\frac{2d}{d+2}})$ ($d$ is the input dimension and the depending constant is at most polynomial order of $d$) via the convex hull technique, which demonstrates the separation with kernel methods with $\\Omega(\\epsilon^{-d})$ to learn the target function in a Barron space.","Second, this metric entropy result allows for building a sharper generalization bound under a general moment hypothesis setting, achieving the rate at $\\mathcal{O}(n^{-\\frac{d+2}{2d+2}})$. Our analysis is novel in that it offers a sharper and refined estimation for metric entropy (with a clear dependence relationship on the dimension $d$) and unbounded sampling in the estimation of the sample error and the output error."],"url":"http://arxiv.org/abs/2404.18769v1","category":"stat.ML"}
{"created":"2024-04-29 15:02:14","title":"PECC: Problem Extraction and Coding Challenges","abstract":"Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMs' capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver.","sentences":["Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning.","Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored.","Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems.","Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code.","A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities.","Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems.","By probing the limits of LLMs' capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver."],"url":"http://arxiv.org/abs/2404.18766v1","category":"cs.AI"}
{"created":"2024-04-29 15:01:46","title":"Mathematical modelling of heat transfer in closed electrical contacts and electrical potential field dynamics with Thomson effect","abstract":"In this study we develop a mathematical model that describe the behavior of electromagnetic fields and heat transfer in closed electrical contacts that arises when instantaneous explosion of the micro-asperity which involves vaporization zone and liquid, solid zones where temperature is defined by a generalized heat equation with Thomson effect. This model account for the nonlinear nature of the thermal coefficients and electrical conductivity depended on temperature. Our proposed solutions are based on similarity transformation which allows us to reduce a Stefan-type problem to a system of nonlinear integral equations whose existence of solution is proved by the fixed point theory in Banach spaces.","sentences":["In this study we develop a mathematical model that describe the behavior of electromagnetic fields and heat transfer in closed electrical contacts that arises when instantaneous explosion of the micro-asperity which involves vaporization zone and liquid, solid zones where temperature is defined by a generalized heat equation with Thomson effect.","This model account for the nonlinear nature of the thermal coefficients and electrical conductivity depended on temperature.","Our proposed solutions are based on similarity transformation which allows us to reduce a Stefan-type problem to a system of nonlinear integral equations whose existence of solution is proved by the fixed point theory in Banach spaces."],"url":"http://arxiv.org/abs/2404.18765v1","category":"math.AP"}
{"created":"2024-04-29 15:01:09","title":"From Density to Geometry: YOLOv8 Instance Segmentation for Reverse Engineering of Optimized Structures","abstract":"This paper introduces YOLOv8-TO, a novel approach for reverse engineering of topology-optimized structures into interpretable geometric parameters using the YOLOv8 instance segmentation model. Density-based topology optimization methods require post-processing to convert the optimal density distribution into a parametric representation for design exploration and integration with CAD tools. Traditional methods such as skeletonization struggle with complex geometries and require manual intervention. YOLOv8-TO addresses these challenges by training a custom YOLOv8 model to automatically detect and reconstruct structural components from binary density distributions. The model is trained on a diverse dataset of both optimized and random structures generated using the Moving Morphable Components method. A custom reconstruction loss function based on the dice coefficient of the predicted geometry is used to train the new regression head of the model via self-supervised learning. The method is evaluated on test sets generated from different topology optimization methods, including out-of-distribution samples, and compared against a skeletonization approach. Results show that YOLOv8-TO significantly outperforms skeletonization in reconstructing visually and structurally similar designs. The method showcases an average improvement of 13.84% in the Dice coefficient, with peak enhancements reaching 20.78%. The method demonstrates good generalization to complex geometries and fast inference times, making it suitable for integration into design workflows using regular workstations. Limitations include the sensitivity to non-max suppression thresholds. YOLOv8-TO represents a significant advancement in topology optimization post-processing, enabling efficient and accurate reverse engineering of optimized structures for design exploration and manufacturing.","sentences":["This paper introduces YOLOv8-TO, a novel approach for reverse engineering of topology-optimized structures into interpretable geometric parameters using the YOLOv8 instance segmentation model.","Density-based topology optimization methods require post-processing to convert the optimal density distribution into a parametric representation for design exploration and integration with CAD tools.","Traditional methods such as skeletonization struggle with complex geometries and require manual intervention.","YOLOv8-TO addresses these challenges by training a custom YOLOv8 model to automatically detect and reconstruct structural components from binary density distributions.","The model is trained on a diverse dataset of both optimized and random structures generated using the Moving Morphable Components method.","A custom reconstruction loss function based on the dice coefficient of the predicted geometry is used to train the new regression head of the model via self-supervised learning.","The method is evaluated on test sets generated from different topology optimization methods, including out-of-distribution samples, and compared against a skeletonization approach.","Results show that YOLOv8-TO significantly outperforms skeletonization in reconstructing visually and structurally similar designs.","The method showcases an average improvement of 13.84% in the Dice coefficient, with peak enhancements reaching 20.78%.","The method demonstrates good generalization to complex geometries and fast inference times, making it suitable for integration into design workflows using regular workstations.","Limitations include the sensitivity to non-max suppression thresholds.","YOLOv8-TO represents a significant advancement in topology optimization post-processing, enabling efficient and accurate reverse engineering of optimized structures for design exploration and manufacturing."],"url":"http://arxiv.org/abs/2404.18763v1","category":"cs.CV"}
{"created":"2024-04-29 15:00:47","title":"Genericity of sublinearly Morse directions in general metric spaces","abstract":"In this paper, we show that for any proper statistically convexcocompact actions on proper metric spaces, the sublinearly Morse boundary has full Patterson-Sullivan measure in the horofundction boundary.","sentences":["In this paper, we show that for any proper statistically convexcocompact actions on proper metric spaces, the sublinearly Morse boundary has full Patterson-Sullivan measure in the horofundction boundary."],"url":"http://arxiv.org/abs/2404.18762v1","category":"math.GR"}
{"created":"2024-04-29 14:57:16","title":"Flow AM: Generating Point Cloud Global Explanations by Latent Alignment","abstract":"Although point cloud models have gained significant improvements in prediction accuracy over recent years, their trustworthiness is still not sufficiently investigated. In terms of global explainability, Activation Maximization (AM) techniques in the image domain are not directly transplantable due to the special structure of the point cloud models. Existing studies exploit generative models to yield global explanations that can be perceived by humans. However, the opacity of the generative models themselves and the introduction of additional priors call into question the plausibility and fidelity of the explanations. In this work, we demonstrate that when the classifier predicts different types of instances, the intermediate layer activations are differently activated, known as activation flows. Based on this property, we propose an activation flow-based AM method that generates global explanations that can be perceived without incorporating any generative model. Furthermore, we reveal that AM based on generative models fails the sanity checks and thus lack of fidelity. Extensive experiments show that our approach dramatically enhances the perceptibility of explanations compared to other AM methods that are not based on generative models. Our code is available at: https://github.com/Explain3D/FlowAM","sentences":["Although point cloud models have gained significant improvements in prediction accuracy over recent years, their trustworthiness is still not sufficiently investigated.","In terms of global explainability, Activation Maximization (AM) techniques in the image domain are not directly transplantable due to the special structure of the point cloud models.","Existing studies exploit generative models to yield global explanations that can be perceived by humans.","However, the opacity of the generative models themselves and the introduction of additional priors call into question the plausibility and fidelity of the explanations.","In this work, we demonstrate that when the classifier predicts different types of instances, the intermediate layer activations are differently activated, known as activation flows.","Based on this property, we propose an activation flow-based AM method that generates global explanations that can be perceived without incorporating any generative model.","Furthermore, we reveal that AM based on generative models fails the sanity checks and thus lack of fidelity.","Extensive experiments show that our approach dramatically enhances the perceptibility of explanations compared to other AM methods that are not based on generative models.","Our code is available at: https://github.com/Explain3D/FlowAM"],"url":"http://arxiv.org/abs/2404.18760v1","category":"cs.CV"}
{"created":"2024-04-29 14:56:11","title":"Transitive Vision-Language Prompt Learning for Domain Generalization","abstract":"The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains. The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems. In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability. Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets.","sentences":["The vision-language pre-training has enabled deep models to make a huge step forward in generalizing across unseen domains.","The recent learning method based on the vision-language pre-training model is a great tool for domain generalization and can solve this problem to a large extent.","However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems.","However, there are still some issues that an advancement still suffers from trading-off between domain invariance and class separability, which are crucial in current DG problems.","In this paper, we introduce a novel prompt learning strategy that leverages deep vision prompts to address domain invariance while utilizing language prompts to ensure class separability, coupled with adaptive weighting mechanisms to balance domain invariance and class separability.","Extensive experiments demonstrate that deep vision prompts effectively extract domain-invariant features, significantly improving the generalization ability of deep models and achieving state-of-the-art performance on three datasets."],"url":"http://arxiv.org/abs/2404.18758v1","category":"cs.CV"}
{"created":"2024-04-29 14:56:04","title":"A Gauss curvature flow approach to the p-harmonic measure of Minkowski problem","abstract":"The Minkowski problem of harmonic measures was first studied by Jerison \\cite{JER1991}. Recently, Akman and Mukherjee \\cite{AKM2023} studied the Minkowski problem corresponding to $p$-harmonic measures on convex domains and generalized Jerison's results. In this paper, we obtain the existence of the smooth solution of the $p$-harmonic measure Minkowski problem by method of the Gauss curvature flow.","sentences":["The Minkowski problem of harmonic measures was first studied by Jerison \\cite{JER1991}.","Recently, Akman and Mukherjee \\cite{AKM2023} studied the Minkowski problem corresponding to $p$-harmonic measures on convex domains and generalized Jerison's results.","In this paper, we obtain the existence of the smooth solution of the $p$-harmonic measure Minkowski problem by method of the Gauss curvature flow."],"url":"http://arxiv.org/abs/2404.18757v1","category":"math.AP"}
{"created":"2024-04-29 14:54:50","title":"K-CIRCT: A Layered, Composable, and Executable Formal Semantics for CIRCT Hardware IRs","abstract":"CIRCT, an open-source EDA framework akin to LLVM for software, is a foundation for various hardware description languages. Despite its crucial role, CIRCT's lack of formal semantics challenges necessary rigorous hardware verification. Thus, this paper introduces K-CIRCT, the first formal semantics in {\\K} for a substantial CIRCT subset adequate for simulating a RISC-V processor. Our semantics are structured into multiple layers: (1) MLIR static semantics, which covers fundamental MLIR concepts across domains; (2) CIRCT common semantics, featuring a generic hardware model that captures key hardware features across dialects; and (3) composable and extensible semantics for specific dialects, formalized individually using a unified approach. This approach has been applied to formalize CIRCT core dialects. We validated our semantics through full-rule coverage tests and assessed its practicality using the popular RISC-V hardware design, riscv-mini.","sentences":["CIRCT, an open-source EDA framework akin to LLVM for software, is a foundation for various hardware description languages.","Despite its crucial role, CIRCT's lack of formal semantics challenges necessary rigorous hardware verification.","Thus, this paper introduces K-CIRCT, the first formal semantics in {\\K} for a substantial CIRCT subset adequate for simulating a RISC-V processor.","Our semantics are structured into multiple layers: (1) MLIR static semantics, which covers fundamental MLIR concepts across domains; (2) CIRCT common semantics, featuring a generic hardware model that captures key hardware features across dialects; and (3) composable and extensible semantics for specific dialects, formalized individually using a unified approach.","This approach has been applied to formalize CIRCT core dialects.","We validated our semantics through full-rule coverage tests and assessed its practicality using the popular RISC-V hardware design, riscv-mini."],"url":"http://arxiv.org/abs/2404.18756v1","category":"cs.SE"}
{"created":"2024-04-29 14:52:11","title":"Spectral measures and iterative bounds for effective diffusivity of steady and space-time periodic flows","abstract":"Over three decades ago the advection-diffusion equation for a steady fluid velocity field was homogenized, leading to a Stieltjes integral representation for the effective diffusivity, which is given in terms of a spectral measure of a compact self-adjoint operator and the P\\'eclet number of the fluid flow. This result was recently extended to space-time periodic flows, instead involving an unbounded self-adjoint operator. Pad\\'e approximants provide rigorous upper and lower bounds for Stieltjes functions in terms of the moments of the spectral measure. However, with the lack of a method for calculating the moments of the spectral measure for general fluid velocity fields, the utility of this powerful mathematical framework for calculating bounds for the effective diffusivity has not been fully realized. Here we significantly expand the applicability of this framework by providing an iterative method that enables an arbitrary number of moments, hence bounds, to be calculated analytically in closed form for both spatially and space-time periodic flows. The method is demonstrated for periodic flows in two spatial dimensions. The known asymptotic behavior of the effective diffusivity for a steady flow is accurately captured by high order upper and lower bounds, demonstrating the ability of the method to provide accurate estimates for the effective diffusivity for a broad range of parameter values.","sentences":["Over three decades ago the advection-diffusion equation for a steady fluid velocity field was homogenized, leading to a Stieltjes integral representation for the effective diffusivity, which is given in terms of a spectral measure of a compact self-adjoint operator and the P\\'eclet number of the fluid flow.","This result was recently extended to space-time periodic flows, instead involving an unbounded self-adjoint operator.","Pad\\'e approximants provide rigorous upper and lower bounds for Stieltjes functions in terms of the moments of the spectral measure.","However, with the lack of a method for calculating the moments of the spectral measure for general fluid velocity fields, the utility of this powerful mathematical framework for calculating bounds for the effective diffusivity has not been fully realized.","Here we significantly expand the applicability of this framework by providing an iterative method that enables an arbitrary number of moments, hence bounds, to be calculated analytically in closed form for both spatially and space-time periodic flows.","The method is demonstrated for periodic flows in two spatial dimensions.","The known asymptotic behavior of the effective diffusivity for a steady flow is accurately captured by high order upper and lower bounds, demonstrating the ability of the method to provide accurate estimates for the effective diffusivity for a broad range of parameter values."],"url":"http://arxiv.org/abs/2404.18754v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 14:50:06","title":"Quantum State Designs with Clifford Enhanced Matrix Product States","abstract":"Nonstabilizerness, or `magic', is a critical quantum resource that, together with entanglement, characterizes the non-classical complexity of quantum states. Here, we address the problem of quantifying the average nonstabilizerness of random Matrix Product States (RMPS). RMPS represent a generalization of random product states featuring bounded entanglement that scales logarithmically with the bond dimension $\\chi$. We demonstrate that the $2$-Stabilizer R\\'enyi Entropy converges to that of Haar random states as $N/\\chi^2$, where $N$ is the system size. This indicates that MPS with a modest bond dimension are as magical as generic states. Subsequently, we introduce the ensemble of Clifford enhanced Matrix Product States ($\\mathcal{C}$MPS), built by the action of Clifford unitaries on RMPS. Leveraging our previous result, we show that $\\mathcal{C}$MPS can approximate $4$-spherical designs with arbitrary accuracy. Specifically, for a constant $N$, $\\mathcal{C}$MPS become close to $4$-designs with a scaling as $\\chi^{-2}$. Our findings indicate that combining Clifford unitaries with polynomially complex tensor network states can generate highly non-trivial quantum states.","sentences":["Nonstabilizerness, or `magic', is a critical quantum resource that, together with entanglement, characterizes the non-classical complexity of quantum states.","Here, we address the problem of quantifying the average nonstabilizerness of random Matrix Product States (RMPS).","RMPS represent a generalization of random product states featuring bounded entanglement that scales logarithmically with the bond dimension $\\chi$. We demonstrate that the $2$-Stabilizer R\\'enyi Entropy converges to that of Haar random states as $N/\\chi^2$, where $N$ is the system size.","This indicates that MPS with a modest bond dimension are as magical as generic states.","Subsequently, we introduce the ensemble of Clifford enhanced Matrix Product States ($\\mathcal{C}$MPS), built by the action of Clifford unitaries on RMPS.","Leveraging our previous result, we show that $\\mathcal{C}$MPS can approximate $4$-spherical designs with arbitrary accuracy.","Specifically, for a constant $N$, $\\mathcal{C}$MPS become close to $4$-designs with a scaling as $\\chi^{-2}$. Our findings indicate that combining Clifford unitaries with polynomially complex tensor network states can generate highly non-trivial quantum states."],"url":"http://arxiv.org/abs/2404.18751v1","category":"quant-ph"}
{"created":"2024-04-29 14:47:32","title":"Evaluating the Effectiveness of Video Anomaly Detection in the Wild: Online Learning and Inference for Real-world Deployment","abstract":"Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare. Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts. Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility. Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously. This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages. Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy. We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains. Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain.","sentences":["Video Anomaly Detection (VAD) identifies unusual activities in video streams, a key technology with broad applications ranging from surveillance to healthcare.","Tackling VAD in real-life settings poses significant challenges due to the dynamic nature of human actions, environmental variations, and domain shifts.","Many research initiatives neglect these complexities, often concentrating on traditional testing methods that fail to account for performance on unseen datasets, creating a gap between theoretical models and their real-world utility.","Online learning is a potential strategy to mitigate this issue by allowing models to adapt to new information continuously.","This paper assesses how well current VAD algorithms can adjust to real-life conditions through an online learning framework, particularly those based on pose analysis, for their efficiency and privacy advantages.","Our proposed framework enables continuous model updates with streaming data from novel environments, thus mirroring actual world challenges and evaluating the models' ability to adapt in real-time while maintaining accuracy.","We investigate three state-of-the-art models in this setting, focusing on their adaptability across different domains.","Our findings indicate that, even under the most challenging conditions, our online learning approach allows a model to preserve 89.39% of its original effectiveness compared to its offline-trained counterpart in a specific target domain."],"url":"http://arxiv.org/abs/2404.18747v1","category":"cs.CV"}
{"created":"2024-04-29 14:46:35","title":"Enhancing Interactive Image Retrieval With Query Rewriting Using Large Language Models and Vision Language Models","abstract":"Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics. Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database. However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall. These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness. To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting. This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration. Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models. To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query. Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10\\% improvement in terms of recall. Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation.","sentences":["Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics.","Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database.","However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall.","These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness.","To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting.","This system incorporates a vision language model (VLM) based image captioner to enhance the quality of text-based queries, resulting in more informative queries with each iteration.","Moreover, we introduce a large language model (LLM) based denoiser to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models.","To evaluate our system, we curate a new dataset by adapting the MSR-VTT video retrieval dataset to the image retrieval task, offering multiple relevant ground truth images for each query.","Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10\\% improvement in terms of recall.","Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation."],"url":"http://arxiv.org/abs/2404.18746v1","category":"cs.MM"}
{"created":"2024-04-29 14:43:15","title":"An unconventional deformation of the nonrelativistic spin-1/2 Fermi gas","abstract":"We explore a generalization of nonrelativistic fermionic statistics that interpolates between bosons and fermions, in which up to $K$ particles may occupy a single-particle state. We show that it can be mapped exactly to $K$ flavors of fermions with imaginary polarization. In particular, for $K\\!=\\!2$, we use such a mapping to derive the virial coefficients and relate them to those of conventional spin-1/2 fermions in an exact fashion. We also use the mapping to derive next-to-leading-order perturbative results for the pressure equation of state. Our results indicate that the $K\\!=\\!2$ particles are more strongly coupled than conventional spin-$1/2$ fermions, as measured by the interaction effects on the virial expansion and on the pressure equation of state. In the regime set by the unitary limit, the proposed $K\\!=\\!2$ deformation represents a universal many-body system whose properties remain largely unknown. In particular, the system can be expected to become superfluid at a critical temperature $T_c$ higher than that of the unitary limit. We suggest it may be possible to realize this system experimentally by engineering a polarized coupling to an electrostatic potential. Finally, we show that the $K\\!=\\!2$ system does not display a sign problem for determinantal Monte Carlo calculations, which indicates that $T_c$ can at least in principle be calculated with conventional methods.","sentences":["We explore a generalization of nonrelativistic fermionic statistics that interpolates between bosons and fermions, in which up to $K$ particles may occupy a single-particle state.","We show that it can be mapped exactly to $K$ flavors of fermions with imaginary polarization.","In particular, for $K\\!=\\!2$, we use such a mapping to derive the virial coefficients and relate them to those of conventional spin-1/2 fermions in an exact fashion.","We also use the mapping to derive next-to-leading-order perturbative results for the pressure equation of state.","Our results indicate that the $K\\!=\\!2$ particles are more strongly coupled than conventional spin-$1/2$ fermions, as measured by the interaction effects on the virial expansion and on the pressure equation of state.","In the regime set by the unitary limit, the proposed $K\\!=\\!2$ deformation represents a universal many-body system whose properties remain largely unknown.","In particular, the system can be expected to become superfluid at a critical temperature $T_c$ higher than that of the unitary limit.","We suggest it may be possible to realize this system experimentally by engineering a polarized coupling to an electrostatic potential.","Finally, we show that the $K\\!=\\!2$ system does not display a sign problem for determinantal Monte Carlo calculations, which indicates that $T_c$ can at least in principle be calculated with conventional methods."],"url":"http://arxiv.org/abs/2404.18743v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 14:42:32","title":"Torsion-induced axions in string theory, quantum gravity and the cosmological tensions","abstract":"We discuss the role of torsion in string theory on inducing pseudoscalar degrees of freedom (axions), which in turn couple to (gravitational) Chern-Simons (CS) anomalous terms. Such interactions can induce inflation, of running vacuum type, not requiring external inflaton fields, through condensation of the anomalous terms as a consequence of primordial chiral gravitational-wave (GW) tensor perturbations in a weak-quantum gravity setting. The presence of an UV cutoff for the GW quantum graviton modes opens up the system, leading to a dissipative behaviour realised via the presence of non trivial imaginary parts of the gravitational CS terms. The naive estimate of the life time of inflation based on such imaginary parts, which afflict the pertinent GW Hamiltonian, is quite consistent with the estimates of the duration of inflation based on an analysis of the condensate-induced linear-axion-potential by means of dynamical systems. Such quantum-gravity effects can also contribute positively to the alleviation of cosmological tensions if they survive today. In the talk we discuss the conditions under which such a result may be achieved. We also discuss the potential role of other axions in string theory, coming from compactification, in inducing enhanced densities of primordial black holes during RVM inflation, thereby contributing to significantly increased percentages of these black holes that can play the role of dark matter components. Moreover, under certain circumstances, that we shall discuss in some detail, it is also possible that the initially massless torsion-induced axions can acquire a non-trivial mass during the radiation era, thereby providing additional dark matter components in the Universe. With regards to this aspect, we also emphasise the role of massive right-handed neutrinos, provided that such excitations exist in the relevant spectra.","sentences":["We discuss the role of torsion in string theory on inducing pseudoscalar degrees of freedom (axions), which in turn couple to (gravitational) Chern-Simons (CS) anomalous terms.","Such interactions can induce inflation, of running vacuum type, not requiring external inflaton fields, through condensation of the anomalous terms as a consequence of primordial chiral gravitational-wave (GW) tensor perturbations in a weak-quantum gravity setting.","The presence of an UV cutoff for the GW quantum graviton modes opens up the system, leading to a dissipative behaviour realised via the presence of non trivial imaginary parts of the gravitational CS terms.","The naive estimate of the life time of inflation based on such imaginary parts, which afflict the pertinent GW Hamiltonian, is quite consistent with the estimates of the duration of inflation based on an analysis of the condensate-induced linear-axion-potential by means of dynamical systems.","Such quantum-gravity effects can also contribute positively to the alleviation of cosmological tensions if they survive today.","In the talk we discuss the conditions under which such a result may be achieved.","We also discuss the potential role of other axions in string theory, coming from compactification, in inducing enhanced densities of primordial black holes during RVM inflation, thereby contributing to significantly increased percentages of these black holes that can play the role of dark matter components.","Moreover, under certain circumstances, that we shall discuss in some detail, it is also possible that the initially massless torsion-induced axions can acquire a non-trivial mass during the radiation era, thereby providing additional dark matter components in the Universe.","With regards to this aspect, we also emphasise the role of massive right-handed neutrinos, provided that such excitations exist in the relevant spectra."],"url":"http://arxiv.org/abs/2404.18741v1","category":"gr-qc"}
{"created":"2024-04-29 14:42:27","title":"Diffuse scattering from dynamically compressed single-crystal zirconium following the pressure-induced $\u03b1\\to\u03c9$ phase transition","abstract":"The prototypical $\\alpha\\to\\omega$ phase transition in zirconium is an ideal test-bed for our understanding of polymorphism under extreme loading conditions. After half a century of study, a consensus had emerged that the transition is realized via one of two distinct displacive mechanisms, depending on the nature of the compression path. However, recent dynamic-compression experiments equipped with in situ diffraction diagnostics performed in the past few years have revealed new transition mechanisms, demonstrating that our understanding of the underlying atomistic dynamics and transition kinetics is in fact far from complete. We present classical molecular dynamics simulations of the $\\alpha\\to\\omega$ phase transition in single-crystal zirconium shock-compressed along the [0001] axis using a machine-learning-class potential. The transition is predicted to proceed primarily via a modified version of the two-stage Usikov-Zilberstein mechanism, whereby the high-pressure $\\omega$-phase heterogeneously nucleates at boundaries between grains of an intermediate $\\beta$-phase. We further observe the fomentation of atomistic disorder at the junctions between $\\beta$ grains, leading to the formation of highly defective interstitial material between the $\\omega$ grains. We directly compare synthetic x-ray diffraction patterns generated from our simulations with those obtained using femtosecond diffraction in recent dynamic-compression experiments, and show that the simulations produce the same unique, anisotropic diffuse scattering signal unlike any previously seen from an elemental metal. Our simulations suggest that the diffuse signal arises from a combination of thermal diffuse scattering, nanoparticle-like scattering from residual kinetically stabilized $\\alpha$ and $\\beta$ grains, and scattering from interstitial defective structures.","sentences":["The prototypical $\\alpha\\to\\omega$ phase transition in zirconium is an ideal test-bed for our understanding of polymorphism under extreme loading conditions.","After half a century of study, a consensus had emerged that the transition is realized via one of two distinct displacive mechanisms, depending on the nature of the compression path.","However, recent dynamic-compression experiments equipped with in situ diffraction diagnostics performed in the past few years have revealed new transition mechanisms, demonstrating that our understanding of the underlying atomistic dynamics and transition kinetics is in fact far from complete.","We present classical molecular dynamics simulations of the $\\alpha\\to\\omega$ phase transition in single-crystal zirconium shock-compressed along the [0001] axis using a machine-learning-class potential.","The transition is predicted to proceed primarily via a modified version of the two-stage Usikov-Zilberstein mechanism, whereby the high-pressure $\\omega$-phase heterogeneously nucleates at boundaries between grains of an intermediate $\\beta$-phase.","We further observe the fomentation of atomistic disorder at the junctions between $\\beta$ grains, leading to the formation of highly defective interstitial material between the $\\omega$ grains.","We directly compare synthetic x-ray diffraction patterns generated from our simulations with those obtained using femtosecond diffraction in recent dynamic-compression experiments, and show that the simulations produce the same unique, anisotropic diffuse scattering signal unlike any previously seen from an elemental metal.","Our simulations suggest that the diffuse signal arises from a combination of thermal diffuse scattering, nanoparticle-like scattering from residual kinetically stabilized $\\alpha$ and $\\beta$ grains, and scattering from interstitial defective structures."],"url":"http://arxiv.org/abs/2404.18740v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-29 14:36:53","title":"A general framework for active space embedding methods: applications in quantum computing","abstract":"We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding calculations based on an orbital space separation of the fragment and environment degrees of freedom. We show its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion approach are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian. Application of this scheme to study strongly correlated molecular systems and localized electronic states in materials is showcased through the accurate prediction of the optical properties for the neutral oxygen vacancy in magnesium oxide (MgO). Despite some discrepancies in absorption predictions, the method demonstrates competitive performance with state-of-the-art ab initio approaches, particularly evidenced by the accurate prediction of the photoluminescence emission peak.","sentences":["We developed a general framework for hybrid quantum-classical computing of molecular and periodic embedding calculations based on an orbital space separation of the fragment and environment degrees of freedom.","We show its potential by presenting a specific implementation of periodic range-separated DFT coupled to a quantum circuit ansatz, whereby the variational quantum eigensolver and the quantum equation-of-motion approach are used to obtain the low-lying spectrum of the embedded fragment Hamiltonian.","Application of this scheme to study strongly correlated molecular systems and localized electronic states in materials is showcased through the accurate prediction of the optical properties for the neutral oxygen vacancy in magnesium oxide (MgO).","Despite some discrepancies in absorption predictions, the method demonstrates competitive performance with state-of-the-art ab initio approaches, particularly evidenced by the accurate prediction of the photoluminescence emission peak."],"url":"http://arxiv.org/abs/2404.18737v1","category":"physics.chem-ph"}
{"created":"2024-04-29 14:34:43","title":"Mapping the Potential of Explainable Artificial Intelligence (XAI) for Fairness Along the AI Lifecycle","abstract":"The widespread use of artificial intelligence (AI) systems across various domains is increasingly highlighting issues related to algorithmic fairness, especially in high-stakes scenarios. Thus, critical considerations of how fairness in AI systems might be improved, and what measures are available to aid this process, are overdue. Many researchers and policymakers see explainable AI (XAI) as a promising way to increase fairness in AI systems. However, there is a wide variety of XAI methods and fairness conceptions expressing different desiderata, and the precise connections between XAI and fairness remain largely nebulous. Besides, different measures to increase algorithmic fairness might be applicable at different points throughout an AI system's lifecycle. Yet, there currently is no coherent mapping of fairness desiderata along the AI lifecycle. In this paper, we set out to bridge both these gaps: We distill eight fairness desiderata, map them along the AI lifecycle, and discuss how XAI could help address each of them. We hope to provide orientation for practical applications and to inspire XAI research specifically focused on these fairness desiderata.","sentences":["The widespread use of artificial intelligence (AI) systems across various domains is increasingly highlighting issues related to algorithmic fairness, especially in high-stakes scenarios.","Thus, critical considerations of how fairness in AI systems might be improved, and what measures are available to aid this process, are overdue.","Many researchers and policymakers see explainable AI (XAI) as a promising way to increase fairness in AI systems.","However, there is a wide variety of XAI methods and fairness conceptions expressing different desiderata, and the precise connections between XAI and fairness remain largely nebulous.","Besides, different measures to increase algorithmic fairness might be applicable at different points throughout an AI system's lifecycle.","Yet, there currently is no coherent mapping of fairness desiderata along the AI lifecycle.","In this paper, we set out to bridge both these gaps: We distill eight fairness desiderata, map them along the AI lifecycle, and discuss how XAI could help address each of them.","We hope to provide orientation for practical applications and to inspire XAI research specifically focused on these fairness desiderata."],"url":"http://arxiv.org/abs/2404.18736v2","category":"cs.LG"}
{"created":"2024-04-29 14:33:24","title":"Tensor cumulants for statistical inference on invariant distributions","abstract":"Many problems in high-dimensional statistics appear to have a statistical-computational gap: a range of values of the signal-to-noise ratio where inference is information-theoretically possible, but (conjecturally) computationally intractable. A canonical such problem is Tensor PCA, where we observe a tensor $Y$ consisting of a rank-one signal plus Gaussian noise. Multiple lines of work suggest that Tensor PCA becomes computationally hard at a critical value of the signal's magnitude. In particular, below this transition, no low-degree polynomial algorithm can detect the signal with high probability; conversely, various spectral algorithms are known to succeed above this transition. We unify and extend this work by considering tensor networks, orthogonally invariant polynomials where multiple copies of $Y$ are \"contracted\" to produce scalars, vectors, matrices, or other tensors. We define a new set of objects, tensor cumulants, which provide an explicit, near-orthogonal basis for invariant polynomials of a given degree. This basis lets us unify and strengthen previous results on low-degree hardness, giving a combinatorial explanation of the hardness transition and of a continuum of subexponential-time algorithms that work below it, and proving tight lower bounds against low-degree polynomials for recovering rather than just detecting the signal. It also lets us analyze a new problem of distinguishing between different tensor ensembles, such as Wigner and Wishart tensors, establishing a sharp computational threshold and giving evidence of a new statistical-computational gap in the Central Limit Theorem for random tensors. Finally, we believe these cumulants are valuable mathematical objects in their own right: they generalize the free cumulants of free probability theory from matrices to tensors, and share many of their properties, including additivity under additive free convolution.","sentences":["Many problems in high-dimensional statistics appear to have a statistical-computational gap: a range of values of the signal-to-noise ratio where inference is information-theoretically possible, but (conjecturally) computationally intractable.","A canonical such problem is Tensor PCA, where we observe a tensor $Y$ consisting of a rank-one signal plus Gaussian noise.","Multiple lines of work suggest that Tensor PCA becomes computationally hard at a critical value of the signal's magnitude.","In particular, below this transition, no low-degree polynomial algorithm can detect the signal with high probability; conversely, various spectral algorithms are known to succeed above this transition.","We unify and extend this work by considering tensor networks, orthogonally invariant polynomials where multiple copies of $Y$ are \"contracted\" to produce scalars, vectors, matrices, or other tensors.","We define a new set of objects, tensor cumulants, which provide an explicit, near-orthogonal basis for invariant polynomials of a given degree.","This basis lets us unify and strengthen previous results on low-degree hardness, giving a combinatorial explanation of the hardness transition and of a continuum of subexponential-time algorithms that work below it, and proving tight lower bounds against low-degree polynomials for recovering rather than just detecting the signal.","It also lets us analyze a new problem of distinguishing between different tensor ensembles, such as Wigner and Wishart tensors, establishing a sharp computational threshold and giving evidence of a new statistical-computational gap in the Central Limit Theorem for random tensors.","Finally, we believe these cumulants are valuable mathematical objects in their own right: they generalize the free cumulants of free probability theory from matrices to tensors, and share many of their properties, including additivity under additive free convolution."],"url":"http://arxiv.org/abs/2404.18735v1","category":"math.ST"}
{"created":"2024-04-29 14:28:16","title":"Interplay between Contractivity and Monotonicity for Reaction Networks","abstract":"This work studies relationships between monotonicity and contractivity, and applies the results to establish that many reaction networks are weakly contractive, and thus, under appropriate compactness conditions, globally convergent to equilibria. Verification of these properties is achieved through a novel algorithm that can be used to generate cones for monotone systems. The results given here allow a unified proof of global convergence for several classes of networks that had been previously studied in the literature.","sentences":["This work studies relationships between monotonicity and contractivity, and applies the results to establish that many reaction networks are weakly contractive, and thus, under appropriate compactness conditions, globally convergent to equilibria.","Verification of these properties is achieved through a novel algorithm that can be used to generate cones for monotone systems.","The results given here allow a unified proof of global convergence for several classes of networks that had been previously studied in the literature."],"url":"http://arxiv.org/abs/2404.18734v1","category":"math.DS"}
{"created":"2024-04-29 14:25:12","title":"Dynamical friction in the quasi-linear formulation of MOND","abstract":"Aims. We explore the dynamical friction on a test mass in gravitational systems in the Quasi linear formulation of Modified Newtonian Dynamics (QuMOND). Methods. Exploiting the quasi linearity of QuMOND we derive a simple expression for the dynamical friction in akin to its Newtonian counterpart in the standard Chandrasekhar derivation. Moreover, adopting a mean field approach based on the Liouville equation we obtain a more rigorous (though in integral form) dynamical friction formula that can be evaluated numerically for a given choice of the QuMOND interpolation function. Results. Consistently with previous work, we observe that dynamical friction is stronger in MOND with respect to a baryon only Newtonian system with the same mass distribution. This amounts to a correction of the Coulomb logarithmic factor via extra terms proportional to the MOND radius of the system. Moreover, with the aid of simple numerical experiments we confirm our theoretical predictions and those of previous work on MOND.","sentences":["Aims.","We explore the dynamical friction on a test mass in gravitational systems in the Quasi linear formulation of Modified Newtonian Dynamics (QuMOND).","Methods.","Exploiting the quasi linearity of QuMOND we derive a simple expression for the dynamical friction in akin to its Newtonian counterpart in the standard Chandrasekhar derivation.","Moreover, adopting a mean field approach based on the Liouville equation we obtain a more rigorous (though in integral form) dynamical friction formula that can be evaluated numerically for a given choice of the QuMOND interpolation function.","Results.","Consistently with previous work, we observe that dynamical friction is stronger in MOND with respect to a baryon only Newtonian system with the same mass distribution.","This amounts to a correction of the Coulomb logarithmic factor via extra terms proportional to the MOND radius of the system.","Moreover, with the aid of simple numerical experiments we confirm our theoretical predictions and those of previous work on MOND."],"url":"http://arxiv.org/abs/2404.18733v1","category":"astro-ph.GA"}
{"created":"2024-04-29 14:17:52","title":"Real Time Multi Organ Classification on Computed Tomography Images","abstract":"Organ segmentation is a fundamental task in medical imaging, and it is useful for many clinical automation pipelines. Typically, the process involves segmenting the entire volume, which can be unnecessary when the points of interest are limited. In those cases, a classifier could be used instead of segmentation. However, there is an inherent trade-off between the context size and the speed of classifiers. To address this issue, we propose a new method that employs a data selection strategy with sparse sampling across a wide field of view without image resampling. This sparse sampling strategy makes it possible to classify voxels into multiple organs in real time without using accelerators. Although our method is an independent classifier, it can generate full segmentation by querying grid locations at any resolution. We have compared our method with existing segmentation techniques, demonstrating its potential for superior runtime in practical applications in medical imaging.","sentences":["Organ segmentation is a fundamental task in medical imaging, and it is useful for many clinical automation pipelines.","Typically, the process involves segmenting the entire volume, which can be unnecessary when the points of interest are limited.","In those cases, a classifier could be used instead of segmentation.","However, there is an inherent trade-off between the context size and the speed of classifiers.","To address this issue, we propose a new method that employs a data selection strategy with sparse sampling across a wide field of view without image resampling.","This sparse sampling strategy makes it possible to classify voxels into multiple organs in real time without using accelerators.","Although our method is an independent classifier, it can generate full segmentation by querying grid locations at any resolution.","We have compared our method with existing segmentation techniques, demonstrating its potential for superior runtime in practical applications in medical imaging."],"url":"http://arxiv.org/abs/2404.18731v1","category":"cs.CV"}
{"created":"2024-04-29 14:16:16","title":"CVTN: Cross Variable and Temporal Integration for Time Series Forecasting","abstract":"In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN). This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences. Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning. Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance. CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning. This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field.","sentences":["In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies.","To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN).","This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences.","Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning.","Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance.","CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning.","This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field."],"url":"http://arxiv.org/abs/2404.18730v1","category":"cs.LG"}
{"created":"2024-04-29 14:14:56","title":"Polynomials with exponents in compact convex sets and associated weighted extremal functions -- Generalized product property","abstract":"A famous result of Siciak is how the Siciak-Zakharyuta functions, sometimes called global extremal functions or pluricomplex Green functions with a pole at infinity, of two sets relate to the Siciak-Zakharyuta function of their cartesian product. In this paper Siciak's result is generalized to the setting of Siciak-Zakharyuta functions with growth given by a compact convex set, along with discussing why this generalization does not work in the weighted setting.","sentences":["A famous result of Siciak is how the Siciak-Zakharyuta functions, sometimes called global extremal functions or pluricomplex Green functions with a pole at infinity, of two sets relate to the Siciak-Zakharyuta function of their cartesian product.","In this paper Siciak's result is generalized to the setting of Siciak-Zakharyuta functions with growth given by a compact convex set, along with discussing why this generalization does not work in the weighted setting."],"url":"http://arxiv.org/abs/2404.18728v1","category":"math.CV"}
{"created":"2024-04-29 14:12:33","title":"Barrier Algorithms for Constrained Non-Convex Optimization","abstract":"In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization. To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints. Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\\varepsilon^{-2})$ (first-order) and $O(\\varepsilon^{-3/2})$ (second-order), respectively.","sentences":["In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization.","To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints.","Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\\varepsilon^{-2})$ (first-order) and $O(\\varepsilon^{-3/2})$ (second-order), respectively."],"url":"http://arxiv.org/abs/2404.18724v1","category":"math.OC"}
{"created":"2024-04-29 14:06:15","title":"Sausage, kink, and fluting MHD wave modes identified in solar magnetic pores by Solar Orbiter/PHI","abstract":"Solar pores are intense concentrations of magnetic flux that emerge through the Sun's photosphere. When compared to sunspots, they are much smaller in diameter and hence can be impacted and buffeted by neighbouring granular activity to generate significant magnetohydrodynamic (MHD) wave energy flux within their confines. However, observations of solar pores from ground-based telescope facilities may struggle to capture subtle motions synonymous with higher-order MHD wave signatures due to seeing effects produced in the Earth's atmosphere. Hence, we have exploited timely seeing-free and high-quality observations of four small magnetic pores from the Polarimetric and Helioseismic Imager (PHI) on board the Solar Orbiter spacecraft. Through acquisition of data under stable observing conditions, we have been able to measure the area fluctuations and horizontal displacements of the solar pores. Cross correlations between perturbations in intensity, area, line-of-sight velocity, and magnetic fields, coupled with the first-time application of novel Proper Orthogonal Decomposition (POD) techniques on the boundary oscillations, provide a comprehensive diagnosis of the embedded MHD waves as sausage and kink modes. Additionally, the previously elusive m = 2 fluting mode is identified in the most magnetically isolated of the four pores. An important consideration lies in how the identified wave modes contribute towards the transfer of energy into the upper solar atmosphere. We find that the four pores examined have approximately 56%, 72%, 52%, and 34% of their total wave energy associated with the identified sausage modes, and around 23%, 17%, 39%, and 49% to their kink modes, respectively, while the first pore also has around an 11% contribution linked to the fluting mode. This study marks the first-time identification of concurrent sausage, kink, and fluting MHD wave modes in solar magnetic pores.","sentences":["Solar pores are intense concentrations of magnetic flux that emerge through the Sun's photosphere.","When compared to sunspots, they are much smaller in diameter and hence can be impacted and buffeted by neighbouring granular activity to generate significant magnetohydrodynamic (MHD) wave energy flux within their confines.","However, observations of solar pores from ground-based telescope facilities may struggle to capture subtle motions synonymous with higher-order MHD wave signatures due to seeing effects produced in the Earth's atmosphere.","Hence, we have exploited timely seeing-free and high-quality observations of four small magnetic pores from the Polarimetric and Helioseismic Imager (PHI) on board the Solar Orbiter spacecraft.","Through acquisition of data under stable observing conditions, we have been able to measure the area fluctuations and horizontal displacements of the solar pores.","Cross correlations between perturbations in intensity, area, line-of-sight velocity, and magnetic fields, coupled with the first-time application of novel Proper Orthogonal Decomposition (POD) techniques on the boundary oscillations, provide a comprehensive diagnosis of the embedded MHD waves as sausage and kink modes.","Additionally, the previously elusive m = 2 fluting mode is identified in the most magnetically isolated of the four pores.","An important consideration lies in how the identified wave modes contribute towards the transfer of energy into the upper solar atmosphere.","We find that the four pores examined have approximately 56%, 72%, 52%, and 34% of their total wave energy associated with the identified sausage modes, and around 23%, 17%, 39%, and 49% to their kink modes, respectively, while the first pore also has around an 11% contribution linked to the fluting mode.","This study marks the first-time identification of concurrent sausage, kink, and fluting MHD wave modes in solar magnetic pores."],"url":"http://arxiv.org/abs/2404.18717v1","category":"astro-ph.SR"}
{"created":"2024-04-29 14:03:38","title":"Degenerate Higher-Order Maxwell Theories in Flat Space-Time","abstract":"We consider, in Minkowski spacetime, higher-order Maxwell Lagrangians with terms quadratic in the derivatives of the field strength tensor, and study their degrees of freedom. Using a 3+1 decomposition of these Lagrangians, we extract the kinetic matrix for the components of the electric field, corresponding to second time derivatives of the gauge field. If the kinetic matrix is invertible, the theory admits five degrees of freedom, namely the usual two polarisations of a photon plus three extra degrees of freedom which are shown to be Ostrogradski ghosts. We also classify the cases where the kinetic matrix is non-invertible and, using analogous simple models, we argue that, even though the degeneracy conditions reduce the number of degrees of freedom, it does not seem possible to fully eliminate all potential Ostrogradski ghosts.","sentences":["We consider, in Minkowski spacetime, higher-order Maxwell Lagrangians with terms quadratic in the derivatives of the field strength tensor, and study their degrees of freedom.","Using a 3+1 decomposition of these Lagrangians, we extract the kinetic matrix for the components of the electric field, corresponding to second time derivatives of the gauge field.","If the kinetic matrix is invertible, the theory admits five degrees of freedom, namely the usual two polarisations of a photon plus three extra degrees of freedom which are shown to be Ostrogradski ghosts.","We also classify the cases where the kinetic matrix is non-invertible and, using analogous simple models, we argue that, even though the degeneracy conditions reduce the number of degrees of freedom, it does not seem possible to fully eliminate all potential Ostrogradski ghosts."],"url":"http://arxiv.org/abs/2404.18715v1","category":"gr-qc"}
{"created":"2024-04-29 14:02:54","title":"Black hole-neutron star mergers with massive neutron stars in numerical relativity","abstract":"We study the merger of black hole-neutron star (BH-NS) binaries in numerical relativity, focusing on the properties of the remnant disk and the ejecta, varying the mass of compactness of the NS and the mass and spin of the BH. We find that within the precision of our numerical simulations, the remnant disk mass and ejecta mass normalized by the NS baryon mass ($\\hat{M}_{\\rm{rem}}$ and $\\hat{M}_{\\rm{eje}}$, respectively), and the cutoff frequency $f_{\\rm{cut}}$ normalized by the initial total gravitational mass of the system at infinite separation approximately agree among the models with the same NS compactness $C_{\\rm{NS}}=M_{\\rm{NS}}/R_{\\rm{NS}}$, mass ratio $Q=M_{\\rm{BH}}/M_{\\rm{NS}}$, and dimensionless BH spin $\\chi_{\\rm{BH}}$ irrespective of the NS mass $M_{\\rm{NS}}$ in the range of $1.092$--$1.691\\,M_\\odot$. This result shows that the merger outcome depends sensitively on $Q$, $\\chi_{\\rm BH}$, and $C_{\\rm{NS}}$ but only weekly on $M_{\\rm{NS}}$. This justifies the approach of studying the dependence of NS tidal disruptions on the NS compactness by fixing the NS mass but changing the EOS. We further perform simulations with massive NSs of $M_{\\rm{NS}}=1.8M_{\\odot}$, and compare our results of $\\hat{M}_{\\rm{rem}}$ and $\\hat{M}_{\\rm{eje}}$ with those given by existing fitting formulas to test their robustness for more compact NSs. We find that the fitting formulas obtained in the previous studies are accurate within the numerical errors assumed, while our results also suggest that further improvement is possible by systematically performing more precise numerical simulations.","sentences":["We study the merger of black hole-neutron star (BH-NS) binaries in numerical relativity, focusing on the properties of the remnant disk and the ejecta, varying the mass of compactness of the NS and the mass and spin of the BH.","We find that within the precision of our numerical simulations, the remnant disk mass and ejecta mass normalized by the NS baryon mass ($\\hat{M}_{\\rm{rem}}$ and $\\hat{M}_{\\rm{eje}}$, respectively), and the cutoff frequency $f_{\\rm{cut}}$ normalized by the initial total gravitational mass of the system at infinite separation approximately agree among the models with the same NS compactness $C_{\\rm{NS}}=M_{\\rm{NS}}/R_{\\rm{NS}}$, mass ratio $Q=M_{\\rm{BH}}/M_{\\rm{NS}}$, and dimensionless BH spin $\\chi_{\\rm{BH}}$ irrespective of the NS mass $M_{\\rm{NS}}$ in the range of $1.092$--$1.691\\,M_\\odot$. This result shows that the merger outcome depends sensitively on $Q$, $\\chi_{\\rm BH}$, and $C_{\\rm{NS}}$ but only weekly on $M_{\\rm{NS}}$. This justifies the approach of studying the dependence of NS tidal disruptions on the NS compactness by fixing the NS mass but changing the EOS.","We further perform simulations with massive NSs of $M_{\\rm{NS}}=1.8M_{\\odot}$, and compare our results of $\\hat{M}_{\\rm{rem}}$ and $\\hat{M}_{\\rm{eje}}$ with those given by existing fitting formulas to test their robustness for more compact NSs.","We find that the fitting formulas obtained in the previous studies are accurate within the numerical errors assumed, while our results also suggest that further improvement is possible by systematically performing more precise numerical simulations."],"url":"http://arxiv.org/abs/2404.18714v1","category":"astro-ph.HE"}
{"created":"2024-04-29 14:02:02","title":"Adaptive Reinforcement Learning for Robot Control","abstract":"Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at \\url{https://github.com/robot-perception-group/adaptive\\_agent/}.","sentences":["Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes.","To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions.","The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential.","The agent is trained using a custom, highly parallelized simulator built on IsaacGym.","We perform zero-shot transfer to fly the blimp in the real world to solve various tasks.","We share our code at \\url{https://github.com/robot-perception-group/adaptive\\_agent/}."],"url":"http://arxiv.org/abs/2404.18713v1","category":"cs.RO"}
{"created":"2024-04-29 13:56:32","title":"Wireless Information and Energy Transfer in the Era of 6G Communications","abstract":"Wireless information and energy transfer (WIET) represents an emerging paradigm which employs controllable transmission of radio-frequency signals for the dual purpose of data communication and wireless charging. As such, WIET is widely regarded as an enabler of envisioned 6G use cases that rely on energy-sustainable Internet-of-Things (IoT) networks, such as smart cities and smart grids. Meeting the quality-of-service demands of WIET, in terms of both data transfer and power delivery, requires effective co-design of the information and energy signals. In this article, we present the main principles and design aspects of WIET, focusing on its integration in 6G networks. First, we discuss how conventional communication notions such as resource allocation and waveform design need to be revisited in the context of WIET. Next, we consider various candidate 6G technologies that can boost WIET efficiency, namely, holographic multiple-input multiple-output, near-field beamforming, terahertz communication, intelligent reflecting surfaces (IRSs), and reconfigurable (fluid) antenna arrays. We introduce respective WIET design methods, analyze the promising performance gains of these WIET systems, and discuss challenges, open issues, and future research directions. Finally, a near-field energy beamforming scheme and a power-based IRS beamforming algorithm are experimentally validated using a wireless energy transfer testbed. The vision of WIET in communication systems has been gaining momentum in recent years, with constant progress with respect to theoretical but also practical aspects. The comprehensive overview of the state of the art of WIET presented in this paper highlights the potentials of WIET systems as well as their overall benefits in 6G networks.","sentences":["Wireless information and energy transfer (WIET) represents an emerging paradigm which employs controllable transmission of radio-frequency signals for the dual purpose of data communication and wireless charging.","As such, WIET is widely regarded as an enabler of envisioned 6G use cases that rely on energy-sustainable Internet-of-Things (IoT) networks, such as smart cities and smart grids.","Meeting the quality-of-service demands of WIET, in terms of both data transfer and power delivery, requires effective co-design of the information and energy signals.","In this article, we present the main principles and design aspects of WIET, focusing on its integration in 6G networks.","First, we discuss how conventional communication notions such as resource allocation and waveform design need to be revisited in the context of WIET.","Next, we consider various candidate 6G technologies that can boost WIET efficiency, namely, holographic multiple-input multiple-output, near-field beamforming, terahertz communication, intelligent reflecting surfaces (IRSs), and reconfigurable (fluid) antenna arrays.","We introduce respective WIET design methods, analyze the promising performance gains of these WIET systems, and discuss challenges, open issues, and future research directions.","Finally, a near-field energy beamforming scheme and a power-based IRS beamforming algorithm are experimentally validated using a wireless energy transfer testbed.","The vision of WIET in communication systems has been gaining momentum in recent years, with constant progress with respect to theoretical but also practical aspects.","The comprehensive overview of the state of the art of WIET presented in this paper highlights the potentials of WIET systems as well as their overall benefits in 6G networks."],"url":"http://arxiv.org/abs/2404.18705v1","category":"cs.IT"}
{"created":"2024-04-29 13:51:41","title":"Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots","abstract":"The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.","sentences":["The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making.","This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots.","This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain.","As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions.","This framework can produce multiple fooled PD plots via a single model.","By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model.","Managerial insights for regulators and practitioners are provided based on the findings."],"url":"http://arxiv.org/abs/2404.18702v1","category":"cs.LG"}
{"created":"2024-04-29 13:51:32","title":"Exploring the evolution of a dwarf spheroidal galaxy with SPH simulations: I. Stellar feedback","abstract":"A fundamental question regarding the evolution of dwarf spheroidal galaxies is the identification of the key physical mechanisms responsible for gas depletion. Here, we focus on the study of stellar feedback in isolated dwarf spheroidal galaxies, by performing numerical simulations using a modified version of the SPH code GADGET-3. The Milky Way satellite Leo II (PGC 34176) in the Local Group was considered as our default model dwarf galaxy. The parameter space for the stellar feedback models was explored to match observational constraints of Leo II, such as residual gas mass, total mass within the tidal radius, star formation history, final stellar mass, stellar ages and metallicity. Additionally, we examined the impact of the binary fraction of stars, initial mass function, dark matter halo mass and initial gas reservoir. Many simulations revealed recent star formation quenching due to stellar feedback. In general, the gas depletion, expected star formation history, total mass of stars and total mass within the tidal radius were adequately reproduced in the simulations when compared to observational estimates. However, there were discrepancies in the distribution of stellar ages and metallicities, which suggested that the cosmic gas infall would play a more complex role in our dwarf spheroidal galaxy than captured by a monolithic infall scenario. Our results suggest that currently quenched dwarf galaxies may not necessarily need to evolve within clusters or groups, and that stellar feedback alone could be a sufficient factor in shaping at least some of these galaxies as we observe them today.","sentences":["A fundamental question regarding the evolution of dwarf spheroidal galaxies is the identification of the key physical mechanisms responsible for gas depletion.","Here, we focus on the study of stellar feedback in isolated dwarf spheroidal galaxies, by performing numerical simulations using a modified version of the SPH code GADGET-3.","The Milky Way satellite Leo II (PGC 34176) in the Local Group was considered as our default model dwarf galaxy.","The parameter space for the stellar feedback models was explored to match observational constraints of Leo II, such as residual gas mass, total mass within the tidal radius, star formation history, final stellar mass, stellar ages and metallicity.","Additionally, we examined the impact of the binary fraction of stars, initial mass function, dark matter halo mass and initial gas reservoir.","Many simulations revealed recent star formation quenching due to stellar feedback.","In general, the gas depletion, expected star formation history, total mass of stars and total mass within the tidal radius were adequately reproduced in the simulations when compared to observational estimates.","However, there were discrepancies in the distribution of stellar ages and metallicities, which suggested that the cosmic gas infall would play a more complex role in our dwarf spheroidal galaxy than captured by a monolithic infall scenario.","Our results suggest that currently quenched dwarf galaxies may not necessarily need to evolve within clusters or groups, and that stellar feedback alone could be a sufficient factor in shaping at least some of these galaxies as we observe them today."],"url":"http://arxiv.org/abs/2404.18701v1","category":"astro-ph.GA"}
{"created":"2024-04-29 13:47:59","title":"Convergence Properties of Score-Based Models using Graduated Optimisation for Linear Inverse Problems","abstract":"The incorporation of generative models as regularisers within variational formulations for inverse problems has proven effective across numerous image reconstruction tasks. However, the resulting optimisation problem is often non-convex and challenging to solve. In this work, we show that score-based generative models (SGMs) can be used in a graduated optimisation framework to solve inverse problems. We show that the resulting graduated non-convexity flow converge to stationary points of the original problem and provide a numerical convergence analysis of a 2D toy example. We further provide experiments on computed tomography image reconstruction, where we show that this framework is able to recover high-quality images, independent of the initial value. The experiments highlight the potential of using SGMs in graduated optimisation frameworks.","sentences":["The incorporation of generative models as regularisers within variational formulations for inverse problems has proven effective across numerous image reconstruction tasks.","However, the resulting optimisation problem is often non-convex and challenging to solve.","In this work, we show that score-based generative models (SGMs) can be used in a graduated optimisation framework to solve inverse problems.","We show that the resulting graduated non-convexity flow converge to stationary points of the original problem and provide a numerical convergence analysis of a 2D toy example.","We further provide experiments on computed tomography image reconstruction, where we show that this framework is able to recover high-quality images, independent of the initial value.","The experiments highlight the potential of using SGMs in graduated optimisation frameworks."],"url":"http://arxiv.org/abs/2404.18699v1","category":"cs.LG"}
{"created":"2024-04-29 13:44:06","title":"Enhanced second harmonic generation in high-$Q$ all-dielectric metasurfaces with backward coupling","abstract":"Here we employ the quasi-bound state in the continuum (quasi-BIC) resonance in all-dielectric metasurfaces for efficient nonlinear processes in consideration of backward coupling. We theoretically study the second-harmonic generation (SHG) from symmetry-broken AlGaAs metasurfaces and reveal the efficiency enhancement empowered by high-$Q$ quasi-BIC resonances. By introducing the correction term of nonlinear polarization at the fundamental wave field to the conventional undepleted approximation, we uncover the effect of backward coupling on the nonlinear conversation efficiency. The SHG efficiency as $3.94\\times10^{-3}$ with the developed depleted approximation, shows a 9$\\%$ decrease compared with $4.33\\times10^{-3}$ in conventional undepleted approximation, under the incident intensity of 10 GW/cm$^{2}$. Our results are of significant importance for designing efficient nonlinear metasurfaces supporting high-$Q$ resonances.","sentences":["Here we employ the quasi-bound state in the continuum (quasi-BIC) resonance in all-dielectric metasurfaces for efficient nonlinear processes in consideration of backward coupling.","We theoretically study the second-harmonic generation (SHG) from symmetry-broken AlGaAs metasurfaces and reveal the efficiency enhancement empowered by high-$Q$ quasi-BIC resonances.","By introducing the correction term of nonlinear polarization at the fundamental wave field to the conventional undepleted approximation, we uncover the effect of backward coupling on the nonlinear conversation efficiency.","The SHG efficiency as $3.94\\times10^{-3}$ with the developed depleted approximation, shows a 9$\\%$ decrease compared with $4.33\\times10^{-3}$ in conventional undepleted approximation, under the incident intensity of 10 GW/cm$^{2}$. Our results are of significant importance for designing efficient nonlinear metasurfaces supporting high-$Q$ resonances."],"url":"http://arxiv.org/abs/2404.18696v1","category":"physics.optics"}
{"created":"2024-04-29 13:43:49","title":"Dual-Modal Prompting for Sketch-Based Image Retrieval","abstract":"Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their corresponding realistic images. In this study, we aim to tackle two major challenges of this task simultaneously: i) zero-shot, dealing with unseen categories, and ii) fine-grained, referring to intra-category instance-level retrieval. Our key innovation lies in the realization that solely addressing this cross-category and fine-grained recognition task from the generalization perspective may be inadequate since the knowledge accumulated from limited seen categories might not be fully valuable or transferable to unseen target categories. Inspired by this, in this work, we propose a dual-modal prompting CLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed. Specifically, to facilitate the adaptation of our DP-CLIP toward unpredictable target categories, we employ a set of images within the target category and the textual category label to respectively construct a set of category-adaptive prompt tokens and channel scales. By integrating the generated guidance, DP-CLIP could gain valuable category-centric insights, efficiently adapting to novel categories and capturing unique discriminative clues for effective retrieval within each target category. With these designs, our DP-CLIP outperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in Acc.@1 on the Sketchy dataset. Meanwhile, in the other two category-level zero-shot SBIR benchmarks, our method also achieves promising performance.","sentences":["Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their corresponding realistic images.","In this study, we aim to tackle two major challenges of this task simultaneously: i) zero-shot, dealing with unseen categories, and ii) fine-grained, referring to intra-category instance-level retrieval.","Our key innovation lies in the realization that solely addressing this cross-category and fine-grained recognition task from the generalization perspective may be inadequate since the knowledge accumulated from limited seen categories might not be fully valuable or transferable to unseen target categories.","Inspired by this, in this work, we propose a dual-modal prompting CLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed.","Specifically, to facilitate the adaptation of our DP-CLIP toward unpredictable target categories, we employ a set of images within the target category and the textual category label to respectively construct a set of category-adaptive prompt tokens and channel scales.","By integrating the generated guidance, DP-CLIP could gain valuable category-centric insights, efficiently adapting to novel categories and capturing unique discriminative clues for effective retrieval within each target category.","With these designs, our DP-CLIP outperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in Acc.@1 on the Sketchy dataset.","Meanwhile, in the other two category-level zero-shot SBIR benchmarks, our method also achieves promising performance."],"url":"http://arxiv.org/abs/2404.18695v1","category":"cs.CV"}
{"created":"2024-04-29 13:38:44","title":"On the spectrality of a class of Moran Measures","abstract":"In this paper, we study the spectrality of a class of Moran measures $\\mu_{\\mathcal{P},\\mathcal{D}}$ on $\\mathbb{R}$ generated by $\\{(p_n,\\mathcal{D}_n)\\}_{n=1}^{\\infty}$, where $\\mathcal{P}=\\{p_n\\}_{n=1}^{\\infty}$ is a sequence of positive integers with $p_n>1$ and $\\mathcal{D}=\\{\\mathcal{D}_{n}\\}_{n=1}^{\\infty}$ is a sequence of digit sets of $\\mathbb{N}$ with the cardinality $\\#\\mathcal{D}_{n}\\in \\{2,3,N_{n}\\}$. We find a countable set $\\Lambda\\subset\\mathbb{R}$ such that the set $\\{e^{-2\\pi i \\lambda x}|\\lambda\\in\\Lambda\\}$ is a orthonormal basis of $L^{2}(\\mu_{\\mathcal{P},\\mathcal{D}})$ under some conditions. As an application, we show that when $\\mu_{\\mathcal{P},\\mathcal{D}}$ is absolutely continuous, $\\mu_{\\mathcal{P},\\mathcal{D}}$ not only is a spectral measure, but also its support set tiles $\\mathbb{R}$ with $\\mathbb{Z}$.","sentences":["In this paper, we study the spectrality of a class of Moran measures $\\mu_{\\mathcal{P},\\mathcal{D}}$ on $\\mathbb{R}$ generated by $\\{(p_n,\\mathcal{D}_n)\\}_{n=1}^{\\infty}$, where $\\mathcal{P}=\\{p_n\\}_{n=1}^{\\infty}$ is a sequence of positive integers with $p_n>1$ and $\\mathcal{D}=\\{\\mathcal{D}_{n}\\}_{n=1}^{\\infty}$ is a sequence of digit sets of $\\mathbb{N}$ with the cardinality $\\#\\mathcal{D}_{n}\\in \\{2,3,N_{n}\\}$. We find a countable set $\\Lambda\\subset\\mathbb{R}$ such that the set $\\{e^{-2\\pi i \\lambda x}|\\lambda\\in\\Lambda\\}$ is a orthonormal basis of $L^{2}(\\mu_{\\mathcal{P},\\mathcal{D}})$ under some conditions.","As an application, we show that when $\\mu_{\\mathcal{P},\\mathcal{D}}$ is absolutely continuous, $\\mu_{\\mathcal{P},\\mathcal{D}}$ not only is a spectral measure, but also its support set tiles $\\mathbb{R}$ with $\\mathbb{Z}$."],"url":"http://arxiv.org/abs/2404.18690v1","category":"math.CA"}
{"created":"2024-04-29 13:36:09","title":"Distributed Source Coding for Parametric and Non-Parametric Regression","abstract":"The design of communication systems dedicated to machine learning tasks is one key aspect of goal-oriented communications. In this framework, this article investigates the interplay between data reconstruction and learning from the same compressed observations, particularly focusing on the regression problem. We establish achievable rate-generalization error regions for both parametric and non-parametric regression, where the generalization error measures the regression performance on previously unseen data. The analysis covers both asymptotic and finite block-length regimes, providing fundamental results and practical insights for the design of coding schemes dedicated to regression. The asymptotic analysis relies on conventional Wyner-Ziv coding schemes which we extend to study the convergence of the generalization error. The finite-length analysis uses the notions of information density and dispersion with additional term for the generalization error. We further investigate the trade-off between reconstruction and regression in both asymptotic and non-asymptotic regimes. Contrary to the existing literature which focused on other learning tasks, our results state that in the case of regression, there is no trade-off between data reconstruction and regression in the asymptotic regime. We also observe the same absence of trade-off for the considered achievable scheme in the finite-length regime, by analyzing correlation between distortion and generalization error.","sentences":["The design of communication systems dedicated to machine learning tasks is one key aspect of goal-oriented communications.","In this framework, this article investigates the interplay between data reconstruction and learning from the same compressed observations, particularly focusing on the regression problem.","We establish achievable rate-generalization error regions for both parametric and non-parametric regression, where the generalization error measures the regression performance on previously unseen data.","The analysis covers both asymptotic and finite block-length regimes, providing fundamental results and practical insights for the design of coding schemes dedicated to regression.","The asymptotic analysis relies on conventional Wyner-Ziv coding schemes which we extend to study the convergence of the generalization error.","The finite-length analysis uses the notions of information density and dispersion with additional term for the generalization error.","We further investigate the trade-off between reconstruction and regression in both asymptotic and non-asymptotic regimes.","Contrary to the existing literature which focused on other learning tasks, our results state that in the case of regression, there is no trade-off between data reconstruction and regression in the asymptotic regime.","We also observe the same absence of trade-off for the considered achievable scheme in the finite-length regime, by analyzing correlation between distortion and generalization error."],"url":"http://arxiv.org/abs/2404.18688v1","category":"cs.IT"}
{"created":"2024-04-29 13:34:19","title":"Socially Adaptive Path Planning Based on Generative Adversarial Network","abstract":"The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians. Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments. However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated. In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm. Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios. Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments. Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths. In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path. In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments. Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths.","sentences":["The natural interaction between robots and pedestrians in the process of autonomous navigation is crucial for the intelligent development of mobile robots, which requires robots to fully consider social rules and guarantee the psychological comfort of pedestrians.","Among the research results in the field of robotic path planning, the learning-based socially adaptive algorithms have performed well in some specific human-robot interaction environments.","However, human-robot interaction scenarios are diverse and constantly changing in daily life, and the generalization of robot socially adaptive path planning remains to be further investigated.","In order to address this issue, this work proposes a new socially adaptive path planning algorithm by combining the generative adversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*) navigation algorithm.","Firstly, a GAN model with strong generalization performance is proposed to adapt the navigation algorithm to more scenarios.","Secondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation algorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction environments.","Finally, we propose a socially adaptive path planning framework named GAN-RTIRL, which combines the GAN model with Rapidly-exploring random Trees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate between planned and demonstration paths.","In the GAN-RTIRL framework, the GAN-RRT* path planner can update the GAN model from the demonstration path.","In this way, the robot can generate more anthropomorphic paths in human-robot interaction environments and has stronger generalization in more complex environments.","Experimental results reveal that our proposed method can effectively improve the anthropomorphic degree of robot motion planning and the homotopy rate between planned and demonstration paths."],"url":"http://arxiv.org/abs/2404.18687v1","category":"cs.RO"}
{"created":"2024-04-29 13:33:35","title":"Dynamic temperature compensation for wavelength-stable entangled biphoton generation","abstract":"A dynamic temperature compensation method is presented to stabilize the wavelength of the entangled biphoton source, which is generated via the spontaneous parametric down-conversion based on a MgO: PPLN waveguide. Utilizing the dispersive Fourier transformation technique combined with a digital proportional-integral-differential algorithm, the small amount of wavelength variation can be instantly identified and then compensated with active temperature correction. The long-term wavelength stability, assessed though Allan deviation, shows nearly a hundredfold enhancement, reaching 2.00*10^(-7) at the averaging time of 10000 s. It offers a simple, ready-to-use solution for precise wavelength control in quantum information processing.","sentences":["A dynamic temperature compensation method is presented to stabilize the wavelength of the entangled biphoton source, which is generated via the spontaneous parametric down-conversion based on a MgO: PPLN waveguide.","Utilizing the dispersive Fourier transformation technique combined with a digital proportional-integral-differential algorithm, the small amount of wavelength variation can be instantly identified and then compensated with active temperature correction.","The long-term wavelength stability, assessed though Allan deviation, shows nearly a hundredfold enhancement, reaching 2.00*10^(-7) at the averaging time of 10000 s.","It offers a simple, ready-to-use solution for precise wavelength control in quantum information processing."],"url":"http://arxiv.org/abs/2404.18686v1","category":"quant-ph"}
{"created":"2024-04-29 13:29:05","title":"New tool for extraction of $^{187}$Os M\u00f6ssbauer parameters with biologically relevant detection sensitivity","abstract":"A large number of osmium complexes with osmium in different oxidation states (II, III, IV, VI) have been reported recently to exhibit good antiproliferative activity in cancer cell lines. Herein, we demonstrate new opportunities offered by $^{187}$Os nuclear forward scattering (NFS) and nuclear inelastic scattering (NIS) of synchrotron radiation for characterization of hyperfine interactions and lattice dynamics in a benchmark Os(VI) complex K$_2$[OsO$_2$(OH)$_4$], by accurate extraction of M\\\"ossbauer parameters and the determination of Os-projected density of phonon states confirmed by first-principles phonon calculations. The values of isomer shift ($\\delta$ = 3.3(1) mm/s) relative to [Os$^{IV}$ Cl$_{6}$]$^{2-}$ and quadrupole splitting ($\\Delta E_Q$ = 12.0(2) mm/s) were determined with NFS, while the Lamb-M\\\"ossbauer factor (0.55(1)), the density of phonon states (DOS), and a full thermodynamics characterization was carried out using the NIS data combined with first principle theoretical calculations. In more general terms, this study provides strong evidence that $^{187}$Os nuclear resonance scattering is a reliable technique for the investigation of hyperfine interactions and Os specific vibrations in osmium(VI) species, which might be potentially applicable for measuring such interactions in osmium complexes of other oxidation states, including those with anticancer activity such as Os(III) and Os(IV).","sentences":["A large number of osmium complexes with osmium in different oxidation states (II, III, IV, VI) have been reported recently to exhibit good antiproliferative activity in cancer cell lines.","Herein, we demonstrate new opportunities offered by $^{187}$Os nuclear forward scattering (NFS) and nuclear inelastic scattering (NIS) of synchrotron radiation for characterization of hyperfine interactions and lattice dynamics in a benchmark Os(VI) complex K$_2$[OsO$_2$(OH)$_4$], by accurate extraction of M\\\"ossbauer parameters and the determination of Os-projected density of phonon states confirmed by first-principles phonon calculations.","The values of isomer shift ($\\delta$ = 3.3(1) mm/s) relative to [Os$^{IV}$ Cl$_{6}$]$^{2-}$ and quadrupole splitting ($\\Delta E_Q$ = 12.0(2) mm/s) were determined with NFS, while the Lamb-M\\\"ossbauer factor (0.55(1)), the density of phonon states (DOS), and a full thermodynamics characterization was carried out using the NIS data combined with first principle theoretical calculations.","In more general terms, this study provides strong evidence that $^{187}$Os nuclear resonance scattering is a reliable technique for the investigation of hyperfine interactions and Os specific vibrations in osmium(VI) species, which might be potentially applicable for measuring such interactions in osmium complexes of other oxidation states, including those with anticancer activity such as Os(III) and Os(IV)."],"url":"http://arxiv.org/abs/2404.18683v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-29 13:24:23","title":"LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs","abstract":"Machine learning's influence is expanding rapidly, now integral to decision-making processes from corporate strategy to the advancements in Industry 4.0. The efficacy of Artificial Intelligence broadly hinges on the caliber of data used during its training phase; optimal performance is tied to exceptional data quality. Data cleaning tools, particularly those that exploit functional dependencies within ontological frameworks or context models, are instrumental in augmenting data quality. Nevertheless, crafting these context models is a demanding task, both in terms of resources and expertise, often necessitating specialized knowledge from domain experts.   In light of these challenges, this paper introduces an innovative approach, called LLMClean, for the automated generation of context models, utilizing Large Language Models to analyze and understand various datasets. LLMClean encompasses a sequence of actions, starting with categorizing the dataset, extracting or mapping relevant models, and ultimately synthesizing the context model. To demonstrate its potential, we have developed and tested a prototype that applies our approach to three distinct datasets from the Internet of Things, healthcare, and Industry 4.0 sectors. The results of our evaluation indicate that our automated approach can achieve data cleaning efficacy comparable with that of context models crafted by human experts.","sentences":["Machine learning's influence is expanding rapidly, now integral to decision-making processes from corporate strategy to the advancements in Industry 4.0.","The efficacy of Artificial Intelligence broadly hinges on the caliber of data used during its training phase; optimal performance is tied to exceptional data quality.","Data cleaning tools, particularly those that exploit functional dependencies within ontological frameworks or context models, are instrumental in augmenting data quality.","Nevertheless, crafting these context models is a demanding task, both in terms of resources and expertise, often necessitating specialized knowledge from domain experts.   ","In light of these challenges, this paper introduces an innovative approach, called LLMClean, for the automated generation of context models, utilizing Large Language Models to analyze and understand various datasets.","LLMClean encompasses a sequence of actions, starting with categorizing the dataset, extracting or mapping relevant models, and ultimately synthesizing the context model.","To demonstrate its potential, we have developed and tested a prototype that applies our approach to three distinct datasets from the Internet of Things, healthcare, and Industry 4.0 sectors.","The results of our evaluation indicate that our automated approach can achieve data cleaning efficacy comparable with that of context models crafted by human experts."],"url":"http://arxiv.org/abs/2404.18681v1","category":"cs.DB"}
{"created":"2024-04-29 13:22:44","title":"Teleparallel Jackiw-Teitelboim gravity","abstract":"We introduce a new class of two dimensional gravity models using ideas motivated by the Teleparallel Equivalent of General Relativity. This leads to a rather natural formulation of a theory that has close links with Jackiw-Teitelboim gravity. After introducing the theory and discussing its vacuum solutions, we present the Hamiltonian analysis. This implies the presence of a single dynamical degree of freedom, which is in sharp contrast to General Relativity, where there are no degrees of freedom in two spacetime dimensions. Our approach can be extended to various other lower-dimensional gravity theories and thus could be of wider interest.","sentences":["We introduce a new class of two dimensional gravity models using ideas motivated by the Teleparallel Equivalent of General Relativity.","This leads to a rather natural formulation of a theory that has close links with Jackiw-Teitelboim gravity.","After introducing the theory and discussing its vacuum solutions, we present the Hamiltonian analysis.","This implies the presence of a single dynamical degree of freedom, which is in sharp contrast to General Relativity, where there are no degrees of freedom in two spacetime dimensions.","Our approach can be extended to various other lower-dimensional gravity theories and thus could be of wider interest."],"url":"http://arxiv.org/abs/2404.18679v1","category":"gr-qc"}
{"created":"2024-04-29 13:19:24","title":"Towards the First Code Contribution: Processes and Information Needs","abstract":"Newcomers to a software project must overcome many barriers before they can successfully place their first code contribution, and they often struggle to find information that is relevant to them. In this work, we argue that much of the information needed by newcomers already exists, albeit scattered among many different sources, and that many barriers can be addressed by automatically identifying, extracting, generating, summarizing, and presenting documentation that is specifically aimed and customized for newcomers. To gain a detailed understanding of the processes followed by newcomers and their information needs before making their first code contribution, we conducted an empirical study. Based on a survey with about 100 practitioners, grounded theory analysis, and validation interviews, we contribute a 16-step model for the processes followed by newcomers to a software project and we identify relevant information, along with individual and project characteristics that influence the relevancy of information types and sources. Our findings form an essential step towards automated tool support that provides relevant information to project newcomers in each step of their contribution processes.","sentences":["Newcomers to a software project must overcome many barriers before they can successfully place their first code contribution, and they often struggle to find information that is relevant to them.","In this work, we argue that much of the information needed by newcomers already exists, albeit scattered among many different sources, and that many barriers can be addressed by automatically identifying, extracting, generating, summarizing, and presenting documentation that is specifically aimed and customized for newcomers.","To gain a detailed understanding of the processes followed by newcomers and their information needs before making their first code contribution, we conducted an empirical study.","Based on a survey with about 100 practitioners, grounded theory analysis, and validation interviews, we contribute a 16-step model for the processes followed by newcomers to a software project and we identify relevant information, along with individual and project characteristics that influence the relevancy of information types and sources.","Our findings form an essential step towards automated tool support that provides relevant information to project newcomers in each step of their contribution processes."],"url":"http://arxiv.org/abs/2404.18677v1","category":"cs.SE"}
{"created":"2024-04-29 13:13:11","title":"Dynamical ejecta from binary neutron star mergers: Impact of residual eccentricity and equation of state implementation","abstract":"Predicting the properties of the matter ejected during and after a neutron star merger is crucial to our ability to use electromagnetic observations of these mergers to constrain the masses of the neutron stars, the equation of state of dense matter, and the role of neutron star mergers in the enrichment of the Universe in heavy elements. Our ability to reliably provide such predictions is however limited by a broad range of factors, including the finite resolution of numerical simulations, their treatment of magnetic fields, neutrinos, and neutrino-matter interactions, and the approximate modeling of the equation of state of dense matter. In this manuscript, we study specifically the role that a small residual eccentricity and different implementations of the same equation of state have on the matter ejected during the merger of a $1.3M_\\odot-1.4M_\\odot$ binary neutron star system. We find that a residual eccentricity $e\\sim 0.01$, as measured $\\sim 4-6$ orbits before merger, causes $O(25\\%-30\\%)$ changes in the amount of ejected mass, mainly due to changes in the amount of matter ejected as a result of core bounces during merger. We note that $O(1\\%)$ residual eccentricities have regularly been used in binary neutron star merger simulations as proxy for circular binaries, potentially creating an additional source of error in predictions for the mass of the dynamical ejecta.","sentences":["Predicting the properties of the matter ejected during and after a neutron star merger is crucial to our ability to use electromagnetic observations of these mergers to constrain the masses of the neutron stars, the equation of state of dense matter, and the role of neutron star mergers in the enrichment of the Universe in heavy elements.","Our ability to reliably provide such predictions is however limited by a broad range of factors, including the finite resolution of numerical simulations, their treatment of magnetic fields, neutrinos, and neutrino-matter interactions, and the approximate modeling of the equation of state of dense matter.","In this manuscript, we study specifically the role that a small residual eccentricity and different implementations of the same equation of state have on the matter ejected during the merger of a $1.3M_\\odot-1.4M_\\odot$ binary neutron star system.","We find that a residual eccentricity $e\\sim 0.01$, as measured $\\sim 4-6$ orbits before merger, causes $O(25\\%-30\\%)$ changes in the amount of ejected mass, mainly due to changes in the amount of matter ejected as a result of core bounces during merger.","We note that $O(1\\%)$ residual eccentricities have regularly been used in binary neutron star merger simulations as proxy for circular binaries, potentially creating an additional source of error in predictions for the mass of the dynamical ejecta."],"url":"http://arxiv.org/abs/2404.18674v1","category":"astro-ph.HE"}
{"created":"2024-04-29 13:13:10","title":"Open-Source Drift Detection Tools in Action: Insights from Two Use Cases","abstract":"Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability. In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools. D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.We prioritize assessing the functional suitability of these tools to identify and analyze data drifts. Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands. Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy.","sentences":["Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability.","In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools.","D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.","We prioritize assessing the functional suitability of these tools to identify and analyze data drifts.","Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands.","Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy."],"url":"http://arxiv.org/abs/2404.18673v1","category":"cs.DB"}
{"created":"2024-04-29 13:12:08","title":"Graph Convolutional Networks and Graph Attention Networks for Approximating Arguments Acceptability -- Technical Report","abstract":"Various approaches have been proposed for providing efficient computational approaches for abstract argumentation. Among them, neural networks have permitted to solve various decision problems, notably related to arguments (credulous or skeptical) acceptability. In this work, we push further this study in various ways. First, relying on the state-of-the-art approach AFGCN, we show how we can improve the performances of the Graph Convolutional Networks (GCNs) regarding both runtime and accuracy. Then, we show that it is possible to improve even more the efficiency of the approach by modifying the architecture of the network, using Graph Attention Networks (GATs) instead.","sentences":["Various approaches have been proposed for providing efficient computational approaches for abstract argumentation.","Among them, neural networks have permitted to solve various decision problems, notably related to arguments (credulous or skeptical) acceptability.","In this work, we push further this study in various ways.","First, relying on the state-of-the-art approach AFGCN, we show how we can improve the performances of the Graph Convolutional Networks (GCNs) regarding both runtime and accuracy.","Then, we show that it is possible to improve even more the efficiency of the approach by modifying the architecture of the network, using Graph Attention Networks (GATs) instead."],"url":"http://arxiv.org/abs/2404.18672v1","category":"cs.AI"}
{"created":"2024-04-29 13:11:20","title":"Uncertainty relation and the constrained quadratic programming","abstract":"The uncertainty relation is a fundamental concept in quantum theory, plays a pivotal role in various quantum information processing tasks. In this study, we explore the additive uncertainty relation pertaining to two or more observables, in terms of their variance,by utilizing the generalized Gell-Mann representation in qudit systems. We find that the tight state-independent lower bound of the variance sum can be characterized as a quadratic programming problem with nonlinear constraints in optimization theory. As illustrative examples, we derive analytical solutions for these quadratic programming problems in lower-dimensional systems, which align with the state-independent lower bounds. Additionally, we introduce a numerical algorithm tailored for solving these quadratic programming instances, highlighting its efficiency and accuracy. The advantage of our approach lies in its potential ability to simultaneously achieve the optimal value of the quadratic programming problem with nonlinear constraints but also precisely identify the extremal state where this optimal value is attained. This enables us to establish a tight state-independent lower bound for the sum of variances, and further identify the extremal state at which this lower bound is realized.","sentences":["The uncertainty relation is a fundamental concept in quantum theory, plays a pivotal role in various quantum information processing tasks.","In this study, we explore the additive uncertainty relation pertaining to two or more observables, in terms of their variance,by utilizing the generalized Gell-Mann representation in qudit systems.","We find that the tight state-independent lower bound of the variance sum can be characterized as a quadratic programming problem with nonlinear constraints in optimization theory.","As illustrative examples, we derive analytical solutions for these quadratic programming problems in lower-dimensional systems, which align with the state-independent lower bounds.","Additionally, we introduce a numerical algorithm tailored for solving these quadratic programming instances, highlighting its efficiency and accuracy.","The advantage of our approach lies in its potential ability to simultaneously achieve the optimal value of the quadratic programming problem with nonlinear constraints but also precisely identify the extremal state where this optimal value is attained.","This enables us to establish a tight state-independent lower bound for the sum of variances, and further identify the extremal state at which this lower bound is realized."],"url":"http://arxiv.org/abs/2404.18671v1","category":"quant-ph"}
{"created":"2024-04-29 12:57:05","title":"Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting","abstract":"Recent developments in neural rendering techniques have greatly enhanced the rendering of photo-realistic 3D scenes across both academic and commercial fields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new benchmarks for rendering quality and speed. Nevertheless, the limitations of 3D-GS become pronounced in synthesizing new viewpoints, especially for views that greatly deviate from those seen during training. Additionally, issues such as dilation and aliasing arise when zooming in or out. These challenges can all be traced back to a single underlying issue: insufficient sampling. In our paper, we present a bootstrapping method that significantly addresses this problem. This approach employs a diffusion model to enhance the rendering of novel views using trained 3D-GS, thereby streamlining the training process. Our results indicate that bootstrapping effectively reduces artifacts, as well as clear enhancements on the evaluation metrics. Furthermore, we show that our method is versatile and can be easily integrated, allowing various 3D reconstruction projects to benefit from our approach.","sentences":["Recent developments in neural rendering techniques have greatly enhanced the rendering of photo-realistic 3D scenes across both academic and commercial fields.","The latest method, known as 3D Gaussian Splatting (3D-GS), has set new benchmarks for rendering quality and speed.","Nevertheless, the limitations of 3D-GS become pronounced in synthesizing new viewpoints, especially for views that greatly deviate from those seen during training.","Additionally, issues such as dilation and aliasing arise when zooming in or out.","These challenges can all be traced back to a single underlying issue: insufficient sampling.","In our paper, we present a bootstrapping method that significantly addresses this problem.","This approach employs a diffusion model to enhance the rendering of novel views using trained 3D-GS, thereby streamlining the training process.","Our results indicate that bootstrapping effectively reduces artifacts, as well as clear enhancements on the evaluation metrics.","Furthermore, we show that our method is versatile and can be easily integrated, allowing various 3D reconstruction projects to benefit from our approach."],"url":"http://arxiv.org/abs/2404.18669v1","category":"cs.GR"}
{"created":"2024-04-29 12:54:33","title":"Szeg\u0151 Recurrence for Multiple Orthogonal Polynomials on the Unit Circle","abstract":"We investigate polynomials that satisfy simultaneous orthogonality conditions with respect to several measures on the unit circle. We generalize the direct and inverse Szeg\\H{o} recurrence relations, identify the analogues of the Verblunsky coefficients, and prove the Christoffel$\\unicode{x2013}$Darboux formula. These results stand directly in analogue with the nearest neighbour recurrence relations from the real line counterpart.","sentences":["We investigate polynomials that satisfy simultaneous orthogonality conditions with respect to several measures on the unit circle.","We generalize the direct and inverse Szeg\\H{o} recurrence relations, identify the analogues of the Verblunsky coefficients, and prove the Christoffel$\\unicode{x2013}$Darboux formula.","These results stand directly in analogue with the nearest neighbour recurrence relations from the real line counterpart."],"url":"http://arxiv.org/abs/2404.18666v1","category":"math.CA"}
{"created":"2024-04-29 12:49:53","title":"Leveraging PointNet and PointNet++ for Lyft Point Cloud Classification Challenge","abstract":"This study investigates the application of PointNet and PointNet++ in the classification of LiDAR-generated point cloud data, a critical component for achieving fully autonomous vehicles. Utilizing a modified dataset from the Lyft 3D Object Detection Challenge, we examine the models' capabilities to handle dynamic and complex environments essential for autonomous navigation. Our analysis shows that PointNet and PointNet++ achieved accuracy rates of 79.53% and 84.24%, respectively. These results underscore the models' robustness in interpreting intricate environmental data, which is pivotal for the safety and efficiency of autonomous vehicles. Moreover, the enhanced detection accuracy, particularly in distinguishing pedestrians from other objects, highlights the potential of these models to contribute substantially to the advancement of autonomous vehicle technology.","sentences":["This study investigates the application of PointNet and PointNet++ in the classification of LiDAR-generated point cloud data, a critical component for achieving fully autonomous vehicles.","Utilizing a modified dataset from the Lyft 3D Object Detection Challenge, we examine the models' capabilities to handle dynamic and complex environments essential for autonomous navigation.","Our analysis shows that PointNet and PointNet++ achieved accuracy rates of 79.53% and 84.24%, respectively.","These results underscore the models' robustness in interpreting intricate environmental data, which is pivotal for the safety and efficiency of autonomous vehicles.","Moreover, the enhanced detection accuracy, particularly in distinguishing pedestrians from other objects, highlights the potential of these models to contribute substantially to the advancement of autonomous vehicle technology."],"url":"http://arxiv.org/abs/2404.18665v1","category":"cs.CV"}
{"created":"2024-04-29 12:46:45","title":"On the determination of path signature from its unitary development","abstract":"We establish an explicit, constructive approach to determine any element $X$ in the tensor algebra $\\mathcal{T}\\left(\\mathbb{R}^d\\right) = \\bigoplus_{n=0}^\\infty\\left(\\mathbb{R}^d\\right)^{\\otimes n}$ from its moment generating function. The only assumption is that $X$ has a nonzero radius of convergence, which relaxes the condition of having an infinite radius of convergence in the literature. The key building block of our approach is tridiagonal antisymmetric matrices, whose sparsity offers a considerable advantage for dimension reduction in applications. In particular, specialising $X$ to the signature space of bounded $p$-variation paths in $\\mathbb{R}^d$ with $1\\leq p <2$, we show that the developments of such sparse matrices are sufficient to separate points over the space of signatures, which yields a refined answer to the \"moment problem\" concerning the signature. Based on the above theoretical investigations, we propose a new distance function for probability measures on the path space, termed as the \"Restricted Path Characteristic Function Distance\" (RPCFD), and validate its effectiveness via numerical experiments on hypothesis testing for examples of fractional Brownian motions.","sentences":["We establish an explicit, constructive approach to determine any element $X$ in the tensor algebra $\\mathcal{T}\\left(\\mathbb{R}^d\\right) = \\bigoplus_{n=0}^\\infty\\left(\\mathbb{R}^d\\right)^{\\otimes n}$ from its moment generating function.","The only assumption is that $X$ has a nonzero radius of convergence, which relaxes the condition of having an infinite radius of convergence in the literature.","The key building block of our approach is tridiagonal antisymmetric matrices, whose sparsity offers a considerable advantage for dimension reduction in applications.","In particular, specialising $X$ to the signature space of bounded $p$-variation paths in $\\mathbb{R}^d$ with $1\\leq p <2$, we show that the developments of such sparse matrices are sufficient to separate points over the space of signatures, which yields a refined answer to the \"moment problem\" concerning the signature.","Based on the above theoretical investigations, we propose a new distance function for probability measures on the path space, termed as the \"Restricted Path Characteristic Function Distance\" (RPCFD), and validate its effectiveness via numerical experiments on hypothesis testing for examples of fractional Brownian motions."],"url":"http://arxiv.org/abs/2404.18661v1","category":"math.PR"}
{"created":"2024-04-29 12:40:07","title":"On the Evaluation of Procedural Level Generation Systems","abstract":"The evaluation of procedural content generation (PCG) systems for generating video game levels is a complex and contested topic. Ideally, the field would have access to robust, generalisable and widely accepted evaluation approaches that can be used to compare novel PCG systems to prior work, but consensus on how to evaluate novel systems is currently limited. We argue that the field can benefit from a structured analysis of how procedural level generation systems can be evaluated, and how these techniques are currently used by researchers. This analysis can then be used to both inform on the current state of affairs, and to provide data to justify changes to this practice. This work aims to provide this by first developing a novel taxonomy of PCG evaluation approaches, and then presenting the results of a survey of recent work in the field through the lens of this taxonomy. The results of this survey highlight several important weaknesses in current practice which we argue could be substantially mitigated by 1) promoting use of evaluation free system descriptions where appropriate, 2) promoting the development of diverse research frameworks, 3) promoting reuse of code and methodology wherever possible.","sentences":["The evaluation of procedural content generation (PCG) systems for generating video game levels is a complex and contested topic.","Ideally, the field would have access to robust, generalisable and widely accepted evaluation approaches that can be used to compare novel PCG systems to prior work, but consensus on how to evaluate novel systems is currently limited.","We argue that the field can benefit from a structured analysis of how procedural level generation systems can be evaluated, and how these techniques are currently used by researchers.","This analysis can then be used to both inform on the current state of affairs, and to provide data to justify changes to this practice.","This work aims to provide this by first developing a novel taxonomy of PCG evaluation approaches, and then presenting the results of a survey of recent work in the field through the lens of this taxonomy.","The results of this survey highlight several important weaknesses in current practice which we argue could be substantially mitigated by 1) promoting use of evaluation free system descriptions where appropriate, 2) promoting the development of diverse research frameworks, 3) promoting reuse of code and methodology wherever possible."],"url":"http://arxiv.org/abs/2404.18657v1","category":"cs.HC"}
{"created":"2024-04-29 12:38:26","title":"Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods","abstract":"Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM's parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge.","sentences":["Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights.","The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining.","This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components.","Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically.","Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA.","To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA.","We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods.","Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA.","Nevertheless, IA provides unique and valuable insights into the LM's parametric knowledge, which are not revealed by NA.","Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge."],"url":"http://arxiv.org/abs/2404.18655v1","category":"cs.CL"}
{"created":"2024-04-29 12:32:35","title":"A brief introduction to non-regular spacetime geometry","abstract":"We give a brief non-technical introduction to non-regular spacetime geometry. In particular, we discuss how curvature, and hence gravity, can be defined without a smooth (differential geometric) calculus.","sentences":["We give a brief non-technical introduction to non-regular spacetime geometry.","In particular, we discuss how curvature, and hence gravity, can be defined without a smooth (differential geometric) calculus."],"url":"http://arxiv.org/abs/2404.18651v1","category":"gr-qc"}
{"created":"2024-04-29 12:32:35","title":"Energy Efficiency Optimization of Multi-unit System with Different Devices","abstract":"The energy efficiency optimization of the power generation system and the energy efficiency optimization of the energy consumption system are unified into the same optimization problem, and a simple method to achieve energy efficiency optimization without establishing an accurate mathematical model of the system is proposed. For systems with similar energy efficiency, it is proved that the best load distribution method between equipment is to keep the operating energy efficiency of each operating device equal, Yao's theorem 1. It is proved that the optimal switching method for the number of operating units between equipment with different energy efficiency is to keep the energy efficiency of the switching point equal, or at the maximum load point of the equipment, Yao's Theorem 2. This article gives two cases, a system composed of equipment with similar efficiency and a system composed of equipment with different efficiency.","sentences":["The energy efficiency optimization of the power generation system and the energy efficiency optimization of the energy consumption system are unified into the same optimization problem, and a simple method to achieve energy efficiency optimization without establishing an accurate mathematical model of the system is proposed.","For systems with similar energy efficiency, it is proved that the best load distribution method between equipment is to keep the operating energy efficiency of each operating device equal, Yao's theorem 1.","It is proved that the optimal switching method for the number of operating units between equipment with different energy efficiency is to keep the energy efficiency of the switching point equal, or at the maximum load point of the equipment, Yao's Theorem 2.","This article gives two cases, a system composed of equipment with similar efficiency and a system composed of equipment with different efficiency."],"url":"http://arxiv.org/abs/2404.18652v1","category":"math.OC"}
{"created":"2024-04-29 12:32:14","title":"Towards Quantitative Evaluation of Explainable AI Methods for Deepfake Detection","abstract":"In this paper we propose a new framework for evaluating the performance of explanation methods on the decisions of a deepfake detector. This framework assesses the ability of an explanation method to spot the regions of a fake image with the biggest influence on the decision of the deepfake detector, by examining the extent to which these regions can be modified through a set of adversarial attacks, in order to flip the detector's prediction or reduce its initial prediction; we anticipate a larger drop in deepfake detection accuracy and prediction, for methods that spot these regions more accurately. Based on this framework, we conduct a comparative study using a state-of-the-art model for deepfake detection that has been trained on the FaceForensics++ dataset, and five explanation methods from the literature. The findings of our quantitative and qualitative evaluations document the advanced performance of the LIME explanation method against the other compared ones, and indicate this method as the most appropriate for explaining the decisions of the utilized deepfake detector.","sentences":["In this paper we propose a new framework for evaluating the performance of explanation methods on the decisions of a deepfake detector.","This framework assesses the ability of an explanation method to spot the regions of a fake image with the biggest influence on the decision of the deepfake detector, by examining the extent to which these regions can be modified through a set of adversarial attacks, in order to flip the detector's prediction or reduce its initial prediction; we anticipate a larger drop in deepfake detection accuracy and prediction, for methods that spot these regions more accurately.","Based on this framework, we conduct a comparative study using a state-of-the-art model for deepfake detection that has been trained on the FaceForensics++ dataset, and five explanation methods from the literature.","The findings of our quantitative and qualitative evaluations document the advanced performance of the LIME explanation method against the other compared ones, and indicate this method as the most appropriate for explaining the decisions of the utilized deepfake detector."],"url":"http://arxiv.org/abs/2404.18649v1","category":"cs.CV"}
{"created":"2024-04-29 12:31:38","title":"Uncertainty-boosted Robust Video Activity Anticipation","abstract":"Video activity anticipation aims to predict what will happen in the future, embracing a broad application prospect ranging from robot vision and autonomous driving. Despite the recent progress, the data uncertainty issue, reflected as the content evolution process and dynamic correlation in event labels, has been somehow ignored. This reduces the model generalization ability and deep understanding on video content, leading to serious error accumulation and degraded performance. In this paper, we address the uncertainty learning problem and propose an uncertainty-boosted robust video activity anticipation framework, which generates uncertainty values to indicate the credibility of the anticipation results. The uncertainty value is used to derive a temperature parameter in the softmax function to modulate the predicted target activity distribution. To guarantee the distribution adjustment, we construct a reasonable target activity label representation by incorporating the activity evolution from the temporal class correlation and the semantic relationship. Moreover, we quantify the uncertainty into relative values by comparing the uncertainty among sample pairs and their temporal-lengths. This relative strategy provides a more accessible way in uncertainty modeling than quantifying the absolute uncertainty values on the whole dataset. Experiments on multiple backbones and benchmarks show our framework achieves promising performance and better robustness/interpretability. Source codes are available at https://github.com/qzhb/UbRV2A.","sentences":["Video activity anticipation aims to predict what will happen in the future, embracing a broad application prospect ranging from robot vision and autonomous driving.","Despite the recent progress, the data uncertainty issue, reflected as the content evolution process and dynamic correlation in event labels, has been somehow ignored.","This reduces the model generalization ability and deep understanding on video content, leading to serious error accumulation and degraded performance.","In this paper, we address the uncertainty learning problem and propose an uncertainty-boosted robust video activity anticipation framework, which generates uncertainty values to indicate the credibility of the anticipation results.","The uncertainty value is used to derive a temperature parameter in the softmax function to modulate the predicted target activity distribution.","To guarantee the distribution adjustment, we construct a reasonable target activity label representation by incorporating the activity evolution from the temporal class correlation and the semantic relationship.","Moreover, we quantify the uncertainty into relative values by comparing the uncertainty among sample pairs and their temporal-lengths.","This relative strategy provides a more accessible way in uncertainty modeling than quantifying the absolute uncertainty values on the whole dataset.","Experiments on multiple backbones and benchmarks show our framework achieves promising performance and better robustness/interpretability.","Source codes are available at https://github.com/qzhb/UbRV2A."],"url":"http://arxiv.org/abs/2404.18648v1","category":"cs.CV"}
{"created":"2024-04-29 12:27:41","title":"Graph Search Trees and the Intermezzo Problem","abstract":"The last in-tree recognition problem asks whether a given spanning tree can be derived by connecting each vertex with its rightmost left neighbor of some search ordering. In this study, we demonstrate that the last-in-tree recognition problem for Generic Search is $\\mathsf{NP}$-complete. We utilize this finding to strengthen a complexity result from order theory. Given partial order $\\pi$ and a set of triples, the $\\mathsf{NP}$-complete intermezzo problem asks for a linear extension of $\\pi$ where each first element of a triple is not between the other two. We show that this problem remains $\\mathsf{NP}$-complete even when the Hasse diagram of the partial order forms a tree of bounded height. In contrast, we give an $\\mathsf{XP}$ algorithm for the problem when parameterized by the width of the partial order. Furthermore, we show that $\\unicode{x2013}$ under the assumption of the Exponential Time Hypothesis $\\unicode{x2013}$ the running time of this algorithm is asymptotically optimal.","sentences":["The last in-tree recognition problem asks whether a given spanning tree can be derived by connecting each vertex with its rightmost left neighbor of some search ordering.","In this study, we demonstrate that the last-in-tree recognition problem for Generic Search is $\\mathsf{NP}$-complete.","We utilize this finding to strengthen a complexity result from order theory.","Given partial order $\\pi$ and a set of triples, the $\\mathsf{NP}$-complete intermezzo problem asks for a linear extension of $\\pi$ where each first element of a triple is not between the other two.","We show that this problem remains $\\mathsf{NP}$-complete even when the Hasse diagram of the partial order forms a tree of bounded height.","In contrast, we give an $\\mathsf{XP}$ algorithm for the problem when parameterized by the width of the partial order.","Furthermore, we show that $\\unicode{x2013}$ under the assumption of the Exponential Time Hypothesis $\\unicode{x2013}$ the running time of this algorithm is asymptotically optimal."],"url":"http://arxiv.org/abs/2404.18645v1","category":"cs.DM"}
{"created":"2024-04-29 12:18:21","title":"Going Beyond Popularity and Positivity Bias: Correcting for Multifactorial Bias in Recommender Systems","abstract":"Two typical forms of bias in user interaction data with recommender systems (RSs) are popularity bias and positivity bias, which manifest themselves as the over-representation of interactions with popular items or items that users prefer, respectively. Debiasing methods aim to mitigate the effect of selection bias on the evaluation and optimization of RSs. However, existing debiasing methods only consider single-factor forms of bias, e.g., only the item (popularity) or only the rating value (positivity). This is in stark contrast with the real world where user selections are generally affected by multiple factors at once. In this work, we consider multifactorial selection bias in RSs. Our focus is on selection bias affected by both item and rating value factors, which is a generalization and combination of popularity and positivity bias. While the concept of multifactorial bias is intuitive, it brings a severe practical challenge as it requires substantially more data for accurate bias estimation. As a solution, we propose smoothing and alternating gradient descent techniques to reduce variance and improve the robustness of its optimization. Our experimental results reveal that, with our proposed techniques, multifactorial bias corrections are more effective and robust than single-factor counterparts on real-world and synthetic datasets.","sentences":["Two typical forms of bias in user interaction data with recommender systems (RSs) are popularity bias and positivity bias, which manifest themselves as the over-representation of interactions with popular items or items that users prefer, respectively.","Debiasing methods aim to mitigate the effect of selection bias on the evaluation and optimization of RSs.","However, existing debiasing methods only consider single-factor forms of bias, e.g., only the item (popularity) or only the rating value (positivity).","This is in stark contrast with the real world where user selections are generally affected by multiple factors at once.","In this work, we consider multifactorial selection bias in RSs.","Our focus is on selection bias affected by both item and rating value factors, which is a generalization and combination of popularity and positivity bias.","While the concept of multifactorial bias is intuitive, it brings a severe practical challenge as it requires substantially more data for accurate bias estimation.","As a solution, we propose smoothing and alternating gradient descent techniques to reduce variance and improve the robustness of its optimization.","Our experimental results reveal that, with our proposed techniques, multifactorial bias corrections are more effective and robust than single-factor counterparts on real-world and synthetic datasets."],"url":"http://arxiv.org/abs/2404.18640v1","category":"cs.IR"}
{"created":"2024-04-29 12:16:08","title":"Reinforcement Learning Problem Solving with Large Language Models","abstract":"Large Language Models (LLMs) encapsulate an extensive amount of world knowledge, and this has enabled their application in various domains to improve the performance of a variety of Natural Language Processing (NLP) tasks. This has also facilitated a more accessible paradigm of conversation-based interactions between humans and AI systems to solve intended problems. However, one interesting avenue that shows untapped potential is the use of LLMs as Reinforcement Learning (RL) agents to enable conversational RL problem solving. Therefore, in this study, we explore the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks. We demonstrate how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks. In addition, we leverage the introduced prompting technique for episode simulation and Q-Learning, facilitated by LLMs. We then show the practicality of our approach through two detailed case studies for \"Research Scientist\" and \"Legal Matter Intake\" workflows.","sentences":["Large Language Models (LLMs) encapsulate an extensive amount of world knowledge, and this has enabled their application in various domains to improve the performance of a variety of Natural Language Processing (NLP) tasks.","This has also facilitated a more accessible paradigm of conversation-based interactions between humans and AI systems to solve intended problems.","However, one interesting avenue that shows untapped potential is the use of LLMs as Reinforcement Learning (RL) agents to enable conversational RL problem solving.","Therefore, in this study, we explore the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks.","We demonstrate how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks.","In addition, we leverage the introduced prompting technique for episode simulation and Q-Learning, facilitated by LLMs.","We then show the practicality of our approach through two detailed case studies for \"Research Scientist\" and \"Legal Matter Intake\" workflows."],"url":"http://arxiv.org/abs/2404.18638v1","category":"cs.AI"}
{"created":"2024-04-29 12:15:49","title":"QOSST: A Highly-Modular Open Source Platform for Experimental Continuous-Variable Quantum Key Distribution","abstract":"Quantum Key Distribution (QKD) enables secret key exchange between two remote parties with information-theoretic security rooted in the laws of quantum physics. Encoding key information in continuous variables (CV), such as the values of quadrature components of coherent states of light, brings implementations much closer to standard optical communication systems, but this comes at the price of significant complexity in the digital signal processing techniques required for operation at low signal-to-noise ratios. In this work, we wish to lower the barriers to entry for CV-QKD experiments associated to this difficulty by providing a highly modular, open source software that is in principle hardware agnostic and can be used in multiple configurations. We benchmarked this software, called QOSST, using an experimental setup with a locally generated local oscillator, frequency multiplexed pilots and RF-heterodyne detection, and obtained state-of-the-art secret key rates of the order of Mbit/s over metropolitan distances at the asymptotic limit. We hope that QOSST can be used to stimulate further experimental advances in CV-QKD and be improved and extended by the community to achieve high performance in a wide variety of configurations.","sentences":["Quantum Key Distribution (QKD) enables secret key exchange between two remote parties with information-theoretic security rooted in the laws of quantum physics.","Encoding key information in continuous variables (CV), such as the values of quadrature components of coherent states of light, brings implementations much closer to standard optical communication systems, but this comes at the price of significant complexity in the digital signal processing techniques required for operation at low signal-to-noise ratios.","In this work, we wish to lower the barriers to entry for CV-QKD experiments associated to this difficulty by providing a highly modular, open source software that is in principle hardware agnostic and can be used in multiple configurations.","We benchmarked this software, called QOSST, using an experimental setup with a locally generated local oscillator, frequency multiplexed pilots and RF-heterodyne detection, and obtained state-of-the-art secret key rates of the order of Mbit/s over metropolitan distances at the asymptotic limit.","We hope that QOSST can be used to stimulate further experimental advances in CV-QKD and be improved and extended by the community to achieve high performance in a wide variety of configurations."],"url":"http://arxiv.org/abs/2404.18637v1","category":"quant-ph"}
{"created":"2024-04-29 12:12:07","title":"A comprehensive study of nonlinear perturbations in the dynamics of planar crack fronts","abstract":"The interaction of crack fronts with asperities is central to the criteria of fracture in heterogeneous materials and for the prediction of fracture surface formation. It is known how dynamic crack fronts respond to small, 1st-order, perturbations. Large and localized disturbances to crack motion, however, induce dynamic and geometric nonlinear effects that are beyond the existing linear theories. Because the determination of the 3D elastic fields surrounding perturbed crack fronts is a necessary step towards any theoretical study of crack front dynamics, we develop a 2nd-order perturbation theory for the asymptotic fields of planar crack fronts. Based on previous work, we consider two models of fracture. In the so-called scalar elastic model, which is analogous to anti-plane (Mode III) fracture, the stress and displacement fields are obtained through matched asymptotic expansions. A self-consistent expansion is used to resolve the fields near tensile (Mode I) crack fronts. Both methods can be extended to higher perturbation orders. The main results of this work are the explicit 2nd-order expressions of the local dynamic energy-release-rates for perturbations of straight fronts. These general formulae recover the known energy-release-rates of curved quasi-static fronts and of straight dynamic fronts.","sentences":["The interaction of crack fronts with asperities is central to the criteria of fracture in heterogeneous materials and for the prediction of fracture surface formation.","It is known how dynamic crack fronts respond to small, 1st-order, perturbations.","Large and localized disturbances to crack motion, however, induce dynamic and geometric nonlinear effects that are beyond the existing linear theories.","Because the determination of the 3D elastic fields surrounding perturbed crack fronts is a necessary step towards any theoretical study of crack front dynamics, we develop a 2nd-order perturbation theory for the asymptotic fields of planar crack fronts.","Based on previous work, we consider two models of fracture.","In the so-called scalar elastic model, which is analogous to anti-plane (Mode III) fracture, the stress and displacement fields are obtained through matched asymptotic expansions.","A self-consistent expansion is used to resolve the fields near tensile (Mode I) crack fronts.","Both methods can be extended to higher perturbation orders.","The main results of this work are the explicit 2nd-order expressions of the local dynamic energy-release-rates for perturbations of straight fronts.","These general formulae recover the known energy-release-rates of curved quasi-static fronts and of straight dynamic fronts."],"url":"http://arxiv.org/abs/2404.18633v1","category":"cond-mat.soft"}
{"created":"2024-04-29 12:11:26","title":"Feature importance to explain multimodal prediction models. A clinical use case","abstract":"Surgery to treat elderly hip fracture patients may cause complications that can lead to early mortality. An early warning system for complications could provoke clinicians to monitor high-risk patients more carefully and address potential complications early, or inform the patient. In this work, we develop a multimodal deep-learning model for post-operative mortality prediction using pre-operative and per-operative data from elderly hip fracture patients. Specifically, we include static patient data, hip and chest images before surgery in pre-operative data, vital signals, and medications administered during surgery in per-operative data. We extract features from image modalities using ResNet and from vital signals using LSTM. Explainable model outcomes are essential for clinical applicability, therefore we compute Shapley values to explain the predictions of our multimodal black box model. We find that i) Shapley values can be used to estimate the relative contribution of each modality both locally and globally, and ii) a modified version of the chain rule can be used to propagate Shapley values through a sequence of models supporting interpretable local explanations. Our findings imply that a multimodal combination of black box models can be explained by propagating Shapley values through the model sequence.","sentences":["Surgery to treat elderly hip fracture patients may cause complications that can lead to early mortality.","An early warning system for complications could provoke clinicians to monitor high-risk patients more carefully and address potential complications early, or inform the patient.","In this work, we develop a multimodal deep-learning model for post-operative mortality prediction using pre-operative and per-operative data from elderly hip fracture patients.","Specifically, we include static patient data, hip and chest images before surgery in pre-operative data, vital signals, and medications administered during surgery in per-operative data.","We extract features from image modalities using ResNet and from vital signals using LSTM.","Explainable model outcomes are essential for clinical applicability, therefore we compute Shapley values to explain the predictions of our multimodal black box model.","We find that i)","Shapley values can be used to estimate the relative contribution of each modality both locally and globally, and ii) a modified version of the chain rule can be used to propagate Shapley values through a sequence of models supporting interpretable local explanations.","Our findings imply that a multimodal combination of black box models can be explained by propagating Shapley values through the model sequence."],"url":"http://arxiv.org/abs/2404.18631v1","category":"cs.LG"}
{"created":"2024-04-29 12:02:06","title":"Self-Avatar Animation in Virtual Reality: Impact of Motion Signals Artifacts on the Full-Body Pose Reconstruction","abstract":"Virtual Reality (VR) applications have revolutionized user experiences by immersing individuals in interactive 3D environments. These environments find applications in numerous fields, including healthcare, education, or architecture. A significant aspect of VR is the inclusion of self-avatars, representing users within the virtual world, which enhances interaction and embodiment. However, generating lifelike full-body self-avatar animations remains challenging, particularly in consumer-grade VR systems, where lower-body tracking is often absent. One method to tackle this problem is by providing an external source of motion information that includes lower body information such as full Cartesian positions estimated from RGB(D) cameras. Nevertheless, the limitations of these systems are multiples: the desynchronization between the two motion sources and occlusions are examples of significant issues that hinder the implementations of such systems. In this paper, we aim to measure the impact on the reconstruction of the articulated self-avatar's full-body pose of (1) the latency between the VR motion features and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4) the inaccuracy of the position estimation algorithm. In addition, we analyze the motion reconstruction errors using ground truth and 3D Cartesian coordinates estimated from \\textit{YOLOv8} pose estimation. These analyzes show that the studied methods are significantly sensitive to any degradation tested, especially regarding the velocity reconstruction error.","sentences":["Virtual Reality (VR) applications have revolutionized user experiences by immersing individuals in interactive 3D environments.","These environments find applications in numerous fields, including healthcare, education, or architecture.","A significant aspect of VR is the inclusion of self-avatars, representing users within the virtual world, which enhances interaction and embodiment.","However, generating lifelike full-body self-avatar animations remains challenging, particularly in consumer-grade VR systems, where lower-body tracking is often absent.","One method to tackle this problem is by providing an external source of motion information that includes lower body information such as full Cartesian positions estimated from RGB(D) cameras.","Nevertheless, the limitations of these systems are multiples: the desynchronization between the two motion sources and occlusions are examples of significant issues that hinder the implementations of such systems.","In this paper, we aim to measure the impact on the reconstruction of the articulated self-avatar's full-body pose of (1) the latency between the VR motion features and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4) the inaccuracy of the position estimation algorithm.","In addition, we analyze the motion reconstruction errors using ground truth and 3D Cartesian coordinates estimated from \\textit{YOLOv8} pose estimation.","These analyzes show that the studied methods are significantly sensitive to any degradation tested, especially regarding the velocity reconstruction error."],"url":"http://arxiv.org/abs/2404.18628v1","category":"cs.CV"}
{"created":"2024-04-29 11:55:19","title":"Towards topology optimization of a hybrid-excited machine using recursive material interpolation","abstract":"Hybrid-excited electrical machines aim to combine the advantages of permanent magnet machines (high efficiency and torque density) with those of separately excited machines (ease of flux-weakening at high speed). These machines are of interest to electric vehicles, and only parametric approaches are available in the literature for their optimization. This work proposes a more general topology optimization methodology by extending the formalism of density methods. The difficulty lies in integrating the numerous natures of materials (conductors, permanent magnets, ferromagnetic material, air...) without strongly deconvexifying the optimization problem, which leads to non-physical results with unsatisfactory performance. To address this issue, a recursive material interpolation is introduced. The hybrid-excited rotors optimized by this approach are compared with those of existing techniques, demonstrating a clear superiority of the recursive interpolation.","sentences":["Hybrid-excited electrical machines aim to combine the advantages of permanent magnet machines (high efficiency and torque density) with those of separately excited machines (ease of flux-weakening at high speed).","These machines are of interest to electric vehicles, and only parametric approaches are available in the literature for their optimization.","This work proposes a more general topology optimization methodology by extending the formalism of density methods.","The difficulty lies in integrating the numerous natures of materials (conductors, permanent magnets, ferromagnetic material, air...) without strongly deconvexifying the optimization problem, which leads to non-physical results with unsatisfactory performance.","To address this issue, a recursive material interpolation is introduced.","The hybrid-excited rotors optimized by this approach are compared with those of existing techniques, demonstrating a clear superiority of the recursive interpolation."],"url":"http://arxiv.org/abs/2404.18625v1","category":"math.OC"}
{"created":"2024-04-29 11:52:20","title":"Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?","abstract":"Vision and language models (VLMs) are currently the most generally performant architectures on multimodal tasks. Next to their predictions, they can also produce explanations, either in post-hoc or CoT settings. However, it is not clear how much they use the vision and text modalities when generating predictions or explanations. In this work, we investigate if VLMs rely on modalities differently when generating explanations as opposed to when they provide answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing tests and measures to VLM decoders. We find that VLMs are less self-consistent than LLMs. The text contributions in VL decoders are much larger than the image contributions across all measured tasks. And the contributions of the image are significantly larger for explanation generations than for answer generation. This difference is even larger in CoT compared to the post-hoc explanation setting. We also provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which to date focused only on VL encoders. We find that VL decoders are still struggling with most phenomena tested by VALSE.","sentences":["Vision and language models (VLMs) are currently the most generally performant architectures on multimodal tasks.","Next to their predictions, they can also produce explanations, either in post-hoc or CoT settings.","However, it is not clear how much they use the vision and text modalities when generating predictions or explanations.","In this work, we investigate if VLMs rely on modalities differently when generating explanations as opposed to when they provide answers.","We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing tests and measures to VLM decoders.","We find that VLMs are less self-consistent than LLMs.","The text contributions in VL decoders are much larger than the image contributions across all measured tasks.","And the contributions of the image are significantly larger for explanation generations than for answer generation.","This difference is even larger in CoT compared to the post-hoc explanation setting.","We also provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which to date focused only on VL encoders.","We find that VL decoders are still struggling with most phenomena tested by VALSE."],"url":"http://arxiv.org/abs/2404.18624v1","category":"cs.CL"}
{"created":"2024-04-29 11:42:47","title":"Conservation Laws For Every Quantum Measurement Outcome","abstract":"In the paradigmatic example of quantum measurements, whenever one measures a system which starts in a superposition of two states of a conserved quantity, it jumps to one of the two states, implying different final values for the quantity that should have been conserved. The standard law of conservation for quantum mechanics handles this jump by stating only that the total distribution of the conserved quantity over repeated measurements is unchanged, but states nothing about individual cases. Here however we show that one can go beyond this and have conservation in each individual instance. We made our arguments in the case of angular momentum of a particle on a circle, where many technicalities simplify, and bring arguments to show that this holds in full generality. Hence we argue that the conservation law in quantum mechanics should be rewritten, to go beyond its hitherto statistical formulation, to state that the total of a conserved quantity is unchanged in every individual measurement outcome. As a further crucial element, we show that conservation can be localised at the level of the system of interest and its relevant frame of reference, and is independent on any assumptions on the distribution of the conserved quantity over the entire universe.","sentences":["In the paradigmatic example of quantum measurements, whenever one measures a system which starts in a superposition of two states of a conserved quantity, it jumps to one of the two states, implying different final values for the quantity that should have been conserved.","The standard law of conservation for quantum mechanics handles this jump by stating only that the total distribution of the conserved quantity over repeated measurements is unchanged, but states nothing about individual cases.","Here however we show that one can go beyond this and have conservation in each individual instance.","We made our arguments in the case of angular momentum of a particle on a circle, where many technicalities simplify, and bring arguments to show that this holds in full generality.","Hence we argue that the conservation law in quantum mechanics should be rewritten, to go beyond its hitherto statistical formulation, to state that the total of a conserved quantity is unchanged in every individual measurement outcome.","As a further crucial element, we show that conservation can be localised at the level of the system of interest and its relevant frame of reference, and is independent on any assumptions on the distribution of the conserved quantity over the entire universe."],"url":"http://arxiv.org/abs/2404.18621v1","category":"quant-ph"}
{"created":"2024-04-29 11:41:34","title":"FlexiFilm: Long Video Generation with Flexible Conditions","abstract":"Generating long and consistent videos has emerged as a significant yet challenging problem. While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation. This results in prominent temporal inconsistency and overexposure. Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation. Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure. Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses. Project page: https://y-ichen.github.io/FlexiFilm-Page/","sentences":["Generating long and consistent videos has emerged as a significant yet challenging problem.","While most existing diffusion-based video generation models, derived from image generation models, demonstrate promising performance in generating short videos, their simple conditioning mechanism and sampling strategy-originally designed for image generation-cause severe performance degradation when adapted to long video generation.","This results in prominent temporal inconsistency and overexposure.","Thus, in this work, we introduce FlexiFilm, a new diffusion model tailored for long video generation.","Our framework incorporates a temporal conditioner to establish a more consistent relationship between generation and multi-modal conditions, and a resampling strategy to tackle overexposure.","Empirical results demonstrate FlexiFilm generates long and consistent videos, each over 30 seconds in length, outperforming competitors in qualitative and quantitative analyses.","Project page: https://y-ichen.github.io/FlexiFilm-Page/"],"url":"http://arxiv.org/abs/2404.18620v1","category":"cs.CV"}
{"created":"2024-04-29 11:41:20","title":"Patterning of 2D second harmonic generation active arrays in ferroelectric nematic fluids","abstract":"Ferroelectric nematic liquid crystals exhibit unique non-linear optical properties, with the potential to become transformative materials for photonic applications. A promising direction relies on the fabrication of tailored polar orientational patterns via photoalignment, thus shaping the non-linear optical susceptibility through thin slabs of the ferroelectric fluid. Here, we explore the fabrication of 2D periodic SHG active arrays in ferroelectric nematic fluids, for different materials, cell thicknesses and motifs. Based on polarizing optical microscopy observations in combination with optical simulations, second harmonic generation microscopy and interferometry, the 3D structure of the motifs are revealed. Two different 2D periodic patterns are explored, showing that the balance between flexoelectric and electrostatic energy can lead to different domain structures, an effect which is rooted in the difference between the flexoelectric properties of the materials. It is shown that by combining the surface-inscribed alignment with different spontaneous degrees of twist, 2D SHG active arrays can be obtained in the micrometre scale, in which adjacent areas exhibit maximum SHG signals at opposite angles.","sentences":["Ferroelectric nematic liquid crystals exhibit unique non-linear optical properties, with the potential to become transformative materials for photonic applications.","A promising direction relies on the fabrication of tailored polar orientational patterns via photoalignment, thus shaping the non-linear optical susceptibility through thin slabs of the ferroelectric fluid.","Here, we explore the fabrication of 2D periodic SHG active arrays in ferroelectric nematic fluids, for different materials, cell thicknesses and motifs.","Based on polarizing optical microscopy observations in combination with optical simulations, second harmonic generation microscopy and interferometry, the 3D structure of the motifs are revealed.","Two different 2D periodic patterns are explored, showing that the balance between flexoelectric and electrostatic energy can lead to different domain structures, an effect which is rooted in the difference between the flexoelectric properties of the materials.","It is shown that by combining the surface-inscribed alignment with different spontaneous degrees of twist, 2D SHG active arrays can be obtained in the micrometre scale, in which adjacent areas exhibit maximum SHG signals at opposite angles."],"url":"http://arxiv.org/abs/2404.18619v1","category":"cond-mat.soft"}
{"created":"2024-04-29 11:39:59","title":"Nonlinear Superconducting Magnetoelectric Effect","abstract":"A supercurrent flow can induce a nonvanishing spin magnetization in noncentrosymmetric superconductors with spin-orbit interaction. Often known as the non-dissipative magnetoelectric effect, these are most commonly found at linear order in supercurrent flow. Here, we argue that a nonlinear superconducting magnetoelectric effect (NSM) can naturally manifest in altermagnet/superconductor (ALM/SC) heterostructures: NSM manifests as a spin polarization generated as a second-order response to a driving supercurrent. Strikingly, we find NSM is the leading order magnetization response in ALM/SC heterostructures and survives even in the presence of centrosymmetry; $C_4 \\mathcal{T}$ symmetry in altermagnets zeroes both the equilibrium magnetization as well as out-of-plane linear magnetoelectric response. This renders NSM a powerful electric and non-dissipative means of controlling magnetization in ALM/SC heterostructures, a promising platform for superconducting spintronics.","sentences":["A supercurrent flow can induce a nonvanishing spin magnetization in noncentrosymmetric superconductors with spin-orbit interaction.","Often known as the non-dissipative magnetoelectric effect, these are most commonly found at linear order in supercurrent flow.","Here, we argue that a nonlinear superconducting magnetoelectric effect (NSM) can naturally manifest in altermagnet/superconductor (ALM/SC) heterostructures: NSM manifests as a spin polarization generated as a second-order response to a driving supercurrent.","Strikingly, we find NSM is the leading order magnetization response in ALM/SC heterostructures and survives even in the presence of centrosymmetry; $C_4 \\mathcal{T}$ symmetry in altermagnets zeroes both the equilibrium magnetization as well as out-of-plane linear magnetoelectric response.","This renders NSM a powerful electric and non-dissipative means of controlling magnetization in ALM/SC heterostructures, a promising platform for superconducting spintronics."],"url":"http://arxiv.org/abs/2404.18616v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 11:33:06","title":"A Few Projective Classes of (Non-Hausdorff) Topological Spaces","abstract":"A class of topological spaces is projective (resp., $\\omega$-projective) if and only if projective systems of spaces (resp., with a countable cofinal subset of indices) in the class are still in the class. A certain number of classes of Hausdorff spaces are known to be, or not to be, ($\\omega$-) projective. We examine classes of spaces that are not necessarily Hausdorff. Sober and compact sober spaces form projective classes, but most classes of locally compact spaces are not even $\\omega$-projective. Guided by the fact that the stably compact spaces are exactly the locally compact, strongly sober spaces, and that the strongly sober spaces are exactly the sober, coherent, compact, weakly Hausdorff (in the sense of Keimel and Lawson) spaces, we examine which classes defined by combinations of those properties are projective. Notably, we find that coherent sober spaces, compact coherent sober spaces, as well as (locally) strongly sober spaces, form projective classes.","sentences":["A class of topological spaces is projective (resp., $\\omega$-projective) if and only if projective systems of spaces (resp., with a countable cofinal subset of indices) in the class are still in the class.","A certain number of classes of Hausdorff spaces are known to be, or not to be, ($\\omega$-) projective.","We examine classes of spaces that are not necessarily Hausdorff.","Sober and compact sober spaces form projective classes, but most classes of locally compact spaces are not even $\\omega$-projective.","Guided by the fact that the stably compact spaces are exactly the locally compact, strongly sober spaces, and that the strongly sober spaces are exactly the sober, coherent, compact, weakly Hausdorff (in the sense of Keimel and Lawson) spaces, we examine which classes defined by combinations of those properties are projective.","Notably, we find that coherent sober spaces, compact coherent sober spaces, as well as (locally) strongly sober spaces, form projective classes."],"url":"http://arxiv.org/abs/2404.18614v1","category":"math.GN"}
{"created":"2024-04-29 11:27:04","title":"No smooth spacetime: Exploring primordial perturbations in Lorentzian quantum cosmology","abstract":"Recent analysis of quantum cosmology has focused on the Lorentzian path integral formulations of both the no-boundary and tunneling proposals. However, it has been criticized that the wave function for linearized perturbations around a homogeneous and isotropic background leads to an inverse Gaussian distribution. This results in divergent correlation functions and cosmological inconsistencies. In this paper we explore these perturbation problems in Lorentzian quantum cosmology, focusing in particular on the quantum creation of closed, flat, and open universes from no spacetime and the beginning of primordial inflation. We show that most quantum cosmological scenarios have serious perturbation problems. We also study the effects of trans-Planckian physics on quantum cosmology, using the generalized Corley-Jacobson dispersion as a case study of modified dispersion relations. Our findings indicate that resolving perturbation problems in Lorentzian quantum cosmology with modified dispersion relations remains a challenge. However, the perturbations can be Gaussian in the quantum creation of the flat or open universe within the confines of the saddle-point approximation and the generalized Corley-Jacobson dispersion.","sentences":["Recent analysis of quantum cosmology has focused on the Lorentzian path integral formulations of both the no-boundary and tunneling proposals.","However, it has been criticized that the wave function for linearized perturbations around a homogeneous and isotropic background leads to an inverse Gaussian distribution.","This results in divergent correlation functions and cosmological inconsistencies.","In this paper we explore these perturbation problems in Lorentzian quantum cosmology, focusing in particular on the quantum creation of closed, flat, and open universes from no spacetime and the beginning of primordial inflation.","We show that most quantum cosmological scenarios have serious perturbation problems.","We also study the effects of trans-Planckian physics on quantum cosmology, using the generalized Corley-Jacobson dispersion as a case study of modified dispersion relations.","Our findings indicate that resolving perturbation problems in Lorentzian quantum cosmology with modified dispersion relations remains a challenge.","However, the perturbations can be Gaussian in the quantum creation of the flat or open universe within the confines of the saddle-point approximation and the generalized Corley-Jacobson dispersion."],"url":"http://arxiv.org/abs/2404.18609v1","category":"gr-qc"}
{"created":"2024-04-29 11:22:45","title":"A new framework to follow up candidates from continuous gravitational-wave searches","abstract":"Searches for continuous gravitational waves from unknown neutron stars are limited in sensitivity due to their high computational cost. For this reason, developing new methods or improving existing ones can increase the probability of making a detection. In this paper we present a new framework that uses MCMC or nested sampling methods to follow-up candidates of continuous gravitational-wave searches. This framework aims to go beyond the capabilities of PYFSTAT (which is limited to the PTEMCEE sampler), by allowing a flexible choice of sampling algorithm (using BILBY as a wrapper) and multi-dimensional correlated prior distributions. We show that MCMC and nested sampling methods can recover the maximum posterior point for much bigger parameter-space regions than previously thought (including for sources in binary systems), and we present tests that examine the capabilities of the new framework: a comparison between the DYNESTY, NESSAI, and PTEMCEE samplers, the usage of correlated priors, and its improved computational cost.","sentences":["Searches for continuous gravitational waves from unknown neutron stars are limited in sensitivity due to their high computational cost.","For this reason, developing new methods or improving existing ones can increase the probability of making a detection.","In this paper we present a new framework that uses MCMC or nested sampling methods to follow-up candidates of continuous gravitational-wave searches.","This framework aims to go beyond the capabilities of PYFSTAT (which is limited to the PTEMCEE sampler), by allowing a flexible choice of sampling algorithm (using BILBY as a wrapper) and multi-dimensional correlated prior distributions.","We show that MCMC and nested sampling methods can recover the maximum posterior point for much bigger parameter-space regions than previously thought (including for sources in binary systems), and we present tests that examine the capabilities of the new framework: a comparison between the DYNESTY, NESSAI, and PTEMCEE samplers, the usage of correlated priors, and its improved computational cost."],"url":"http://arxiv.org/abs/2404.18608v1","category":"gr-qc"}
{"created":"2024-04-29 11:19:15","title":"CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial Animation Generation","abstract":"Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations. The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions. Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations. Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions. To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns. To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions. We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions. Experimental results demonstrate that our method outperforms existing state-of-the-art methods.","sentences":["Speech-driven 3D facial animation technology has been developed for years, but its practical application still lacks expectations.","The main challenges lie in data limitations, lip alignment, and the naturalness of facial expressions.","Although lip alignment has seen many related studies, existing methods struggle to synthesize natural and realistic expressions, resulting in a mechanical and stiff appearance of facial animations.","Even with some research extracting emotional features from speech, the randomness of facial movements limits the effective expression of emotions.","To address this issue, this paper proposes a method called CSTalk (Correlation Supervised) that models the correlations among different regions of facial movements and supervises the training of the generative model to generate realistic expressions that conform to human facial motion patterns.","To generate more intricate animations, we employ a rich set of control parameters based on the metahuman character model and capture a dataset for five different emotions.","We train a generative network using an autoencoder structure and input an emotion embedding vector to achieve the generation of user-control expressions.","Experimental results demonstrate that our method outperforms existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.18604v1","category":"cs.CV"}
{"created":"2024-04-29 11:19:04","title":"Beating Posits at Their Own Game: Takum Arithmetic","abstract":"Recent evaluations have highlighted the tapered posit number format as a promising alternative to the uniform precision IEEE 754 floating-point numbers, which suffer from various deficiencies. Although the posit encoding scheme offers superior coding efficiency at values close to unity, its efficiency markedly diminishes with deviation from unity. This reduction in efficiency leads to suboptimal encodings and a consequent diminution in dynamic range, thereby rendering posits suboptimal for general-purpose computer arithmetic.   This paper introduces and formally proves 'takum' as a novel general-purpose logarithmic tapered-precision number format, synthesising the advantages of posits in low-bit applications with high encoding efficiency for numbers distant from unity. Takums exhibit an asymptotically constant dynamic range in terms of bit string length, which is delineated in the paper to be suitable for a general-purpose number format. It is demonstrated that takums either match or surpass existing alternatives. Moreover, takums address several issues previously identified in posits while unveiling novel and beneficial arithmetic properties.","sentences":["Recent evaluations have highlighted the tapered posit number format as a promising alternative to the uniform precision IEEE 754 floating-point numbers, which suffer from various deficiencies.","Although the posit encoding scheme offers superior coding efficiency at values close to unity, its efficiency markedly diminishes with deviation from unity.","This reduction in efficiency leads to suboptimal encodings and a consequent diminution in dynamic range, thereby rendering posits suboptimal for general-purpose computer arithmetic.   ","This paper introduces and formally proves 'takum' as a novel general-purpose logarithmic tapered-precision number format, synthesising the advantages of posits in low-bit applications with high encoding efficiency for numbers distant from unity.","Takums exhibit an asymptotically constant dynamic range in terms of bit string length, which is delineated in the paper to be suitable for a general-purpose number format.","It is demonstrated that takums either match or surpass existing alternatives.","Moreover, takums address several issues previously identified in posits while unveiling novel and beneficial arithmetic properties."],"url":"http://arxiv.org/abs/2404.18603v1","category":"math.NA"}
{"created":"2024-04-29 11:15:26","title":"Rigid dualizing complexes of affine Hecke algebras","abstract":"We identify the rigid dualizing complex of the (generic) affine Hecke algebra $H_q$ attached to a reduced root system and deduce some structural properties as a consequence. For example, we show that the classical Hecke algebra $H_{q^\\pm}$ as well as $H_q/q$ are, under a certain condition on the root system, Frobenius over their centers with Nakayama automorphism given by an explicit involution.","sentences":["We identify the rigid dualizing complex of the (generic) affine Hecke algebra $H_q$ attached to a reduced root system and deduce some structural properties as a consequence.","For example, we show that the classical Hecke algebra $H_{q^\\pm}$ as well as $H_q/q$ are, under a certain condition on the root system, Frobenius over their centers with Nakayama automorphism given by an explicit involution."],"url":"http://arxiv.org/abs/2404.18601v1","category":"math.RT"}
{"created":"2024-04-29 11:14:11","title":"Self-supervised learning for classifying paranasal anomalies in the maxillary sinus","abstract":"Purpose: Paranasal anomalies, frequently identified in routine radiological screenings, exhibit diverse morphological characteristics. Due to the diversity of anomalies, supervised learning methods require large labelled dataset exhibiting diverse anomaly morphology. Self-supervised learning (SSL) can be used to learn representations from unlabelled data. However, there are no SSL methods designed for the downstream task of classifying paranasal anomalies in the maxillary sinus (MS).   Methods: Our approach uses a 3D Convolutional Autoencoder (CAE) trained in an unsupervised anomaly detection (UAD) framework. Initially, we train the 3D CAE to reduce reconstruction errors when reconstructing normal maxillary sinus (MS) image. Then, this CAE is applied to an unlabelled dataset to generate coarse anomaly locations by creating residual MS images. Following this, a 3D Convolutional Neural Network (CNN) reconstructs these residual images, which forms our SSL task. Lastly, we fine-tune the encoder part of the 3D CNN on a labelled dataset of normal and anomalous MS images.   Results: The proposed SSL technique exhibits superior performance compared to existing generic self-supervised methods, especially in scenarios with limited annotated data. When trained on just 10% of the annotated dataset, our method achieves an Area Under the Precision-Recall Curve (AUPRC) of 0.79 for the downstream classification task. This performance surpasses other methods, with BYOL attaining an AUPRC of 0.75, SimSiam at 0.74, SimCLR at 0.73 and Masked Autoencoding using SparK at 0.75.   Conclusion: A self-supervised learning approach that inherently focuses on localizing paranasal anomalies proves to be advantageous, particularly when the subsequent task involves differentiating normal from anomalous maxillary sinuses. Access our code at https://github.com/mtec-tuhh/self-supervised-paranasal-anomaly","sentences":["Purpose: Paranasal anomalies, frequently identified in routine radiological screenings, exhibit diverse morphological characteristics.","Due to the diversity of anomalies, supervised learning methods require large labelled dataset exhibiting diverse anomaly morphology.","Self-supervised learning (SSL) can be used to learn representations from unlabelled data.","However, there are no SSL methods designed for the downstream task of classifying paranasal anomalies in the maxillary sinus (MS).   ","Methods: Our approach uses a 3D Convolutional Autoencoder (CAE) trained in an unsupervised anomaly detection (UAD) framework.","Initially, we train the 3D CAE to reduce reconstruction errors when reconstructing normal maxillary sinus (MS) image.","Then, this CAE is applied to an unlabelled dataset to generate coarse anomaly locations by creating residual MS images.","Following this, a 3D Convolutional Neural Network (CNN) reconstructs these residual images, which forms our SSL task.","Lastly, we fine-tune the encoder part of the 3D CNN on a labelled dataset of normal and anomalous MS images.   ","Results:","The proposed SSL technique exhibits superior performance compared to existing generic self-supervised methods, especially in scenarios with limited annotated data.","When trained on just 10% of the annotated dataset, our method achieves an Area Under the Precision-Recall Curve (AUPRC) of 0.79 for the downstream classification task.","This performance surpasses other methods, with BYOL attaining an AUPRC of 0.75, SimSiam at 0.74, SimCLR at 0.73 and Masked Autoencoding using SparK at 0.75.   Conclusion: A self-supervised learning approach that inherently focuses on localizing paranasal anomalies proves to be advantageous, particularly when the subsequent task involves differentiating normal from anomalous maxillary sinuses.","Access our code at https://github.com/mtec-tuhh/self-supervised-paranasal-anomaly"],"url":"http://arxiv.org/abs/2404.18599v1","category":"eess.IV"}
{"created":"2024-04-29 11:13:37","title":"Anywhere: A Multi-Agent Framework for Reliable and Diverse Foreground-Conditioned Image Inpainting","abstract":"Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes. However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as \"over-imagination\", inconsistency between foreground and background, and limited diversity. In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues. Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models. This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer. The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts. In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image. The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed. Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating \"over-imagination\", resolving foreground-background discrepancies, and enhancing diversity. It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results.","sentences":["Recent advancements in image inpainting, particularly through diffusion modeling, have yielded promising outcomes.","However, when tested in scenarios involving the completion of images based on the foreground objects, current methods that aim to inpaint an image in an end-to-end manner encounter challenges such as \"over-imagination\", inconsistency between foreground and background, and limited diversity.","In response, we introduce Anywhere, a pioneering multi-agent framework designed to address these issues.","Anywhere utilizes a sophisticated pipeline framework comprising various agents such as Visual Language Model (VLM), Large Language Model (LLM), and image generation models.","This framework consists of three principal components: the prompt generation module, the image generation module, and the outcome analyzer.","The prompt generation module conducts a semantic analysis of the input foreground image, leveraging VLM to predict relevant language descriptions and LLM to recommend optimal language prompts.","In the image generation module, we employ a text-guided canny-to-image generation model to create a template image based on the edge map of the foreground image and language prompts, and an image refiner to produce the outcome by blending the input foreground and the template image.","The outcome analyzer employs VLM to evaluate image content rationality, aesthetic score, and foreground-background relevance, triggering prompt and image regeneration as needed.","Extensive experiments demonstrate that our Anywhere framework excels in foreground-conditioned image inpainting, mitigating \"over-imagination\", resolving foreground-background discrepancies, and enhancing diversity.","It successfully elevates foreground-conditioned image inpainting to produce more reliable and diverse results."],"url":"http://arxiv.org/abs/2404.18598v1","category":"cs.CV"}
{"created":"2024-04-29 11:08:44","title":"Two-dimensional correlation propagation dynamics with a cluster discrete phase-space method","abstract":"Nonequilibrium dynamics of highly-controlled quantum systems is a challenging issue in statistical physics and quantum many-body physics, relevant to recent experimental developments of analog and digital quantum simulations. In this work, we develop a discrete phase-space approach for general SU($N$) spin systems that utilizes cluster mean field equations, which capture non-trivial quantum correlations inside each cluster, beyond the capability of the standard discrete truncated Wigner approximation for individual classical spins. Our formalism, based on a cluster phase-point operator, makes it possible to realize scalable numerical samplings of cluster phase-space variables, where the total number of noise variables for a direct product state is independent of the choice of the separation into finite regions of clusters. We numerically demonstrate that the cluster discrete truncated Wigner approximation (C-dTWA) method can reproduce key results in a recent experiment on the correlation propagation dynamics in a two dimensional Bose-Hubbard system. We also compare the results of C-dTWA for clusters of $2\\times 2$ sites with those of a two-dimensional tensor network method and discuss that both approaches agree very well in a short time region, where the energy is well conserved in the tensor network simulations. Since we formulate the C-dTWA method in a general form, it can be potentially applied to various dynamical problems in isolated and open quantum systems even in higher dimensions.","sentences":["Nonequilibrium dynamics of highly-controlled quantum systems is a challenging issue in statistical physics and quantum many-body physics, relevant to recent experimental developments of analog and digital quantum simulations.","In this work, we develop a discrete phase-space approach for general SU($N$) spin systems that utilizes cluster mean field equations, which capture non-trivial quantum correlations inside each cluster, beyond the capability of the standard discrete truncated Wigner approximation for individual classical spins.","Our formalism, based on a cluster phase-point operator, makes it possible to realize scalable numerical samplings of cluster phase-space variables, where the total number of noise variables for a direct product state is independent of the choice of the separation into finite regions of clusters.","We numerically demonstrate that the cluster discrete truncated Wigner approximation (C-dTWA) method can reproduce key results in a recent experiment on the correlation propagation dynamics in a two dimensional Bose-Hubbard system.","We also compare the results of C-dTWA for clusters of $2\\times 2$ sites with those of a two-dimensional tensor network method and discuss that both approaches agree very well in a short time region, where the energy is well conserved in the tensor network simulations.","Since we formulate the C-dTWA method in a general form, it can be potentially applied to various dynamical problems in isolated and open quantum systems even in higher dimensions."],"url":"http://arxiv.org/abs/2404.18594v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 10:59:41","title":"Higher-spin self-dual General Relativity: 6d and 4d pictures, covariant vs. lightcone","abstract":"We study the higher-spin extension of self-dual General Relativity (GR) with cosmological constant, proposed by Krasnov, Skvortsov and Tran. We show that this theory is actually a gauge-fixing of a 6d diffeomorphism-invariant Abelian theory, living on (4d spacetime)x(2d spinor space) modulo a finite group. On the other hand, we point out that the theory respects the 4d geometry of a self-dual GR solution, with no backreaction from the higher-spin fields. We also present a lightcone ansatz that reduces the covariant fields to one scalar field for each helicity. The field equations governing these scalars have only cubic vertices. We compare our lightcone ansatz to Metsaev's lightcone formalism. We conclude with a new perspective on the lightcone formalism in (A)dS spacetime: not merely a complication of its Minkowski-space cousin, it has a built-in Lorentz covariance, and is closely related to Vasiliev's concept of unfolding.","sentences":["We study the higher-spin extension of self-dual General Relativity (GR) with cosmological constant, proposed by Krasnov, Skvortsov and Tran.","We show that this theory is actually a gauge-fixing of a 6d diffeomorphism-invariant Abelian theory, living on (4d spacetime)x(2d spinor space) modulo a finite group.","On the other hand, we point out that the theory respects the 4d geometry of a self-dual GR solution, with no backreaction from the higher-spin fields.","We also present a lightcone ansatz that reduces the covariant fields to one scalar field for each helicity.","The field equations governing these scalars have only cubic vertices.","We compare our lightcone ansatz to Metsaev's lightcone formalism.","We conclude with a new perspective on the lightcone formalism in (A)dS spacetime: not merely a complication of its Minkowski-space cousin, it has a built-in Lorentz covariance, and is closely related to Vasiliev's concept of unfolding."],"url":"http://arxiv.org/abs/2404.18589v1","category":"hep-th"}
{"created":"2024-04-29 10:58:41","title":"Unlocking Potentials of Near-Field Propagation: ELAA-Empowered Integrated Sensing and Communication","abstract":"The exploration of extremely large antenna arrays (ELAAs) using high-frequency spectrum has led to a paradigm shift in electromagnetic radiation field, transitioning from the common use case of far-field propagation to near-field propagation. This shift necessitates the modification of the conventional planar-wavefront approximation to more accurate spherical waves, exerting a profound impact on wireless transmission technologies encompassing communication and sensing. Concurrently, integrated sensing and communication (ISAC) has gained prominence in the context of the sixth-generation (6G) wireless networks owing to its ability to cater to the ever-increasing demands of future networks. In line with this evolving trend, this article presents a systematical investigation on ELAA-empowered near-field ISAC. We begin by introducing the fundamentals of near-field propagation with an emphasis on its double-edged effects to near-field communications. Then, we turn to near-field sensing and expound upon various typical applications. Following the separate elaborations on communications and sensing, we articulate in-depth advantages of ELAA-empowered ISAC in near field, particularly including featured opportunities arising from the dual-functional integrations, potential ISAC applications benefiting from the additional degrees-of-freedom in near field, and enablements of other complementary technologies. Finally, we outline key technical challenges that merit further exploration in the realm of ELAA-empowered near-field ISAC.","sentences":["The exploration of extremely large antenna arrays (ELAAs) using high-frequency spectrum has led to a paradigm shift in electromagnetic radiation field, transitioning from the common use case of far-field propagation to near-field propagation.","This shift necessitates the modification of the conventional planar-wavefront approximation to more accurate spherical waves, exerting a profound impact on wireless transmission technologies encompassing communication and sensing.","Concurrently, integrated sensing and communication (ISAC) has gained prominence in the context of the sixth-generation (6G) wireless networks owing to its ability to cater to the ever-increasing demands of future networks.","In line with this evolving trend, this article presents a systematical investigation on ELAA-empowered near-field ISAC.","We begin by introducing the fundamentals of near-field propagation with an emphasis on its double-edged effects to near-field communications.","Then, we turn to near-field sensing and expound upon various typical applications.","Following the separate elaborations on communications and sensing, we articulate in-depth advantages of ELAA-empowered ISAC in near field, particularly including featured opportunities arising from the dual-functional integrations, potential ISAC applications benefiting from the additional degrees-of-freedom in near field, and enablements of other complementary technologies.","Finally, we outline key technical challenges that merit further exploration in the realm of ELAA-empowered near-field ISAC."],"url":"http://arxiv.org/abs/2404.18587v1","category":"cs.IT"}
{"created":"2024-04-29 10:47:51","title":"Controller Synthesis in Timed B\u00fcchi Automata: Robustness and Punctual Guards","abstract":"We consider the synthesis problem on timed automata with B\\\"uchi objectives, where delay choices made by a controller are subjected to small perturbations. Usually, the controller needs to avoid punctual guards, such as testing the equality of a clock to a constant. In this work, we generalize to a robustness setting that allows for punctual transitions in the automaton to be taken by controller with no perturbation. In order to characterize cycles that resist perturbations in our setting, we introduce a new structural requirement on the reachability relation along an accepting cycle of the automaton. This property is formulated on the region abstraction, and generalizes the existing characterization of winning cycles in the absence of punctual guards. We show that the problem remains within PSPACE despite the presence of punctual guards.","sentences":["We consider the synthesis problem on timed automata with B\\\"uchi objectives, where delay choices made by a controller are subjected to small perturbations.","Usually, the controller needs to avoid punctual guards, such as testing the equality of a clock to a constant.","In this work, we generalize to a robustness setting that allows for punctual transitions in the automaton to be taken by controller with no perturbation.","In order to characterize cycles that resist perturbations in our setting, we introduce a new structural requirement on the reachability relation along an accepting cycle of the automaton.","This property is formulated on the region abstraction, and generalizes the existing characterization of winning cycles in the absence of punctual guards.","We show that the problem remains within PSPACE despite the presence of punctual guards."],"url":"http://arxiv.org/abs/2404.18584v1","category":"cs.GT"}
{"created":"2024-04-29 10:47:37","title":"Context Matters: Leveraging Spatiotemporal Metadata for Semi-Supervised Learning on Remote Sensing Images","abstract":"Remote sensing projects typically generate large amounts of imagery that can be used to train powerful deep neural networks. However, the amount of labeled images is often small, as remote sensing applications generally require expert labelers. Thus, semi-supervised learning (SSL), i.e., learning with a small pool of labeled and a larger pool of unlabeled data, is particularly useful in this domain. Current SSL approaches generate pseudo-labels from model predictions for unlabeled samples. As the quality of these pseudo-labels is crucial for performance, utilizing additional information to improve pseudo-label quality yields a promising direction. For remote sensing images, geolocation and recording time are generally available and provide a valuable source of information as semantic concepts, such as land cover, are highly dependent on spatiotemporal context, e.g., due to seasonal effects and vegetation zones. In this paper, we propose to exploit spatiotemporal metainformation in SSL to improve the quality of pseudo-labels and, therefore, the final model performance. We show that directly adding the available metadata to the input of the predictor at test time degenerates the prediction quality for metadata outside the spatiotemporal distribution of the training set. Thus, we propose a teacher-student SSL framework where only the teacher network uses metainformation to improve the quality of pseudo-labels on the training set. Correspondingly, our student network benefits from the improved pseudo-labels but does not receive metadata as input, making it invariant to spatiotemporal shifts at test time. Furthermore, we propose methods for encoding and injecting spatiotemporal information into the model and introduce a novel distillation mechanism to enhance the knowledge transfer between teacher and student. Our framework dubbed Spatiotemporal SSL can be easily combined with several stat...","sentences":["Remote sensing projects typically generate large amounts of imagery that can be used to train powerful deep neural networks.","However, the amount of labeled images is often small, as remote sensing applications generally require expert labelers.","Thus, semi-supervised learning (SSL), i.e., learning with a small pool of labeled and a larger pool of unlabeled data, is particularly useful in this domain.","Current SSL approaches generate pseudo-labels from model predictions for unlabeled samples.","As the quality of these pseudo-labels is crucial for performance, utilizing additional information to improve pseudo-label quality yields a promising direction.","For remote sensing images, geolocation and recording time are generally available and provide a valuable source of information as semantic concepts, such as land cover, are highly dependent on spatiotemporal context, e.g., due to seasonal effects and vegetation zones.","In this paper, we propose to exploit spatiotemporal metainformation in SSL to improve the quality of pseudo-labels and, therefore, the final model performance.","We show that directly adding the available metadata to the input of the predictor at test time degenerates the prediction quality for metadata outside the spatiotemporal distribution of the training set.","Thus, we propose a teacher-student SSL framework where only the teacher network uses metainformation to improve the quality of pseudo-labels on the training set.","Correspondingly, our student network benefits from the improved pseudo-labels but does not receive metadata as input, making it invariant to spatiotemporal shifts at test time.","Furthermore, we propose methods for encoding and injecting spatiotemporal information into the model and introduce a novel distillation mechanism to enhance the knowledge transfer between teacher and student.","Our framework dubbed Spatiotemporal SSL can be easily combined with several stat..."],"url":"http://arxiv.org/abs/2404.18583v1","category":"cs.CV"}
{"created":"2024-04-29 10:45:14","title":"Dressing vs. Fixing: On How to Extract and Interpret Gauge-Invariant Content","abstract":"There is solid consensus among physicists and philosophers that, in gauge field theory, for a quantity to be physically meaningful or real, it must be gauge-invariant. Yet, every ``elementary\" field in the Standard Model of particle physics is actually gauge-variant. This has led a number of researchers to insist that new manifestly gauge-invariant approaches must be established. Indeed, in the foundational literature, dissatisfaction with standard methods for reducing gauge symmetries has been expressed: Spontaneous symmetry breaking is deemed conceptually dubious, while gauge fixing suffers the same limitations and is subject to the same criticisms as coordinate choices in General Relativity.   An alternative gauge-invariant proposal was recently introduced in the literature, the so-called ``dressing field method\" (DFM). It is a mathematically subtle tool, and unfortunately prone to be confused with simple gauge transformations, hence with standard gauge~fixings. As a matter of fact, in the physics literature the two are often conflated, and in the philosophy community some doubts have been raised about whether there is any substantial difference between them. Clarifying this issue is of special significance for anyone interested in both the foundational issues of gauge theories and their invariant formulation. It is thus our objective to establish as precisely as possible the technical and conceptual distinctions between the DFM and gauge fixing.","sentences":["There is solid consensus among physicists and philosophers that, in gauge field theory, for a quantity to be physically meaningful or real, it must be gauge-invariant.","Yet, every ``elementary\" field in the Standard Model of particle physics is actually gauge-variant.","This has led a number of researchers to insist that new manifestly gauge-invariant approaches must be established.","Indeed, in the foundational literature, dissatisfaction with standard methods for reducing gauge symmetries has been expressed: Spontaneous symmetry breaking is deemed conceptually dubious, while gauge fixing suffers the same limitations and is subject to the same criticisms as coordinate choices in General Relativity.   ","An alternative gauge-invariant proposal was recently introduced in the literature, the so-called ``dressing field method\" (DFM).","It is a mathematically subtle tool, and unfortunately prone to be confused with simple gauge transformations, hence with standard gauge~fixings.","As a matter of fact, in the physics literature the two are often conflated, and in the philosophy community some doubts have been raised about whether there is any substantial difference between them.","Clarifying this issue is of special significance for anyone interested in both the foundational issues of gauge theories and their invariant formulation.","It is thus our objective to establish as precisely as possible the technical and conceptual distinctions between the DFM and gauge fixing."],"url":"http://arxiv.org/abs/2404.18582v1","category":"physics.hist-ph"}
{"created":"2024-04-29 10:41:15","title":"Dark energy in light of recent DESI BAO and Hubble tension","abstract":"It might be necessary to access the impact of recent released Dark Energy Spectroscopic Instrument (DESI) data on the state equation of dark energy in the Hubble-tension-free cosmologies. In this paper, we report the MCMC analysis for the $w_0w_a$CDM model with pre-recombination early dark energy (EDE) using Planck+DESI+Pantheon+SH0ES dataset. The results reveal that $H_0=72.43\\pm 0.75$, $w_0=-0.971\\pm0.035$, $w_a=-0.347\\pm0.120$ for axion-like EDE, while $H_0=73.32\\pm 0.64$, $w_0=-0.923\\pm0.054$, $w_a=-0.435\\pm0.132$ for AdS-EDE, and also not surprisingly the spectral index of primordial scalar perturbation moves towards $n_s=1$. It looks like that in corresponding models the bestfit $H_0$ are higher than those with pre-DESI BAO data, but without the further exacerbation of $S_8$ tension.","sentences":["It might be necessary to access the impact of recent released Dark Energy Spectroscopic Instrument (DESI) data on the state equation of dark energy in the Hubble-tension-free cosmologies.","In this paper, we report the MCMC analysis for the $w_0w_a$CDM model with pre-recombination early dark energy (EDE) using Planck+DESI+Pantheon+SH0ES dataset.","The results reveal that $H_0=72.43\\pm 0.75$, $w_0=-0.971\\pm0.035$, $w_a=-0.347\\pm0.120$ for axion-like EDE, while $H_0=73.32\\pm 0.64$, $w_0=-0.923\\pm0.054$, $w_a=-0.435\\pm0.132$ for AdS-EDE, and also not surprisingly the spectral index of primordial scalar perturbation moves towards $n_s=1$. It looks like that in corresponding models the bestfit $H_0$ are higher than those with pre-DESI BAO data, but without the further exacerbation of $S_8$ tension."],"url":"http://arxiv.org/abs/2404.18579v1","category":"astro-ph.CO"}
{"created":"2024-04-29 10:37:38","title":"Assessing Quality Metrics for Neural Reality Gap Input Mitigation in Autonomous Driving Testing","abstract":"Simulation-based testing of automated driving systems (ADS) is the industry standard, being a controlled, safe, and cost-effective alternative to real-world testing. Despite these advantages, virtual simulations often fail to accurately replicate real-world conditions like image fidelity, texture representation, and environmental accuracy. This can lead to significant differences in ADS behavior between simulated and real-world domains, a phenomenon known as the sim2real gap. Researchers have used Image-to-Image (I2I) neural translation to mitigate the sim2real gap, enhancing the realism of simulated environments by transforming synthetic data into more authentic representations of real-world conditions. However, while promising, these techniques may potentially introduce artifacts, distortions, or inconsistencies in the generated data that can affect the effectiveness of ADS testing. In our empirical study, we investigated how the quality of image-to-image (I2I) techniques influences the mitigation of the sim2real gap, using a set of established metrics from the literature. We evaluated two popular generative I2I architectures, pix2pix, and CycleGAN, across two ADS perception tasks at a model level, namely vehicle detection and end-to-end lane keeping, using paired simulated and real-world datasets. Our findings reveal that the effectiveness of I2I architectures varies across different ADS tasks, and existing evaluation metrics do not consistently align with the ADS behavior. Thus, we conducted task-specific fine-tuning of perception metrics, which yielded a stronger correlation. Our findings indicate that a perception metric that incorporates semantic elements, tailored to each task, can facilitate selecting the most appropriate I2I technique for a reliable assessment of the sim2real gap mitigation.","sentences":["Simulation-based testing of automated driving systems (ADS) is the industry standard, being a controlled, safe, and cost-effective alternative to real-world testing.","Despite these advantages, virtual simulations often fail to accurately replicate real-world conditions like image fidelity, texture representation, and environmental accuracy.","This can lead to significant differences in ADS behavior between simulated and real-world domains, a phenomenon known as the sim2real gap.","Researchers have used Image-to-Image (I2I) neural translation to mitigate the sim2real gap, enhancing the realism of simulated environments by transforming synthetic data into more authentic representations of real-world conditions.","However, while promising, these techniques may potentially introduce artifacts, distortions, or inconsistencies in the generated data that can affect the effectiveness of ADS testing.","In our empirical study, we investigated how the quality of image-to-image (I2I) techniques influences the mitigation of the sim2real gap, using a set of established metrics from the literature.","We evaluated two popular generative I2I architectures, pix2pix, and CycleGAN, across two ADS perception tasks at a model level, namely vehicle detection and end-to-end lane keeping, using paired simulated and real-world datasets.","Our findings reveal that the effectiveness of I2I architectures varies across different ADS tasks, and existing evaluation metrics do not consistently align with the ADS behavior.","Thus, we conducted task-specific fine-tuning of perception metrics, which yielded a stronger correlation.","Our findings indicate that a perception metric that incorporates semantic elements, tailored to each task, can facilitate selecting the most appropriate I2I technique for a reliable assessment of the sim2real gap mitigation."],"url":"http://arxiv.org/abs/2404.18577v1","category":"cs.SE"}
{"created":"2024-04-29 10:31:57","title":"GEEvo: Game Economy Generation and Balancing with Evolutionary Algorithms","abstract":"Game economy design significantly shapes the player experience and progression speed. Modern game economies are becoming increasingly complex and can be very sensitive to even minor numerical adjustments, which may have an unexpected impact on the overall gaming experience. Consequently, thorough manual testing and fine-tuning during development are essential. Unlike existing works that address algorithmic balancing for specific games or genres, this work adopts a more abstract approach, focusing on game balancing through its economy, detached from a specific game. We propose GEEvo (Game Economy Evolution), a framework to generate graph-based game economies and balancing both, newly generated or existing economies. GEEvo uses a two-step approach where evolutionary algorithms are used to first generate an economy and then balance it based on specified objectives, such as generated resources or damage dealt over time. We define different objectives by differently parameterizing the fitness function using data from multiple simulation runs of the economy. To support this, we define a lightweight and flexible game economy simulation framework. Our method is tested and benchmarked with various balancing objectives on a generated dataset, and we conduct a case study evaluating damage balancing for two fictional economies of two popular game character classes.","sentences":["Game economy design significantly shapes the player experience and progression speed.","Modern game economies are becoming increasingly complex and can be very sensitive to even minor numerical adjustments, which may have an unexpected impact on the overall gaming experience.","Consequently, thorough manual testing and fine-tuning during development are essential.","Unlike existing works that address algorithmic balancing for specific games or genres, this work adopts a more abstract approach, focusing on game balancing through its economy, detached from a specific game.","We propose GEEvo (Game Economy Evolution), a framework to generate graph-based game economies and balancing both, newly generated or existing economies.","GEEvo uses a two-step approach where evolutionary algorithms are used to first generate an economy and then balance it based on specified objectives, such as generated resources or damage dealt over time.","We define different objectives by differently parameterizing the fitness function using data from multiple simulation runs of the economy.","To support this, we define a lightweight and flexible game economy simulation framework.","Our method is tested and benchmarked with various balancing objectives on a generated dataset, and we conduct a case study evaluating damage balancing for two fictional economies of two popular game character classes."],"url":"http://arxiv.org/abs/2404.18574v1","category":"cs.NE"}
{"created":"2024-04-29 10:20:21","title":"Exponential Convergence of $hp$-ILGFEM for semilinear elliptic boundary value problems with monomial reaction","abstract":"We study the fully explicit numerical approximation of a semilinear elliptic boundary value model problem, which features a monomial reaction and analytic forcing, in a bounded polygon $\\Omega\\subset\\mathbb{R}^2$ with a finite number of straight edges. In particular, we analyze the convergence of $hp$-type iterative linearized Galerkin ($hp$-ILG) solvers. Our convergence analysis is carried out for conforming $hp$-finite element (FE) Galerkin discretizations on sequences of regular, simplicial partitions of $\\Omega$, with geometric corner refinement, with polynomial degrees increasing in sync with the geometric mesh refinement towards the corners of $\\Omega$. For a sequence of discrete solutions generated by the ILG solver, with a stopping criterion that is consistent with the exponential convergence of the exact $hp$-FE Galerkin solution, we prove exponential convergence in $\\mathrm{H}^1(\\Omega)$ to the unique weak solution of the boundary value problem. Numerical experiments illustrate the exponential convergence of the numerical approximations obtained from the proposed scheme in terms of the number of degrees of freedom as well as of the computational complexity involved.","sentences":["We study the fully explicit numerical approximation of a semilinear elliptic boundary value model problem, which features a monomial reaction and analytic forcing, in a bounded polygon $\\Omega\\subset\\mathbb{R}^2$ with a finite number of straight edges.","In particular, we analyze the convergence of $hp$-type iterative linearized Galerkin ($hp$-ILG) solvers.","Our convergence analysis is carried out for conforming $hp$-finite element (FE) Galerkin discretizations on sequences of regular, simplicial partitions of $\\Omega$, with geometric corner refinement, with polynomial degrees increasing in sync with the geometric mesh refinement towards the corners of $\\Omega$. For a sequence of discrete solutions generated by the ILG solver, with a stopping criterion that is consistent with the exponential convergence of the exact $hp$-FE Galerkin solution, we prove exponential convergence in $\\mathrm{H}^1(\\Omega)$ to the unique weak solution of the boundary value problem.","Numerical experiments illustrate the exponential convergence of the numerical approximations obtained from the proposed scheme in terms of the number of degrees of freedom as well as of the computational complexity involved."],"url":"http://arxiv.org/abs/2404.18569v1","category":"math.NA"}
{"created":"2024-04-29 10:14:58","title":"Assessing Cybersecurity Vulnerabilities in Code Large Language Models","abstract":"Instruction-tuned Code Large Language Models (Code LLMs) are increasingly utilized as AI coding assistants and integrated into various applications. However, the cybersecurity vulnerabilities and implications arising from the widespread integration of these models are not yet fully understood due to limited research in this domain. To bridge this gap, this paper presents EvilInstructCoder, a framework specifically designed to assess the cybersecurity vulnerabilities of instruction-tuned Code LLMs to adversarial attacks. EvilInstructCoder introduces the Adversarial Code Injection Engine to automatically generate malicious code snippets and inject them into benign code to poison instruction tuning datasets. It incorporates practical threat models to reflect real-world adversaries with varying capabilities and evaluates the exploitability of instruction-tuned Code LLMs under these diverse adversarial attack scenarios. Through the use of EvilInstructCoder, we conduct a comprehensive investigation into the exploitability of instruction tuning for coding tasks using three state-of-the-art Code LLM models: CodeLlama, DeepSeek-Coder, and StarCoder2, under various adversarial attack scenarios. Our experimental results reveal a significant vulnerability in these models, demonstrating that adversaries can manipulate the models to generate malicious payloads within benign code contexts in response to natural language instructions. For instance, under the backdoor attack setting, by poisoning only 81 samples (0.5\\% of the entire instruction dataset), we achieve Attack Success Rate at 1 (ASR@1) scores ranging from 76\\% to 86\\% for different model families. Our study sheds light on the critical cybersecurity vulnerabilities posed by instruction-tuned Code LLMs and emphasizes the urgent necessity for robust defense mechanisms to mitigate the identified vulnerabilities.","sentences":["Instruction-tuned Code Large Language Models (Code LLMs) are increasingly utilized as AI coding assistants and integrated into various applications.","However, the cybersecurity vulnerabilities and implications arising from the widespread integration of these models are not yet fully understood due to limited research in this domain.","To bridge this gap, this paper presents EvilInstructCoder, a framework specifically designed to assess the cybersecurity vulnerabilities of instruction-tuned Code LLMs to adversarial attacks.","EvilInstructCoder introduces the Adversarial Code Injection Engine to automatically generate malicious code snippets and inject them into benign code to poison instruction tuning datasets.","It incorporates practical threat models to reflect real-world adversaries with varying capabilities and evaluates the exploitability of instruction-tuned Code LLMs under these diverse adversarial attack scenarios.","Through the use of EvilInstructCoder, we conduct a comprehensive investigation into the exploitability of instruction tuning for coding tasks using three state-of-the-art Code LLM models: CodeLlama, DeepSeek-Coder, and StarCoder2, under various adversarial attack scenarios.","Our experimental results reveal a significant vulnerability in these models, demonstrating that adversaries can manipulate the models to generate malicious payloads within benign code contexts in response to natural language instructions.","For instance, under the backdoor attack setting, by poisoning only 81 samples (0.5\\% of the entire instruction dataset), we achieve Attack Success Rate at 1 (ASR@1) scores ranging from 76\\% to 86\\% for different model families.","Our study sheds light on the critical cybersecurity vulnerabilities posed by instruction-tuned Code LLMs and emphasizes the urgent necessity for robust defense mechanisms to mitigate the identified vulnerabilities."],"url":"http://arxiv.org/abs/2404.18567v1","category":"cs.CR"}
{"created":"2024-04-29 10:13:49","title":"Pre-relaxation in quantum, classical, and quantum-classical two-impurity models","abstract":"We numerically study the relaxation dynamics of impurity-host systems, focusing on the presence of long-lived metastable states in the non-equilibrium dynamics after an initial excitation of the impurities. In generic systems, an excited impurity coupled to a large bath at zero temperature is expected to relax and approach its ground state over time. However, certain exceptional cases exhibit metastability, where the system remains in an excited state on timescales largely exceeding the typical relaxation time. We study this phenomenon for three prototypical impurity models: a tight-binding quantum model of independent spinless fermions on a lattice with two stub impurities, a classical-spin Heisenberg model with two weakly coupled classical impurity spins, and a tight-binding quantum model of independent electrons with two classical impurity spins. Through numerical integration of the fundamental equations of motion, we find that all three models exhibit similar qualitative behavior: complete relaxation for nearest-neighbor impurities and incomplete or strongly delayed relaxation for next-nearest-neighbor impurities. The underlying mechanisms leading to this behavior differ between models and include impurity-induced bound states, emergent approximately conserved local observables, and exact cancellation of local and nonlocal dissipation effects.","sentences":["We numerically study the relaxation dynamics of impurity-host systems, focusing on the presence of long-lived metastable states in the non-equilibrium dynamics after an initial excitation of the impurities.","In generic systems, an excited impurity coupled to a large bath at zero temperature is expected to relax and approach its ground state over time.","However, certain exceptional cases exhibit metastability, where the system remains in an excited state on timescales largely exceeding the typical relaxation time.","We study this phenomenon for three prototypical impurity models: a tight-binding quantum model of independent spinless fermions on a lattice with two stub impurities, a classical-spin Heisenberg model with two weakly coupled classical impurity spins, and a tight-binding quantum model of independent electrons with two classical impurity spins.","Through numerical integration of the fundamental equations of motion, we find that all three models exhibit similar qualitative behavior: complete relaxation for nearest-neighbor impurities and incomplete or strongly delayed relaxation for next-nearest-neighbor impurities.","The underlying mechanisms leading to this behavior differ between models and include impurity-induced bound states, emergent approximately conserved local observables, and exact cancellation of local and nonlocal dissipation effects."],"url":"http://arxiv.org/abs/2404.18566v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 10:12:04","title":"Injecting Salesperson's Dialogue Strategies in Large Language Models with Chain-of-Thought Reasoning","abstract":"Recent research in dialogue systems and corpora has focused on two main categories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD systems help users accomplish specific tasks, while open-domain systems aim to create engaging conversations. However, in real-world scenarios, user intents are often revealed during interactions. A recent study introduced SalesBot, which simulates dialogues transitioning from chit-chat to task-oriented scenarios to train sales agents. Unfortunately, the initial data lacked smooth transitions and coherent long-turn dialogues, resulting in poor naturalness in sales-customer interactions. To address these issues, this paper presents SalesBot 2.0, an improved dataset. It leverages commonsense knowledge from large language models (LLMs) through strategic prompting. Additionally, we introduce a novel model called SalesAgent, trained on salesperson's interactions, using chain-of-thought (CoT) reasoning. This model excels in transitioning topics, understanding user intents, and selecting appropriate strategies. Experiments using diverse user simulations validate the effectiveness of our method in controlling dialogue strategies in LLMs. Furthermore, SalesBot 2.0 enhances coherence and reduces aggression, facilitating better model learning for sales-customer interactions.","sentences":["Recent research in dialogue systems and corpora has focused on two main categories: task-oriented (TOD) and open-domain (chit-chat) dialogues.","TOD systems help users accomplish specific tasks, while open-domain systems aim to create engaging conversations.","However, in real-world scenarios, user intents are often revealed during interactions.","A recent study introduced SalesBot, which simulates dialogues transitioning from chit-chat to task-oriented scenarios to train sales agents.","Unfortunately, the initial data lacked smooth transitions and coherent long-turn dialogues, resulting in poor naturalness in sales-customer interactions.","To address these issues, this paper presents SalesBot 2.0, an improved dataset.","It leverages commonsense knowledge from large language models (LLMs) through strategic prompting.","Additionally, we introduce a novel model called SalesAgent, trained on salesperson's interactions, using chain-of-thought (CoT) reasoning.","This model excels in transitioning topics, understanding user intents, and selecting appropriate strategies.","Experiments using diverse user simulations validate the effectiveness of our method in controlling dialogue strategies in LLMs.","Furthermore, SalesBot 2.0 enhances coherence and reduces aggression, facilitating better model learning for sales-customer interactions."],"url":"http://arxiv.org/abs/2404.18564v1","category":"cs.CL"}
{"created":"2024-04-29 10:02:45","title":"LangBiTe: A Platform for Testing Bias in Large Language Models","abstract":"The integration of Large Language Models (LLMs) into various software applications raises concerns about their potential biases. Typically, those models are trained on a vast amount of data scrapped from forums, websites, social media and other internet sources, which may instill harmful and discriminating behavior into the model. To address this issue, we present LangBiTe, a testing platform to systematically assess the presence of biases within an LLM. LangBiTe enables development teams to tailor their test scenarios, and automatically generate and execute the test cases according to a set of user-defined ethical requirements. Each test consists of a prompt fed into the LLM and a corresponding test oracle that scrutinizes the LLM's response for the identification of biases. LangBite provides users with the bias evaluation of LLMs, and end-to-end traceability between the initial ethical requirements and the insights obtained.","sentences":["The integration of Large Language Models (LLMs) into various software applications raises concerns about their potential biases.","Typically, those models are trained on a vast amount of data scrapped from forums, websites, social media and other internet sources, which may instill harmful and discriminating behavior into the model.","To address this issue, we present LangBiTe, a testing platform to systematically assess the presence of biases within an LLM.","LangBiTe enables development teams to tailor their test scenarios, and automatically generate and execute the test cases according to a set of user-defined ethical requirements.","Each test consists of a prompt fed into the LLM and a corresponding test oracle that scrutinizes the LLM's response for the identification of biases.","LangBite provides users with the bias evaluation of LLMs, and end-to-end traceability between the initial ethical requirements and the insights obtained."],"url":"http://arxiv.org/abs/2404.18558v1","category":"cs.SE"}
{"created":"2024-04-29 09:56:32","title":"Doubly Adaptive Importance Sampling","abstract":"We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors. Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling. The amount of damping in the updates adapts to the posterior and guarantees the effective sample size. Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling. The result is a generic, embarrassingly parallel and adaptive posterior approximation method. Numerical studies on simulated and real data show its competitiveness with other, less general methods.","sentences":["We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors.","Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow.","Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling.","The amount of damping in the updates adapts to the posterior and guarantees the effective sample size.","Gaussianity enables the use of Stein's lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling.","The result is a generic, embarrassingly parallel and adaptive posterior approximation method.","Numerical studies on simulated and real data show its competitiveness with other, less general methods."],"url":"http://arxiv.org/abs/2404.18556v1","category":"stat.CO"}
{"created":"2024-04-29 09:54:06","title":"Machine Learning for Quantum Computing Specialists","abstract":"Quantum machine learning (QML) is a promising early use case for quantum computing. There has been progress in the last five years from theoretical studies and numerical simulations to proof of concepts. Use cases demonstrated on contemporary quantum devices include classifying medical images and items from the Iris dataset, classifying and generating handwritten images, toxicity screening, and learning a probability distribution. Potential benefits of QML include faster training and identification of feature maps not found classically. Although, these examples lack the scale for commercial exploitation, and it may be several years before QML algorithms replace the classical solutions, QML is an exciting area.   This article is written for those who already have a sound knowledge of quantum computing and now wish to gain a basic overview of the terminology and some applications of classical machine learning ready to study quantum machine learning. The reader will already understand the relevant relevant linear algebra, including Hilbert spaces, a vector space with an inner product.","sentences":["Quantum machine learning (QML) is a promising early use case for quantum computing.","There has been progress in the last five years from theoretical studies and numerical simulations to proof of concepts.","Use cases demonstrated on contemporary quantum devices include classifying medical images and items from the Iris dataset, classifying and generating handwritten images, toxicity screening, and learning a probability distribution.","Potential benefits of QML include faster training and identification of feature maps not found classically.","Although, these examples lack the scale for commercial exploitation, and it may be several years before QML algorithms replace the classical solutions, QML is an exciting area.   ","This article is written for those who already have a sound knowledge of quantum computing and now wish to gain a basic overview of the terminology and some applications of classical machine learning ready to study quantum machine learning.","The reader will already understand the relevant relevant linear algebra, including Hilbert spaces, a vector space with an inner product."],"url":"http://arxiv.org/abs/2404.18555v1","category":"quant-ph"}
{"created":"2024-04-29 09:53:26","title":"Triality over Schemes","abstract":"Working over an arbitrary base scheme, we provide an alternative development of triality which does not use Octonion algebras or symmetric composition algebras. Instead, we use the Clifford algebra of the split hyperbolic quadratic form of rank 8 and computations with Chevalley generators of groups of type $D_4$. Following the strategy of The Book of Involutions [KMRT], we then define the stack of trialitarian triples and show it is equivalent to the gerbe of $\\mathbf{PGO}_8^+$--torsors. We show it has endomorphisms generating a group isomorphic to $\\mathbb{S}_3$ and that several familiar cohomological properties of $\\mathbf{PGO}_8^+$ follow in this setting as a result. Next, we define the stack of trialitarian algebras and show it is equivalent to the gerbe of $\\mathbf{PGO}_8^+\\rtimes \\mathbb{S}_3$--torsors. Because of this, it is also equivalent to the gerbes of simply connected, respectively adjoint, groups of type $D_4$. We define $\\mathbf{Spin}_\\mathcal{T}$ and $\\mathbf{PGO}^+_\\mathcal{T}$ for a trialitarian algebra and define concrete functors $\\mathcal{T} \\mapsto \\mathbf{Spin}_\\mathcal{T}$ and $\\mathcal{T} \\mapsto \\mathbf{PGO}^+_\\mathcal{T}$ which realize these equivalences.","sentences":["Working over an arbitrary base scheme, we provide an alternative development of triality which does not use Octonion algebras or symmetric composition algebras.","Instead, we use the Clifford algebra of the split hyperbolic quadratic form of rank 8 and computations with Chevalley generators of groups of type $D_4$. Following the strategy of The Book of Involutions [KMRT], we then define the stack of trialitarian triples and show it is equivalent to the gerbe of $\\mathbf{PGO}_8^+$--torsors.","We show it has endomorphisms generating a group isomorphic to $\\mathbb{S}_3$ and that several familiar cohomological properties of $\\mathbf{PGO}_8^+$ follow in this setting as a result.","Next, we define the stack of trialitarian algebras and show it is equivalent to the gerbe of $\\mathbf{PGO}_8^+\\rtimes \\mathbb{S}_3$--torsors.","Because of this, it is also equivalent to the gerbes of simply connected, respectively adjoint, groups of type $D_4$. We define $\\mathbf{Spin}_\\mathcal{T}$ and $\\mathbf{PGO}^+_\\mathcal{T}$ for a trialitarian algebra and define concrete functors $\\mathcal{T} \\mapsto \\mathbf{Spin}_\\mathcal{T}$ and $\\mathcal{T} \\mapsto \\mathbf{PGO}^+_\\mathcal{T}$ which realize these equivalences."],"url":"http://arxiv.org/abs/2404.18554v1","category":"math.AG"}
{"created":"2024-04-29 09:51:25","title":"Evaluating the effectiveness of predicting covariates in LSTM Networks for Time Series Forecasting","abstract":"Autoregressive Recurrent Neural Networks are widely employed in time-series forecasting tasks, demonstrating effectiveness in univariate and certain multivariate scenarios. However, their inherent structure does not readily accommodate the integration of future, time-dependent covariates. A proposed solution, outlined by Salinas et al 2019, suggests forecasting both covariates and the target variable in a multivariate framework. In this study, we conducted comprehensive tests on publicly available time-series datasets, artificially introducing highly correlated covariates to future time-step values. Our evaluation aimed to assess the performance of an LSTM network when considering these covariates and compare it against a univariate baseline. As part of this study we introduce a novel approach using seasonal time segments in combination with an RNN architecture, which is both simple and extremely effective over long forecast horizons with comparable performance to many state of the art architectures. Our findings from the results of more than 120 models reveal that under certain conditions jointly training covariates with target variables can improve overall performance of the model, but often there exists a significant performance disparity between multivariate and univariate predictions. Surprisingly, even when provided with covariates informing the network about future target values, multivariate predictions exhibited inferior performance. In essence, compelling the network to predict multiple values can prove detrimental to model performance, even in the presence of informative covariates. These results suggest that LSTM architectures may not be suitable for forecasting tasks where predicting covariates would typically be expected to enhance model accuracy.","sentences":["Autoregressive Recurrent Neural Networks are widely employed in time-series forecasting tasks, demonstrating effectiveness in univariate and certain multivariate scenarios.","However, their inherent structure does not readily accommodate the integration of future, time-dependent covariates.","A proposed solution, outlined by Salinas et al 2019, suggests forecasting both covariates and the target variable in a multivariate framework.","In this study, we conducted comprehensive tests on publicly available time-series datasets, artificially introducing highly correlated covariates to future time-step values.","Our evaluation aimed to assess the performance of an LSTM network when considering these covariates and compare it against a univariate baseline.","As part of this study we introduce a novel approach using seasonal time segments in combination with an RNN architecture, which is both simple and extremely effective over long forecast horizons with comparable performance to many state of the art architectures.","Our findings from the results of more than 120 models reveal that under certain conditions jointly training covariates with target variables can improve overall performance of the model, but often there exists a significant performance disparity between multivariate and univariate predictions.","Surprisingly, even when provided with covariates informing the network about future target values, multivariate predictions exhibited inferior performance.","In essence, compelling the network to predict multiple values can prove detrimental to model performance, even in the presence of informative covariates.","These results suggest that LSTM architectures may not be suitable for forecasting tasks where predicting covariates would typically be expected to enhance model accuracy."],"url":"http://arxiv.org/abs/2404.18553v1","category":"cs.LG"}
{"created":"2024-04-29 09:50:16","title":"SIDBench: A Python Framework for Reliably Assessing Synthetic Image Detection Methods","abstract":"The generative AI technology offers an increasing variety of tools for generating entirely synthetic images that are increasingly indistinguishable from real ones. Unlike methods that alter portions of an image, the creation of completely synthetic images presents a unique challenge and several Synthetic Image Detection (SID) methods have recently appeared to tackle it. Yet, there is often a large gap between experimental results on benchmark datasets and the performance of methods in the wild. To better address the evaluation needs of SID and help close this gap, this paper introduces a benchmarking framework that integrates several state-of-the-art SID models. Our selection of integrated models was based on the utilization of varied input features, and different network architectures, aiming to encompass a broad spectrum of techniques. The framework leverages recent datasets with a diverse set of generative models, high level of photo-realism and resolution, reflecting the rapid improvements in image synthesis technology. Additionally, the framework enables the study of how image transformations, common in assets shared online, such as JPEG compression, affect detection performance. SIDBench is available on https://github.com/mever-team/sidbench and is designed in a modular manner to enable easy inclusion of new datasets and SID models.","sentences":["The generative AI technology offers an increasing variety of tools for generating entirely synthetic images that are increasingly indistinguishable from real ones.","Unlike methods that alter portions of an image, the creation of completely synthetic images presents a unique challenge and several Synthetic Image Detection (SID) methods have recently appeared to tackle it.","Yet, there is often a large gap between experimental results on benchmark datasets and the performance of methods in the wild.","To better address the evaluation needs of SID and help close this gap, this paper introduces a benchmarking framework that integrates several state-of-the-art SID models.","Our selection of integrated models was based on the utilization of varied input features, and different network architectures, aiming to encompass a broad spectrum of techniques.","The framework leverages recent datasets with a diverse set of generative models, high level of photo-realism and resolution, reflecting the rapid improvements in image synthesis technology.","Additionally, the framework enables the study of how image transformations, common in assets shared online, such as JPEG compression, affect detection performance.","SIDBench is available on https://github.com/mever-team/sidbench and is designed in a modular manner to enable easy inclusion of new datasets and SID models."],"url":"http://arxiv.org/abs/2404.18552v1","category":"cs.CV"}
{"created":"2024-04-29 09:45:46","title":"IncidentResponseGPT: Generating Traffic Incident Response Plans with Generative Artificial Intelligence","abstract":"Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion. Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle. This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans. By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents. The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems. Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents. The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management. By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems.","sentences":["Traffic congestion due to road incidents poses a significant challenge in urban environments, leading to increased pollution, economic losses, and traffic congestion.","Efficiently managing these incidents is imperative for mitigating their adverse effects; however, the complexity of urban traffic systems and the variety of potential incidents represent a considerable obstacle.","This paper introduces IncidentResponseGPT, an innovative solution designed to assist traffic management authorities by providing rapid, informed, and adaptable traffic incident response plans.","By integrating a Generative AI platform with real-time traffic incident reports and operational guidelines, our system aims to streamline the decision-making process in responding to traffic incidents.","The research addresses the critical challenges involved in deploying AI in traffic management, including overcoming the complexity of urban traffic networks, ensuring real-time decision-making capabilities, aligning with local laws and regulations, and securing public acceptance for AI-driven systems.","Through a combination of text analysis of accident reports, validation of AI recommendations through traffic simulation, and implementation of transparent and validated AI systems, IncidentResponseGPT offers a promising approach to optimizing traffic flow and reducing congestion in the face of traffic incidents.","The relevance of this work extends to traffic management authorities, emergency response teams, and municipal bodies, all integral stakeholders in urban traffic control and incident management.","By proposing a novel solution to the identified challenges, this research aims to develop a framework that not only facilitates faster resolution of traffic incidents but also minimizes their overall impact on urban traffic systems."],"url":"http://arxiv.org/abs/2404.18550v1","category":"cs.LG"}
{"created":"2024-04-29 09:40:40","title":"On the duality in constant-roll inflation","abstract":"There is a duality in the observables $n_s$, $r$ and the inflaton potential between large and small $\\eta_H$ for the constant-roll inflation if the slow-roll parameter $\\epsilon_H$ is negligible. In general, the duality between $\\eta_H$ and $\\bar{\\eta}_H$ does not hold for the background evolution of the inflation. For some particular solutions for the constant-roll inflation with $\\eta_H$ being a constant, we find that in the small field approximation, the potential takes the quadratic form and it remains the same when the parameter $\\eta_H$ changes to $\\bar{\\eta}_H=3-\\eta_H$. If the scalar field is small and the contribution of $\\epsilon_H$ is negligible, we find that there exists the logarithmic duality and the duality between large and small $\\eta_H$ for the primordial curvature perturbation in inflationary models with the quadratic potential.","sentences":["There is a duality in the observables $n_s$, $r$ and the inflaton potential between large and small $\\eta_H$ for the constant-roll inflation if the slow-roll parameter $\\epsilon_H$ is negligible.","In general, the duality between $\\eta_H$ and $\\bar{\\eta}_H$ does not hold for the background evolution of the inflation.","For some particular solutions for the constant-roll inflation with $\\eta_H$ being a constant, we find that in the small field approximation, the potential takes the quadratic form and it remains the same when the parameter $\\eta_H$ changes to $\\bar{\\eta}_H=3-\\eta_H$. If the scalar field is small and the contribution of $\\epsilon_H$ is negligible, we find that there exists the logarithmic duality and the duality between large and small $\\eta_H$ for the primordial curvature perturbation in inflationary models with the quadratic potential."],"url":"http://arxiv.org/abs/2404.18548v1","category":"gr-qc"}
{"created":"2024-04-29 09:37:24","title":"ir_explain: a Python Library of Explainable IR Methods","abstract":"While recent advancements in Neural Ranking Models have resulted in significant improvements over traditional statistical retrieval models, it is generally acknowledged that the use of large neural architectures and the application of complex language models in Information Retrieval (IR) have reduced the transparency of retrieval methods. Consequently, Explainability and Interpretability have emerged as important research topics in IR. Several axiomatic and post-hoc explanation methods, as well as approaches that attempt to be interpretable-by-design, have been proposed. This article presents \\irexplain, an open-source Python library that implements a variety of well-known techniques for Explainable IR (ExIR) within a common, extensible framework. \\irexplain supports the three standard categories of post-hoc explanations, namely pointwise, pairwise, and listwise explanations. The library is designed to make it easy to reproduce state-of-the-art ExIR baselines on standard test collections, as well as to explore new approaches to explaining IR models and methods. To facilitate adoption, \\irexplain is well-integrated with widely-used toolkits such as Pyserini and \\irdatasets.","sentences":["While recent advancements in Neural Ranking Models have resulted in significant improvements over traditional statistical retrieval models, it is generally acknowledged that the use of large neural architectures and the application of complex language models in Information Retrieval (IR) have reduced the transparency of retrieval methods.","Consequently, Explainability and Interpretability have emerged as important research topics in IR.","Several axiomatic and post-hoc explanation methods, as well as approaches that attempt to be interpretable-by-design, have been proposed.","This article presents \\irexplain, an open-source Python library that implements a variety of well-known techniques for Explainable IR (ExIR) within a common, extensible framework.","\\irexplain supports the three standard categories of post-hoc explanations, namely pointwise, pairwise, and listwise explanations.","The library is designed to make it easy to reproduce state-of-the-art ExIR baselines on standard test collections, as well as to explore new approaches to explaining IR models and methods.","To facilitate adoption, \\irexplain is well-integrated with widely-used toolkits such as Pyserini and \\irdatasets."],"url":"http://arxiv.org/abs/2404.18546v1","category":"cs.IR"}
{"created":"2024-04-29 09:33:53","title":"OAEI Machine Learning Dataset for Online Model Generation","abstract":"Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI). More and more systems use machine learning-based approaches, including large language models. The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used. This sampling is against the OAEI rules and makes a fair comparison impossible. Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks. In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks. Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems. We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems.","sentences":["Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI).","More and more systems use machine learning-based approaches, including large language models.","The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used.","This sampling is against the OAEI rules and makes a fair comparison impossible.","Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks.","In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks.","Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems.","We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems."],"url":"http://arxiv.org/abs/2404.18542v1","category":"cs.IR"}
{"created":"2024-04-29 09:28:57","title":"Machine Learning for Windows Malware Detection and Classification: Methods, Challenges and Ongoing Research","abstract":"In this chapter, readers will explore how machine learning has been applied to build malware detection systems designed for the Windows operating system. This chapter starts by introducing the main components of a Machine Learning pipeline, highlighting the challenges of collecting and maintaining up-to-date datasets. Following this introduction, various state-of-the-art malware detectors are presented, encompassing both feature-based and deep learning-based detectors. Subsequent sections introduce the primary challenges encountered by machine learning-based malware detectors, including concept drift and adversarial attacks. Lastly, this chapter concludes by providing a brief overview of the ongoing research on adversarial defenses.","sentences":["In this chapter, readers will explore how machine learning has been applied to build malware detection systems designed for the Windows operating system.","This chapter starts by introducing the main components of a Machine Learning pipeline, highlighting the challenges of collecting and maintaining up-to-date datasets.","Following this introduction, various state-of-the-art malware detectors are presented, encompassing both feature-based and deep learning-based detectors.","Subsequent sections introduce the primary challenges encountered by machine learning-based malware detectors, including concept drift and adversarial attacks.","Lastly, this chapter concludes by providing a brief overview of the ongoing research on adversarial defenses."],"url":"http://arxiv.org/abs/2404.18541v1","category":"cs.CR"}
{"created":"2024-04-29 09:27:31","title":"Enhancing Boundary Segmentation for Topological Accuracy with Skeleton-based Methods","abstract":"Topological consistency plays a crucial role in the task of boundary segmentation for reticular images, such as cell membrane segmentation in neuron electron microscopic images, grain boundary segmentation in material microscopic images and road segmentation in aerial images. In these fields, topological changes in segmentation results have a serious impact on the downstream tasks, which can even exceed the misalignment of the boundary itself. To enhance the topology accuracy in segmentation results, we propose the Skea-Topo Aware loss, which is a novel loss function that takes into account the shape of each object and topological significance of the pixels. It consists of two components. First, the skeleton-aware weighted loss improves the segmentation accuracy by better modeling the object geometry with skeletons. Second, a boundary rectified term effectively identifies and emphasizes topological critical pixels in the prediction errors using both foreground and background skeletons in the ground truth and predictions. Experiments prove that our method improves topological consistency by up to 7 points in VI compared to 13 state-of-art methods, based on objective and subjective assessments across three different boundary segmentation datasets. The code is available at https://github.com/clovermini/Skea_topo.","sentences":["Topological consistency plays a crucial role in the task of boundary segmentation for reticular images, such as cell membrane segmentation in neuron electron microscopic images, grain boundary segmentation in material microscopic images and road segmentation in aerial images.","In these fields, topological changes in segmentation results have a serious impact on the downstream tasks, which can even exceed the misalignment of the boundary itself.","To enhance the topology accuracy in segmentation results, we propose the Skea-Topo Aware loss, which is a novel loss function that takes into account the shape of each object and topological significance of the pixels.","It consists of two components.","First, the skeleton-aware weighted loss improves the segmentation accuracy by better modeling the object geometry with skeletons.","Second, a boundary rectified term effectively identifies and emphasizes topological critical pixels in the prediction errors using both foreground and background skeletons in the ground truth and predictions.","Experiments prove that our method improves topological consistency by up to 7 points in VI compared to 13 state-of-art methods, based on objective and subjective assessments across three different boundary segmentation datasets.","The code is available at https://github.com/clovermini/Skea_topo."],"url":"http://arxiv.org/abs/2404.18539v1","category":"cs.CV"}
{"created":"2024-04-29 09:27:17","title":"Symmetry group based domain decomposition to enhance physics-informed neural networks for solving partial differential equations","abstract":"Domain decomposition provides an effective way to tackle the dilemma of physics-informed neural networks (PINN) which struggle to accurately and efficiently solve partial differential equations (PDEs) in the whole domain, but the lack of efficient tools for dealing with the interfaces between two adjacent sub-domains heavily hinders the training effects, even leads to the discontinuity of the learned solutions. In this paper, we propose a symmetry group based domain decomposition strategy to enhance the PINN for solving the forward and inverse problems of the PDEs possessing a Lie symmetry group. Specifically, for the forward problem, we first deploy the symmetry group to generate the dividing-lines having known solution information which can be adjusted flexibly and are used to divide the whole training domain into a finite number of non-overlapping sub-domains, then utilize the PINN and the symmetry-enhanced PINN methods to learn the solutions in each sub-domain and finally stitch them to the overall solution of PDEs. For the inverse problem, we first utilize the symmetry group acting on the data of the initial and boundary conditions to generate labeled data in the interior domain of PDEs and then find the undetermined parameters as well as the solution by only training the neural networks in a sub-domain. Consequently, the proposed method can predict high-accuracy solutions of PDEs which are failed by the vanilla PINN in the whole domain and the extended physics-informed neural network in the same sub-domains. Numerical results of the Korteweg-de Vries equation with a translation symmetry and the nonlinear viscous fluid equation with a scaling symmetry show that the accuracies of the learned solutions are improved largely.","sentences":["Domain decomposition provides an effective way to tackle the dilemma of physics-informed neural networks (PINN) which struggle to accurately and efficiently solve partial differential equations (PDEs) in the whole domain, but the lack of efficient tools for dealing with the interfaces between two adjacent sub-domains heavily hinders the training effects, even leads to the discontinuity of the learned solutions.","In this paper, we propose a symmetry group based domain decomposition strategy to enhance the PINN for solving the forward and inverse problems of the PDEs possessing a Lie symmetry group.","Specifically, for the forward problem, we first deploy the symmetry group to generate the dividing-lines having known solution information which can be adjusted flexibly and are used to divide the whole training domain into a finite number of non-overlapping sub-domains, then utilize the PINN and the symmetry-enhanced PINN methods to learn the solutions in each sub-domain and finally stitch them to the overall solution of PDEs.","For the inverse problem, we first utilize the symmetry group acting on the data of the initial and boundary conditions to generate labeled data in the interior domain of PDEs and then find the undetermined parameters as well as the solution by only training the neural networks in a sub-domain.","Consequently, the proposed method can predict high-accuracy solutions of PDEs which are failed by the vanilla PINN in the whole domain and the extended physics-informed neural network in the same sub-domains.","Numerical results of the Korteweg-de Vries equation with a translation symmetry and the nonlinear viscous fluid equation with a scaling symmetry show that the accuracies of the learned solutions are improved largely."],"url":"http://arxiv.org/abs/2404.18538v1","category":"cs.LG"}
{"created":"2024-04-29 09:27:15","title":"General Approach on Shadow Radius and Photon Spheres in Asymptotically Flat Spacetimes and the Impact of Mass-Dependent Variations","abstract":"Recent observations of black hole shadows have revolutionized our ability to probe gravity in extreme environments. This manuscript presents a novel analytic method to calculate, in leading-order terms, the key parameters of photon sphere and shadow radius. This method offers advantages for complex metrics where traditional approaches are cumbersome. We further explore the impact of black hole mass on the photon sphere radius, providing insights into black hole interactions with their surroundings. Our findings contribute significantly to black hole physics and gravity under extreme conditions. By leveraging future advancements in observations, such as the next-generation Event Horizon Telescope (ngEHT), this work paves the way for even more precise tests of gravity near black holes.","sentences":["Recent observations of black hole shadows have revolutionized our ability to probe gravity in extreme environments.","This manuscript presents a novel analytic method to calculate, in leading-order terms, the key parameters of photon sphere and shadow radius.","This method offers advantages for complex metrics where traditional approaches are cumbersome.","We further explore the impact of black hole mass on the photon sphere radius, providing insights into black hole interactions with their surroundings.","Our findings contribute significantly to black hole physics and gravity under extreme conditions.","By leveraging future advancements in observations, such as the next-generation Event Horizon Telescope (ngEHT), this work paves the way for even more precise tests of gravity near black holes."],"url":"http://arxiv.org/abs/2404.18536v1","category":"gr-qc"}
{"created":"2024-04-29 09:27:15","title":"Time Series Data Augmentation as an Imbalanced Learning Problem","abstract":"Recent state-of-the-art forecasting methods are trained on collections of time series. These methods, often referred to as global models, can capture common patterns in different time series to improve their generalization performance. However, they require large amounts of data that might not be readily available. Besides this, global models sometimes fail to capture relevant patterns unique to a particular time series. In these cases, data augmentation can be useful to increase the sample size of time series datasets. The main contribution of this work is a novel method for generating univariate time series synthetic samples. Our approach stems from the insight that the observations concerning a particular time series of interest represent only a small fraction of all observations. In this context, we frame the problem of training a forecasting model as an imbalanced learning task. Oversampling strategies are popular approaches used to deal with the imbalance problem in machine learning. We use these techniques to create synthetic time series observations and improve the accuracy of forecasting models. We carried out experiments using 7 different databases that contain a total of 5502 univariate time series. We found that the proposed solution outperforms both a global and a local model, thus providing a better trade-off between these two approaches.","sentences":["Recent state-of-the-art forecasting methods are trained on collections of time series.","These methods, often referred to as global models, can capture common patterns in different time series to improve their generalization performance.","However, they require large amounts of data that might not be readily available.","Besides this, global models sometimes fail to capture relevant patterns unique to a particular time series.","In these cases, data augmentation can be useful to increase the sample size of time series datasets.","The main contribution of this work is a novel method for generating univariate time series synthetic samples.","Our approach stems from the insight that the observations concerning a particular time series of interest represent only a small fraction of all observations.","In this context, we frame the problem of training a forecasting model as an imbalanced learning task.","Oversampling strategies are popular approaches used to deal with the imbalance problem in machine learning.","We use these techniques to create synthetic time series observations and improve the accuracy of forecasting models.","We carried out experiments using 7 different databases that contain a total of 5502 univariate time series.","We found that the proposed solution outperforms both a global and a local model, thus providing a better trade-off between these two approaches."],"url":"http://arxiv.org/abs/2404.18537v1","category":"cs.LG"}
{"created":"2024-04-29 09:22:54","title":"Evaluating and Mitigating Linguistic Discrimination in Large Language Models","abstract":"By training on text in various languages, large language models (LLMs) typically possess multilingual support and demonstrate remarkable capabilities in solving tasks described in different languages. However, LLMs can exhibit linguistic discrimination due to the uneven distribution of training data across languages. That is, LLMs are hard to keep the consistency of responses when faced with the same task but depicted in different languages.   In this study, we first explore the consistency in the LLMs' outputs responding to queries in various languages from two aspects: safety and quality. We conduct this analysis with two datasets (AdvBench and NQ) based on four LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro). The results show that LLMs exhibit stronger human alignment capabilities with queries in English, French, Russian, and Spanish (only 1.04\\% of harmful queries successfully jailbreak on average) compared to queries in Bengali, Georgian, Nepali and Maithili (27.7\\% of harmful queries jailbreak successfully on average). Moreover, for queries in English, Danish, Czech and Slovenian, LLMs tend to produce responses with a higher quality (with 0.1494 $F_1$ score on average) compared to the other languages. Upon these findings, we propose LDFighter, a similarity-based voting, to mitigate the linguistic discrimination in LLMs. LDFighter ensures consistent service for different language speakers. We evaluate LDFighter with both benign queries and harmful queries. The results show that LDFighter not only significantly reduces the jailbreak success rate but also improve the response quality on average, demonstrating its effectiveness.","sentences":["By training on text in various languages, large language models (LLMs) typically possess multilingual support and demonstrate remarkable capabilities in solving tasks described in different languages.","However, LLMs can exhibit linguistic discrimination due to the uneven distribution of training data across languages.","That is, LLMs are hard to keep the consistency of responses when faced with the same task but depicted in different languages.   ","In this study, we first explore the consistency in the LLMs' outputs responding to queries in various languages from two aspects: safety and quality.","We conduct this analysis with two datasets (AdvBench and NQ) based on four LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro).","The results show that LLMs exhibit stronger human alignment capabilities with queries in English, French, Russian, and Spanish (only 1.04\\% of harmful queries successfully jailbreak on average) compared to queries in Bengali, Georgian, Nepali and Maithili (27.7\\% of harmful queries jailbreak successfully on average).","Moreover, for queries in English, Danish, Czech and Slovenian, LLMs tend to produce responses with a higher quality (with 0.1494 $F_1$ score on average) compared to the other languages.","Upon these findings, we propose LDFighter, a similarity-based voting, to mitigate the linguistic discrimination in LLMs.","LDFighter ensures consistent service for different language speakers.","We evaluate LDFighter with both benign queries and harmful queries.","The results show that LDFighter not only significantly reduces the jailbreak success rate but also improve the response quality on average, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2404.18534v1","category":"cs.CL"}
{"created":"2024-04-29 09:20:25","title":"Evaluating Concept-based Explanations of Language Models: A Study on Faithfulness and Readability","abstract":"Despite the surprisingly high intelligence exhibited by Large Language Models (LLMs), we are somehow intimidated to fully deploy them into real-life applications considering their black-box nature. Concept-based explanations arise as a promising avenue for explaining what the LLMs have learned, making them more transparent to humans. However, current evaluations for concepts tend to be heuristic and non-deterministic, e.g. case study or human evaluation, hindering the development of the field. To bridge the gap, we approach concept-based explanation evaluation via faithfulness and readability. We first introduce a formal definition of concept generalizable to diverse concept-based explanations. Based on this, we quantify faithfulness via the difference in the output upon perturbation. We then provide an automatic measure for readability, by measuring the coherence of patterns that maximally activate a concept. This measure serves as a cost-effective and reliable substitute for human evaluation. Finally, based on measurement theory, we describe a meta-evaluation method for evaluating the above measures via reliability and validity, which can be generalized to other tasks as well. Extensive experimental analysis has been conducted to validate and inform the selection of concept evaluation measures.","sentences":["Despite the surprisingly high intelligence exhibited by Large Language Models (LLMs), we are somehow intimidated to fully deploy them into real-life applications considering their black-box nature.","Concept-based explanations arise as a promising avenue for explaining what the LLMs have learned, making them more transparent to humans.","However, current evaluations for concepts tend to be heuristic and non-deterministic, e.g. case study or human evaluation, hindering the development of the field.","To bridge the gap, we approach concept-based explanation evaluation via faithfulness and readability.","We first introduce a formal definition of concept generalizable to diverse concept-based explanations.","Based on this, we quantify faithfulness via the difference in the output upon perturbation.","We then provide an automatic measure for readability, by measuring the coherence of patterns that maximally activate a concept.","This measure serves as a cost-effective and reliable substitute for human evaluation.","Finally, based on measurement theory, we describe a meta-evaluation method for evaluating the above measures via reliability and validity, which can be generalized to other tasks as well.","Extensive experimental analysis has been conducted to validate and inform the selection of concept evaluation measures."],"url":"http://arxiv.org/abs/2404.18533v2","category":"cs.AI"}
{"created":"2024-04-29 09:19:05","title":"MileBench: Benchmarking MLLMs in Long Context","abstract":"Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 20 models, revealed that while the closed-source GPT-4(Vision) and Gemini 1.5 outperform others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.","sentences":["Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope.","Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs.","To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs.","This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation.","We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios.","Our experimental results, obtained from testing 20 models, revealed that while the closed-source GPT-4(Vision) and Gemini 1.5 outperform others, most open-source MLLMs struggle in long-context situations.","Interestingly, the performance gap tends to widen with an increase in the number of images.","We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images."],"url":"http://arxiv.org/abs/2404.18532v1","category":"cs.CL"}
{"created":"2024-04-29 09:17:36","title":"A Framework to Model ML Engineering Processes","abstract":"The development of Machine Learning (ML) based systems is complex and requires multidisciplinary teams with diverse skill sets. This may lead to communication issues or misapplication of best practices. Process models can alleviate these challenges by standardizing task orchestration, providing a common language to facilitate communication, and nurturing a collaborative environment. Unfortunately, current process modeling languages are not suitable for describing the development of such systems. In this paper, we introduce a framework for modeling ML-based software development processes, built around a domain-specific language and derived from an analysis of scientific and gray literature. A supporting toolkit is also available.","sentences":["The development of Machine Learning (ML) based systems is complex and requires multidisciplinary teams with diverse skill sets.","This may lead to communication issues or misapplication of best practices.","Process models can alleviate these challenges by standardizing task orchestration, providing a common language to facilitate communication, and nurturing a collaborative environment.","Unfortunately, current process modeling languages are not suitable for describing the development of such systems.","In this paper, we introduce a framework for modeling ML-based software development processes, built around a domain-specific language and derived from an analysis of scientific and gray literature.","A supporting toolkit is also available."],"url":"http://arxiv.org/abs/2404.18531v1","category":"cs.SE"}
{"created":"2024-04-29 09:14:35","title":"Qubit encoding for a mixture of localized functions","abstract":"One of the crucial generic techniques for quantum computation is the amplitude encoding. Although such techniques have been proposed, each of them often requires exponential classical-computational cost or an oracle whose explicit construction is not provided. Given the recent demands for practical quantum computation, we develop moderately specialized encoding techniques that generate an arbitrary linear combination of localized complex functions. We demonstrate that $n_{\\mathrm{loc}}$ discrete Lorentzian functions as an expansion basis set lead to efficient probabilistic encoding, whose computational time is $\\mathcal{O} ( \\max ( n_{\\mathrm{loc}}^2 \\log n_{\\mathrm{loc}}, n_{\\mathrm{loc}}^2 \\log n_q, n_q \\log n_q ))$ for $n_q$ data qubits equipped with $\\log_2 n_{\\mathrm{loc}}$ ancillae. Furthermore, amplitude amplification in combination with amplitude reduction renders it deterministic with controllable errors and the computational time is reduced to $\\mathcal{O} ( \\max ( n_{\\mathrm{loc}}^{3/2} \\log n_{\\mathrm{loc}}, n_{\\mathrm{loc}}^{3/2} \\log n_q, n_q \\log n_q )).$ We provide estimation of required resources for application of our scheme to quantum chemistry in real space. We also show the results on real superconducting quantum computers to confirm the validity of our techniques.","sentences":["One of the crucial generic techniques for quantum computation is the amplitude encoding.","Although such techniques have been proposed, each of them often requires exponential classical-computational cost or an oracle whose explicit construction is not provided.","Given the recent demands for practical quantum computation, we develop moderately specialized encoding techniques that generate an arbitrary linear combination of localized complex functions.","We demonstrate that $n_{\\mathrm{loc}}$ discrete Lorentzian functions as an expansion basis set lead to efficient probabilistic encoding, whose computational time is $\\mathcal{O} ( \\max ( n_{\\mathrm{loc}}^2 \\log n_{\\mathrm{loc}}, n_{\\mathrm{loc}}^2 \\log n_q, n_q \\log n_q ))","$ for $n_q$ data qubits equipped with $\\log_2 n_{\\mathrm{loc}}$ ancillae.","Furthermore, amplitude amplification in combination with amplitude reduction renders it deterministic with controllable errors and the computational time is reduced to $\\mathcal{O} ( \\max ( n_{\\mathrm{loc}}^{3/2} \\log n_{\\mathrm{loc}}, n_{\\mathrm{loc}}^{3/2} \\log n_q, n_q \\log n_q )).$","We provide estimation of required resources for application of our scheme to quantum chemistry in real space.","We also show the results on real superconducting quantum computers to confirm the validity of our techniques."],"url":"http://arxiv.org/abs/2404.18529v1","category":"quant-ph"}
{"created":"2024-04-29 09:12:53","title":"Generation of Uncorrelated Residual Variables for Chemical Process Fault Diagnosis via Transfer Learning-based Input-Output Decoupled Network","abstract":"Structural decoupling has played an essential role in model-based fault isolation and estimation in past decades, which facilitates accurate fault localization and reconstruction thanks to the diagonal transfer matrix design. However, traditional methods exhibit limited effectiveness in modeling high-dimensional nonlinearity and big data, and the decoupling idea has not been well-valued in data-driven frameworks. Known for big data and complex feature extraction capabilities, deep learning has recently been used to develop residual generation models. Nevertheless, it lacks decoupling-related diagnostic designs. To this end, this paper proposes a transfer learning-based input-output decoupled network (TDN) for diagnostic purposes, which consists of an input-output decoupled network (IDN) and a pre-trained variational autocoder (VAE). In IDN, uncorrelated residual variables are generated by diagonalization and parallel computing operations. During the transfer learning phase, knowledge of normal status is provided according to VAE's loss and maximum mean discrepancy loss to guide the training of IDN. After training, IDN learns the mapping from faulty to normal, thereby serving as the fault detection index and the estimated fault signal simultaneously. At last, the effectiveness of the developed TDN is verified by a numerical example and a chemical simulation.","sentences":["Structural decoupling has played an essential role in model-based fault isolation and estimation in past decades, which facilitates accurate fault localization and reconstruction thanks to the diagonal transfer matrix design.","However, traditional methods exhibit limited effectiveness in modeling high-dimensional nonlinearity and big data, and the decoupling idea has not been well-valued in data-driven frameworks.","Known for big data and complex feature extraction capabilities, deep learning has recently been used to develop residual generation models.","Nevertheless, it lacks decoupling-related diagnostic designs.","To this end, this paper proposes a transfer learning-based input-output decoupled network (TDN) for diagnostic purposes, which consists of an input-output decoupled network (IDN) and a pre-trained variational autocoder (VAE).","In IDN, uncorrelated residual variables are generated by diagonalization and parallel computing operations.","During the transfer learning phase, knowledge of normal status is provided according to VAE's loss and maximum mean discrepancy loss to guide the training of IDN.","After training, IDN learns the mapping from faulty to normal, thereby serving as the fault detection index and the estimated fault signal simultaneously.","At last, the effectiveness of the developed TDN is verified by a numerical example and a chemical simulation."],"url":"http://arxiv.org/abs/2404.18528v1","category":"cs.LG"}
{"created":"2024-04-29 09:12:31","title":"Bridging Data Barriers among Participants: Assessing the Potential of Geoenergy through Federated Learning","abstract":"Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns. This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties. Hyperparameter tuning of the models is achieved through Bayesian Optimization. To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector. The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy. FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model. The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters. This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques.","sentences":["Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns.","This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties.","Hyperparameter tuning of the models is achieved through Bayesian Optimization.","To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector.","The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy.","FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model.","The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters.","This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques."],"url":"http://arxiv.org/abs/2404.18527v1","category":"cs.LG"}
{"created":"2024-04-29 09:11:32","title":"Experimental and Theoretical Bulk Phase Diagram and Interfacial Tension of Ouzo","abstract":"Ouzo is a well-known drink in Mediterranean countries, with ingredients water, alcohol and trans-anethole oil. The oil is insoluble in water, but completely soluble in alcohol, so when water is added to the spirit, the available alcohol is depleted and the mixture exhibits spontaneous emulsification. This process is commonly known as the louche or Ouzo effect. Although the phase boundaries of this archetypal ternary mixture are well known, the properties of coexisting phases have not previously been studied. Here, we present a detailed experimental investigation into the phase behaviour, including tie-lines connecting coexisting phases, determination of the critical point (also called the plait point in ternary systems) and measurements of the surface tension and density for varying alcohol concentrations. Additionally, we present a theory for the thermodynamics and phase diagram of the system. With suitable selection of the interaction parameters, the theory captures nearly all features of the experimental work. This simple model can be used to determine both bulk and non-uniform (e.g. interfacial) properties, paving the way for a wide range of future applications of the model to ternary mixtures in general. We show how our accurate equilibrium phase diagram can be used to provide improved understanding of non-equilibrium phenomena.","sentences":["Ouzo is a well-known drink in Mediterranean countries, with ingredients water, alcohol and trans-anethole oil.","The oil is insoluble in water, but completely soluble in alcohol, so when water is added to the spirit, the available alcohol is depleted and the mixture exhibits spontaneous emulsification.","This process is commonly known as the louche or Ouzo effect.","Although the phase boundaries of this archetypal ternary mixture are well known, the properties of coexisting phases have not previously been studied.","Here, we present a detailed experimental investigation into the phase behaviour, including tie-lines connecting coexisting phases, determination of the critical point (also called the plait point in ternary systems) and measurements of the surface tension and density for varying alcohol concentrations.","Additionally, we present a theory for the thermodynamics and phase diagram of the system.","With suitable selection of the interaction parameters, the theory captures nearly all features of the experimental work.","This simple model can be used to determine both bulk and non-uniform (e.g. interfacial) properties, paving the way for a wide range of future applications of the model to ternary mixtures in general.","We show how our accurate equilibrium phase diagram can be used to provide improved understanding of non-equilibrium phenomena."],"url":"http://arxiv.org/abs/2404.18524v1","category":"cond-mat.soft"}
{"created":"2024-04-29 09:11:15","title":"Dynamical Blockade Optimizing via Particle Swarm Optimization Algorithm","abstract":"Photon blockade in weak nonlinear regime is an exciting and promising subject that has been extensively studied in the steady state. However, how to achieve dynamic blockade in a single bosonic mode with weak nonlinearity using only pulsed driving field remains unexplored. Here, we propose to optimize the parameters of the pulsed driving field to achieve dynamic blockade in a single bosonic mode with weak nonlinearity via the particle swarm optimization (PSO) algorithm. We demonstrate that both Gaussian and rectangular pulses can be used to generate dynamic photon blockade in a single bosonic mode with weak nonlinearity. Based on the Fourier series expansions of the pulsed driving field, we identify that there are many paths for two-photon excitation in the bosonic mode, even only driven by pulsed field, and the dynamic blockade in weak nonlinear regime is induced by the destructive interference between them. Our work not only highlights the effectiveness of PSO algorithm in optimizing dynamical blockade, but also opens a way to optimize the parameters for other quantum effects, such as quantum entanglement and quantum squeezing.","sentences":["Photon blockade in weak nonlinear regime is an exciting and promising subject that has been extensively studied in the steady state.","However, how to achieve dynamic blockade in a single bosonic mode with weak nonlinearity using only pulsed driving field remains unexplored.","Here, we propose to optimize the parameters of the pulsed driving field to achieve dynamic blockade in a single bosonic mode with weak nonlinearity via the particle swarm optimization (PSO) algorithm.","We demonstrate that both Gaussian and rectangular pulses can be used to generate dynamic photon blockade in a single bosonic mode with weak nonlinearity.","Based on the Fourier series expansions of the pulsed driving field, we identify that there are many paths for two-photon excitation in the bosonic mode, even only driven by pulsed field, and the dynamic blockade in weak nonlinear regime is induced by the destructive interference between them.","Our work not only highlights the effectiveness of PSO algorithm in optimizing dynamical blockade, but also opens a way to optimize the parameters for other quantum effects, such as quantum entanglement and quantum squeezing."],"url":"http://arxiv.org/abs/2404.18523v1","category":"quant-ph"}
{"created":"2024-04-29 09:05:01","title":"On the Impact of Data Heterogeneity in Federated Learning Environments with Application to Healthcare Networks","abstract":"Federated Learning (FL) allows multiple privacy-sensitive applications to leverage their dataset for a global model construction without any disclosure of the information. One of those domains is healthcare, where groups of silos collaborate in order to generate a global predictor with improved accuracy and generalization. However, the inherent challenge lies in the high heterogeneity of medical data, necessitating sophisticated techniques for assessment and compensation. This paper presents a comprehensive exploration of the mathematical formalization and taxonomy of heterogeneity within FL environments, focusing on the intricacies of medical data. In particular, we address the evaluation and comparison of the most popular FL algorithms with respect to their ability to cope with quantity-based, feature and label distribution-based heterogeneity. The goal is to provide a quantitative evaluation of the impact of data heterogeneity in FL systems for healthcare networks as well as a guideline on FL algorithm selection. Our research extends beyond existing studies by benchmarking seven of the most common FL algorithms against the unique challenges posed by medical data use cases. The paper targets the prediction of the risk of stroke recurrence through a set of tabular clinical reports collected by different federated hospital silos: data heterogeneity frequently encountered in this scenario and its impact on FL performance are discussed.","sentences":["Federated Learning (FL) allows multiple privacy-sensitive applications to leverage their dataset for a global model construction without any disclosure of the information.","One of those domains is healthcare, where groups of silos collaborate in order to generate a global predictor with improved accuracy and generalization.","However, the inherent challenge lies in the high heterogeneity of medical data, necessitating sophisticated techniques for assessment and compensation.","This paper presents a comprehensive exploration of the mathematical formalization and taxonomy of heterogeneity within FL environments, focusing on the intricacies of medical data.","In particular, we address the evaluation and comparison of the most popular FL algorithms with respect to their ability to cope with quantity-based, feature and label distribution-based heterogeneity.","The goal is to provide a quantitative evaluation of the impact of data heterogeneity in FL systems for healthcare networks as well as a guideline on FL algorithm selection.","Our research extends beyond existing studies by benchmarking seven of the most common FL algorithms against the unique challenges posed by medical data use cases.","The paper targets the prediction of the risk of stroke recurrence through a set of tabular clinical reports collected by different federated hospital silos: data heterogeneity frequently encountered in this scenario and its impact on FL performance are discussed."],"url":"http://arxiv.org/abs/2404.18519v1","category":"cs.LG"}
{"created":"2024-04-29 09:03:19","title":"From ChatGPT, DALL-E 3 to Sora: How has Generative AI Changed Digital Humanities Research and Services?","abstract":"Generative large-scale language models create the fifth paradigm of scientific research, organically combine data science and computational intelligence, transform the research paradigm of natural language processing and multimodal information processing, promote the new trend of AI-enabled social science research, and provide new ideas for digital humanities research and application. This article profoundly explores the application of large-scale language models in digital humanities research, revealing their significant potential in ancient book protection, intelligent processing, and academic innovation. The article first outlines the importance of ancient book resources and the necessity of digital preservation, followed by a detailed introduction to developing large-scale language models, such as ChatGPT, and their applications in document management, content understanding, and cross-cultural research. Through specific cases, the article demonstrates how AI can assist in the organization, classification, and content generation of ancient books. Then, it explores the prospects of AI applications in artistic innovation and cultural heritage preservation. Finally, the article explores the challenges and opportunities in the interaction of technology, information, and society in the digital humanities triggered by AI technologies.","sentences":["Generative large-scale language models create the fifth paradigm of scientific research, organically combine data science and computational intelligence, transform the research paradigm of natural language processing and multimodal information processing, promote the new trend of AI-enabled social science research, and provide new ideas for digital humanities research and application.","This article profoundly explores the application of large-scale language models in digital humanities research, revealing their significant potential in ancient book protection, intelligent processing, and academic innovation.","The article first outlines the importance of ancient book resources and the necessity of digital preservation, followed by a detailed introduction to developing large-scale language models, such as ChatGPT, and their applications in document management, content understanding, and cross-cultural research.","Through specific cases, the article demonstrates how AI can assist in the organization, classification, and content generation of ancient books.","Then, it explores the prospects of AI applications in artistic innovation and cultural heritage preservation.","Finally, the article explores the challenges and opportunities in the interaction of technology, information, and society in the digital humanities triggered by AI technologies."],"url":"http://arxiv.org/abs/2404.18518v1","category":"cs.DL"}
{"created":"2024-04-29 09:02:09","title":"Distributions of statistics on separable permutations","abstract":"We derive functional equations for distributions of six classical statistics (ascents, descents, left-to-right maxima, right-to-left maxima, left-to-right minima, and right-to-left minima) on separable and irreducible separable permutations. The equations are used to find a third degree equation for joint distribution of ascents and descents on separable permutations that generalizes the respective known result for the descent distribution. Moreover, our general functional equations allow us to derive explicitly (joint) distribution of any subset of maxima and minima statistics on irreducible, reducible and all separable permutations. In particular, there are two equivalence classes of distributions of a pair of maxima or minima statistics. Finally, we present three unimodality conjectures about distributions of statistics on separable permutations.","sentences":["We derive functional equations for distributions of six classical statistics (ascents, descents, left-to-right maxima, right-to-left maxima, left-to-right minima, and right-to-left minima) on separable and irreducible separable permutations.","The equations are used to find a third degree equation for joint distribution of ascents and descents on separable permutations that generalizes the respective known result for the descent distribution.","Moreover, our general functional equations allow us to derive explicitly (joint) distribution of any subset of maxima and minima statistics on irreducible, reducible and all separable permutations.","In particular, there are two equivalence classes of distributions of a pair of maxima or minima statistics.","Finally, we present three unimodality conjectures about distributions of statistics on separable permutations."],"url":"http://arxiv.org/abs/2404.18517v1","category":"math.CO"}
{"created":"2024-04-29 08:50:27","title":"Scalable Event-by-event Processing of Neuromorphic Sensory Signals With Deep State-Space Models","abstract":"Event-based sensors are well suited for real-time processing due to their fast response times and encoding of the sensory data as successive temporal differences. These and other valuable properties, such as a high dynamic range, are suppressed when the data is converted to a frame-based format. However, most current methods either collapse events into frames or cannot scale up when processing the event data directly event-by-event. In this work, we address the key challenges of scaling up event-by-event modeling of the long event streams emitted by such sensors, which is a particularly relevant problem for neuromorphic computing. While prior methods can process up to a few thousand time steps, our model, based on modern recurrent deep state-space models, scales to event streams of millions of events for both training and inference.We leverage their stable parameterization for learning long-range dependencies, parallelizability along the sequence dimension, and their ability to integrate asynchronous events effectively to scale them up to long event streams.We further augment these with novel event-centric techniques enabling our model to match or beat the state-of-the-art performance on several event stream benchmarks. In the Spiking Speech Commands task, we improve state-of-the-art by a large margin of 6.6% to 87.1%. On the DVS128-Gestures dataset, we achieve competitive results without using frames or convolutional neural networks. Our work demonstrates, for the first time, that it is possible to use fully event-based processing with purely recurrent networks to achieve state-of-the-art task performance in several event-based benchmarks.","sentences":["Event-based sensors are well suited for real-time processing due to their fast response times and encoding of the sensory data as successive temporal differences.","These and other valuable properties, such as a high dynamic range, are suppressed when the data is converted to a frame-based format.","However, most current methods either collapse events into frames or cannot scale up when processing the event data directly event-by-event.","In this work, we address the key challenges of scaling up event-by-event modeling of the long event streams emitted by such sensors, which is a particularly relevant problem for neuromorphic computing.","While prior methods can process up to a few thousand time steps, our model, based on modern recurrent deep state-space models, scales to event streams of millions of events for both training and inference.","We leverage their stable parameterization for learning long-range dependencies, parallelizability along the sequence dimension, and their ability to integrate asynchronous events effectively to scale them up to long event streams.","We further augment these with novel event-centric techniques enabling our model to match or beat the state-of-the-art performance on several event stream benchmarks.","In the Spiking Speech Commands task, we improve state-of-the-art by a large margin of 6.6% to 87.1%.","On the DVS128-Gestures dataset, we achieve competitive results without using frames or convolutional neural networks.","Our work demonstrates, for the first time, that it is possible to use fully event-based processing with purely recurrent networks to achieve state-of-the-art task performance in several event-based benchmarks."],"url":"http://arxiv.org/abs/2404.18508v1","category":"cs.LG"}
{"created":"2024-04-29 08:48:40","title":"Characteristics of active and inactive motions in high-Reynolds-number turbulent boundary layers","abstract":"Wall-scaled (attached) eddies play a significant role in the overall drag experienced in high-Reynolds-number turbulent boundary layers (TBLs). This study aims to delve into the underlying mechanisms driving this phenomenon by dissecting the active and inactive components of these attached eddies, as initially proposed by Townsend (1976). Employing a recently introduced energy-decomposition scheme, we analyze TBL datasets covering a wide range of Reynolds numbers ($Re_{\\tau}$ $\\sim$ $\\mathcal{O}$(10$^{3}$)--$\\mathcal{O}$(10$^{6}$)). This analysis provides empirical evidence of the distinct contributions of these components to drag generation, and reveals that while active motions are responsible solely for generating Reynolds shear stresses, inactive motions are crucial for transporting streamwise momentum from the logarithmic region to the wall, thus corroborating earlier hypotheses in the literature.","sentences":["Wall-scaled (attached) eddies play a significant role in the overall drag experienced in high-Reynolds-number turbulent boundary layers (TBLs).","This study aims to delve into the underlying mechanisms driving this phenomenon by dissecting the active and inactive components of these attached eddies, as initially proposed by Townsend (1976).","Employing a recently introduced energy-decomposition scheme, we analyze TBL datasets covering a wide range of Reynolds numbers ($Re_{\\tau}$ $\\sim$ $\\mathcal{O}$(10$^{3}$)--$\\mathcal{O}$(10$^{6}$)).","This analysis provides empirical evidence of the distinct contributions of these components to drag generation, and reveals that while active motions are responsible solely for generating Reynolds shear stresses, inactive motions are crucial for transporting streamwise momentum from the logarithmic region to the wall, thus corroborating earlier hypotheses in the literature."],"url":"http://arxiv.org/abs/2404.18506v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 08:47:58","title":"R3MG: R-tree based agglomeration of polytopal grids with applications to multilevel methods","abstract":"We present a novel approach to perform agglomeration of polygonal and polyhedral grids based on spatial indices. Agglomeration strategies are a key ingredient in polytopal methods for PDEs as they are used to generate (hierarchies of) computational grids from an initial grid. Spatial indices are specialized data structures that significantly accelerate queries involving spatial relationships in arbitrary space dimensions. We show how the construction of the R-tree spatial database of an arbitrary fine grid offers a natural and efficient agglomeration strategy with the following characteristics: i) the process is fully automated, robust, and dimension-independent, ii) it automatically produces a balanced and nested hierarchy of agglomerates, and iii) the shape of the agglomerates is tightly close to the respective axis aligned bounding boxes. Moreover, the R-tree approach provides a full hierarchy of nested agglomerates which permits fast query and allows for efficient geometric multigrid methods to be applied also to those cases where a hierarchy of grids is not present at construction time. We present several examples based on polygonal discontinuous Galerkin methods, confirming the effectiveness of our approach in the context of challenging three-dimensional geometries and the design of geometric multigrid preconditioners.","sentences":["We present a novel approach to perform agglomeration of polygonal and polyhedral grids based on spatial indices.","Agglomeration strategies are a key ingredient in polytopal methods for PDEs as they are used to generate (hierarchies of) computational grids from an initial grid.","Spatial indices are specialized data structures that significantly accelerate queries involving spatial relationships in arbitrary space dimensions.","We show how the construction of the R-tree spatial database of an arbitrary fine grid offers a natural and efficient agglomeration strategy with the following characteristics: i) the process is fully automated, robust, and dimension-independent, ii) it automatically produces a balanced and nested hierarchy of agglomerates, and iii) the shape of the agglomerates is tightly close to the respective axis aligned bounding boxes.","Moreover, the R-tree approach provides a full hierarchy of nested agglomerates which permits fast query and allows for efficient geometric multigrid methods to be applied also to those cases where a hierarchy of grids is not present at construction time.","We present several examples based on polygonal discontinuous Galerkin methods, confirming the effectiveness of our approach in the context of challenging three-dimensional geometries and the design of geometric multigrid preconditioners."],"url":"http://arxiv.org/abs/2404.18505v1","category":"math.NA"}
{"created":"2024-04-29 08:44:42","title":"Did the Big Bang and cosmic inflation really happen? (A tale of alternative cosmological models)","abstract":"A popular science article designed to introduce people familiar with basic cosmological nomenclature with models alternative to cosmological inflation. The paper briefly discusses the modern view of the Big Bang model, inflation (both its advantages and potential deficiencies). This is followed by a discussion of historical alternative models and modern approaches such as matter bounce, ekpyrotic Universe, Conformal Cyclic Cosmology, Hartle-Hawking state and loop quantum cosmology. The final aspect of the paper is to present the advantages and potential problems associated with alternative models and to present the conceptual challenges associated with the uniqueness of cosmology as a specific domain of physics.","sentences":["A popular science article designed to introduce people familiar with basic cosmological nomenclature with models alternative to cosmological inflation.","The paper briefly discusses the modern view of the Big Bang model, inflation (both its advantages and potential deficiencies).","This is followed by a discussion of historical alternative models and modern approaches such as matter bounce, ekpyrotic Universe, Conformal Cyclic Cosmology, Hartle-Hawking state and loop quantum cosmology.","The final aspect of the paper is to present the advantages and potential problems associated with alternative models and to present the conceptual challenges associated with the uniqueness of cosmology as a specific domain of physics."],"url":"http://arxiv.org/abs/2404.18503v1","category":"physics.pop-ph"}
{"created":"2024-04-29 08:43:58","title":"Towards Classical Software Verification using Quantum Computers","abstract":"We explore the possibility of accelerating the formal verification of classical programs with a quantum computer.   A common source of security flaws stems from the existence of common programming errors like use after free, null-pointer dereference, or division by zero. To aid in the discovery of such errors, we try to verify that no such flaws exist.   In our approach, for some code snippet and undesired behaviour, a SAT instance is generated, which is satisfiable precisely if the behavior is present in the code. It is in turn converted to an optimization problem, that is solved on a quantum computer. This approach holds the potential of an asymptotically polynomial speedup.   Minimal examples of common errors, like out-of-bounds and overflows, but also synthetic instances with special properties, specific number of solutions, or structure, are tested with different solvers and tried on a quantum device.   We use the near-standard Quantum Approximation Optimisation Algorithm, an application of the Grover algorithm, and the Quantum Singular Value Transformation to find the optimal solution, and with it a satisfying assignment.","sentences":["We explore the possibility of accelerating the formal verification of classical programs with a quantum computer.   ","A common source of security flaws stems from the existence of common programming errors like use after free, null-pointer dereference, or division by zero.","To aid in the discovery of such errors, we try to verify that no such flaws exist.   ","In our approach, for some code snippet and undesired behaviour, a SAT instance is generated, which is satisfiable precisely if the behavior is present in the code.","It is in turn converted to an optimization problem, that is solved on a quantum computer.","This approach holds the potential of an asymptotically polynomial speedup.   ","Minimal examples of common errors, like out-of-bounds and overflows, but also synthetic instances with special properties, specific number of solutions, or structure, are tested with different solvers and tried on a quantum device.   ","We use the near-standard Quantum Approximation Optimisation Algorithm, an application of the Grover algorithm, and the Quantum Singular Value Transformation to find the optimal solution, and with it a satisfying assignment."],"url":"http://arxiv.org/abs/2404.18502v1","category":"quant-ph"}
{"created":"2024-04-29 08:41:32","title":"Hyperplane Representations of Interventional Characteristic Imset Polytopes","abstract":"Characteristic imsets are 0/1-vectors representing directed acyclic graphs whose edges represent direct cause-effect relations between jointly distributed random variables. A characteristic imset (CIM) polytope is the convex hull of a collection of characteristic imsets. CIM polytopes arise as feasible regions of a linear programming approach to the problem of causal disovery, which aims to infer a cause-effect structure from data. Linear optimization methods typically require a hyperplane representation of the feasible region, which has proven difficult to compute for CIM polytopes despite continued efforts. We solve this problem for CIM polytopes that are the convex hull of imsets associated to DAGs whose underlying graph of adjacencies is a tree. Our methods use the theory of toric fiber products as well as the novel notion of interventional CIM polytopes. Our solution is obtained as a corollary of a more general result for interventional CIM polytopes. The identified hyperplanes are applied to yield a linear optimization-based causal discovery algorithm for learning polytree causal networks from a combination of observational and interventional data.","sentences":["Characteristic imsets are 0/1-vectors representing directed acyclic graphs whose edges represent direct cause-effect relations between jointly distributed random variables.","A characteristic imset (CIM) polytope is the convex hull of a collection of characteristic imsets.","CIM polytopes arise as feasible regions of a linear programming approach to the problem of causal disovery, which aims to infer a cause-effect structure from data.","Linear optimization methods typically require a hyperplane representation of the feasible region, which has proven difficult to compute for CIM polytopes despite continued efforts.","We solve this problem for CIM polytopes that are the convex hull of imsets associated to DAGs whose underlying graph of adjacencies is a tree.","Our methods use the theory of toric fiber products as well as the novel notion of interventional CIM polytopes.","Our solution is obtained as a corollary of a more general result for interventional CIM polytopes.","The identified hyperplanes are applied to yield a linear optimization-based causal discovery algorithm for learning polytree causal networks from a combination of observational and interventional data."],"url":"http://arxiv.org/abs/2404.18500v1","category":"math.CO"}
{"created":"2024-04-29 08:41:17","title":"Quantitative Tools for Time Series Analysis in Natural Language Processing: A Practitioners Guide","abstract":"Natural language processing tools have become frequently used in social sciences such as economics, political science, and sociology. Many publications apply topic modeling to elicit latent topics in text corpora and their development over time. Here, most publications rely on visual inspections and draw inference on changes, structural breaks, and developments over time. We suggest using univariate time series econometrics to introduce more quantitative rigor that can strengthen the analyses. In particular, we discuss the econometric topics of non-stationarity as well as structural breaks. This paper serves as a comprehensive practitioners guide to provide researchers in the social and life sciences as well as the humanities with concise advice on how to implement econometric time series methods to thoroughly investigate topic prevalences over time. We provide coding advice for the statistical software R throughout the paper. The application of the discussed tools to a sample dataset completes the analysis.","sentences":["Natural language processing tools have become frequently used in social sciences such as economics, political science, and sociology.","Many publications apply topic modeling to elicit latent topics in text corpora and their development over time.","Here, most publications rely on visual inspections and draw inference on changes, structural breaks, and developments over time.","We suggest using univariate time series econometrics to introduce more quantitative rigor that can strengthen the analyses.","In particular, we discuss the econometric topics of non-stationarity as well as structural breaks.","This paper serves as a comprehensive practitioners guide to provide researchers in the social and life sciences as well as the humanities with concise advice on how to implement econometric time series methods to thoroughly investigate topic prevalences over time.","We provide coding advice for the statistical software R throughout the paper.","The application of the discussed tools to a sample dataset completes the analysis."],"url":"http://arxiv.org/abs/2404.18499v1","category":"econ.GN"}
{"created":"2024-04-29 08:34:52","title":"A Tensor Product Space for Studying the Interaction of Bipartite States of Light with Nanostructures","abstract":"Pairs of entangled photons are important for applications in quantum nanophotonics, where their theoretical description must accommodate their bipartite character. Such character is shared at the other end of the intensity range by, for example, the two degenerate instances of the pump field involved in second-harmonic generation. Describing the interaction of nanophotonic structures with bipartite states of light is, regardless of their intensity, a challenge with important technological applications. Here, we develop a theoretical framework for studying the interaction of material structures with bipartite states of light. The basic element is the symmetrized tensor product space of two copies of an electromagnetic Hilbert space. One of the benefits inherited from the single Hilbert space is that consequences of material symmetries are readily deduced. We derive selection rules for second-order non-linear processes in objects with rotational and/or mirror symmetries. We numerically verify several selection rules by combining quantum-chemical calculations with a Maxwell solver to simulate second-harmonic generation in two different MoS$_2$ clusters. The computationally convenient scattering matrix method is also extended to the tensor product space when the response of the object to one part of the state is independent of the other. For such a case, we obtain the relation between the scattering matrix in the single Hilbert space and the scattering matrix for bipartite states. Such a separable case is relevant for the entanglement evolution of biphoton states interacting with nanostructures. We discuss some possibilities for accommodating the computations of non-linear effects in the framework, for example, through a non-separable scattering operator, where the response of the object to one part of the state depends on the other part.","sentences":["Pairs of entangled photons are important for applications in quantum nanophotonics, where their theoretical description must accommodate their bipartite character.","Such character is shared at the other end of the intensity range by, for example, the two degenerate instances of the pump field involved in second-harmonic generation.","Describing the interaction of nanophotonic structures with bipartite states of light is, regardless of their intensity, a challenge with important technological applications.","Here, we develop a theoretical framework for studying the interaction of material structures with bipartite states of light.","The basic element is the symmetrized tensor product space of two copies of an electromagnetic Hilbert space.","One of the benefits inherited from the single Hilbert space is that consequences of material symmetries are readily deduced.","We derive selection rules for second-order non-linear processes in objects with rotational and/or mirror symmetries.","We numerically verify several selection rules by combining quantum-chemical calculations with a Maxwell solver to simulate second-harmonic generation in two different MoS$_2$ clusters.","The computationally convenient scattering matrix method is also extended to the tensor product space when the response of the object to one part of the state is independent of the other.","For such a case, we obtain the relation between the scattering matrix in the single Hilbert space and the scattering matrix for bipartite states.","Such a separable case is relevant for the entanglement evolution of biphoton states interacting with nanostructures.","We discuss some possibilities for accommodating the computations of non-linear effects in the framework, for example, through a non-separable scattering operator, where the response of the object to one part of the state depends on the other part."],"url":"http://arxiv.org/abs/2404.18498v1","category":"physics.optics"}
{"created":"2024-04-29 08:27:50","title":"AI-powered Code Review with LLMs: Early Results","abstract":"In this paper, we present a novel approach to improving software quality and efficiency through a Large Language Model (LLM)-based model designed to review code and identify potential issues. Our proposed LLM-based AI agent model is trained on large code repositories. This training includes code reviews, bug reports, and documentation of best practices. It aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code. Unlike traditional static code analysis tools, our LLM-based AI agent has the ability to predict future potential risks in the code. This supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques. Furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward LLM feedback. For future work, we aim to assess the accuracy and efficiency of LLM-generated documentation updates in comparison to manual methods. This will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews. Our goal is to not only refine the accuracy of our LLM-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education.","sentences":["In this paper, we present a novel approach to improving software quality and efficiency through a Large Language Model (LLM)-based model designed to review code and identify potential issues.","Our proposed LLM-based AI agent model is trained on large code repositories.","This training includes code reviews, bug reports, and documentation of best practices.","It aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code.","Unlike traditional static code analysis tools, our LLM-based AI agent has the ability to predict future potential risks in the code.","This supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques.","Furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward LLM feedback.","For future work, we aim to assess the accuracy and efficiency of LLM-generated documentation updates in comparison to manual methods.","This will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews.","Our goal is to not only refine the accuracy of our LLM-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education."],"url":"http://arxiv.org/abs/2404.18496v1","category":"cs.SE"}
{"created":"2024-04-29 08:22:04","title":"No compact split limit Ricci flow of type II from the blow-down","abstract":"By Perelman's $\\mathcal L$-geodesic theory, we study the blow-down solutions on a noncompact $\\kappa$-noncollapsed steady gradient Ricci soliton $(M^n, g)$ $(n\\ge 4)$ with nonnegative curvature operator and positive Ricci curvature away from a compact set of $M$. We prove that any $(n-1)$-dimensional compact split ancient solution from the blow-down of $(M, g)$ is of type I. The result is a generalization of our previous work from $n=4$ to any dimension.","sentences":["By Perelman's $\\mathcal L$-geodesic theory, we study the blow-down solutions on a noncompact $\\kappa$-noncollapsed steady gradient Ricci soliton $(M^n, g)$ $(n\\ge 4)$ with nonnegative curvature operator and positive Ricci curvature away from a compact set of $M$. We prove that any $(n-1)$-dimensional compact split ancient solution from the blow-down of $(M, g)$ is of type I.","The result is a generalization of our previous work from $n=4$ to any dimension."],"url":"http://arxiv.org/abs/2404.18494v1","category":"math.DG"}
{"created":"2024-04-29 07:41:51","title":"Semi-infinite simple exclusion process: from current fluctuations to target survival","abstract":"The symmetric simple exclusion process (SEP), where diffusive particles cannot overtake each other, is a paradigmatic model of transport in the single-file geometry. In this model, the study of currents has attracted a lot of attention, but so far most results are restricted to two geometries: (i) a finite system between two reservoirs, which does not conserve the number of particles but reaches a nonequilibrium steady state, and (ii) an infinite system which conserves the number of particles but never reaches a steady state. Here, we determine the full cumulant generating function of the integrated current in the important intermediate situation of a semi-infinite system connected to a reservoir, which does not conserve the number of particles and never reaches a steady state. This result is obtained thanks to the determination of the full spatial structure of the correlations which remarkably obey the very same closed equation recently obtained in the infinite geometry. Besides their intrinsic interest, these results allow us to solve two open problems: the survival probability of a fixed target in the SEP, and the statistics of the number of particles injected by a localized source.","sentences":["The symmetric simple exclusion process (SEP), where diffusive particles cannot overtake each other, is a paradigmatic model of transport in the single-file geometry.","In this model, the study of currents has attracted a lot of attention, but so far most results are restricted to two geometries: (i) a finite system between two reservoirs, which does not conserve the number of particles but reaches a nonequilibrium steady state, and (ii) an infinite system which conserves the number of particles but never reaches a steady state.","Here, we determine the full cumulant generating function of the integrated current in the important intermediate situation of a semi-infinite system connected to a reservoir, which does not conserve the number of particles and never reaches a steady state.","This result is obtained thanks to the determination of the full spatial structure of the correlations which remarkably obey the very same closed equation recently obtained in the infinite geometry.","Besides their intrinsic interest, these results allow us to solve two open problems: the survival probability of a fixed target in the SEP, and the statistics of the number of particles injected by a localized source."],"url":"http://arxiv.org/abs/2404.18481v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-29 17:26:44","title":"An optimal lower bound for smooth convex functions","abstract":"First order methods endowed with global convergence guarantees operate using global lower bounds on the objective. The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance. In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information. The bound can be readily employed by the Gradient Method with Memory to improve its performance. Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant. We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search. Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods.","sentences":["First order methods endowed with global convergence guarantees operate using global lower bounds on the objective.","The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance.","In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information.","The bound can be readily employed by the Gradient Method with Memory to improve its performance.","Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant.","We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search.","Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods."],"url":"http://arxiv.org/abs/2404.18889v1","category":"math.OC"}
{"created":"2024-04-29 16:30:24","title":"VISION: Toward a Standardized Process for Radiology Image Management at the National Level","abstract":"The compilation and analysis of radiological images poses numerous challenges for researchers. The sheer volume of data as well as the computational needs of algorithms capable of operating on images are extensive. Additionally, the assembly of these images alone is difficult, as these exams may differ widely in terms of clinical context, structured annotation available for model training, modality, and patient identifiers. In this paper, we describe our experiences and challenges in establishing a trusted collection of radiology images linked to the United States Department of Veterans Affairs (VA) electronic health record database. We also discuss implications in making this repository research-ready for medical investigators. Key insights include uncovering the specific procedures required for transferring images from a clinical to a research-ready environment, as well as roadblocks and bottlenecks in this process that may hinder future efforts at automation.","sentences":["The compilation and analysis of radiological images poses numerous challenges for researchers.","The sheer volume of data as well as the computational needs of algorithms capable of operating on images are extensive.","Additionally, the assembly of these images alone is difficult, as these exams may differ widely in terms of clinical context, structured annotation available for model training, modality, and patient identifiers.","In this paper, we describe our experiences and challenges in establishing a trusted collection of radiology images linked to the United States Department of Veterans Affairs (VA) electronic health record database.","We also discuss implications in making this repository research-ready for medical investigators.","Key insights include uncovering the specific procedures required for transferring images from a clinical to a research-ready environment, as well as roadblocks and bottlenecks in this process that may hinder future efforts at automation."],"url":"http://arxiv.org/abs/2404.18842v1","category":"cs.CV"}
{"created":"2024-04-29 16:19:47","title":"It's Difficult to be Neutral -- Human and LLM-based Sentiment Annotation of Patient Comments","abstract":"Sentiment analysis is an important tool for aggregating patient voices, in order to provide targeted improvements in healthcare services. A prerequisite for this is the availability of in-domain data annotated for sentiment. This article documents an effort to add sentiment annotations to free-text comments in patient surveys collected by the Norwegian Institute of Public Health (NIPH). However, annotation can be a time-consuming and resource-intensive process, particularly when it requires domain expertise. We therefore also evaluate a possible alternative to human annotation, using large language models (LLMs) as annotators. We perform an extensive evaluation of the approach for two openly available pretrained LLMs for Norwegian, experimenting with different configurations of prompts and in-context learning, comparing their performance to human annotators. We find that even for zero-shot runs, models perform well above the baseline for binary sentiment, but still cannot compete with human annotators on the full dataset.","sentences":["Sentiment analysis is an important tool for aggregating patient voices, in order to provide targeted improvements in healthcare services.","A prerequisite for this is the availability of in-domain data annotated for sentiment.","This article documents an effort to add sentiment annotations to free-text comments in patient surveys collected by the Norwegian Institute of Public Health (NIPH).","However, annotation can be a time-consuming and resource-intensive process, particularly when it requires domain expertise.","We therefore also evaluate a possible alternative to human annotation, using large language models (LLMs) as annotators.","We perform an extensive evaluation of the approach for two openly available pretrained LLMs for Norwegian, experimenting with different configurations of prompts and in-context learning, comparing their performance to human annotators.","We find that even for zero-shot runs, models perform well above the baseline for binary sentiment, but still cannot compete with human annotators on the full dataset."],"url":"http://arxiv.org/abs/2404.18832v1","category":"cs.CL"}
{"created":"2024-04-29 15:33:56","title":"Efficiency-Effectiveness Tradeoff of Probabilistic Structured Queries for Cross-Language Information Retrieval","abstract":"Probabilistic Structured Queries (PSQ) is a cross-language information retrieval (CLIR) method that uses translation probabilities statistically derived from aligned corpora. PSQ is a strong baseline for efficient CLIR using sparse indexing. It is, therefore, useful as the first stage in a cascaded neural CLIR system whose second stage is more effective but too inefficient to be used on its own to search a large text collection. In this reproducibility study, we revisit PSQ by introducing an efficient Python implementation. Unconstrained use of all translation probabilities that can be estimated from aligned parallel text would in the limit assign a weight to every vocabulary term, precluding use of an inverted index to serve queries efficiently. Thus, PSQ's effectiveness and efficiency both depend on how translation probabilities are pruned. This paper presents experiments over a range of modern CLIR test collections to demonstrate that achieving Pareto optimal PSQ effectiveness-efficiency tradeoffs benefits from multi-criteria pruning, which has not been fully explored in prior work. Our Python PSQ implementation is available on GitHub(https://github.com/hltcoe/PSQ) and unpruned translation tables are available on Huggingface Models(https://huggingface.co/hltcoe/psq_translation_tables).","sentences":["Probabilistic Structured Queries (PSQ) is a cross-language information retrieval (CLIR) method that uses translation probabilities statistically derived from aligned corpora.","PSQ is a strong baseline for efficient CLIR using sparse indexing.","It is, therefore, useful as the first stage in a cascaded neural CLIR system whose second stage is more effective but too inefficient to be used on its own to search a large text collection.","In this reproducibility study, we revisit PSQ by introducing an efficient Python implementation.","Unconstrained use of all translation probabilities that can be estimated from aligned parallel text would in the limit assign a weight to every vocabulary term, precluding use of an inverted index to serve queries efficiently.","Thus, PSQ's effectiveness and efficiency both depend on how translation probabilities are pruned.","This paper presents experiments over a range of modern CLIR test collections to demonstrate that achieving Pareto optimal PSQ effectiveness-efficiency tradeoffs benefits from multi-criteria pruning, which has not been fully explored in prior work.","Our Python PSQ implementation is available on GitHub(https://github.com/hltcoe/PSQ) and unpruned translation tables are available on Huggingface Models(https://huggingface.co/hltcoe/psq_translation_tables)."],"url":"http://arxiv.org/abs/2404.18797v1","category":"cs.IR"}
{"created":"2024-04-29 15:23:16","title":"3D Mapping of Glacier Moulins: Challenges and lessons learned","abstract":"In this paper, we present a field report of the mapping of the Athabasca Glacier, using a custom-made lidar-inertial mapping platform. With the increasing autonomy of robotics, a wider spectrum of applications emerges. Among these, the surveying of environmental areas presents arduous and hazardous challenges for human operators. Leveraging automated platforms for data collection holds the promise of unlocking new applications and a deeper comprehension of the environment. Over the course of a week-long deployment, we collected glacier data using a tailor-made measurement platform and reflected on the inherent challenges associated with such experiments. We focus on the insights gained and the forthcoming challenges that robotics must surmount to effectively map these terrains.","sentences":["In this paper, we present a field report of the mapping of the Athabasca Glacier, using a custom-made lidar-inertial mapping platform.","With the increasing autonomy of robotics, a wider spectrum of applications emerges.","Among these, the surveying of environmental areas presents arduous and hazardous challenges for human operators.","Leveraging automated platforms for data collection holds the promise of unlocking new applications and a deeper comprehension of the environment.","Over the course of a week-long deployment, we collected glacier data using a tailor-made measurement platform and reflected on the inherent challenges associated with such experiments.","We focus on the insights gained and the forthcoming challenges that robotics must surmount to effectively map these terrains."],"url":"http://arxiv.org/abs/2404.18790v1","category":"cs.RO"}
{"created":"2024-04-29 14:19:22","title":"Two-way Homogeneity Pursuit for Quantile Network Vector Autoregression","abstract":"While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data. To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes. Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability. To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure. Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups. With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences. Moreover, we propose a quantile information criterion for consistently selecting the number of groups. Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks.","sentences":["While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data.","To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes.","Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability.","To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure.","Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups.","With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences.","Moreover, we propose a quantile information criterion for consistently selecting the number of groups.","Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks."],"url":"http://arxiv.org/abs/2404.18732v1","category":"stat.ME"}
{"created":"2024-04-29 14:16:08","title":"Fast Swarming of UAVs in GNSS-denied Feature-poor Environments without Explicit Communication","abstract":"A decentralized swarm approach for the fast cooperative flight of Unmanned Aerial Vehicles (UAVs) in feature-poor environments without any external localization and communication is introduced in this paper.   A novel model of a UAV neighborhood is proposed to achieve robust onboard mutual perception and flocking state feedback control, which is designed to decrease the inter-agent oscillations common in standard reactive swarm models employed in fast collective motion.   The novel swarming methodology is supplemented with an enhanced Multi-Robot State Estimation (MRSE) strategy to increase the reliability of the purely onboard localization, which may be unreliable in real environments.   Although MRSE and the neighborhood model may rely on information exchange between agents, we introduce a communication-less version of the swarming framework based on estimating communicated states to decrease dependence on the often unreliable communication networks of large swarms.   The proposed solution has been verified by a set of complex real-world experiments to demonstrate its overall capability in different conditions, including a UAV interception-motivated task with a group velocity reaching the physical limits of the individual hardware platforms.","sentences":["A decentralized swarm approach for the fast cooperative flight of Unmanned Aerial Vehicles (UAVs) in feature-poor environments without any external localization and communication is introduced in this paper.   ","A novel model of a UAV neighborhood is proposed to achieve robust onboard mutual perception and flocking state feedback control, which is designed to decrease the inter-agent oscillations common in standard reactive swarm models employed in fast collective motion.   ","The novel swarming methodology is supplemented with an enhanced Multi-Robot State Estimation (MRSE) strategy to increase the reliability of the purely onboard localization, which may be unreliable in real environments.   ","Although MRSE and the neighborhood model may rely on information exchange between agents, we introduce a communication-less version of the swarming framework based on estimating communicated states to decrease dependence on the often unreliable communication networks of large swarms.   ","The proposed solution has been verified by a set of complex real-world experiments to demonstrate its overall capability in different conditions, including a UAV interception-motivated task with a group velocity reaching the physical limits of the individual hardware platforms."],"url":"http://arxiv.org/abs/2404.18729v1","category":"cs.RO"}
{"created":"2024-04-29 14:14:33","title":"The Constant in HATE: Analyzing Toxicity in Reddit across Topics and Languages","abstract":"Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities. This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations. We collect 1.5 million comment threads from 481 communities in six languages: English, German, Spanish, Turkish,Arabic, and Dutch, covering 80 topics such as Culture, Politics, and News. We thoroughly analyze how toxicity spikes within different communities in relation to specific topics. We observe consistent patterns of increased toxicity across languages for certain topics, while also noting significant variations within specific language communities.","sentences":["Toxic language remains an ongoing challenge on social media platforms, presenting significant issues for users and communities.","This paper provides a cross-topic and cross-lingual analysis of toxicity in Reddit conversations.","We collect 1.5 million comment threads from 481 communities in six languages: English, German, Spanish, Turkish,Arabic, and Dutch, covering 80 topics such as Culture, Politics, and News.","We thoroughly analyze how toxicity spikes within different communities in relation to specific topics.","We observe consistent patterns of increased toxicity across languages for certain topics, while also noting significant variations within specific language communities."],"url":"http://arxiv.org/abs/2404.18726v1","category":"cs.CL"}
{"created":"2024-04-29 13:59:10","title":"Three-state Opinion Dynamics for Financial Markets on Complex Networks","abstract":"This work investigates the effects of complex networks on the collective behavior of a three-state opinion formation model in economic systems. Our model considers two distinct types of investors in financial markets: noise traders and fundamentalists. Financial states evolve via probabilistic dynamics that include economic strategies with local and global influences. The local majoritarian opinion drives noise traders' market behavior, while the market index influences the financial decisions of fundamentalist agents. We introduce a level of market anxiety $q$ present in the decision-making process that influences financial action. In our investigation, nodes of a complex network represent market agents, whereas the links represent their financial interactions. We investigate the stochastic dynamics of the model on three distinct network topologies, including scale-free networks, small-world networks and Erd{\\\"o}s-R\\'enyi random graphs. Our model mirrors various traits observed in real-world financial return series, such as heavy-tailed return distributions, volatility clustering, and short-term memory correlation of returns. The histograms of returns are fitted by coupled Gaussian distributions, quantitatively revealing transitions from a leptokurtic to a mesokurtic regime under specific economic heterogeneity. We show that the market dynamics depend mainly on the average agent connectivity, anxiety level, and market composition rather than on specific features of network topology.","sentences":["This work investigates the effects of complex networks on the collective behavior of a three-state opinion formation model in economic systems.","Our model considers two distinct types of investors in financial markets: noise traders and fundamentalists.","Financial states evolve via probabilistic dynamics that include economic strategies with local and global influences.","The local majoritarian opinion drives noise traders' market behavior, while the market index influences the financial decisions of fundamentalist agents.","We introduce a level of market anxiety $q$ present in the decision-making process that influences financial action.","In our investigation, nodes of a complex network represent market agents, whereas the links represent their financial interactions.","We investigate the stochastic dynamics of the model on three distinct network topologies, including scale-free networks, small-world networks and Erd{\\\"o}s-R\\'enyi random graphs.","Our model mirrors various traits observed in real-world financial return series, such as heavy-tailed return distributions, volatility clustering, and short-term memory correlation of returns.","The histograms of returns are fitted by coupled Gaussian distributions, quantitatively revealing transitions from a leptokurtic to a mesokurtic regime under specific economic heterogeneity.","We show that the market dynamics depend mainly on the average agent connectivity, anxiety level, and market composition rather than on specific features of network topology."],"url":"http://arxiv.org/abs/2404.18709v1","category":"physics.soc-ph"}
{"created":"2024-04-29 13:57:02","title":"The Socface Project: Large-Scale Collection, Processing, and Analysis of a Century of French Censuses","abstract":"This paper presents a complete processing workflow for extracting information from French census lists from 1836 to 1936. These lists contain information about individuals living in France and their households. We aim at extracting all the information contained in these tables using automatic handwritten table recognition. At the end of the Socface project, in which our work is taking place, the extracted information will be redistributed to the departmental archives, and the nominative lists will be freely available to the public, allowing anyone to browse hundreds of millions of records. The extracted data will be used by demographers to analyze social change over time, significantly improving our understanding of French economic and social structures. For this project, we developed a complete processing workflow: large-scale data collection from French departmental archives, collaborative annotation of documents, training of handwritten table text and structure recognition models, and mass processing of millions of images. We present the tools we have developed to easily collect and process millions of pages. We also show that it is possible to process such a wide variety of tables with a single table recognition model that uses the image of the entire page to recognize information about individuals, categorize them and automatically group them into households. The entire process has been successfully used to process the documents of a departmental archive, representing more than 450,000 images.","sentences":["This paper presents a complete processing workflow for extracting information from French census lists from 1836 to 1936.","These lists contain information about individuals living in France and their households.","We aim at extracting all the information contained in these tables using automatic handwritten table recognition.","At the end of the Socface project, in which our work is taking place, the extracted information will be redistributed to the departmental archives, and the nominative lists will be freely available to the public, allowing anyone to browse hundreds of millions of records.","The extracted data will be used by demographers to analyze social change over time, significantly improving our understanding of French economic and social structures.","For this project, we developed a complete processing workflow: large-scale data collection from French departmental archives, collaborative annotation of documents, training of handwritten table text and structure recognition models, and mass processing of millions of images.","We present the tools we have developed to easily collect and process millions of pages.","We also show that it is possible to process such a wide variety of tables with a single table recognition model that uses the image of the entire page to recognize information about individuals, categorize them and automatically group them into households.","The entire process has been successfully used to process the documents of a departmental archive, representing more than 450,000 images."],"url":"http://arxiv.org/abs/2404.18706v1","category":"cs.CV"}
{"created":"2024-04-29 13:42:55","title":"Beyond Gaze Points: Augmenting Eye Movement with Brainwave Data for Multimodal User Authentication in Extended Reality","abstract":"The increasing adoption of Extended Reality (XR) in various applications underscores the need for secure and user-friendly authentication methods. However, existing methods can disrupt the immersive experience in XR settings, or suffer from higher false acceptance rates. In this paper, we introduce a multimodal biometric authentication system that combines eye movement and brainwave patterns, as captured by consumer-grade low-fidelity sensors. Our multimodal authentication exploits the non-invasive and hands-free properties of eye movement and brainwaves to provide a seamless XR user experience and enhanced security as well. Using synchronized eye and brainwave data collected from 30 participants through consumer-grade devices, we investigated whether twin neural networks can utilize these biometrics for identity verification. Our multimodal authentication system yields an excellent Equal Error Rate (EER) of 0.298\\%, which means an 83.6\\% reduction in EER compared to the single eye movement modality or a 93.9\\% reduction in EER compared to the single brainwave modality.","sentences":["The increasing adoption of Extended Reality (XR) in various applications underscores the need for secure and user-friendly authentication methods.","However, existing methods can disrupt the immersive experience in XR settings, or suffer from higher false acceptance rates.","In this paper, we introduce a multimodal biometric authentication system that combines eye movement and brainwave patterns, as captured by consumer-grade low-fidelity sensors.","Our multimodal authentication exploits the non-invasive and hands-free properties of eye movement and brainwaves to provide a seamless XR user experience and enhanced security as well.","Using synchronized eye and brainwave data collected from 30 participants through consumer-grade devices, we investigated whether twin neural networks can utilize these biometrics for identity verification.","Our multimodal authentication system yields an excellent Equal Error Rate (EER) of 0.298\\%, which means an 83.6\\% reduction in EER compared to the single eye movement modality or a 93.9\\% reduction in EER compared to the single brainwave modality."],"url":"http://arxiv.org/abs/2404.18694v1","category":"cs.CR"}
{"created":"2024-04-29 13:15:20","title":"Self-Propelled Collective Motion with Multiplicative Scalar Noise","abstract":"The emergence of order from initial disordered movement in self-propelled collective motion is an instance of nonequilibrium phase transition, which is known to be first order in the thermodynamic limit. Here, we introduce a multiplicative scalar noise model of collective motion as a modification of the original Vicsek model, which more closely mimics the particles' behavior. We allow for more individual movement in sparsely populated neighborhoods, the mechanism of which is not incorporated in the original Vicsek model. This is especially important in the low velocity and density regime where the probability of a clear neighborhood is relatively high. The modification, thus, removes the shortcoming of the Vicsek model in predicting continuous phase transition in this regime. The onset of collective motion in the proposed model is numerically studied in detail, indicating a first order phase transition in both high and low velocity/density regimes for systems with comparatively smaller size which is computationally desirable.","sentences":["The emergence of order from initial disordered movement in self-propelled collective motion is an instance of nonequilibrium phase transition, which is known to be first order in the thermodynamic limit.","Here, we introduce a multiplicative scalar noise model of collective motion as a modification of the original Vicsek model, which more closely mimics the particles' behavior.","We allow for more individual movement in sparsely populated neighborhoods, the mechanism of which is not incorporated in the original Vicsek model.","This is especially important in the low velocity and density regime where the probability of a clear neighborhood is relatively high.","The modification, thus, removes the shortcoming of the Vicsek model in predicting continuous phase transition in this regime.","The onset of collective motion in the proposed model is numerically studied in detail, indicating a first order phase transition in both high and low velocity/density regimes for systems with comparatively smaller size which is computationally desirable."],"url":"http://arxiv.org/abs/2404.18675v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-29 12:46:33","title":"Diversity in the radiation-induced transcriptomic temporal response of mouse brain tissue regions","abstract":"A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities. This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation. This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation. Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy). This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months). The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA). It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees. The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals. Furthermore, in addition to radiation-responsive pathways, Cytoscape's functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2] [3], indicating late behavioural changes in animals prenatally exposed to radiation.","sentences":["A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities.","This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation.","This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation.","Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy).","This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months).","The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA).","It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees.","The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals.","Furthermore, in addition to radiation-responsive pathways, Cytoscape's functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2]","[3], indicating late behavioural changes in animals prenatally exposed to radiation."],"url":"http://arxiv.org/abs/2404.18660v1","category":"q-bio.NC"}
{"created":"2024-04-29 12:28:52","title":"Dynamical Photon Condensation into Wannier-Stark States","abstract":"Strongly coupled light-matter systems can exhibit nonequilibrium collective phenomena due to loss and gain processes on the one hand and effective photon-photon interactions on the other hand. Here we study a photonic lattice system composed of a linear array of driven-dissipative coupled cavities (or cavity modes) with linearly increasing resonance frequencies across the lattice. The model amounts to a driven-dissipative Bose-Hubbard model in a tilted potential without the particle-conservation constraint. We predict a diverse range of stationary and non-stationary states resulted from the interplay of the tilt, tunneling, on-site interactions, and the loss and gain processes. Our key finding is that, under weak on-site interactions, photons mostly Bose condense into a selected, single-particle Wannier-Stark state, instead of exhibiting expected Bloch oscillations. As the strength of the photon-photon interactions increase, a non-stationary regime emerges which is marked surprisingly by periodic Bloch-type oscillations. These intriguing, nontrivial effects are a direct consequence of the driven-dissipative nature of the system.","sentences":["Strongly coupled light-matter systems can exhibit nonequilibrium collective phenomena due to loss and gain processes on the one hand and effective photon-photon interactions on the other hand.","Here we study a photonic lattice system composed of a linear array of driven-dissipative coupled cavities (or cavity modes) with linearly increasing resonance frequencies across the lattice.","The model amounts to a driven-dissipative Bose-Hubbard model in a tilted potential without the particle-conservation constraint.","We predict a diverse range of stationary and non-stationary states resulted from the interplay of the tilt, tunneling, on-site interactions, and the loss and gain processes.","Our key finding is that, under weak on-site interactions, photons mostly Bose condense into a selected, single-particle Wannier-Stark state, instead of exhibiting expected Bloch oscillations.","As the strength of the photon-photon interactions increase, a non-stationary regime emerges which is marked surprisingly by periodic Bloch-type oscillations.","These intriguing, nontrivial effects are a direct consequence of the driven-dissipative nature of the system."],"url":"http://arxiv.org/abs/2404.18647v1","category":"quant-ph"}
{"created":"2024-04-29 12:06:06","title":"4D-DRESS: A 4D Dataset of Real-world Human Clothing with Semantic Annotations","abstract":"The studies of human clothing for digital avatars have predominantly relied on synthetic datasets. While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics. Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes. 4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans. Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans. To address this, we develop a semi-automatic 4D human parsing pipeline. We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements. Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction. 4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing. Website: https://ait.ethz.ch/4d-dress.","sentences":["The studies of human clothing for digital avatars have predominantly relied on synthetic datasets.","While easy to collect, synthetic data often fall short in realism and fail to capture authentic clothing dynamics.","Addressing this gap, we introduce 4D-DRESS, the first real-world 4D dataset advancing human clothing research with its high-quality 4D textured scans and garment meshes.","4D-DRESS captures 64 outfits in 520 human motion sequences, amounting to 78k textured scans.","Creating a real-world clothing dataset is challenging, particularly in annotating and segmenting the extensive and complex 4D human scans.","To address this, we develop a semi-automatic 4D human parsing pipeline.","We efficiently combine a human-in-the-loop process with automation to accurately label 4D scans in diverse garments and body movements.","Leveraging precise annotations and high-quality garment meshes, we establish several benchmarks for clothing simulation and reconstruction.","4D-DRESS offers realistic and challenging data that complements synthetic sources, paving the way for advancements in research of lifelike human clothing.","Website: https://ait.ethz.ch/4d-dress."],"url":"http://arxiv.org/abs/2404.18630v1","category":"cs.CV"}
{"created":"2024-04-29 11:40:27","title":"CoSense3D: an Agent-based Efficient Learning Framework for Collective Perception","abstract":"Collective Perception has attracted significant attention in recent years due to its advantage for mitigating occlusion and expanding the field-of-view, thereby enhancing reliability, efficiency, and, most crucially, decision-making safety. However, developing collective perception models is highly resource demanding due to extensive requirements of processing input data for many agents, usually dozens of images and point clouds for a single frame. This not only slows down the model development process for collective perception but also impedes the utilization of larger models. In this paper, we propose an agent-based training framework that handles the deep learning modules and agent data separately to have a cleaner data flow structure. This framework not only provides an API for flexibly prototyping the data processing pipeline and defining the gradient calculation for each agent, but also provides the user interface for interactive training, testing and data visualization. Training experiment results of four collective object detection models on the prominent collective perception benchmark OPV2V show that the agent-based training can significantly reduce the GPU memory consumption and training time while retaining inference performance. The framework and model implementations are available at \\url{https://github.com/YuanYunshuang/CoSense3D}","sentences":["Collective Perception has attracted significant attention in recent years due to its advantage for mitigating occlusion and expanding the field-of-view, thereby enhancing reliability, efficiency, and, most crucially, decision-making safety.","However, developing collective perception models is highly resource demanding due to extensive requirements of processing input data for many agents, usually dozens of images and point clouds for a single frame.","This not only slows down the model development process for collective perception but also impedes the utilization of larger models.","In this paper, we propose an agent-based training framework that handles the deep learning modules and agent data separately to have a cleaner data flow structure.","This framework not only provides an API for flexibly prototyping the data processing pipeline and defining the gradient calculation for each agent, but also provides the user interface for interactive training, testing and data visualization.","Training experiment results of four collective object detection models on the prominent collective perception benchmark OPV2V show that the agent-based training can significantly reduce the GPU memory consumption and training time while retaining inference performance.","The framework and model implementations are available at \\url{https://github.com/YuanYunshuang/CoSense3D}"],"url":"http://arxiv.org/abs/2404.18617v1","category":"cs.CV"}
{"created":"2024-04-29 11:30:50","title":"Enhancing Prosthetic Safety and Environmental Adaptability: A Visual-Inertial Prosthesis Motion Estimation Approach on Uneven Terrains","abstract":"Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles. However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences. In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them. To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles. Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation. The motion of prosthetic joint and toe are derived using the prosthesis model parameters. Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm. The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains.","sentences":["Environment awareness is crucial for enhancing walking safety and stability of amputee wearing powered prosthesis when crossing uneven terrains such as stairs and obstacles.","However, existing environmental perception systems for prosthesis only provide terrain types and corresponding parameters, which fails to prevent potential collisions when crossing uneven terrains and may lead to falls and other severe consequences.","In this paper, a visual-inertial motion estimation approach is proposed for prosthesis to perceive its movement and the changes of spatial relationship between the prosthesis and uneven terrain when traversing them.","To achieve this, we estimate the knee motion by utilizing a depth camera to perceive the environment and align feature points extracted from stairs and obstacles.","Subsequently, an error-state Kalman filter is incorporated to fuse the inertial data into visual estimations to reduce the feature extraction error and obtain a more robust estimation.","The motion of prosthetic joint and toe are derived using the prosthesis model parameters.","Experiment conducted on our collected dataset and stair walking trials with a powered prosthesis shows that the proposed method can accurately tracking the motion of the human leg and prosthesis with an average root-mean-square error of toe trajectory less than 5 cm.","The proposed method is expected to enable the environmental adaptive control for prosthesis, thereby enhancing amputee's safety and mobility in uneven terrains."],"url":"http://arxiv.org/abs/2404.18612v1","category":"cs.RO"}
{"created":"2024-04-29 11:17:42","title":"Unraveling the Italian and English Telegram Conspiracy Spheres through Message Forwarding","abstract":"Telegram has grown into a significant platform for news and information sharing, favored for its anonymity and minimal moderation. This openness, however, makes it vulnerable to misinformation and conspiracy theories. In this study, we explore the dynamics of conspiratorial narrative dissemination within Telegram, focusing on Italian and English landscapes. In particular, we leverage the mechanism of message forwarding within Telegram and collect two extensive datasets through snowball strategy. We adopt a network-based approach and build the Italian and English Telegram networks to reveal their respective communities. By employing topic modeling, we uncover distinct narratives and dynamics of misinformation spread. Results highlight differences between Italian and English conspiracy landscapes, with Italian discourse involving assorted conspiracy theories and alternative news sources intertwined with legitimate news sources, whereas English discourse is characterized by a more focused approach on specific narratives such as QAnon and political conspiracies. Finally, we show that our methodology exhibits robustness across initial seed selections, suggesting broader applicability. This study contributes to understanding information and misinformation spread on Italian and English Telegram ecosystems through the mechanism of message forwarding","sentences":["Telegram has grown into a significant platform for news and information sharing, favored for its anonymity and minimal moderation.","This openness, however, makes it vulnerable to misinformation and conspiracy theories.","In this study, we explore the dynamics of conspiratorial narrative dissemination within Telegram, focusing on Italian and English landscapes.","In particular, we leverage the mechanism of message forwarding within Telegram and collect two extensive datasets through snowball strategy.","We adopt a network-based approach and build the Italian and English Telegram networks to reveal their respective communities.","By employing topic modeling, we uncover distinct narratives and dynamics of misinformation spread.","Results highlight differences between Italian and English conspiracy landscapes, with Italian discourse involving assorted conspiracy theories and alternative news sources intertwined with legitimate news sources, whereas English discourse is characterized by a more focused approach on specific narratives such as QAnon and political conspiracies.","Finally, we show that our methodology exhibits robustness across initial seed selections, suggesting broader applicability.","This study contributes to understanding information and misinformation spread on Italian and English Telegram ecosystems through the mechanism of message forwarding"],"url":"http://arxiv.org/abs/2404.18602v1","category":"cs.SI"}
{"created":"2024-04-29 11:11:26","title":"FauxPy: A Fault Localization Tool for Python","abstract":"This paper presents FauxPy, a fault localization tool for Python programs. FauxPy supports seven well-known fault localization techniques in four families: spectrum-based, mutation-based, predicate switching, and stack trace fault localization. It is implemented as plugin of the popular Pytest testing framework, but also works with tests written for Unittest and Hypothesis (two other popular testing frameworks). The paper showcases how to use FauxPy on two illustrative examples, and then discusses its main features and capabilities from a user's perspective. To demonstrate that FauxPy is applicable to analyze Python projects of realistic size, the paper also summarizes the results of an extensive experimental evaluation that applied FauxPy to 135 real-world bugs from the BugsInPy curated collection. To our knowledge, FauxPy is the first open-source fault localization tool for Python that supports multiple fault localization families.","sentences":["This paper presents FauxPy, a fault localization tool for Python programs.","FauxPy supports seven well-known fault localization techniques in four families: spectrum-based, mutation-based, predicate switching, and stack trace fault localization.","It is implemented as plugin of the popular Pytest testing framework, but also works with tests written for Unittest and Hypothesis (two other popular testing frameworks).","The paper showcases how to use FauxPy on two illustrative examples, and then discusses its main features and capabilities from a user's perspective.","To demonstrate that FauxPy is applicable to analyze Python projects of realistic size, the paper also summarizes the results of an extensive experimental evaluation that applied FauxPy to 135 real-world bugs from the BugsInPy curated collection.","To our knowledge, FauxPy is the first open-source fault localization tool for Python that supports multiple fault localization families."],"url":"http://arxiv.org/abs/2404.18596v1","category":"cs.SE"}
{"created":"2024-04-29 09:01:39","title":"Downlink Pilots are Essential for Cell-Free Massive MIMO with Multi-Antenna Users","abstract":"We consider a cell-free massive MIMO system with multiple antennas on the users and access points. In previous works, the downlink spectral efficiency (SE) has been evaluated using the hardening bound that requires no downlink pilots. This approach works well when having single-antenna users. In this paper, we show that much higher SEs can be achieved if downlink pilots are sent since the effective channel matrix does not harden when having multi-antenna users. We propose a pilot-based downlink estimation scheme and derive a new SE expression that utilizes zero-forcing combining. We show numerically how the number of users and user antennas affects the SE.","sentences":["We consider a cell-free massive MIMO system with multiple antennas on the users and access points.","In previous works, the downlink spectral efficiency (SE) has been evaluated using the hardening bound that requires no downlink pilots.","This approach works well when having single-antenna users.","In this paper, we show that much higher SEs can be achieved if downlink pilots are sent since the effective channel matrix does not harden when having multi-antenna users.","We propose a pilot-based downlink estimation scheme and derive a new SE expression that utilizes zero-forcing combining.","We show numerically how the number of users and user antennas affects the SE."],"url":"http://arxiv.org/abs/2404.18516v1","category":"eess.SP"}
{"created":"2024-04-29 08:50:17","title":"Towards Image Synthesis with Photon Counting Stellar Intensity Interferometry","abstract":"Stellar intensity interferometry (SII) is based on the correlation of the light intensity fluctuations of a star detected at two or more telescopes, with no need to combine the collected photons directly. A measurement of the correlation in full \"photon-counting mode\" was experimented with fast photon counters in Italy (2016-2020) and is currently being adapted to the ASTRI Mini-Array. Performing image synthesis with \"photon-counting\" SII requires a series of preparatory activities that involve the optimization of the pipelines for the treatment of time series acquired at extremely high photon rates, the development of efficient and innovative algorithms for the cross-correlation of the arrival times in large time series and the development of a preliminary version of a dedicated pipeline for the synthesis of images starting from interferometric data. Here we present the project and the present status of the activities.","sentences":["Stellar intensity interferometry (SII) is based on the correlation of the light intensity fluctuations of a star detected at two or more telescopes, with no need to combine the collected photons directly.","A measurement of the correlation in full \"photon-counting mode\" was experimented with fast photon counters in Italy (2016-2020) and is currently being adapted to the ASTRI Mini-Array.","Performing image synthesis with \"photon-counting\" SII requires a series of preparatory activities that involve the optimization of the pipelines for the treatment of time series acquired at extremely high photon rates, the development of efficient and innovative algorithms for the cross-correlation of the arrival times in large time series and the development of a preliminary version of a dedicated pipeline for the synthesis of images starting from interferometric data.","Here we present the project and the present status of the activities."],"url":"http://arxiv.org/abs/2404.18507v1","category":"astro-ph.IM"}
{"created":"2024-04-29 08:34:35","title":"PHOBIC: Perfect Hashing with Optimized Bucket Sizes and Interleaved Coding","abstract":"A minimal perfect hash function (MPHF) maps a set of n keys to {1, ..., n} without collisions. Such functions find widespread application e.g. in bioinformatics and databases. In this paper we revisit PTHash - a construction technique particularly designed for fast queries. PTHash distributes the input keys into small buckets and, for each bucket, it searches for a hash function seed that places its keys in the output domain without collisions. The collection of all seeds is then stored in a compressed way. Since the first buckets are easier to place, buckets are considered in non-increasing order of size. Additionally, PTHash heuristically produces an imbalanced distribution of bucket sizes by distributing 60% of the keys into 30% of the buckets. Our main contribution is to characterize, up to lower order terms, an optimal distribution of expected bucket sizes. We arrive at a simple, closed form solution which improves construction throughput for space efficient configurations in practice. Our second contribution is a novel encoding scheme for the seeds. We split the keys into partitions. Within each partition, we run the bucket distribution and search step. We then store the seeds in an interleaved way by consecutively placing the seeds for the i-th buckets from all partitions. The seeds for the i-th bucket of each partition follow the same statistical distribution. This allows us to tune a compressor for each bucket. Hence, we call our technique PHOBIC - Perfect Hashing with Optimized Bucket sizes and Interleaved Coding. Compared to PTHash, PHOBIC is 0.17 bits/key more space efficient for same query time and construction throughput. We also contribute a GPU implementation to further accelerate MPHF construction. For a configuration with fast queries, PHOBIC-GPU can construct a perfect hash function at 2.17 bits/key in 28 ns per key, which can be queried in 37 ns on the CPU.","sentences":["A minimal perfect hash function (MPHF) maps a set of n keys to {1, ..., n} without collisions.","Such functions find widespread application e.g. in bioinformatics and databases.","In this paper we revisit PTHash - a construction technique particularly designed for fast queries.","PTHash distributes the input keys into small buckets and, for each bucket, it searches for a hash function seed that places its keys in the output domain without collisions.","The collection of all seeds is then stored in a compressed way.","Since the first buckets are easier to place, buckets are considered in non-increasing order of size.","Additionally, PTHash heuristically produces an imbalanced distribution of bucket sizes by distributing 60% of the keys into 30% of the buckets.","Our main contribution is to characterize, up to lower order terms, an optimal distribution of expected bucket sizes.","We arrive at a simple, closed form solution which improves construction throughput for space efficient configurations in practice.","Our second contribution is a novel encoding scheme for the seeds.","We split the keys into partitions.","Within each partition, we run the bucket distribution and search step.","We then store the seeds in an interleaved way by consecutively placing the seeds for the i-th buckets from all partitions.","The seeds for the i-th bucket of each partition follow the same statistical distribution.","This allows us to tune a compressor for each bucket.","Hence, we call our technique PHOBIC - Perfect Hashing with Optimized Bucket sizes and Interleaved Coding.","Compared to PTHash, PHOBIC is 0.17 bits/key more space efficient for same query time and construction throughput.","We also contribute a GPU implementation to further accelerate MPHF construction.","For a configuration with fast queries, PHOBIC-GPU can construct a perfect hash function at 2.17 bits/key in 28 ns per key, which can be queried in 37 ns on the CPU."],"url":"http://arxiv.org/abs/2404.18497v1","category":"cs.DS"}
{"created":"2024-04-29 08:16:30","title":"Reduced-Rank Multi-objective Policy Learning and Optimization","abstract":"Evaluating the causal impacts of possible interventions is crucial for informing decision-making, especially towards improving access to opportunity. However, if causal effects are heterogeneous and predictable from covariates, personalized treatment decisions can improve individual outcomes and contribute to both efficiency and equity. In practice, however, causal researchers do not have a single outcome in mind a priori and often collect multiple outcomes of interest that are noisy estimates of the true target of interest. For example, in government-assisted social benefit programs, policymakers collect many outcomes to understand the multidimensional nature of poverty. The ultimate goal is to learn an optimal treatment policy that in some sense maximizes multiple outcomes simultaneously. To address such issues, we present a data-driven dimensionality-reduction methodology for multiple outcomes in the context of optimal policy learning with multiple objectives. We learn a low-dimensional representation of the true outcome from the observed outcomes using reduced rank regression. We develop a suite of estimates that use the model to denoise observed outcomes, including commonly-used index weightings. These methods improve estimation error in policy evaluation and optimization, including on a case study of real-world cash transfer and social intervention data. Reducing the variance of noisy social outcomes can improve the performance of algorithmic allocations.","sentences":["Evaluating the causal impacts of possible interventions is crucial for informing decision-making, especially towards improving access to opportunity.","However, if causal effects are heterogeneous and predictable from covariates, personalized treatment decisions can improve individual outcomes and contribute to both efficiency and equity.","In practice, however, causal researchers do not have a single outcome in mind a priori and often collect multiple outcomes of interest that are noisy estimates of the true target of interest.","For example, in government-assisted social benefit programs, policymakers collect many outcomes to understand the multidimensional nature of poverty.","The ultimate goal is to learn an optimal treatment policy that in some sense maximizes multiple outcomes simultaneously.","To address such issues, we present a data-driven dimensionality-reduction methodology for multiple outcomes in the context of optimal policy learning with multiple objectives.","We learn a low-dimensional representation of the true outcome from the observed outcomes using reduced rank regression.","We develop a suite of estimates that use the model to denoise observed outcomes, including commonly-used index weightings.","These methods improve estimation error in policy evaluation and optimization, including on a case study of real-world cash transfer and social intervention data.","Reducing the variance of noisy social outcomes can improve the performance of algorithmic allocations."],"url":"http://arxiv.org/abs/2404.18490v1","category":"cs.LG"}
{"created":"2024-04-29 07:56:36","title":"Exponential synchronization of the Kuramoto model with inertia and frustration under locally coupled network","abstract":"We study the collective synchronized behavior of the Kuramoto model with inertia and frustration effects on a connected and symmetric network. We aim to establish sufficient frameworks for achieving complete frequency synchronization, taking into account initial configuration, small inertia and frustration, and large coupling strength. More precisely, we first demonstrate that the phase diameter will be uniformly bounded by a small value after a finite time. Then we prove that the frequency diameter exhibits exponential decay to zero. Our approach relies on a careful construction of energy functionals, which effectively control the dissipation of phase and frequency diameters.","sentences":["We study the collective synchronized behavior of the Kuramoto model with inertia and frustration effects on a connected and symmetric network.","We aim to establish sufficient frameworks for achieving complete frequency synchronization, taking into account initial configuration, small inertia and frustration, and large coupling strength.","More precisely, we first demonstrate that the phase diameter will be uniformly bounded by a small value after a finite time.","Then we prove that the frequency diameter exhibits exponential decay to zero.","Our approach relies on a careful construction of energy functionals, which effectively control the dissipation of phase and frequency diameters."],"url":"http://arxiv.org/abs/2404.18487v1","category":"math.DS"}
{"created":"2024-04-29 07:53:50","title":"Coupling in situ and remote sensing data to assess $\u03b1$- and $\u03b2$-diversity over biogeographic gradients","abstract":"The challenges presented by climate change are escalating and pose significant threats to global biodiversity, which in turn increases the risk of species extinctions. Therefore, meticulous monitoring efforts are necessary to mitigate the consequential impacts on both human well-being and environmental equilibrium. Biodiversity mapping is pivotal for establishing conservation priorities, often accomplished by assessing alpha, beta, and gamma diversity levels. Two main data sources, in situ and remote sensing (RS) data, are key for this task. In situ methods entail direct data collection from specific study areas, offering detailed insights into ecological patterns, albeit limited by resource constraints. Conversely, RS provides a broader observational platform, albeit at lower spatial resolution than in situ approaches. RS-derived diversity metrics have potential, particularly in linking spectral and biological diversity through high-resolution imagery for precise differentiation at fine scales. Coupling in situ and RS data underscores their complementary nature, contingent upon various factors including study scale and logistical considerations. In situ methods excel in precision, while RS offers efficiency and broader coverage. Despite prior investigations predominantly relying on limited datasets, our study endeavors to employ both in situ and RS data to assess plant and spectral species diversity across France at a high spatial resolution, integrating diverse metrics to unravel different biogeographical structures while gaining in understanding the relationship between plant and spectral diversity within and across bioregions.","sentences":["The challenges presented by climate change are escalating and pose significant threats to global biodiversity, which in turn increases the risk of species extinctions.","Therefore, meticulous monitoring efforts are necessary to mitigate the consequential impacts on both human well-being and environmental equilibrium.","Biodiversity mapping is pivotal for establishing conservation priorities, often accomplished by assessing alpha, beta, and gamma diversity levels.","Two main data sources, in situ and remote sensing (RS) data, are key for this task.","In situ methods entail direct data collection from specific study areas, offering detailed insights into ecological patterns, albeit limited by resource constraints.","Conversely, RS provides a broader observational platform, albeit at lower spatial resolution than in situ approaches.","RS-derived diversity metrics have potential, particularly in linking spectral and biological diversity through high-resolution imagery for precise differentiation at fine scales.","Coupling in situ and RS data underscores their complementary nature, contingent upon various factors including study scale and logistical considerations.","In situ methods excel in precision, while RS offers efficiency and broader coverage.","Despite prior investigations predominantly relying on limited datasets, our study endeavors to employ both in situ and RS data to assess plant and spectral species diversity across France at a high spatial resolution, integrating diverse metrics to unravel different biogeographical structures while gaining in understanding the relationship between plant and spectral diversity within and across bioregions."],"url":"http://arxiv.org/abs/2404.18485v1","category":"q-bio.PE"}
{"created":"2024-04-29 07:11:39","title":"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls using Large Language Model for Stock Performance Prediction","abstract":"In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock performance is a critical challenge that has attracted both academics and investors. While previous studies have used deep learning-based models to obtain a general view of ECCs, they often fail to capture detailed, complex information. Our study introduces a novel framework: \\textbf{ECC Analyzer}, combining Large Language Models (LLMs) and multi-modal techniques to extract richer, more predictive insights. The model begins by summarizing the transcript's structure and analyzing the speakers' mode and confidence level by detecting variations in tone and pitch for audio. This analysis helps investors form an overview perception of the ECCs. Moreover, this model uses the Retrieval-Augmented Generation (RAG) based methods to meticulously extract the focuses that have a significant impact on stock performance from an expert's perspective, providing a more targeted analysis. The model goes a step further by enriching these extracted focuses with additional layers of analysis, such as sentiment and audio segment features. By integrating these insights, the ECC Analyzer performs multi-task predictions of stock performance, including volatility, value-at-risk (VaR), and return for different intervals. The results show that our model outperforms traditional analytic benchmarks, confirming the effectiveness of using advanced LLM techniques in financial analytics.","sentences":["In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock performance is a critical challenge that has attracted both academics and investors.","While previous studies have used deep learning-based models to obtain a general view of ECCs, they often fail to capture detailed, complex information.","Our study introduces a novel framework: \\textbf{ECC Analyzer}, combining Large Language Models (LLMs) and multi-modal techniques to extract richer, more predictive insights.","The model begins by summarizing the transcript's structure and analyzing the speakers' mode and confidence level by detecting variations in tone and pitch for audio.","This analysis helps investors form an overview perception of the ECCs.","Moreover, this model uses the Retrieval-Augmented Generation (RAG) based methods to meticulously extract the focuses that have a significant impact on stock performance from an expert's perspective, providing a more targeted analysis.","The model goes a step further by enriching these extracted focuses with additional layers of analysis, such as sentiment and audio segment features.","By integrating these insights, the ECC Analyzer performs multi-task predictions of stock performance, including volatility, value-at-risk (VaR), and return for different intervals.","The results show that our model outperforms traditional analytic benchmarks, confirming the effectiveness of using advanced LLM techniques in financial analytics."],"url":"http://arxiv.org/abs/2404.18470v1","category":"cs.CE"}
{"created":"2024-04-29 07:11:26","title":"Error-Resilient Weakly Constrained Coding via Row-by-Row Coding","abstract":"A weakly constrained code is a collection of finite-length strings over a finite alphabet in which certain substrings or patterns occur according to some prescribed frequencies. Buzaglo and Siegel (ITW 2017) gave a construction of weakly constrained codes based on row-by-row coding, that achieved the capacity of the weak constraint. In this paper, we propose a method to make this row-by-row coding scheme resilient to errors.","sentences":["A weakly constrained code is a collection of finite-length strings over a finite alphabet in which certain substrings or patterns occur according to some prescribed frequencies.","Buzaglo and Siegel (ITW 2017) gave a construction of weakly constrained codes based on row-by-row coding, that achieved the capacity of the weak constraint.","In this paper, we propose a method to make this row-by-row coding scheme resilient to errors."],"url":"http://arxiv.org/abs/2404.18469v1","category":"cs.IT"}
{"created":"2024-04-29 06:59:30","title":"M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework","abstract":"Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling. Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods. To this end, we introduce M3oE, an adaptive multi-domain multi-task mixture-of-experts recommendation framework. M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives. We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner. Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks. The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization. To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively. Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance. The implementation code is available to ensure reproducibility.","sentences":["Multi-domain recommendation and multi-task recommendation have demonstrated their effectiveness in leveraging common information from different domains and objectives for comprehensive user modeling.","Nonetheless, the practical recommendation usually faces multiple domains and tasks simultaneously, which cannot be well-addressed by current methods.","To this end, we introduce M3oE, an adaptive multi-domain multi-task mixture-of-experts recommendation framework.","M3oE integrates multi-domain information, maps knowledge across domains and tasks, and optimizes multiple objectives.","We leverage three mixture-of-experts modules to learn common, domain-aspect, and task-aspect user preferences respectively to address the complex dependencies among multiple domains and tasks in a disentangled manner.","Additionally, we design a two-level fusion mechanism for precise control over feature extraction and fusion across diverse domains and tasks.","The framework's adaptability is further enhanced by applying AutoML technique, which allows dynamic structure optimization.","To the best of the authors' knowledge, our M3oE is the first effort to solve multi-domain multi-task recommendation self-adaptively.","Extensive experiments on two benchmark datasets against diverse baselines demonstrate M3oE's superior performance.","The implementation code is available to ensure reproducibility."],"url":"http://arxiv.org/abs/2404.18465v1","category":"cs.IR"}
{"created":"2024-04-29 06:42:27","title":"Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we Prompt them in","abstract":"Ethical reasoning is a crucial skill for Large Language Models (LLMs). However, moral values are not universal, but rather influenced by language and culture. This paper explores how three prominent LLMs -- GPT-4, ChatGPT, and Llama2-70B-Chat -- perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted. We extend the study of ethical reasoning of LLMs by Rao et al. (2023) to a multilingual setup following their framework of probing LLMs with ethical dilemmas and policies from three branches of normative ethics: deontology, virtue, and consequentialism. We experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most consistent and unbiased ethical reasoner across languages, while ChatGPT and Llama2-70B-Chat show significant moral value bias when we move to languages other than English. Interestingly, the nature of this bias significantly vary across languages for all LLMs, including GPT-4.","sentences":["Ethical reasoning is a crucial skill for Large Language Models (LLMs).","However, moral values are not universal, but rather influenced by language and culture.","This paper explores how three prominent LLMs -- GPT-4, ChatGPT, and Llama2-70B-Chat -- perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted.","We extend the study of ethical reasoning of LLMs by Rao et al. (2023) to a multilingual setup following their framework of probing LLMs with ethical dilemmas and policies from three branches of normative ethics: deontology, virtue, and consequentialism.","We experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili.","We find that GPT-4 is the most consistent and unbiased ethical reasoner across languages, while ChatGPT and Llama2-70B-Chat show significant moral value bias when we move to languages other than English.","Interestingly, the nature of this bias significantly vary across languages for all LLMs, including GPT-4."],"url":"http://arxiv.org/abs/2404.18460v1","category":"cs.CL"}
{"created":"2024-04-29 06:00:59","title":"Strategic Behavior and AI Training Data","abstract":"Human-created works represent critical data inputs to artificial intelligence (AI). Strategic behavior can play a major role for AI training datasets, be it in limiting access to existing works or in deciding which types of new works to create or whether to create new works at all. We examine creators' behavioral change when their works become training data for AI. Specifically, we focus on contributors on Unsplash, a popular stock image platform with about 6 million high-quality photos and illustrations. In the summer of 2020, Unsplash launched an AI research program by releasing a dataset of 25,000 images for commercial use. We study contributors' reactions, comparing contributors whose works were included in this dataset to contributors whose works were not included. Our results suggest that treated contributors left the platform at a higher-than-usual rate and substantially slowed down the rate of new uploads. Professional and more successful photographers react stronger than amateurs and less successful photographers. We also show that affected users changed the variety and novelty of contributions to the platform, with long-run implications for the stock of works potentially available for AI training. Taken together, our findings highlight the trade-off between interests of rightsholders and promoting innovation at the technological frontier. We discuss implications for copyright and AI policy.","sentences":["Human-created works represent critical data inputs to artificial intelligence (AI).","Strategic behavior can play a major role for AI training datasets, be it in limiting access to existing works or in deciding which types of new works to create or whether to create new works at all.","We examine creators' behavioral change when their works become training data for AI.","Specifically, we focus on contributors on Unsplash, a popular stock image platform with about 6 million high-quality photos and illustrations.","In the summer of 2020, Unsplash launched an AI research program by releasing a dataset of 25,000 images for commercial use.","We study contributors' reactions, comparing contributors whose works were included in this dataset to contributors whose works were not included.","Our results suggest that treated contributors left the platform at a higher-than-usual rate and substantially slowed down the rate of new uploads.","Professional and more successful photographers react stronger than amateurs and less successful photographers.","We also show that affected users changed the variety and novelty of contributions to the platform, with long-run implications for the stock of works potentially available for AI training.","Taken together, our findings highlight the trade-off between interests of rightsholders and promoting innovation at the technological frontier.","We discuss implications for copyright and AI policy."],"url":"http://arxiv.org/abs/2404.18445v1","category":"econ.GN"}
{"created":"2024-04-29 05:57:03","title":"U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models","abstract":"U-Nets are among the most widely used architectures in computer vision, renowned for their exceptional performance in applications such as image segmentation, denoising, and diffusion modeling. However, a theoretical explanation of the U-Net architecture design has not yet been fully established.   This paper introduces a novel interpretation of the U-Net architecture by studying certain generative hierarchical models, which are tree-structured graphical models extensively utilized in both language and image domains. With their encoder-decoder structure, long skip connections, and pooling and up-sampling layers, we demonstrate how U-Nets can naturally implement the belief propagation denoising algorithm in such generative hierarchical models, thereby efficiently approximating the denoising functions. This leads to an efficient sample complexity bound for learning the denoising function using U-Nets within these models. Additionally, we discuss the broader implications of these findings for diffusion models in generative hierarchical models. We also demonstrate that the conventional architecture of convolutional neural networks (ConvNets) is ideally suited for classification tasks within these models. This offers a unified view of the roles of ConvNets and U-Nets, highlighting the versatility of generative hierarchical models in modeling complex data distributions across language and image domains.","sentences":["U-Nets are among the most widely used architectures in computer vision, renowned for their exceptional performance in applications such as image segmentation, denoising, and diffusion modeling.","However, a theoretical explanation of the U-Net architecture design has not yet been fully established.   ","This paper introduces a novel interpretation of the U-Net architecture by studying certain generative hierarchical models, which are tree-structured graphical models extensively utilized in both language and image domains.","With their encoder-decoder structure, long skip connections, and pooling and up-sampling layers, we demonstrate how U-Nets can naturally implement the belief propagation denoising algorithm in such generative hierarchical models, thereby efficiently approximating the denoising functions.","This leads to an efficient sample complexity bound for learning the denoising function using U-Nets within these models.","Additionally, we discuss the broader implications of these findings for diffusion models in generative hierarchical models.","We also demonstrate that the conventional architecture of convolutional neural networks (ConvNets) is ideally suited for classification tasks within these models.","This offers a unified view of the roles of ConvNets and U-Nets, highlighting the versatility of generative hierarchical models in modeling complex data distributions across language and image domains."],"url":"http://arxiv.org/abs/2404.18444v1","category":"cs.LG"}
{"created":"2024-04-29 05:40:08","title":"BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers","abstract":"Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources. We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever's efficacy on various biomedical applications. BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters. The training data and model checkpoints are released at \\url{https://huggingface.co/BMRetriever} to ensure transparency, reproducibility, and application to new domains.","sentences":["Developing effective biomedical retrieval models is important for excelling at knowledge-intensive biomedical tasks but still challenging due to the deficiency of sufficient publicly annotated biomedical data and computational resources.","We present BMRetriever, a series of dense retrievers for enhancing biomedical retrieval via unsupervised pre-training on large biomedical corpora, followed by instruction fine-tuning on a combination of labeled datasets and synthetic pairs.","Experiments on 5 biomedical tasks across 11 datasets verify BMRetriever's efficacy on various biomedical applications.","BMRetriever also exhibits strong parameter efficiency, with the 410M variant outperforming baselines up to 11.7 times larger, and the 2B variant matching the performance of models with over 5B parameters.","The training data and model checkpoints are released at \\url{https://huggingface.co/BMRetriever} to ensure transparency, reproducibility, and application to new domains."],"url":"http://arxiv.org/abs/2404.18443v1","category":"cs.CL"}
{"created":"2024-04-29 05:32:48","title":"Potential Paradigm Shift in Hazard Risk Management: AI-Based Weather Forecast for Tropical Cyclone Hazards","abstract":"The advents of Artificial Intelligence (AI)-driven models marks a paradigm shift in risk management strategies for meteorological hazards. This study specifically employs tropical cyclones (TCs) as a focal example. We engineer a perturbation-based method to produce ensemble forecasts using the advanced Pangu AI weather model. Unlike traditional approaches that often generate fewer than 20 scenarios from Weather Research and Forecasting (WRF) simulations for one event, our method facilitates the rapid nature of AI-driven model to create thousands of scenarios. We offer open-source access to our model and evaluate its effectiveness through retrospective case studies of significant TC events: Hurricane Irma (2017), Typhoon Mangkhut (2018), and TC Debbie (2017), affecting regions across North America, East Asia, and Australia. Our findings indicate that the AI-generated ensemble forecasts align closely with the European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble predictions up to seven days prior to landfall. This approach could substantially enhance the effectiveness of weather forecast-driven risk analysis and management, providing unprecedented operational speed, user-friendliness, and global applicability.","sentences":["The advents of Artificial Intelligence (AI)-driven models marks a paradigm shift in risk management strategies for meteorological hazards.","This study specifically employs tropical cyclones (TCs) as a focal example.","We engineer a perturbation-based method to produce ensemble forecasts using the advanced Pangu AI weather model.","Unlike traditional approaches that often generate fewer than 20 scenarios from Weather Research and Forecasting (WRF) simulations for one event, our method facilitates the rapid nature of AI-driven model to create thousands of scenarios.","We offer open-source access to our model and evaluate its effectiveness through retrospective case studies of significant TC events: Hurricane Irma (2017), Typhoon Mangkhut (2018), and TC Debbie (2017), affecting regions across North America, East Asia, and Australia.","Our findings indicate that the AI-generated ensemble forecasts align closely with the European Centre for Medium-Range Weather Forecasts (ECMWF) ensemble predictions up to seven days prior to landfall.","This approach could substantially enhance the effectiveness of weather forecast-driven risk analysis and management, providing unprecedented operational speed, user-friendliness, and global applicability."],"url":"http://arxiv.org/abs/2404.18440v1","category":"physics.ao-ph"}
{"created":"2024-04-29 05:00:40","title":"The Jive Verification System and its Transformative Impact on Weather Forecasting Operations","abstract":"Forecast verification is critical for continuous improvement in meteorological organizations. The Jive verification system was originally developed to assess the accuracy of public weather forecasts issued by the Australian Bureau of Meteorology. It started as a research project in 2015 and gradually evolved into the operational verification system that went live in 2022. The system includes daily verification dashboards for forecasters to visualize recent forecast performance and \"Evidence Targeted Automation\" dashboards for exploring the performance of competing forecast systems. Additionally, there is a Jupyter Notebook server with the Jive Python library which supports research experiments, case studies, and the development of new verification metrics and tools.   This paper shows how the Jive verification project helped bring verification to the forefront at the Bureau of Meteorology, leading to more accurate, streamlined forecasts. Jive has been used to provide evidence for forecast automation decisions and has helped to understand the evolving role of meteorologists in the forecast process. It has given operational meteorologists tools for evaluating forecast processes, including identifying when and how manual interventions lead to superior predictions. The project also led to new verification science, including novel metrics that are decision-focused, including for extreme conditions. Additionally, Jive has provided the Bureau with an enterprise-wide data analysis environment and has prompted a clarification of forecast definitions.   These collective impacts have resulted in more accurate forecasts, ultimately benefiting society, and building trust with forecast users. These positive outcomes highlight the importance of meteorological organizations investing in verification science and technology.","sentences":["Forecast verification is critical for continuous improvement in meteorological organizations.","The Jive verification system was originally developed to assess the accuracy of public weather forecasts issued by the Australian Bureau of Meteorology.","It started as a research project in 2015 and gradually evolved into the operational verification system that went live in 2022.","The system includes daily verification dashboards for forecasters to visualize recent forecast performance and \"Evidence Targeted Automation\" dashboards for exploring the performance of competing forecast systems.","Additionally, there is a Jupyter Notebook server with the Jive Python library which supports research experiments, case studies, and the development of new verification metrics and tools.   ","This paper shows how the Jive verification project helped bring verification to the forefront at the Bureau of Meteorology, leading to more accurate, streamlined forecasts.","Jive has been used to provide evidence for forecast automation decisions and has helped to understand the evolving role of meteorologists in the forecast process.","It has given operational meteorologists tools for evaluating forecast processes, including identifying when and how manual interventions lead to superior predictions.","The project also led to new verification science, including novel metrics that are decision-focused, including for extreme conditions.","Additionally, Jive has provided the Bureau with an enterprise-wide data analysis environment and has prompted a clarification of forecast definitions.   ","These collective impacts have resulted in more accurate forecasts, ultimately benefiting society, and building trust with forecast users.","These positive outcomes highlight the importance of meteorological organizations investing in verification science and technology."],"url":"http://arxiv.org/abs/2404.18429v1","category":"physics.ao-ph"}
{"created":"2024-04-29 04:59:40","title":"Geospatial Big Data: Survey and Challenges","abstract":"In recent years, geospatial big data (GBD) has obtained attention across various disciplines, categorized into big earth observation data and big human behavior data. Identifying geospatial patterns from GBD has been a vital research focus in the fields of urban management and environmental sustainability. This paper reviews the evolution of GBD mining and its integration with advanced artificial intelligence (AI) techniques. GBD consists of data generated by satellites, sensors, mobile devices, and geographical information systems, and we categorize geospatial data based on different perspectives. We outline the process of GBD mining and demonstrate how it can be incorporated into a unified framework. Additionally, we explore new technologies like large language models (LLM), the Metaverse, and knowledge graphs, and how they could make GBD even more useful. We also share examples of GBD helping with city management and protecting the environment. Finally, we discuss the real challenges that come up when working with GBD, such as issues with data retrieval and security. Our goal is to give readers a clear view of where GBD mining stands today and where it might go next.","sentences":["In recent years, geospatial big data (GBD) has obtained attention across various disciplines, categorized into big earth observation data and big human behavior data.","Identifying geospatial patterns from GBD has been a vital research focus in the fields of urban management and environmental sustainability.","This paper reviews the evolution of GBD mining and its integration with advanced artificial intelligence (AI) techniques.","GBD consists of data generated by satellites, sensors, mobile devices, and geographical information systems, and we categorize geospatial data based on different perspectives.","We outline the process of GBD mining and demonstrate how it can be incorporated into a unified framework.","Additionally, we explore new technologies like large language models (LLM), the Metaverse, and knowledge graphs, and how they could make GBD even more useful.","We also share examples of GBD helping with city management and protecting the environment.","Finally, we discuss the real challenges that come up when working with GBD, such as issues with data retrieval and security.","Our goal is to give readers a clear view of where GBD mining stands today and where it might go next."],"url":"http://arxiv.org/abs/2404.18428v1","category":"cs.DB"}
{"created":"2024-04-29 04:47:23","title":"Unsupervised Dynamics Prediction with Object-Centric Kinematics","abstract":"Human perception involves discerning complex multi-object scenes into time-static object appearance (\\ie, size, shape, color) and time-varying object motion (\\ie, location, velocity, acceleration). This innate ability to unconsciously understand the environment is the motivation behind the success of dynamics modeling. Object-centric representations have emerged as a promising tool for dynamics prediction, yet they primarily focus on the objects' appearance, often overlooking other crucial attributes. In this paper, we propose Object-Centric Kinematics (OCK), a framework for dynamics prediction leveraging object-centric representations. Our model utilizes a novel component named object kinematics, which comprises low-level structured states of objects' position, velocity, and acceleration. The object kinematics are obtained via either implicit or explicit approaches, enabling comprehensive spatiotemporal object reasoning, and integrated through various transformer mechanisms, facilitating effective object-centric dynamics modeling. Our model demonstrates superior performance when handling objects and backgrounds in complex scenes characterized by a wide range of object attributes and dynamic movements. Moreover, our model demonstrates generalization capabilities across diverse synthetic environments, highlighting its potential for broad applicability in vision-related tasks.","sentences":["Human perception involves discerning complex multi-object scenes into time-static object appearance (\\ie, size, shape, color) and time-varying object motion (\\ie, location, velocity, acceleration).","This innate ability to unconsciously understand the environment is the motivation behind the success of dynamics modeling.","Object-centric representations have emerged as a promising tool for dynamics prediction, yet they primarily focus on the objects' appearance, often overlooking other crucial attributes.","In this paper, we propose Object-Centric Kinematics (OCK), a framework for dynamics prediction leveraging object-centric representations.","Our model utilizes a novel component named object kinematics, which comprises low-level structured states of objects' position, velocity, and acceleration.","The object kinematics are obtained via either implicit or explicit approaches, enabling comprehensive spatiotemporal object reasoning, and integrated through various transformer mechanisms, facilitating effective object-centric dynamics modeling.","Our model demonstrates superior performance when handling objects and backgrounds in complex scenes characterized by a wide range of object attributes and dynamic movements.","Moreover, our model demonstrates generalization capabilities across diverse synthetic environments, highlighting its potential for broad applicability in vision-related tasks."],"url":"http://arxiv.org/abs/2404.18423v1","category":"cs.CV"}
{"created":"2024-04-29 04:32:11","title":"Research on Intelligent Aided Diagnosis System of Medical Image Based on Computer Deep Learning","abstract":"This paper combines Struts and Hibernate two architectures together, using DAO (Data Access Object) to store and access data. Then a set of dual-mode humidity medical image library suitable for deep network is established, and a dual-mode medical image assisted diagnosis method based on the image is proposed. Through the test of various feature extraction methods, the optimal operating characteristic under curve product (AUROC) is 0.9985, the recall rate is 0.9814, and the accuracy is 0.9833. This method can be applied to clinical diagnosis, and it is a practical method. Any outpatient doctor can register quickly through the system, or log in to the platform to upload the image to obtain more accurate images. Through the system, each outpatient physician can quickly register or log in to the platform for image uploading, thus obtaining more accurate images. The segmentation of images can guide doctors in clinical departments. Then the image is analyzed to determine the location and nature of the tumor, so as to make targeted treatment.","sentences":["This paper combines Struts and Hibernate two architectures together, using DAO (Data Access Object) to store and access data.","Then a set of dual-mode humidity medical image library suitable for deep network is established, and a dual-mode medical image assisted diagnosis method based on the image is proposed.","Through the test of various feature extraction methods, the optimal operating characteristic under curve product (AUROC) is 0.9985, the recall rate is 0.9814, and the accuracy is 0.9833.","This method can be applied to clinical diagnosis, and it is a practical method.","Any outpatient doctor can register quickly through the system, or log in to the platform to upload the image to obtain more accurate images.","Through the system, each outpatient physician can quickly register or log in to the platform for image uploading, thus obtaining more accurate images.","The segmentation of images can guide doctors in clinical departments.","Then the image is analyzed to determine the location and nature of the tumor, so as to make targeted treatment."],"url":"http://arxiv.org/abs/2404.18419v1","category":"cs.CV"}
{"created":"2024-04-29 04:11:28","title":"Capabilities of Gemini Models in Medicine","abstract":"Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.","sentences":["Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data.","Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine.","Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders.","We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin.","On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1% accuracy, using a novel uncertainty-guided search strategy.","On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health & medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5%.","We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning.","Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education.","Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain."],"url":"http://arxiv.org/abs/2404.18416v1","category":"cs.AI"}
{"created":"2024-04-29 04:01:30","title":"3AM: An Ambiguity-Aware Multi-Modal Machine Translation Dataset","abstract":"Multimodal machine translation (MMT) is a challenging task that seeks to improve translation quality by incorporating visual information. However, recent studies have indicated that the visual information provided by existing MMT datasets is insufficient, causing models to disregard it and overestimate their capabilities. This issue presents a significant obstacle to the development of MMT research. This paper presents a novel solution to this issue by introducing 3AM, an ambiguity-aware MMT dataset comprising 26,000 parallel sentence pairs in English and Chinese, each with corresponding images. Our dataset is specifically designed to include more ambiguity and a greater variety of both captions and images than other MMT datasets. We utilize a word sense disambiguation model to select ambiguous data from vision-and-language datasets, resulting in a more challenging dataset. We further benchmark several state-of-the-art MMT models on our proposed dataset. Experimental results show that MMT models trained on our dataset exhibit a greater ability to exploit visual information than those trained on other MMT datasets. Our work provides a valuable resource for researchers in the field of multimodal learning and encourages further exploration in this area. The data, code and scripts are freely available at https://github.com/MaxyLee/3AM.","sentences":["Multimodal machine translation (MMT) is a challenging task that seeks to improve translation quality by incorporating visual information.","However, recent studies have indicated that the visual information provided by existing MMT datasets is insufficient, causing models to disregard it and overestimate their capabilities.","This issue presents a significant obstacle to the development of MMT research.","This paper presents a novel solution to this issue by introducing 3AM, an ambiguity-aware MMT dataset comprising 26,000 parallel sentence pairs in English and Chinese, each with corresponding images.","Our dataset is specifically designed to include more ambiguity and a greater variety of both captions and images than other MMT datasets.","We utilize a word sense disambiguation model to select ambiguous data from vision-and-language datasets, resulting in a more challenging dataset.","We further benchmark several state-of-the-art MMT models on our proposed dataset.","Experimental results show that MMT models trained on our dataset exhibit a greater ability to exploit visual information than those trained on other MMT datasets.","Our work provides a valuable resource for researchers in the field of multimodal learning and encourages further exploration in this area.","The data, code and scripts are freely available at https://github.com/MaxyLee/3AM."],"url":"http://arxiv.org/abs/2404.18413v1","category":"cs.CV"}
{"created":"2024-04-29 03:57:43","title":"PKU-AIGIQA-4K: A Perceptual Quality Assessment Database for Both Text-to-Image and Image-to-Image AI-Generated Images","abstract":"In recent years, image generation technology has rapidly advanced, resulting in the creation of a vast array of AI-generated images (AIGIs). However, the quality of these AIGIs is highly inconsistent, with low-quality AIGIs severely impairing the visual experience of users. Due to the widespread application of AIGIs, the AI-generated image quality assessment (AIGIQA), aimed at evaluating the quality of AIGIs from the perspective of human perception, has garnered increasing interest among scholars. Nonetheless, current research has not yet fully explored this field. We have observed that existing databases are limited to images generated from single scenario settings. Databases such as AGIQA-1K, AGIQA-3K, and AIGCIQA2023, for example, only include images generated by text-to-image generative models. This oversight highlights a critical gap in the current research landscape, underscoring the need for dedicated databases catering to image-to-image scenarios, as well as more comprehensive databases that encompass a broader range of AI-generated image scenarios. Addressing these issues, we have established a large scale perceptual quality assessment database for both text-to-image and image-to-image AIGIs, named PKU-AIGIQA-4K. We then conduct a well-organized subjective experiment to collect quality labels for AIGIs and perform a comprehensive analysis of the PKU-AIGIQA-4K database. Regarding the use of image prompts during the training process, we propose three image quality assessment (IQA) methods based on pre-trained models that include a no-reference method NR-AIGCIQA, a full-reference method FR-AIGCIQA, and a partial-reference method PR-AIGCIQA. Finally, leveraging the PKU-AIGIQA-4K database, we conduct extensive benchmark experiments and compare the performance of the proposed methods and the current IQA methods.","sentences":["In recent years, image generation technology has rapidly advanced, resulting in the creation of a vast array of AI-generated images (AIGIs).","However, the quality of these AIGIs is highly inconsistent, with low-quality AIGIs severely impairing the visual experience of users.","Due to the widespread application of AIGIs, the AI-generated image quality assessment (AIGIQA), aimed at evaluating the quality of AIGIs from the perspective of human perception, has garnered increasing interest among scholars.","Nonetheless, current research has not yet fully explored this field.","We have observed that existing databases are limited to images generated from single scenario settings.","Databases such as AGIQA-1K, AGIQA-3K, and AIGCIQA2023, for example, only include images generated by text-to-image generative models.","This oversight highlights a critical gap in the current research landscape, underscoring the need for dedicated databases catering to image-to-image scenarios, as well as more comprehensive databases that encompass a broader range of AI-generated image scenarios.","Addressing these issues, we have established a large scale perceptual quality assessment database for both text-to-image and image-to-image AIGIs, named PKU-AIGIQA-4K. We then conduct a well-organized subjective experiment to collect quality labels for AIGIs and perform a comprehensive analysis of the PKU-AIGIQA-4K database.","Regarding the use of image prompts during the training process, we propose three image quality assessment (IQA) methods based on pre-trained models that include a no-reference method NR-AIGCIQA, a full-reference method FR-AIGCIQA, and a partial-reference method PR-AIGCIQA.","Finally, leveraging the PKU-AIGIQA-4K database, we conduct extensive benchmark experiments and compare the performance of the proposed methods and the current IQA methods."],"url":"http://arxiv.org/abs/2404.18409v1","category":"cs.CV"}
{"created":"2024-04-29 03:30:06","title":"LLM-SR: Scientific Equation Discovery via Programming with Large Language Models","abstract":"Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces. Traditional methods of equation discovery largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeletons, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters. We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established equation discovery baselines","sentences":["Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines.","However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces.","Traditional methods of equation discovery largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on.","To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner.","Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs.","The LLM iteratively proposes new equation skeletons, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters.","We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established equation discovery baselines"],"url":"http://arxiv.org/abs/2404.18400v1","category":"cs.LG"}
{"created":"2024-04-29 02:45:23","title":"Equivalence: An analysis of artists' roles with Image Generative AI from Conceptual Art perspective through an interactive installation design practice","abstract":"Over the past year, the emergence of advanced text-to-image Generative AI models has significantly impacted the art world, challenging traditional notions of creativity and the role of artists. This study explores how artists interact with these technologies, using a 5P model (Purpose, People, Process, Product, and Press) based on Rhodes' creativity framework to compare the artistic processes behind Conceptual Art and Image Generative AI. To exemplify this framework, a practical case study titled \"Equivalence\", a multi-screen interactive installation that converts users' speech input into continuously evolving paintings developed based on Stable Diffusion and NLP algorithms, was developed. Through comprehensive analysis and the case study, this work aims to broaden our understanding of artists' roles and foster a deeper appreciation for the creative aspects inherent in artwork created with Image Generative AI.","sentences":["Over the past year, the emergence of advanced text-to-image Generative AI models has significantly impacted the art world, challenging traditional notions of creativity and the role of artists.","This study explores how artists interact with these technologies, using a 5P model (Purpose, People, Process, Product, and Press) based on Rhodes' creativity framework to compare the artistic processes behind Conceptual Art and Image Generative AI.","To exemplify this framework, a practical case study titled \"Equivalence\", a multi-screen interactive installation that converts users' speech input into continuously evolving paintings developed based on Stable Diffusion and NLP algorithms, was developed.","Through comprehensive analysis and the case study, this work aims to broaden our understanding of artists' roles and foster a deeper appreciation for the creative aspects inherent in artwork created with Image Generative AI."],"url":"http://arxiv.org/abs/2404.18385v2","category":"cs.HC"}
{"created":"2024-04-29 02:25:49","title":"Field Notes on Deploying Research Robots in Public Spaces","abstract":"Human-robot interaction requires to be studied in the wild. In the summers of 2022 and 2023, we deployed two trash barrel service robots through the wizard-of-oz protocol in public spaces to study human-robot interactions in urban settings. We deployed the robots at two different public plazas in downtown Manhattan and Brooklyn for a collective of 20 hours of field time. To date, relatively few long-term human-robot interaction studies have been conducted in shared public spaces. To support researchers aiming to fill this gap, we would like to share some of our insights and learned lessons that would benefit both researchers and practitioners on how to deploy robots in public spaces. We share best practices and lessons learned with the HRI research community to encourage more in-the-wild research of robots in public spaces and call for the community to share their lessons learned to a GitHub repository.","sentences":["Human-robot interaction requires to be studied in the wild.","In the summers of 2022 and 2023, we deployed two trash barrel service robots through the wizard-of-oz protocol in public spaces to study human-robot interactions in urban settings.","We deployed the robots at two different public plazas in downtown Manhattan and Brooklyn for a collective of 20 hours of field time.","To date, relatively few long-term human-robot interaction studies have been conducted in shared public spaces.","To support researchers aiming to fill this gap, we would like to share some of our insights and learned lessons that would benefit both researchers and practitioners on how to deploy robots in public spaces.","We share best practices and lessons learned with the HRI research community to encourage more in-the-wild research of robots in public spaces and call for the community to share their lessons learned to a GitHub repository."],"url":"http://arxiv.org/abs/2404.18375v1","category":"cs.RO"}
{"created":"2024-04-29 02:23:53","title":"6G comprehensive intelligence: network operations and optimization based on Large Language Models","abstract":"The sixth generation mobile communication standard (6G) can promote the development of Industrial Internet and Internet of Things (IoT). To achieve comprehensive intelligent development of the network and provide customers with higher quality personalized services. This paper proposes a network performance optimization and intelligent operation network architecture based on Large Language Model (LLM), aiming to build a comprehensive intelligent 6G network system. The Large Language Model, with more parameters and stronger learning ability, can more accurately capture patterns and features in data, which can achieve more accurate content output and high intelligence and provide strong support for related research such as network data security, privacy protection, and health assessment. This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.","sentences":["The sixth generation mobile communication standard (6G) can promote the development of Industrial Internet and Internet of Things (IoT).","To achieve comprehensive intelligent development of the network and provide customers with higher quality personalized services.","This paper proposes a network performance optimization and intelligent operation network architecture based on Large Language Model (LLM), aiming to build a comprehensive intelligent 6G network system.","The Large Language Model, with more parameters and stronger learning ability, can more accurately capture patterns and features in data, which can achieve more accurate content output and high intelligence and provide strong support for related research such as network data security, privacy protection, and health assessment.","This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence."],"url":"http://arxiv.org/abs/2404.18373v1","category":"cs.NI"}
{"created":"2024-04-29 02:10:35","title":"\"What Keeps People Secure is That They Met The Security Team\": Deconstructing Drivers And Goals of Organizational Security Awareness","abstract":"Security awareness campaigns in organizations now collectively cost billions of dollars annually. There is increasing focus on ensuring certain security behaviors among employees. On the surface, this would imply a user-centered view of security in organizations. Despite this, the basis of what security awareness managers do and what decides this are unclear. We conducted n=15 semi-structured interviews with full-time security awareness managers, with experience across various national and international companies in European countries, with thousands of employees. Through thematic analysis, we identify that success in awareness management is fragile while having the potential to improve; there are a range of restrictions, and mismatched drivers and goals for security awareness, affecting how it is structured, delivered, measured, and improved. We find that security awareness as a practice is underspecified, and split between messaging around secure behaviors and connecting to employees, with a lack of recognition for the measures that awareness managers regard as important. We discuss ways forward, including alternative indicators of success, and security usability advocacy for employees.","sentences":["Security awareness campaigns in organizations now collectively cost billions of dollars annually.","There is increasing focus on ensuring certain security behaviors among employees.","On the surface, this would imply a user-centered view of security in organizations.","Despite this, the basis of what security awareness managers do and what decides this are unclear.","We conducted n=15 semi-structured interviews with full-time security awareness managers, with experience across various national and international companies in European countries, with thousands of employees.","Through thematic analysis, we identify that success in awareness management is fragile while having the potential to improve; there are a range of restrictions, and mismatched drivers and goals for security awareness, affecting how it is structured, delivered, measured, and improved.","We find that security awareness as a practice is underspecified, and split between messaging around secure behaviors and connecting to employees, with a lack of recognition for the measures that awareness managers regard as important.","We discuss ways forward, including alternative indicators of success, and security usability advocacy for employees."],"url":"http://arxiv.org/abs/2404.18365v1","category":"cs.CR"}
{"created":"2024-04-29 02:04:07","title":"Reactive Composition of UAV Delivery Services in Urban Environments","abstract":"We propose a novel failure-aware reactive UAV delivery service composition framework. A skyway network infrastructure is presented for the effective provisioning of services in urban areas. We present a formal drone delivery service model and a system architecture for reactive drone delivery services. We develop radius-based, cell density-based, and two-phased algorithms to reduce the search space and perform reactive service compositions when a service failure occurs. We conduct a set of experiments with a real drone dataset to demonstrate the effectiveness of our proposed approach.","sentences":["We propose a novel failure-aware reactive UAV delivery service composition framework.","A skyway network infrastructure is presented for the effective provisioning of services in urban areas.","We present a formal drone delivery service model and a system architecture for reactive drone delivery services.","We develop radius-based, cell density-based, and two-phased algorithms to reduce the search space and perform reactive service compositions when a service failure occurs.","We conduct a set of experiments with a real drone dataset to demonstrate the effectiveness of our proposed approach."],"url":"http://arxiv.org/abs/2404.18363v1","category":"cs.RO"}
{"created":"2024-04-29 01:49:07","title":"FoundaBench: Evaluating Chinese Fundamental Knowledge Capabilities of Large Language Models","abstract":"In the burgeoning field of large language models (LLMs), the assessment of fundamental knowledge remains a critical challenge, particularly for models tailored to Chinese language and culture. This paper introduces FoundaBench, a pioneering benchmark designed to rigorously evaluate the fundamental knowledge capabilities of Chinese LLMs. FoundaBench encompasses a diverse array of 3354 multiple-choice questions across common sense and K-12 educational subjects, meticulously curated to reflect the breadth and depth of everyday and academic knowledge. We present an extensive evaluation of 12 state-of-the-art LLMs using FoundaBench, employing both traditional assessment methods and our CircularEval protocol to mitigate potential biases in model responses. Our results highlight the superior performance of models pre-trained on Chinese corpora, and reveal a significant disparity between models' reasoning and memory recall capabilities. The insights gleaned from FoundaBench evaluations set a new standard for understanding the fundamental knowledge of LLMs, providing a robust framework for future advancements in the field.","sentences":["In the burgeoning field of large language models (LLMs), the assessment of fundamental knowledge remains a critical challenge, particularly for models tailored to Chinese language and culture.","This paper introduces FoundaBench, a pioneering benchmark designed to rigorously evaluate the fundamental knowledge capabilities of Chinese LLMs.","FoundaBench encompasses a diverse array of 3354 multiple-choice questions across common sense and K-12 educational subjects, meticulously curated to reflect the breadth and depth of everyday and academic knowledge.","We present an extensive evaluation of 12 state-of-the-art LLMs using FoundaBench, employing both traditional assessment methods and our CircularEval protocol to mitigate potential biases in model responses.","Our results highlight the superior performance of models pre-trained on Chinese corpora, and reveal a significant disparity between models' reasoning and memory recall capabilities.","The insights gleaned from FoundaBench evaluations set a new standard for understanding the fundamental knowledge of LLMs, providing a robust framework for future advancements in the field."],"url":"http://arxiv.org/abs/2404.18359v1","category":"cs.CL"}
{"created":"2024-04-29 01:35:58","title":"Pi\u00e8ces de viole des Cinq Livres and their statistical signatures: the musical work of Marin Marais and Jordi Savall","abstract":"This study analyzes the spectrum of audio signals related to the work of \"Pi\\`eces de viole des Cinq Livres\" based on the collaborative work between Marin Marais and Jordi Savall for the underlying musical information. In particular, we explore the identification of possible statistical signatures related to this musical work. Based on the complex systems approach, we compute the spectrum of audio signals, analyze and identify their best-fit statistical distributions, and plot their relative frequencies using the scientific pitch notation. Findings suggest that the collection of frequency components related to the spectrum of each of the books that form this audio work show highly skewed and associated statistical distributions. Therefore, the most frequent statistical distribution that best describes the collection of these audio data and may be associated with a singular statistical signature is the exponential.","sentences":["This study analyzes the spectrum of audio signals related to the work of \"Pi\\`eces de viole des Cinq Livres\" based on the collaborative work between Marin Marais and Jordi Savall for the underlying musical information.","In particular, we explore the identification of possible statistical signatures related to this musical work.","Based on the complex systems approach, we compute the spectrum of audio signals, analyze and identify their best-fit statistical distributions, and plot their relative frequencies using the scientific pitch notation.","Findings suggest that the collection of frequency components related to the spectrum of each of the books that form this audio work show highly skewed and associated statistical distributions.","Therefore, the most frequent statistical distribution that best describes the collection of these audio data and may be associated with a singular statistical signature is the exponential."],"url":"http://arxiv.org/abs/2404.18355v1","category":"cs.SD"}
{"created":"2024-04-29 01:24:14","title":"Do Neutral Prompts Produce Insecure Code? FormAI-v2 Dataset: Labelling Vulnerabilities in Code Generated by Large Language Models","abstract":"This study provides a comparative analysis of state-of-the-art large language models (LLMs), analyzing how likely they generate vulnerabilities when writing simple C programs using a neutral zero-shot prompt. We address a significant gap in the literature concerning the security properties of code produced by these models without specific directives. N. Tihanyi et al. introduced the FormAI dataset at PROMISE '23, containing 112,000 GPT-3.5-generated C programs, with over 51.24% identified as vulnerable. We expand that work by introducing the FormAI-v2 dataset comprising 265,000 compilable C programs generated using various LLMs, including robust models such as Google's GEMINI-pro, OpenAI's GPT-4, and TII's 180 billion-parameter Falcon, to Meta's specialized 13 billion-parameter CodeLLama2 and various other compact models. Each program in the dataset is labelled based on the vulnerabilities detected in its source code through formal verification using the Efficient SMT-based Context-Bounded Model Checker (ESBMC). This technique eliminates false positives by delivering a counterexample and ensures the exclusion of false negatives by completing the verification process. Our study reveals that at least 63.47% of the generated programs are vulnerable. The differences between the models are minor, as they all display similar coding errors with slight variations. Our research highlights that while LLMs offer promising capabilities for code generation, deploying their output in a production environment requires risk assessment and validation.","sentences":["This study provides a comparative analysis of state-of-the-art large language models (LLMs), analyzing how likely they generate vulnerabilities when writing simple C programs using a neutral zero-shot prompt.","We address a significant gap in the literature concerning the security properties of code produced by these models without specific directives.","N. Tihanyi et al. introduced the FormAI dataset at PROMISE '23, containing 112,000 GPT-3.5-generated C programs, with over 51.24% identified as vulnerable.","We expand that work by introducing the FormAI-v2 dataset comprising 265,000 compilable C programs generated using various LLMs, including robust models such as Google's GEMINI-pro, OpenAI's GPT-4, and TII's 180 billion-parameter Falcon, to Meta's specialized 13 billion-parameter CodeLLama2 and various other compact models.","Each program in the dataset is labelled based on the vulnerabilities detected in its source code through formal verification using the Efficient SMT-based Context-Bounded Model Checker (ESBMC).","This technique eliminates false positives by delivering a counterexample and ensures the exclusion of false negatives by completing the verification process.","Our study reveals that at least 63.47% of the generated programs are vulnerable.","The differences between the models are minor, as they all display similar coding errors with slight variations.","Our research highlights that while LLMs offer promising capabilities for code generation, deploying their output in a production environment requires risk assessment and validation."],"url":"http://arxiv.org/abs/2404.18353v1","category":"cs.CR"}
{"created":"2024-04-29 01:19:17","title":"Post-hoc and manifold explanations analysis of facial expression data based on deep learning","abstract":"The complex information processing system of humans generates a lot of objective and subjective evaluations, making the exploration of human cognitive products of great cutting-edge theoretical value. In recent years, deep learning technologies, which are inspired by biological brain mechanisms, have made significant strides in the application of psychological or cognitive scientific research, particularly in the memorization and recognition of facial data. This paper investigates through experimental research how neural networks process and store facial expression data and associate these data with a range of psychological attributes produced by humans. Researchers utilized deep learning model VGG16, demonstrating that neural networks can learn and reproduce key features of facial data, thereby storing image memories. Moreover, the experimental results reveal the potential of deep learning models in understanding human emotions and cognitive processes and establish a manifold visualization interpretation of cognitive products or psychological attributes from a non-Euclidean space perspective, offering new insights into enhancing the explainability of AI. This study not only advances the application of AI technology in the field of psychology but also provides a new psychological theoretical understanding the information processing of the AI. The code is available in here: https://github.com/NKUShaw/Psychoinformatics.","sentences":["The complex information processing system of humans generates a lot of objective and subjective evaluations, making the exploration of human cognitive products of great cutting-edge theoretical value.","In recent years, deep learning technologies, which are inspired by biological brain mechanisms, have made significant strides in the application of psychological or cognitive scientific research, particularly in the memorization and recognition of facial data.","This paper investigates through experimental research how neural networks process and store facial expression data and associate these data with a range of psychological attributes produced by humans.","Researchers utilized deep learning model VGG16, demonstrating that neural networks can learn and reproduce key features of facial data, thereby storing image memories.","Moreover, the experimental results reveal the potential of deep learning models in understanding human emotions and cognitive processes and establish a manifold visualization interpretation of cognitive products or psychological attributes from a non-Euclidean space perspective, offering new insights into enhancing the explainability of AI.","This study not only advances the application of AI technology in the field of psychology but also provides a new psychological theoretical understanding the information processing of the AI.","The code is available in here: https://github.com/NKUShaw/Psychoinformatics."],"url":"http://arxiv.org/abs/2404.18352v1","category":"cs.CV"}
{"created":"2024-04-29 01:12:26","title":"L-DIT: A dApp for Live Detectability, Identifiability and Trackability for ASOs on the Behavioral Dynamics Blockchain","abstract":"As the number of Anthropogenic Space Objects (ASOs) grows, there is an urgent need to ensure space safety, security, and sustainability (S3) for long-term space use. Currently, no globally effective method can quantify the safety, security, and Sustainability of all ASOs in orbit. Existing methods such as the Space Sustainability Rating (SSR) rely on volunteering private information to provide sustainability ratings. However, the need for such sensitive data might prove to be a barrier to adoption for space entities. For effective comparison of ASOs, the rating mechanism should apply to all ASOs, even retroactively, so that the sustainability of a single ASO can be assessed holistically. Lastly, geopolitical boundaries and alignments play a crucial and limiting role in a volunteered rating system, limiting the space safety, security, and sustainability. This work presents a Live Detectability, Identifiability, and Trackability (L-DIT) score through a distributed app (dApp) built on top of the Behavioral Dynamics blockchain (BDB). The BDB chain is a space situational awareness (SSA) chain that provides verified and cross-checked ASO data from multiple sources. This unique combination of consensus-based information from BDB and permissionless access to data allows the DIT scoring method presented here to be applied to all ASOs. While the underlying BDB chain collects, filters, and validates SSA data from various open (and closed if available) sources, the L-DIT dApp consumes the data from the chain to provide L-DIT score that can contribute towards an operator's, manufacturer's, or owner's sustainability practices. Our dApp provides data for all ASOs, allowing their sustainability score to be compared against other ASOs, regardless of geopolitical alignments, providing business value to entities such as space insurance providers and enabling compliance validation and enforcement.","sentences":["As the number of Anthropogenic Space Objects (ASOs) grows, there is an urgent need to ensure space safety, security, and sustainability (S3) for long-term space use.","Currently, no globally effective method can quantify the safety, security, and Sustainability of all ASOs in orbit.","Existing methods such as the Space Sustainability Rating (SSR) rely on volunteering private information to provide sustainability ratings.","However, the need for such sensitive data might prove to be a barrier to adoption for space entities.","For effective comparison of ASOs, the rating mechanism should apply to all ASOs, even retroactively, so that the sustainability of a single ASO can be assessed holistically.","Lastly, geopolitical boundaries and alignments play a crucial and limiting role in a volunteered rating system, limiting the space safety, security, and sustainability.","This work presents a Live Detectability, Identifiability, and Trackability (L-DIT) score through a distributed app (dApp) built on top of the Behavioral Dynamics blockchain (BDB).","The BDB chain is a space situational awareness (SSA) chain that provides verified and cross-checked ASO data from multiple sources.","This unique combination of consensus-based information from BDB and permissionless access to data allows the DIT scoring method presented here to be applied to all ASOs.","While the underlying BDB chain collects, filters, and validates SSA data from various open (and closed if available) sources, the L-DIT dApp consumes the data from the chain to provide L-DIT score that can contribute towards an operator's, manufacturer's, or owner's sustainability practices.","Our dApp provides data for all ASOs, allowing their sustainability score to be compared against other ASOs, regardless of geopolitical alignments, providing business value to entities such as space insurance providers and enabling compliance validation and enforcement."],"url":"http://arxiv.org/abs/2404.18350v1","category":"cs.CR"}
{"created":"2024-04-28 22:11:24","title":"Multi-stage Attack Detection and Prediction Using Graph Neural Networks: An IoT Feasibility Study","abstract":"With the ever-increasing reliance on digital networks for various aspects of modern life, ensuring their security has become a critical challenge. Intrusion Detection Systems play a crucial role in ensuring network security, actively identifying and mitigating malicious behaviours. However, the relentless advancement of cyber-threats has rendered traditional/classical approaches insufficient in addressing the sophistication and complexity of attacks. This paper proposes a novel 3-stage intrusion detection system inspired by a simplified version of the Lockheed Martin cyber kill chain to detect advanced multi-step attacks. The proposed approach consists of three models, each responsible for detecting a group of attacks with common characteristics. The detection outcome of the first two stages is used to conduct a feasibility study on the possibility of predicting attacks in the third stage. Using the ToN IoT dataset, we achieved an average of 94% F1-Score among different stages, outperforming the benchmark approaches based on Random-forest model. Finally, we comment on the feasibility of this approach to be integrated in a real-world system and propose various possible future work.","sentences":["With the ever-increasing reliance on digital networks for various aspects of modern life, ensuring their security has become a critical challenge.","Intrusion Detection Systems play a crucial role in ensuring network security, actively identifying and mitigating malicious behaviours.","However, the relentless advancement of cyber-threats has rendered traditional/classical approaches insufficient in addressing the sophistication and complexity of attacks.","This paper proposes a novel 3-stage intrusion detection system inspired by a simplified version of the Lockheed Martin cyber kill chain to detect advanced multi-step attacks.","The proposed approach consists of three models, each responsible for detecting a group of attacks with common characteristics.","The detection outcome of the first two stages is used to conduct a feasibility study on the possibility of predicting attacks in the third stage.","Using the ToN IoT dataset, we achieved an average of 94% F1-Score among different stages, outperforming the benchmark approaches based on Random-forest model.","Finally, we comment on the feasibility of this approach to be integrated in a real-world system and propose various possible future work."],"url":"http://arxiv.org/abs/2404.18328v1","category":"cs.CR"}
{"created":"2024-04-28 21:47:34","title":"SAFE-RL: Saliency-Aware Counterfactual Explainer for Deep Reinforcement Learning Policies","abstract":"While Deep Reinforcement Learning (DRL) has emerged as a promising solution for intricate control tasks, the lack of explainability of the learned policies impedes its uptake in safety-critical applications, such as automated driving systems (ADS). Counterfactual (CF) explanations have recently gained prominence for their ability to interpret black-box Deep Learning (DL) models. CF examples are associated with minimal changes in the input, resulting in a complementary output by the DL model. Finding such alternations, particularly for high-dimensional visual inputs, poses significant challenges. Besides, the temporal dependency introduced by the reliance of the DRL agent action on a history of past state observations further complicates the generation of CF examples. To address these challenges, we propose using a saliency map to identify the most influential input pixels across the sequence of past observed states by the agent. Then, we feed this map to a deep generative model, enabling the generation of plausible CFs with constrained modifications centred on the salient regions. We evaluate the effectiveness of our framework in diverse domains, including ADS, Atari Pong, Pacman and space-invaders games, using traditional performance metrics such as validity, proximity and sparsity. Experimental results demonstrate that this framework generates more informative and plausible CFs than the state-of-the-art for a wide range of environments and DRL agents. In order to foster research in this area, we have made our datasets and codes publicly available at https://github.com/Amir-Samadi/SAFE-RL.","sentences":["While Deep Reinforcement Learning (DRL) has emerged as a promising solution for intricate control tasks, the lack of explainability of the learned policies impedes its uptake in safety-critical applications, such as automated driving systems (ADS).","Counterfactual (CF) explanations have recently gained prominence for their ability to interpret black-box Deep Learning (DL) models.","CF examples are associated with minimal changes in the input, resulting in a complementary output by the DL model.","Finding such alternations, particularly for high-dimensional visual inputs, poses significant challenges.","Besides, the temporal dependency introduced by the reliance of the DRL agent action on a history of past state observations further complicates the generation of CF examples.","To address these challenges, we propose using a saliency map to identify the most influential input pixels across the sequence of past observed states by the agent.","Then, we feed this map to a deep generative model, enabling the generation of plausible CFs with constrained modifications centred on the salient regions.","We evaluate the effectiveness of our framework in diverse domains, including ADS, Atari Pong, Pacman and space-invaders games, using traditional performance metrics such as validity, proximity and sparsity.","Experimental results demonstrate that this framework generates more informative and plausible CFs than the state-of-the-art for a wide range of environments and DRL agents.","In order to foster research in this area, we have made our datasets and codes publicly available at https://github.com/Amir-Samadi/SAFE-RL."],"url":"http://arxiv.org/abs/2404.18326v1","category":"cs.LG"}
{"created":"2024-04-28 21:42:01","title":"Canonical extensions via fitted sublocales","abstract":"We build on a recent result stating that the frame $\\mathsf{SE}(L)$ of strongly exact filters for a frame $L$ is anti-isomorphic to the coframe $\\mathsf{S}_o(L)$ of fitted sublocales. The collection $\\mathsf{E}(L)$ of exact filters of $L$ is known to be a sublocale of this frame. We consider several other subcollections of $\\mathsf{SE}(L)$: the collections $\\mathcal{J}(\\mathsf{CP}(L))$ and $\\mathcal{J}(\\mathsf{SO}(L))$ of intersections of completely prime and Scott-open filters, respectively, and the collection $\\mathsf{R}(L)$ of regular elements of the frame of filters. We show that all of these are sublocales of $\\mathsf{SE}(L)$, and as such they correspond to subcolocales of $\\mathsf{S}_o(L)$, which all turn out to have a concise description. By using the theory of polarities of Birkhoff, one can show that all of the structures mentioned above enjoy universal properties which are variations of that of the canonical extension. We also show how some of these subcollections can be described as polarities and give three new equivalent definitions of subfitness in terms of the lattice of filters.","sentences":["We build on a recent result stating that the frame $\\mathsf{SE}(L)$ of strongly exact filters for a frame $L$ is anti-isomorphic to the coframe $\\mathsf{S}_o(L)$ of fitted sublocales.","The collection $\\mathsf{E}(L)$ of exact filters of $L$ is known to be a sublocale of this frame.","We consider several other subcollections of $\\mathsf{SE}(L)$: the collections $\\mathcal{J}(\\mathsf{CP}(L))$ and $\\mathcal{J}(\\mathsf{SO}(L))$ of intersections of completely prime and Scott-open filters, respectively, and the collection $\\mathsf{R}(L)$ of regular elements of the frame of filters.","We show that all of these are sublocales of $\\mathsf{SE}(L)$, and as such they correspond to subcolocales of $\\mathsf{S}_o(L)$, which all turn out to have a concise description.","By using the theory of polarities of Birkhoff, one can show that all of the structures mentioned above enjoy universal properties which are variations of that of the canonical extension.","We also show how some of these subcollections can be described as polarities and give three new equivalent definitions of subfitness in terms of the lattice of filters."],"url":"http://arxiv.org/abs/2404.18325v1","category":"math.CT"}
{"created":"2024-04-28 20:55:19","title":"Design and Optimization of Reconfigurable Intelligent Surfaces Using the PEEC Method","abstract":"The design and optimization of Reconfigurable Intelligent Surfaces (RISs) are key challenges for future wireless communication systems. RISs are devices that can manipulate electromagnetic (EM) waves in a programmable way, thus enhancing the performance and efficiency of wireless links. To achieve this goal, it is essential to have reliable EM models that can capture the behavior of RISs in different scenarios. This work demonstrates that the Partial Elements Equivalent Circuit (PEEC) method is a powerful tool for EM analysis of RIS-aided wireless links. It might also be integrated with optimization algorithms in order to optimize wireless communication networks.","sentences":["The design and optimization of Reconfigurable Intelligent Surfaces (RISs) are key challenges for future wireless communication systems.","RISs are devices that can manipulate electromagnetic (EM) waves in a programmable way, thus enhancing the performance and efficiency of wireless links.","To achieve this goal, it is essential to have reliable EM models that can capture the behavior of RISs in different scenarios.","This work demonstrates that the Partial Elements Equivalent Circuit (PEEC) method is a powerful tool for EM analysis of RIS-aided wireless links.","It might also be integrated with optimization algorithms in order to optimize wireless communication networks."],"url":"http://arxiv.org/abs/2404.18315v1","category":"cs.IT"}
{"created":"2024-04-28 20:44:53","title":"Towards Real-time Learning in Large Language Models: A Critical Review","abstract":"Real-time learning concerns the ability of learning systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data may be insufficient or difficult to obtain. This review provides a comprehensive analysis of real-time learning in Large Language Models. It synthesizes the state-of-the-art real-time learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for real-time learning by describing specific achievements from these related topics and their critical factors. Finally, the paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of real-time learning and its implications for designing and developing LLM-based learning systems addressing real-world problems.","sentences":["Real-time learning concerns the ability of learning systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks.","It is a critical ability for intelligent, real-world systems, especially when data may be insufficient or difficult to obtain.","This review provides a comprehensive analysis of real-time learning in Large Language Models.","It synthesizes the state-of-the-art real-time learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning.","We demonstrate their utility for real-time learning by describing specific achievements from these related topics and their critical factors.","Finally, the paper highlights current problems and challenges for future research in the field.","By consolidating the latest relevant research developments, this review offers a comprehensive understanding of real-time learning and its implications for designing and developing LLM-based learning systems addressing real-world problems."],"url":"http://arxiv.org/abs/2404.18311v2","category":"cs.LG"}
{"created":"2024-04-28 20:40:37","title":"Multiport Network Modeling for Reconfigurable Intelligent Surfaces: Numerical Validation with a Full-Wave PEEC Simulator","abstract":"Reconfigurable Intelligent Surface (RIS) modeling and optimization are a crucial steps in developing the next generation of wireless communications. To this aim, the availability of accurate electromagnetic (EM) models is of paramount important for the design of RIS-assisted communication links. In this work, we validate a widely-used analytical multiport network for RISs by means of a well-established full-wave numerical method based on the Partial Elements Equivalent Circuit (PEEC) approach. Numerical results show good agreement between the two methods, thus demonstrating i) the considered multiport network model being effective and ii) the PEEC method being appropriate for EM modeling of RIS-assisted wireless links.","sentences":["Reconfigurable Intelligent Surface (RIS) modeling and optimization are a crucial steps in developing the next generation of wireless communications.","To this aim, the availability of accurate electromagnetic (EM) models is of paramount important for the design of RIS-assisted communication links.","In this work, we validate a widely-used analytical multiport network for RISs by means of a well-established full-wave numerical method based on the Partial Elements Equivalent Circuit (PEEC) approach.","Numerical results show good agreement between the two methods, thus demonstrating i) the considered multiport network model being effective and ii) the PEEC method being appropriate for EM modeling of RIS-assisted wireless links."],"url":"http://arxiv.org/abs/2404.18310v1","category":"cs.IT"}
{"created":"2024-04-28 20:21:03","title":"Retrieval-Oriented Knowledge for Click-Through Rate Prediction","abstract":"Click-through rate (CTR) prediction plays an important role in personalized recommendations. Recently, sample-level retrieval-based models (e.g., RIM) have achieved remarkable performance by retrieving and aggregating relevant samples. However, their inefficiency at the inference stage makes them impractical for industrial applications. To overcome this issue, this paper proposes a universal plug-and-play Retrieval-Oriented Knowledge (ROK) framework. Specifically, a knowledge base, consisting of a retrieval-oriented embedding layer and a knowledge encoder, is designed to preserve and imitate the retrieved & aggregated representations in a decomposition-reconstruction paradigm. Knowledge distillation and contrastive learning methods are utilized to optimize the knowledge base, and the learned retrieval-enhanced representations can be integrated with arbitrary CTR models in both instance-wise and feature-wise manners. Extensive experiments on three large-scale datasets show that ROK achieves competitive performance with the retrieval-based CTR models while reserving superior inference efficiency and model compatibility.","sentences":["Click-through rate (CTR) prediction plays an important role in personalized recommendations.","Recently, sample-level retrieval-based models (e.g., RIM) have achieved remarkable performance by retrieving and aggregating relevant samples.","However, their inefficiency at the inference stage makes them impractical for industrial applications.","To overcome this issue, this paper proposes a universal plug-and-play Retrieval-Oriented Knowledge (ROK) framework.","Specifically, a knowledge base, consisting of a retrieval-oriented embedding layer and a knowledge encoder, is designed to preserve and imitate the retrieved & aggregated representations in a decomposition-reconstruction paradigm.","Knowledge distillation and contrastive learning methods are utilized to optimize the knowledge base, and the learned retrieval-enhanced representations can be integrated with arbitrary CTR models in both instance-wise and feature-wise manners.","Extensive experiments on three large-scale datasets show that ROK achieves competitive performance with the retrieval-based CTR models while reserving superior inference efficiency and model compatibility."],"url":"http://arxiv.org/abs/2404.18304v1","category":"cs.IR"}
{"created":"2024-04-28 19:49:51","title":"Search for the Z boson decay to $\u03c4\u03c4\u03bc\u03bc$ in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"The first search for the Z boson decay to $\\tau\\tau\\mu\\mu$ at the CERN LHC is presented, based on data collected by the CMS experiment at the LHC in proton-proton collisions at a center-of-mass energy of 13 TeV and corresponding to an integrated luminosity of 138 fb$^{-1}$. The data are compatible with the predicted background. For the first time, an upper limit at the 95% confidence level of 6.9 times the standard model expectation is placed on the ratio of the Z $\\to$ $\\tau\\tau\\mu\\mu$ to Z $\\to$ 4$\\mu$ branching fractions. Limits are also placed on the six flavor-conserving four-lepton effective-field-theory operators involving two muons and two tau leptons, for the first time testing all such operators.","sentences":["The first search for the Z boson decay to $\\tau\\tau\\mu\\mu$ at the CERN LHC is presented, based on data collected by the CMS experiment at the LHC in proton-proton collisions at a center-of-mass energy of 13 TeV and corresponding to an integrated luminosity of 138 fb$^{-1}$.","The data are compatible with the predicted background.","For the first time, an upper limit at the 95% confidence level of 6.9 times the standard model expectation is placed on the ratio of the Z $\\to$ $\\tau\\tau\\mu\\mu$ to Z $\\to$ 4$\\mu$ branching fractions.","Limits are also placed on the six flavor-conserving four-lepton effective-field-theory operators involving two muons and two tau leptons, for the first time testing all such operators."],"url":"http://arxiv.org/abs/2404.18298v1","category":"hep-ex"}
{"created":"2024-04-28 19:44:56","title":"Using Deep Q-Learning to Dynamically Toggle between Push/Pull Actions in Computational Trust Mechanisms","abstract":"Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee's perspective. This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents' continuous entry and exit from the system. In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes. Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility. We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment. We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments.","sentences":["Recent work on decentralized computational trust models for open Multi Agent Systems has resulted in the development of CA, a biologically inspired model which focuses on the trustee's perspective.","This new model addresses a serious unresolved problem in existing trust and reputation models, namely the inability to handle constantly changing behaviors and agents' continuous entry and exit from the system.","In previous work, we compared CA to FIRE, a well-known trust and reputation model, and found that CA is superior when the trustor population changes, whereas FIRE is more resilient to the trustee population changes.","Thus, in this paper, we investigate how the trustors can detect the presence of several dynamic factors in their environment and then decide which trust model to employ in order to maximize utility.","We frame this problem as a machine learning problem in a partially observable environment, where the presence of several dynamic factors is not known to the trustor and we describe how an adaptable trustor can rely on a few measurable features so as to assess the current state of the environment and then use Deep Q Learning (DQN), in a single-agent Reinforcement Learning setting, to learn how to adapt to a changing environment.","We ran a series of simulation experiments to compare the performance of the adaptable trustor with the performance of trustors using only one model (FIRE or CA) and we show that an adaptable agent is indeed capable of learning when to use each model and, thus, perform consistently in dynamic environments."],"url":"http://arxiv.org/abs/2404.18296v1","category":"cs.AI"}
{"created":"2024-04-28 19:35:00","title":"Panoptic Segmentation and Labelling of Lumbar Spine Vertebrae using Modified Attention Unet","abstract":"Segmentation and labeling of vertebrae in MRI images of the spine are critical for the diagnosis of illnesses and abnormalities. These steps are indispensable as MRI technology provides detailed information about the tissue structure of the spine. Both supervised and unsupervised segmentation methods exist, yet acquiring sufficient data remains challenging for achieving high accuracy. In this study, we propose an enhancing approach based on modified attention U-Net architecture for panoptic segmentation of 3D sliced MRI data of the lumbar spine. Our method achieves an impressive accuracy of 99.5\\% by incorporating novel masking logic, thus significantly advancing the state-of-the-art in vertebral segmentation and labeling. This contributes to more precise and reliable diagnosis and treatment planning.","sentences":["Segmentation and labeling of vertebrae in MRI images of the spine are critical for the diagnosis of illnesses and abnormalities.","These steps are indispensable as MRI technology provides detailed information about the tissue structure of the spine.","Both supervised and unsupervised segmentation methods exist, yet acquiring sufficient data remains challenging for achieving high accuracy.","In this study, we propose an enhancing approach based on modified attention U-Net architecture for panoptic segmentation of 3D sliced MRI data of the lumbar spine.","Our method achieves an impressive accuracy of 99.5\\% by incorporating novel masking logic, thus significantly advancing the state-of-the-art in vertebral segmentation and labeling.","This contributes to more precise and reliable diagnosis and treatment planning."],"url":"http://arxiv.org/abs/2404.18291v1","category":"cs.CV"}
{"created":"2024-04-28 18:47:14","title":"Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)","abstract":"The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations.   By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer & Bender, 2023].","sentences":["The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems.","In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical.","This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs.","Our framework combines the Large Language Model Bias Index (LLMBI)","[Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND)","[Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations.   ","By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI","[Mehrabi et al., 2021].","This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer & Bender, 2023]."],"url":"http://arxiv.org/abs/2404.18276v1","category":"cs.CL"}
{"created":"2024-04-28 18:31:09","title":"Pragmatic Formal Verification of Sequential Error Detection and Correction Codes (ECCs) used in Safety-Critical Design","abstract":"Error Detection and Correction Codes (ECCs) are often used in digital designs to protect data integrity. Especially in safety-critical systems such as automotive electronics, ECCs are widely used and the verification of such complex logic becomes more critical considering the ISO 26262 safety standards. Exhaustive verification of ECC using formal methods has been a challenge given the high number of data bits to protect. As an example, for an ECC of 128 data bits with a possibility to detect up to four-bit errors, the combination of bit errors is given by 128C1 + 128C2 + 128C3 + 128C4 = 1.1 * 10^7. This vast analysis space often leads to bounded proof results. Moreover, the complexity and state-space increase further if the ECC has sequential encoding and decoding stages. To overcome such problems and sign-off the design with confidence within reasonable proof time, we present a pragmatic formal verification approach of complex ECC cores with several complexity reduction techniques and know-how that were learnt during the course of verification. We discuss using the linearity of the syndrome generator as a helper assertion, using the abstract model as glue logic to compare the RTL with the sequential version of the circuit, k-induction-based model checking and using mathematical relations captured as properties to simplify the verification in order to get an unbounded proof result within 24 hours of proof runtime.","sentences":["Error Detection and Correction Codes (ECCs) are often used in digital designs to protect data integrity.","Especially in safety-critical systems such as automotive electronics, ECCs are widely used and the verification of such complex logic becomes more critical considering the ISO 26262 safety standards.","Exhaustive verification of ECC using formal methods has been a challenge given the high number of data bits to protect.","As an example, for an ECC of 128 data bits with a possibility to detect up to four-bit errors, the combination of bit errors is given by 128C1 + 128C2 + 128C3 + 128C4 = 1.1 * 10^7.","This vast analysis space often leads to bounded proof results.","Moreover, the complexity and state-space increase further if the ECC has sequential encoding and decoding stages.","To overcome such problems and sign-off the design with confidence within reasonable proof time, we present a pragmatic formal verification approach of complex ECC cores with several complexity reduction techniques and know-how that were learnt during the course of verification.","We discuss using the linearity of the syndrome generator as a helper assertion, using the abstract model as glue logic to compare the RTL with the sequential version of the circuit, k-induction-based model checking and using mathematical relations captured as properties to simplify the verification in order to get an unbounded proof result within 24 hours of proof runtime."],"url":"http://arxiv.org/abs/2404.18270v1","category":"cs.AI"}
{"created":"2024-04-28 18:07:13","title":"Modeling Orthographic Variation Improves NLP Performance for Nigerian Pidgin","abstract":"Nigerian Pidgin is an English-derived contact language and is traditionally an oral language, spoken by approximately 100 million people. No orthographic standard has yet been adopted, and thus the few available Pidgin datasets that exist are characterised by noise in the form of orthographic variations. This contributes to under-performance of models in critical NLP tasks. The current work is the first to describe various types of orthographic variations commonly found in Nigerian Pidgin texts, and model this orthographic variation. The variations identified in the dataset form the basis of a phonetic-theoretic framework for word editing, which is used to generate orthographic variations to augment training data. We test the effect of this data augmentation on two critical NLP tasks: machine translation and sentiment analysis. The proposed variation generation framework augments the training data with new orthographic variants which are relevant for the test set but did not occur in the training set originally. Our results demonstrate the positive effect of augmenting the training data with a combination of real texts from other corpora as well as synthesized orthographic variation, resulting in performance improvements of 2.1 points in sentiment analysis and 1.4 BLEU points in translation to English.","sentences":["Nigerian Pidgin is an English-derived contact language and is traditionally an oral language, spoken by approximately 100 million people.","No orthographic standard has yet been adopted, and thus the few available Pidgin datasets that exist are characterised by noise in the form of orthographic variations.","This contributes to under-performance of models in critical NLP tasks.","The current work is the first to describe various types of orthographic variations commonly found in Nigerian Pidgin texts, and model this orthographic variation.","The variations identified in the dataset form the basis of a phonetic-theoretic framework for word editing, which is used to generate orthographic variations to augment training data.","We test the effect of this data augmentation on two critical NLP tasks: machine translation and sentiment analysis.","The proposed variation generation framework augments the training data with new orthographic variants which are relevant for the test set but did not occur in the training set originally.","Our results demonstrate the positive effect of augmenting the training data with a combination of real texts from other corpora as well as synthesized orthographic variation, resulting in performance improvements of 2.1 points in sentiment analysis and 1.4 BLEU points in translation to English."],"url":"http://arxiv.org/abs/2404.18264v1","category":"cs.CL"}
{"created":"2024-04-28 17:56:14","title":"Generating Situated Reflection Triggers about Alternative Solution Paths: A Case Study of Generative AI for Computer-Supported Collaborative Learning","abstract":"An advantage of Large Language Models (LLMs) is their contextualization capability - providing different responses based on student inputs like solution strategy or prior discussion, to potentially better engage students than standard feedback. We present a design and evaluation of a proof-of-concept LLM application to offer students dynamic and contextualized feedback. Specifically, we augment an Online Programming Exercise bot for a college-level Cloud Computing course with ChatGPT, which offers students contextualized reflection triggers during a collaborative query optimization task in database design. We demonstrate that LLMs can be used to generate highly situated reflection triggers that incorporate details of the collaborative discussion happening in context. We discuss in depth the exploration of the design space of the triggers and their correspondence with the learning objectives as well as the impact on student learning in a pilot study with 34 students.","sentences":["An advantage of Large Language Models (LLMs) is their contextualization capability - providing different responses based on student inputs like solution strategy or prior discussion, to potentially better engage students than standard feedback.","We present a design and evaluation of a proof-of-concept LLM application to offer students dynamic and contextualized feedback.","Specifically, we augment an Online Programming Exercise bot for a college-level Cloud Computing course with ChatGPT, which offers students contextualized reflection triggers during a collaborative query optimization task in database design.","We demonstrate that LLMs can be used to generate highly situated reflection triggers that incorporate details of the collaborative discussion happening in context.","We discuss in depth the exploration of the design space of the triggers and their correspondence with the learning objectives as well as the impact on student learning in a pilot study with 34 students."],"url":"http://arxiv.org/abs/2404.18262v1","category":"cs.AI"}
{"created":"2024-04-28 17:36:43","title":"PatentGPT: A Large Language Model for Intellectual Property","abstract":"In recent years, large language models have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields. However, the application of large language models in the Intellectual Property (IP) space is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field. In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain. Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models. By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP demain. What is impressive is that our model significantly outperformed GPT-4 on the 2019 China Patent Agent Qualification Examination by achieving a score of 65, reaching the level of human experts. Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain.","sentences":["In recent years, large language models have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields.","However, the application of large language models in the Intellectual Property (IP) space is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field.","In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain.","Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models.","By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP demain.","What is impressive is that our model significantly outperformed GPT-4 on the 2019 China Patent Agent Qualification Examination by achieving a score of 65, reaching the level of human experts.","Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain."],"url":"http://arxiv.org/abs/2404.18255v2","category":"cs.CL"}
{"created":"2024-04-28 16:50:12","title":"LEGENT: Open Platform for Embodied Agents","abstract":"Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in physical environments. Existing integrations often feature limited open sourcing, challenging collective progress in this field. We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs. LEGENT offers a dual approach: a rich, interactive 3D environment with communicable and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale. In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities.","sentences":["Despite advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), their integration into language-grounded, human-like embodied agents remains incomplete, hindering complex real-life task performance in physical environments.","Existing integrations often feature limited open sourcing, challenging collective progress in this field.","We introduce LEGENT, an open, scalable platform for developing embodied agents using LLMs and LMMs.","LEGENT offers a dual approach: a rich, interactive 3D environment with communicable and actionable agents, paired with a user-friendly interface, and a sophisticated data generation pipeline utilizing advanced algorithms to exploit supervision from simulated worlds at scale.","In our experiments, an embryonic vision-language-action model trained on LEGENT-generated data surpasses GPT-4V in embodied tasks, showcasing promising generalization capabilities."],"url":"http://arxiv.org/abs/2404.18243v1","category":"cs.CL"}
{"created":"2024-04-28 16:29:22","title":"Flood Data Analysis on SpaceNet 8 Using Apache Sedona","abstract":"With the escalating frequency of floods posing persistent threats to human life and property, satellite remote sensing has emerged as an indispensable tool for monitoring flood hazards. SpaceNet8 offers a unique opportunity to leverage cutting-edge artificial intelligence technologies to assess these hazards. A significant contribution of this research is its application of Apache Sedona, an advanced platform specifically designed for the efficient and distributed processing of large-scale geospatial data. This platform aims to enhance the efficiency of error analysis, a critical aspect of improving flood damage detection accuracy. Based on Apache Sedona, we introduce a novel approach that addresses the challenges associated with inaccuracies in flood damage detection. This approach involves the retrieval of cases from historical flood events, the adaptation of these cases to current scenarios, and the revision of the model based on clustering algorithms to refine its performance. Through the replication of both the SpaceNet8 baseline and its top-performing models, we embark on a comprehensive error analysis. This analysis reveals several main sources of inaccuracies. To address these issues, we employ data visual interpretation and histogram equalization techniques, resulting in significant improvements in model metrics. After these enhancements, our indicators show a notable improvement, with precision up by 5%, F1 score by 2.6%, and IoU by 4.5%. This work highlights the importance of advanced geospatial data processing tools, such as Apache Sedona. By improving the accuracy and efficiency of flood detection, this research contributes to safeguarding public safety and strengthening infrastructure resilience in flood-prone areas, making it a valuable addition to the field of remote sensing and disaster management.","sentences":["With the escalating frequency of floods posing persistent threats to human life and property, satellite remote sensing has emerged as an indispensable tool for monitoring flood hazards.","SpaceNet8 offers a unique opportunity to leverage cutting-edge artificial intelligence technologies to assess these hazards.","A significant contribution of this research is its application of Apache Sedona, an advanced platform specifically designed for the efficient and distributed processing of large-scale geospatial data.","This platform aims to enhance the efficiency of error analysis, a critical aspect of improving flood damage detection accuracy.","Based on Apache Sedona, we introduce a novel approach that addresses the challenges associated with inaccuracies in flood damage detection.","This approach involves the retrieval of cases from historical flood events, the adaptation of these cases to current scenarios, and the revision of the model based on clustering algorithms to refine its performance.","Through the replication of both the SpaceNet8 baseline and its top-performing models, we embark on a comprehensive error analysis.","This analysis reveals several main sources of inaccuracies.","To address these issues, we employ data visual interpretation and histogram equalization techniques, resulting in significant improvements in model metrics.","After these enhancements, our indicators show a notable improvement, with precision up by 5%, F1 score by 2.6%, and IoU by 4.5%.","This work highlights the importance of advanced geospatial data processing tools, such as Apache Sedona.","By improving the accuracy and efficiency of flood detection, this research contributes to safeguarding public safety and strengthening infrastructure resilience in flood-prone areas, making it a valuable addition to the field of remote sensing and disaster management."],"url":"http://arxiv.org/abs/2404.18235v1","category":"cs.CV"}
{"created":"2024-04-28 15:56:41","title":"From Persona to Personalization: A Survey on Role-Playing Language Agents","abstract":"Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.","sentences":["Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas.","By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance.","RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals.","Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones.","In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies.","We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services.","We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation.","Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs.","Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research.","Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony."],"url":"http://arxiv.org/abs/2404.18231v1","category":"cs.CL"}
{"created":"2024-04-28 15:28:31","title":"Event-scale Internal Tide Variability via X-band Marine Radar","abstract":"A combined radar remote sensing and in situ data set is used to track packets of nonlinear internal waves as they propagate and shoal across the inner shelf (40m - 9m). The dataset consists of high space-time resolution (5m, 2min) radar image time series collected over a 10km radial footprint, with over a dozen synchronous and co-located moorings measuring temperature, salinity, and velocity throughout the water column. The internal bores and higher-frequency internal waves that make up the internal tide are tracked in the radar image time series and, therefore, provide continuous cross-shore speed and angle estimates as the waves propagate across the inner shelf. We compare radar-estimated speeds to those estimated from moorings and confirm a cross-shore shoaling profile that deviates from linear theory. We additionally use combined remote sensing and in situ data to perform a detailed analysis on four consecutive internal tides. These analyses reveal intra-packet speed variability, tide-tide influences, reflected internal waves, and an instance of internal wave polarity reversal observed in the radar and moorings.","sentences":["A combined radar remote sensing and in situ data set is used to track packets of nonlinear internal waves as they propagate and shoal across the inner shelf (40m - 9m).","The dataset consists of high space-time resolution (5m, 2min) radar image time series collected over a 10km radial footprint, with over a dozen synchronous and co-located moorings measuring temperature, salinity, and velocity throughout the water column.","The internal bores and higher-frequency internal waves that make up the internal tide are tracked in the radar image time series and, therefore, provide continuous cross-shore speed and angle estimates as the waves propagate across the inner shelf.","We compare radar-estimated speeds to those estimated from moorings and confirm a cross-shore shoaling profile that deviates from linear theory.","We additionally use combined remote sensing and in situ data to perform a detailed analysis on four consecutive internal tides.","These analyses reveal intra-packet speed variability, tide-tide influences, reflected internal waves, and an instance of internal wave polarity reversal observed in the radar and moorings."],"url":"http://arxiv.org/abs/2404.18218v1","category":"physics.ao-ph"}
{"created":"2024-04-28 15:13:36","title":"Contrastive Learning Method for Sequential Recommendation based on Multi-Intention Disentanglement","abstract":"Sequential recommendation is one of the important branches of recommender system, aiming to achieve personalized recommended items for the future through the analysis and prediction of users' ordered historical interactive behaviors. However, along with the growth of the user volume and the increasingly rich behavioral information, how to understand and disentangle the user's interactive multi-intention effectively also poses challenges to behavior prediction and sequential recommendation. In light of these challenges, we propose a Contrastive Learning sequential recommendation method based on Multi-Intention Disentanglement (MIDCL). In our work, intentions are recognized as dynamic and diverse, and user behaviors are often driven by current multi-intentions, which means that the model needs to not only mine the most relevant implicit intention for each user, but also impair the influence from irrelevant intentions. Therefore, we choose Variational Auto-Encoder (VAE) to realize the disentanglement of users' multi-intentions, and propose two types of contrastive learning paradigms for finding the most relevant user's interactive intention, and maximizing the mutual information of positive sample pairs, respectively. Experimental results show that MIDCL not only has significant superiority over most existing baseline methods, but also brings a more interpretable case to the research about intention-based prediction and recommendation.","sentences":["Sequential recommendation is one of the important branches of recommender system, aiming to achieve personalized recommended items for the future through the analysis and prediction of users' ordered historical interactive behaviors.","However, along with the growth of the user volume and the increasingly rich behavioral information, how to understand and disentangle the user's interactive multi-intention effectively also poses challenges to behavior prediction and sequential recommendation.","In light of these challenges, we propose a Contrastive Learning sequential recommendation method based on Multi-Intention Disentanglement (MIDCL).","In our work, intentions are recognized as dynamic and diverse, and user behaviors are often driven by current multi-intentions, which means that the model needs to not only mine the most relevant implicit intention for each user, but also impair the influence from irrelevant intentions.","Therefore, we choose Variational Auto-Encoder (VAE) to realize the disentanglement of users' multi-intentions, and propose two types of contrastive learning paradigms for finding the most relevant user's interactive intention, and maximizing the mutual information of positive sample pairs, respectively.","Experimental results show that MIDCL not only has significant superiority over most existing baseline methods, but also brings a more interpretable case to the research about intention-based prediction and recommendation."],"url":"http://arxiv.org/abs/2404.18214v1","category":"cs.IR"}
{"created":"2024-04-28 15:12:56","title":"S$^2$Mamba: A Spatial-spectral State Space Model for Hyperspectral Image Classification","abstract":"Land cover analysis using hyperspectral images (HSI) remains an open problem due to their low spatial resolution and complex spectral information. Recent studies are primarily dedicated to designing Transformer-based architectures for spatial-spectral long-range dependencies modeling, which is computationally expensive with quadratic complexity. Selective structured state space model (Mamba), which is efficient for modeling long-range dependencies with linear complexity, has recently shown promising progress. However, its potential in hyperspectral image processing that requires handling numerous spectral bands has not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a spatial-spectral state space model for hyperspectral image classification, to excavate spatial-spectral contextual features, resulting in more efficient and accurate land cover analysis. In S$^2$Mamba, two selective structured state space models through different dimensions are designed for feature extraction, one for spatial, and the other for spectral, along with a spatial-spectral mixture gate for optimal fusion. More specifically, S$^2$Mamba first captures spatial contextual relations by interacting each pixel with its adjacent through a Patch Cross Scanning module and then explores semantic information from continuous spectral bands through a Bi-directional Spectral Scanning module. Considering the distinct expertise of the two attributes in homogenous and complicated texture scenes, we realize the Spatial-spectral Mixture Gate by a group of learnable matrices, allowing for the adaptive incorporation of representations learned across different dimensions. Extensive experiments conducted on HSI classification benchmarks demonstrate the superiority and prospect of S$^2$Mamba. The code will be available at: https://github.com/PURE-melo/S2Mamba.","sentences":["Land cover analysis using hyperspectral images (HSI) remains an open problem due to their low spatial resolution and complex spectral information.","Recent studies are primarily dedicated to designing Transformer-based architectures for spatial-spectral long-range dependencies modeling, which is computationally expensive with quadratic complexity.","Selective structured state space model (Mamba), which is efficient for modeling long-range dependencies with linear complexity, has recently shown promising progress.","However, its potential in hyperspectral image processing that requires handling numerous spectral bands has not yet been explored.","In this paper, we innovatively propose S$^2$Mamba, a spatial-spectral state space model for hyperspectral image classification, to excavate spatial-spectral contextual features, resulting in more efficient and accurate land cover analysis.","In S$^2$Mamba, two selective structured state space models through different dimensions are designed for feature extraction, one for spatial, and the other for spectral, along with a spatial-spectral mixture gate for optimal fusion.","More specifically, S$^2$Mamba first captures spatial contextual relations by interacting each pixel with its adjacent through a Patch Cross Scanning module and then explores semantic information from continuous spectral bands through a Bi-directional Spectral Scanning module.","Considering the distinct expertise of the two attributes in homogenous and complicated texture scenes, we realize the Spatial-spectral Mixture Gate by a group of learnable matrices, allowing for the adaptive incorporation of representations learned across different dimensions.","Extensive experiments conducted on HSI classification benchmarks demonstrate the superiority and prospect of S$^2$Mamba.","The code will be available at: https://github.com/PURE-melo/S2Mamba."],"url":"http://arxiv.org/abs/2404.18213v1","category":"cs.CV"}
{"created":"2024-04-28 15:07:53","title":"Paint by Inpaint: Learning to Add Image Objects by Removing Them First","abstract":"Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to the utilization of segmentation mask datasets alongside inpainting models that inpaint within these masks. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones; moreover, it maintains consistency between source and target by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. We show that the trained model surpasses existing ones both qualitatively and quantitatively, and release the large-scale dataset alongside the trained models for the community.","sentences":["Image editing has advanced significantly with the introduction of text-conditioned diffusion models.","Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge.","We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to the utilization of segmentation mask datasets alongside inpainting models that inpaint within these masks.","Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions.","Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images.","Unlike other editing datasets, ours features natural target images instead of synthetic ones; moreover, it maintains consistency between source and target by construction.","Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions.","We show that the trained model surpasses existing ones both qualitatively and quantitatively, and release the large-scale dataset alongside the trained models for the community."],"url":"http://arxiv.org/abs/2404.18212v1","category":"cs.CV"}
{"created":"2024-04-28 15:04:54","title":"4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on Relational DBs","abstract":"Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer. We conclude by presenting evaluations using 4DBInfer, the results of which highlight the importance of considering each such dimension in the design of RDB predictive models, as well as the limitations of more naive approaches such as simply joining adjacent tables. Our source code is released at https://github.com/awslabs/multi-table-benchmark .","sentences":["Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing.","This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes.","As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics.","To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs.","Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks.","From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer.","We conclude by presenting evaluations using 4DBInfer, the results of which highlight the importance of considering each such dimension in the design of RDB predictive models, as well as the limitations of more naive approaches such as simply joining adjacent tables.","Our source code is released at https://github.com/awslabs/multi-table-benchmark ."],"url":"http://arxiv.org/abs/2404.18209v1","category":"cs.LG"}
{"created":"2024-04-28 14:47:09","title":"LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM","abstract":"Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision. To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds. To compensate for the loss of perception in the 3D domain, structural features are extracted as well. These quality logits and structural features are then combined and regressed into quality scores. Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model understanding and assessment accuracy. We hope our contributions can inspire subsequent investigations into the fusion of LMMs with PCQA, fostering advancements in 3D visual quality analysis and beyond.","sentences":["Although large multi-modality models (LMMs) have seen extensive exploration and application in various quality assessment studies, their integration into Point Cloud Quality Assessment (PCQA) remains unexplored.","Given LMMs' exceptional performance and robustness in low-level vision and quality assessment tasks, this study aims to investigate the feasibility of imparting PCQA knowledge to LMMs through text supervision.","To achieve this, we transform quality labels into textual descriptions during the fine-tuning phase, enabling LMMs to derive quality rating logits from 2D projections of point clouds.","To compensate for the loss of perception in the 3D domain, structural features are extracted as well.","These quality logits and structural features are then combined and regressed into quality scores.","Our experimental results affirm the effectiveness of our approach, showcasing a novel integration of LMMs into PCQA that enhances model understanding and assessment accuracy.","We hope our contributions can inspire subsequent investigations into the fusion of LMMs with PCQA, fostering advancements in 3D visual quality analysis and beyond."],"url":"http://arxiv.org/abs/2404.18203v1","category":"cs.CV"}
{"created":"2024-04-28 14:42:02","title":"WorldGPT: Empowering LLM as Multimodal World Model","abstract":"World models are progressively being employed across diverse fields, extending from basic environment simulation to complex scenario construction. However, existing models are mainly trained on domain-specific states and actions, and confined to single-modality state representations. In this paper, We introduce WorldGPT, a generalist world model built upon Multimodal Large Language Model (MLLM). WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains. To further enhance WorldGPT's capability in specialized scenarios and long-term tasks, we have integrated it with a novel cognitive architecture that combines memory offloading, knowledge retrieval, and context reflection. As for evaluation, we build WorldNet, a multimodal state transition prediction benchmark encompassing varied real-life scenarios. Conducting evaluations on WorldNet directly demonstrates WorldGPT's capability to accurately model state transition patterns, affirming its effectiveness in understanding and predicting the dynamics of complex scenarios. We further explore WorldGPT's emerging potential in serving as a world simulator, helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine-tuning purposes. The project is available on \\url{https://github.com/DCDmllm/WorldGPT}.","sentences":["World models are progressively being employed across diverse fields, extending from basic environment simulation to complex scenario construction.","However, existing models are mainly trained on domain-specific states and actions, and confined to single-modality state representations.","In this paper, We introduce WorldGPT, a generalist world model built upon Multimodal Large Language Model (MLLM).","WorldGPT acquires an understanding of world dynamics through analyzing millions of videos across various domains.","To further enhance WorldGPT's capability in specialized scenarios and long-term tasks, we have integrated it with a novel cognitive architecture that combines memory offloading, knowledge retrieval, and context reflection.","As for evaluation, we build WorldNet, a multimodal state transition prediction benchmark encompassing varied real-life scenarios.","Conducting evaluations on WorldNet directly demonstrates WorldGPT's capability to accurately model state transition patterns, affirming its effectiveness in understanding and predicting the dynamics of complex scenarios.","We further explore WorldGPT's emerging potential in serving as a world simulator, helping multimodal agents generalize to unfamiliar domains through efficiently synthesising multimodal instruction instances which are proved to be as reliable as authentic data for fine-tuning purposes.","The project is available on \\url{https://github.com/DCDmllm/WorldGPT}."],"url":"http://arxiv.org/abs/2404.18202v1","category":"cs.AI"}
{"created":"2024-04-28 14:34:28","title":"Permutation-equivariant quantum convolutional neural networks","abstract":"The Symmetric group $S_{n}$ manifests itself in large classes of quantum systems as the invariance of certain characteristics of a quantum state with respect to permuting the qubits. The subgroups of $S_{n}$ arise, among many other contexts, to describe label symmetry of classical images with respect to spatial transformations, e.g. reflection or rotation. Equipped with the formalism of geometric quantum machine learning, in this work we propose the architectures of equivariant quantum convolutional neural networks (EQCNNs) adherent to $S_{n}$ and its subgroups. We demonstrate that a careful choice of pixel-to-qubit embedding order can facilitate easy construction of EQCNNs for small subgroups of $S_{n}$. Our novel EQCNN architecture corresponding to the full permutation group $S_{n}$ is built by applying all possible QCNNs with equal probability, which can also be conceptualized as a dropout strategy in quantum neural networks. For subgroups of $S_{n}$, our numerical results using MNIST datasets show better classification accuracy than non-equivariant QCNNs. The $S_{n}$-equivariant QCNN architecture shows significantly improved training and test performance than non-equivariant QCNN for classification of connected and non-connected graphs. When trained with sufficiently large number of data, the $S_{n}$-equivariant QCNN shows better average performance compared to $S_{n}$-equivariant QNN . These results contribute towards building powerful quantum machine learning architectures in permutation-symmetric systems.","sentences":["The Symmetric group $S_{n}$ manifests itself in large classes of quantum systems as the invariance of certain characteristics of a quantum state with respect to permuting the qubits.","The subgroups of $S_{n}$ arise, among many other contexts, to describe label symmetry of classical images with respect to spatial transformations, e.g. reflection or rotation.","Equipped with the formalism of geometric quantum machine learning, in this work we propose the architectures of equivariant quantum convolutional neural networks (EQCNNs) adherent to $S_{n}$ and its subgroups.","We demonstrate that a careful choice of pixel-to-qubit embedding order can facilitate easy construction of EQCNNs for small subgroups of $S_{n}$. Our novel EQCNN architecture corresponding to the full permutation group $S_{n}$ is built by applying all possible QCNNs with equal probability, which can also be conceptualized as a dropout strategy in quantum neural networks.","For subgroups of $S_{n}$, our numerical results using MNIST datasets show better classification accuracy than non-equivariant QCNNs.","The $S_{n}$-equivariant QCNN architecture shows significantly improved training and test performance than non-equivariant QCNN for classification of connected and non-connected graphs.","When trained with sufficiently large number of data, the $S_{n}$-equivariant QCNN shows better average performance compared to $S_{n}$-equivariant QNN .","These results contribute towards building powerful quantum machine learning architectures in permutation-symmetric systems."],"url":"http://arxiv.org/abs/2404.18198v1","category":"quant-ph"}
{"created":"2024-04-28 14:26:27","title":"A General Causal Inference Framework for Cross-Sectional Observational Data","abstract":"Causal inference methods for observational data are highly regarded due to their wide applicability. While there are already numerous methods available for de-confounding bias, these methods generally assume that covariates consist solely of confounders or make naive assumptions about the covariates. Such assumptions face challenges in both theory and practice, particularly when dealing with high-dimensional covariates. Relaxing these naive assumptions and identifying the confounding covariates that truly require correction can effectively enhance the practical significance of these methods. Therefore, this paper proposes a General Causal Inference (GCI) framework specifically designed for cross-sectional observational data, which precisely identifies the key confounding covariates and provides corresponding identification algorithm. Specifically, based on progressive derivations of the Markov property on Directed Acyclic Graph, we conclude that the key confounding covariates are equivalent to the common root ancestors of the treatment and the outcome variable. Building upon this conclusion, the GCI framework is composed of a novel Ancestor Set Identification (ASI) algorithm and de-confounding inference methods. Firstly, the ASI algorithm is theoretically supported by the conditional independence properties and causal asymmetry between variables, enabling the identification of key confounding covariates. Subsequently, the identified confounding covariates are used in the de-confounding inference methods to obtain unbiased causal effect estimation, which can support informed decision-making. Extensive experiments on synthetic datasets demonstrate that the GCI framework can effectively identify the critical confounding covariates and significantly improve the precision, stability, and interpretability of causal inference in observational studies.","sentences":["Causal inference methods for observational data are highly regarded due to their wide applicability.","While there are already numerous methods available for de-confounding bias, these methods generally assume that covariates consist solely of confounders or make naive assumptions about the covariates.","Such assumptions face challenges in both theory and practice, particularly when dealing with high-dimensional covariates.","Relaxing these naive assumptions and identifying the confounding covariates that truly require correction can effectively enhance the practical significance of these methods.","Therefore, this paper proposes a General Causal Inference (GCI) framework specifically designed for cross-sectional observational data, which precisely identifies the key confounding covariates and provides corresponding identification algorithm.","Specifically, based on progressive derivations of the Markov property on Directed Acyclic Graph, we conclude that the key confounding covariates are equivalent to the common root ancestors of the treatment and the outcome variable.","Building upon this conclusion, the GCI framework is composed of a novel Ancestor Set Identification (ASI) algorithm and de-confounding inference methods.","Firstly, the ASI algorithm is theoretically supported by the conditional independence properties and causal asymmetry between variables, enabling the identification of key confounding covariates.","Subsequently, the identified confounding covariates are used in the de-confounding inference methods to obtain unbiased causal effect estimation, which can support informed decision-making.","Extensive experiments on synthetic datasets demonstrate that the GCI framework can effectively identify the critical confounding covariates and significantly improve the precision, stability, and interpretability of causal inference in observational studies."],"url":"http://arxiv.org/abs/2404.18197v1","category":"stat.ME"}
{"created":"2024-04-28 14:05:23","title":"Exploring the Robustness of In-Context Learning with Noisy Labels","abstract":"Recently, the mysterious In-Context Learning (ICL) ability exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest. However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored. In this paper, inspired by prior research that studies ICL ability using simple function classes, we take a closer look at this problem by investigating the robustness of Transformers against noisy labels. Specifically, we first conduct a thorough evaluation and analysis of the robustness of Transformers against noisy labels during in-context learning and show that they exhibit notable resilience against diverse types of noise in demonstration labels. Furthermore, we delve deeper into this problem by exploring whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference, and find that such noise can indeed improve the robustness of ICL. Overall, our fruitful analysis and findings provide a comprehensive understanding of the resilience of Transformer models against label noises during ICL and provide valuable insights into the research on Transformers in natural language processing. Our code is available at https://github.com/InezYu0928/in-context-learning.","sentences":["Recently, the mysterious In-Context Learning (ICL) ability exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest.","However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored.","In this paper, inspired by prior research that studies ICL ability using simple function classes, we take a closer look at this problem by investigating the robustness of Transformers against noisy labels.","Specifically, we first conduct a thorough evaluation and analysis of the robustness of Transformers against noisy labels during in-context learning and show that they exhibit notable resilience against diverse types of noise in demonstration labels.","Furthermore, we delve deeper into this problem by exploring whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference, and find that such noise can indeed improve the robustness of ICL.","Overall, our fruitful analysis and findings provide a comprehensive understanding of the resilience of Transformer models against label noises during ICL and provide valuable insights into the research on Transformers in natural language processing.","Our code is available at https://github.com/InezYu0928/in-context-learning."],"url":"http://arxiv.org/abs/2404.18191v1","category":"cs.CL"}
{"created":"2024-04-28 13:39:33","title":"Ranked List Truncation for Large Language Model-based Re-Ranking","abstract":"We study ranked list truncation (RLT) from a novel \"retrieve-then-re-rank\" perspective, where we optimize re-ranking by truncating the retrieved list (i.e., trim re-ranking candidates). RLT is crucial for re-ranking as it can improve re-ranking efficiency by sending variable-length candidate lists to a re-ranker on a per-query basis. It also has the potential to improve re-ranking effectiveness. Despite its importance, there is limited research into applying RLT methods to this new perspective. To address this research gap, we reproduce existing RLT methods in the context of re-ranking, especially newly emerged large language model (LLM)-based re-ranking. In particular, we examine to what extent established findings on RLT for retrieval are generalizable to the \"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT methods in the context of LLM-based re-ranking with lexical first-stage retrieval, (ii) investigating the impact of different types of first-stage retrievers on RLT methods, and (iii) investigating the impact of different types of re-rankers on RLT methods. We perform experiments on the TREC 2019 and 2020 deep learning tracks, investigating 8 RLT methods for pipelines involving 3 retrievers and 2 re-rankers. We reach new insights into RLT methods in the context of re-ranking.","sentences":["We study ranked list truncation (RLT) from a novel \"retrieve-then-re-rank\" perspective, where we optimize re-ranking by truncating the retrieved list (i.e., trim re-ranking candidates).","RLT is crucial for re-ranking as it can improve re-ranking efficiency by sending variable-length candidate lists to a re-ranker on a per-query basis.","It also has the potential to improve re-ranking effectiveness.","Despite its importance, there is limited research into applying RLT methods to this new perspective.","To address this research gap, we reproduce existing RLT methods in the context of re-ranking, especially newly emerged large language model (LLM)-based re-ranking.","In particular, we examine to what extent established findings on RLT for retrieval are generalizable to the \"retrieve-then-re-rank\" setup from three perspectives: (i) assessing RLT methods in the context of LLM-based re-ranking with lexical first-stage retrieval, (ii) investigating the impact of different types of first-stage retrievers on RLT methods, and (iii) investigating the impact of different types of re-rankers on RLT methods.","We perform experiments on the TREC 2019 and 2020 deep learning tracks, investigating 8 RLT methods for pipelines involving 3 retrievers and 2 re-rankers.","We reach new insights into RLT methods in the context of re-ranking."],"url":"http://arxiv.org/abs/2404.18185v1","category":"cs.IR"}
{"created":"2024-04-28 13:37:45","title":"Application and practice of AI technology in quantitative investment","abstract":"With the continuous development of artificial intelligence technology, using machine learning technology to predict market trends may no longer be out of reach. In recent years, artificial intelligence has become a research hotspot in the academic circle,and it has been widely used in image recognition, natural language processing and other fields, and also has a huge impact on the field of quantitative investment. As an investment method to obtain stable returns through data analysis, model construction and program trading, quantitative investment is deeply loved by financial institutions and investors. At the same time, as an important application field of quantitative investment, the quantitative investment strategy based on artificial intelligence technology arises at the historic moment.How to apply artificial intelligence to quantitative investment, so as to better achieve profit and risk control, has also become the focus and difficulty of the research. From a global perspective, inflation in the US and the Federal Reserve are the concerns of investors, which to some extent affects the direction of global assets, including the Chinese stock market. This paper studies the application of AI technology, quantitative investment, and AI technology in quantitative investment, aiming to provide investors with auxiliary decision-making, reduce the difficulty of investment analysis, and help them to obtain higher returns.","sentences":["With the continuous development of artificial intelligence technology, using machine learning technology to predict market trends may no longer be out of reach.","In recent years, artificial intelligence has become a research hotspot in the academic circle,and it has been widely used in image recognition, natural language processing and other fields, and also has a huge impact on the field of quantitative investment.","As an investment method to obtain stable returns through data analysis, model construction and program trading, quantitative investment is deeply loved by financial institutions and investors.","At the same time, as an important application field of quantitative investment, the quantitative investment strategy based on artificial intelligence technology arises at the historic moment.","How to apply artificial intelligence to quantitative investment, so as to better achieve profit and risk control, has also become the focus and difficulty of the research.","From a global perspective, inflation in the US and the Federal Reserve are the concerns of investors, which to some extent affects the direction of global assets, including the Chinese stock market.","This paper studies the application of AI technology, quantitative investment, and AI technology in quantitative investment, aiming to provide investors with auxiliary decision-making, reduce the difficulty of investment analysis, and help them to obtain higher returns."],"url":"http://arxiv.org/abs/2404.18184v1","category":"q-fin.PM"}
{"created":"2024-04-28 13:29:35","title":"Innovative Application of Artificial Intelligence Technology in Bank Credit Risk Management","abstract":"With the rapid growth of technology, especially the widespread application of artificial intelligence (AI) technology, the risk management level of commercial banks is constantly reaching new heights. In the current wave of digitalization, AI has become a key driving force for the strategic transformation of financial institutions, especially the banking industry. For commercial banks, the stability and safety of asset quality are crucial, which directly relates to the long-term stable growth of the bank. Among them, credit risk management is particularly core because it involves the flow of a large amount of funds and the accuracy of credit decisions. Therefore, establishing a scientific and effective credit risk decision-making mechanism is of great strategic significance for commercial banks. In this context, the innovative application of AI technology has brought revolutionary changes to bank credit risk management. Through deep learning and big data analysis, AI can accurately evaluate the credit status of borrowers, timely identify potential risks, and provide banks with more accurate and comprehensive credit decision support. At the same time, AI can also achieve realtime monitoring and early warning, helping banks intervene before risks occur and reduce losses.","sentences":["With the rapid growth of technology, especially the widespread application of artificial intelligence (AI) technology, the risk management level of commercial banks is constantly reaching new heights.","In the current wave of digitalization, AI has become a key driving force for the strategic transformation of financial institutions, especially the banking industry.","For commercial banks, the stability and safety of asset quality are crucial, which directly relates to the long-term stable growth of the bank.","Among them, credit risk management is particularly core because it involves the flow of a large amount of funds and the accuracy of credit decisions.","Therefore, establishing a scientific and effective credit risk decision-making mechanism is of great strategic significance for commercial banks.","In this context, the innovative application of AI technology has brought revolutionary changes to bank credit risk management.","Through deep learning and big data analysis, AI can accurately evaluate the credit status of borrowers, timely identify potential risks, and provide banks with more accurate and comprehensive credit decision support.","At the same time, AI can also achieve realtime monitoring and early warning, helping banks intervene before risks occur and reduce losses."],"url":"http://arxiv.org/abs/2404.18183v1","category":"q-fin.RM"}
{"created":"2024-04-28 13:18:47","title":"Assessing Image Quality Using a Simple Generative Representation","abstract":"Perceptual image quality assessment (IQA) is the task of predicting the visual quality of an image as perceived by a human observer. Current state-of-the-art techniques are based on deep representations trained in discriminative manner. Such representations may ignore visually important features, if they are not predictive of class labels. Recent generative models successfully learn low-dimensional representations using auto-encoding and have been argued to preserve better visual features. Here we leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference. We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time.","sentences":["Perceptual image quality assessment (IQA) is the task of predicting the visual quality of an image as perceived by a human observer.","Current state-of-the-art techniques are based on deep representations trained in discriminative manner.","Such representations may ignore visually important features, if they are not predictive of class labels.","Recent generative models successfully learn low-dimensional representations using auto-encoding and have been argued to preserve better visual features.","Here we leverage existing auto-encoders and propose VAE-QA, a simple and efficient method for predicting image quality in the presence of a full-reference.","We evaluate our approach on four standard benchmarks and find that it significantly improves generalization across datasets, has fewer trainable parameters, a smaller memory footprint and faster run time."],"url":"http://arxiv.org/abs/2404.18178v1","category":"eess.IV"}
{"created":"2024-04-28 13:12:49","title":"Mamba-FETrack: Frame-Event Tracking via State Space Model","abstract":"RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.","sentences":["RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream).","Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets.","However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism.","This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking.","Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams.","Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network.","The fused features will be fed into the tracking head for target object localization.","Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker.","Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9.","The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively.","We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking.","The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}."],"url":"http://arxiv.org/abs/2404.18174v1","category":"cs.CV"}
{"created":"2024-04-28 12:30:53","title":"fMRI Exploration of Visual Quality Assessment","abstract":"Despite significant strides in visual quality assessment, the neural mechanisms underlying visual quality perception remain insufficiently explored. This study employed fMRI to examine brain activity during image quality assessment and identify differences in human processing of images with varying quality. Fourteen healthy participants underwent tasks assessing both image quality and content classification while undergoing functional MRI scans. The collected behavioral data was statistically analyzed, and univariate and functional connectivity analyses were conducted on the imaging data. The findings revealed that quality assessment is a more complex task than content classification, involving enhanced activation in high-level cognitive brain regions for fine-grained visual analysis. Moreover, the research showed the brain's adaptability to different visual inputs, adopting different strategies depending on the input's quality. In response to high-quality images, the brain primarily uses specialized visual areas for precise analysis, whereas with low-quality images, it recruits additional resources including higher-order visual cortices and related cognitive and attentional networks to decode and recognize complex, ambiguous signals effectively. This study pioneers the intersection of neuroscience and image quality research, providing empirical evidence through fMRI linking image quality to neural processing. It contributes novel insights into the human visual system's response to diverse image qualities, thereby paving the way for advancements in objective image quality assessment algorithms.","sentences":["Despite significant strides in visual quality assessment, the neural mechanisms underlying visual quality perception remain insufficiently explored.","This study employed fMRI to examine brain activity during image quality assessment and identify differences in human processing of images with varying quality.","Fourteen healthy participants underwent tasks assessing both image quality and content classification while undergoing functional MRI scans.","The collected behavioral data was statistically analyzed, and univariate and functional connectivity analyses were conducted on the imaging data.","The findings revealed that quality assessment is a more complex task than content classification, involving enhanced activation in high-level cognitive brain regions for fine-grained visual analysis.","Moreover, the research showed the brain's adaptability to different visual inputs, adopting different strategies depending on the input's quality.","In response to high-quality images, the brain primarily uses specialized visual areas for precise analysis, whereas with low-quality images, it recruits additional resources including higher-order visual cortices and related cognitive and attentional networks to decode and recognize complex, ambiguous signals effectively.","This study pioneers the intersection of neuroscience and image quality research, providing empirical evidence through fMRI linking image quality to neural processing.","It contributes novel insights into the human visual system's response to diverse image qualities, thereby paving the way for advancements in objective image quality assessment algorithms."],"url":"http://arxiv.org/abs/2404.18162v1","category":"cs.MM"}
{"created":"2024-04-28 12:25:09","title":"IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning","abstract":"Continual learning (CL) remains one of the long-standing challenges for deep neural networks due to catastrophic forgetting of previously acquired knowledge. Although rehearsal-based approaches have been fairly successful in mitigating catastrophic forgetting, they suffer from overfitting on buffered samples and prior information loss, hindering generalization under low-buffer regimes. Inspired by how humans learn using strong inductive biases, we propose IMEX-Reg to improve the generalization performance of experience rehearsal in CL under low buffer regimes. Specifically, we employ a two-pronged implicit-explicit regularization approach using contrastive representation learning (CRL) and consistency regularization. To further leverage the global relationship between representations learned using CRL, we propose a regularization strategy to guide the classifier toward the activation correlations in the unit hypersphere of the CRL. Our results show that IMEX-Reg significantly improves generalization performance and outperforms rehearsal-based approaches in several CL scenarios. It is also robust to natural and adversarial corruptions with less task-recency bias. Additionally, we provide theoretical insights to support our design decisions further.","sentences":["Continual learning (CL) remains one of the long-standing challenges for deep neural networks due to catastrophic forgetting of previously acquired knowledge.","Although rehearsal-based approaches have been fairly successful in mitigating catastrophic forgetting, they suffer from overfitting on buffered samples and prior information loss, hindering generalization under low-buffer regimes.","Inspired by how humans learn using strong inductive biases, we propose IMEX-Reg to improve the generalization performance of experience rehearsal in CL under low buffer regimes.","Specifically, we employ a two-pronged implicit-explicit regularization approach using contrastive representation learning (CRL) and consistency regularization.","To further leverage the global relationship between representations learned using CRL, we propose a regularization strategy to guide the classifier toward the activation correlations in the unit hypersphere of the CRL.","Our results show that IMEX-Reg significantly improves generalization performance and outperforms rehearsal-based approaches in several CL scenarios.","It is also robust to natural and adversarial corruptions with less task-recency bias.","Additionally, we provide theoretical insights to support our design decisions further."],"url":"http://arxiv.org/abs/2404.18161v1","category":"cs.LG"}
{"created":"2024-04-28 12:23:01","title":"Evaluating ROCKET and Catch22 features for calf behaviour classification from accelerometer data using Machine Learning models","abstract":"Monitoring calf behaviour continuously would be beneficial to identify routine practices (e.g., weaning, dehorning, etc.) that impact calf welfare in dairy farms. In that regard, accelerometer data collected from neck collars can be used along with Machine Learning models to classify calf behaviour automatically. Hand-crafted features are commonly used in Machine Learning models, while ROCKET and Catch22 features are specifically designed for time-series classification problems in related fields. This study aims to compare the performance of ROCKET and Catch22 features to Hand-Crafted features. 30 Irish Holstein Friesian and Jersey pre-weaned calves were monitored using accelerometer sensors allowing for 27.4 hours of annotated behaviors. Additional time-series were computed from the raw X, Y and Z-axis and split into 3-second time windows. ROCKET, Catch22 and Hand-Crafted features were calculated for each time window, and the dataset was then split into the train, validation and test sets. Each set of features was used to train three Machine Learning models (Random Forest, eXtreme Gradient Boosting, and RidgeClassifierCV) to classify six behaviours indicative of pre-weaned calf welfare (drinking milk, grooming, lying, running, walking and other). Models were tuned with the validation set, and the performance of each feature-model combination was evaluated with the test set. The best performance across the three models was obtained with ROCKET [average balanced accuracy +/- standard deviation] (0.70 +/- 0.07), followed by Catch22 (0.69 +/- 0.05), surpassing Hand-Crafted (0.65 +/- 0.034). The best balanced accuracy (0.77) was obtained with ROCKET and RidgeClassifierCV, followed by Catch22 and Random Forest (0.73). Thus, tailoring these approaches for specific behaviours and contexts will be crucial in advancing precision livestock farming and enhancing animal welfare on a larger scale.","sentences":["Monitoring calf behaviour continuously would be beneficial to identify routine practices (e.g., weaning, dehorning, etc.) that impact calf welfare in dairy farms.","In that regard, accelerometer data collected from neck collars can be used along with Machine Learning models to classify calf behaviour automatically.","Hand-crafted features are commonly used in Machine Learning models, while ROCKET and Catch22 features are specifically designed for time-series classification problems in related fields.","This study aims to compare the performance of ROCKET and Catch22 features to Hand-Crafted features.","30 Irish Holstein Friesian and Jersey pre-weaned calves were monitored using accelerometer sensors allowing for 27.4 hours of annotated behaviors.","Additional time-series were computed from the raw X, Y and Z-axis and split into 3-second time windows.","ROCKET, Catch22 and Hand-Crafted features were calculated for each time window, and the dataset was then split into the train, validation and test sets.","Each set of features was used to train three Machine Learning models (Random Forest, eXtreme Gradient Boosting, and RidgeClassifierCV) to classify six behaviours indicative of pre-weaned calf welfare (drinking milk, grooming, lying, running, walking and other).","Models were tuned with the validation set, and the performance of each feature-model combination was evaluated with the test set.","The best performance across the three models was obtained with ROCKET [average balanced accuracy +/- standard deviation] (0.70 +/- 0.07), followed by Catch22 (0.69 +/- 0.05), surpassing Hand-Crafted (0.65 +/- 0.034).","The best balanced accuracy (0.77) was obtained with ROCKET and RidgeClassifierCV, followed by Catch22 and Random Forest (0.73).","Thus, tailoring these approaches for specific behaviours and contexts will be crucial in advancing precision livestock farming and enhancing animal welfare on a larger scale."],"url":"http://arxiv.org/abs/2404.18159v1","category":"cs.LG"}
{"created":"2024-04-28 11:55:50","title":"RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar Object Detection With Simulation","abstract":"Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels. To address this challenge, we present RadSimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection. Remarkably, our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets. RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar suppliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications.","sentences":["Object detection in radar imagery with neural networks shows great potential for improving autonomous driving.","However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels.","To address this challenge, we present RadSimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection.","Remarkably, our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets.","RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar suppliers, and has faster run-time.","This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications."],"url":"http://arxiv.org/abs/2404.18150v1","category":"cs.CV"}
{"created":"2024-04-28 11:48:13","title":"Compressed Deepfake Video Detection Based on 3D Spatiotemporal Trajectories","abstract":"The misuse of deepfake technology by malicious actors poses a potential threat to nations, societies, and individuals. However, existing methods for detecting deepfakes primarily focus on uncompressed videos, such as noise characteristics, local textures, or frequency statistics. When applied to compressed videos, these methods experience a decrease in detection performance and are less suitable for real-world scenarios. In this paper, we propose a deepfake video detection method based on 3D spatiotemporal trajectories. Specifically, we utilize a robust 3D model to construct spatiotemporal motion features, integrating feature details from both 2D and 3D frames to mitigate the influence of large head rotation angles or insufficient lighting within frames. Furthermore, we separate facial expressions from head movements and design a sequential analysis method based on phase space motion trajectories to explore the feature differences between genuine and fake faces in deepfake videos. We conduct extensive experiments to validate the performance of our proposed method on several compressed deepfake benchmarks. The robustness of the well-designed features is verified by calculating the consistent distribution of facial landmarks before and after video compression.Our method yields satisfactory results and showcases its potential for practical applications.","sentences":["The misuse of deepfake technology by malicious actors poses a potential threat to nations, societies, and individuals.","However, existing methods for detecting deepfakes primarily focus on uncompressed videos, such as noise characteristics, local textures, or frequency statistics.","When applied to compressed videos, these methods experience a decrease in detection performance and are less suitable for real-world scenarios.","In this paper, we propose a deepfake video detection method based on 3D spatiotemporal trajectories.","Specifically, we utilize a robust 3D model to construct spatiotemporal motion features, integrating feature details from both 2D and 3D frames to mitigate the influence of large head rotation angles or insufficient lighting within frames.","Furthermore, we separate facial expressions from head movements and design a sequential analysis method based on phase space motion trajectories to explore the feature differences between genuine and fake faces in deepfake videos.","We conduct extensive experiments to validate the performance of our proposed method on several compressed deepfake benchmarks.","The robustness of the well-designed features is verified by calculating the consistent distribution of facial landmarks before and after video compression.","Our method yields satisfactory results and showcases its potential for practical applications."],"url":"http://arxiv.org/abs/2404.18149v1","category":"cs.CV"}
{"created":"2024-04-28 11:27:30","title":"Generative AI for Visualization: State of the Art and Future Directions","abstract":"Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion model and large language model have also drastically increase the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI and generative algorithms. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.","sentences":["Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design.","Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations.","Concurrently, recent major breakthroughs in GenAI like diffusion model and large language model have also drastically increase the potential of GenAI4VIS.","From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research.","Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction.","For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations.","Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI and generative algorithms.","By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research."],"url":"http://arxiv.org/abs/2404.18144v1","category":"cs.LG"}
{"created":"2024-04-28 11:24:32","title":"Tracking Transforming Objects: A Benchmark","abstract":"Tracking transforming objects holds significant importance in various fields due to the dynamic nature of many real-world scenarios. By enabling systems accurately represent transforming objects over time, tracking transforming objects facilitates advancements in areas such as autonomous systems, human-computer interaction, and security applications. Moreover, understanding the behavior of transforming objects provides valuable insights into complex interactions or processes, contributing to the development of intelligent systems capable of robust and adaptive perception in dynamic environments. However, current research in the field mainly focuses on tracking generic objects. In this study, we bridge this gap by collecting a novel dedicated Dataset for Tracking Transforming Objects, called DTTO, which contains 100 sequences, amounting to approximately 9.3K frames. We provide carefully hand-annotated bounding boxes for each frame within these sequences, making DTTO the pioneering benchmark dedicated to tracking transforming objects. We thoroughly evaluate 20 state-of-the-art trackers on the benchmark, aiming to comprehend the performance of existing methods and provide a comparison for future research on DTTO. With the release of DTTO, our goal is to facilitate further research and applications related to tracking transforming objects.","sentences":["Tracking transforming objects holds significant importance in various fields due to the dynamic nature of many real-world scenarios.","By enabling systems accurately represent transforming objects over time, tracking transforming objects facilitates advancements in areas such as autonomous systems, human-computer interaction, and security applications.","Moreover, understanding the behavior of transforming objects provides valuable insights into complex interactions or processes, contributing to the development of intelligent systems capable of robust and adaptive perception in dynamic environments.","However, current research in the field mainly focuses on tracking generic objects.","In this study, we bridge this gap by collecting a novel dedicated Dataset for Tracking Transforming Objects, called DTTO, which contains 100 sequences, amounting to approximately 9.3K frames.","We provide carefully hand-annotated bounding boxes for each frame within these sequences, making DTTO the pioneering benchmark dedicated to tracking transforming objects.","We thoroughly evaluate 20 state-of-the-art trackers on the benchmark, aiming to comprehend the performance of existing methods and provide a comparison for future research on DTTO.","With the release of DTTO, our goal is to facilitate further research and applications related to tracking transforming objects."],"url":"http://arxiv.org/abs/2404.18143v1","category":"cs.CV"}
{"created":"2024-04-28 10:10:21","title":"Enhancing Fairness in Neural Networks Using FairVIC","abstract":"Mitigating bias in automated decision-making systems, specifically deep learning models, is a critical challenge in achieving fairness. This complexity stems from factors such as nuanced definitions of fairness, unique biases in each dataset, and the trade-off between fairness and model accuracy. To address such issues, we introduce FairVIC, an innovative approach designed to enhance fairness in neural networks by addressing inherent biases at the training stage. FairVIC differs from traditional approaches that typically address biases at the data preprocessing stage. Instead, it integrates variance, invariance and covariance into the loss function to minimise the model's dependency on protected characteristics for making predictions, thus promoting fairness. Our experimentation and evaluation consists of training neural networks on three datasets known for their biases, comparing our results to state-of-the-art algorithms, evaluating on different sizes of model architectures, and carrying out sensitivity analysis to examine the fairness-accuracy trade-off. Through our implementation of FairVIC, we observed a significant improvement in fairness across all metrics tested, without compromising the model's accuracy to a detrimental extent. Our findings suggest that FairVIC presents a straightforward, out-of-the-box solution for the development of fairer deep learning models, thereby offering a generalisable solution applicable across many tasks and datasets.","sentences":["Mitigating bias in automated decision-making systems, specifically deep learning models, is a critical challenge in achieving fairness.","This complexity stems from factors such as nuanced definitions of fairness, unique biases in each dataset, and the trade-off between fairness and model accuracy.","To address such issues, we introduce FairVIC, an innovative approach designed to enhance fairness in neural networks by addressing inherent biases at the training stage.","FairVIC differs from traditional approaches that typically address biases at the data preprocessing stage.","Instead, it integrates variance, invariance and covariance into the loss function to minimise the model's dependency on protected characteristics for making predictions, thus promoting fairness.","Our experimentation and evaluation consists of training neural networks on three datasets known for their biases, comparing our results to state-of-the-art algorithms, evaluating on different sizes of model architectures, and carrying out sensitivity analysis to examine the fairness-accuracy trade-off.","Through our implementation of FairVIC, we observed a significant improvement in fairness across all metrics tested, without compromising the model's accuracy to a detrimental extent.","Our findings suggest that FairVIC presents a straightforward, out-of-the-box solution for the development of fairer deep learning models, thereby offering a generalisable solution applicable across many tasks and datasets."],"url":"http://arxiv.org/abs/2404.18134v1","category":"cs.LG"}
{"created":"2024-04-28 10:03:52","title":"Allocating Mixed Goods with Customized Fairness and Indivisibility Ratio","abstract":"We consider the problem of fairly allocating a combination of divisible and indivisible goods. While fairness criteria like envy-freeness (EF) and proportionality (PROP) can always be achieved for divisible goods, only their relaxed versions, such as the ''up to one'' relaxations EF1 and PROP1, can be satisfied when the goods are indivisible. The ''up to one'' relaxations require the fairness conditions to be satisfied provided that one good can be completely eliminated or added in the comparison. In this work, we bridge the gap between the two extremes and propose ''up to a fraction'' relaxations for the allocation of mixed divisible and indivisible goods. The fraction is determined based on the proportion of indivisible goods, which we call the indivisibility ratio. The new concepts also introduce asymmetric conditions that are customized for individuals with varying indivisibility ratios. We provide both upper and lower bounds on the fractions of the modified item in order to satisfy the fairness criterion. Our results are tight up to a constant for EF and asymptotically tight for PROP.","sentences":["We consider the problem of fairly allocating a combination of divisible and indivisible goods.","While fairness criteria like envy-freeness (EF) and proportionality (PROP) can always be achieved for divisible goods, only their relaxed versions, such as the ''up to one'' relaxations EF1 and PROP1, can be satisfied when the goods are indivisible.","The ''up to one'' relaxations require the fairness conditions to be satisfied provided that one good can be completely eliminated or added in the comparison.","In this work, we bridge the gap between the two extremes and propose ''up to a fraction'' relaxations for the allocation of mixed divisible and indivisible goods.","The fraction is determined based on the proportion of indivisible goods, which we call the indivisibility ratio.","The new concepts also introduce asymmetric conditions that are customized for individuals with varying indivisibility ratios.","We provide both upper and lower bounds on the fractions of the modified item in order to satisfy the fairness criterion.","Our results are tight up to a constant for EF and asymptotically tight for PROP."],"url":"http://arxiv.org/abs/2404.18132v1","category":"cs.GT"}
{"created":"2024-04-28 10:02:28","title":"Logic Agent: Enhancing Validity with Logic Rule Invocation","abstract":"Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.","sentences":["Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks.","Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness.","Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation.","Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms.","The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process.","This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence.","Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks."],"url":"http://arxiv.org/abs/2404.18130v1","category":"cs.AI"}
{"created":"2024-04-28 09:31:36","title":"A Large-Scale Empirical Study of COVID-19 Contact Tracing Mobile App Reviews","abstract":"Since the beginning of 2020, the novel coronavirus has begun to sweep across the globe. Given the prevalence of smartphones everywhere, many countries across continents also developed COVID-19 contract tracing apps that users can install to get a warning of potential contacts with infected people. Unlike regular apps that undergo detailed requirement analysis, carefully designed development, rigorous testing, contact tracing apps were deployed after rapid development. Therefore such apps may not reach expectations for all end users. Users share their opinions and experience of the usage of the apps in the app store. This paper aims to understand the types of topics users discuss in the reviews of the COVID-19 contact tracing apps across the continents by analyzing the app reviews. We collected all the reviews of 35 COVID-19 contact tracing apps developed by 34 countries across the globe. We group the app reviews into the following geographical regions: Asia, Europe, North America, Latin America, Africa, Middle East, and Australasia (Australia and NZ). We run topic modeling on the app reviews of each region. We analyze the produced topics and their evolution over time by categorizing them into hierarchies and computing the ratings of reviews related to the topics. While privacy could be a concern with such apps, we only find privacy-related topics in Australasia, North America, and Middle East. Topics related to usability and performance of the apps are prevalent across all regions. Users frequently complained about the lack of features, user interface and the negative impact of such apps on their mobile batteries. Still, we also find that many users praised the apps because they helped them stay aware of the potential danger of getting infected. The finding of this study is expected to help app developers utilize their resources to address the reported issues in a prioritized way.","sentences":["Since the beginning of 2020, the novel coronavirus has begun to sweep across the globe.","Given the prevalence of smartphones everywhere, many countries across continents also developed COVID-19 contract tracing apps that users can install to get a warning of potential contacts with infected people.","Unlike regular apps that undergo detailed requirement analysis, carefully designed development, rigorous testing, contact tracing apps were deployed after rapid development.","Therefore such apps may not reach expectations for all end users.","Users share their opinions and experience of the usage of the apps in the app store.","This paper aims to understand the types of topics users discuss in the reviews of the COVID-19 contact tracing apps across the continents by analyzing the app reviews.","We collected all the reviews of 35 COVID-19 contact tracing apps developed by 34 countries across the globe.","We group the app reviews into the following geographical regions: Asia, Europe, North America, Latin America, Africa, Middle East, and Australasia (Australia and NZ).","We run topic modeling on the app reviews of each region.","We analyze the produced topics and their evolution over time by categorizing them into hierarchies and computing the ratings of reviews related to the topics.","While privacy could be a concern with such apps, we only find privacy-related topics in Australasia, North America, and Middle East.","Topics related to usability and performance of the apps are prevalent across all regions.","Users frequently complained about the lack of features, user interface and the negative impact of such apps on their mobile batteries.","Still, we also find that many users praised the apps because they helped them stay aware of the potential danger of getting infected.","The finding of this study is expected to help app developers utilize their resources to address the reported issues in a prioritized way."],"url":"http://arxiv.org/abs/2404.18125v1","category":"cs.SE"}
{"created":"2024-04-28 08:51:35","title":"Exploring system size dependence of jet modification in heavy-ion collisions","abstract":"In relativistic heavy-ion collisions, jet quenching in quark-gluon plasma (QGP) has been extensively studied, revealing important insights into the properties of the color deconfined nuclear matter. Over the past decade, there has been a surge of interest in the exploration of QGP droplets in small collision systems like $p$+$p$ or $p$+A collisions driven by the observation of collective flow phenomena. However, the absence of jet quenching, a key QGP signature, in these systems poses a puzzle. Understanding how jet quenching evolves with system size is crucial for uncovering the underlying physics. In this study, we employ the linear Boltzmann transport (LBT) model to investigate jet modification in $^{96}$Ru+$^{96}$Ru, $^{96}$Zr+$^{96}$Zr, and $^{197}$Au+$^{197}$Au collisions at $\\sqrt{s_\\mathrm{NN}}=200$ GeV. Our findings highlight the system size sensitivity exhibited by jet nuclear modification factor ($R_\\mathrm{AA}$) and jet shape ($\\rho$), contrasting to the relatively weak responses of jet mass ($M$), girth ($g$) and momentum dispersion ($p_\\mathrm{T}{D}$) to system size variations. These results offer invaluable insights into the system size dependence of the QGP properties and to be validated experimentally at the Relativistic Heavy-Ion Collider.","sentences":["In relativistic heavy-ion collisions, jet quenching in quark-gluon plasma (QGP) has been extensively studied, revealing important insights into the properties of the color deconfined nuclear matter.","Over the past decade, there has been a surge of interest in the exploration of QGP droplets in small collision systems like $p$+$p$ or $p$+A collisions driven by the observation of collective flow phenomena.","However, the absence of jet quenching, a key QGP signature, in these systems poses a puzzle.","Understanding how jet quenching evolves with system size is crucial for uncovering the underlying physics.","In this study, we employ the linear Boltzmann transport (LBT) model to investigate jet modification in $^{96}$Ru+$^{96}$Ru, $^{96}$Zr+$^{96}$Zr, and $^{197}$Au+$^{197}$Au collisions at $\\sqrt{s_\\mathrm{NN}}=200$ GeV. Our findings highlight the system size sensitivity exhibited by jet nuclear modification factor ($R_\\mathrm{AA}$) and jet shape ($\\rho$), contrasting to the relatively weak responses of jet mass ($M$), girth ($g$) and momentum dispersion ($p_\\mathrm{T}{D}$) to system size variations.","These results offer invaluable insights into the system size dependence of the QGP properties and to be validated experimentally at the Relativistic Heavy-Ion Collider."],"url":"http://arxiv.org/abs/2404.18115v1","category":"nucl-th"}
{"created":"2024-04-28 08:04:04","title":"Finding Beautiful and Happy Images for Mental Health and Well-being Applications","abstract":"This paper explores how artificial intelligence (AI) technology can contribute to achieve progress on good health and well-being, one of the United Nations' 17 Sustainable Development Goals. It is estimated that one in ten of the global population lived with a mental disorder. Inspired by studies showing that engaging and viewing beautiful natural images can make people feel happier and less stressful, lead to higher emotional well-being, and can even have therapeutic values, we explore how AI can help to promote mental health by developing automatic algorithms for finding beautiful and happy images. We first construct a large image database consisting of nearly 20K very high resolution colour photographs of natural scenes where each image is labelled with beautifulness and happiness scores by about 10 observers. Statistics of the database shows that there is a good correlation between the beautifulness and happiness scores which provides anecdotal evidence to corroborate that engaging beautiful natural images can potentially benefit mental well-being. Building on this unique database, the very first of its kind, we have developed a deep learning based model for automatically predicting the beautifulness and happiness scores of natural images. Experimental results are presented to show that it is possible to develop AI algorithms to automatically assess an image's beautifulness and happiness values which can in turn be used to develop applications for promoting mental health and well-being.","sentences":["This paper explores how artificial intelligence (AI) technology can contribute to achieve progress on good health and well-being, one of the United Nations' 17 Sustainable Development Goals.","It is estimated that one in ten of the global population lived with a mental disorder.","Inspired by studies showing that engaging and viewing beautiful natural images can make people feel happier and less stressful, lead to higher emotional well-being, and can even have therapeutic values, we explore how AI can help to promote mental health by developing automatic algorithms for finding beautiful and happy images.","We first construct a large image database consisting of nearly 20K very high resolution colour photographs of natural scenes where each image is labelled with beautifulness and happiness scores by about 10 observers.","Statistics of the database shows that there is a good correlation between the beautifulness and happiness scores which provides anecdotal evidence to corroborate that engaging beautiful natural images can potentially benefit mental well-being.","Building on this unique database, the very first of its kind, we have developed a deep learning based model for automatically predicting the beautifulness and happiness scores of natural images.","Experimental results are presented to show that it is possible to develop AI algorithms to automatically assess an image's beautifulness and happiness values which can in turn be used to develop applications for promoting mental health and well-being."],"url":"http://arxiv.org/abs/2404.18109v1","category":"cs.CV"}
{"created":"2024-04-28 06:50:55","title":"USAT: A Universal Speaker-Adaptive Text-to-Speech Approach","abstract":"Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure.","sentences":["Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset.","The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem.","While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations.","Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents.","While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting.","In addition, prior approaches only provide either zero-shot or few-shot adaptation, constraining their utility across varied real-world scenarios with different demands.","Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers, inadvertently neglecting a vast portion of non-native speakers with diverse accents.","Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as \"instant\" and \"fine-grained\" adaptations based on their merits.","To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder.","To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure."],"url":"http://arxiv.org/abs/2404.18094v1","category":"cs.SD"}
{"created":"2024-04-28 06:25:56","title":"Online,Target-Free LiDAR-Camera Extrinsic Calibration via Cross-Modal Mask Matching","abstract":"LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in intelligent vehicles. Offline, target-based approaches have long been the preferred choice in this field. However, they often demonstrate poor adaptability to real-world environments. This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations. In contrast, online, target-free approaches provide greater adaptability yet typically lack robustness, primarily due to the challenges in cross-modal feature matching. Therefore, in this article, we unleash the full potential of large vision models (LVMs), which are emerging as a significant trend in the fields of computer vision and robotics, especially for embodied artificial intelligence, to achieve robust and accurate online, target-free LCEC across a variety of challenging scenarios. Our main contributions are threefold: we introduce a novel framework known as MIAS-LCEC, provide an open-source versatile calibration toolbox with an interactive visualization interface, and publish three real-world datasets captured from various indoor and outdoor environments. The cornerstone of our framework and toolbox is the cross-modal mask matching (C3M) algorithm, developed based on a state-of-the-art (SoTA) LVM and capable of generating sufficient and reliable matches. Extensive experiments conducted on these real-world datasets demonstrate the robustness of our approach and its superior performance compared to SoTA methods, particularly for the solid-state LiDARs with super-wide fields of view.","sentences":["LiDAR-camera extrinsic calibration (LCEC) is crucial for data fusion in intelligent vehicles.","Offline, target-based approaches have long been the preferred choice in this field.","However, they often demonstrate poor adaptability to real-world environments.","This is largely because extrinsic parameters may change significantly due to moderate shocks or during extended operations in environments with vibrations.","In contrast, online, target-free approaches provide greater adaptability yet typically lack robustness, primarily due to the challenges in cross-modal feature matching.","Therefore, in this article, we unleash the full potential of large vision models (LVMs), which are emerging as a significant trend in the fields of computer vision and robotics, especially for embodied artificial intelligence, to achieve robust and accurate online, target-free LCEC across a variety of challenging scenarios.","Our main contributions are threefold: we introduce a novel framework known as MIAS-LCEC, provide an open-source versatile calibration toolbox with an interactive visualization interface, and publish three real-world datasets captured from various indoor and outdoor environments.","The cornerstone of our framework and toolbox is the cross-modal mask matching (C3M) algorithm, developed based on a state-of-the-art (SoTA) LVM and capable of generating sufficient and reliable matches.","Extensive experiments conducted on these real-world datasets demonstrate the robustness of our approach and its superior performance compared to SoTA methods, particularly for the solid-state LiDARs with super-wide fields of view."],"url":"http://arxiv.org/abs/2404.18083v1","category":"cs.RO"}
{"created":"2024-04-28 06:17:42","title":"ComposerX: Multi-Agent Symbolic Music Composition with LLMs","abstract":"Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.","sentences":["Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints.","While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts.","To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework.","We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4.","The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions."],"url":"http://arxiv.org/abs/2404.18081v1","category":"cs.SD"}
{"created":"2024-04-28 05:46:28","title":"Generative AI for Low-Carbon Artificial Intelligence of Things","abstract":"By integrating Artificial Intelligence (AI) with the Internet of Things (IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields. However, AIoT is facing the challenges of energy consumption and carbon emissions due to the continuous advancement of mobile technology. Fortunately, Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT due to its excellent reasoning and generation capabilities. In this article, we explore the potential of GAI for carbon emissions reduction and propose a novel GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main impacts that cause carbon emissions in AIoT, and then introduce GAI techniques and their relations to carbon emissions. We then explore the application prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon emissions of network components. Subsequently, we propose a Large Language Model (LLM)-enabled carbon emission optimization framework, in which we design pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more accurate and reliable optimization problems. Furthermore, we utilize Generative Diffusion Models (GDMs) to identify optimal strategies for carbon emission reduction. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we insightfully provide open research directions for low-carbon AIoT.","sentences":["By integrating Artificial Intelligence (AI) with the Internet of Things (IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.","However, AIoT is facing the challenges of energy consumption and carbon emissions due to the continuous advancement of mobile technology.","Fortunately, Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT due to its excellent reasoning and generation capabilities.","In this article, we explore the potential of GAI for carbon emissions reduction and propose a novel GAI-enabled solution for low-carbon AIoT.","Specifically, we first study the main impacts that cause carbon emissions in AIoT, and then introduce GAI techniques and their relations to carbon emissions.","We then explore the application prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon emissions of network components.","Subsequently, we propose a Large Language Model (LLM)-enabled carbon emission optimization framework, in which we design pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more accurate and reliable optimization problems.","Furthermore, we utilize Generative Diffusion Models (GDMs) to identify optimal strategies for carbon emission reduction.","Simulation results demonstrate the effectiveness of the proposed framework.","Finally, we insightfully provide open research directions for low-carbon AIoT."],"url":"http://arxiv.org/abs/2404.18077v1","category":"cs.NI"}
{"created":"2024-04-28 05:33:15","title":"MMAC-Copilot: Multi-modal Agent Collaboration Operating System Copilot","abstract":"Autonomous virtual agents are often limited by their singular mode of interaction with real-world environments, restricting their versatility. To address this, we propose the Multi-Modal Agent Collaboration framework (MMAC-Copilot), a framework utilizes the collective expertise of diverse agents to enhance interaction ability with operating systems. The framework introduces a team collaboration chain, enabling each participating agent to contribute insights based on their specific domain knowledge, effectively reducing the hallucination associated with knowledge domain gaps. To evaluate the performance of MMAC-Copilot, we conducted experiments using both the GAIA benchmark and our newly introduced Visual Interaction Benchmark (VIBench). VIBench focuses on non-API-interactable applications across various domains, including 3D gaming, recreation, and office scenarios. MMAC-Copilot achieved exceptional performance on GAIA, with an average improvement of 6.8\\% over existing leading systems. Furthermore, it demonstrated remarkable capability on VIBench, particularly in managing various methods of interaction within systems and applications. These results underscore MMAC-Copilot's potential in advancing the field of autonomous virtual agents through its innovative approach to agent collaboration.","sentences":["Autonomous virtual agents are often limited by their singular mode of interaction with real-world environments, restricting their versatility.","To address this, we propose the Multi-Modal Agent Collaboration framework (MMAC-Copilot), a framework utilizes the collective expertise of diverse agents to enhance interaction ability with operating systems.","The framework introduces a team collaboration chain, enabling each participating agent to contribute insights based on their specific domain knowledge, effectively reducing the hallucination associated with knowledge domain gaps.","To evaluate the performance of MMAC-Copilot, we conducted experiments using both the GAIA benchmark and our newly introduced Visual Interaction Benchmark (VIBench).","VIBench focuses on non-API-interactable applications across various domains, including 3D gaming, recreation, and office scenarios.","MMAC-Copilot achieved exceptional performance on GAIA, with an average improvement of 6.8\\% over existing leading systems.","Furthermore, it demonstrated remarkable capability on VIBench, particularly in managing various methods of interaction within systems and applications.","These results underscore MMAC-Copilot's potential in advancing the field of autonomous virtual agents through its innovative approach to agent collaboration."],"url":"http://arxiv.org/abs/2404.18074v1","category":"cs.AI"}
{"created":"2024-04-28 05:30:50","title":"Charge and spin density wave orders in field-biased Bernal bilayer graphene","abstract":"This paper aims to clarify the nature of a surprising ordered phase recently reported in biased Bernal bilayer graphene that occurs at the phase boundary between the isospin-polarized and unpolarized phases. Strong nonlinearity of transport at abnormally small currents, with $dI/dV$ vs. $I$ sharply rising and then falling back, is typical for a charge/spin-density-wave state (CDW or SDW) sliding transport. Here, however, it is observed at an isospin-order phase boundary, prompting a question about the CDW/SDW mechanism and its relation to the quantum critical point. We argue that the observed phase diagram cannot be understood within a standard weak-coupling picture. Rather, it points to a mechanism that relies on an effective interaction enhancement at a quantum critical point. We develop a detailed strong-coupling framework accounting for the soft collective modes that explain these observations.","sentences":["This paper aims to clarify the nature of a surprising ordered phase recently reported in biased Bernal bilayer graphene that occurs at the phase boundary between the isospin-polarized and unpolarized phases.","Strong nonlinearity of transport at abnormally small currents, with $dI/dV$ vs. $I$ sharply rising and then falling back, is typical for a charge/spin-density-wave state (CDW or SDW) sliding transport.","Here, however, it is observed at an isospin-order phase boundary, prompting a question about the CDW/SDW mechanism and its relation to the quantum critical point.","We argue that the observed phase diagram cannot be understood within a standard weak-coupling picture.","Rather, it points to a mechanism that relies on an effective interaction enhancement at a quantum critical point.","We develop a detailed strong-coupling framework accounting for the soft collective modes that explain these observations."],"url":"http://arxiv.org/abs/2404.18073v1","category":"cond-mat.str-el"}
{"created":"2024-04-28 04:32:44","title":"Quantized Context Based LIF Neurons for Recurrent Spiking Neural Networks in 45nm","abstract":"In this study, we propose the first hardware implementation of a context-based recurrent spiking neural network (RSNN) emphasizing on integrating dual information streams within the neocortical pyramidal neurons specifically Context- Dependent Leaky Integrate and Fire (CLIF) neuron models, essential element in RSNN. We present a quantized version of the CLIF neuron (qCLIF), developed through a hardware-software codesign approach utilizing the sparse activity of RSNN. Implemented in a 45nm technology node, the qCLIF is compact (900um^2) and achieves a high accuracy of 90% despite 8 bit quantization on DVS gesture classification dataset. Our analysis spans a network configuration from 10 to 200 qCLIF neurons, supporting up to 82k synapses within a 1.86 mm^2 footprint, demonstrating scalability and efficiency","sentences":["In this study, we propose the first hardware implementation of a context-based recurrent spiking neural network (RSNN) emphasizing on integrating dual information streams within the neocortical pyramidal neurons specifically Context- Dependent Leaky Integrate and Fire (CLIF) neuron models, essential element in RSNN.","We present a quantized version of the CLIF neuron (qCLIF), developed through a hardware-software codesign approach utilizing the sparse activity of RSNN.","Implemented in a 45nm technology node, the qCLIF is compact (900um^2) and achieves a high accuracy of 90% despite 8 bit quantization on DVS gesture classification dataset.","Our analysis spans a network configuration from 10 to 200 qCLIF neurons, supporting up to 82k synapses within a 1.86 mm^2 footprint, demonstrating scalability and efficiency"],"url":"http://arxiv.org/abs/2404.18066v1","category":"cs.NE"}
{"created":"2024-04-28 04:05:10","title":"Grounded Compositional and Diverse Text-to-3D with Pretrained Multi-View Diffusion Model","abstract":"In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model. Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS). However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts. To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline. We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset. We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images. Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt.","sentences":["In this paper, we propose an effective two-stage approach named Grounded-Dreamer to generate 3D assets that can accurately follow complex, compositional text prompts while achieving high fidelity by using a pre-trained multi-view diffusion model.","Multi-view diffusion models, such as MVDream, have shown to generate high-fidelity 3D assets using score distillation sampling (SDS).","However, applied naively, these methods often fail to comprehend compositional text prompts, and may often entirely omit certain subjects or parts.","To address this issue, we first advocate leveraging text-guided 4-view images as the bottleneck in the text-to-3D pipeline.","We then introduce an attention refocusing mechanism to encourage text-aligned 4-view image generation, without the necessity to re-train the multi-view diffusion model or craft a high-quality compositional 3D dataset.","We further propose a hybrid optimization strategy to encourage synergy between the SDS loss and the sparse RGB reference images.","Our method consistently outperforms previous state-of-the-art (SOTA) methods in generating compositional 3D assets, excelling in both quality and accuracy, and enabling diverse 3D from the same text prompt."],"url":"http://arxiv.org/abs/2404.18065v1","category":"cs.CV"}
{"created":"2024-04-28 01:58:19","title":"LIKO: LiDAR, Inertial, and Kinematic Odometry for Bipedal Robots","abstract":"High-frequency and accurate state estimation is crucial for biped robots. This paper presents a tightly-coupled LiDAR-Inertial-Kinematic Odometry (LIKO) for biped robot state estimation based on an iterated extended Kalman filter. Beyond state estimation, the foot contact position is also modeled and estimated. This allows for both position and velocity updates from kinematic measurement. Additionally, the use of kinematic measurement results in an increased output state frequency of about 1kHz. This ensures temporal continuity of the estimated state and makes it practical for control purposes of biped robots. We also announce a biped robot dataset consisting of LiDAR, inertial measurement unit (IMU), joint encoders, force/torque (F/T) sensors, and motion capture ground truth to evaluate the proposed method. The dataset is collected during robot locomotion, and our approach reached the best quantitative result among other LIO-based methods and biped robot state estimation algorithms. The dataset and source code will be available at https://github.com/Mr-Zqr/LIKO.","sentences":["High-frequency and accurate state estimation is crucial for biped robots.","This paper presents a tightly-coupled LiDAR-Inertial-Kinematic Odometry (LIKO) for biped robot state estimation based on an iterated extended Kalman filter.","Beyond state estimation, the foot contact position is also modeled and estimated.","This allows for both position and velocity updates from kinematic measurement.","Additionally, the use of kinematic measurement results in an increased output state frequency of about 1kHz.","This ensures temporal continuity of the estimated state and makes it practical for control purposes of biped robots.","We also announce a biped robot dataset consisting of LiDAR, inertial measurement unit (IMU), joint encoders, force/torque (F/T) sensors, and motion capture ground truth to evaluate the proposed method.","The dataset is collected during robot locomotion, and our approach reached the best quantitative result among other LIO-based methods and biped robot state estimation algorithms.","The dataset and source code will be available at https://github.com/Mr-Zqr/LIKO."],"url":"http://arxiv.org/abs/2404.18047v1","category":"cs.RO"}
{"created":"2024-04-28 00:44:37","title":"Current laboratory performance of starlight suppression systems, and potential pathways to desired Habitable Worlds Observatory exoplanet science capabilities","abstract":"We summarize the current best polychromatic (10 to 20 % bandwidth) contrast performance demonstrated in the laboratory by different starlight suppression approaches and systems designed to directly characterize exoplanets around nearby stars. We present results obtained by internal coronagraph and external starshade experimental testbeds using entrance apertures equivalent to off-axis or on-axis telescopes, either monolithic or segmented. For a given angular separation and spectral bandwidth, the performance of each starlight suppression system is characterized by the values of raw contrast (before image processing), off-axis (exoplanet) core throughput, and post-calibration contrast (the final 1 sigma detection limit of off-axis point sources, after image processing). To place the current laboratory results in the perspective of the future Habitable Worlds Observatory (HWO) mission, we simulate visible observations of a fiducial Earth/Sun twin system at 12 pc, assuming a 6m (inscribed diameter) collecting aperture and a realistic end-to-end optical throughput. The exposure times required for broadband exoearth detection (20% bandwidth around a wavelength of 0.55 microns) and visible spectroscopic observations (R=70) are then computed assuming various levels of starlight suppression performance, including the values currently demonstrated in the laboratory. Using spectroscopic exposure time as a simple metric, our results point to key starlight suppression system design performance improvements and trades to be conducted in support of HWO exoplanet science capabilities. These trades may be explored via numerical studies, lab experiments, as well as high contrast space-based observations and demonstrations.","sentences":["We summarize the current best polychromatic (10 to 20 % bandwidth) contrast performance demonstrated in the laboratory by different starlight suppression approaches and systems designed to directly characterize exoplanets around nearby stars.","We present results obtained by internal coronagraph and external starshade experimental testbeds using entrance apertures equivalent to off-axis or on-axis telescopes, either monolithic or segmented.","For a given angular separation and spectral bandwidth, the performance of each starlight suppression system is characterized by the values of raw contrast (before image processing), off-axis (exoplanet) core throughput, and post-calibration contrast (the final 1 sigma detection limit of off-axis point sources, after image processing).","To place the current laboratory results in the perspective of the future Habitable Worlds Observatory (HWO) mission, we simulate visible observations of a fiducial Earth/Sun twin system at 12 pc, assuming a 6m (inscribed diameter) collecting aperture and a realistic end-to-end optical throughput.","The exposure times required for broadband exoearth detection (20% bandwidth around a wavelength of 0.55 microns) and visible spectroscopic observations (R=70) are then computed assuming various levels of starlight suppression performance, including the values currently demonstrated in the laboratory.","Using spectroscopic exposure time as a simple metric, our results point to key starlight suppression system design performance improvements and trades to be conducted in support of HWO exoplanet science capabilities.","These trades may be explored via numerical studies, lab experiments, as well as high contrast space-based observations and demonstrations."],"url":"http://arxiv.org/abs/2404.18036v1","category":"astro-ph.IM"}
{"created":"2024-04-28 00:41:57","title":"Micro-swimmer collective dynamics in Brinkman flows","abstract":"Suspensions of swimming micro-organisms are known to undergo intricate collective dynamics as a result of hydrodynamic and collision interactions. Micro-swimmers, such as bacteria and micro-algae, naturally live and have evolved in complex habitats that include impurities, obstacles and interfaces. To elucidate their dynamics in a heterogeneous environment, we consider a continuum theory where the the micro-swimmers are embedded in a Brinkman wet porous medium, which models viscous flow with an additional resistance or friction due to the presence of smaller stationary obstacles. The conservation equation for the swimmer configurations includes advection and rotation by the immersing fluid, and is coupled to the viscous Brinkman fluid flow with an active stress due to the swimmers' motion in it. Resistance alters individual swimmer locomotion and the way it disturbs the surrounding fluid, and thus it alters its hydrodynamic interactions with others and and such affects collective dynamics.The entropy analysis and the linear stability analysis of the system of equations both reveal that resistance delays and hinders the onset and development of the collective swimming instabilities, and can completely suppress it if sufficiently large. Simulations of the full nonlinear system confirm these. We contrast the results with previous theoretical studies on micro-swimmers in homogeneous viscous flow, and discuss relevant experimental realizations.","sentences":["Suspensions of swimming micro-organisms are known to undergo intricate collective dynamics as a result of hydrodynamic and collision interactions.","Micro-swimmers, such as bacteria and micro-algae, naturally live and have evolved in complex habitats that include impurities, obstacles and interfaces.","To elucidate their dynamics in a heterogeneous environment, we consider a continuum theory where the the micro-swimmers are embedded in a Brinkman wet porous medium, which models viscous flow with an additional resistance or friction due to the presence of smaller stationary obstacles.","The conservation equation for the swimmer configurations includes advection and rotation by the immersing fluid, and is coupled to the viscous Brinkman fluid flow with an active stress due to the swimmers' motion in it.","Resistance alters individual swimmer locomotion and the way it disturbs the surrounding fluid, and thus it alters its hydrodynamic interactions with others and and such affects collective dynamics.","The entropy analysis and the linear stability analysis of the system of equations both reveal that resistance delays and hinders the onset and development of the collective swimming instabilities, and can completely suppress it if sufficiently large.","Simulations of the full nonlinear system confirm these.","We contrast the results with previous theoretical studies on micro-swimmers in homogeneous viscous flow, and discuss relevant experimental realizations."],"url":"http://arxiv.org/abs/2404.18035v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 17:59:49","title":"Tunable exciton valley-pseudospin orders in moir\u00e9 Bose-Hubbard model","abstract":"Spin and charge are the two most important degrees of freedom of electrons. Their interplay lies at the heart of numerous strongly correlated phenomena including Hubbard model physics and high temperature superconductivity. Such interplay for bosons, on the other hand, is largely unexplored in condensed matter systems. Here we demonstrate a unique realization of the spin-1/2 Bose-Hubbard model through excitons in a semiconducting moir\\'e superlattice. We find evidence of a transient in-plane ferromagnetic (FM-$xy$) order of exciton spin - here valley pseudospin - around exciton filling $\\nu_{ex}$ = 1, which transitions into a FM-$z$ order both with increasing exciton filling and a small magnetic field of 10 mT. The phase diagram is different from the fermion case and is qualitatively captured by a simple phenomenological model, highlighting the unique consequence of Bose-Einstein statistics. Our study paves the way for engineering exotic phases of matter from spinor bosons, as well as for unconventional devices in optics and quantum information science.","sentences":["Spin and charge are the two most important degrees of freedom of electrons.","Their interplay lies at the heart of numerous strongly correlated phenomena including Hubbard model physics and high temperature superconductivity.","Such interplay for bosons, on the other hand, is largely unexplored in condensed matter systems.","Here we demonstrate a unique realization of the spin-1/2 Bose-Hubbard model through excitons in a semiconducting moir\\'e superlattice.","We find evidence of a transient in-plane ferromagnetic (FM-$xy$) order of exciton spin - here valley pseudospin - around exciton filling $\\nu_{ex}$ = 1, which transitions into a FM-$z$ order both with increasing exciton filling and a small magnetic field of 10 mT. The phase diagram is different from the fermion case and is qualitatively captured by a simple phenomenological model, highlighting the unique consequence of Bose-Einstein statistics.","Our study paves the way for engineering exotic phases of matter from spinor bosons, as well as for unconventional devices in optics and quantum information science."],"url":"http://arxiv.org/abs/2404.18931v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 17:53:55","title":"Interaction driven topological phase transitions of hardcore bosons on a two-leg ladder","abstract":"We investigate the topological properties of hardcore bosons possessing nearest-neighbor repulsive interactions on a two-leg ladder. We show that by allowing dimerized interactions instead of hopping dimerization, the system exhibits topological phases and phase transitions under proper conditions. First, by assuming uniform hopping throughout the ladder, we show that when interaction along the legs are dimerized and the dimerization pattern is different in the legs, a trivial rung-Mott insulator to a topological bond order phase transition occurs as a function of the dimerization strength. However, for a fixed dimerization strength, the system exhibits a topological to trivial phase transition with increase in the rung hopping. A completely different scenario appears when the rung interaction is turned on. We obtain that for a ladder with uniform hopping, the repulsive interaction either turns the topological phase into a trivial rung-Mott insulator or a charge density wave phase. Such feature is absent when the dimerization pattern in the nearest neighbour interaction is considered to be identical in both the legs of the ladder. We numerically obtain the ground state properties and show the signatures of topological phase transition through Thouless charge pumping.","sentences":["We investigate the topological properties of hardcore bosons possessing nearest-neighbor repulsive interactions on a two-leg ladder.","We show that by allowing dimerized interactions instead of hopping dimerization, the system exhibits topological phases and phase transitions under proper conditions.","First, by assuming uniform hopping throughout the ladder, we show that when interaction along the legs are dimerized and the dimerization pattern is different in the legs, a trivial rung-Mott insulator to a topological bond order phase transition occurs as a function of the dimerization strength.","However, for a fixed dimerization strength, the system exhibits a topological to trivial phase transition with increase in the rung hopping.","A completely different scenario appears when the rung interaction is turned on.","We obtain that for a ladder with uniform hopping, the repulsive interaction either turns the topological phase into a trivial rung-Mott insulator or a charge density wave phase.","Such feature is absent when the dimerization pattern in the nearest neighbour interaction is considered to be identical in both the legs of the ladder.","We numerically obtain the ground state properties and show the signatures of topological phase transition through Thouless charge pumping."],"url":"http://arxiv.org/abs/2404.18912v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 17:39:59","title":"Capacity threshold for the Ising perceptron","abstract":"We show that the capacity of the Ising perceptron is with high probability upper bounded by the constant $\\alpha_\\star \\approx 0.833$ conjectured by Krauth and M\\'ezard, under the condition that an explicit two-variable function $\\mathscr{S}_\\star(\\lambda_1,\\lambda_2)$ is maximized at $(1,0)$. The earlier work of Ding and Sun proves the matching lower bound subject to a similar numerical condition, and together these results give a conditional proof of the conjecture of Krauth and M\\'ezard.","sentences":["We show that the capacity of the Ising perceptron is with high probability upper bounded by the constant $\\alpha_\\star \\approx 0.833$ conjectured by Krauth and M\\'ezard, under the condition that an explicit two-variable function $\\mathscr{S}_\\star(\\lambda_1,\\lambda_2)$ is maximized at $(1,0)$. The earlier work of Ding and Sun proves the matching lower bound subject to a similar numerical condition, and together these results give a conditional proof of the conjecture of Krauth and M\\'ezard."],"url":"http://arxiv.org/abs/2404.18902v1","category":"math.PR"}
{"created":"2024-04-29 17:21:18","title":"PrescientFuzz: A more effective exploration approach for grey-box fuzzing","abstract":"In this paper, we introduce an approach for improving the early exploration of grey-box fuzzing campaigns; allowing the fuzzer to reach the interesting coverage earlier. To do this, it leverages information from the system under test's (SUT's) control flow graph in order to decide which inputs are likely to lead to discovering most coverage when mutated.","sentences":["In this paper, we introduce an approach for improving the early exploration of grey-box fuzzing campaigns; allowing the fuzzer to reach the interesting coverage earlier.","To do this, it leverages information from the system under test's (SUT's) control flow graph in order to decide which inputs are likely to lead to discovering most coverage when mutated."],"url":"http://arxiv.org/abs/2404.18887v1","category":"cs.SE"}
{"created":"2024-04-29 17:14:46","title":"SCN as a Local Probe of Protein Structural Dynamics","abstract":"The dynamics of lysozyme is probed by attaching -SCN to all alanine-residues. The 1-dimensional infrared spectra exhibit frequency shifts in the position of the maximum absorption by 4 cm$^{-1}$ which is consistent with experiments in different solvents and indicates moderately strong interactions of the vibrational probe with its environment. Isotopic substitution $^{12}$C $\\rightarrow ^{13}$C leads to a red-shift by $-47$ cm$^{-1}$ which is consistent with experiments with results on CN-substituted copper complexes in solution. The low-frequency, far-infrared part of the protein spectra contain label-specific information in the difference spectra when compared with the wild type protein. Depending on the positioning of the labels, local structural changes are observed. For example, introducing the -SCN label at Ala129 leads to breaking of the $\\alpha-$helical structure with concomitant change in the far-infrared spectrum. Finally, changes in the local hydration of SCN-labelled Alanine residues as a function of time can be related to angular reorientation of the label. It is concluded that -SCN is potentially useful for probing protein dynamics, both in the high-frequency (CN-stretch) and far-infrared part of the spectrum.","sentences":["The dynamics of lysozyme is probed by attaching -SCN to all alanine-residues.","The 1-dimensional infrared spectra exhibit frequency shifts in the position of the maximum absorption by 4 cm$^{-1}$ which is consistent with experiments in different solvents and indicates moderately strong interactions of the vibrational probe with its environment.","Isotopic substitution $^{12}$C $\\rightarrow ^{13}$C leads to a red-shift by $-47$ cm$^{-1}$ which is consistent with experiments with results on CN-substituted copper complexes in solution.","The low-frequency, far-infrared part of the protein spectra contain label-specific information in the difference spectra when compared with the wild type protein.","Depending on the positioning of the labels, local structural changes are observed.","For example, introducing the -SCN label at Ala129 leads to breaking of the $\\alpha-$helical structure with concomitant change in the far-infrared spectrum.","Finally, changes in the local hydration of SCN-labelled Alanine residues as a function of time can be related to angular reorientation of the label.","It is concluded that -SCN is potentially useful for probing protein dynamics, both in the high-frequency (CN-stretch) and far-infrared part of the spectrum."],"url":"http://arxiv.org/abs/2404.18879v1","category":"physics.chem-ph"}
{"created":"2024-04-29 17:14:21","title":"Spin coupling is all you need: Encoding strong electron correlation on quantum computers","abstract":"The performance of quantum algorithms for eigenvalue problems, such as computing Hamiltonian spectra, depends strongly on the overlap of the initial wavefunction and the target eigenvector. In a basis of Slater determinants, the representation of energy eigenstates of systems with $N$ strongly correlated electrons requires a number of determinants that scales exponentially with $N$. On classical processors, this restricts simulations to systems where $N$ is small. Here, we show that quantum computers can efficiently simulate strongly correlated molecular systems by directly encoding the dominant entanglement structure in the form of spin-coupled initial states. This avoids resorting to expensive classical or quantum state preparation heuristics and instead exploits symmetries in the wavefunction. We provide quantum circuits for deterministic preparation of a family of spin eigenfunctions with ${N \\choose N/2}$ Slater determinants with depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ local gates. Their use as highly entangled initial states in quantum algorithms reduces the total runtime of quantum phase estimation and related fault-tolerant methods by orders of magnitude. Furthermore, we assess the application of spin-coupled wavefunctions as initial states for a range of heuristic quantum algorithms, namely the variational quantum eigensolver, adiabatic state preparation, and different versions of quantum subspace diagonalization (QSD) including QSD based on real-time-evolved states. We also propose a novel QSD algorithm that exploits states obtained through adaptive quantum eigensolvers. For all algorithms, we demonstrate that using spin-coupled initial states drastically reduces the quantum resources required to simulate strongly correlated ground and excited states. Our work paves the way towards scalable quantum simulation of electronic structure for classically challenging systems.","sentences":["The performance of quantum algorithms for eigenvalue problems, such as computing Hamiltonian spectra, depends strongly on the overlap of the initial wavefunction and the target eigenvector.","In a basis of Slater determinants, the representation of energy eigenstates of systems with $N$ strongly correlated electrons requires a number of determinants that scales exponentially with $N$. On classical processors, this restricts simulations to systems where $N$ is small.","Here, we show that quantum computers can efficiently simulate strongly correlated molecular systems by directly encoding the dominant entanglement structure in the form of spin-coupled initial states.","This avoids resorting to expensive classical or quantum state preparation heuristics and instead exploits symmetries in the wavefunction.","We provide quantum circuits for deterministic preparation of a family of spin eigenfunctions with ${N \\choose N/2}$ Slater determinants with depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ local gates.","Their use as highly entangled initial states in quantum algorithms reduces the total runtime of quantum phase estimation and related fault-tolerant methods by orders of magnitude.","Furthermore, we assess the application of spin-coupled wavefunctions as initial states for a range of heuristic quantum algorithms, namely the variational quantum eigensolver, adiabatic state preparation, and different versions of quantum subspace diagonalization (QSD) including QSD based on real-time-evolved states.","We also propose a novel QSD algorithm that exploits states obtained through adaptive quantum eigensolvers.","For all algorithms, we demonstrate that using spin-coupled initial states drastically reduces the quantum resources required to simulate strongly correlated ground and excited states.","Our work paves the way towards scalable quantum simulation of electronic structure for classically challenging systems."],"url":"http://arxiv.org/abs/2404.18878v1","category":"quant-ph"}
{"created":"2024-04-29 17:08:15","title":"The Essense of Useful Evaluation Through Quantitative Types (Extended Version)","abstract":"Several evaluation notions for lambda calculus qualify as reasonable cost models according to Slot and van Emde Boas' Invariance Thesis. A notable result achieved by Accattoli and Dal Lago is that leftmost-outermost reduction is reasonable, where the term representation uses sharing and the steps are useful. These results, initially studied in call-by-name, have also been extended to call-by-value. However, the existing formulations of usefulness lack inductive structure, making it challenging in particular to define and reason about type systems on top of the untyped syntax. Additionally, no type-based quantitative interpretations exist for useful evaluation. In this work, we establish the first inductive definition of useful evaluation for open weak call-by-value. This new useful strategy connects to a previous implementation of usefulness through a low-level abstract machine, incurring only in linear time overhead, thus providing a reasonable cost model for open call-by-value implementation. We also propose a semantic interpretation of useful call-by-value using a non-idempotent intersection type system equipped with a notion of tightness. The resulting interpretation is quantitative, i.e. provides exact step-count information for program evaluation. This turns out to be the first semantical interpretation in the literature for a notion of useful evaluation.","sentences":["Several evaluation notions for lambda calculus qualify as reasonable cost models according to Slot and van Emde Boas' Invariance Thesis.","A notable result achieved by Accattoli and Dal Lago is that leftmost-outermost reduction is reasonable, where the term representation uses sharing and the steps are useful.","These results, initially studied in call-by-name, have also been extended to call-by-value.","However, the existing formulations of usefulness lack inductive structure, making it challenging in particular to define and reason about type systems on top of the untyped syntax.","Additionally, no type-based quantitative interpretations exist for useful evaluation.","In this work, we establish the first inductive definition of useful evaluation for open weak call-by-value.","This new useful strategy connects to a previous implementation of usefulness through a low-level abstract machine, incurring only in linear time overhead, thus providing a reasonable cost model for open call-by-value implementation.","We also propose a semantic interpretation of useful call-by-value using a non-idempotent intersection type system equipped with a notion of tightness.","The resulting interpretation is quantitative, i.e. provides exact step-count information for program evaluation.","This turns out to be the first semantical interpretation in the literature for a notion of useful evaluation."],"url":"http://arxiv.org/abs/2404.18874v1","category":"cs.LO"}
{"created":"2024-04-29 17:02:45","title":"Avalanche Dynamics and the Effect of Straining in Dislocation Systems with Quenched Disorder","abstract":"The plastic deformation of crystalline and other heterogeneous materials often manifests in stochastic intermittent events indicating the criticality of plastic behavior. Previous studies demonstrated that the presence of short-ranged quenched disorder modifies this behavior disrupting long-range static and dynamic correlations consequently localizing dislocation avalanches. However, these observations were mostly confined to relaxed materials devoid of deformation history. In this work our focus is on how straining affects static and dynamic correlations, avalanche dynamics and local yield stresses. We demonstrate that the interplay between severe straining and confining quenched disorder induces critical behavior characterized by dislocation avalanches distinct from those at lower stresses. Namely, near the flow stress many avalanches, even if triggered locally, evolve into events affecting a larger region by exciting small clusters of dislocations all around the sample. This type of avalanches differ from the ones at low strains where plastic events typically consist of one compact cluster of dislocations which is either local or it is already quite extended at the onset of the avalanche. Furthermore, we examine the impact of avalanches on local yield stresses. It is shown in detail in this work that while some statistical features of the local yield thresholds are robust to straining, others are significantly affected by the deformation history.","sentences":["The plastic deformation of crystalline and other heterogeneous materials often manifests in stochastic intermittent events indicating the criticality of plastic behavior.","Previous studies demonstrated that the presence of short-ranged quenched disorder modifies this behavior disrupting long-range static and dynamic correlations consequently localizing dislocation avalanches.","However, these observations were mostly confined to relaxed materials devoid of deformation history.","In this work our focus is on how straining affects static and dynamic correlations, avalanche dynamics and local yield stresses.","We demonstrate that the interplay between severe straining and confining quenched disorder induces critical behavior characterized by dislocation avalanches distinct from those at lower stresses.","Namely, near the flow stress many avalanches, even if triggered locally, evolve into events affecting a larger region by exciting small clusters of dislocations all around the sample.","This type of avalanches differ from the ones at low strains where plastic events typically consist of one compact cluster of dislocations which is either local or it is already quite extended at the onset of the avalanche.","Furthermore, we examine the impact of avalanches on local yield stresses.","It is shown in detail in this work that while some statistical features of the local yield thresholds are robust to straining, others are significantly affected by the deformation history."],"url":"http://arxiv.org/abs/2404.18871v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-29 16:57:42","title":"Optimization of District Heating Network Parameters in Steady-State Operation","abstract":"We examine the modeling, simulation, and optimization of district heating systems, which are widely used for thermal transport using steam or hot water as a carrier. We propose a generalizable framework to specify network models and scenario parameters, and develop an optimization method for evaluating system states including pressures, fluid flow rates, and temperatures throughout the network. The network modeling includes pipes, thermal plants, pumps, and passive or controllable loads as system components. We propose basic models for thermodynamic fluid transport and enforce the balance of physical quantities in steady-state flow over co-located outgoing and return networks. We formulate an optimization problem with steam and hot water as the outgoing and return carriers, as in legacy 20th century systems. The physical laws and engineering limitations are specified for each component type, and the thermal network flow optimization (TNFO) problem is formulated and solved for a realistic test network under several scenarios.","sentences":["We examine the modeling, simulation, and optimization of district heating systems, which are widely used for thermal transport using steam or hot water as a carrier.","We propose a generalizable framework to specify network models and scenario parameters, and develop an optimization method for evaluating system states including pressures, fluid flow rates, and temperatures throughout the network.","The network modeling includes pipes, thermal plants, pumps, and passive or controllable loads as system components.","We propose basic models for thermodynamic fluid transport and enforce the balance of physical quantities in steady-state flow over co-located outgoing and return networks.","We formulate an optimization problem with steam and hot water as the outgoing and return carriers, as in legacy 20th century systems.","The physical laws and engineering limitations are specified for each component type, and the thermal network flow optimization (TNFO) problem is formulated and solved for a realistic test network under several scenarios."],"url":"http://arxiv.org/abs/2404.18868v1","category":"math.OC"}
{"created":"2024-04-29 16:52:07","title":"PlanNetX: Learning an Efficient Neural Network Planner from MPC for Longitudinal Control","abstract":"Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems. However, the computational complexity of online optimization can be problematic on embedded devices. Especially, when we need to guarantee fixed control frequencies. Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network. In this work, we instead learn the whole planned trajectory of the MPC. We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC. We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data. Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning.","sentences":["Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems.","However, the computational complexity of online optimization can be problematic on embedded devices.","Especially, when we need to guarantee fixed control frequencies.","Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network.","In this work, we instead learn the whole planned trajectory of the MPC.","We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC.","We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data.","Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning."],"url":"http://arxiv.org/abs/2404.18863v1","category":"cs.RO"}
{"created":"2024-04-29 16:51:30","title":"A Survey on Vision Mamba: Models, Applications and Challenges","abstract":"Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks. Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting. Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers. Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model. Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works. To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches. This paper begins by delineating the formulation of the original Mamba model. Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba. We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others. Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion. Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.","sentences":["Mamba, a recent selective structured state space model, performs excellently on long sequence modeling tasks.","Mamba mitigates the modeling constraints of convolutional neural networks and offers advanced modeling capabilities similar to those of Transformers, through global receptive fields and dynamic weighting.","Crucially, it achieves this without incurring the quadratic computational complexity typically associated with Transformers.","Due to its advantages over the former two mainstream foundation models, Mamba exhibits great potential to be a visual foundation model.","Researchers are actively applying Mamba to various computer vision tasks, leading to numerous emerging works.","To help keep pace with the rapid advancements in computer vision, this paper aims to provide a comprehensive review of visual Mamba approaches.","This paper begins by delineating the formulation of the original Mamba model.","Subsequently, our review of visual Mamba delves into several representative backbone networks to elucidate the core insights of the visual Mamba.","We then categorize related works using different modalities, including image, video, point cloud, multi-modal, and others.","Specifically, for image applications, we further organize them into distinct tasks to facilitate a more structured discussion.","Finally, we discuss the challenges and future research directions for visual Mamba, providing insights for future research in this quickly evolving area.","A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models."],"url":"http://arxiv.org/abs/2404.18861v1","category":"cs.CV"}
{"created":"2024-04-29 16:49:44","title":"VT-MRF-SPF: Variable Target Markov Random Field Scalable Particle Filter","abstract":"Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions. However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring. Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility. To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation. We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality. Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations.","sentences":["Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions.","However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring.","Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility.","To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation.","We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality.","Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations."],"url":"http://arxiv.org/abs/2404.18857v1","category":"stat.ME"}
{"created":"2024-04-29 16:44:27","title":"A Comprehensive Rubric for Annotating Pathological Speech","abstract":"Rubrics are a commonly used tool for labeling voice corpora in speech quality assessment, although their application in the context of pathological speech remains relatively limited. In this study, we introduce a comprehensive rubric based on various dimensions of speech quality, including phonetics, fluency, and prosody. The objective is to establish standardized criteria for identifying errors within the speech of individuals with Down syndrome, thereby enabling the development of automated assessment systems. To achieve this objective, we utilized the Prautocal corpus. To assess the quality of annotations using our rubric, two experiments were conducted, focusing on phonetics and fluency. For phonetic evaluation, we employed the Goodness of Pronunciation (GoP) metric, utilizing automatic segmentation systems and correlating the results with evaluations conducted by a specialized speech therapist. While the obtained correlation values were not notably high, a positive trend was observed. In terms of fluency assessment, deep learning models like wav2vec were used to extract audio features, and we employed an SVM classifier trained on a corpus focused on identifying fluency issues to categorize Prautocal corpus samples. The outcomes highlight the complexities of evaluating such phenomena, with variability depending on the specific type of disfluency detected.","sentences":["Rubrics are a commonly used tool for labeling voice corpora in speech quality assessment, although their application in the context of pathological speech remains relatively limited.","In this study, we introduce a comprehensive rubric based on various dimensions of speech quality, including phonetics, fluency, and prosody.","The objective is to establish standardized criteria for identifying errors within the speech of individuals with Down syndrome, thereby enabling the development of automated assessment systems.","To achieve this objective, we utilized the Prautocal corpus.","To assess the quality of annotations using our rubric, two experiments were conducted, focusing on phonetics and fluency.","For phonetic evaluation, we employed the Goodness of Pronunciation (GoP) metric, utilizing automatic segmentation systems and correlating the results with evaluations conducted by a specialized speech therapist.","While the obtained correlation values were not notably high, a positive trend was observed.","In terms of fluency assessment, deep learning models like wav2vec were used to extract audio features, and we employed an SVM classifier trained on a corpus focused on identifying fluency issues to categorize Prautocal corpus samples.","The outcomes highlight the complexities of evaluating such phenomena, with variability depending on the specific type of disfluency detected."],"url":"http://arxiv.org/abs/2404.18851v1","category":"cs.CL"}
{"created":"2024-04-29 16:41:17","title":"Cyclic measurements and simplified quantum state tomography","abstract":"Tomographic reconstruction of quantum states plays a fundamental role in benchmarking quantum systems and retrieving information from quantum computers. Among the informationally complete sets of quantum measurements the tight ones provide a linear reconstruction formula and minimize the propagation of statistical errors. However, implementing tight measurements in the lab is challenging due to the high number of required measurement projections, involving a series of experimental setup preparations. In this work, we introduce the notion of cyclic tight measurements, that allow us to perform full quantum state tomography while considering only repeated application of a single unitary-based quantum device during the measurement stage process. This type of measurements significantly simplifies the complexity of the experimental setup required to retrieve the quantum state of a physical system. Additionally, we design feasible setup preparation procedure that produce well-approximated cyclic tight measurements, in every finite dimension.","sentences":["Tomographic reconstruction of quantum states plays a fundamental role in benchmarking quantum systems and retrieving information from quantum computers.","Among the informationally complete sets of quantum measurements the tight ones provide a linear reconstruction formula and minimize the propagation of statistical errors.","However, implementing tight measurements in the lab is challenging due to the high number of required measurement projections, involving a series of experimental setup preparations.","In this work, we introduce the notion of cyclic tight measurements, that allow us to perform full quantum state tomography while considering only repeated application of a single unitary-based quantum device during the measurement stage process.","This type of measurements significantly simplifies the complexity of the experimental setup required to retrieve the quantum state of a physical system.","Additionally, we design feasible setup preparation procedure that produce well-approximated cyclic tight measurements, in every finite dimension."],"url":"http://arxiv.org/abs/2404.18847v1","category":"quant-ph"}
{"created":"2024-04-29 16:37:11","title":"Quantum Benchmarking via Random Dynamical Quantum Maps","abstract":"We present a benchmarking protocol for universal quantum computers, achieved through the simulation of random dynamical quantum maps. This protocol provides a holistic assessment of system-wide error rates, encapsulating both gate inaccuracies and the errors associated with mid-circuit qubit measurements and resets. By employing random quantum circuits and segmenting mid-circuit qubit measurement and reset in a repeated fashion, we steer the system of qubits to an ensemble of steady-states. These steady-states are described by random Wishart matrices, and align with the steady-state characteristics previously identified in random Lindbladian dynamics, including the universality property. The protocol assesses the resulting ensemble probability distribution measured in the computational basis, effectively avoiding a tomographic reconstruction. Our various numerical simulations demonstrate the relationship between the final distribution and different error sources. Additionally, we implement the protocol on state-of-the-art transmon qubits provided by IBM Quantum, drawing comparisons between empirical results, theoretical expectations, and simulations derived from a fitted noise model of the device.","sentences":["We present a benchmarking protocol for universal quantum computers, achieved through the simulation of random dynamical quantum maps.","This protocol provides a holistic assessment of system-wide error rates, encapsulating both gate inaccuracies and the errors associated with mid-circuit qubit measurements and resets.","By employing random quantum circuits and segmenting mid-circuit qubit measurement and reset in a repeated fashion, we steer the system of qubits to an ensemble of steady-states.","These steady-states are described by random Wishart matrices, and align with the steady-state characteristics previously identified in random Lindbladian dynamics, including the universality property.","The protocol assesses the resulting ensemble probability distribution measured in the computational basis, effectively avoiding a tomographic reconstruction.","Our various numerical simulations demonstrate the relationship between the final distribution and different error sources.","Additionally, we implement the protocol on state-of-the-art transmon qubits provided by IBM Quantum, drawing comparisons between empirical results, theoretical expectations, and simulations derived from a fitted noise model of the device."],"url":"http://arxiv.org/abs/2404.18846v1","category":"quant-ph"}
{"created":"2024-04-29 16:35:06","title":"The Role of Normal and Non-Normal Contributions to Enstrophy Production in the Near-Wall Region of a Turbulent Channel Flow","abstract":"The turbulent boundary-layer is a region where both preferential dissipation of energy and the production of significant vorticity arises as a consequence of the strong velocity gradients. Previous work has shown that, following a Reynolds decomposition of the enstrophy production, the purely fluctuating contribution is the dominant term and that near the wall this varies in a complex manner with height. In this study we additionally decompose the strain rate and vorticity terms into normal and non-normal components using a Schur decomposition and are able to explain all these features in terms of contributions at different heights from constituents involving different combinations of normal and non-normal quantities. What is surprising about our results is that while the mean shear and the action of larger scale structures should mean that non-normal effects are of over-riding importance, the most important individual term involves the fluctuating, normal straining in the transverse direction. Furthermore, the reason that the term that involves only non-normal contributions is smaller on average than that involving normal straining coupled to non-normal vorticity is that in the former case there are individual constituents that are negative in the mean. Hence, we not only explain the nature of near-wall enstrophy production in greater detail, but highlight how local straining that is orthogonal to the direction of the dominant mean and fluctuating shear plays a crucial role in amplifying vorticity that is yet to have developed sufficiently to gain a solid body rotational component.","sentences":["The turbulent boundary-layer is a region where both preferential dissipation of energy and the production of significant vorticity arises as a consequence of the strong velocity gradients.","Previous work has shown that, following a Reynolds decomposition of the enstrophy production, the purely fluctuating contribution is the dominant term and that near the wall this varies in a complex manner with height.","In this study we additionally decompose the strain rate and vorticity terms into normal and non-normal components using a Schur decomposition and are able to explain all these features in terms of contributions at different heights from constituents involving different combinations of normal and non-normal quantities.","What is surprising about our results is that while the mean shear and the action of larger scale structures should mean that non-normal effects are of over-riding importance, the most important individual term involves the fluctuating, normal straining in the transverse direction.","Furthermore, the reason that the term that involves only non-normal contributions is smaller on average than that involving normal straining coupled to non-normal vorticity is that in the former case there are individual constituents that are negative in the mean.","Hence, we not only explain the nature of near-wall enstrophy production in greater detail, but highlight how local straining that is orthogonal to the direction of the dominant mean and fluctuating shear plays a crucial role in amplifying vorticity that is yet to have developed sufficiently to gain a solid body rotational component."],"url":"http://arxiv.org/abs/2404.18844v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 16:28:14","title":"Fast Quantum Process Tomography via Riemannian Gradient Descent","abstract":"Constrained optimization plays a crucial role in the fields of quantum physics and quantum information science and becomes especially challenging for high-dimensional complex structure problems. One specific issue is that of quantum process tomography, in which the goal is to retrieve the underlying quantum process based on a given set of measurement data. In this paper, we introduce a modified version of stochastic gradient descent on a Riemannian manifold that integrates recent advancements in numerical methods for Riemannian optimization. This approach inherently supports the physically driven constraints of a quantum process, takes advantage of state-of-the-art large-scale stochastic objective optimization, and has superior performance to traditional approaches such as maximum likelihood estimation and projected least squares. The data-driven approach enables accurate, order-of-magnitude faster results, and works with incomplete data. We demonstrate our approach on simulations of quantum processes and in hardware by characterizing an engineered process on quantum computers.","sentences":["Constrained optimization plays a crucial role in the fields of quantum physics and quantum information science and becomes especially challenging for high-dimensional complex structure problems.","One specific issue is that of quantum process tomography, in which the goal is to retrieve the underlying quantum process based on a given set of measurement data.","In this paper, we introduce a modified version of stochastic gradient descent on a Riemannian manifold that integrates recent advancements in numerical methods for Riemannian optimization.","This approach inherently supports the physically driven constraints of a quantum process, takes advantage of state-of-the-art large-scale stochastic objective optimization, and has superior performance to traditional approaches such as maximum likelihood estimation and projected least squares.","The data-driven approach enables accurate, order-of-magnitude faster results, and works with incomplete data.","We demonstrate our approach on simulations of quantum processes and in hardware by characterizing an engineered process on quantum computers."],"url":"http://arxiv.org/abs/2404.18840v1","category":"quant-ph"}
{"created":"2024-04-29 16:27:29","title":"Construction of local reduced spaces for Friedrichs' systems via randomized training","abstract":"This contribution extends the localized training approach, traditionally employed for multiscale problems and parameterized partial differential equations (PDEs) featuring locally heterogeneous coefficients, to the class of linear, positive symmetric operators, known as Friedrichs' operators. Considering a local subdomain with corresponding oversampling domain we prove the compactness of the transfer operator which maps boundary data to solutions on the interior domain. While a Caccioppoli-inequality quantifying the energy decay to the interior holds true for all Friedrichs' systems, showing a compactness result for the graph-spaces hosting the solution is additionally necessary. We discuss the mixed formulation of a convection-diffusion-reaction problem where the necessary compactness result is obtained by the Picard-Weck-Weber theorem. Our numerical results, focusing on a scenario involving heterogeneous diffusion fields with multiple high-conductivity channels, demonstrate the effectiveness of the proposed method.","sentences":["This contribution extends the localized training approach, traditionally employed for multiscale problems and parameterized partial differential equations (PDEs) featuring locally heterogeneous coefficients, to the class of linear, positive symmetric operators, known as Friedrichs' operators.","Considering a local subdomain with corresponding oversampling domain we prove the compactness of the transfer operator which maps boundary data to solutions on the interior domain.","While a Caccioppoli-inequality quantifying the energy decay to the interior holds true for all Friedrichs' systems, showing a compactness result for the graph-spaces hosting the solution is additionally necessary.","We discuss the mixed formulation of a convection-diffusion-reaction problem where the necessary compactness result is obtained by the Picard-Weck-Weber theorem.","Our numerical results, focusing on a scenario involving heterogeneous diffusion fields with multiple high-conductivity channels, demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.18839v1","category":"math.NA"}
{"created":"2024-04-29 16:15:01","title":"Demonstration of system-bath physics on gate-based quantum computer","abstract":"We demonstrate algorithmic cooling on IBM-Q devices. We utilize inherent qubit noise to simulate the equilibration of an interacting spin system towards its ground state, when coupled to a dissipative auxiliary-spin bath. The steady-state correlations in the system are defined by the system Hamiltonian and are stable as long as the algorithm can be executed. In particular, we demonstrate the relaxation of system spins to ferromagnetic and antiferromagnetic ordering, controlled by the definition of the Hamiltonian. We are able to perform simulated cooling for global systems of up to three system spins and four auxiliary spins.","sentences":["We demonstrate algorithmic cooling on IBM-Q devices.","We utilize inherent qubit noise to simulate the equilibration of an interacting spin system towards its ground state, when coupled to a dissipative auxiliary-spin bath.","The steady-state correlations in the system are defined by the system Hamiltonian and are stable as long as the algorithm can be executed.","In particular, we demonstrate the relaxation of system spins to ferromagnetic and antiferromagnetic ordering, controlled by the definition of the Hamiltonian.","We are able to perform simulated cooling for global systems of up to three system spins and four auxiliary spins."],"url":"http://arxiv.org/abs/2404.18828v1","category":"quant-ph"}
{"created":"2024-04-29 16:11:49","title":"Extreme fragmentation of a Bose gas","abstract":"Fragmentation of an interacting Bose gas refers to the macroscopic occupation of a finite set of single-particle eigenstates. This phenomenon is related to the notion of particle-number squeezing in quantum optics, an exquisite property of quantum states that can offer metrological gain. So far, fragmentation has only been partially achieved in experiments involving a large number $N$ of bosons in few modes. Here, we introduce a practical and efficient scheme to prepare fragmented states in systems realizing the $L$-mode Bose-Hubbard model. We demonstrate how a large energy detuning between the modes can be used as a practical control parameter to successfully fragment a Bose gas over an extremely short preparation time. Applying an optimal-control approach within realistic experimental constraints, we obtain total fragmentation at a high filling factor, realizing $\\ket{N/L,...,N/L}$ Fock states with hundreds of bosons in very few modes over a few tunneling times.","sentences":["Fragmentation of an interacting Bose gas refers to the macroscopic occupation of a finite set of single-particle eigenstates.","This phenomenon is related to the notion of particle-number squeezing in quantum optics, an exquisite property of quantum states that can offer metrological gain.","So far, fragmentation has only been partially achieved in experiments involving a large number $N$ of bosons in few modes.","Here, we introduce a practical and efficient scheme to prepare fragmented states in systems realizing the $L$-mode Bose-Hubbard model.","We demonstrate how a large energy detuning between the modes can be used as a practical control parameter to successfully fragment a Bose gas over an extremely short preparation time.","Applying an optimal-control approach within realistic experimental constraints, we obtain total fragmentation at a high filling factor, realizing $\\ket{N/L,...,N/L}$ Fock states with hundreds of bosons in very few modes over a few tunneling times."],"url":"http://arxiv.org/abs/2404.18827v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-29 16:10:32","title":"Winning the Social Media Influence Battle: Uncertainty-Aware Opinions to Understand and Spread True Information via Competitive Influence Maximization","abstract":"Competitive Influence Maximization (CIM) involves entities competing to maximize influence in online social networks (OSNs). Current Deep Reinforcement Learning (DRL) methods in CIM rely on simplistic binary opinion models (i.e., an opinion is represented by either 0 or 1) and often overlook the complexity of users' behavioral characteristics and their prior knowledge. We propose a novel DRL-based framework that enhances CIM analysis by integrating Subjective Logic (SL) to accommodate uncertain opinions, users' behaviors, and their preferences. This approach targets the mitigation of false information by effectively propagating true information. By modeling two competitive agents, one spreading true information and the other spreading false information, we capture the strategic interplay essential to CIM. Our framework utilizes an uncertainty-based opinion model (UOM) to assess the impact on information quality in OSNs, emphasizing the importance of user behavior alongside network topology in selecting influential seed nodes. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, achieving faster and more influential results (i.e., outperforming over 20%) under realistic network conditions. Moreover, our method shows robust performance in partially observable networks, effectively doubling the performance when users are predisposed to disbelieve true information.","sentences":["Competitive Influence Maximization (CIM) involves entities competing to maximize influence in online social networks (OSNs).","Current Deep Reinforcement Learning (DRL) methods in CIM rely on simplistic binary opinion models (i.e., an opinion is represented by either 0 or 1) and often overlook the complexity of users' behavioral characteristics and their prior knowledge.","We propose a novel DRL-based framework that enhances CIM analysis by integrating Subjective Logic (SL) to accommodate uncertain opinions, users' behaviors, and their preferences.","This approach targets the mitigation of false information by effectively propagating true information.","By modeling two competitive agents, one spreading true information and the other spreading false information, we capture the strategic interplay essential to CIM.","Our framework utilizes an uncertainty-based opinion model (UOM) to assess the impact on information quality in OSNs, emphasizing the importance of user behavior alongside network topology in selecting influential seed nodes.","Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, achieving faster and more influential results (i.e., outperforming over 20%) under realistic network conditions.","Moreover, our method shows robust performance in partially observable networks, effectively doubling the performance when users are predisposed to disbelieve true information."],"url":"http://arxiv.org/abs/2404.18826v2","category":"cs.SI"}
{"created":"2024-04-29 16:01:09","title":"Photo-induced insulator-metal transition in paramagnetic (V$_{1-x}$Cr$_{x}$)$_2$O$_3$","abstract":"Pump-probe experiments with femtosecond time resolution allow to disentangle the electronic dynamics from the lattice response and thus provide valuable insights into the non-equilibrium behavior of correlated materials. In Cr-doped V$_2$O$_3$, a multi-orbital Mott-Hubbard material which has been intensively investigated for decades, time-resolved experiments reported a photo-induced insulator-metal transition leading to a transient metal state with nonthermal properties. Here, we combine non-equilibrium dynamical mean-field theory with realistic first principles modeling to simulate the ultrafast response of this material to a laser excitation. Our calculations reproduce the insulating initial state, with orbital occupations in agreement with experiment, and reveal an ultrafast pump-induced gap filling associated with a charge reshuffling between the $e_g^\\pi$ and $a_{1g}$ orbitals. However, in contrast to the related compound VO$_2$, the electronic system thermalizes within a few tens of femtoseconds and we find no evidence for the existence of a metastable nonthermal metal. This suggests that the reported nonthermal behavior in the experiments may be associated with the mismatch between the electronic and lattice temperatures.","sentences":["Pump-probe experiments with femtosecond time resolution allow to disentangle the electronic dynamics from the lattice response and thus provide valuable insights into the non-equilibrium behavior of correlated materials.","In Cr-doped V$_2$O$_3$, a multi-orbital Mott-Hubbard material which has been intensively investigated for decades, time-resolved experiments reported a photo-induced insulator-metal transition leading to a transient metal state with nonthermal properties.","Here, we combine non-equilibrium dynamical mean-field theory with realistic first principles modeling to simulate the ultrafast response of this material to a laser excitation.","Our calculations reproduce the insulating initial state, with orbital occupations in agreement with experiment, and reveal an ultrafast pump-induced gap filling associated with a charge reshuffling between the $e_g^\\pi$ and $a_{1g}$ orbitals.","However, in contrast to the related compound VO$_2$, the electronic system thermalizes within a few tens of femtoseconds and we find no evidence for the existence of a metastable nonthermal metal.","This suggests that the reported nonthermal behavior in the experiments may be associated with the mismatch between the electronic and lattice temperatures."],"url":"http://arxiv.org/abs/2404.18819v1","category":"cond-mat.str-el"}
{"created":"2024-04-29 15:53:03","title":"Hiding from Facebook: An Encryption Protocol resistant to Correlation Attacks","abstract":"In many social networks, one publishes information that one wants to reveal (e.g., the photograph of some friends) together with information that may lead to privacy breaches (e.g., the name of these people). One might want to hide this sensitive information by encrypting it and sharing the decryption key only with trusted people, but this might not be enough. If the cipher associated to a face is always the same, correlation between the output of a face recognition system and the cipher can give useful clues and help train recognizers to identify untagged instances of the face. We refer to these as \"correlation attacks\".   In this paper we present a coding system that attempts to counter correlation attacks by associating to each instance of a face a different encryption of the same tag in such a way that the correlation between different instances is minimal.   In addition, we present a key distribution code that allows only the owner of the images to encode the tags, but allows a group of trusted friends to decode them.","sentences":["In many social networks, one publishes information that one wants to reveal (e.g., the photograph of some friends) together with information that may lead to privacy breaches (e.g., the name of these people).","One might want to hide this sensitive information by encrypting it and sharing the decryption key only with trusted people, but this might not be enough.","If the cipher associated to a face is always the same, correlation between the output of a face recognition system and the cipher can give useful clues and help train recognizers to identify untagged instances of the face.","We refer to these as \"correlation attacks\".   ","In this paper we present a coding system that attempts to counter correlation attacks by associating to each instance of a face a different encryption of the same tag in such a way that the correlation between different instances is minimal.   ","In addition, we present a key distribution code that allows only the owner of the images to encode the tags, but allows a group of trusted friends to decode them."],"url":"http://arxiv.org/abs/2404.18817v1","category":"cs.CR"}
{"created":"2024-04-29 15:52:05","title":"Bifurcations for Lagrangian systems and geodesics","abstract":"In this paper we shall use the abstract bifurcation theorems developed by the author in previous papers to study bifurcations of solutions for Lagrangian systems on manifolds linearly or nonlinearly dependent on parameters under various boundary value conditions. As applications, many bifurcation results for geodesics on Finsler and Riemannian manifolds are derived.","sentences":["In this paper we shall use the abstract bifurcation theorems developed by the author in previous papers to study bifurcations of solutions for Lagrangian systems on manifolds linearly or nonlinearly dependent on parameters under various boundary value conditions.","As applications, many bifurcation results for geodesics on Finsler and Riemannian manifolds are derived."],"url":"http://arxiv.org/abs/2404.18815v1","category":"math.DS"}
{"created":"2024-04-29 15:51:49","title":"Belt and Brace: When Federated Learning Meets Differential Privacy","abstract":"Federated learning (FL) has great potential for large-scale machine learning (ML) without exposing raw data.Differential privacy (DP) is the de facto standard of privacy protection with provable guarantees.Advances in ML suggest that DP would be a perfect fit for FL with comprehensive privacy preservation. Hence, extensive efforts have been devoted to achieving practically usable FL with DP, which however is still challenging.Practitioners often not only are not fully aware of its development and categorization, but also face a hard choice between privacy and utility. Therefore, it calls for a holistic review of current advances and an investigation on the challenges and opportunities for highly usable FL systems with a DP guarantee. In this article, we first introduce the primary concepts of FL and DP, and highlight the benefits of integration. We then review the current developments by categorizing different paradigms and notions. Aiming at usable FL with DP, we present the optimization principles to seek a better tradeoff between model utility and privacy loss. Finally, we discuss future challenges in the emergent areas and relevant research topics.","sentences":["Federated learning (FL) has great potential for large-scale machine learning (ML) without exposing raw data.","Differential privacy (DP) is the de facto standard of privacy protection with provable guarantees.","Advances in ML suggest that DP would be a perfect fit for FL with comprehensive privacy preservation.","Hence, extensive efforts have been devoted to achieving practically usable FL with DP, which however is still challenging.","Practitioners often not only are not fully aware of its development and categorization, but also face a hard choice between privacy and utility.","Therefore, it calls for a holistic review of current advances and an investigation on the challenges and opportunities for highly usable FL systems with a DP guarantee.","In this article, we first introduce the primary concepts of FL and DP, and highlight the benefits of integration.","We then review the current developments by categorizing different paradigms and notions.","Aiming at usable FL with DP, we present the optimization principles to seek a better tradeoff between model utility and privacy loss.","Finally, we discuss future challenges in the emergent areas and relevant research topics."],"url":"http://arxiv.org/abs/2404.18814v1","category":"cs.CR"}
{"created":"2024-04-29 15:49:27","title":"Efficient Inverted Indexes for Approximate Retrieval over Learned Sparse Representations","abstract":"Learned sparse representations form an attractive class of contextual embeddings for text retrieval. That is so because they are effective models of relevance and are interpretable by design. Despite their apparent compatibility with inverted indexes, however, retrieval over sparse embeddings remains challenging. That is due to the distributional differences between learned embeddings and term frequency-based lexical models of relevance such as BM25. Recognizing this challenge, a great deal of research has gone into, among other things, designing retrieval algorithms tailored to the properties of learned sparse representations, including approximate retrieval systems. In fact, this task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on a large benchmark dataset by throughput and recall. In this work, we propose a novel organization of the inverted index that enables fast yet effective approximate retrieval over learned sparse embeddings. Our approach organizes inverted lists into geometrically-cohesive blocks, each equipped with a summary vector. During query processing, we quickly determine if a block must be evaluated using the summaries. As we show experimentally, single-threaded query processing using our method, Seismic, reaches sub-millisecond per-query latency on various sparse embeddings of the MS MARCO dataset while maintaining high recall. Our results indicate that Seismic is one to two orders of magnitude faster than state-of-the-art inverted index-based solutions and further outperforms the winning (graph-based) submissions to the BigANN Challenge by a significant margin.","sentences":["Learned sparse representations form an attractive class of contextual embeddings for text retrieval.","That is so because they are effective models of relevance and are interpretable by design.","Despite their apparent compatibility with inverted indexes, however, retrieval over sparse embeddings remains challenging.","That is due to the distributional differences between learned embeddings and term frequency-based lexical models of relevance such as BM25.","Recognizing this challenge, a great deal of research has gone into, among other things, designing retrieval algorithms tailored to the properties of learned sparse representations, including approximate retrieval systems.","In fact, this task featured prominently in the latest BigANN Challenge at NeurIPS 2023, where approximate algorithms were evaluated on a large benchmark dataset by throughput and recall.","In this work, we propose a novel organization of the inverted index that enables fast yet effective approximate retrieval over learned sparse embeddings.","Our approach organizes inverted lists into geometrically-cohesive blocks, each equipped with a summary vector.","During query processing, we quickly determine if a block must be evaluated using the summaries.","As we show experimentally, single-threaded query processing using our method, Seismic, reaches sub-millisecond per-query latency on various sparse embeddings of the MS MARCO dataset while maintaining high recall.","Our results indicate that Seismic is one to two orders of magnitude faster than state-of-the-art inverted index-based solutions and further outperforms the winning (graph-based) submissions to the BigANN Challenge by a significant margin."],"url":"http://arxiv.org/abs/2404.18812v1","category":"cs.IR"}
{"created":"2024-04-29 15:44:35","title":"The Landscape of Unfolding with Machine Learning","abstract":"Recent innovations from machine learning allow for data unfolding, without binning and including correlations across many dimensions. We describe a set of known, upgraded, and new methods for ML-based unfolding. The performance of these approaches are evaluated on the same two datasets. We find that all techniques are capable of accurately reproducing the particle-level spectra across complex observables. Given that these approaches are conceptually diverse, they offer an exciting toolkit for a new class of measurements that can probe the Standard Model with an unprecedented level of detail and may enable sensitivity to new phenomena.","sentences":["Recent innovations from machine learning allow for data unfolding, without binning and including correlations across many dimensions.","We describe a set of known, upgraded, and new methods for ML-based unfolding.","The performance of these approaches are evaluated on the same two datasets.","We find that all techniques are capable of accurately reproducing the particle-level spectra across complex observables.","Given that these approaches are conceptually diverse, they offer an exciting toolkit for a new class of measurements that can probe the Standard Model with an unprecedented level of detail and may enable sensitivity to new phenomena."],"url":"http://arxiv.org/abs/2404.18807v1","category":"hep-ph"}
{"created":"2024-04-29 15:36:13","title":"Location-Based Load Balancing for Energy-Efficient Cell-Free Networks","abstract":"Cell-Free Massive MIMO (CF mMIMO) has emerged as a potential enabler for future networks. It has been shown that these networks are much more energy-efficient than classical cellular systems when they are serving users at peak capacity. However, these CF mMIMO networks are designed for peak traffic loads, and when this is not the case, they are significantly over-dimensioned and not at all energy efficient. To this end, Adaptive Access Point (AP) ON/OFF Switching (ASO) strategies have been developed to save energy when the network is not at peak traffic loads by putting unnecessary APs to sleep. Unfortunately, the existing strategies rely on measuring channel state information between every user and every access point, resulting in significant measurement energy consumption overheads. Furthermore, the current state-of-art approach has a computational complexity that scales exponentially with the number of APs. In this work, we present a novel convex feasibility testing method that allows checking per-user Quality-of-Service (QoS) requirements without necessarily considering all possible access point activations. We then propose an iterative algorithm for activating access points until all users' requirements are fulfilled. We show that our method has comparable performance to the optimal solution whilst avoiding solving costly mixed-integer problems and measuring channel state information on only a limited subset of APs.","sentences":["Cell-Free Massive MIMO (CF mMIMO) has emerged as a potential enabler for future networks.","It has been shown that these networks are much more energy-efficient than classical cellular systems when they are serving users at peak capacity.","However, these CF mMIMO networks are designed for peak traffic loads, and when this is not the case, they are significantly over-dimensioned and not at all energy efficient.","To this end, Adaptive Access Point (AP) ON/OFF Switching (ASO) strategies have been developed to save energy when the network is not at peak traffic loads by putting unnecessary APs to sleep.","Unfortunately, the existing strategies rely on measuring channel state information between every user and every access point, resulting in significant measurement energy consumption overheads.","Furthermore, the current state-of-art approach has a computational complexity that scales exponentially with the number of APs.","In this work, we present a novel convex feasibility testing method that allows checking per-user Quality-of-Service (QoS) requirements without necessarily considering all possible access point activations.","We then propose an iterative algorithm for activating access points until all users' requirements are fulfilled.","We show that our method has comparable performance to the optimal solution whilst avoiding solving costly mixed-integer problems and measuring channel state information on only a limited subset of APs."],"url":"http://arxiv.org/abs/2404.18799v1","category":"eess.SP"}
{"created":"2024-04-29 15:34:32","title":"Multi-Agent Synchronization Tasks","abstract":"In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone. The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial. In this paper, we introduce and define $\\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks. We describe one MST, that we call $\\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies. Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component. Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context.","sentences":["In multi-agent reinforcement learning (MARL), coordination plays a crucial role in enhancing agents' performance beyond what they could achieve through cooperation alone.","The interdependence of agents' actions, coupled with the need for communication, leads to a domain where effective coordination is crucial.","In this paper, we introduce and define $\\textit{Multi-Agent Synchronization Tasks}$ (MSTs), a novel subset of multi-agent tasks.","We describe one MST, that we call $\\textit{Synchronized Predator-Prey}$, offering a detailed description that will serve as the basis for evaluating a selection of recent state-of-the-art (SOTA) MARL algorithms explicitly designed to address coordination challenges through the use of communication strategies.","Furthermore, we present empirical evidence that reveals the limitations of the algorithms assessed to solve MSTs, demonstrating their inability to scale effectively beyond 2-agent coordination tasks in scenarios where communication is a requisite component.","Finally, the results raise questions about the applicability of recent SOTA approaches for complex coordination tasks (i.e. MSTs) and prompt further exploration into the underlying causes of their limitations in this context."],"url":"http://arxiv.org/abs/2404.18798v1","category":"cs.MA"}
{"created":"2024-04-29 15:29:24","title":"When Lawvere meets Peirce: an equational presentation of boolean hyperdoctrines","abstract":"Fo-bicategories are a categorification of Peirce's calculus of relations. Notably, their laws provide a proof system for first-order logic that is both purely equational and complete. This paper illustrates a correspondence between fo-bicategories and Lawvere's hyperdoctrines. To streamline our proof, we introduce peircean bicategories, which offer a more succinct characterization of fo-bicategories.","sentences":["Fo-bicategories are a categorification of Peirce's calculus of relations.","Notably, their laws provide a proof system for first-order logic that is both purely equational and complete.","This paper illustrates a correspondence between fo-bicategories and Lawvere's hyperdoctrines.","To streamline our proof, we introduce peircean bicategories, which offer a more succinct characterization of fo-bicategories."],"url":"http://arxiv.org/abs/2404.18795v1","category":"math.CT"}
{"created":"2024-04-29 15:27:50","title":"Optimality and uniqueness of the D_4 root system","abstract":"We prove that the $D_4$ root system (the set of vertices of the regular $24$-cell) is the unique optimal kissing configuration in $\\mathbb R^4$, and is an optimal spherical code. For this, we use semidefinite programming to compute an exact optimal solution to the second level of the Lasserre hierarchy. We also improve the upper bound for the kissing number problem in $\\mathbb R^6$ to $77$.","sentences":["We prove that the $D_4$ root system (the set of vertices of the regular $24$-cell) is the unique optimal kissing configuration in $\\mathbb R^4$, and is an optimal spherical code.","For this, we use semidefinite programming to compute an exact optimal solution to the second level of the Lasserre hierarchy.","We also improve the upper bound for the kissing number problem in $\\mathbb R^6$ to $77$."],"url":"http://arxiv.org/abs/2404.18794v1","category":"math.MG"}
{"created":"2024-04-29 15:26:58","title":"Bergman local isometries are biholomorphisms","abstract":"We prove that a proper holomorphic local isometry between bounded domains with respect to the Bergman metrics is necessarily a biholomorphism. The proof relies on a new method grounded in Information Geometry theories.","sentences":["We prove that a proper holomorphic local isometry between bounded domains with respect to the Bergman metrics is necessarily a biholomorphism.","The proof relies on a new method grounded in Information Geometry theories."],"url":"http://arxiv.org/abs/2404.18792v1","category":"math.CV"}
{"created":"2024-04-29 15:17:59","title":"Whale Optimization Algorithm-based Fractional Order Fuzzy Type-II PI Control for Modular Multilevel Converters","abstract":"Designing a robust controller for Modular Multilevel Converters (MMCs) is crucial to ensure stability and optimal dynamic performance under various operating conditions, including faulty and disturbed scenarios. The primary objective of controlling grid-connected MMCs (GC-MMCs) is to accurately track real and reactive power references while maintaining excellent harmonic performance in the output response. This paper proposes a novel model-free control strategy for GC-MMCs, employing a Fractional Order Proportional-Integral (FOPI) controller and a Fractional Order Fuzzy type-II Proportional-Integral (FOFPI) controller. The FOFPI controller utilizes a type-II Fuzzy Inference System (FIS) to adaptively adjust the proportional and derivative gains during the control process, enabling effective control of the MMC under diverse operating conditions. The type-II FIS, which leverages type-II fuzzy sets, can mitigate uncertainty and nonlinearity in the system. Furthermore, the incorporation of fractional-order mathematics enhances the flexibility of the proposed controllers. To optimize the initial parameters of the proposed controllers, the Whale Optimization Algorithm (WOA), a meta-heuristic algorithm, is employed. The results demonstrate that the proposed controllers exhibit superior performance under voltage disturbance conditions, varying input voltage, and can ensure the stability of the MMC.","sentences":["Designing a robust controller for Modular Multilevel Converters (MMCs) is crucial to ensure stability and optimal dynamic performance under various operating conditions, including faulty and disturbed scenarios.","The primary objective of controlling grid-connected MMCs (GC-MMCs) is to accurately track real and reactive power references while maintaining excellent harmonic performance in the output response.","This paper proposes a novel model-free control strategy for GC-MMCs, employing a Fractional Order Proportional-Integral (FOPI) controller and a Fractional Order Fuzzy type-II Proportional-Integral (FOFPI) controller.","The FOFPI controller utilizes a type-II Fuzzy Inference System (FIS) to adaptively adjust the proportional and derivative gains during the control process, enabling effective control of the MMC under diverse operating conditions.","The type-II FIS, which leverages type-II fuzzy sets, can mitigate uncertainty and nonlinearity in the system.","Furthermore, the incorporation of fractional-order mathematics enhances the flexibility of the proposed controllers.","To optimize the initial parameters of the proposed controllers, the Whale Optimization Algorithm (WOA), a meta-heuristic algorithm, is employed.","The results demonstrate that the proposed controllers exhibit superior performance under voltage disturbance conditions, varying input voltage, and can ensure the stability of the MMC."],"url":"http://arxiv.org/abs/2404.18782v1","category":"eess.SY"}
{"created":"2024-04-29 15:16:33","title":"Optimal time sampling in physics-informed neural networks","abstract":"Physics-informed neural networks (PINN) is a extremely powerful paradigm used to solve equations encountered in scientific computing applications. An important part of the procedure is the minimization of the equation residual which includes, when the equation is time-dependent, a time sampling. It was argued in the literature that the sampling need not be uniform but should overweight initial time instants, but no rigorous explanation was provided for these choice. In this paper we take some prototypical examples and, under standard hypothesis concerning the neural network convergence, we show that the optimal time sampling follows a truncated exponential distribution. In particular we explain when the time sampling is best to be uniform and when it should not be. The findings are illustrated with numerical examples on linear equation, Burgers' equation and the Lorenz system.","sentences":["Physics-informed neural networks (PINN) is a extremely powerful paradigm used to solve equations encountered in scientific computing applications.","An important part of the procedure is the minimization of the equation residual which includes, when the equation is time-dependent, a time sampling.","It was argued in the literature that the sampling need not be uniform but should overweight initial time instants, but no rigorous explanation was provided for these choice.","In this paper we take some prototypical examples and, under standard hypothesis concerning the neural network convergence, we show that the optimal time sampling follows a truncated exponential distribution.","In particular we explain when the time sampling is best to be uniform and when it should not be.","The findings are illustrated with numerical examples on linear equation, Burgers' equation and the Lorenz system."],"url":"http://arxiv.org/abs/2404.18780v1","category":"cs.LG"}
{"created":"2024-04-29 15:03:40","title":"Non-stabilizerness versus entanglement in matrix product states","abstract":"In this paper, we investigate the relationship between entanglement and non-stabilizerness (also known as magic) in matrix product states (MPSs). We study the relation between magic and the bond dimension used to approximate the ground state of a many-body system in two different contexts: full state of magic and mutual magic (the non-stabilizer analogue of mutual information, thus free of boundary effects) of spin-1 anisotropic Heisenberg chains. Our results indicate that obtaining converged results for non-stabilizerness is typically considerably easier than entanglement. For full state magic at critical points and at sufficiently large volumes, we observe convergence with $1/\\chi^2$, with $\\chi$ being the MPS bond dimension. At small volumes, magic saturation is so quick that, within error bars, we cannot appreciate any finite-$\\chi$ correction. Mutual magic also shows a fast convergence with bond dimension, whose specific functional form is however hindered by sampling errors. As a by-product of our study, we show how Pauli-Markov chains (originally formulated to evaluate magic) resets the state of the art in terms of computing mutual information for MPS. We illustrate this last fact by verifying the logarithmic increase of mutual information between connected partitions at critical points. By comparing mutual information and mutual magic, we observe that, for connected partitions, the latter is typically scaling much slower - if at all - with the partition size, while for disconnected partitions, both are constant in size.","sentences":["In this paper, we investigate the relationship between entanglement and non-stabilizerness (also known as magic) in matrix product states (MPSs).","We study the relation between magic and the bond dimension used to approximate the ground state of a many-body system in two different contexts: full state of magic and mutual magic (the non-stabilizer analogue of mutual information, thus free of boundary effects) of spin-1 anisotropic Heisenberg chains.","Our results indicate that obtaining converged results for non-stabilizerness is typically considerably easier than entanglement.","For full state magic at critical points and at sufficiently large volumes, we observe convergence with $1/\\chi^2$, with $\\chi$ being the MPS bond dimension.","At small volumes, magic saturation is so quick that, within error bars, we cannot appreciate any finite-$\\chi$ correction.","Mutual magic also shows a fast convergence with bond dimension, whose specific functional form is however hindered by sampling errors.","As a by-product of our study, we show how Pauli-Markov chains (originally formulated to evaluate magic) resets the state of the art in terms of computing mutual information for MPS.","We illustrate this last fact by verifying the logarithmic increase of mutual information between connected partitions at critical points.","By comparing mutual information and mutual magic, we observe that, for connected partitions, the latter is typically scaling much slower - if at all - with the partition size, while for disconnected partitions, both are constant in size."],"url":"http://arxiv.org/abs/2404.18768v1","category":"quant-ph"}
{"created":"2024-04-29 15:02:57","title":"A Port-Hamiltonian System Perspective on Electromagneto-Quasistatic Field Formulations of Darwin-Type","abstract":"Electromagneto-quasistatic (EMQS) field formulations are often dubbed as Darwin-type field formulations which approximate the Maxwell equations by neglecting radiation effects while modelling resistive, capacitive, and inductive effects. A common feature of EMQS field models is the Darwin-Amp\\'ere equation formulated with the magnetic vector potential and the electric scalar potential. EMQS field formulations yield different approximations to the Maxwell equations by choice of additional gauge equations. These EMQS formulations are analyzed within the port-Hamiltonian system (PHS) framework. It is shown via the PHS compatibility equation that formulations based on the combination of the Darwin-Amp\\'ere equation and the full Maxwell continuity equation yield port-Hamiltonian systems implying numerical stability and specific EMQS energy conservation.","sentences":["Electromagneto-quasistatic (EMQS) field formulations are often dubbed as Darwin-type field formulations which approximate the Maxwell equations by neglecting radiation effects while modelling resistive, capacitive, and inductive effects.","A common feature of EMQS field models is the Darwin-Amp\\'ere equation formulated with the magnetic vector potential and the electric scalar potential.","EMQS field formulations yield different approximations to the Maxwell equations by choice of additional gauge equations.","These EMQS formulations are analyzed within the port-Hamiltonian system (PHS) framework.","It is shown via the PHS compatibility equation that formulations based on the combination of the Darwin-Amp\\'ere equation and the full Maxwell continuity equation yield port-Hamiltonian systems implying numerical stability and specific EMQS energy conservation."],"url":"http://arxiv.org/abs/2404.18767v1","category":"cs.CE"}
{"created":"2024-04-29 14:56:47","title":"Towards A Structured Overview of Use Cases for Natural Language Processing in the Legal Domain: A German Perspective","abstract":"In recent years, the field of Legal Tech has risen in prevalence, as the Natural Language Processing (NLP) and legal disciplines have combined forces to digitalize legal processes. Amidst the steady flow of research solutions stemming from the NLP domain, the study of use cases has fallen behind, leading to a number of innovative technical methods without a place in practice. In this work, we aim to build a structured overview of Legal Tech use cases, grounded in NLP literature, but also supplemented by voices from legal practice in Germany. Based upon a Systematic Literature Review, we identify seven categories of NLP technologies for the legal domain, which are then studied in juxtaposition to 22 legal use cases. In the investigation of these use cases, we identify 15 ethical, legal, and social aspects (ELSA), shedding light on the potential concerns of digitally transforming the legal domain.","sentences":["In recent years, the field of Legal Tech has risen in prevalence, as the Natural Language Processing (NLP) and legal disciplines have combined forces to digitalize legal processes.","Amidst the steady flow of research solutions stemming from the NLP domain, the study of use cases has fallen behind, leading to a number of innovative technical methods without a place in practice.","In this work, we aim to build a structured overview of Legal Tech use cases, grounded in NLP literature, but also supplemented by voices from legal practice in Germany.","Based upon a Systematic Literature Review, we identify seven categories of NLP technologies for the legal domain, which are then studied in juxtaposition to 22 legal use cases.","In the investigation of these use cases, we identify 15 ethical, legal, and social aspects (ELSA), shedding light on the potential concerns of digitally transforming the legal domain."],"url":"http://arxiv.org/abs/2404.18759v1","category":"cs.CL"}
{"created":"2024-04-29 14:48:07","title":"Unveiling the Impact of B-site Distribution on the Frustration Effect in Double Perovskite Ca2FeReO6 Using Monte Carlo Simulation and Molecular Field Theory","abstract":"This work systematically investigates the spin glass behavior of the double perovskite Ca2FeReO6. Building on previous studies, we have developed a formula to quantify the ions distribution at B-site, incorporating the next-nearest neighbor interactions. Employing molecular field theory and Monte Carlo simulations, the influence of various arrangements of two B-site ions on frustration effects was uncovered. B-site is segmented into a and b-site, defining the number of nearest neighbors from Fea to Feb (and vice versa) as Zx(Zy). The significant frustration effects occur when 1<Zx(or Zy)<3, with Zx is not equal to Zy and also when Zx(or Zy) ~ 3 while Zy(or Zx) ~ 4. All of these are reflected in the variations observed in ground state magnetization and the Thermal Energy Step relation to Zx and Zy. The model proposed in this work can be applied to most B-site disordered in perovskite systems and even to other chemically disordered in frustrated systems.","sentences":["This work systematically investigates the spin glass behavior of the double perovskite Ca2FeReO6.","Building on previous studies, we have developed a formula to quantify the ions distribution at B-site, incorporating the next-nearest neighbor interactions.","Employing molecular field theory and Monte Carlo simulations, the influence of various arrangements of two B-site ions on frustration effects was uncovered.","B-site is segmented into a and b-site, defining the number of nearest neighbors from Fea to Feb (and vice versa) as Zx(Zy).","The significant frustration effects occur when 1<Zx(or Zy)<3, with Zx is not equal to Zy and also when Zx(or Zy) ~ 3","while Zy(or Zx) ~ 4. All of these are reflected in the variations observed in ground state magnetization and the Thermal Energy Step relation to Zx and Zy.","The model proposed in this work can be applied to most B-site disordered in perovskite systems and even to other chemically disordered in frustrated systems."],"url":"http://arxiv.org/abs/2404.18748v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-29 14:46:28","title":"Positive and non-positive measurements in energy extraction from quantum batteries","abstract":"We extend the concept of stochastic energy extraction from quantum batteries to the scenario where both positive operator-valued (POV) and physically realizable non-positive operator-valued measurements are applied (NPOVMs) on the auxiliary connected to the battery in presence of noise. The process involves joint evolution of the battery and the auxiliary for a particular time interval, an interaction of the auxiliary with its environment which induces noise in the auxiliary, and performing a POVM or NPOVM on the auxiliary, and finally the selection of a particular measurement outcome. Application of POVM on the auxiliary can be realised by attaching an external system to the auxiliary, which is initially in a product state with the rest of the system, and performing a joint projective measurement on the auxiliary and external. On the other hand, if there are interactions leading to correlations among the auxiliary, environment, and external systems, then performing the projective measurement on the auxiliary-environment-external system can be interpreted as a physically realizable NPOVM operation on the auxiliary. We however utilize interaction between the auxiliary and the environment to implement NPOVMs on the auxiliary. We find the expressions of stochastically extractable energy by performing POVMs and NPOVMs on the auxiliary and show that the latter does not depend on the applied noise. Focusing on a particular model of the governing Hamiltonian of a qubit battery and an auxiliary, and considering the presence of amplitude damping noise affecting the auxiliary, we show that stochastically extractable energy using NPOVMs is no less than that using POVMs. This advantage of using NPOVMs remains also for bit-flip noise. For dephasing noise, the energies by applying POVMs and NPOVMs are the same. We additionally consider the case when a limited set of measurement operators are allowed.","sentences":["We extend the concept of stochastic energy extraction from quantum batteries to the scenario where both positive operator-valued (POV) and physically realizable non-positive operator-valued measurements are applied (NPOVMs) on the auxiliary connected to the battery in presence of noise.","The process involves joint evolution of the battery and the auxiliary for a particular time interval, an interaction of the auxiliary with its environment which induces noise in the auxiliary, and performing a POVM or NPOVM on the auxiliary, and finally the selection of a particular measurement outcome.","Application of POVM on the auxiliary can be realised by attaching an external system to the auxiliary, which is initially in a product state with the rest of the system, and performing a joint projective measurement on the auxiliary and external.","On the other hand, if there are interactions leading to correlations among the auxiliary, environment, and external systems, then performing the projective measurement on the auxiliary-environment-external system can be interpreted as a physically realizable NPOVM operation on the auxiliary.","We however utilize interaction between the auxiliary and the environment to implement NPOVMs on the auxiliary.","We find the expressions of stochastically extractable energy by performing POVMs and NPOVMs on the auxiliary and show that the latter does not depend on the applied noise.","Focusing on a particular model of the governing Hamiltonian of a qubit battery and an auxiliary, and considering the presence of amplitude damping noise affecting the auxiliary, we show that stochastically extractable energy using NPOVMs is no less than that using POVMs.","This advantage of using NPOVMs remains also for bit-flip noise.","For dephasing noise, the energies by applying POVMs and NPOVMs are the same.","We additionally consider the case when a limited set of measurement operators are allowed."],"url":"http://arxiv.org/abs/2404.18745v1","category":"quant-ph"}
{"created":"2024-04-29 14:37:56","title":"A faster algorithm for the Fr\u00e9chet distance in 1D for the imbalanced case","abstract":"The fine-grained complexity of computing the Fr\\'echet distance has been a topic of much recent work, starting with the quadratic SETH-based conditional lower bound by Bringmann from 2014. Subsequent work established largely the same complexity lower bounds for the Fr\\'echet distance in 1D. However, the imbalanced case, which was shown by Bringmann to be tight in dimensions $d\\geq 2$, was still left open. Filling in this gap, we show that a faster algorithm for the Fr\\'echet distance in the imbalanced case is possible: Given two 1-dimensional curves of complexity $n$ and $n^{\\alpha}$ for some $\\alpha \\in (0,1)$, we can compute their Fr\\'echet distance in $O(n^{2\\alpha} \\log^2 n + n \\log n)$ time. This rules out a conditional lower bound of the form $O((nm)^{1-\\epsilon})$ that Bringmann showed for $d \\geq 2$ and any $\\varepsilon>0$ in turn showing a strict separation with the setting $d=1$. At the heart of our approach lies a data structure that stores a 1-dimensional curve $P$ of complexity $n$, and supports queries with a curve $Q$ of complexity~$m$ for the continuous Fr\\'echet distance between $P$ and $Q$. The data structure has size in $\\mathcal{O}(n\\log n)$ and uses query time in $\\mathcal{O}(m^2 \\log^2 n)$. Our proof uses a key lemma that is based on the concept of visiting orders and may be of independent interest. We demonstrate this by substantially simplifying the correctness proof of a clustering algorithm by Driemel, Krivo\\v{s}ija and Sohler from 2015.","sentences":["The fine-grained complexity of computing the Fr\\'echet distance has been a topic of much recent work, starting with the quadratic SETH-based conditional lower bound by Bringmann from 2014.","Subsequent work established largely the same complexity lower bounds for the Fr\\'echet distance in 1D. However, the imbalanced case, which was shown by Bringmann to be tight in dimensions $d\\geq 2$, was still left open.","Filling in this gap, we show that a faster algorithm for the Fr\\'echet distance in the imbalanced case is possible: Given two 1-dimensional curves of complexity $n$ and $n^{\\alpha}$ for some $\\alpha \\in (0,1)$, we can compute their Fr\\'echet distance in $O(n^{2\\alpha} \\log^2 n + n \\log n)$ time.","This rules out a conditional lower bound of the form $O((nm)^{1-\\epsilon})$ that Bringmann showed for $d \\geq 2$ and any $\\varepsilon>0$ in turn showing a strict separation with the setting $d=1$. At the heart of our approach lies a data structure that stores a 1-dimensional curve $P$ of complexity $n$, and supports queries with a curve $Q$ of complexity~$m$ for the continuous Fr\\'echet distance between $P$ and $Q$. The data structure has size in $\\mathcal{O}(n\\log n)$ and uses query time in $\\mathcal{O}(m^2 \\log^2 n)$. Our proof uses a key lemma that is based on the concept of visiting orders and may be of independent interest.","We demonstrate this by substantially simplifying the correctness proof of a clustering algorithm by Driemel, Krivo\\v{s}ija and Sohler from 2015."],"url":"http://arxiv.org/abs/2404.18738v1","category":"cs.CG"}
{"created":"2024-04-29 14:10:13","title":"Risk-Aware Coverage Path Planning for Lunar Micro-Rovers Leveraging Global and Local Environmental Data","abstract":"This paper presents a novel 3D myopic coverage path planning algorithm for lunar micro-rovers that can explore unknown environments with limited sensing and computational capabilities. The algorithm expands upon traditional non-graph path planning methods to accommodate the complexities of lunar terrain, utilizing global data with local topographic features into motion cost calculations. The algorithm also integrates localization and mapping to update the rover's pose and map the environment. The resulting environment map's accuracy is evaluated and tested in a 3D simulator. Outdoor field tests were conducted to validate the algorithm's efficacy in sim-to-real scenarios. The results showed that the algorithm could achieve high coverage with low energy consumption and computational cost, while incrementally exploring the terrain and avoiding obstacles. This study contributes to the advancement of path planning methodologies for space exploration, paving the way for efficient, scalable and autonomous exploration of lunar environments by small rovers.","sentences":["This paper presents a novel 3D myopic coverage path planning algorithm for lunar micro-rovers that can explore unknown environments with limited sensing and computational capabilities.","The algorithm expands upon traditional non-graph path planning methods to accommodate the complexities of lunar terrain, utilizing global data with local topographic features into motion cost calculations.","The algorithm also integrates localization and mapping to update the rover's pose and map the environment.","The resulting environment map's accuracy is evaluated and tested in a 3D simulator.","Outdoor field tests were conducted to validate the algorithm's efficacy in sim-to-real scenarios.","The results showed that the algorithm could achieve high coverage with low energy consumption and computational cost, while incrementally exploring the terrain and avoiding obstacles.","This study contributes to the advancement of path planning methodologies for space exploration, paving the way for efficient, scalable and autonomous exploration of lunar environments by small rovers."],"url":"http://arxiv.org/abs/2404.18721v1","category":"cs.RO"}
{"created":"2024-04-29 14:10:08","title":"Innovative Integration of Visual Foundation Model with a Robotic Arm on a Mobile Platform","abstract":"In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration. This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform. The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties. By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical. This synthesis enables dynamic object segmentation, tracking, and grasping. It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems. Empirical assessments in both simulated and real-world demonstrate the system's capabilities. This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond.","sentences":["In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration.","This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform.","The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties.","By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical.","This synthesis enables dynamic object segmentation, tracking, and grasping.","It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems.","Empirical assessments in both simulated and real-world demonstrate the system's capabilities.","This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond."],"url":"http://arxiv.org/abs/2404.18720v1","category":"cs.RO"}
{"created":"2024-04-29 14:08:11","title":"Rapid Computation of the Plasma Dispersion Function: Rational and Multi-pole Approximation, and Improved Accuracy","abstract":"The plasma dispersion function $Z(s)$ is a fundamental complex special integral function widely used in the field of plasma physics. The simplest and most rapid, yet accurate, approach to calculating it is through rational or equivalent multi-pole expansions. In this work, we summarize the numerical coefficients that are practically useful to the community. Besides the Pade approximation to obtain coefficients, which are accurate for both small and large arguments, we also employ optimization methods to enhance the accuracy of the approximation for the intermediate range. The best coefficients provided here for calculating $Z(s)$ can deliver twelve significant decimal digits. This work serves as a foundational database for the community for further applications.","sentences":["The plasma dispersion function $Z(s)$ is a fundamental complex special integral function widely used in the field of plasma physics.","The simplest and most rapid, yet accurate, approach to calculating it is through rational or equivalent multi-pole expansions.","In this work, we summarize the numerical coefficients that are practically useful to the community.","Besides the Pade approximation to obtain coefficients, which are accurate for both small and large arguments, we also employ optimization methods to enhance the accuracy of the approximation for the intermediate range.","The best coefficients provided here for calculating $Z(s)$ can deliver twelve significant decimal digits.","This work serves as a foundational database for the community for further applications."],"url":"http://arxiv.org/abs/2404.18719v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 14:00:51","title":"Anomalous Spin and Orbital Hall Phenomena in Antiferromagnetic Systems","abstract":"We investigate anomalous spin and orbital Hall phenomena in antiferromagnetic (AF) materials via orbital pumping experiments. Conducting spin and orbital pumping experiments on YIG/Pt/Ir20Mn80 heterostructures, we unexpectedly observe strong spin and orbital anomalous signals in an out-of-plane configuration. We report a sevenfold increase in the signal of the anomalous inverse orbital Hall effect (AIOHE) compared to conventional effects. Our study suggests expanding the Orbital Hall angle ({\\theta}_OH) to a rank 3 tensor, akin to the Spin Hall angle ({\\theta}_SH), to explain AIOHE. This work pioneers converting spin-orbital currents into charge current, advancing the spin-orbitronics domain in AF materials.","sentences":["We investigate anomalous spin and orbital Hall phenomena in antiferromagnetic (AF) materials via orbital pumping experiments.","Conducting spin and orbital pumping experiments on YIG/Pt/Ir20Mn80 heterostructures, we unexpectedly observe strong spin and orbital anomalous signals in an out-of-plane configuration.","We report a sevenfold increase in the signal of the anomalous inverse orbital Hall effect (AIOHE) compared to conventional effects.","Our study suggests expanding the Orbital Hall angle ({\\theta}_OH) to a rank 3 tensor, akin to the Spin Hall angle ({\\theta}_SH), to explain AIOHE.","This work pioneers converting spin-orbital currents into charge current, advancing the spin-orbitronics domain in AF materials."],"url":"http://arxiv.org/abs/2404.18712v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 13:55:42","title":"A geometric approach for stability analysis of delay systems: Applications to network dynamics","abstract":"Investigating the network stability or synchronization dynamics of multi-agent systems with time delays is of significant importance in numerous real-world applications. Such investigations often rely on solving the transcendental characteristic equations (TCEs) obtained from linearization of the considered systems around specific solutions. While stability results based on the TCEs with real-valued coefficients induced by symmetric networks in time-delayed models have been extensively explored in the literature, there remains a notable gap in stability analysis for the TCEs with complexvalued coefficients arising from asymmetric networked dynamics with time delays. To address this challenge comprehensively, we propose a rigorously geometric approach. By identifying and studying the stability crossing curves in the complex plane, we are able to determine the stability region of these systems. This approach is not only suitable for analyzing the stability of models with discrete time delays but also for models with various types of delays, including distributed time delays. Additionally, it can also handle random networks. We demonstrate the efficacy of this approach in designing delayed control strategies for car-following systems, mechanical systems, and deep brain stimulation modeling, where involved are complex-valued TCEs or/and different types of delays. All these therefore highlight the broad applicability of our approach across diverse domains.","sentences":["Investigating the network stability or synchronization dynamics of multi-agent systems with time delays is of significant importance in numerous real-world applications.","Such investigations often rely on solving the transcendental characteristic equations (TCEs) obtained from linearization of the considered systems around specific solutions.","While stability results based on the TCEs with real-valued coefficients induced by symmetric networks in time-delayed models have been extensively explored in the literature, there remains a notable gap in stability analysis for the TCEs with complexvalued coefficients arising from asymmetric networked dynamics with time delays.","To address this challenge comprehensively, we propose a rigorously geometric approach.","By identifying and studying the stability crossing curves in the complex plane, we are able to determine the stability region of these systems.","This approach is not only suitable for analyzing the stability of models with discrete time delays but also for models with various types of delays, including distributed time delays.","Additionally, it can also handle random networks.","We demonstrate the efficacy of this approach in designing delayed control strategies for car-following systems, mechanical systems, and deep brain stimulation modeling, where involved are complex-valued TCEs or/and different types of delays.","All these therefore highlight the broad applicability of our approach across diverse domains."],"url":"http://arxiv.org/abs/2404.18704v1","category":"math.DS"}
{"created":"2024-04-29 13:49:12","title":"Real-fluid Transport Property Computations Based on the Boltzmann-weighted Full-dimensional Potential Model","abstract":"The intermolecular potential plays crucial roles in real-fluid interactions away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy fluid, plasma interactions, etc. We propose a Boltzmann-weighted Full-dimensional (BWF) potential model for real-fluid computations. It includes diverse intermolecular interactions so as to determine the potential well, molecular diameter, dipole moment, polarizability of species without introducing bath gases, allowing more accurate descriptions of potential surfaces with more potential parameters. The anisotropy and temperature dependence of potential parameters are also considered by applying the Boltzmann weighting on all orientations. Through the high-level Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential energy surface datasets are obtained in 432 orientations for each species. Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are derived by training the dataset exceeding 5*106 data, including nonpolar and polar molecules, radicals, long-chain molecules, and ions. These BWF transport properties calculated by the BWF potential have been compared against the Lennard-Jones transport properties as well as experimental viscosity, mass diffusivity, and thermal conductivity coefficients. It shows discrepancies of viscosity coefficients within 1% and 5% for nonpolar and polar molecules, respectively. Furthermore, this potential model is applied to study radicals, long-chain molecules, and ions, for which the experimental data is rarely accessed in high accuracy. It indicates significant prediction improvements of complex interactions between various particles. The new transport properties are also embedded to predict the laminar flame speeds and the flame extinction limits of methane, dimethyl ether, and n-heptane at elevated pressures, confirming its predictivity and effectiveness.","sentences":["The intermolecular potential plays crucial roles in real-fluid interactions away from the ideal-gas equilibrium, such as supercritical fluid, high-enthalpy fluid, plasma interactions, etc.","We propose a Boltzmann-weighted Full-dimensional (BWF) potential model for real-fluid computations.","It includes diverse intermolecular interactions so as to determine the potential well, molecular diameter, dipole moment, polarizability of species without introducing bath gases, allowing more accurate descriptions of potential surfaces with more potential parameters.","The anisotropy and temperature dependence of potential parameters are also considered by applying the Boltzmann weighting on all orientations.","Through the high-level Symmetry-Adapted Perturbation Theory calculations, full-dimensional potential energy surface datasets are obtained in 432 orientations for each species.","Subsequently, the Boltzmann-weighted Full-dimensional potential parameters are derived by training the dataset exceeding 5*106 data, including nonpolar and polar molecules, radicals, long-chain molecules, and ions.","These BWF transport properties calculated by the BWF potential have been compared against the Lennard-Jones transport properties as well as experimental viscosity, mass diffusivity, and thermal conductivity coefficients.","It shows discrepancies of viscosity coefficients within 1% and 5% for nonpolar and polar molecules, respectively.","Furthermore, this potential model is applied to study radicals, long-chain molecules, and ions, for which the experimental data is rarely accessed in high accuracy.","It indicates significant prediction improvements of complex interactions between various particles.","The new transport properties are also embedded to predict the laminar flame speeds and the flame extinction limits of methane, dimethyl ether, and n-heptane at elevated pressures, confirming its predictivity and effectiveness."],"url":"http://arxiv.org/abs/2404.18700v1","category":"physics.app-ph"}
{"created":"2024-04-29 13:46:12","title":"Investigation of shallow water waves near the coast or in lake environments via the KdV-Calogero-Bogoyavlenskii-Schiff equation","abstract":"Shallow water waves phenomena in nature attract the attention of scholars and play an important role in fields such as tsunamis, tidal waves, solitary waves, and hydraulic engineering. Hereby, for the shallow water waves phenomena in various natural environments, we study the KdV-Calogero-Bogoyavlenskii-Schiff (KdV-CBS) equation. Based on the Bell polynomial theory, the B{\\\"a}cklund transformation, Lax pair and infinite conservation laws of the KdV-CBS equation are derived, and it is proved that it is completely integrable in Lax pair sense. Various types of mixed solutions are constructed by using a combination of Homoclinic test method and Mathematica symbolic computations. These findings have important significance for the discipline, offering vital insights into the intricate dynamics of the KdV-CBS equation. We hope that our research results could help the researchers understand the nonlinear complex phenomena of the shallow water waves in oceans, rivers and coastal areas.","sentences":["Shallow water waves phenomena in nature attract the attention of scholars and play an important role in fields such as tsunamis, tidal waves, solitary waves, and hydraulic engineering.","Hereby, for the shallow water waves phenomena in various natural environments, we study the KdV-Calogero-Bogoyavlenskii-Schiff (KdV-CBS) equation.","Based on the Bell polynomial theory, the B{\\\"a}cklund transformation, Lax pair and infinite conservation laws of the KdV-CBS equation are derived, and it is proved that it is completely integrable in Lax pair sense.","Various types of mixed solutions are constructed by using a combination of Homoclinic test method and Mathematica symbolic computations.","These findings have important significance for the discipline, offering vital insights into the intricate dynamics of the KdV-CBS equation.","We hope that our research results could help the researchers understand the nonlinear complex phenomena of the shallow water waves in oceans, rivers and coastal areas."],"url":"http://arxiv.org/abs/2404.18697v1","category":"nlin.SI"}
{"created":"2024-04-29 13:42:15","title":"Natural homotopy of multipointed d-spaces","abstract":"We identify Grandis' directed spaces as a full reflective subcategory of the category of multipointed $d$-spaces. When the multipointed $d$-space realizes a precubical set, its reflection coincides with the standard realization of the precubical set as a directed space. The reflection enables us to extend the construction of the natural system of topological spaces in Baues-Wirsching's sense from directed spaces to multipointed $d$-spaces. In the case of a cellular multipointed $d$-space, there is a discrete version of this natural system which is proved to be bisimilar up to homotopy. We also prove that these constructions are invariant up to homotopy under globular subdivision. These results are the globular analogue of Dubut's results. Finally, we point the incompatibility between the notion of bisimilar natural systems and the q-model structure of multipointed $d$-spaces. This means that either the notion of bisimilar natural systems is too rigid or new model structures should be considered on multipointed $d$-spaces.","sentences":["We identify Grandis' directed spaces as a full reflective subcategory of the category of multipointed $d$-spaces.","When the multipointed $d$-space realizes a precubical set, its reflection coincides with the standard realization of the precubical set as a directed space.","The reflection enables us to extend the construction of the natural system of topological spaces in Baues-Wirsching's sense from directed spaces to multipointed $d$-spaces.","In the case of a cellular multipointed $d$-space, there is a discrete version of this natural system which is proved to be bisimilar up to homotopy.","We also prove that these constructions are invariant up to homotopy under globular subdivision.","These results are the globular analogue of Dubut's results.","Finally, we point the incompatibility between the notion of bisimilar natural systems and the q-model structure of multipointed $d$-spaces.","This means that either the notion of bisimilar natural systems is too rigid or new model structures should be considered on multipointed $d$-spaces."],"url":"http://arxiv.org/abs/2404.18693v1","category":"math.AT"}
{"created":"2024-04-29 13:30:57","title":"FALE: Fairness-Aware ALE Plots for Auditing Bias in Subgroups","abstract":"Fairness is steadily becoming a crucial requirement of Machine Learning (ML) systems. A particularly important notion is subgroup fairness, i.e., fairness in subgroups of individuals that are defined by more than one attributes. Identifying bias in subgroups can become both computationally challenging, as well as problematic with respect to comprehensibility and intuitiveness of the finding to end users. In this work we focus on the latter aspects; we propose an explainability method tailored to identifying potential bias in subgroups and visualizing the findings in a user friendly manner to end users. In particular, we extend the ALE plots explainability method, proposing FALE (Fairness aware Accumulated Local Effects) plots, a method for measuring the change in fairness for an affected population corresponding to different values of a feature (attribute). We envision FALE to function as an efficient, user friendly, comprehensible and reliable first-stage tool for identifying subgroups with potential bias issues.","sentences":["Fairness is steadily becoming a crucial requirement of Machine Learning (ML) systems.","A particularly important notion is subgroup fairness, i.e., fairness in subgroups of individuals that are defined by more than one attributes.","Identifying bias in subgroups can become both computationally challenging, as well as problematic with respect to comprehensibility and intuitiveness of the finding to end users.","In this work we focus on the latter aspects; we propose an explainability method tailored to identifying potential bias in subgroups and visualizing the findings in a user friendly manner to end users.","In particular, we extend the ALE plots explainability method, proposing FALE (Fairness aware Accumulated Local Effects) plots, a method for measuring the change in fairness for an affected population corresponding to different values of a feature (attribute).","We envision FALE to function as an efficient, user friendly, comprehensible and reliable first-stage tool for identifying subgroups with potential bias issues."],"url":"http://arxiv.org/abs/2404.18685v1","category":"cs.LG"}
{"created":"2024-04-29 12:55:22","title":"A semi-analytical $x$-space solution for parton evolution -- Application to non-singlet and singlet DGLAP equation","abstract":"We present a novel semi-analytical method for parton evolution. It is based on constructing a family of analytic functions spanning $x$-space which is closed under the considered evolution equation. Using these functions as a basis, the original integro-differential evolution equation transforms into a system of coupled ordinary differential equations, which can be solved numerically by restriction to a suitably chosen finite subsystem. The evolved distributions are obtained as analytic functions in $x$ with numerically obtained coefficients, providing insight into the analytic behavior of the evolved parton distributions. As a proof-of-principle, we apply our method to the leading order non-singlet and singlet DGLAP equation. Comparing our results to traditional Mellin-space methods, we find good agreement. The method is implemented in the code $\\texttt{POMPOM}$ in $\\texttt{Mathematica}$ as well as in $\\texttt{Python}$.","sentences":["We present a novel semi-analytical method for parton evolution.","It is based on constructing a family of analytic functions spanning $x$-space which is closed under the considered evolution equation.","Using these functions as a basis, the original integro-differential evolution equation transforms into a system of coupled ordinary differential equations, which can be solved numerically by restriction to a suitably chosen finite subsystem.","The evolved distributions are obtained as analytic functions in $x$ with numerically obtained coefficients, providing insight into the analytic behavior of the evolved parton distributions.","As a proof-of-principle, we apply our method to the leading order non-singlet and singlet DGLAP equation.","Comparing our results to traditional Mellin-space methods, we find good agreement.","The method is implemented in the code $\\texttt{POMPOM}$ in $\\texttt{Mathematica}$ as well as in $\\texttt{Python}$."],"url":"http://arxiv.org/abs/2404.18667v1","category":"hep-ph"}
{"created":"2024-04-29 12:49:30","title":"Reading Order Independent Metrics for Information Extraction in Handwritten Documents","abstract":"Information Extraction processes in handwritten documents tend to rely on obtaining an automatic transcription and performing Named Entity Recognition (NER) over such transcription. For this reason, in publicly available datasets, the performance of the systems is usually evaluated with metrics particular to each dataset. Moreover, most of the metrics employed are sensitive to reading order errors. Therefore, they do not reflect the expected final application of the system and introduce biases in more complex documents. In this paper, we propose and publicly release a set of reading order independent metrics tailored to Information Extraction evaluation in handwritten documents. In our experimentation, we perform an in-depth analysis of the behavior of the metrics to recommend what we consider to be the minimal set of metrics to evaluate a task correctly.","sentences":["Information Extraction processes in handwritten documents tend to rely on obtaining an automatic transcription and performing Named Entity Recognition (NER) over such transcription.","For this reason, in publicly available datasets, the performance of the systems is usually evaluated with metrics particular to each dataset.","Moreover, most of the metrics employed are sensitive to reading order errors.","Therefore, they do not reflect the expected final application of the system and introduce biases in more complex documents.","In this paper, we propose and publicly release a set of reading order independent metrics tailored to Information Extraction evaluation in handwritten documents.","In our experimentation, we perform an in-depth analysis of the behavior of the metrics to recommend what we consider to be the minimal set of metrics to evaluate a task correctly."],"url":"http://arxiv.org/abs/2404.18664v1","category":"cs.CV"}
{"created":"2024-04-29 12:48:42","title":"Terrain characterisation for online adaptability of automated sonar processing: Lessons learnt from operationally applying ATR to sidescan sonar in MCM applications","abstract":"The performance of Automated Recognition (ATR) algorithms on side-scan sonar imagery has shown to degrade rapidly when deployed on non benign environments. Complex seafloors and acoustic artefacts constitute distractors in the form of strong textural patterns, creating false detections or preventing detections of true objects. This paper presents two online seafloor characterisation techniques to improve explainability during Autonomous Underwater Vehicles (AUVs) missions. Importantly and as opposed to previous work in the domain, these techniques are not based on a model and require limited input from human operators, making it suitable for real-time onboard processing. Both techniques rely on an unsupervised machine learning approach to extract terrain features which relate to the human understanding of terrain complexity. The first technnique provides a quantitative, application-driven terrain characterisation metric based on the performance of an ATR algorithm. The second method provides a way to incorporate subject matter expertise and enables contextualisation and explainability in support for scenario-dependent subjective terrain characterisation. The terrain complexity matches the expectation of seasoned users making this tool desirable and trustworthy in comparison to traditional unsupervised approaches. We finally detail an application of these techniques to repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy framework Neptune.","sentences":["The performance of Automated Recognition (ATR) algorithms on side-scan sonar imagery has shown to degrade rapidly when deployed on non benign environments.","Complex seafloors and acoustic artefacts constitute distractors in the form of strong textural patterns, creating false detections or preventing detections of true objects.","This paper presents two online seafloor characterisation techniques to improve explainability during Autonomous Underwater Vehicles (AUVs) missions.","Importantly and as opposed to previous work in the domain, these techniques are not based on a model and require limited input from human operators, making it suitable for real-time onboard processing.","Both techniques rely on an unsupervised machine learning approach to extract terrain features which relate to the human understanding of terrain complexity.","The first technnique provides a quantitative, application-driven terrain characterisation metric based on the performance of an ATR algorithm.","The second method provides a way to incorporate subject matter expertise and enables contextualisation and explainability in support for scenario-dependent subjective terrain characterisation.","The terrain complexity matches the expectation of seasoned users making this tool desirable and trustworthy in comparison to traditional unsupervised approaches.","We finally detail an application of these techniques to repair a Mine Countermeasures (MCM) mission carried with SeeByte autonomy framework Neptune."],"url":"http://arxiv.org/abs/2404.18663v1","category":"cs.CV"}
{"created":"2024-04-29 12:35:03","title":"A Scoping Review on Simulation-based Design Optimization in Marine Engineering: Trends, Best Practices, and Gaps","abstract":"This scoping review assesses the current use of simulation-based design optimization (SBDO) in marine engineering, focusing on identifying research trends, methodologies, and application areas. Analyzing 277 studies from Scopus and Web of Science, the review finds that SBDO is predominantly applied to optimizing marine vessel hulls, including both surface and underwater types, and extends to key components like bows, sterns, propellers, and fins. It also covers marine structures and renewable energy systems. A notable trend is the preference for deterministic single-objective optimization methods, indicating potential growth areas in multi-objective and stochastic approaches. The review points out the necessity of integrating more comprehensive multidisciplinary optimization methods to address the complex challenges in marine environments. Despite the extensive application of SBDO in marine engineering, there remains a need for enhancing the methodologies' efficiency and robustness. This review offers a critical overview of SBDO's role in marine engineering and highlights opportunities for future research to advance the field.","sentences":["This scoping review assesses the current use of simulation-based design optimization (SBDO) in marine engineering, focusing on identifying research trends, methodologies, and application areas.","Analyzing 277 studies from Scopus and Web of Science, the review finds that SBDO is predominantly applied to optimizing marine vessel hulls, including both surface and underwater types, and extends to key components like bows, sterns, propellers, and fins.","It also covers marine structures and renewable energy systems.","A notable trend is the preference for deterministic single-objective optimization methods, indicating potential growth areas in multi-objective and stochastic approaches.","The review points out the necessity of integrating more comprehensive multidisciplinary optimization methods to address the complex challenges in marine environments.","Despite the extensive application of SBDO in marine engineering, there remains a need for enhancing the methodologies' efficiency and robustness.","This review offers a critical overview of SBDO's role in marine engineering and highlights opportunities for future research to advance the field."],"url":"http://arxiv.org/abs/2404.18654v1","category":"math.OC"}
{"created":"2024-04-29 12:32:28","title":"Enhancing RSS-Based Visible Light Positioning by Optimal Calibrating the LED Tilt and Gain","abstract":"This paper presents an optimal calibration scheme and a weighted least squares (LS) localization algorithm for received signal strength (RSS) based visible light positioning (VLP) systems, focusing on the often overlooked impact of light emitting diode (LED) tilt. By optimally calibrating LED tilt and gain, we significantly enhance VLP localization accuracy. Our algorithm outperforms both machine learning Gaussian processes (GPs) and traditional multilateration techniques. Against GPs, it achieves improvements of 58% and 74% in the 50th and 99th percentiles, respectively. When compared to multilateration, it reduces the 50th percentile error from 7.4 cm to 3.2 cm and the 99th percentile error from 25.7 cm to 11 cm. We introduce a low-complexity estimator for tilt and gain that meets the Cramer-Rao lower bound (CRLB) for the mean squared error (MSE), emphasizing its precision and efficiency. Further, we elaborate on optimal calibration measurement placement and refine the observation model to include residual calibration errors, thereby improving localization performance. The weighted LS algorithm's effectiveness is validated through simulations and real-world data, consistently outperforming GPs and multilateration, across various training set sizes and reducing outlier errors. Our findings underscore the critical role of LED tilt calibration in advancing VLP system accuracy and contribute to a more precise model for indoor positioning technologies.","sentences":["This paper presents an optimal calibration scheme and a weighted least squares (LS) localization algorithm for received signal strength (RSS) based visible light positioning (VLP) systems, focusing on the often overlooked impact of light emitting diode (LED) tilt.","By optimally calibrating LED tilt and gain, we significantly enhance VLP localization accuracy.","Our algorithm outperforms both machine learning Gaussian processes (GPs) and traditional multilateration techniques.","Against GPs, it achieves improvements of 58% and 74% in the 50th and 99th percentiles, respectively.","When compared to multilateration, it reduces the 50th percentile error from 7.4 cm to 3.2 cm and the 99th percentile error from 25.7 cm to 11 cm.","We introduce a low-complexity estimator for tilt and gain that meets the Cramer-Rao lower bound (CRLB) for the mean squared error (MSE), emphasizing its precision and efficiency.","Further, we elaborate on optimal calibration measurement placement and refine the observation model to include residual calibration errors, thereby improving localization performance.","The weighted LS algorithm's effectiveness is validated through simulations and real-world data, consistently outperforming GPs and multilateration, across various training set sizes and reducing outlier errors.","Our findings underscore the critical role of LED tilt calibration in advancing VLP system accuracy and contribute to a more precise model for indoor positioning technologies."],"url":"http://arxiv.org/abs/2404.18650v1","category":"eess.SP"}
{"created":"2024-04-29 12:28:47","title":"Flatband makes the wave go round","abstract":"Persistent oscillations are a hallmark of non-ergodic time evolution. While time-crystalline behavior results from, e.g., many-body localization, here we show that ever-revolving solitary waves emerge in flatband Heisenberg quantum spin systems.","sentences":["Persistent oscillations are a hallmark of non-ergodic time evolution.","While time-crystalline behavior results from, e.g., many-body localization, here we show that ever-revolving solitary waves emerge in flatband Heisenberg quantum spin systems."],"url":"http://arxiv.org/abs/2404.18646v1","category":"cond-mat.str-el"}
{"created":"2024-04-29 12:19:49","title":"Computing Scattering Cross Sections For Spherically Symmetric Potentials","abstract":"This paper introduces students and instructors to the use of Scilab for calculating phase shifts from phenomenological potentials in nuclear, atomic, and molecular scattering, beginning with a Riccati-type nonlinear differential equation derived from the time-independent Schr$\\ddot{\\text{o}}$dinger equation. For spherically symmetric potentials, this equation can be easily solved using Xcos, a Scilab toolbox. \\cite{PRC}\\cite{bobby} Scilab is open source software that is used for numerical computations and can be freely downloaded for Windows, Linux and Mac OS.\\cite{scilab} Xcos is a Scilab toolbox dedicated to the modeling and simulation of hybrid dynamic systems \\cite{rachna}. Xcos provides users with the ability to model complex dynamical systems using a block diagram editor (GUI based), offering a step-by-step procedure for learning to solve complex differential equations interactively.","sentences":["This paper introduces students and instructors to the use of Scilab for calculating phase shifts from phenomenological potentials in nuclear, atomic, and molecular scattering, beginning with a Riccati-type nonlinear differential equation derived from the time-independent Schr$\\ddot{\\text{o}}$dinger equation.","For spherically symmetric potentials, this equation can be easily solved using Xcos, a Scilab toolbox.","\\cite{PRC}\\cite{bobby} Scilab is open source software that is used for numerical computations and can be freely downloaded for Windows, Linux and Mac OS.\\cite{scilab} Xcos is a Scilab toolbox dedicated to the modeling and simulation of hybrid dynamic systems \\cite{rachna}.","Xcos provides users with the ability to model complex dynamical systems using a block diagram editor (GUI based), offering a step-by-step procedure for learning to solve complex differential equations interactively."],"url":"http://arxiv.org/abs/2404.18643v1","category":"physics.ed-ph"}
{"created":"2024-04-29 12:18:02","title":"Efficient preconditioners for coupled Stokes-Darcy problems","abstract":"Coupled systems of free flow and porous media arise in a variety of technical and environmental applications. For laminar flow regimes, such systems are described by the Stokes equations in the free-flow region and Darcy's law in the porous medium. An appropriate set of coupling conditions is needed on the fluid-porous interface. Discretisations of the Stokes-Darcy problems yield large, sparse, ill-conditioned, and, depending on the interface conditions, non-symmetric linear systems. Therefore, robust and efficient preconditioners are needed to accelerate convergence of the applied Krylov method. In this work, we develop and investigate block diagonal, block triangular and constraint preconditioners for the coupled Stokes-Darcy problems. We apply two classical sets of coupling conditions considering the Beavers-Joseph and the Beavers-Joseph-Saffman condition for the tangential velocity. For the Beavers-Joseph interface condition, the resulting system is non-symmetric, therefore GMRES method is used. Spectral and field-of-values bounds independent of the grid width are derived for the exact versions of the preconditioners. Furthermore, we develop efficient inexact versions of the preconditioners. We demonstrate the effectiveness and robustness of the proposed preconditioners in numerical experiments.","sentences":["Coupled systems of free flow and porous media arise in a variety of technical and environmental applications.","For laminar flow regimes, such systems are described by the Stokes equations in the free-flow region and Darcy's law in the porous medium.","An appropriate set of coupling conditions is needed on the fluid-porous interface.","Discretisations of the Stokes-Darcy problems yield large, sparse, ill-conditioned, and, depending on the interface conditions, non-symmetric linear systems.","Therefore, robust and efficient preconditioners are needed to accelerate convergence of the applied Krylov method.","In this work, we develop and investigate block diagonal, block triangular and constraint preconditioners for the coupled Stokes-Darcy problems.","We apply two classical sets of coupling conditions considering the Beavers-Joseph and the Beavers-Joseph-Saffman condition for the tangential velocity.","For the Beavers-Joseph interface condition, the resulting system is non-symmetric, therefore GMRES method is used.","Spectral and field-of-values bounds independent of the grid width are derived for the exact versions of the preconditioners.","Furthermore, we develop efficient inexact versions of the preconditioners.","We demonstrate the effectiveness and robustness of the proposed preconditioners in numerical experiments."],"url":"http://arxiv.org/abs/2404.18639v1","category":"math.NA"}
{"created":"2024-04-29 12:15:06","title":"Semi-device independent characterization of multiphoton indistinguishability","abstract":"Multiphoton indistinguishability is a central resource for quantum enhancement in sensing and computation. Developing and certifying large scale photonic devices requires reliable and accurate characterization of this resource, preferably using methods that are robust against experimental errors. Here, we propose a set of methods for the characterization of multiphoton indistinguishability, based on measurements of bunching and photon number variance. Our methods are robust in a semi-device independent way, in the sense of being effective even when the interferometers are incorrectly dialled. We demonstrate the effectiveness of this approach using an advanced photonic platform comprising a quantum-dot single-photon source and a universal fully-programmable integrated photonic processor. Our results show the practical usefulness of our methods, providing robust certification tools that can be scaled up to larger systems.","sentences":["Multiphoton indistinguishability is a central resource for quantum enhancement in sensing and computation.","Developing and certifying large scale photonic devices requires reliable and accurate characterization of this resource, preferably using methods that are robust against experimental errors.","Here, we propose a set of methods for the characterization of multiphoton indistinguishability, based on measurements of bunching and photon number variance.","Our methods are robust in a semi-device independent way, in the sense of being effective even when the interferometers are incorrectly dialled.","We demonstrate the effectiveness of this approach using an advanced photonic platform comprising a quantum-dot single-photon source and a universal fully-programmable integrated photonic processor.","Our results show the practical usefulness of our methods, providing robust certification tools that can be scaled up to larger systems."],"url":"http://arxiv.org/abs/2404.18636v1","category":"quant-ph"}
{"created":"2024-04-29 12:11:40","title":"Mass Spectra of $\u039e$ and $\u03a9$ Baryons using hypercentral Constituent Quark Model","abstract":"Hadron spectroscopy is an important tool towards the study of internal quark dynamics in a composite system. The present article focuses on the study of resonance spectra of strange baryons with S=-2, -3. The non-relativistic approach utilizes screened potential as hypercentral one to obtain masses. The spin-dependent part for all possible hyperfine states has been incorporated.","sentences":["Hadron spectroscopy is an important tool towards the study of internal quark dynamics in a composite system.","The present article focuses on the study of resonance spectra of strange baryons with S=-2, -3.","The non-relativistic approach utilizes screened potential as hypercentral one to obtain masses.","The spin-dependent part for all possible hyperfine states has been incorporated."],"url":"http://arxiv.org/abs/2404.18632v1","category":"hep-ph"}
{"created":"2024-04-29 12:03:27","title":"Differentiable Voronoi Diagrams for Simulation of Cell-Based Mechanical Systems","abstract":"Navigating topological transitions in cellular mechanical systems is a significant challenge for existing simulation methods. While abstract models lack predictive capabilities at the cellular level, explicit network representations struggle with topology changes, and per-cell representations are computationally too demanding for large-scale simulations. To address these challenges, we propose a novel cell-centered approach based on differentiable Voronoi diagrams. Representing each cell with a Voronoi site, our method defines shape and topology of the interface network implicitly. In this way, we substantially reduce the number of problem variables, eliminate the need for explicit contact handling, and ensure continuous geometry changes during topological transitions. Closed-form derivatives of network positions facilitate simulation with Newton-type methods for a wide range of per-cell energies. Finally, we extend our differentiable Voronoi diagrams to enable coupling with arbitrary rigid and deformable boundaries. We apply our approach to a diverse set of examples, highlighting splitting and merging of cells as well as neighborhood changes. We illustrate applications to inverse problems by matching soap foam simulations to real-world images. Comparative analysis with explicit cell models reveals that our method achieves qualitatively comparable results at significantly faster computation times.","sentences":["Navigating topological transitions in cellular mechanical systems is a significant challenge for existing simulation methods.","While abstract models lack predictive capabilities at the cellular level, explicit network representations struggle with topology changes, and per-cell representations are computationally too demanding for large-scale simulations.","To address these challenges, we propose a novel cell-centered approach based on differentiable Voronoi diagrams.","Representing each cell with a Voronoi site, our method defines shape and topology of the interface network implicitly.","In this way, we substantially reduce the number of problem variables, eliminate the need for explicit contact handling, and ensure continuous geometry changes during topological transitions.","Closed-form derivatives of network positions facilitate simulation with Newton-type methods for a wide range of per-cell energies.","Finally, we extend our differentiable Voronoi diagrams to enable coupling with arbitrary rigid and deformable boundaries.","We apply our approach to a diverse set of examples, highlighting splitting and merging of cells as well as neighborhood changes.","We illustrate applications to inverse problems by matching soap foam simulations to real-world images.","Comparative analysis with explicit cell models reveals that our method achieves qualitatively comparable results at significantly faster computation times."],"url":"http://arxiv.org/abs/2404.18629v1","category":"cs.GR"}
{"created":"2024-04-29 11:51:50","title":"The Bohr-type inequalities for holomorphic functions with lacunary series in complex Banach space","abstract":"In this paper, we study the Bohr inequality with lacunary series to the single valued (resp. vector-valued) holomorphic function defined in unit ball of finite dimensional Banach sequence space. Also, we extend the Bohr inequality with an alternating series to the higher-dimensional space.","sentences":["In this paper, we study the Bohr inequality with lacunary series to the single valued (resp.","vector-valued) holomorphic function defined in unit ball of finite dimensional Banach sequence space.","Also, we extend the Bohr inequality with an alternating series to the higher-dimensional space."],"url":"http://arxiv.org/abs/2404.18623v1","category":"math.CV"}
{"created":"2024-04-29 11:31:35","title":"P53 Orchestrates Cancer Metabolism: Unveiling Strategies to Reverse the Warburg Effect","abstract":"Cancer cells exhibit significant alterations in their metabolism, characterised by a reduction in oxidative phosphorylation (OXPHOS) and an increased reliance on glycolysis, even in the presence of oxygen. This metabolic shift, known as the Warburg effect, is pivotal in fuelling cancer's uncontrolled growth, invasion, and therapeutic resistance. While dysregulation of many genes contributes to this metabolic shift, the tumour suppressor gene p53 emerges as a master player. Yet, the molecular mechanisms remain elusive. This study introduces a comprehensive mathematical model, integrating essential p53 targets, offering insights into how p53 orchestrates its targets to redirect cancer metabolism towards an OXPHOS-dominant state. Simulation outcomes align closely with experimental data comparing glucose metabolism in colon cancer cells with wild-type and mutated p53. Additionally, our findings reveal the dynamic capability of elevated p53 activation to fully reverse the Warburg effect, highlighting the significance of its activity levels not just in triggering apoptosis (programmed cell death) post-chemotherapy but also in modifying the metabolic pathways implicated in treatment resistance. In scenarios of p53 mutations, our analysis suggests targeting glycolysis-instigating signalling pathways as an alternative strategy, whereas targeting solely synthesis of cytochrome c oxidase 2 (SCO2) does support mitochondrial respiration but may not effectively suppress the glycolysis pathway, potentially boosting the energy production and cancer cell viability.","sentences":["Cancer cells exhibit significant alterations in their metabolism, characterised by a reduction in oxidative phosphorylation (OXPHOS) and an increased reliance on glycolysis, even in the presence of oxygen.","This metabolic shift, known as the Warburg effect, is pivotal in fuelling cancer's uncontrolled growth, invasion, and therapeutic resistance.","While dysregulation of many genes contributes to this metabolic shift, the tumour suppressor gene p53 emerges as a master player.","Yet, the molecular mechanisms remain elusive.","This study introduces a comprehensive mathematical model, integrating essential p53 targets, offering insights into how p53 orchestrates its targets to redirect cancer metabolism towards an OXPHOS-dominant state.","Simulation outcomes align closely with experimental data comparing glucose metabolism in colon cancer cells with wild-type and mutated p53.","Additionally, our findings reveal the dynamic capability of elevated p53 activation to fully reverse the Warburg effect, highlighting the significance of its activity levels not just in triggering apoptosis (programmed cell death) post-chemotherapy but also in modifying the metabolic pathways implicated in treatment resistance.","In scenarios of p53 mutations, our analysis suggests targeting glycolysis-instigating signalling pathways as an alternative strategy, whereas targeting solely synthesis of cytochrome c oxidase 2 (SCO2) does support mitochondrial respiration but may not effectively suppress the glycolysis pathway, potentially boosting the energy production and cancer cell viability."],"url":"http://arxiv.org/abs/2404.18613v1","category":"q-bio.MN"}
{"created":"2024-04-29 11:29:46","title":"Statistical formulation of Onsager-Machlup variational principle","abstract":"Onsager's variational principle (OVP) provides us with a systematic way to derive dynamical equations for various soft matter and active matter. By reformulating the Onsager-Machlup variational principle (OMVP), which is a time-global principle, we propose a new method to incorporate thermal fluctuations. To demonstrate the utility of the statistical formulation of OMVP (SOMVP), we obtain the diffusion constant of a Brownian particle embedded in a viscous fluid only by maximizing the modified Onsager-Machlup integral for the surrounding fluid. We also apply our formulation to a Brownian particle in a steady shear flow, which is a typical example of a non-equilibrium system. Possible extensions of our formulation to internally driven active systems are discussed.","sentences":["Onsager's variational principle (OVP) provides us with a systematic way to derive dynamical equations for various soft matter and active matter.","By reformulating the Onsager-Machlup variational principle (OMVP), which is a time-global principle, we propose a new method to incorporate thermal fluctuations.","To demonstrate the utility of the statistical formulation of OMVP (SOMVP), we obtain the diffusion constant of a Brownian particle embedded in a viscous fluid only by maximizing the modified Onsager-Machlup integral for the surrounding fluid.","We also apply our formulation to a Brownian particle in a steady shear flow, which is a typical example of a non-equilibrium system.","Possible extensions of our formulation to internally driven active systems are discussed."],"url":"http://arxiv.org/abs/2404.18611v1","category":"cond-mat.soft"}
{"created":"2024-04-29 11:28:25","title":"Differentiable Geodesic Distance for Intrinsic Minimization on Triangle Meshes","abstract":"Computing intrinsic distances on discrete surfaces is at the heart of many minimization problems in geometry processing and beyond. Solving these problems is extremely challenging as it demands the computation of on-surface distances along with their derivatives. We present a novel approach for intrinsic minimization of distance-based objectives defined on triangle meshes. Using a variational formulation of shortest-path geodesics, we compute first and second-order distance derivatives based on the implicit function theorem, thus opening the door to efficient Newton-type minimization solvers. We demonstrate our differentiable geodesic distance framework on a wide range of examples, including geodesic networks and membranes on surfaces of arbitrary genus, two-way coupling between hosting surface and embedded system, differentiable geodesic Voronoi diagrams, and efficient computation of Karcher means on complex shapes. Our analysis shows that second-order descent methods based on our differentiable geodesics outperform existing first-order and quasi-Newton methods by large margins.","sentences":["Computing intrinsic distances on discrete surfaces is at the heart of many minimization problems in geometry processing and beyond.","Solving these problems is extremely challenging as it demands the computation of on-surface distances along with their derivatives.","We present a novel approach for intrinsic minimization of distance-based objectives defined on triangle meshes.","Using a variational formulation of shortest-path geodesics, we compute first and second-order distance derivatives based on the implicit function theorem, thus opening the door to efficient Newton-type minimization solvers.","We demonstrate our differentiable geodesic distance framework on a wide range of examples, including geodesic networks and membranes on surfaces of arbitrary genus, two-way coupling between hosting surface and embedded system, differentiable geodesic Voronoi diagrams, and efficient computation of Karcher means on complex shapes.","Our analysis shows that second-order descent methods based on our differentiable geodesics outperform existing first-order and quasi-Newton methods by large margins."],"url":"http://arxiv.org/abs/2404.18610v1","category":"cs.GR"}
{"created":"2024-04-29 11:21:38","title":"Femtoscopy can tell whether $Z_c(3900)$ and $Z_{cs}(3985)$ are resonances or virtual states","abstract":"There have been extended and heated discussions on the nature of the two exotic states, $Z_c(3900)$ and $Z_{cs}(3985)$, particularly whether they are resonances or virtual states. We demonstrate for the first time that the femtoscopic technique can be used to unambiguously distinguish between such two scenarios. More concretely, we show that the $D^0D^{*-}$/$D^0D_s^{*-}$ correlation functions are significantly different in the high-momentum region, especially in small collision systems of the order of 1 fm, as produced in pp collisions at the LHC, which can unambiguously tell whether $Z_c(3900)$/$Z_{cs}(3985)$ is a resonant or a virtual state. For the $Z_{cs}(3985)$, the $D^0D_s^{*-}$ results at zero momentum are significantly different in the two scenarios. We hope all these discoveries can stimulate further experimental studies and help clarify the nature of the many exotic states discovered.","sentences":["There have been extended and heated discussions on the nature of the two exotic states, $Z_c(3900)$ and $Z_{cs}(3985)$, particularly whether they are resonances or virtual states.","We demonstrate for the first time that the femtoscopic technique can be used to unambiguously distinguish between such two scenarios.","More concretely, we show that the $D^0D^{*-}$/$D^0D_s^{*-}$ correlation functions are significantly different in the high-momentum region, especially in small collision systems of the order of 1 fm, as produced in pp collisions at the LHC, which can unambiguously tell whether $Z_c(3900)$/$Z_{cs}(3985)$ is a resonant or a virtual state.","For the $Z_{cs}(3985)$, the $D^0D_s^{*-}$ results at zero momentum are significantly different in the two scenarios.","We hope all these discoveries can stimulate further experimental studies and help clarify the nature of the many exotic states discovered."],"url":"http://arxiv.org/abs/2404.18607v1","category":"hep-ph"}
{"created":"2024-04-29 11:14:53","title":"Network-theory based modeling of avalanche dynamics in percolative tunnelling networks","abstract":"Brain-like self-assembled networks can infer and analyze information out of unorganized noisy signals with minimal power consumption. These networks are characterized by spatiotemporal avalanches and their crackling behavior, and their physical models are expected to predict and understand their computational capabilities. Here, we use a network theory-based approach to provide a physical model for percolative tunnelling networks, found in Ag-hBN system, consisting of nodes (atomic clusters) of Ag intercalated in the hBN van der Waals layers. By modeling a single edge plasticity through constitutive electrochemical filament formation, and annihilation through Joule heating, we identify independent parameters that determine the network connectivity. We construct a phase diagram and show that a small region of the parameter space contains signals which are long-range temporally correlated, and only a subset of them contains crackling avalanche dynamics. Physical systems spontaneously selforganize to this region for possibly maximizing the efficiency of information transfer.","sentences":["Brain-like self-assembled networks can infer and analyze information out of unorganized noisy signals with minimal power consumption.","These networks are characterized by spatiotemporal avalanches and their crackling behavior, and their physical models are expected to predict and understand their computational capabilities.","Here, we use a network theory-based approach to provide a physical model for percolative tunnelling networks, found in Ag-hBN system, consisting of nodes (atomic clusters) of Ag intercalated in the hBN van der Waals layers.","By modeling a single edge plasticity through constitutive electrochemical filament formation, and annihilation through Joule heating, we identify independent parameters that determine the network connectivity.","We construct a phase diagram and show that a small region of the parameter space contains signals which are long-range temporally correlated, and only a subset of them contains crackling avalanche dynamics.","Physical systems spontaneously selforganize to this region for possibly maximizing the efficiency of information transfer."],"url":"http://arxiv.org/abs/2404.18600v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-29 11:04:43","title":"Atomicity in Distributed Quantum Computing","abstract":"Atomicity is a ubiquitous assumption in distributed computing, under which actions are indivisible and appear sequential. In classical computing, this assumption has several theoretical and practical guarantees. In quantum computing, although atomicity is still commonly assumed, it has not been seriously studied, and a rigorous basis for it is missing. Classical results on atomicity do not directly carry over to distributed quantum computing, due to new challenges caused by quantum entanglement and the measurement problem from the underlying quantum mechanics.   In this paper, we initiate the study of atomicity in distributed quantum computing. A formal model of (non-atomic) distributed quantum system is established. Based on the Dijkstra-Lamport condition, the system dynamics and observable dynamics of a distributed quantum system are defined, which correspond to the quantum state of and classically observable events in the system, respectively. Within this framework, we prove that local actions can be regarded as if they were atomic, up to the observable dynamics of the system.","sentences":["Atomicity is a ubiquitous assumption in distributed computing, under which actions are indivisible and appear sequential.","In classical computing, this assumption has several theoretical and practical guarantees.","In quantum computing, although atomicity is still commonly assumed, it has not been seriously studied, and a rigorous basis for it is missing.","Classical results on atomicity do not directly carry over to distributed quantum computing, due to new challenges caused by quantum entanglement and the measurement problem from the underlying quantum mechanics.   ","In this paper, we initiate the study of atomicity in distributed quantum computing.","A formal model of (non-atomic) distributed quantum system is established.","Based on the Dijkstra-Lamport condition, the system dynamics and observable dynamics of a distributed quantum system are defined, which correspond to the quantum state of and classically observable events in the system, respectively.","Within this framework, we prove that local actions can be regarded as if they were atomic, up to the observable dynamics of the system."],"url":"http://arxiv.org/abs/2404.18592v1","category":"quant-ph"}
{"created":"2024-04-29 11:04:43","title":"A hybrid prognosis approach for robust lifetime control of commercial wind turbines","abstract":"Dynamic fluctuations in the wind field to which a wind turbine (WT) is exposed to are responsible for fatigue loads on its components. To reduce structural loads in WTs, advanced control schemes have been proposed. In recent years, prognosis-based lifetime control of WTs has become increasingly important. In this approach, the prognostic controller gains are adapted based on the stateof-health (SOH) of the WT component to achieve the desired lifetime. However, stochastic wind dynamics complicates estimation of the SOH of a WT. More recently, robust controllers have been combined with real-time damage evaluation models to meet prognosis objectives. Most rely on model-based online load cycle counting algorithms to determine fatigue damage, with analytical models providing the degradation estimate. However, most use load measurements that are either unreliable or unavailable in commercial WTs, limiting their practicality. In this contribution, a hybrid prognosis scheme combining data-driven load prediction and model-based damage estimation models for robust lifetime control of commercial WTs is proposed. A data-driven support vector machine (SVM) regression model is trained using loading data obtained from dynamic simulations using a {\\mu}-synthesis robust disturbance accommodating controller (RDAC). The regression model uses available WT measurements to predict tower load. Based on this prediction, an online rain-flow counting (RFC) damage evaluation model estimates the damage level and lifetime of the tower. The RDAC controller gains are dynamically adapted to achieve a predefined damage limit and lifetime. The proposed approach is evaluated on a 5 MW reference WT and its performance is compared with a model-based prognosis scheme using ideal WT tower measurement. Results demonstrate the efficacy of the proposed approach to control the fatigue lifetime in WT components.","sentences":["Dynamic fluctuations in the wind field to which a wind turbine (WT) is exposed to are responsible for fatigue loads on its components.","To reduce structural loads in WTs, advanced control schemes have been proposed.","In recent years, prognosis-based lifetime control of WTs has become increasingly important.","In this approach, the prognostic controller gains are adapted based on the stateof-health (SOH) of the WT component to achieve the desired lifetime.","However, stochastic wind dynamics complicates estimation of the SOH of a WT.","More recently, robust controllers have been combined with real-time damage evaluation models to meet prognosis objectives.","Most rely on model-based online load cycle counting algorithms to determine fatigue damage, with analytical models providing the degradation estimate.","However, most use load measurements that are either unreliable or unavailable in commercial WTs, limiting their practicality.","In this contribution, a hybrid prognosis scheme combining data-driven load prediction and model-based damage estimation models for robust lifetime control of commercial WTs is proposed.","A data-driven support vector machine (SVM) regression model is trained using loading data obtained from dynamic simulations using a {\\mu}-synthesis robust disturbance accommodating controller (RDAC).","The regression model uses available WT measurements to predict tower load.","Based on this prediction, an online rain-flow counting (RFC) damage evaluation model estimates the damage level and lifetime of the tower.","The RDAC controller gains are dynamically adapted to achieve a predefined damage limit and lifetime.","The proposed approach is evaluated on a 5 MW reference WT and its performance is compared with a model-based prognosis scheme using ideal WT tower measurement.","Results demonstrate the efficacy of the proposed approach to control the fatigue lifetime in WT components."],"url":"http://arxiv.org/abs/2404.18593v1","category":"eess.SY"}
{"created":"2024-04-29 11:01:38","title":"A statistical analysis of the first stages of freezing and melting of Lennard-Jones particles: Number and size distributions of transient nuclei","abstract":"The freezing/melting transition is at the heart of many natural and industrial processes. In the classical picture, the transition proceeds via the nucleation of the new phase, which has to overcome a barrier associated to the free energy cost of the growing nucleus. The total nucleation rate is also influenced by a kinetic factor which somehow depends on the number of attempts to create a nucleus, that translates into a significant density of proto-nuclei in the system. These transient tiny nuclei are not accessible to experiments, but they can be observed in molecular simulations, and their number and size distributions can be acquired and analysed. The number distributions are carefully characterized as a function of the system size, showing limited finite size effects. It is also shown that the proto-nuclei do exist even in the stable phase, in agreement with the fact that the volume contribution to their free energy is negligible in the first stages of nucleation. Moreover, the number and size distributions vary continuously between the stable and the metastable phases, in particular when crossing the coexistence temperature. The size distributions associated to \\emph{any} nucleus and to the \\emph{largest} one have also been calculated, and their relation recently established for bubbles in a liquid [J. Puibasset, J. Chem. Phys. 157, 191102 (2022)] has been shown to apply here. This is an important relation for free energy barrier calculations with biased molecular simulations.","sentences":["The freezing/melting transition is at the heart of many natural and industrial processes.","In the classical picture, the transition proceeds via the nucleation of the new phase, which has to overcome a barrier associated to the free energy cost of the growing nucleus.","The total nucleation rate is also influenced by a kinetic factor which somehow depends on the number of attempts to create a nucleus, that translates into a significant density of proto-nuclei in the system.","These transient tiny nuclei are not accessible to experiments, but they can be observed in molecular simulations, and their number and size distributions can be acquired and analysed.","The number distributions are carefully characterized as a function of the system size, showing limited finite size effects.","It is also shown that the proto-nuclei do exist even in the stable phase, in agreement with the fact that the volume contribution to their free energy is negligible in the first stages of nucleation.","Moreover, the number and size distributions vary continuously between the stable and the metastable phases, in particular when crossing the coexistence temperature.","The size distributions associated to \\emph{any} nucleus and to the \\emph{largest} one have also been calculated, and their relation recently established for bubbles in a liquid [J. Puibasset, J. Chem.","Phys. 157, 191102 (2022)] has been shown to apply here.","This is an important relation for free energy barrier calculations with biased molecular simulations."],"url":"http://arxiv.org/abs/2404.18590v1","category":"cond-mat.soft"}
{"created":"2024-04-29 10:55:08","title":"FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering","abstract":"Table Question Answering (TQA) aims at composing an answer to a question based on tabular data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a significant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experimental analysis reveals that none of the examined state-of-the-art TQA systems consistently excels in these three aspects. Our benchmark is a crucial instrument for monitoring the behavior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.","sentences":["Table Question Answering (TQA) aims at composing an answer to a question based on tabular data.","While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a significant obstacle to the development of robust TQA systems.","In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems.","They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities.","To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English.","Our extensive experimental analysis reveals that none of the examined state-of-the-art TQA systems consistently excels in these three aspects.","Our benchmark is a crucial instrument for monitoring the behavior of TQA systems and paves the way for the development of robust TQA systems.","We release our benchmark publicly."],"url":"http://arxiv.org/abs/2404.18585v1","category":"cs.CL"}
{"created":"2024-04-29 10:41:30","title":"Data-Driven Dynamics Modeling of Miniature Robotic Blimps Using Neural ODEs With Parameter Auto-Tuning","abstract":"Miniature robotic blimps, as one type of lighter-than-air aerial vehicles, have attracted increasing attention in the science and engineering community for their enhanced safety, extended endurance, and quieter operation compared to quadrotors. Accurately modeling the dynamics of these robotic blimps poses a significant challenge due to the complex aerodynamics stemming from their large lifting bodies. Traditional first-principle models have difficulty obtaining accurate aerodynamic parameters and often overlook high-order nonlinearities, thus coming to its limit in modeling the motion dynamics of miniature robotic blimps. To tackle this challenge, this letter proposes the Auto-tuning Blimp-oriented Neural Ordinary Differential Equation method (ABNODE), a data-driven approach that integrates first-principle and neural network modeling. Spiraling motion experiments of robotic blimps are conducted, comparing the ABNODE with first-principle and other data-driven benchmark models, the results of which demonstrate the effectiveness of the proposed method.","sentences":["Miniature robotic blimps, as one type of lighter-than-air aerial vehicles, have attracted increasing attention in the science and engineering community for their enhanced safety, extended endurance, and quieter operation compared to quadrotors.","Accurately modeling the dynamics of these robotic blimps poses a significant challenge due to the complex aerodynamics stemming from their large lifting bodies.","Traditional first-principle models have difficulty obtaining accurate aerodynamic parameters and often overlook high-order nonlinearities, thus coming to its limit in modeling the motion dynamics of miniature robotic blimps.","To tackle this challenge, this letter proposes the Auto-tuning Blimp-oriented Neural Ordinary Differential Equation method (ABNODE), a data-driven approach that integrates first-principle and neural network modeling.","Spiraling motion experiments of robotic blimps are conducted, comparing the ABNODE with first-principle and other data-driven benchmark models, the results of which demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2404.18580v1","category":"cs.RO"}
{"created":"2024-04-29 10:40:31","title":"Scheme for braiding Majorana zero modes in vortices using an STT-matrix","abstract":"Recently conducted experiments on two-dimensional topological superconductors have revealed various indications of Majorana zero modes (MZMs). However, progress in the manipulation of MZM braiding has been limited, impeding the realization of topological quantum computing. In this study, we propose a potential braiding scheme based on a spintronic device matrix. This scheme involves utilizing a matrix composed of spin-transfer torque devices (STT-matrix) alongside a two-dimensional topological superconductor material. By programming the ON/OFF states of the spintronic devices within the STT-matrix, it becomes possible to manipulate vortices hosting MZMs in the two-dimensional topological superconductor. To further investigate this concept, we construct a time-dependent Ginzburg-Landau model and perform numerical simulations to analyze vortex-driving dynamics, MZM braiding processes, and MZM fusion phenomena. Our findings demonstrate that this system exhibits high versatility and flexibility in manipulating vortices. With advancements in spintronic device technology, our proposed scheme offers a feasible and practical method for operating MZMs within vortices present in topological superconductors.","sentences":["Recently conducted experiments on two-dimensional topological superconductors have revealed various indications of Majorana zero modes (MZMs).","However, progress in the manipulation of MZM braiding has been limited, impeding the realization of topological quantum computing.","In this study, we propose a potential braiding scheme based on a spintronic device matrix.","This scheme involves utilizing a matrix composed of spin-transfer torque devices (STT-matrix) alongside a two-dimensional topological superconductor material.","By programming the ON/OFF states of the spintronic devices within the STT-matrix, it becomes possible to manipulate vortices hosting MZMs in the two-dimensional topological superconductor.","To further investigate this concept, we construct a time-dependent Ginzburg-Landau model and perform numerical simulations to analyze vortex-driving dynamics, MZM braiding processes, and MZM fusion phenomena.","Our findings demonstrate that this system exhibits high versatility and flexibility in manipulating vortices.","With advancements in spintronic device technology, our proposed scheme offers a feasible and practical method for operating MZMs within vortices present in topological superconductors."],"url":"http://arxiv.org/abs/2404.18578v2","category":"cond-mat.supr-con"}
{"created":"2024-04-29 10:32:59","title":"The catalog of Hvar Observatory solar observations","abstract":"We compile the catalog of Hvar Observatory solar observations in the time period corresponding to regular digitally stored chromospheric and photospheric observations 2010-2019. We make basic characterisation of observed phenomena and compare them to catalogs which are based on full disc solar images. We compile a catalog of observed ARs consisting of 1100 entries, where each AR is classified according to McIntosh and Mt Wilson classifications. We find that HVAR observations are biased towards more frequently observing more complex ARs and observing them in longer time periods, likely related to the small FOV not encompassing the whole solar disc. In H$\\alpha$ observations we catalog conspicuous filaments/prominences and flares. We characterise filaments according to their location, chirality (if possible) and eruptive signatures. Analysis of the eruptive filaments reveals a slight bias in HVAR catalog towards observation of partial eruptions, possibly related to the observers tendency to observe filament which already showed some activity. In the flare catalog we focus on their observed eruptive signatures (loops or ribbons) and their shape. In addition, we associate them to GOES soft X-ray flares to determine their corresponding class. We find that HVAR observations seem biased towards more frequently observing stronger flares and observing them in longer time periods. We demonstrate the feasibility of the catalog on a case study of the flare detected on 2 August 2011 in HVAR H$\\alpha$ observations and related Sun-to-Earth phenomena. Through flare-CME-ICME association we demonstrate the agreement of remote and in situ properties. The data used for this study, as well as the catalog, are made publicly available.","sentences":["We compile the catalog of Hvar Observatory solar observations in the time period corresponding to regular digitally stored chromospheric and photospheric observations 2010-2019.","We make basic characterisation of observed phenomena and compare them to catalogs which are based on full disc solar images.","We compile a catalog of observed ARs consisting of 1100 entries, where each AR is classified according to McIntosh and Mt Wilson classifications.","We find that HVAR observations are biased towards more frequently observing more complex ARs and observing them in longer time periods, likely related to the small FOV not encompassing the whole solar disc.","In H$\\alpha$ observations we catalog conspicuous filaments/prominences and flares.","We characterise filaments according to their location, chirality (if possible) and eruptive signatures.","Analysis of the eruptive filaments reveals a slight bias in HVAR catalog towards observation of partial eruptions, possibly related to the observers tendency to observe filament which already showed some activity.","In the flare catalog we focus on their observed eruptive signatures (loops or ribbons) and their shape.","In addition, we associate them to GOES soft X-ray flares to determine their corresponding class.","We find that HVAR observations seem biased towards more frequently observing stronger flares and observing them in longer time periods.","We demonstrate the feasibility of the catalog on a case study of the flare detected on 2 August 2011 in HVAR H$\\alpha$ observations and related Sun-to-Earth phenomena.","Through flare-CME-ICME association we demonstrate the agreement of remote and in situ properties.","The data used for this study, as well as the catalog, are made publicly available."],"url":"http://arxiv.org/abs/2404.18576v1","category":"astro-ph.SR"}
{"created":"2024-04-29 10:28:28","title":"Predicting Safety Misbehaviours in Autonomous Driving Systems using Uncertainty Quantification","abstract":"The automated real-time recognition of unexpected situations plays a crucial role in the safety of autonomous vehicles, especially in unsupported and unpredictable scenarios. This paper evaluates different Bayesian uncertainty quantification methods from the deep learning domain for the anticipatory testing of safety-critical misbehaviours during system-level simulation-based testing. Specifically, we compute uncertainty scores as the vehicle executes, following the intuition that high uncertainty scores are indicative of unsupported runtime conditions that can be used to distinguish safe from failure-inducing driving behaviors. In our study, we conducted an evaluation of the effectiveness and computational overhead associated with two Bayesian uncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for misbehaviour avoidance. Overall, for three benchmarks from the Udacity simulator comprising both out-of-distribution and unsafe conditions introduced via mutation testing, both methods successfully detected a high number of out-of-bounds episodes providing early warnings several seconds in advance, outperforming two state-of-the-art misbehaviour prediction methods based on autoencoders and attention maps in terms of effectiveness and efficiency. Notably, Deep Ensembles detected most misbehaviours without any false alarms and did so even when employing a relatively small number of models, making them computationally feasible for real-time detection. Our findings suggest that incorporating uncertainty quantification methods is a viable approach for building fail-safe mechanisms in deep neural network-based autonomous vehicles.","sentences":["The automated real-time recognition of unexpected situations plays a crucial role in the safety of autonomous vehicles, especially in unsupported and unpredictable scenarios.","This paper evaluates different Bayesian uncertainty quantification methods from the deep learning domain for the anticipatory testing of safety-critical misbehaviours during system-level simulation-based testing.","Specifically, we compute uncertainty scores as the vehicle executes, following the intuition that high uncertainty scores are indicative of unsupported runtime conditions that can be used to distinguish safe from failure-inducing driving behaviors.","In our study, we conducted an evaluation of the effectiveness and computational overhead associated with two Bayesian uncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for misbehaviour avoidance.","Overall, for three benchmarks from the Udacity simulator comprising both out-of-distribution and unsafe conditions introduced via mutation testing, both methods successfully detected a high number of out-of-bounds episodes providing early warnings several seconds in advance, outperforming two state-of-the-art misbehaviour prediction methods based on autoencoders and attention maps in terms of effectiveness and efficiency.","Notably, Deep Ensembles detected most misbehaviours without any false alarms and did so even when employing a relatively small number of models, making them computationally feasible for real-time detection.","Our findings suggest that incorporating uncertainty quantification methods is a viable approach for building fail-safe mechanisms in deep neural network-based autonomous vehicles."],"url":"http://arxiv.org/abs/2404.18573v1","category":"cs.LG"}
{"created":"2024-04-29 10:28:14","title":"Learning Governing Equations of Unobserved States in Dynamical Systems","abstract":"Data driven modelling and scientific machine learning have been responsible for significant advances in determining suitable models to describe data. Within dynamical systems, neural ordinary differential equations (ODEs), where the system equations are set to be governed by a neural network, have become a popular tool for this challenge in recent years. However, less emphasis has been placed on systems that are only partially-observed. In this work, we employ a hybrid neural ODE structure, where the system equations are governed by a combination of a neural network and domain-specific knowledge, together with symbolic regression (SR), to learn governing equations of partially-observed dynamical systems. We test this approach on two case studies: A 3-dimensional model of the Lotka-Volterra system and a 5-dimensional model of the Lorenz system. We demonstrate that the method is capable of successfully learning the true underlying governing equations of unobserved states within these systems, with robustness to measurement noise.","sentences":["Data driven modelling and scientific machine learning have been responsible for significant advances in determining suitable models to describe data.","Within dynamical systems, neural ordinary differential equations (ODEs), where the system equations are set to be governed by a neural network, have become a popular tool for this challenge in recent years.","However, less emphasis has been placed on systems that are only partially-observed.","In this work, we employ a hybrid neural ODE structure, where the system equations are governed by a combination of a neural network and domain-specific knowledge, together with symbolic regression (SR), to learn governing equations of partially-observed dynamical systems.","We test this approach on two case studies: A 3-dimensional model of the Lotka-Volterra system and a 5-dimensional model of the Lorenz system.","We demonstrate that the method is capable of successfully learning the true underlying governing equations of unobserved states within these systems, with robustness to measurement noise."],"url":"http://arxiv.org/abs/2404.18572v1","category":"cs.LG"}
{"created":"2024-04-29 10:19:41","title":"Multigrid method for nonlinear eigenvalue problems based on Newton iteration","abstract":"In this paper, a novel multigrid method based on Newton iteration is proposed to solve nonlinear eigenvalue problems. Instead of handling the eigenvalue $\\lambda$ and eigenfunction $u$ separately, we treat the eigenpair $(\\lambda, u)$ as one element in a product space $\\mathbb R \\times H_0^1(\\Omega)$. Then in the presented multigrid method, only one discrete linear boundary value problem needs to be solved for each level of the multigrid sequence. Because we avoid solving large-scale nonlinear eigenvalue problems directly, the overall efficiency is significantly improved. The optimal error estimate and linear computational complexity can be derived simultaneously. In addition, we also provide an improved multigrid method coupled with a mixing scheme to further guarantee the convergence and stability of the iteration scheme. More importantly, we prove convergence for the residuals after each iteration step. For nonlinear eigenvalue problems, such theoretical analysis is missing from the existing literatures on the mixing iteration scheme.","sentences":["In this paper, a novel multigrid method based on Newton iteration is proposed to solve nonlinear eigenvalue problems.","Instead of handling the eigenvalue $\\lambda$ and eigenfunction $u$ separately, we treat the eigenpair $(\\lambda, u)$ as one element in a product space $\\mathbb R \\times H_0^1(\\Omega)$.","Then in the presented multigrid method, only one discrete linear boundary value problem needs to be solved for each level of the multigrid sequence.","Because we avoid solving large-scale nonlinear eigenvalue problems directly, the overall efficiency is significantly improved.","The optimal error estimate and linear computational complexity can be derived simultaneously.","In addition, we also provide an improved multigrid method coupled with a mixing scheme to further guarantee the convergence and stability of the iteration scheme.","More importantly, we prove convergence for the residuals after each iteration step.","For nonlinear eigenvalue problems, such theoretical analysis is missing from the existing literatures on the mixing iteration scheme."],"url":"http://arxiv.org/abs/2404.18568v1","category":"math.NA"}
{"created":"2024-04-29 10:09:16","title":"Time Reversal for Near-Field Communications on Multi-chip Wireless Networks","abstract":"Wireless Network-on-Chip (WNoC) has been proposed as a low-latency, versatile, and broadcast-capable complement to current interconnects in the quest for satisfying the ever-increasing communications needs of modern computing systems. However, to realize the promise of WNoC, multiple wireless links operating at several tens of Gb/s need to be created within a computing package. Unfortunately, the highly integrated and enclosed nature of such computing packages incurs significant Co-Channel Interference (CCI) and Inter-Symbol Interference (ISI), not only preventing the deployment of multiple spatial channels, but also severely limiting the symbol rate of each individual channel. In this work, Time Reversal (TR) is proposed as a means to compensate the channel impairments and enable multiple concurrent high-speed links at the chip scale. We offer evidence, via full-wave simulations at 140 GHz, that TR can increase the symbol rate by an order of magnitude and allow the deployment of multiple concurrent links towards achieving aggregate speeds in excess of 100 Gb/s. Finally, the challenges relative to the realization of TR at the chip scale are analyzed from the implementation, protocol support, and architectural perspectives.","sentences":["Wireless Network-on-Chip (WNoC) has been proposed as a low-latency, versatile, and broadcast-capable complement to current interconnects in the quest for satisfying the ever-increasing communications needs of modern computing systems.","However, to realize the promise of WNoC, multiple wireless links operating at several tens of Gb/s need to be created within a computing package.","Unfortunately, the highly integrated and enclosed nature of such computing packages incurs significant Co-Channel Interference (CCI) and Inter-Symbol Interference (ISI), not only preventing the deployment of multiple spatial channels, but also severely limiting the symbol rate of each individual channel.","In this work, Time Reversal (TR) is proposed as a means to compensate the channel impairments and enable multiple concurrent high-speed links at the chip scale.","We offer evidence, via full-wave simulations at 140 GHz, that TR can increase the symbol rate by an order of magnitude and allow the deployment of multiple concurrent links towards achieving aggregate speeds in excess of 100 Gb/s. Finally, the challenges relative to the realization of TR at the chip scale are analyzed from the implementation, protocol support, and architectural perspectives."],"url":"http://arxiv.org/abs/2404.18562v2","category":"cs.AR"}
{"created":"2024-04-29 10:07:05","title":"Social Optima of Linear Forward-Backward Stochastic System","abstract":"A linear quadratic (LQ) stochastic optimization system involving large population, which is driven by forward-backward stochastic differential equation (FBSDE), is investigated in this paper. Agents cooperate with each other to minimize the so-called social objective, which is rather different from mean field (MF) game. Employing forward-backward person-by-person optimality principle, we derive an auxiliary LQ control problem by decentralized information. A decentralized strategy is obtained by virtue of an MF-type forward-backward stochastic differential equation consistency condition. Applying Riccati equation decoupling method, we solve the consistency condition system. We also verify the asymptotic social optimality in this framework.","sentences":["A linear quadratic (LQ) stochastic optimization system involving large population, which is driven by forward-backward stochastic differential equation (FBSDE), is investigated in this paper.","Agents cooperate with each other to minimize the so-called social objective, which is rather different from mean field (MF) game.","Employing forward-backward person-by-person optimality principle, we derive an auxiliary LQ control problem by decentralized information.","A decentralized strategy is obtained by virtue of an MF-type forward-backward stochastic differential equation consistency condition.","Applying Riccati equation decoupling method, we solve the consistency condition system.","We also verify the asymptotic social optimality in this framework."],"url":"http://arxiv.org/abs/2404.18561v1","category":"math.OC"}
{"created":"2024-04-29 10:06:55","title":"Non-convex Pose Graph Optimization in SLAM via Proximal Linearized Riemannian ADMM","abstract":"Pose graph optimization (PGO) is a well-known technique for solving the pose-based simultaneous localization and mapping (SLAM) problem. In this paper, we represent the rotation and translation by a unit quaternion and a three-dimensional vector, and propose a new PGO model based on the von Mises-Fisher distribution. The constraints derived from the unit quaternions are spherical manifolds, and the projection onto the constraints can be calculated by normalization. Then a proximal linearized Riemannian alternating direction method of multipliers (PieADMM) is developed to solve the proposed model, which not only has low memory requirements, but also can update the poses in parallel. Furthermore, we establish the iteration complexity of $O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of our model. The efficiency of our proposed algorithm is demonstrated by numerical experiments on two synthetic and four 3D SLAM benchmark datasets.","sentences":["Pose graph optimization (PGO) is a well-known technique for solving the pose-based simultaneous localization and mapping (SLAM) problem.","In this paper, we represent the rotation and translation by a unit quaternion and a three-dimensional vector, and propose a new PGO model based on the von Mises-Fisher distribution.","The constraints derived from the unit quaternions are spherical manifolds, and the projection onto the constraints can be calculated by normalization.","Then a proximal linearized Riemannian alternating direction method of multipliers (PieADMM) is developed to solve the proposed model, which not only has low memory requirements, but also can update the poses in parallel.","Furthermore, we establish the iteration complexity of $O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of our model.","The efficiency of our proposed algorithm is demonstrated by numerical experiments on two synthetic and four 3D SLAM benchmark datasets."],"url":"http://arxiv.org/abs/2404.18560v1","category":"math.OC"}
{"created":"2024-04-29 09:35:04","title":"Equivariant quantizations of the positive nilradical and covariant differential calculi","abstract":"Consider a decomposition $\\mathfrak{n} = \\mathfrak{n}_1 \\oplus \\cdots \\oplus \\mathfrak{n}_r$ of the positive nilradical of a complex semisimple Lie algebra of rank $r$, where each $\\mathfrak{n}_k$ is a module under an appropriate Levi factor. We show that this can be quantized as a finite-dimensional subspace $\\mathfrak{n}^q_k = \\mathfrak{n}^q_1 \\oplus \\cdots \\oplus \\mathfrak{n}^q_r$ of the positive part of the quantized enveloping algebra, where each $\\mathfrak{n}^q_k$ is a module under the left adjoint action of a quantized Levi factor. Furthermore, we show that $\\mathbb{C} \\oplus \\mathfrak{n}^q$ is a left coideal, with the possible exception of components corresponding to some exceptional Lie algebras. Finally we use these quantizations to construct covariant first-order differential calculi on quantum flag manifolds, compatible in a certain sense with the decomposition above, which coincide with those introduced by Heckenberger-Kolb in the irreducible case.","sentences":["Consider a decomposition $\\mathfrak{n} = \\mathfrak{n}_1 \\oplus \\cdots \\oplus \\mathfrak{n}_r$ of the positive nilradical of a complex semisimple Lie algebra of rank $r$, where each $\\mathfrak{n}_k$ is a module under an appropriate Levi factor.","We show that this can be quantized as a finite-dimensional subspace $\\mathfrak{n}^q_k = \\mathfrak{n}^q_1 \\oplus \\cdots \\oplus \\mathfrak{n}^q_r$ of the positive part of the quantized enveloping algebra, where each $\\mathfrak{n}^q_k$ is a module under the left adjoint action of a quantized Levi factor.","Furthermore, we show that $\\mathbb{C} \\oplus \\mathfrak{n}^q$ is a left coideal, with the possible exception of components corresponding to some exceptional Lie algebras.","Finally we use these quantizations to construct covariant first-order differential calculi on quantum flag manifolds, compatible in a certain sense with the decomposition above, which coincide with those introduced by Heckenberger-Kolb in the irreducible case."],"url":"http://arxiv.org/abs/2404.18544v1","category":"math.QA"}
{"created":"2024-04-29 09:28:45","title":"Tunable coupling of a quantum phononic resonator to a transmon qubit with flip-chip architecture","abstract":"A hybrid system with tunable coupling between phonons and qubits shows great potential for advancing quantum information processing. In this work, we demonstrate strong and tunable coupling between a surface acoustic wave (SAW) resonator and a transmon qubit based on galvanic-contact flip-chip technique. The coupling strength varies from $2\\pi\\times$7.0 MHz to -$2\\pi\\times$20.6 MHz, which is extracted from different vacuum Rabi oscillation frequencies. The phonon-induced ac Stark shift of the qubit at different coupling strengths is also shown. Our approach offers a good experimental platform for exploring quantum acoustics and hybrid systems.","sentences":["A hybrid system with tunable coupling between phonons and qubits shows great potential for advancing quantum information processing.","In this work, we demonstrate strong and tunable coupling between a surface acoustic wave (SAW) resonator and a transmon qubit based on galvanic-contact flip-chip technique.","The coupling strength varies from $2\\pi\\times$7.0 MHz to -$2\\pi\\times$20.6 MHz, which is extracted from different vacuum Rabi oscillation frequencies.","The phonon-induced ac Stark shift of the qubit at different coupling strengths is also shown.","Our approach offers a good experimental platform for exploring quantum acoustics and hybrid systems."],"url":"http://arxiv.org/abs/2404.18540v1","category":"quant-ph"}
{"created":"2024-04-29 09:12:04","title":"Optomechanically Induced Transparency on Exceptional Surfaces","abstract":"Exceptional points (EPs) are singularities in non-Hermitian systems, where the system transmission spectrum varies significantly at the phase transition point. Here, we propose a practical scheme to study the changes of the optomechanically induced transparency (OMIT) spectrum on the exceptional surface (ES), which is formed by designing the structure of the waveguide in a non-Hermitian cavity optomechanical system. By comparing the transmission spectra of the system at different normal points, EPs on the same or different ESs, and exceptional derived points, we find that the peak-valley conversion of the system transmission spectra is obtained at the phase transition point and the arbitrary manipulation of the system transmission spectrum can be realized by moving the system on the same or different ESs. Furthermore, the phenomena of conversion and enhancement of the fast-slow light in the system transmission spectra have also been discovered in our researches. Different from the isolated EP, our proposal can discuss the system properties at different EPs, can find a richer transmission spectrum, and can provide more convenient options for experimental implementation, which paves the way for studying the nature of non-Hermitian systems in a higher dimension.","sentences":["Exceptional points (EPs) are singularities in non-Hermitian systems, where the system transmission spectrum varies significantly at the phase transition point.","Here, we propose a practical scheme to study the changes of the optomechanically induced transparency (OMIT) spectrum on the exceptional surface (ES), which is formed by designing the structure of the waveguide in a non-Hermitian cavity optomechanical system.","By comparing the transmission spectra of the system at different normal points, EPs on the same or different ESs, and exceptional derived points, we find that the peak-valley conversion of the system transmission spectra is obtained at the phase transition point and the arbitrary manipulation of the system transmission spectrum can be realized by moving the system on the same or different ESs.","Furthermore, the phenomena of conversion and enhancement of the fast-slow light in the system transmission spectra have also been discovered in our researches.","Different from the isolated EP, our proposal can discuss the system properties at different EPs, can find a richer transmission spectrum, and can provide more convenient options for experimental implementation, which paves the way for studying the nature of non-Hermitian systems in a higher dimension."],"url":"http://arxiv.org/abs/2404.18526v1","category":"quant-ph"}
{"created":"2024-04-29 09:11:41","title":"Enabling Efficient and Flexible Interpretability of Data-driven Anomaly Detection in Industrial Processes with AcME-AD","abstract":"While Machine Learning has become crucial for Industry 4.0, its opaque nature hinders trust and impedes the transformation of valuable insights into actionable decision, a challenge exacerbated in the evolving Industry 5.0 with its human-centric focus. This paper addresses this need by testing the applicability of AcME-AD in industrial settings. This recently developed framework facilitates fast and user-friendly explanations for anomaly detection. AcME-AD is model-agnostic, offering flexibility, and prioritizes real-time efficiency. Thus, it seems suitable for seamless integration with industrial Decision Support Systems. We present the first industrial application of AcME-AD, showcasing its effectiveness through experiments. These tests demonstrate AcME-AD's potential as a valuable tool for explainable AD and feature-based root cause analysis within industrial environments, paving the way for trustworthy and actionable insights in the age of Industry 5.0.","sentences":["While Machine Learning has become crucial for Industry 4.0, its opaque nature hinders trust and impedes the transformation of valuable insights into actionable decision, a challenge exacerbated in the evolving Industry 5.0 with its human-centric focus.","This paper addresses this need by testing the applicability of AcME-AD in industrial settings.","This recently developed framework facilitates fast and user-friendly explanations for anomaly detection.","AcME-AD is model-agnostic, offering flexibility, and prioritizes real-time efficiency.","Thus, it seems suitable for seamless integration with industrial Decision Support Systems.","We present the first industrial application of AcME-AD, showcasing its effectiveness through experiments.","These tests demonstrate AcME-AD's potential as a valuable tool for explainable AD and feature-based root cause analysis within industrial environments, paving the way for trustworthy and actionable insights in the age of Industry 5.0."],"url":"http://arxiv.org/abs/2404.18525v1","category":"cs.LG"}
{"created":"2024-04-29 09:07:44","title":"Quantum Backbone Networks for Hybrid Quantum Dataframe Transmission","abstract":"To realize a global quantum Internet, there is a need for communication between quantum subnetworks. To accomplish this task, there have been multiple design proposals for a quantum backbone network and quantum subnetworks. In this work, we elaborate on the design that uses entanglement and quantum teleportation to build the quantum backbone between packetized quantum networks. We design a network interface to interconnect packetized quantum networks with entanglement-based quantum backbone networks and, moreover, design a scheme to accomplish data transmission over this hybrid quantum network model. We analyze the use of various implementations of the backbone network, focusing our study on backbone networks that use satellite links to continuously distribute entanglement resources. For feasibility, we analyze various system parameters via simulation to benchmark the performance of the overall network.","sentences":["To realize a global quantum Internet, there is a need for communication between quantum subnetworks.","To accomplish this task, there have been multiple design proposals for a quantum backbone network and quantum subnetworks.","In this work, we elaborate on the design that uses entanglement and quantum teleportation to build the quantum backbone between packetized quantum networks.","We design a network interface to interconnect packetized quantum networks with entanglement-based quantum backbone networks and, moreover, design a scheme to accomplish data transmission over this hybrid quantum network model.","We analyze the use of various implementations of the backbone network, focusing our study on backbone networks that use satellite links to continuously distribute entanglement resources.","For feasibility, we analyze various system parameters via simulation to benchmark the performance of the overall network."],"url":"http://arxiv.org/abs/2404.18521v1","category":"quant-ph"}
{"created":"2024-04-29 09:00:50","title":"An Agile Formal Specification Language Design Based on K Framework","abstract":"Formal Methods (FMs) are currently essential for verifying the safety and reliability of software systems. However, the specification writing in formal methods tends to be complex and challenging to learn, requiring familiarity with various intricate formal specification languages and verification technologies. In response to the increasing complexity of software frameworks, existing specification writing methods fall short in meeting agility requirements. To address this, this paper introduces an Agile Formal Specification Language (ASL). The ASL is defined based on the K Framework and YAML Ain't Markup Language (YAML). The design of ASL incorporates agile design principles, making the writing of formal specifications simpler, more efficient, and scalable. Additionally, a specification translation algorithm is developed, capable of converting ASL into K formal specification language that can be executed for verification. Experimental evaluations demonstrate that the proposed method significantly reduces the code size needed for specification writing, enhancing agility in formal specification writing.","sentences":["Formal Methods (FMs) are currently essential for verifying the safety and reliability of software systems.","However, the specification writing in formal methods tends to be complex and challenging to learn, requiring familiarity with various intricate formal specification languages and verification technologies.","In response to the increasing complexity of software frameworks, existing specification writing methods fall short in meeting agility requirements.","To address this, this paper introduces an Agile Formal Specification Language (ASL).","The ASL is defined based on the K Framework and YAML Ain't Markup Language (YAML).","The design of ASL incorporates agile design principles, making the writing of formal specifications simpler, more efficient, and scalable.","Additionally, a specification translation algorithm is developed, capable of converting ASL into K formal specification language that can be executed for verification.","Experimental evaluations demonstrate that the proposed method significantly reduces the code size needed for specification writing, enhancing agility in formal specification writing."],"url":"http://arxiv.org/abs/2404.18515v1","category":"cs.SE"}
{"created":"2024-04-29 08:57:26","title":"Floquet Amorphous Topological Orders in a Rydberg Glass","abstract":"We study the Floquet amorphous topological orders in experimentally accessible one-dimensional array of randomly pointed Rydberg atoms with periodic driving. The filling factor in the chain is tunable by applying a microwave field. We give a complete characterization of the topological properties from both the single-particle and many-body aspect. The periodic driving results in richer topological phases. At the single-particle level, we calculate the real space winding numbers and polarization, confirming robust amorphous topological phases with 0-type and $\\pi$-type edge modes. We show a structural disorder induced topological phase transition acompanied with localization transition in the nonequilibrium system. Furthermore, in the many-body case we find the existence of amorphous topological orders of the hardcore bosons half-filled the chain, in contrast described by the topological entanglement entropy and the string order. Possible detection methods are also addressed.","sentences":["We study the Floquet amorphous topological orders in experimentally accessible one-dimensional array of randomly pointed Rydberg atoms with periodic driving.","The filling factor in the chain is tunable by applying a microwave field.","We give a complete characterization of the topological properties from both the single-particle and many-body aspect.","The periodic driving results in richer topological phases.","At the single-particle level, we calculate the real space winding numbers and polarization, confirming robust amorphous topological phases with 0-type and $\\pi$-type edge modes.","We show a structural disorder induced topological phase transition acompanied with localization transition in the nonequilibrium system.","Furthermore, in the many-body case we find the existence of amorphous topological orders of the hardcore bosons half-filled the chain, in contrast described by the topological entanglement entropy and the string order.","Possible detection methods are also addressed."],"url":"http://arxiv.org/abs/2404.18512v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 08:46:43","title":"Multisensor Data Fusion for Automatized Insect Monitoring (KInsecta)","abstract":"Insect populations are declining globally, making systematic monitoring essential for conservation. Most classical methods involve death traps and counter insect conservation. This paper presents a multisensor approach that uses AI-based data fusion for insect classification. The system is designed as low-cost setup and consists of a camera module and an optical wing beat sensor as well as environmental sensors to measure temperature, irradiance or daytime as prior information. The system has been tested in the laboratory and in the field. First tests on a small very unbalanced data set with 7 species show promising results for species classification. The multisensor system will support biodiversity and agriculture studies.","sentences":["Insect populations are declining globally, making systematic monitoring essential for conservation.","Most classical methods involve death traps and counter insect conservation.","This paper presents a multisensor approach that uses AI-based data fusion for insect classification.","The system is designed as low-cost setup and consists of a camera module and an optical wing beat sensor as well as environmental sensors to measure temperature, irradiance or daytime as prior information.","The system has been tested in the laboratory and in the field.","First tests on a small very unbalanced data set with 7 species show promising results for species classification.","The multisensor system will support biodiversity and agriculture studies."],"url":"http://arxiv.org/abs/2404.18504v1","category":"cs.LG"}
{"created":"2024-04-29 08:23:18","title":"A Simple Example of Pathological Foliations in Skew-Product Diffeomorphisms","abstract":"Inspired by examples of Katok and Milnor \\cite{Milnor1997}, we construct a simple example of skew-product volume preserving diffeomorphism where the center foliation is pathological in the sense that, there is a full measure set whose intersection with any center leaf contains at most one point.","sentences":["Inspired by examples of Katok and Milnor \\cite{Milnor1997}, we construct a simple example of skew-product volume preserving diffeomorphism where the center foliation is pathological in the sense that, there is a full measure set whose intersection with any center leaf contains at most one point."],"url":"http://arxiv.org/abs/2404.18495v1","category":"math.DS"}
{"created":"2024-04-29 08:16:34","title":"Emergent Non-Abelian Thouless Pumping Induced by the Quasiperiodic Disorder","abstract":"We investigate the non-Abelian Thouless pumping in a disorder tunable Lieb chain with degenerate flat bands. The results reveal that quasiperiodic disorder will cause a topological phase transition from the trivial (without non-Abelian Thouless pumping) to the non-trivial (with non-Abelian Thouless pumping) phase. The mechanism behind is that the monopole originally outside the topological region can be driven into the topological region due to the introduction of quasiperiodic disorder. Moreover, since the corresponding monopole will turn into a nodal line to spread beyond the boundaries of the topological region, the system with large disorder strength will result in the disappearance of non-Abelian Thouless pumping. Furthermore, we numerically simulate the Thouless pumping of non-Abelian systems, and the evolution results of center of mass' displacement are consistent with the Chern number. Finally, we discuss the localization properties of the system and find that, similar to [PRL 130, 206401(2023)], the inverse Anderson transition does not occur in the system with the increase of quasiperiodic strength, while the system still maintains the coexistence of localized and extended states.","sentences":["We investigate the non-Abelian Thouless pumping in a disorder tunable Lieb chain with degenerate flat bands.","The results reveal that quasiperiodic disorder will cause a topological phase transition from the trivial (without non-Abelian Thouless pumping) to the non-trivial (with non-Abelian Thouless pumping) phase.","The mechanism behind is that the monopole originally outside the topological region can be driven into the topological region due to the introduction of quasiperiodic disorder.","Moreover, since the corresponding monopole will turn into a nodal line to spread beyond the boundaries of the topological region, the system with large disorder strength will result in the disappearance of non-Abelian Thouless pumping.","Furthermore, we numerically simulate the Thouless pumping of non-Abelian systems, and the evolution results of center of mass' displacement are consistent with the Chern number.","Finally, we discuss the localization properties of the system and find that, similar to [PRL 130, 206401(2023)], the inverse Anderson transition does not occur in the system with the increase of quasiperiodic strength, while the system still maintains the coexistence of localized and extended states."],"url":"http://arxiv.org/abs/2404.18491v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-29 08:01:40","title":"Emergent dynamics of the inertial Kuramoto model with frustration on a locally coupled graph","abstract":"We study the synchronized behavior of the inertial Kuramoto oscillators with frustration effect under a symmetric and connected network. Due to the lack of second-order gradient flow structure and singularity of second-order derivative of diameter, we shift to construct convex combinations of oscillators and related new energy functions that can control the phase and frequency diameters. Under sufficient frameworks on initial data and system parameters, we derive first-order dissipative differential inequalities of constructed energy functions. This eventually gives rise to the emergence of frequency synchronization exponentially fast.","sentences":["We study the synchronized behavior of the inertial Kuramoto oscillators with frustration effect under a symmetric and connected network.","Due to the lack of second-order gradient flow structure and singularity of second-order derivative of diameter, we shift to construct convex combinations of oscillators and related new energy functions that can control the phase and frequency diameters.","Under sufficient frameworks on initial data and system parameters, we derive first-order dissipative differential inequalities of constructed energy functions.","This eventually gives rise to the emergence of frequency synchronization exponentially fast."],"url":"http://arxiv.org/abs/2404.18488v1","category":"math.DS"}
{"created":"2024-04-29 07:41:29","title":"Asymptotic stability of composite waves of viscous shock and rarefaction for relaxed compressible Navier-Stokes equations","abstract":"The time asymptotic stability for one-dimensional relaxed compressible Navier-Stokes equations is studied. We show that the composite waves of viscous shock and rarefaction are asymptotically nonlinear stable with both small wave strength and small initial perturbations. Moreover, as the relaxation parameter goes to zero, the solutions of relaxed system are shown to converge globally in time to that of classical system. The methods are based on relative entropy, the a-contraction with shifts theory and basic energy estimates.","sentences":["The time asymptotic stability for one-dimensional relaxed compressible Navier-Stokes equations is studied.","We show that the composite waves of viscous shock and rarefaction are asymptotically nonlinear stable with both small wave strength and small initial perturbations.","Moreover, as the relaxation parameter goes to zero, the solutions of relaxed system are shown to converge globally in time to that of classical system.","The methods are based on relative entropy, the a-contraction with shifts theory and basic energy estimates."],"url":"http://arxiv.org/abs/2404.18480v1","category":"math.AP"}
{"created":"2024-04-29 07:33:06","title":"ChatGPT as an inventor: Eliciting the strengths and weaknesses of current large language models against humans in engineering design","abstract":"This study compares the design practices and performance of ChatGPT 4.0, a large language model (LLM), against graduate engineering students in a 48-hour prototyping hackathon, based on a dataset comprising more than 100 prototypes. The LLM participated by instructing two participants who executed its instructions and provided objective feedback, generated ideas autonomously and made all design decisions without human intervention. The LLM exhibited similar prototyping practices to human participants and finished second among six teams, successfully designing and providing building instructions for functional prototypes. The LLM's concept generation capabilities were particularly strong. However, the LLM prematurely abandoned promising concepts when facing minor difficulties, added unnecessary complexity to designs, and experienced design fixation. Communication between the LLM and participants was challenging due to vague or unclear descriptions, and the LLM had difficulty maintaining continuity and relevance in answers. Based on these findings, six recommendations for implementing an LLM like ChatGPT in the design process are proposed, including leveraging it for ideation, ensuring human oversight for key decisions, implementing iterative feedback loops, prompting it to consider alternatives, and assigning specific and manageable tasks at a subsystem level.","sentences":["This study compares the design practices and performance of ChatGPT 4.0, a large language model (LLM), against graduate engineering students in a 48-hour prototyping hackathon, based on a dataset comprising more than 100 prototypes.","The LLM participated by instructing two participants who executed its instructions and provided objective feedback, generated ideas autonomously and made all design decisions without human intervention.","The LLM exhibited similar prototyping practices to human participants and finished second among six teams, successfully designing and providing building instructions for functional prototypes.","The LLM's concept generation capabilities were particularly strong.","However, the LLM prematurely abandoned promising concepts when facing minor difficulties, added unnecessary complexity to designs, and experienced design fixation.","Communication between the LLM and participants was challenging due to vague or unclear descriptions, and the LLM had difficulty maintaining continuity and relevance in answers.","Based on these findings, six recommendations for implementing an LLM like ChatGPT in the design process are proposed, including leveraging it for ideation, ensuring human oversight for key decisions, implementing iterative feedback loops, prompting it to consider alternatives, and assigning specific and manageable tasks at a subsystem level."],"url":"http://arxiv.org/abs/2404.18479v1","category":"cs.HC"}
{"created":"2024-04-29 06:48:03","title":"Efficient bound preserving and asymptotic preserving semi-implicit schemes for the fast reaction-diffusion system","abstract":"We consider a special type of fast reaction-diffusion systems in which the coefficients of the reaction terms of the two substances are much larger than those of the diffusion terms while the diffusive motion to the substrate is negligible. Specifically speaking, the rate constants of the reaction terms are $O(1/\\epsilon)$ while the diffusion coefficients are $O(1)$ where the parameter $\\epsilon$ is small. When the rate constants of the reaction terms become highly large, i.e. $\\epsilon$ tends to 0, the singular limit behavior of such a fast reaction-diffusion system is inscribed by the Stefan problem with latent heat, which brings great challenges in numerical simulations. In this paper, we adopt a semi-implicit scheme, which is first-order accurate in time and can accurately approximate the interface propagation even when the reaction becomes extremely fast, that is to say, the parameter $\\epsilon$ is sufficiently small. The scheme satisfies the positivity, bound preserving properties and has $L^2$ stability and the linearized stability results of the system. For better performance on numerical simulations, we then construct a semi-implicit Runge-Kutta scheme which is second-order accurate in time. Numerous numerical tests are carried out to demonstrate the properties, such as the order of accuracy, positivity and bound preserving, the capturing of the sharp interface with various $\\epsilon$ and to simulate the dynamics of the substances and the substrate, and to explore the heat transfer process, such as solid melting or liquid solidification in two dimensions.","sentences":["We consider a special type of fast reaction-diffusion systems in which the coefficients of the reaction terms of the two substances are much larger than those of the diffusion terms while the diffusive motion to the substrate is negligible.","Specifically speaking, the rate constants of the reaction terms are $O(1/\\epsilon)$ while the diffusion coefficients are $O(1)$ where the parameter $\\epsilon$ is small.","When the rate constants of the reaction terms become highly large, i.e. $\\epsilon$ tends to 0, the singular limit behavior of such a fast reaction-diffusion system is inscribed by the Stefan problem with latent heat, which brings great challenges in numerical simulations.","In this paper, we adopt a semi-implicit scheme, which is first-order accurate in time and can accurately approximate the interface propagation even when the reaction becomes extremely fast, that is to say, the parameter $\\epsilon$ is sufficiently small.","The scheme satisfies the positivity, bound preserving properties and has $L^2$ stability and the linearized stability results of the system.","For better performance on numerical simulations, we then construct a semi-implicit Runge-Kutta scheme which is second-order accurate in time.","Numerous numerical tests are carried out to demonstrate the properties, such as the order of accuracy, positivity and bound preserving, the capturing of the sharp interface with various $\\epsilon$ and to simulate the dynamics of the substances and the substrate, and to explore the heat transfer process, such as solid melting or liquid solidification in two dimensions."],"url":"http://arxiv.org/abs/2404.18463v1","category":"math.NA"}
{"created":"2024-04-29 06:26:48","title":"Sustained Oscillations in Hyperbolic-Parabolic Systems","abstract":"We construct examples of oscillating solutions with persistent oscillations for various hyperbolic-parabolic systems with singular diffusion matrices that appear in mechanics. These include, an example for the equations of nonlinear viscoelasticity of Kelvin-Voigt type with stored energy that violates rank-one convexity, which amounts to a time-dependent variant of twinning solutions. An example pertaining to the system of gas dynamics with thermal effects for a viscous, adiabatic gas. Finally, an example for the compressible Navier-Stokes system in one-space dimension with nonmonotone pressure function. We also study the existence of oscillating solutions for linear hyperbolic-parabolic systems with singular diffusion matrices.","sentences":["We construct examples of oscillating solutions with persistent oscillations for various hyperbolic-parabolic systems with singular diffusion matrices that appear in mechanics.","These include, an example for the equations of nonlinear viscoelasticity of Kelvin-Voigt type with stored energy that violates rank-one convexity, which amounts to a time-dependent variant of twinning solutions.","An example pertaining to the system of gas dynamics with thermal effects for a viscous, adiabatic gas.","Finally, an example for the compressible Navier-Stokes system in one-space dimension with nonmonotone pressure function.","We also study the existence of oscillating solutions for linear hyperbolic-parabolic systems with singular diffusion matrices."],"url":"http://arxiv.org/abs/2404.18457v1","category":"math.AP"}
{"created":"2024-04-29 06:24:33","title":"Oscillations in Compressible Navier-Stokes and Homogenization in Phase Transition problems","abstract":"In the first part of this article we present some exact solutions for special hyperbolic-parabolic systems with sustained oscillations induced by the initial data, most notably the compressible Navier-Stokes system with non-monotone pressure. This part complements \\cite{Tzavaras23} where such examples are extensively studied. The second part deals with the problem of homogenization for one-dimensional models describing phase transitions for viscoelastic materials . Ideas from the kinetic formulation of conservation laws are employed to derive effective equations that describe the propagation of oscillations.","sentences":["In the first part of this article we present some exact solutions for special hyperbolic-parabolic systems with sustained oscillations induced by the initial data, most notably the compressible Navier-Stokes system with non-monotone pressure.","This part complements \\cite{Tzavaras23} where such examples are extensively studied.","The second part deals with the problem of homogenization for one-dimensional models describing phase transitions for viscoelastic materials .","Ideas from the kinetic formulation of conservation laws are employed to derive effective equations that describe the propagation of oscillations."],"url":"http://arxiv.org/abs/2404.18455v1","category":"math.AP"}
{"created":"2024-04-29 06:23:35","title":"Fostering Trust in Smart Inverters: A Framework for Firmware Update Management and Tracking in VPP Context","abstract":"Ensuring the reliability and security of smart inverters that provide the interface between distributed energy resources (DERs) and the power grid becomes paramount with the surge in integrating DERs into the (smart) power grid. Despite the importance of having updated firmware / software versions within a reasonable time frame, existing methods for establishing trust through firmware updates lack effective historical tracking and verification. This paper introduces a novel framework to manage and track firmware update history, leveraging verifiable credentials. By tracking the update history and implementing a trust cycle based on these verifiable updates, we aim to improve grid resilience, enhance cybersecurity, and increase transparency for stakeholders.","sentences":["Ensuring the reliability and security of smart inverters that provide the interface between distributed energy resources (DERs) and the power grid becomes paramount with the surge in integrating DERs into the (smart) power grid.","Despite the importance of having updated firmware / software versions within a reasonable time frame, existing methods for establishing trust through firmware updates lack effective historical tracking and verification.","This paper introduces a novel framework to manage and track firmware update history, leveraging verifiable credentials.","By tracking the update history and implementing a trust cycle based on these verifiable updates, we aim to improve grid resilience, enhance cybersecurity, and increase transparency for stakeholders."],"url":"http://arxiv.org/abs/2404.18453v1","category":"cs.CR"}
{"created":"2024-04-29 06:23:28","title":"Random Reshuffling with Momentum for Nonconvex Problems: Iteration Complexity and Last Iterate Convergence","abstract":"Random reshuffling with momentum (RRM) corresponds to the SGD optimizer with momentum option enabled, as found in popular machine learning libraries like PyTorch and TensorFlow. Despite its widespread use in practical applications, the understanding of its convergence properties in nonconvex scenarios remains limited. Under a Lipschitz smoothness assumption, this paper provides one of the first iteration complexities for RRM. Specifically, we prove that RRM achieves the iteration complexity $O(n^{-1/3}((1-\\beta^n)T)^{-2/3})$ where $n$ denotes the number of component functions $f(\\cdot;i)$ and $\\beta \\in [0,1)$ is the momentum parameter. Furthermore, every accumulation point of a sequence of iterates $\\{x^k\\}_k$ generated by RRM is shown to be a stationary point of the problem. In addition, under the Kurdyka-Lojasiewicz inequality - a local geometric property - the iterates $\\{x^k\\}_k$ provably converge to a unique stationary point $x^*$ of the objective function. Importantly, in our analysis, this last iterate convergence is obtained without requiring convexity nor a priori boundedness of the iterates. Finally, for polynomial step size schemes, convergence rates of the form $\\|x^k - x^*\\| = O(k^{-p})$, $\\|\\nabla f(x^k)\\|^2 = O(k^{-q})$, and $|f(x^k) - f(x^*)| = O(k^{-q})$, $p \\in (0,1]$, $q \\in (0,2]$ are derived.","sentences":["Random reshuffling with momentum (RRM) corresponds to the SGD optimizer with momentum option enabled, as found in popular machine learning libraries like PyTorch and TensorFlow.","Despite its widespread use in practical applications, the understanding of its convergence properties in nonconvex scenarios remains limited.","Under a Lipschitz smoothness assumption, this paper provides one of the first iteration complexities for RRM.","Specifically, we prove that RRM achieves the iteration complexity $O(n^{-1/3}((1-\\beta^n)T)^{-2/3})$ where $n$ denotes the number of component functions $f(\\cdot;i)$ and $\\beta \\in","[0,1)$ is the momentum parameter.","Furthermore, every accumulation point of a sequence of iterates $\\{x^k\\}_k$ generated by RRM is shown to be a stationary point of the problem.","In addition, under the Kurdyka-Lojasiewicz inequality - a local geometric property - the iterates $\\{x^k\\}_k$ provably converge to a unique stationary point $x^*$ of the objective function.","Importantly, in our analysis, this last iterate convergence is obtained without requiring convexity nor a priori boundedness of the iterates.","Finally, for polynomial step size schemes, convergence rates of the form $\\|x^k - x^*\\|","= O(k^{-p})$, $\\|\\nabla f(x^k)\\|^2 = O(k^{-q})$, and $|f(x^k) - f(x^*)| = O(k^{-q})$, $p \\in (0,1]$, $q \\in (0,2]$ are derived."],"url":"http://arxiv.org/abs/2404.18452v1","category":"math.OC"}
{"created":"2024-04-29 06:20:45","title":"Detailed derivation of the $\\mathbf{^3P_0}$ strong decay model applied to baryons","abstract":"We provide an in-detail derivation through the $^3P_0$ pair creation model of the transition matrix for a baryon decaying into a meson-baryon system. The meson's analysis was conducted in Ref. [1] and we extend the same formalism to the baryon sector, focusing on the $\\Delta(1232)\\to \\pi N$ strong decay width because all hadrons involved in the reaction are very well established, the two hadrons in the final state are stable, avoiding further analysis, all quarks are light and so equivalent, and the decay width of the process is relatively well measured. Taking advantage of a Gaussian expansion method for the hadron's radial wave functions, the expression of the invariant matrix element can be related with the mean-square radii of hadrons involved in the decay. We use their experimental measures in such a way that only the strength of the quark-antiquark pair creation from the vacuum is a free parameter. This is then taken from our previous study of strong decay widths in the meson sector, obtaining a quite compatible result with experiment for the calculated $\\Delta(1232)\\to \\pi N$ decay width.","sentences":["We provide an in-detail derivation through the $^3P_0$ pair creation model of the transition matrix for a baryon decaying into a meson-baryon system.","The meson's analysis was conducted in Ref.","[1] and we extend the same formalism to the baryon sector, focusing on the $\\Delta(1232)\\to \\pi N$ strong decay width because all hadrons involved in the reaction are very well established, the two hadrons in the final state are stable, avoiding further analysis, all quarks are light and so equivalent, and the decay width of the process is relatively well measured.","Taking advantage of a Gaussian expansion method for the hadron's radial wave functions, the expression of the invariant matrix element can be related with the mean-square radii of hadrons involved in the decay.","We use their experimental measures in such a way that only the strength of the quark-antiquark pair creation from the vacuum is a free parameter.","This is then taken from our previous study of strong decay widths in the meson sector, obtaining a quite compatible result with experiment for the calculated $\\Delta(1232)\\to \\pi N$ decay width."],"url":"http://arxiv.org/abs/2404.18450v1","category":"hep-ph"}
{"created":"2024-04-29 06:10:45","title":"The PRODSAT phase of random quantum satisfiability","abstract":"The $k$-QSAT problem is a quantum analog of the famous $k$-SAT constraint satisfaction problem. We must determine the zero energy ground states of a Hamiltonian of $N$ qubits consisting of a sum of $M$ random $k$-local rank-one projectors. It is known that product states of zero energy exist with high probability if and only if the underlying factor graph has a clause-covering dimer configuration. This means that the threshold of the PRODSAT phase is a purely geometric quantity equal to the dimer covering threshold. We revisit and fully prove this result through a combination of complex analysis and algebraic methods based on Buchberger's algorithm for complex polynomial equations with random coefficients. We also discuss numerical experiments investigating the presence of entanglement in the PRODSAT phase in the sense that product states do not span the whole zero energy ground state space.","sentences":["The $k$-QSAT problem is a quantum analog of the famous $k$-SAT constraint satisfaction problem.","We must determine the zero energy ground states of a Hamiltonian of $N$ qubits consisting of a sum of $M$ random $k$-local rank-one projectors.","It is known that product states of zero energy exist with high probability if and only if the underlying factor graph has a clause-covering dimer configuration.","This means that the threshold of the PRODSAT phase is a purely geometric quantity equal to the dimer covering threshold.","We revisit and fully prove this result through a combination of complex analysis and algebraic methods based on Buchberger's algorithm for complex polynomial equations with random coefficients.","We also discuss numerical experiments investigating the presence of entanglement in the PRODSAT phase in the sense that product states do not span the whole zero energy ground state space."],"url":"http://arxiv.org/abs/2404.18447v1","category":"cs.IT"}
{"created":"2024-04-29 05:22:47","title":"Three-Dimension Collision-Free Trajectory Planning of UAVs Based on ADS-B Information in Low-Altitude Urban Airspace","abstract":"The environment of low-altitude urban airspace is complex and variable due to numerous obstacles, non-cooperative aircrafts, and birds. Unmanned aerial vehicles (UAVs) leveraging environmental information to achieve three-dimension collision-free trajectory planning is the prerequisite to ensure airspace security. However, the timely information of surrounding situation is difficult to acquire by UAVs, which further brings security risks. As a mature technology leveraged in traditional civil aviation, the automatic dependent surveillance-broadcast (ADS-B) realizes continuous surveillance of the information of aircrafts. Consequently, we leverage ADS-B for surveillance and information broadcasting, and divide the aerial airspace into multiple sub-airspaces to improve flight safety in UAV trajectory planning. In detail, we propose the secure sub-airspaces planning (SSP) algorithm and particle swarm optimization rapidly-exploring random trees (PSO-RRT) algorithm for the UAV trajectory planning in law-altitude airspace. The performance of the proposed algorithm is verified by simulations and the results show that SSP reduces both the maximum number of UAVs in the sub-airspace and the length of the trajectory, and PSO-RRT reduces the cost of UAV trajectory in the sub-airspace.","sentences":["The environment of low-altitude urban airspace is complex and variable due to numerous obstacles, non-cooperative aircrafts, and birds.","Unmanned aerial vehicles (UAVs) leveraging environmental information to achieve three-dimension collision-free trajectory planning is the prerequisite to ensure airspace security.","However, the timely information of surrounding situation is difficult to acquire by UAVs, which further brings security risks.","As a mature technology leveraged in traditional civil aviation, the automatic dependent surveillance-broadcast (ADS-B) realizes continuous surveillance of the information of aircrafts.","Consequently, we leverage ADS-B for surveillance and information broadcasting, and divide the aerial airspace into multiple sub-airspaces to improve flight safety in UAV trajectory planning.","In detail, we propose the secure sub-airspaces planning (SSP) algorithm and particle swarm optimization rapidly-exploring random trees (PSO-RRT) algorithm for the UAV trajectory planning in law-altitude airspace.","The performance of the proposed algorithm is verified by simulations and the results show that SSP reduces both the maximum number of UAVs in the sub-airspace and the length of the trajectory, and PSO-RRT reduces the cost of UAV trajectory in the sub-airspace."],"url":"http://arxiv.org/abs/2404.18436v1","category":"eess.SY"}
{"created":"2024-04-29 05:17:33","title":"ShadowMaskFormer: Mask Augmented Patch Embeddings for Shadow Removal","abstract":"Transformer recently emerged as the de facto model for computer vision tasks and has also been successfully applied to shadow removal. However, these existing methods heavily rely on intricate modifications to the attention mechanisms within the transformer blocks while using a generic patch embedding. As a result, it often leads to complex architectural designs requiring additional computation resources. In this work, we aim to explore the efficacy of incorporating shadow information within the early processing stage. Accordingly, we propose a transformer-based framework with a novel patch embedding that is tailored for shadow removal, dubbed ShadowMaskFormer. Specifically, we present a simple and effective mask-augmented patch embedding to integrate shadow information and promote the model's emphasis on acquiring knowledge for shadow regions. Extensive experiments conducted on the ISTD, ISTD+, and SRD benchmark datasets demonstrate the efficacy of our method against state-of-the-art approaches while using fewer model parameters.","sentences":["Transformer recently emerged as the de facto model for computer vision tasks and has also been successfully applied to shadow removal.","However, these existing methods heavily rely on intricate modifications to the attention mechanisms within the transformer blocks while using a generic patch embedding.","As a result, it often leads to complex architectural designs requiring additional computation resources.","In this work, we aim to explore the efficacy of incorporating shadow information within the early processing stage.","Accordingly, we propose a transformer-based framework with a novel patch embedding that is tailored for shadow removal, dubbed ShadowMaskFormer.","Specifically, we present a simple and effective mask-augmented patch embedding to integrate shadow information and promote the model's emphasis on acquiring knowledge for shadow regions.","Extensive experiments conducted on the ISTD, ISTD+, and SRD benchmark datasets demonstrate the efficacy of our method against state-of-the-art approaches while using fewer model parameters."],"url":"http://arxiv.org/abs/2404.18433v1","category":"cs.CV"}
{"created":"2024-04-29 05:15:45","title":"Structure-preserving particle methods for the Landau collision operator using the metriplectic framework","abstract":"We present a novel family of particle discretisation methods for the nonlinear Landau collision operator. We exploit the metriplectic structure underlying the Vlasov-Maxwell-Landau system in order to obtain disretisation schemes that automatically preserve mass, momentum, and energy, warrant monotonic dissipation of entropy, and are thus guaranteed to respect the laws of thermodynamics. In contrast to recent works that used radial basis functions and similar methods for regularisation, here we use an auxiliary spline or finite element representation of the distribution function to this end. Discrete gradient methods are employed to guarantee the aforementioned properties in the time discrete domain as well.","sentences":["We present a novel family of particle discretisation methods for the nonlinear Landau collision operator.","We exploit the metriplectic structure underlying the Vlasov-Maxwell-Landau system in order to obtain disretisation schemes that automatically preserve mass, momentum, and energy, warrant monotonic dissipation of entropy, and are thus guaranteed to respect the laws of thermodynamics.","In contrast to recent works that used radial basis functions and similar methods for regularisation, here we use an auxiliary spline or finite element representation of the distribution function to this end.","Discrete gradient methods are employed to guarantee the aforementioned properties in the time discrete domain as well."],"url":"http://arxiv.org/abs/2404.18432v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 05:13:51","title":"Self-Similar Collapse in Painleve-Gullstrand Coordinates","abstract":"We report a family of self-similar exact solutions in General Relativity. The solutions are found in a Painleve-Gullstrand coordinate system but can also be transformed smoothly into a diagonal form. The solutions represent a gravitational collapse leading to three possible outcomes, depending on the parameter space : (i) a collapse followed by a bounce and dispersal of the clustered matter distribution, (ii) a rapid collapse followed by a bounce and an eventual re-collapse, and (iii) a standard collapse leading to zero proper volume. Profiles of the energy conditions are studied for all of the scenarios, and it is noted that a bounce is usually associated with a violation of the Null Energy Condition. It is found that more than one null surfaces (apparent horizons) can develop during the collapse. We also discuss that for a general metric tensor having a conformal symmetry, some regions of the parameter space allows a formation of null throat, much like a wormhole. Matching the metric with a Schwarzschild metric in Painleve-Gullstrand form leads to the geodesic equation for a zero energy falling particle in the exterior.","sentences":["We report a family of self-similar exact solutions in General Relativity.","The solutions are found in a Painleve-Gullstrand coordinate system but can also be transformed smoothly into a diagonal form.","The solutions represent a gravitational collapse leading to three possible outcomes, depending on the parameter space : (i) a collapse followed by a bounce and dispersal of the clustered matter distribution, (ii) a rapid collapse followed by a bounce and an eventual re-collapse, and (iii) a standard collapse leading to zero proper volume.","Profiles of the energy conditions are studied for all of the scenarios, and it is noted that a bounce is usually associated with a violation of the Null Energy Condition.","It is found that more than one null surfaces (apparent horizons) can develop during the collapse.","We also discuss that for a general metric tensor having a conformal symmetry, some regions of the parameter space allows a formation of null throat, much like a wormhole.","Matching the metric with a Schwarzschild metric in Painleve-Gullstrand form leads to the geodesic equation for a zero energy falling particle in the exterior."],"url":"http://arxiv.org/abs/2404.18431v1","category":"gr-qc"}
{"created":"2024-04-29 04:56:52","title":"Efficient Meta-Learning Enabled Lightweight Multiscale Few-Shot Object Detection in Remote Sensing Images","abstract":"Presently, the task of few-shot object detection (FSOD) in remote sensing images (RSIs) has become a focal point of attention. Numerous few-shot detectors, particularly those based on two-stage detectors, face challenges when dealing with the multiscale complexities inherent in RSIs. Moreover, these detectors present impractical characteristics in real-world applications, mainly due to their unwieldy model parameters when handling large amount of data. In contrast, we recognize the advantages of one-stage detectors, including high detection speed and a global receptive field. Consequently, we choose the YOLOv7 one-stage detector as a baseline and subject it to a novel meta-learning training framework. This transformation allows the detector to adeptly address FSOD tasks while capitalizing on its inherent advantage of lightweight. Additionally, we thoroughly investigate the samples generated by the meta-learning strategy and introduce a novel meta-sampling approach to retain samples produced by our designed meta-detection head. Coupled with our devised meta-cross loss, we deliberately utilize ``negative samples\" that are often overlooked to extract valuable knowledge from them. This approach serves to enhance detection accuracy and efficiently refine the overall meta-learning strategy. To validate the effectiveness of our proposed detector, we conducted performance comparisons with current state-of-the-art detectors using the DIOR and NWPU VHR-10.v2 datasets, yielding satisfactory results.","sentences":["Presently, the task of few-shot object detection (FSOD) in remote sensing images (RSIs) has become a focal point of attention.","Numerous few-shot detectors, particularly those based on two-stage detectors, face challenges when dealing with the multiscale complexities inherent in RSIs.","Moreover, these detectors present impractical characteristics in real-world applications, mainly due to their unwieldy model parameters when handling large amount of data.","In contrast, we recognize the advantages of one-stage detectors, including high detection speed and a global receptive field.","Consequently, we choose the YOLOv7 one-stage detector as a baseline and subject it to a novel meta-learning training framework.","This transformation allows the detector to adeptly address FSOD tasks while capitalizing on its inherent advantage of lightweight.","Additionally, we thoroughly investigate the samples generated by the meta-learning strategy and introduce a novel meta-sampling approach to retain samples produced by our designed meta-detection head.","Coupled with our devised meta-cross loss, we deliberately utilize ``negative samples\" that are often overlooked to extract valuable knowledge from them.","This approach serves to enhance detection accuracy and efficiently refine the overall meta-learning strategy.","To validate the effectiveness of our proposed detector, we conducted performance comparisons with current state-of-the-art detectors using the DIOR and NWPU VHR-10.v2 datasets, yielding satisfactory results."],"url":"http://arxiv.org/abs/2404.18426v1","category":"cs.CV"}
{"created":"2024-04-29 17:50:28","title":"Studies of $T_{cc}^+$ Decays and Transverse-Momentum-Dependent $J/\u03c8$ Production Using Effective Field Theory","abstract":"We describe the application of effective field theories for quantum chromodynamics (QCD) to two bound states involving heavy quarks: the $T_{cc}^+$ exotic meson and the $J/\\psi$. We study the decay of the $T_{cc}^+$ in an effective theory for hadronic molecules, and find agreement with experiment. We also use the nonrelativistic QCD (NRQCD) factorization formalism to derive the leading-order transverse momentum dependent fragmentation functions (FFs) for quarks and gluons fragmenting to $J/\\psi$. We then make use of these TMD FFs to compare the $J/\\psi$ production mechanisms of light quark fragmentation and photon-gluon fusion, where the conclusions we draw can motivate future experiments at the Electron Ion Collider, shedding light on the inner structure of nucleons and testing ideas from NRQCD factorization. These results showcase the utility of effective field theories in explaining experiments and testing key concepts in nuclear/particle physics.","sentences":["We describe the application of effective field theories for quantum chromodynamics (QCD) to two bound states involving heavy quarks: the $T_{cc}^+$ exotic meson and the $J/\\psi$. We study the decay of the $T_{cc}^+$ in an effective theory for hadronic molecules, and find agreement with experiment.","We also use the nonrelativistic QCD (NRQCD) factorization formalism to derive the leading-order transverse momentum dependent fragmentation functions (FFs) for quarks and gluons fragmenting to $J/\\psi$.","We then make use of these TMD FFs to compare the $J/\\psi$ production mechanisms of light quark fragmentation and photon-gluon fusion, where the conclusions we draw can motivate future experiments at the Electron Ion Collider, shedding light on the inner structure of nucleons and testing ideas from NRQCD factorization.","These results showcase the utility of effective field theories in explaining experiments and testing key concepts in nuclear/particle physics."],"url":"http://arxiv.org/abs/2404.18907v1","category":"hep-ph"}
{"created":"2024-04-29 17:36:19","title":"Finite Element Approximation of the Fractional Porous Medium Equation","abstract":"We construct a finite element method for the numerical solution of a fractional porous medium equation on a bounded open Lipschitz polytopal domain $\\Omega \\subset \\mathbb{R}^{d}$, where $d = 2$ or $3$. The pressure in the model is defined as the solution of a fractional Poisson equation, involving the fractional Neumann Laplacian in terms of its spectral definition. We perform a rigorous passage to the limit as the spatial and temporal discretization parameters tend to zero and show that a subsequence of the sequence of finite element approximations defined by the proposed numerical method converges to a bounded and nonnegative weak solution of the initial-boundary-value problem under consideration. This result can be therefore viewed as a constructive proof of the existence of a nonnegative, energy-dissipative, weak solution to the initial-boundary-value problem for the fractional porous medium equation under consideration, based on the Neumann Laplacian. The convergence proof relies on results concerning the finite element approximation of the spectral fractional Laplacian and compactness techniques for nonlinear partial differential equations, together with properties of the equation, which are shown to be inherited by the numerical method. We also prove that the total energy associated with the problem under consideration exhibits exponential decay in time.","sentences":["We construct a finite element method for the numerical solution of a fractional porous medium equation on a bounded open Lipschitz polytopal domain $\\Omega \\subset \\mathbb{R}^{d}$, where $d = 2$ or $3$. The pressure in the model is defined as the solution of a fractional Poisson equation, involving the fractional Neumann Laplacian in terms of its spectral definition.","We perform a rigorous passage to the limit as the spatial and temporal discretization parameters tend to zero and show that a subsequence of the sequence of finite element approximations defined by the proposed numerical method converges to a bounded and nonnegative weak solution of the initial-boundary-value problem under consideration.","This result can be therefore viewed as a constructive proof of the existence of a nonnegative, energy-dissipative, weak solution to the initial-boundary-value problem for the fractional porous medium equation under consideration, based on the Neumann Laplacian.","The convergence proof relies on results concerning the finite element approximation of the spectral fractional Laplacian and compactness techniques for nonlinear partial differential equations, together with properties of the equation, which are shown to be inherited by the numerical method.","We also prove that the total energy associated with the problem under consideration exhibits exponential decay in time."],"url":"http://arxiv.org/abs/2404.18901v1","category":"math.NA"}
{"created":"2024-04-29 17:35:27","title":"Non-planar integrated correlator in $\\mathcal{N}=4$ SYM","abstract":"Integrated correlator of four superconformal stress-tensor primaries in $SU(N)$ $\\mathcal{N}=4$ super Yang-Mills (SYM) theory in the perturbative limit takes a remarkably simple form, where the $L$-loop coefficient is given by a rational multiple of $\\zeta(2L+1)$. In this letter, we extend the previous analysis of expressing the perturbative integrated correlator as a linear combination of periods of $f$-graphs, graphical representations for loop integrands, to the non-planar sector at four loops. At this loop order, multiple zeta values make their first appearance when evaluating periods of non-planar $f$-graphs, but cancel non-trivially in the weighted sum. The remaining single zeta value, along with the rational number prefactor, makes a perfect agreement with the prediction from supersymmmetric localisation.","sentences":["Integrated correlator of four superconformal stress-tensor primaries in $SU(N)$ $\\mathcal{N}=4$ super Yang-Mills (SYM) theory in the perturbative limit takes a remarkably simple form, where the $L$-loop coefficient is given by a rational multiple of $\\zeta(2L+1)$. In this letter, we extend the previous analysis of expressing the perturbative integrated correlator as a linear combination of periods of $f$-graphs, graphical representations for loop integrands, to the non-planar sector at four loops.","At this loop order, multiple zeta values make their first appearance when evaluating periods of non-planar $f$-graphs, but cancel non-trivially in the weighted sum.","The remaining single zeta value, along with the rational number prefactor, makes a perfect agreement with the prediction from supersymmmetric localisation."],"url":"http://arxiv.org/abs/2404.18900v1","category":"hep-th"}
{"created":"2024-04-29 17:34:42","title":"Celestial Optical Theorem","abstract":"We establish the nonperturbative celestial optical theorem from the unitarity of $S$-matrix. This theorem provides a set of nonperturbative bootstrap equations of the conformal partial wave (CPW) coefficients. The celestial optical theorem implies that the imaginary part of CPW coefficient with appropriate conformal dimensions is non-negative. By making certain assumptions and using the celestial optical theorem, we derive nonperturbative results concerning the analytic structure of CPW coefficients. We discover that the CPW coefficients of four massless particles must and only have simple poles located at specific positions. The CPW coefficients involving massive particles exhibit double-trace poles, indicating the existence of double-trace operators in nonperturbative CCFT. It is worth noting that, in contrast to AdS/CFT, the conformal dimensions of double-trace operators do not receive anomalous dimensions.","sentences":["We establish the nonperturbative celestial optical theorem from the unitarity of $S$-matrix.","This theorem provides a set of nonperturbative bootstrap equations of the conformal partial wave (CPW) coefficients.","The celestial optical theorem implies that the imaginary part of CPW coefficient with appropriate conformal dimensions is non-negative.","By making certain assumptions and using the celestial optical theorem, we derive nonperturbative results concerning the analytic structure of CPW coefficients.","We discover that the CPW coefficients of four massless particles must and only have simple poles located at specific positions.","The CPW coefficients involving massive particles exhibit double-trace poles, indicating the existence of double-trace operators in nonperturbative CCFT.","It is worth noting that, in contrast to AdS/CFT, the conformal dimensions of double-trace operators do not receive anomalous dimensions."],"url":"http://arxiv.org/abs/2404.18898v1","category":"hep-th"}
{"created":"2024-04-29 17:34:24","title":"Neural network prediction of model parameters for strong lensing samples from Hyper Suprime-Cam Survey","abstract":"Galaxies that cause the strong gravitational lensing of background galaxies provide us crucial information about the distribution of matter around them. Traditional modelling methods that analyse such strong lenses are both time and resource consuming, require sophisticated lensing codes and modelling expertise. To study the large lens population expected from imaging surveys such as LSST, we need fast and automated analysis methods. In this work, we build and train a simple convolutional neural network with an aim to rapidly predict model parameters of gravitational lenses. We focus on the most important lens mass model parameters, namely, the Einstein radius, the axis ratio and the position angle of the major axis of the mass distribution. The network is trained on a variety of simulated data with an increasing degree of realism and shows satisfactory performance on simulated test data. The trained network is then applied to the real sample of galaxy-scale candidate lenses from the Subaru HSC, a precursor survey to LSST. Unlike the simulated lenses, we do not have the ground truth for the real lenses. Therefore, we have compared our predictions with those from YattaLens, a lens modelling pipeline. Additionally, we also compare the parameter predictions for 10 HSC lenses that were also studied by other conventional modelling methods. These comparisons show a fair quantitative agreement on the Einstein radius, although the axis ratio and the position angle from the network as well as the individual modelling methods, seem to have systematic uncertainties beyond the quoted errors.","sentences":["Galaxies that cause the strong gravitational lensing of background galaxies provide us crucial information about the distribution of matter around them.","Traditional modelling methods that analyse such strong lenses are both time and resource consuming, require sophisticated lensing codes and modelling expertise.","To study the large lens population expected from imaging surveys such as LSST, we need fast and automated analysis methods.","In this work, we build and train a simple convolutional neural network with an aim to rapidly predict model parameters of gravitational lenses.","We focus on the most important lens mass model parameters, namely, the Einstein radius, the axis ratio and the position angle of the major axis of the mass distribution.","The network is trained on a variety of simulated data with an increasing degree of realism and shows satisfactory performance on simulated test data.","The trained network is then applied to the real sample of galaxy-scale candidate lenses from the Subaru HSC, a precursor survey to LSST.","Unlike the simulated lenses, we do not have the ground truth for the real lenses.","Therefore, we have compared our predictions with those from YattaLens, a lens modelling pipeline.","Additionally, we also compare the parameter predictions for 10 HSC lenses that were also studied by other conventional modelling methods.","These comparisons show a fair quantitative agreement on the Einstein radius, although the axis ratio and the position angle from the network as well as the individual modelling methods, seem to have systematic uncertainties beyond the quoted errors."],"url":"http://arxiv.org/abs/2404.18897v1","category":"astro-ph.CO"}
{"created":"2024-04-29 17:33:52","title":"Overcoming Knowledge Barriers: Online Imitation Learning from Observation with Pretrained World Models","abstract":"Incorporating the successful paradigm of pretraining and finetuning from Computer Vision and Natural Language Processing into decision-making has become increasingly popular in recent years. In this paper, we study Imitation Learning from Observation with pretrained models and find existing approaches such as BCO and AIME face knowledge barriers, specifically the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatly limiting their performance. The EKB arises when pretrained models lack knowledge about unseen observations, leading to errors in action inference. The DKB results from policies trained on limited demonstrations, hindering adaptability to diverse scenarios. We thoroughly analyse the underlying mechanism of these barriers and propose AIME-v2 upon AIME as a solution. AIME-v2 uses online interactions with data-driven regulariser to alleviate the EKB and mitigates the DKB by introducing a surrogate reward function to enhance policy training. Experimental results on tasks from the DeepMind Control Suite and Meta-World benchmarks demonstrate the effectiveness of these modifications in improving both sample-efficiency and converged performance. The study contributes valuable insights into resolving knowledge barriers for enhanced decision-making in pretraining-based approaches. Code will be available at https://github.com/argmax-ai/aime-v2.","sentences":["Incorporating the successful paradigm of pretraining and finetuning from Computer Vision and Natural Language Processing into decision-making has become increasingly popular in recent years.","In this paper, we study Imitation Learning from Observation with pretrained models and find existing approaches such as BCO and AIME face knowledge barriers, specifically the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB), greatly limiting their performance.","The EKB arises when pretrained models lack knowledge about unseen observations, leading to errors in action inference.","The DKB results from policies trained on limited demonstrations, hindering adaptability to diverse scenarios.","We thoroughly analyse the underlying mechanism of these barriers and propose AIME-v2 upon AIME as a solution.","AIME-v2 uses online interactions with data-driven regulariser to alleviate the EKB and mitigates the DKB by introducing a surrogate reward function to enhance policy training.","Experimental results on tasks from the DeepMind Control Suite and Meta-World benchmarks demonstrate the effectiveness of these modifications in improving both sample-efficiency and converged performance.","The study contributes valuable insights into resolving knowledge barriers for enhanced decision-making in pretraining-based approaches.","Code will be available at https://github.com/argmax-ai/aime-v2."],"url":"http://arxiv.org/abs/2404.18896v1","category":"cs.LG"}
{"created":"2024-04-29 17:30:41","title":"Odd viscosity suppresses intermittency in direct turbulent cascades","abstract":"Intermittency refers to the broken self-similarity of turbulent flows caused by anomalous spatio-temporal fluctuations. In this Letter, we ask how intermittency is affected by a non-dissipative viscosity, known as odd viscosity, which appears in parity-breaking fluids such as magnetized polyatomic gases, electron fluids under magnetic field and spinning colloids or grains. Using a combination of Navier-Stokes simulations and theory, we show that intermittency is suppressed by odd viscosity at small scales. This effect is caused by parity-breaking waves, induced by odd viscosity, that break the multiple scale invariances of the Navier-Stokes equations. Building on this insight, we construct a two-channel helical shell model that reproduces the basic phenomenology of turbulent odd-viscous fluids including the suppression of anomalous scaling. Our findings illustrate how a fully developed direct cascade that is entirely self-similar can emerge below a tunable length scale, paving the way for designing turbulent flows with adjustable levels of intermittency.","sentences":["Intermittency refers to the broken self-similarity of turbulent flows caused by anomalous spatio-temporal fluctuations.","In this Letter, we ask how intermittency is affected by a non-dissipative viscosity, known as odd viscosity, which appears in parity-breaking fluids such as magnetized polyatomic gases, electron fluids under magnetic field and spinning colloids or grains.","Using a combination of Navier-Stokes simulations and theory, we show that intermittency is suppressed by odd viscosity at small scales.","This effect is caused by parity-breaking waves, induced by odd viscosity, that break the multiple scale invariances of the Navier-Stokes equations.","Building on this insight, we construct a two-channel helical shell model that reproduces the basic phenomenology of turbulent odd-viscous fluids including the suppression of anomalous scaling.","Our findings illustrate how a fully developed direct cascade that is entirely self-similar can emerge below a tunable length scale, paving the way for designing turbulent flows with adjustable levels of intermittency."],"url":"http://arxiv.org/abs/2404.18894v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 17:19:24","title":"Reputation in Repeated Global Games of Regime Change with Exit","abstract":"I study a repeated binary-action supermodular game with endogenous exit where many short-lived agents attempt to coordinate a revolt against a regime. The regime undertakes costly actions to increase the short-run players' coordination frictions, though acts only after if the revolt is unsuccessful, inducing a lack-of-commitment problem. In the complete-information repeated game, a folk theorem holds, with payoff multiplicity arising due to both the regime's dynamic incentives and agents' stage-game strategic complementarities. Neither the regime's reputational incentives nor belief dispersion among agents (via global-games type uncertainty) alone meaningfully refine the equilibrium payoff set. Together, though, the interaction between these two forces uniquely select the regime's highest payoff in equilibrium. Furthermore, under a Markov refinement, they select a unique equilibrium where the regime plays their optimal commitment action. Methodologically, I develop tools to analyze repeated games with endogenous exit where the regime's commitment action flexibly varies with their discount rate.","sentences":["I study a repeated binary-action supermodular game with endogenous exit where many short-lived agents attempt to coordinate a revolt against a regime.","The regime undertakes costly actions to increase the short-run players' coordination frictions, though acts only after if the revolt is unsuccessful, inducing a lack-of-commitment problem.","In the complete-information repeated game, a folk theorem holds, with payoff multiplicity arising due to both the regime's dynamic incentives and agents' stage-game strategic complementarities.","Neither the regime's reputational incentives nor belief dispersion among agents (via global-games type uncertainty) alone meaningfully refine the equilibrium payoff set.","Together, though, the interaction between these two forces uniquely select the regime's highest payoff in equilibrium.","Furthermore, under a Markov refinement, they select a unique equilibrium where the regime plays their optimal commitment action.","Methodologically, I develop tools to analyze repeated games with endogenous exit where the regime's commitment action flexibly varies with their discount rate."],"url":"http://arxiv.org/abs/2404.18884v1","category":"econ.TH"}
{"created":"2024-04-29 16:52:00","title":"Conformal Prediction Sets for Populations of Graphs","abstract":"The analysis of data such as graphs has been gaining increasing attention in the past years. This is justified by the numerous applications in which they appear. Several methods are present to predict graphs, but much fewer to quantify the uncertainty of the prediction. The present work proposes an uncertainty quantification methodology for graphs, based on conformal prediction. The method works both for graphs with the same set of nodes (labelled graphs) and graphs with no clear correspondence between the set of nodes across the observed graphs (unlabelled graphs). The unlabelled case is dealt with the creation of prediction sets embedded in a quotient space. The proposed method does not rely on distributional assumptions, it achieves finite-sample validity, and it identifies interpretable prediction sets. To explore the features of this novel forecasting technique, we perform two simulation studies to show the methodology in both the labelled and the unlabelled case. We showcase the applicability of the method in analysing the performance of different teams during the FIFA 2018 football world championship via their player passing networks.","sentences":["The analysis of data such as graphs has been gaining increasing attention in the past years.","This is justified by the numerous applications in which they appear.","Several methods are present to predict graphs, but much fewer to quantify the uncertainty of the prediction.","The present work proposes an uncertainty quantification methodology for graphs, based on conformal prediction.","The method works both for graphs with the same set of nodes (labelled graphs) and graphs with no clear correspondence between the set of nodes across the observed graphs (unlabelled graphs).","The unlabelled case is dealt with the creation of prediction sets embedded in a quotient space.","The proposed method does not rely on distributional assumptions, it achieves finite-sample validity, and it identifies interpretable prediction sets.","To explore the features of this novel forecasting technique, we perform two simulation studies to show the methodology in both the labelled and the unlabelled case.","We showcase the applicability of the method in analysing the performance of different teams during the FIFA 2018 football world championship via their player passing networks."],"url":"http://arxiv.org/abs/2404.18862v1","category":"stat.ME"}
{"created":"2024-04-29 16:28:38","title":"Deep orthogonal decomposition: a continuously adaptive data-driven approach to model order reduction","abstract":"We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations. The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis. In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems. Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation. For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders. The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries.","sentences":["We develop a novel deep learning technique, termed Deep Orthogonal Decomposition (DOD), for dimensionality reduction and reduced order modeling of parameter dependent partial differential equations.","The approach consists in the construction of a deep neural network model that approximates the solution manifold through a continuously adaptive local basis.","In contrast to global methods, such as Principal Orthogonal Decomposition (POD), the adaptivity allows the DOD to overcome the Kolmogorov barrier, making the approach applicable to a wide spectrum of parametric problems.","Furthermore, due to its hybrid linear-nonlinear nature, the DOD can accommodate both intrusive and nonintrusive techniques, providing highly interpretable latent representations and tighter control on error propagation.","For this reason, the proposed approach stands out as a valuable alternative to other nonlinear techniques, such as deep autoencoders.","The methodology is discussed both theoretically and practically, evaluating its performances on problems featuring nonlinear PDEs, singularities, and parametrized geometries."],"url":"http://arxiv.org/abs/2404.18841v1","category":"math.NA"}
{"created":"2024-04-29 16:25:47","title":"NNLL-fast 2.0: Coloured Sparticle Production at the LHC Run 3 with $\\sqrt{S}$ = 13.6 TeV","abstract":"We report on updated precision predictions for total cross sections of coloured supersymmetric particle production at the LHC with a centre-of-mass energy of $\\sqrt S$ = 13.6 TeV, computed with the modern PDF4LHC21 set. The cross sections are calculated at an approximated NNLO accuracy in QCD and contain corrections from the threshold resummation of soft-gluon emission up to NNLL accuracy as well as Coulomb-gluon contributions including bound-state terms. The corrections are found to increase the cross sections and reduce the theoretical uncertainty as compared to the best available fixed-order calculations. These predictions constitute the state-of-the-art calculations and update the existing results for $\\sqrt S$ = 13 TeV. We make our new results publicly available in the version 2.0 update to the code package NNLL-fast.","sentences":["We report on updated precision predictions for total cross sections of coloured supersymmetric particle production at the LHC with a centre-of-mass energy of $\\sqrt S$ = 13.6 TeV, computed with the modern PDF4LHC21 set.","The cross sections are calculated at an approximated NNLO accuracy in QCD and contain corrections from the threshold resummation of soft-gluon emission up to NNLL accuracy as well as Coulomb-gluon contributions including bound-state terms.","The corrections are found to increase the cross sections and reduce the theoretical uncertainty as compared to the best available fixed-order calculations.","These predictions constitute the state-of-the-art calculations and update the existing results for $\\sqrt S$ = 13 TeV. We make our new results publicly available in the version 2.0 update to the code package NNLL-fast."],"url":"http://arxiv.org/abs/2404.18837v1","category":"hep-ph"}
{"created":"2024-04-29 15:41:57","title":"Endhered patterns in matchings and RNA","abstract":"An endhered (end-adhered) pattern is a subset of arcs in matchings, such that the corresponding starting points are consecutive and the same holds for the ending points. Such patterns are in one-to-one correspondence with the permutations. We focus on the occurrence frequency of such patterns in matchings and real-world RNA structures with pseudoknots. We present combinatorial results related to the distribution and asymptotic behavior of the pattern 21, which corresponds to two consecutive stacked bonds frequently encountered in RNA, and the pattern 12, representing the archetypal minimal pseudoknot. We show that in matchings these two patterns are equidistributed, which is quite different from what we can find in real-world RNAs. We also examine the distribution of endhered patterns of size 3, showing how the patterns change under the transformation called endhered twist. Finally, we compute the distributions of endhered patterns of size 2 and 3 in real-world secondary RNA structures with pseudoknots and discuss possible outcomes of our study.","sentences":["An endhered (end-adhered) pattern is a subset of arcs in matchings, such that the corresponding starting points are consecutive and the same holds for the ending points.","Such patterns are in one-to-one correspondence with the permutations.","We focus on the occurrence frequency of such patterns in matchings and real-world RNA structures with pseudoknots.","We present combinatorial results related to the distribution and asymptotic behavior of the pattern 21, which corresponds to two consecutive stacked bonds frequently encountered in RNA, and the pattern 12, representing the archetypal minimal pseudoknot.","We show that in matchings these two patterns are equidistributed, which is quite different from what we can find in real-world RNAs.","We also examine the distribution of endhered patterns of size 3, showing how the patterns change under the transformation called endhered twist.","Finally, we compute the distributions of endhered patterns of size 2 and 3 in real-world secondary RNA structures with pseudoknots and discuss possible outcomes of our study."],"url":"http://arxiv.org/abs/2404.18802v1","category":"math.CO"}
{"created":"2024-04-29 15:20:58","title":"Detection of Fe and Ti on the dayside of the ultrahot Jupiter MASCARA-1b with CARMENES","abstract":"Ultrahot Jupiters are a type of gaseous exoplanet that orbit extremely close to their host star, resulting in significantly high equilibrium temperatures. In recent years, high-resolution emission spectroscopy has been broadly employed in observing the atmospheres of ultrahot Jupiters. We used the CARMENES spectrograph to observe the high-resolution spectra of the dayside hemisphere of MASCARA-1b in both visible and near-infrared. Through cross-correlation analysis, we detected signals of \\ion{Fe}{i} and \\ion{Ti}{i}. Based on these detections, we conducted an atmospheric retrieval and discovered the presence of a strong inversion layer in the planet's atmosphere. The retrieved Ti and Fe abundances are broadly consistent with solar abundances. In particular, we obtained a relative abundance of [Ti/Fe] as $-1.0 \\pm 0.8$ under the free retrieval and $-0.4^{+0.5}_{-0.8}$ under the chemical equilibrium retrieval, suggesting the absence of significant titanium depletion on this planet. Furthermore, we considered the influence of planetary rotation on spectral line profiles. The resulting equatorial rotation speed was determined to be $4.4^{+1.6}_{-2.0}\\,\\mathrm{km\\,s^{-1}}$, which agrees with the rotation speed induced by tidal locking.","sentences":["Ultrahot Jupiters are a type of gaseous exoplanet that orbit extremely close to their host star, resulting in significantly high equilibrium temperatures.","In recent years, high-resolution emission spectroscopy has been broadly employed in observing the atmospheres of ultrahot Jupiters.","We used the CARMENES spectrograph to observe the high-resolution spectra of the dayside hemisphere of MASCARA-1b in both visible and near-infrared.","Through cross-correlation analysis, we detected signals of \\ion{Fe}{i} and \\ion{Ti}{i}.","Based on these detections, we conducted an atmospheric retrieval and discovered the presence of a strong inversion layer in the planet's atmosphere.","The retrieved Ti and Fe abundances are broadly consistent with solar abundances.","In particular, we obtained a relative abundance of [Ti/Fe] as $-1.0 \\pm 0.8$ under the free retrieval and $-0.4^{+0.5}_{-0.8}$ under the chemical equilibrium retrieval, suggesting the absence of significant titanium depletion on this planet.","Furthermore, we considered the influence of planetary rotation on spectral line profiles.","The resulting equatorial rotation speed was determined to be $4.4^{+1.6}_{-2.0}\\,\\mathrm{km\\,s^{-1}}$, which agrees with the rotation speed induced by tidal locking."],"url":"http://arxiv.org/abs/2404.18788v1","category":"astro-ph.EP"}
{"created":"2024-04-29 14:41:59","title":"Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification","abstract":"Similar to humans, animals make extensive use of verbal and non-verbal forms of communication, including a large range of audio signals. In this paper, we address dog vocalizations and explore the use of self-supervised speech representation models pre-trained on human speech to address dog bark classification tasks that find parallels in human-centered tasks in speech recognition. We specifically address four tasks: dog recognition, breed identification, gender classification, and context grounding. We show that using speech embedding representations significantly improves over simpler classification baselines. Further, we also find that models pre-trained on large human speech acoustics can provide additional performance boosts on several tasks.","sentences":["Similar to humans, animals make extensive use of verbal and non-verbal forms of communication, including a large range of audio signals.","In this paper, we address dog vocalizations and explore the use of self-supervised speech representation models pre-trained on human speech to address dog bark classification tasks that find parallels in human-centered tasks in speech recognition.","We specifically address four tasks: dog recognition, breed identification, gender classification, and context grounding.","We show that using speech embedding representations significantly improves over simpler classification baselines.","Further, we also find that models pre-trained on large human speech acoustics can provide additional performance boosts on several tasks."],"url":"http://arxiv.org/abs/2404.18739v1","category":"cs.CL"}
{"created":"2024-04-29 14:14:33","title":"Tuning the BCS-BEC crossover of electron-hole pairing with pressure","abstract":"In graphite, a moderate magnetic field confines electrons and holes into their lowest Landau levels. In the extreme quantum limit, two insulating states with a dome-like field dependence of the their critical temperatures are induced by the magnetic field. Here, we study the evolution of the first dome (below 60 T) under hydrostatic pressure up to 1.7 GPa. With increasing pressure, the field-temperature phase boundary shifts towards higher magnetic fields, yet the maximum critical temperature remains unchanged. According to our fermiology data, pressure amplifies the density and the effective mass of hole-like and electron-like carriers. Thanks to this information, we verify the persistent relevance of the BCS relation between the critical temperature and the density of states in the weak-coupling boundary of the dome. In contrast, the strong-coupling summit of the dome does not show any detectable change with pressure. We argue that this is because the out-of-plane BCS coherence length approaches the interplane distance that shows little change with pressure. Thus, the BCS-BEC crossover is tunable by magnetic field and pressure, but with a locked summit.","sentences":["In graphite, a moderate magnetic field confines electrons and holes into their lowest Landau levels.","In the extreme quantum limit, two insulating states with a dome-like field dependence of the their critical temperatures are induced by the magnetic field.","Here, we study the evolution of the first dome (below 60 T) under hydrostatic pressure up to 1.7 GPa.","With increasing pressure, the field-temperature phase boundary shifts towards higher magnetic fields, yet the maximum critical temperature remains unchanged.","According to our fermiology data, pressure amplifies the density and the effective mass of hole-like and electron-like carriers.","Thanks to this information, we verify the persistent relevance of the BCS relation between the critical temperature and the density of states in the weak-coupling boundary of the dome.","In contrast, the strong-coupling summit of the dome does not show any detectable change with pressure.","We argue that this is because the out-of-plane BCS coherence length approaches the interplane distance that shows little change with pressure.","Thus, the BCS-BEC crossover is tunable by magnetic field and pressure, but with a locked summit."],"url":"http://arxiv.org/abs/2404.18727v1","category":"cond-mat.str-el"}
{"created":"2024-04-29 13:58:03","title":"Iconic Gesture Semantics","abstract":"The \"meaning\" of an iconic gesture is conditioned on its informational evaluation. Only informational evaluation lifts a gesture to a quasi-linguistic level that can interact with verbal content. Interaction is either vacuous or regimented by usual lexicon-driven inferences. Informational evaluation is spelled out as extended exemplification (extemplification) in terms of perceptual classification of a gesture's visual iconic model. The iconic model is derived from Frege/Montague-like truth-functional evaluation of a gesture's form within spatially extended domains. We further argue that the perceptual classification of instances of visual communication requires a notion of meaning different from Frege/Montague frameworks. Therefore, a heuristic for gesture interpretation is provided that can guide the working semanticist. In sum, an iconic gesture semantics is introduced which covers the full range from kinematic gesture representations over model-theoretic evaluation to inferential interpretation in dynamic semantic frameworks.","sentences":["The \"meaning\" of an iconic gesture is conditioned on its informational evaluation.","Only informational evaluation lifts a gesture to a quasi-linguistic level that can interact with verbal content.","Interaction is either vacuous or regimented by usual lexicon-driven inferences.","Informational evaluation is spelled out as extended exemplification (extemplification) in terms of perceptual classification of a gesture's visual iconic model.","The iconic model is derived from Frege/Montague-like truth-functional evaluation of a gesture's form within spatially extended domains.","We further argue that the perceptual classification of instances of visual communication requires a notion of meaning different from Frege/Montague frameworks.","Therefore, a heuristic for gesture interpretation is provided that can guide the working semanticist.","In sum, an iconic gesture semantics is introduced which covers the full range from kinematic gesture representations over model-theoretic evaluation to inferential interpretation in dynamic semantic frameworks."],"url":"http://arxiv.org/abs/2404.18708v1","category":"cs.CL"}
{"created":"2024-04-29 13:30:27","title":"Work Smarter...Not Harder: Efficient Minimization of Dependency Length in SOV Languages","abstract":"Dependency length minimization is a universally observed quantitative property of natural languages. However, the extent of dependency length minimization, and the cognitive mechanisms through which the language processor achieves this minimization remain unclear. This research offers mechanistic insights by postulating that moving a short preverbal constituent next to the main verb explains preverbal constituent ordering decisions better than global minimization of dependency length in SOV languages. This approach constitutes a least-effort strategy because it's just one operation but simultaneously reduces the length of all preverbal dependencies linked to the main verb. We corroborate this strategy using large-scale corpus evidence across all seven SOV languages that are prominently represented in the Universal Dependency Treebank. These findings align with the concept of bounded rationality, where decision-making is influenced by 'quick-yet-economical' heuristics rather than exhaustive searches for optimal solutions. Overall, this work sheds light on the role of bounded rationality in linguistic decision-making and language evolution.","sentences":["Dependency length minimization is a universally observed quantitative property of natural languages.","However, the extent of dependency length minimization, and the cognitive mechanisms through which the language processor achieves this minimization remain unclear.","This research offers mechanistic insights by postulating that moving a short preverbal constituent next to the main verb explains preverbal constituent ordering decisions better than global minimization of dependency length in SOV languages.","This approach constitutes a least-effort strategy because it's just one operation but simultaneously reduces the length of all preverbal dependencies linked to the main verb.","We corroborate this strategy using large-scale corpus evidence across all seven SOV languages that are prominently represented in the Universal Dependency Treebank.","These findings align with the concept of bounded rationality, where decision-making is influenced by 'quick-yet-economical' heuristics rather than exhaustive searches for optimal solutions.","Overall, this work sheds light on the role of bounded rationality in linguistic decision-making and language evolution."],"url":"http://arxiv.org/abs/2404.18684v1","category":"cs.CL"}
{"created":"2024-04-29 13:27:20","title":"Human Factors in Model-Driven Engineering: Future Research Goals and Initiatives for MDE","abstract":"Purpose: Software modelling and Model-Driven Engineering (MDE) is traditionally studied from a technical perspective. However, one of the core motivations behind the use of software models is inherently human-centred. Models aim to enable practitioners to communicate about software designs, make software understandable, or make software easier to write through domain-specific modelling languages. Several recent studies challenge the idea that these aims can always be reached and indicate that human factors play a role in the success of MDE. However, there is an under-representation of research focusing on human factors in modelling. Methods: During a GI-Dagstuhl seminar, topics related to human factors in modelling were discussed by 26 expert participants from research and industry. Results: In breakout groups, five topics were covered in depth, namely modelling human aspects, factors of modeller experience, diversity and inclusion in MDE, collaboration and MDE, and teaching human-aware MDE. Conclusion: We summarise our insights gained during the discussions on the five topics. We formulate research goals, questions, and propositions that support directing future initiatives towards an MDE community that is aware of and supportive of human factors and values.","sentences":["Purpose: Software modelling and Model-Driven Engineering (MDE) is traditionally studied from a technical perspective.","However, one of the core motivations behind the use of software models is inherently human-centred.","Models aim to enable practitioners to communicate about software designs, make software understandable, or make software easier to write through domain-specific modelling languages.","Several recent studies challenge the idea that these aims can always be reached and indicate that human factors play a role in the success of MDE.","However, there is an under-representation of research focusing on human factors in modelling.","Methods: During a GI-Dagstuhl seminar, topics related to human factors in modelling were discussed by 26 expert participants from research and industry.","Results:","In breakout groups, five topics were covered in depth, namely modelling human aspects, factors of modeller experience, diversity and inclusion in MDE, collaboration and MDE, and teaching human-aware MDE.","Conclusion: We summarise our insights gained during the discussions on the five topics.","We formulate research goals, questions, and propositions that support directing future initiatives towards an MDE community that is aware of and supportive of human factors and values."],"url":"http://arxiv.org/abs/2404.18682v1","category":"cs.SE"}
{"created":"2024-04-29 13:05:59","title":"Enhancing Uncertain Demand Prediction in Hospitals Using Simple and Advanced Machine Learning","abstract":"Early and timely prediction of patient care demand not only affects effective resource allocation but also influences clinical decision-making as well as patient experience. Accurately predicting patient care demand, however, is a ubiquitous challenge for hospitals across the world due, in part, to the demand's time-varying temporal variability, and, in part, to the difficulty in modelling trends in advance. To address this issue, here, we develop two methods, a relatively simple time-vary linear model, and a more advanced neural network model. The former forecasts patient arrivals hourly over a week based on factors such as day of the week and previous 7-day arrival patterns. The latter leverages a long short-term memory (LSTM) model, capturing non-linear relationships between past data and a three-day forecasting window. We evaluate the predictive capabilities of the two proposed approaches compared to two na\\\"ive approaches - a reduced-rank vector autoregressive (VAR) model and the TBATS model. Using patient care demand data from Rambam Medical Center in Israel, our results show that both proposed models effectively capture hourly variations of patient demand. Additionally, the linear model is more explainable thanks to its simple architecture, whereas, by accurately modelling weekly seasonal trends, the LSTM model delivers lower prediction errors. Taken together, our explorations suggest the utility of machine learning in predicting time-varying patient care demand; additionally, it is possible to predict patient care demand with good accuracy (around 4 patients) three days or a week in advance using machine learning.","sentences":["Early and timely prediction of patient care demand not only affects effective resource allocation but also influences clinical decision-making as well as patient experience.","Accurately predicting patient care demand, however, is a ubiquitous challenge for hospitals across the world due, in part, to the demand's time-varying temporal variability, and, in part, to the difficulty in modelling trends in advance.","To address this issue, here, we develop two methods, a relatively simple time-vary linear model, and a more advanced neural network model.","The former forecasts patient arrivals hourly over a week based on factors such as day of the week and previous 7-day arrival patterns.","The latter leverages a long short-term memory (LSTM) model, capturing non-linear relationships between past data and a three-day forecasting window.","We evaluate the predictive capabilities of the two proposed approaches compared to two na\\\"ive approaches - a reduced-rank vector autoregressive (VAR) model and the TBATS model.","Using patient care demand data from Rambam Medical Center in Israel, our results show that both proposed models effectively capture hourly variations of patient demand.","Additionally, the linear model is more explainable thanks to its simple architecture, whereas, by accurately modelling weekly seasonal trends, the LSTM model delivers lower prediction errors.","Taken together, our explorations suggest the utility of machine learning in predicting time-varying patient care demand; additionally, it is possible to predict patient care demand with good accuracy (around 4 patients) three days or a week in advance using machine learning."],"url":"http://arxiv.org/abs/2404.18670v1","category":"cs.LG"}
{"created":"2024-04-29 12:46:32","title":"Decoherence induced by a sparse bath of two-level fluctuators: peculiar features of $1/f$ noise in high-quality qubits","abstract":"Progress in fabrication of semiconductor and superconductor qubits has greatly diminished the number of decohering defects, thus decreasing the devastating low-frequency $1/f$ noise and extending the qubits' coherence times (dephasing time $T_2^*$ and the echo decay time $T_2$). However, large qubit-to-qubit variation of the coherence properties remains a problem, making it difficult to produce a large-scale register where all qubits have a uniformly high quality. In this work we show that large variability is a characteristic feature of a qubit dephased by a sparse bath made of many ($n\\gg 1$) decohering defects, coupled to the qubit with similar strength. We model the defects as two-level fluctuators (TLFs) whose transition rates $\\gamma$ are sampled from a log-uniform distribution over an interval $[\\gamma_{m},\\gamma_M]$, which is a standard model for $1/f$ noise. We investigate decoherence by such a bath in the limit of high-quality qubit, i.e.\\ when the TLF density $d$ is small (the limit of sparse bath, with $d=n/w\\ll 1$, where $n$ is the number of TLFs and $w=\\ln{[\\gamma_M/\\gamma_{m}]}$ is the log-width of the distribution). We show that different realizations of the bath produce very similar noise power spectra $S(f)\\sim 1/f$, but lead to drastically different coherence times $T_2^*$ and $T_2$. Thus, the spectral density $S(f)$ does not determine coherence of a qubit coupled to a sparse TLF bath, as opposed to a dense bath; instead, decoherence is controlled by only a few exceptional fluctuators, determined by their value of $\\gamma$. We show that removing only two of these TLFs greatly increases $T_2$ and $T_2^*$ times. Our findings help theoretical understanding and further improvements in the coherence properties of semiconductor and superconductor qubits, battling the $1/f$ noise in these platforms.","sentences":["Progress in fabrication of semiconductor and superconductor qubits has greatly diminished the number of decohering defects, thus decreasing the devastating low-frequency $1/f$ noise and extending the qubits' coherence times (dephasing time $T_2^*$ and the echo decay time $T_2$).","However, large qubit-to-qubit variation of the coherence properties remains a problem, making it difficult to produce a large-scale register where all qubits have a uniformly high quality.","In this work we show that large variability is a characteristic feature of a qubit dephased by a sparse bath made of many ($n\\gg 1$) decohering defects, coupled to the qubit with similar strength.","We model the defects as two-level fluctuators (TLFs) whose transition rates $\\gamma$ are sampled from a log-uniform distribution over an interval $[\\gamma_{m},\\gamma_M]$, which is a standard model for $1/f$ noise.","We investigate decoherence by such a bath in the limit of high-quality qubit, i.e.\\ when the TLF density $d$ is small (the limit of sparse bath, with $d=n/w\\ll 1$, where $n$ is the number of TLFs and $w=\\ln{[\\gamma_M/\\gamma_{m}]}$ is the log-width of the distribution).","We show that different realizations of the bath produce very similar noise power spectra $S(f)\\sim 1/f$, but lead to drastically different coherence times $T_2^*$ and $T_2$. Thus, the spectral density $S(f)$ does not determine coherence of a qubit coupled to a sparse TLF bath, as opposed to a dense bath; instead, decoherence is controlled by only a few exceptional fluctuators, determined by their value of $\\gamma$. We show that removing only two of these TLFs greatly increases $T_2$ and $T_2^*$ times.","Our findings help theoretical understanding and further improvements in the coherence properties of semiconductor and superconductor qubits, battling the $1/f$ noise in these platforms."],"url":"http://arxiv.org/abs/2404.18659v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 12:34:47","title":"Bayesian Active Search on Parameter Space: a 95 GeV Spin-0 Resonance in the ($B-L$)SSM","abstract":"In the attempt to explain possible data anomalies from collider experiments in terms of New Physics (NP) models, computationally expensive scans over their parameter spaces are typically required in order to match theoretical predictions to experimental observations. Under the assumption that anomalies seen at a mass of about 95 GeV by the Large Electron-Positron (LEP) and Large Hadron Collider (LHC) experiments correspond to a NP signal, which we attempt to interpret as a spin-0 resonance in the $(B-L)$ Supersymmetric Standard Model ($(B-L)$SSM), chosen as an illustrative example, we introduce a novel Machine Learning (ML) approach based on a multi-objective active search method, called b-CASTOR, able to achieve high sample efficiency and diversity, due to the use of probabilistic surrogate models and a volume based search policy, outperforming competing algorithms, such as those based on Markov-Chain Monte Carlo (MCMC) methods.","sentences":["In the attempt to explain possible data anomalies from collider experiments in terms of New Physics (NP) models, computationally expensive scans over their parameter spaces are typically required in order to match theoretical predictions to experimental observations.","Under the assumption that anomalies seen at a mass of about 95 GeV by the Large Electron-Positron (LEP) and Large Hadron Collider (LHC) experiments correspond to a NP signal, which we attempt to interpret as a spin-0 resonance in the $(B-L)$ Supersymmetric Standard Model ($(B-L)$SSM), chosen as an illustrative example, we introduce a novel Machine Learning (ML) approach based on a multi-objective active search method, called b-CASTOR, able to achieve high sample efficiency and diversity, due to the use of probabilistic surrogate models and a volume based search policy, outperforming competing algorithms, such as those based on Markov-Chain Monte Carlo (MCMC) methods."],"url":"http://arxiv.org/abs/2404.18653v1","category":"hep-ph"}
{"created":"2024-04-29 12:24:43","title":"Low-Overhead Defect-Adaptive Surface Code with Bandage-Like Super-Stabilizers","abstract":"To make practical quantum algorithms work, large-scale quantum processors protected by error-correcting codes are required to resist noise and ensure reliable computational outcomes. However, a major challenge arises from defects in processor fabrication, as well as occasional losses or cosmic rays during the computing process, all of which can lead to qubit malfunctions and disrupt error-correcting codes' normal operations. In this context, we introduce an automatic adapter to implement the surface code on defective lattices. Unlike previous approaches, this adapter leverages newly proposed bandage-like super-stabilizers to save more qubits when defects are clustered, thus enhancing the code distance and reducing super-stabilizer weight. For instance, in comparison with earlier methods, with a code size of 27 and a random defect rate of 2\\%, the disabled qubits decrease by $1/3$, and the average preserved code distance increases by 63\\%. This demonstrates a significant reduction in overhead when handling defects using our approach, and this advantage amplifies with increasing processor size and defect rates. Our work presents a low-overhead, automated solution to the challenge of adapting the surface code to defects, an essential step towards scaling up the construction of large-scale quantum computers for practical applications.","sentences":["To make practical quantum algorithms work, large-scale quantum processors protected by error-correcting codes are required to resist noise and ensure reliable computational outcomes.","However, a major challenge arises from defects in processor fabrication, as well as occasional losses or cosmic rays during the computing process, all of which can lead to qubit malfunctions and disrupt error-correcting codes' normal operations.","In this context, we introduce an automatic adapter to implement the surface code on defective lattices.","Unlike previous approaches, this adapter leverages newly proposed bandage-like super-stabilizers to save more qubits when defects are clustered, thus enhancing the code distance and reducing super-stabilizer weight.","For instance, in comparison with earlier methods, with a code size of 27 and a random defect rate of 2\\%, the disabled qubits decrease by $1/3$, and the average preserved code distance increases by 63\\%.","This demonstrates a significant reduction in overhead when handling defects using our approach, and this advantage amplifies with increasing processor size and defect rates.","Our work presents a low-overhead, automated solution to the challenge of adapting the surface code to defects, an essential step towards scaling up the construction of large-scale quantum computers for practical applications."],"url":"http://arxiv.org/abs/2404.18644v1","category":"quant-ph"}
{"created":"2024-04-29 11:55:43","title":"Analysis for Implicit and Implicit-Explicit ADER and DeC Methods for Ordinary Differential Equations, Advection-Diffusion and Advection-Dispersion Equations","abstract":"In this manuscript, we present the development of implicit and implicit-explicit ADER and DeC methodologies within the DeC framework using the two-operators formulation, with a focus on their stability analysis both as solvers for ordinary differential equations (ODEs) and within the context of linear partial differential equations (PDEs). To analyze their stability, we reinterpret these methods as Runge-Kutta schemes and uncover significant variations in stability behavior, ranging from A-stable to bounded stability regions, depending on the chosen order, method, and quadrature nodes. This differentiation contrasts with their explicit counterparts. When applied to advection-diffusion and advection-dispersion equations employing finite difference spatial discretization, the von Neumann stability analysis demonstrates stability under CFL-like conditions. Particularly noteworthy is the stability maintenance observed for the advection-diffusion equation, even under spatial-independent constraints. Furthermore, we establish precise boundaries for relevant coefficients and provide suggestions regarding the suitability of specific schemes for different problem.","sentences":["In this manuscript, we present the development of implicit and implicit-explicit ADER and DeC methodologies within the DeC framework using the two-operators formulation, with a focus on their stability analysis both as solvers for ordinary differential equations (ODEs) and within the context of linear partial differential equations (PDEs).","To analyze their stability, we reinterpret these methods as Runge-Kutta schemes and uncover significant variations in stability behavior, ranging from A-stable to bounded stability regions, depending on the chosen order, method, and quadrature nodes.","This differentiation contrasts with their explicit counterparts.","When applied to advection-diffusion and advection-dispersion equations employing finite difference spatial discretization, the von Neumann stability analysis demonstrates stability under CFL-like conditions.","Particularly noteworthy is the stability maintenance observed for the advection-diffusion equation, even under spatial-independent constraints.","Furthermore, we establish precise boundaries for relevant coefficients and provide suggestions regarding the suitability of specific schemes for different problem."],"url":"http://arxiv.org/abs/2404.18626v1","category":"math.NA"}
{"created":"2024-04-29 11:19:48","title":"Uncertainties from metrology in the integrated luminosity measurement with the updated design of the very forward region of a detector at CEPC","abstract":"At an $e^{+}e^{-}$ collider machine-detector interface includes, among others, a calorimeter dedicated for precision measurement of the integrated luminosity at a per mill level or better. Usually, such a device is placed at the outgoing beams, to keep the spatial symmetries of the head-on collisions at accelerators with a non-zero crossing angle. At CEPC it is currently proposed that the luminometer is placed on the z-axis. We review a feasibility of precision measurement of the integrated luminosity at the $Z^{0}$ pole, from the point of view of systematic effects arising from metrology uncertainties, either concerning the luminometer position and alignment or the beam properties, with the luminometer centered around the z-axis.","sentences":["At an $e^{+}e^{-}$ collider machine-detector interface includes, among others, a calorimeter dedicated for precision measurement of the integrated luminosity at a per mill level or better.","Usually, such a device is placed at the outgoing beams, to keep the spatial symmetries of the head-on collisions at accelerators with a non-zero crossing angle.","At CEPC it is currently proposed that the luminometer is placed on the z-axis.","We review a feasibility of precision measurement of the integrated luminosity at the $Z^{0}$ pole, from the point of view of systematic effects arising from metrology uncertainties, either concerning the luminometer position and alignment or the beam properties, with the luminometer centered around the z-axis."],"url":"http://arxiv.org/abs/2404.18605v1","category":"hep-ex"}
{"created":"2024-04-29 09:36:41","title":"Implication of odd-even staggering in the charge radii of calcium isotopes","abstract":"Inspired by the evidently observed odd-even staggering and the inverted parabolic-like shape of charge radii along calcium isotopic chain, the ground state properties of calcium isotopes are investigated by constraining the root-mean-square (rms) charge radii under the covariant energy density functionals with effective forces NL3 and PK1. In this work, the pairing correlations are tackled by solving the state-dependent Bardeen-Cooper-Schrieffer equations. The calculated results suggest that the binding energies obtained by constraint method have been reduced less than $0.1\\%$. But for charge radii, the corresponding results deriving from NL3 and PK1 forces have been increased by about $1.0\\%$ and $2.0\\%$, respectively. This means that charge radius is a more sensitive quantity in the calibrated protocol. Meanwhile, it is found that the reproduced charge radii of calcium isotopes is attributed to the rather strong isospin dependence of effective potential. The odd-even oscillation behavior can also be presented in the neutron skin thickness and proton Fermi energy along calcium isotopic chain, but keep opposite trends with respect to the corresponding binding energy and charge radius. As encountered in charge radii, the weakening odd-even oscillation behavior is still emerged from the proton Fermi energies at the neutron numbers $N=20$ and $28$ as well, but not in binding energy and neutron skin thickness.","sentences":["Inspired by the evidently observed odd-even staggering and the inverted parabolic-like shape of charge radii along calcium isotopic chain, the ground state properties of calcium isotopes are investigated by constraining the root-mean-square (rms) charge radii under the covariant energy density functionals with effective forces NL3 and PK1.","In this work, the pairing correlations are tackled by solving the state-dependent Bardeen-Cooper-Schrieffer equations.","The calculated results suggest that the binding energies obtained by constraint method have been reduced less than $0.1\\%$.","But for charge radii, the corresponding results deriving from NL3 and PK1 forces have been increased by about $1.0\\%$ and $2.0\\%$, respectively.","This means that charge radius is a more sensitive quantity in the calibrated protocol.","Meanwhile, it is found that the reproduced charge radii of calcium isotopes is attributed to the rather strong isospin dependence of effective potential.","The odd-even oscillation behavior can also be presented in the neutron skin thickness and proton Fermi energy along calcium isotopic chain, but keep opposite trends with respect to the corresponding binding energy and charge radius.","As encountered in charge radii, the weakening odd-even oscillation behavior is still emerged from the proton Fermi energies at the neutron numbers $N=20$ and $28$ as well, but not in binding energy and neutron skin thickness."],"url":"http://arxiv.org/abs/2404.18545v1","category":"nucl-th"}
{"created":"2024-04-29 09:25:18","title":"Adaptive (re)operations facilitate environmental flow maintenance downstream of multi-purpose reservoirs","abstract":"Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services. However, impoundment of water impacts instream aquatic ecosystems. Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases. To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry. A related issue is - how does the order in which releases are prioritized influences sectoral performances? We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India. We formulate two versions of a multi-objective decision problem. PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases. PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases. Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives. We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals.","sentences":["Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services.","However, impoundment of water impacts instream aquatic ecosystems.","Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.","But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases.","To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry.","A related issue is - how does the order in which releases are prioritized influences sectoral performances?","We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India.","We formulate two versions of a multi-objective decision problem.","PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases.","PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases.","Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives.","We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals."],"url":"http://arxiv.org/abs/2404.18535v1","category":"math.OC"}
{"created":"2024-04-29 09:09:11","title":"Did Fourier Really Meet M\u00f6bius? Fast Subset Convolution via FFT","abstract":"In their seminal work on subset convolution, Bj\\\"orklund, Husfeldt, Kaski and Koivisto introduced the now well-known $O(2^n n^2)$-time evaluation of the subset convolution in the sum-product ring. This sparked a wave of remarkable results for fundamental problems, such as the minimum Steiner tree and the chromatic number. However, in spite of its theoretical improvement, large intermediate outputs and floating-point precision errors due to alternating addition and subtraction in its set function transforms make the algorithm unusable in practice.   We provide a simple FFT-based algorithm that completely eliminates the need for set function transforms and maintains the running time of the original algorithm. This makes it possible to take advantage of nearly sixty years of research on efficient FFT implementations.","sentences":["In their seminal work on subset convolution, Bj\\\"orklund, Husfeldt, Kaski and Koivisto introduced the now well-known $O(2^n n^2)$-time evaluation of the subset convolution in the sum-product ring.","This sparked a wave of remarkable results for fundamental problems, such as the minimum Steiner tree and the chromatic number.","However, in spite of its theoretical improvement, large intermediate outputs and floating-point precision errors due to alternating addition and subtraction in its set function transforms make the algorithm unusable in practice.   ","We provide a simple FFT-based algorithm that completely eliminates the need for set function transforms and maintains the running time of the original algorithm.","This makes it possible to take advantage of nearly sixty years of research on efficient FFT implementations."],"url":"http://arxiv.org/abs/2404.18522v1","category":"cs.DS"}
{"created":"2024-04-29 08:59:21","title":"Evolution of secondary electron spectrum during cosmic-ray discharge in the universe","abstract":"We recently found that streaming cosmic rays (CRs) induce a resistive electric field that can accelerate secondary electrons produced by CR ionization. In this work, we study the evolution of the energy spectrum of secondary electrons by numerically solving the one-dimensional Boltzmann equation and Ohm's law. We show that the accelerated secondary electrons further ionize a gas, that is, the electron avalanche occurs, resulting in increased ionization and excitation of the gas. Although the resistive electric field becomes weaker than one before the CR discharge, the weak resistive electric field weakly accelerates the secondary electrons. The quasi-steady state is almost independent of the initial resistive electric field, but depends on the electron fraction in the gas. The resistive electric field in the quasi-steady state is larger for the higher electron fraction, which makes the number of secondary electrons that can ionize the gas larger, resulting in a higher ionization rate. The CR discharge could explain the high ionization rate that are observed in some molecular clouds.","sentences":["We recently found that streaming cosmic rays (CRs) induce a resistive electric field that can accelerate secondary electrons produced by CR ionization.","In this work, we study the evolution of the energy spectrum of secondary electrons by numerically solving the one-dimensional Boltzmann equation and Ohm's law.","We show that the accelerated secondary electrons further ionize a gas, that is, the electron avalanche occurs, resulting in increased ionization and excitation of the gas.","Although the resistive electric field becomes weaker than one before the CR discharge, the weak resistive electric field weakly accelerates the secondary electrons.","The quasi-steady state is almost independent of the initial resistive electric field, but depends on the electron fraction in the gas.","The resistive electric field in the quasi-steady state is larger for the higher electron fraction, which makes the number of secondary electrons that can ionize the gas larger, resulting in a higher ionization rate.","The CR discharge could explain the high ionization rate that are observed in some molecular clouds."],"url":"http://arxiv.org/abs/2404.18513v1","category":"astro-ph.HE"}
{"created":"2024-04-29 07:33:05","title":"Long-time behaviour of supercritical finite circular mechanism branching processes","abstract":"This paper concentrates on the limit behavior of discrete-time branching process with circular mechanism. Three types of limit behaviour of discrete-time branching process with circular mechanism are given explicitly under various moment conditions on branching rates. It is shown that the rate of the first one is geometric, while the other two are supergeometric.","sentences":["This paper concentrates on the limit behavior of discrete-time branching process with circular mechanism.","Three types of limit behaviour of discrete-time branching process with circular mechanism are given explicitly under various moment conditions on branching rates.","It is shown that the rate of the first one is geometric, while the other two are supergeometric."],"url":"http://arxiv.org/abs/2404.18478v1","category":"math.PR"}
{"created":"2024-04-29 07:25:57","title":"Robust $\u03bc$-distortion constraints on primordial supermassive black holes from non-Gaussian perturbations","abstract":"Explaining the origin of supermassive black holes via a primordial origin is severely challenged by the tight spectral distortion constraints on the amplitude of the primordial perturbations. Following the first calculation of how the $\\mu$ constraints are modified by non-Gaussianity in a companion paper, we here make the first robust constraints on primordial black hole formation under large non-Gaussianity. Even the infinite $f_{\\rm NL}$ limit is insufficiently non-Gaussian but much higher-order non-Gaussianity of the form ${\\cal R}={\\cal R}_{\\rm G}^5$ may allow the formation of any mass primordial black hole without conflicting with distortion constraints. We caution that such extreme models face other challenges.","sentences":["Explaining the origin of supermassive black holes via a primordial origin is severely challenged by the tight spectral distortion constraints on the amplitude of the primordial perturbations.","Following the first calculation of how the $\\mu$ constraints are modified by non-Gaussianity in a companion paper, we here make the first robust constraints on primordial black hole formation under large non-Gaussianity.","Even the infinite $f_{\\rm NL}$ limit is insufficiently non-Gaussian but much higher-order non-Gaussianity of the form ${\\cal R}={\\cal R}_{\\rm G}^5$ may allow the formation of any mass primordial black hole without conflicting with distortion constraints.","We caution that such extreme models face other challenges."],"url":"http://arxiv.org/abs/2404.18475v1","category":"astro-ph.CO"}
{"created":"2024-04-29 07:25:52","title":"Spectral distortions from acoustic dissipation with non-Gaussian (or not) perturbations","abstract":"A well-known route to form primordial black holes in the early universe relies on the existence of unusually large primordial curvature fluctuations, confined to a narrow range of wavelengths that would be too small to be constrained by Cosmic Microwave Background (CMB) anisotropies. This scenario would however boost the generation of $\\mu$-type spectral distortions in the CMB due to an enhanced dissipation of acoustic waves. Previous studies of $\\mu$-distortion bounds on the primordial spectrum were based on the assumptions of Gaussian primordial fluctuations. In this work, we push the calculation of $\\mu$-distortions to one higher order in photon anisotropies. We discuss how to derive bounds on primordial spectrum peaks obeying non-Gaussian statistics under the assumption of local (perturbative or not) non-Gaussianity. We find that, depending on the value of the peak scale, the bounds may either remain stable or get tighter by several orders of magnitude, but only when the departure from Gaussian statistics is very strong. Our results are translated in terms of bounds on primordial supermassive black hole mass in a companion paper.","sentences":["A well-known route to form primordial black holes in the early universe relies on the existence of unusually large primordial curvature fluctuations, confined to a narrow range of wavelengths that would be too small to be constrained by Cosmic Microwave Background (CMB) anisotropies.","This scenario would however boost the generation of $\\mu$-type spectral distortions in the CMB due to an enhanced dissipation of acoustic waves.","Previous studies of $\\mu$-distortion bounds on the primordial spectrum were based on the assumptions of Gaussian primordial fluctuations.","In this work, we push the calculation of $\\mu$-distortions to one higher order in photon anisotropies.","We discuss how to derive bounds on primordial spectrum peaks obeying non-Gaussian statistics under the assumption of local (perturbative or not) non-Gaussianity.","We find that, depending on the value of the peak scale, the bounds may either remain stable or get tighter by several orders of magnitude, but only when the departure from Gaussian statistics is very strong.","Our results are translated in terms of bounds on primordial supermassive black hole mass in a companion paper."],"url":"http://arxiv.org/abs/2404.18474v1","category":"astro-ph.CO"}
{"created":"2024-04-29 07:10:37","title":"Dominance between combinations of infinite-mean Pareto random variables","abstract":"We study stochastic dominance between portfolios of independent and identically distributed (iid) extremely heavy-tailed (i.e., infinite-mean) Pareto random variables. With the notion of majorization order, we show that a more diversified portfolio of iid extremely heavy-tailed Pareto random variables is larger in the sense of first-order stochastic dominance. This result is further generalized for Pareto random variables caused by triggering events, random variables with tails being Pareto, bounded Pareto random variables, and positively dependent Pareto random variables. These results provide an important implication in investment: Diversification of extremely heavy-tailed Pareto profits uniformly increases investors' profitability, leading to a diversification benefit. Remarkably, different from the finite-mean setting, such a diversification benefit does not depend on the decision maker's risk aversion.","sentences":["We study stochastic dominance between portfolios of independent and identically distributed (iid) extremely heavy-tailed (i.e., infinite-mean)","Pareto random variables.","With the notion of majorization order, we show that a more diversified portfolio of iid extremely heavy-tailed Pareto random variables is larger in the sense of first-order stochastic dominance.","This result is further generalized for Pareto random variables caused by triggering events, random variables with tails being Pareto, bounded Pareto random variables, and positively dependent Pareto random variables.","These results provide an important implication in investment: Diversification of extremely heavy-tailed Pareto profits uniformly increases investors' profitability, leading to a diversification benefit.","Remarkably, different from the finite-mean setting, such a diversification benefit does not depend on the decision maker's risk aversion."],"url":"http://arxiv.org/abs/2404.18467v1","category":"q-fin.PM"}
{"created":"2024-04-29 06:25:00","title":"Equivalence Checking of Parameterised Quantum Circuits","abstract":"Parameterised quantum circuits (PQCs) hold great promise for demonstrating quantum advantages in practical applications of quantum computation. Examples of successful applications include the variational quantum eigensolver, the quantum approximate optimisation algorithm, and quantum machine learning. However, before executing PQCs on real quantum devices, they undergo compilation and optimisation procedures. Given the inherent error-proneness of these processes, it becomes crucial to verify the equivalence between the original PQC and its compiled or optimised version. Unfortunately, most existing quantum circuit verifiers cannot directly handle parameterised quantum circuits; instead, they require parameter substitution to perform verification.   In this paper, we address the critical challenge of equivalence checking for PQCs. We propose a novel compact representation for PQCs based on tensor decision diagrams. Leveraging this representation, we present an algorithm for verifying PQC equivalence without the need for instantiation. Our approach ensures both effectiveness and efficiency, as confirmed by experimental evaluations. The decision-diagram representations offer a powerful tool for analysing and verifying parameterised quantum circuits, bridging the gap between theoretical models and practical implementations.","sentences":["Parameterised quantum circuits (PQCs) hold great promise for demonstrating quantum advantages in practical applications of quantum computation.","Examples of successful applications include the variational quantum eigensolver, the quantum approximate optimisation algorithm, and quantum machine learning.","However, before executing PQCs on real quantum devices, they undergo compilation and optimisation procedures.","Given the inherent error-proneness of these processes, it becomes crucial to verify the equivalence between the original PQC and its compiled or optimised version.","Unfortunately, most existing quantum circuit verifiers cannot directly handle parameterised quantum circuits; instead, they require parameter substitution to perform verification.   ","In this paper, we address the critical challenge of equivalence checking for PQCs.","We propose a novel compact representation for PQCs based on tensor decision diagrams.","Leveraging this representation, we present an algorithm for verifying PQC equivalence without the need for instantiation.","Our approach ensures both effectiveness and efficiency, as confirmed by experimental evaluations.","The decision-diagram representations offer a powerful tool for analysing and verifying parameterised quantum circuits, bridging the gap between theoretical models and practical implementations."],"url":"http://arxiv.org/abs/2404.18456v1","category":"quant-ph"}
{"created":"2024-04-29 06:17:56","title":"MFP: Making Full Use of Probability Maps for Interactive Image Segmentation","abstract":"In recent interactive segmentation algorithms, previous probability maps are used as network input to help predictions in the current segmentation round. However, despite the utilization of previous masks, useful information contained in the probability maps is not well propagated to the current predictions. In this paper, to overcome this limitation, we propose a novel and effective algorithm for click-based interactive image segmentation, called MFP, which attempts to make full use of probability maps. We first modulate previous probability maps to enhance their representations of user-specified objects. Then, we feed the modulated probability maps as additional input to the segmentation network. We implement the proposed MFP algorithm based on the ResNet-34, HRNet-18, and ViT-B backbones and assess the performance extensively on various datasets. It is demonstrated that MFP meaningfully outperforms the existing algorithms using identical backbones. The source codes are available at \\href{https://github.com/cwlee00/MFP}{https://github.com/cwlee00/MFP}.","sentences":["In recent interactive segmentation algorithms, previous probability maps are used as network input to help predictions in the current segmentation round.","However, despite the utilization of previous masks, useful information contained in the probability maps is not well propagated to the current predictions.","In this paper, to overcome this limitation, we propose a novel and effective algorithm for click-based interactive image segmentation, called MFP, which attempts to make full use of probability maps.","We first modulate previous probability maps to enhance their representations of user-specified objects.","Then, we feed the modulated probability maps as additional input to the segmentation network.","We implement the proposed MFP algorithm based on the ResNet-34, HRNet-18, and ViT-B backbones and assess the performance extensively on various datasets.","It is demonstrated that MFP meaningfully outperforms the existing algorithms using identical backbones.","The source codes are available at \\href{https://github.com/cwlee00/MFP}{https://github.com/cwlee00/MFP}."],"url":"http://arxiv.org/abs/2404.18448v1","category":"cs.CV"}
{"created":"2024-04-29 04:53:09","title":"Design of Tunable Perfect Absorbers in the Mid-IR Spectrum Using Graphene-Based Multilayer Structures: Emerging Applications in Atmospheric Window Matching","abstract":"This paper introduces tunable and switchable Perfect Absorbers (PAs) operating within the mid-infrared spectrum, specifically targeting the 3 to 5 um range at 0.25 um intervals. This spectrum is engineered for minimal atmospheric absorption and unique transmission characteristics. Our approach uses graphene-based nanophotonic aperiodic multilayer structures, optimized through micro-genetic algorithms within an inverse design framework. This combination broadens the design space, enabling highly accurate absorption control. Using the Transfer-Matrix-Method for simulations, we tailor absorption characteristics while keeping the structures under 2 um thick. Our results demonstrate PA tunability and switchability by adjusting graphene layers' chemical potentials. For instance, a 4 um peak can shift to 4.22 um by changing the graphene potential from 0 eV to 1 eV, without reducing efficiency. Our research also reveals PA adaptability to incident angles, maintaining 90% absorption up to 52 degrees, demonstrating versatility for applications like thermal photovoltaics, sensors, and stealth technology. This research deepens our understanding of nanophotonic materials and advances optical devices for the mid-IR range.","sentences":["This paper introduces tunable and switchable Perfect Absorbers (PAs) operating within the mid-infrared spectrum, specifically targeting the 3 to 5 um range at 0.25 um intervals.","This spectrum is engineered for minimal atmospheric absorption and unique transmission characteristics.","Our approach uses graphene-based nanophotonic aperiodic multilayer structures, optimized through micro-genetic algorithms within an inverse design framework.","This combination broadens the design space, enabling highly accurate absorption control.","Using the Transfer-Matrix-Method for simulations, we tailor absorption characteristics while keeping the structures under 2 um thick.","Our results demonstrate PA tunability and switchability by adjusting graphene layers' chemical potentials.","For instance, a 4 um peak can shift to 4.22 um by changing the graphene potential from 0 eV to 1 eV, without reducing efficiency.","Our research also reveals PA adaptability to incident angles, maintaining 90% absorption up to 52 degrees, demonstrating versatility for applications like thermal photovoltaics, sensors, and stealth technology.","This research deepens our understanding of nanophotonic materials and advances optical devices for the mid-IR range."],"url":"http://arxiv.org/abs/2404.18425v1","category":"physics.app-ph"}
{"created":"2024-04-29 04:15:11","title":"Decomposition Model Assisted Energy-Saving Design in Radio Access Network","abstract":"The continuous emergence of novel services and massive connections involve huge energy consumption towards ultra-dense radio access networks. Moreover, there exist much more number of controllable parameters that can be adjusted to reduce the energy consumption from a network-wide perspective. However, a network-level energy-saving intent usually contains multiple network objectives and constraints. Therefore, it is critical to decompose a network-level energy-saving intent into multiple levels of configurated operations from a top-down refinement perspective. In this work, we utilize a softgoal interdependency graph decomposition model to assist energy-saving scheme design. Meanwhile, we propose an energy-saving approach based on deep Q-network, which achieve a better trade-off among the energy consumption, the throughput, and the first packet delay. In addition, we illustrate how the decomposition model can assist in making energy-saving decisions. Evaluation results demonstrate the performance gain of the proposed scheme in accelerating the model training process.","sentences":["The continuous emergence of novel services and massive connections involve huge energy consumption towards ultra-dense radio access networks.","Moreover, there exist much more number of controllable parameters that can be adjusted to reduce the energy consumption from a network-wide perspective.","However, a network-level energy-saving intent usually contains multiple network objectives and constraints.","Therefore, it is critical to decompose a network-level energy-saving intent into multiple levels of configurated operations from a top-down refinement perspective.","In this work, we utilize a softgoal interdependency graph decomposition model to assist energy-saving scheme design.","Meanwhile, we propose an energy-saving approach based on deep Q-network, which achieve a better trade-off among the energy consumption, the throughput, and the first packet delay.","In addition, we illustrate how the decomposition model can assist in making energy-saving decisions.","Evaluation results demonstrate the performance gain of the proposed scheme in accelerating the model training process."],"url":"http://arxiv.org/abs/2404.18418v1","category":"cs.NI"}
{"created":"2024-04-29 04:14:02","title":"Domain Reasoning in TopKAT","abstract":"TopKAT is the algebraic theory of Kleene algebra with tests (KAT) extended with a top element. Compared to KAT, one pleasant feature of TopKAT is that, in relational models, the top element allows us to express the domain and codomain of a relation. This enables several applications in program logics, such as proving under-approximate specifications or reachability properties of imperative programs. However, while TopKAT inherits many pleasant features of KATs, such as having a decidable equational theory, it is incomplete with respect to relational models. In other words, there are properties that hold true of all relational TopKATs but cannot be proved with the axioms of TopKAT. This issue is potentially worrisome for program-logic applications, in which relational models play a key role.   In this paper, we further investigate the completeness properties of TopKAT with respect to relational models. We show that TopKAT is complete with respect to (co)domain comparison of KAT terms, but incomplete when comparing the (co)domain of arbitrary TopKAT terms. Since the encoding of under-approximate specifications in TopKAT hinges on this type of formula, the aforementioned incompleteness results have a limited impact when using TopKAT to reason about such specifications.","sentences":["TopKAT is the algebraic theory of Kleene algebra with tests (KAT) extended with a top element.","Compared to KAT, one pleasant feature of TopKAT is that, in relational models, the top element allows us to express the domain and codomain of a relation.","This enables several applications in program logics, such as proving under-approximate specifications or reachability properties of imperative programs.","However, while TopKAT inherits many pleasant features of KATs, such as having a decidable equational theory, it is incomplete with respect to relational models.","In other words, there are properties that hold true of all relational TopKATs but cannot be proved with the axioms of TopKAT.","This issue is potentially worrisome for program-logic applications, in which relational models play a key role.   ","In this paper, we further investigate the completeness properties of TopKAT with respect to relational models.","We show that TopKAT is complete with respect to (co)domain comparison of KAT terms, but incomplete when comparing the (co)domain of arbitrary TopKAT terms.","Since the encoding of under-approximate specifications in TopKAT hinges on this type of formula, the aforementioned incompleteness results have a limited impact when using TopKAT to reason about such specifications."],"url":"http://arxiv.org/abs/2404.18417v1","category":"cs.PL"}
{"created":"2024-04-29 04:10:38","title":"Photo-dynamical Analysis of Circumbinary Multi-planet system TOI-1338: a Fully Coplanar Configuration with a Puffy Planet","abstract":"TOI-1338 is the first circumbinary planet system discovered by TESS. It has one transiting planet at P$\\sim$95 day and an outer non-transiting planet at P$\\sim$215 day complemented by RV observation. Here we present a global photo-dynamical modeling of the TOI-1338 system that self-consistently accounts for the mutual gravitational interactions between all known bodies in the system. As a result, the three-dimensional architecture of the system can be established by comparing the model with additional data from TESS Extended Mission and published HARPS/ESPRESSO radial velocity data. We report an inconsistency of binary RV signal between HARPS and ESPRESSO, which could be due to the contamination of the secondary star. According to stability analysis, the RV data via ESPRESSO is preferred. Our results are summarized as follows: (1) the inner transiting planet is extremely coplanar to the binary plane $\\Delta I_b \\sim 0.12 ^\\circ$, making it a permanently transiting circumbinary planet at any nodal precession phases. We updated the future transit ephemerides with improved precisions. (2) The outer planet, despite its non-transiting nature, is also coplanar with the binary plane by $\\Delta I_c=9.1^{+6.0 \\circ}_{-4.8}$ (22$^\\circ$ for 99\\% upper limit). (3) The inner planet could have a density of $0.137 \\pm 0.026$ g/cm$^{-3}$. With a TESS magnitude of 11.45, TOI-1338 b is an optimal circumbinary planet for ground-based follow-up and transit spectroscopy.","sentences":["TOI-1338 is the first circumbinary planet system discovered by TESS.","It has one transiting planet at P$\\sim$95 day and an outer non-transiting planet at P$\\sim$215 day complemented by RV observation.","Here we present a global photo-dynamical modeling of the TOI-1338 system that self-consistently accounts for the mutual gravitational interactions between all known bodies in the system.","As a result, the three-dimensional architecture of the system can be established by comparing the model with additional data from TESS Extended Mission and published HARPS/ESPRESSO radial velocity data.","We report an inconsistency of binary RV signal between HARPS and ESPRESSO, which could be due to the contamination of the secondary star.","According to stability analysis, the RV data via ESPRESSO is preferred.","Our results are summarized as follows: (1) the inner transiting planet is extremely coplanar to the binary plane $\\Delta I_b \\sim 0.12 ^\\circ$, making it a permanently transiting circumbinary planet at any nodal precession phases.","We updated the future transit ephemerides with improved precisions.","(2) The outer planet, despite its non-transiting nature, is also coplanar with the binary plane by $\\Delta I_c=9.1^{+6.0 \\circ}_{-4.8}$ (22$^\\circ$ for 99\\% upper limit).","(3) The inner planet could have a density of $0.137 \\pm 0.026$ g/cm$^{-3}$. With a TESS magnitude of 11.45, TOI-1338 b is an optimal circumbinary planet for ground-based follow-up and transit spectroscopy."],"url":"http://arxiv.org/abs/2404.18415v1","category":"astro-ph.EP"}
{"created":"2024-04-29 04:00:19","title":"Multi-modal Perception Dataset of In-water Objects for Autonomous Surface Vehicles","abstract":"This paper introduces the first publicly accessible multi-modal perception dataset for autonomous maritime navigation, focusing on in-water obstacles within the aquatic environment to enhance situational awareness for Autonomous Surface Vehicles (ASVs). This dataset, consisting of diverse objects encountered under varying environmental conditions, aims to bridge the research gap in marine robotics by providing a multi-modal, annotated, and ego-centric perception dataset, for object detection and classification. We also show the applicability of the proposed dataset's framework using deep learning-based open-source perception algorithms that have shown success. We expect that our dataset will contribute to development of the marine autonomy pipeline and marine (field) robotics. Please note this is a work-in-progress paper about our on-going research that we plan to release in full via future publication.","sentences":["This paper introduces the first publicly accessible multi-modal perception dataset for autonomous maritime navigation, focusing on in-water obstacles within the aquatic environment to enhance situational awareness for Autonomous Surface Vehicles (ASVs).","This dataset, consisting of diverse objects encountered under varying environmental conditions, aims to bridge the research gap in marine robotics by providing a multi-modal, annotated, and ego-centric perception dataset, for object detection and classification.","We also show the applicability of the proposed dataset's framework using deep learning-based open-source perception algorithms that have shown success.","We expect that our dataset will contribute to development of the marine autonomy pipeline and marine (field) robotics.","Please note this is a work-in-progress paper about our on-going research that we plan to release in full via future publication."],"url":"http://arxiv.org/abs/2404.18411v1","category":"cs.RO"}
{"created":"2024-04-29 03:38:56","title":"The defect b-theorem under bulk RG flows","abstract":"It is known that for RG flows confined to a two-dimensional defect, where the bulk maintains its conformal nature, the coefficient of the Euler density in the defect's Weyl anomaly (termed b) cannot increase as the flow progresses from the ultraviolet to the infrared, a principle known as the b-theorem. In this paper, we investigate whether this theorem still holds when the bulk, instead of being critical, also undergoes an RG flow. To address this question, we examine two distinct and perturbatively tractable examples. Our analysis reveals that a straightforward extension of the b-theorem to these cases of RG flows fails.","sentences":["It is known that for RG flows confined to a two-dimensional defect, where the bulk maintains its conformal nature, the coefficient of the Euler density in the defect's Weyl anomaly (termed b) cannot increase as the flow progresses from the ultraviolet to the infrared, a principle known as the b-theorem.","In this paper, we investigate whether this theorem still holds when the bulk, instead of being critical, also undergoes an RG flow.","To address this question, we examine two distinct and perturbatively tractable examples.","Our analysis reveals that a straightforward extension of the b-theorem to these cases of RG flows fails."],"url":"http://arxiv.org/abs/2404.18403v1","category":"hep-th"}
{"created":"2024-04-29 03:36:05","title":"Spectral-Spatial Mamba for Hyperspectral Image Classification","abstract":"Recently, deep learning models have achieved excellent performance in hyperspectral image (HSI) classification. Among the many deep models, Transformer has gradually attracted interest for its excellence in modeling the long-range dependencies of spatial-spectral features in HSI. However, Transformer has the problem of quadratic computational complexity due to the self-attention mechanism, which is heavier than other models and thus has limited adoption in HSI processing. Fortunately, the recently emerging state space model-based Mamba shows great computational efficiency while achieving the modeling power of Transformers. Therefore, in this paper, we make a preliminary attempt to apply the Mamba to HSI classification, leading to the proposed spectral-spatial Mamba (SS-Mamba). Specifically, the proposed SS-Mamba mainly consists of spectral-spatial token generation module and several stacked spectral-spatial Mamba blocks. Firstly, the token generation module converts any given HSI cube to spatial and spectral tokens as sequences. And then these tokens are sent to stacked spectral-spatial mamba blocks (SS-MB). Each SS-MB block consists of two basic mamba blocks and a spectral-spatial feature enhancement module. The spatial and spectral tokens are processed separately by the two basic mamba blocks, respectively. Besides, the feature enhancement module modulates spatial and spectral tokens using HSI sample's center region information. In this way, the spectral and spatial tokens cooperate with each other and achieve information fusion within each block. The experimental results conducted on widely used HSI datasets reveal that the proposed model achieves competitive results compared with the state-of-the-art methods. The Mamba-based method opens a new window for HSI classification.","sentences":["Recently, deep learning models have achieved excellent performance in hyperspectral image (HSI) classification.","Among the many deep models, Transformer has gradually attracted interest for its excellence in modeling the long-range dependencies of spatial-spectral features in HSI.","However, Transformer has the problem of quadratic computational complexity due to the self-attention mechanism, which is heavier than other models and thus has limited adoption in HSI processing.","Fortunately, the recently emerging state space model-based Mamba shows great computational efficiency while achieving the modeling power of Transformers.","Therefore, in this paper, we make a preliminary attempt to apply the Mamba to HSI classification, leading to the proposed spectral-spatial Mamba (SS-Mamba).","Specifically, the proposed SS-Mamba mainly consists of spectral-spatial token generation module and several stacked spectral-spatial Mamba blocks.","Firstly, the token generation module converts any given HSI cube to spatial and spectral tokens as sequences.","And then these tokens are sent to stacked spectral-spatial mamba blocks (SS-MB).","Each SS-MB block consists of two basic mamba blocks and a spectral-spatial feature enhancement module.","The spatial and spectral tokens are processed separately by the two basic mamba blocks, respectively.","Besides, the feature enhancement module modulates spatial and spectral tokens using HSI sample's center region information.","In this way, the spectral and spatial tokens cooperate with each other and achieve information fusion within each block.","The experimental results conducted on widely used HSI datasets reveal that the proposed model achieves competitive results compared with the state-of-the-art methods.","The Mamba-based method opens a new window for HSI classification."],"url":"http://arxiv.org/abs/2404.18401v1","category":"cs.CV"}
{"created":"2024-04-29 02:55:54","title":"SPECIAL: Synopsis Assisted Secure Collaborative Analytics","abstract":"Secure collaborative analytics (SCA) enable the processing of analytical SQL queries across multiple owners' data, even when direct data sharing is not feasible. Although essential for strong privacy, the large overhead from data-oblivious primitives in traditional SCA has hindered its practical adoption. Recent SCA variants that permit controlled leakages under differential privacy (DP) show a better balance between privacy and efficiency. However, they still face significant challenges, such as potentially unbounded privacy loss, suboptimal query planning, and lossy processing. To address these challenges, we introduce SPECIAL, the first SCA system that simultaneously ensures bounded privacy loss, advanced query planning, and lossless processing. SPECIAL employs a novel synopsis-assisted secure processing model, where a one-time privacy cost is spent to acquire private synopses (table statistics) from owner data. These synopses then allow SPECIAL to estimate (compaction) sizes for secure operations (e.g., filter, join) and index encrypted data without extra privacy loss. Crucially, these estimates and indexes can be prepared before runtime, thereby facilitating efficient query planning and accurate cost estimations. Moreover, by using one-sided noise mechanisms and private upper bound techniques, SPECIAL ensures strict lossless processing for complex queries (e.g., multi-join). Through a comprehensive benchmark, we show that SPECIAL significantly outperforms cutting-edge SCAs, with up to 80X faster query times and over 900X smaller memory for complex queries. Moreover, it also achieves up to an 89X reduction in privacy loss under continual processing.","sentences":["Secure collaborative analytics (SCA) enable the processing of analytical SQL queries across multiple owners' data, even when direct data sharing is not feasible.","Although essential for strong privacy, the large overhead from data-oblivious primitives in traditional SCA has hindered its practical adoption.","Recent SCA variants that permit controlled leakages under differential privacy (DP) show a better balance between privacy and efficiency.","However, they still face significant challenges, such as potentially unbounded privacy loss, suboptimal query planning, and lossy processing.","To address these challenges, we introduce SPECIAL, the first SCA system that simultaneously ensures bounded privacy loss, advanced query planning, and lossless processing.","SPECIAL employs a novel synopsis-assisted secure processing model, where a one-time privacy cost is spent to acquire private synopses (table statistics) from owner data.","These synopses then allow SPECIAL to estimate (compaction) sizes for secure operations (e.g., filter, join) and index encrypted data without extra privacy loss.","Crucially, these estimates and indexes can be prepared before runtime, thereby facilitating efficient query planning and accurate cost estimations.","Moreover, by using one-sided noise mechanisms and private upper bound techniques, SPECIAL ensures strict lossless processing for complex queries (e.g., multi-join).","Through a comprehensive benchmark, we show that SPECIAL significantly outperforms cutting-edge SCAs, with up to 80X faster query times and over 900X smaller memory for complex queries.","Moreover, it also achieves up to an 89X reduction in privacy loss under continual processing."],"url":"http://arxiv.org/abs/2404.18388v1","category":"cs.CR"}
{"created":"2024-04-29 02:27:42","title":"Inference for the panel ARMA-GARCH model when both $N$ and $T$ are large","abstract":"We propose a panel ARMA-GARCH model to capture the dynamics of large panel data with $N$ individuals over $T$ time periods. For this model, we provide a two-step estimation procedure to estimate the ARMA parameters and GARCH parameters stepwisely. Under some regular conditions, we show that all of the proposed estimators are asymptotically normal with the convergence rate $(NT)^{-1/2}$, and they have the asymptotic biases when both $N$ and $T$ diverge to infinity at the same rate. Particularly, we find that the asymptotic biases result from the fixed effect, estimation effect, and unobservable initial values. To correct the biases, we further propose the bias-corrected version of estimators by using either the analytical asymptotics or jackknife method. Our asymptotic results are based on a new central limit theorem for the linear-quadratic form in the martingale difference sequence, when the weight matrix is uniformly bounded in row and column. Simulations and one real example are given to demonstrate the usefulness of our panel ARMA-GARCH model.","sentences":["We propose a panel ARMA-GARCH model to capture the dynamics of large panel data with $N$ individuals over $T$ time periods.","For this model, we provide a two-step estimation procedure to estimate the ARMA parameters and GARCH parameters stepwisely.","Under some regular conditions, we show that all of the proposed estimators are asymptotically normal with the convergence rate $(NT)^{-1/2}$, and they have the asymptotic biases when both $N$ and $T$ diverge to infinity at the same rate.","Particularly, we find that the asymptotic biases result from the fixed effect, estimation effect, and unobservable initial values.","To correct the biases, we further propose the bias-corrected version of estimators by using either the analytical asymptotics or jackknife method.","Our asymptotic results are based on a new central limit theorem for the linear-quadratic form in the martingale difference sequence, when the weight matrix is uniformly bounded in row and column.","Simulations and one real example are given to demonstrate the usefulness of our panel ARMA-GARCH model."],"url":"http://arxiv.org/abs/2404.18377v1","category":"stat.ME"}
{"created":"2024-04-29 02:26:38","title":"Supersymmetric spectrum for vector multiplet on Euclidean AdS$_2$","abstract":"Quantum study of supersymmetric theories on Euclidean two dimensional anti-de Sitter space (EAdS$_2$) requires complexified spectrum. For a chiral multiplet, we showed that the spectrum of the Dirac operator acquires a universal shift of $\\text{i}/2$ from the real spectrum to make the supersymmetry between boson and fermion manifest, where both the bosonic and fermionic eigenfunctions are normalizable using an appropriate definition of Euclidean inner product. We extend this analysis to the vector multiplet, where we show that the gaugino requires both $+\\text{i}/2$ and $-\\text{i}/2$ shift from the real spectrum, and there is additional isolated point at vanishing spectral parameter which is mapped by supersymmetry to the boundary zero modes of the vector field. Furthermore, this spectral analysis shows that not every bosonic fields in the vector multiplet can satisfy normalizable boundary condition. Nevertheless, aided by a reorganization of fields into a cohomological form, we find the supersymmetry mapping between bosons and fermions in terms of the expansion coefficients with respect to the newly constructed basis.","sentences":["Quantum study of supersymmetric theories on Euclidean two dimensional anti-de Sitter space (EAdS$_2$) requires complexified spectrum.","For a chiral multiplet, we showed that the spectrum of the Dirac operator acquires a universal shift of $\\text{i}/2$ from the real spectrum to make the supersymmetry between boson and fermion manifest, where both the bosonic and fermionic eigenfunctions are normalizable using an appropriate definition of Euclidean inner product.","We extend this analysis to the vector multiplet, where we show that the gaugino requires both $+\\text{i}/2$ and $-\\text{i}/2$ shift from the real spectrum, and there is additional isolated point at vanishing spectral parameter which is mapped by supersymmetry to the boundary zero modes of the vector field.","Furthermore, this spectral analysis shows that not every bosonic fields in the vector multiplet can satisfy normalizable boundary condition.","Nevertheless, aided by a reorganization of fields into a cohomological form, we find the supersymmetry mapping between bosons and fermions in terms of the expansion coefficients with respect to the newly constructed basis."],"url":"http://arxiv.org/abs/2404.18376v1","category":"hep-th"}
{"created":"2024-04-29 02:24:25","title":"Trajectory Optimization for Adaptive Informative Path Planning with Multimodal Sensing","abstract":"We consider the problem of an autonomous agent equipped with multiple sensors, each with different sensing precision and energy costs. The agent's goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments. The challenge lies in reasoning about the effects of sensing and movement while respecting the agent's resource and dynamic constraints. We formulate the problem as a trajectory optimization problem and solve it using a projection-based trajectory optimization approach where the objective is to reduce the variance of the Gaussian process world belief. Our approach outperforms previous approaches in long horizon trajectories by achieving an overall variance reduction of up to 85% and reducing the root-mean square error in the environment belief by 50%. This approach was developed in support of rover path planning for the NASA VIPER Mission.","sentences":["We consider the problem of an autonomous agent equipped with multiple sensors, each with different sensing precision and energy costs.","The agent's goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments.","The challenge lies in reasoning about the effects of sensing and movement while respecting the agent's resource and dynamic constraints.","We formulate the problem as a trajectory optimization problem and solve it using a projection-based trajectory optimization approach where the objective is to reduce the variance of the Gaussian process world belief.","Our approach outperforms previous approaches in long horizon trajectories by achieving an overall variance reduction of up to 85% and reducing the root-mean square error in the environment belief by 50%.","This approach was developed in support of rover path planning for the NASA VIPER Mission."],"url":"http://arxiv.org/abs/2404.18374v1","category":"cs.RO"}
{"created":"2024-04-29 02:17:31","title":"QANA: LLM-based Question Generation and Network Analysis for Zero-shot Key Point Analysis and Beyond","abstract":"The proliferation of social media has led to information overload and increased interest in opinion mining. We propose \"Question-Answering Network Analysis\" (QANA), a novel opinion mining framework that utilizes Large Language Models (LLMs) to generate questions from users' comments, constructs a bipartite graph based on the comments' answerability to the questions, and applies centrality measures to examine the importance of opinions. We investigate the impact of question generation styles, LLM selections, and the choice of embedding model on the quality of the constructed QA networks by comparing them with annotated Key Point Analysis datasets. QANA achieves comparable performance to previous state-of-the-art supervised models in a zero-shot manner for Key Point Matching task, also reducing the computational cost from quadratic to linear. For Key Point Generation, questions with high PageRank or degree centrality align well with manually annotated key points. Notably, QANA enables analysts to assess the importance of key points from various aspects according to their selection of centrality measure. QANA's primary contribution lies in its flexibility to extract key points from a wide range of perspectives, which enhances the quality and impartiality of opinion mining.","sentences":["The proliferation of social media has led to information overload and increased interest in opinion mining.","We propose \"Question-Answering Network Analysis\" (QANA), a novel opinion mining framework that utilizes Large Language Models (LLMs) to generate questions from users' comments, constructs a bipartite graph based on the comments' answerability to the questions, and applies centrality measures to examine the importance of opinions.","We investigate the impact of question generation styles, LLM selections, and the choice of embedding model on the quality of the constructed QA networks by comparing them with annotated Key Point Analysis datasets.","QANA achieves comparable performance to previous state-of-the-art supervised models in a zero-shot manner for Key Point Matching task, also reducing the computational cost from quadratic to linear.","For Key Point Generation, questions with high PageRank or degree centrality align well with manually annotated key points.","Notably, QANA enables analysts to assess the importance of key points from various aspects according to their selection of centrality measure.","QANA's primary contribution lies in its flexibility to extract key points from a wide range of perspectives, which enhances the quality and impartiality of opinion mining."],"url":"http://arxiv.org/abs/2404.18371v1","category":"cs.CL"}
{"created":"2024-04-29 02:16:24","title":"Out-of-distribution generalization under random, dense distributional shifts","abstract":"Many existing approaches for estimating parameters in settings with distributional shifts operate under an invariance assumption. For example, under covariate shift, it is assumed that p(y|x) remains invariant. We refer to such distribution shifts as sparse, since they may be substantial but affect only a part of the data generating system. In contrast, in various real-world settings, shifts might be dense. More specifically, these dense distributional shifts may arise through numerous small and random changes in the population and environment. First, we will discuss empirical evidence for such random dense distributional shifts and explain why commonly used models for distribution shifts-including adversarial approaches-may not be appropriate under these conditions. Then, we will develop tools to infer parameters and make predictions for partially observed, shifted distributions. Finally, we will apply the framework to several real-world data sets and discuss diagnostics to evaluate the fit of the distributional uncertainty model.","sentences":["Many existing approaches for estimating parameters in settings with distributional shifts operate under an invariance assumption.","For example, under covariate shift, it is assumed that p(y|x) remains invariant.","We refer to such distribution shifts as sparse, since they may be substantial but affect only a part of the data generating system.","In contrast, in various real-world settings, shifts might be dense.","More specifically, these dense distributional shifts may arise through numerous small and random changes in the population and environment.","First, we will discuss empirical evidence for such random dense distributional shifts and explain why commonly used models for distribution shifts-including adversarial approaches-may not be appropriate under these conditions.","Then, we will develop tools to infer parameters and make predictions for partially observed, shifted distributions.","Finally, we will apply the framework to several real-world data sets and discuss diagnostics to evaluate the fit of the distributional uncertainty model."],"url":"http://arxiv.org/abs/2404.18370v1","category":"stat.ME"}
{"created":"2024-04-29 02:08:16","title":"Interface motion from Glauber-Kawasaki dynamics of non-gradient type","abstract":"We consider the Glauber-Kawasaki dynamics on a $d$-dimensional periodic lattice of size $N$, that is, a stochastic time evolution of particles performing random walks with interaction subject to the exclusion rule (Kawasaki part), in general, of non-gradient type, together with the effect of the creation and annihilation of particles (Glauber part) whose rates are set to favor two levels of particle density, called sparse and dense. We then study the limit of our dynamics under the hydrodynamic space-time scaling, that is, $1/N$ in space and a diffusive scaling $N^2$ for the Kawasaki part and another scaling $K=K(N)$, which diverges slower, for the Glauber part in time. In the limit as $N\\to\\infty$, we show that the particles autonomously make phase separation into sparse or dense phases at the microscopic level, and an interface separating two regions is formed at the macroscopic level and evolves under an anisotropic curvature flow.   In the present article, we show that the particle density at the macroscopic level is well approximated by a solution of a reaction-diffusion equation with a nonlinear diffusion term of divergence form and a large reaction term. Furthermore, by applying the results of Funaki, Gu and Wang [arXiv:2404.12234] for the convergence rate of the diffusion matrix approximated by local functions, we obtain a quantitative hydrodynamic limit as well as the upper bound for the allowed diverging speed of $K=K(N)$.   The above result for the derivation of the interface motion is proved by combining our result with that in a companion paper by Funaki and Park [arXiv:2403.01732], in which we analyzed the asymptotic behavior of the solution of the reaction-diffusion equation obtained in the present article and derived an anisotropic curvature flow in the situation where the macroscopic reaction term determined from the Glauber part is bistable and balanced.","sentences":["We consider the Glauber-Kawasaki dynamics on a $d$-dimensional periodic lattice of size $N$, that is, a stochastic time evolution of particles performing random walks with interaction subject to the exclusion rule (Kawasaki part), in general, of non-gradient type, together with the effect of the creation and annihilation of particles (Glauber part) whose rates are set to favor two levels of particle density, called sparse and dense.","We then study the limit of our dynamics under the hydrodynamic space-time scaling, that is, $1/N$ in space and a diffusive scaling $N^2$ for the Kawasaki part and another scaling $K=K(N)$, which diverges slower, for the Glauber part in time.","In the limit as $N\\to\\infty$, we show that the particles autonomously make phase separation into sparse or dense phases at the microscopic level, and an interface separating two regions is formed at the macroscopic level and evolves under an anisotropic curvature flow.   ","In the present article, we show that the particle density at the macroscopic level is well approximated by a solution of a reaction-diffusion equation with a nonlinear diffusion term of divergence form and a large reaction term.","Furthermore, by applying the results of Funaki, Gu and Wang [arXiv:2404.12234] for the convergence rate of the diffusion matrix approximated by local functions, we obtain a quantitative hydrodynamic limit as well as the upper bound for the allowed diverging speed of $K=K(N)$.   The above result for the derivation of the interface motion is proved by combining our result with that in a companion paper by Funaki and Park","[arXiv:2403.01732], in which we analyzed the asymptotic behavior of the solution of the reaction-diffusion equation obtained in the present article and derived an anisotropic curvature flow in the situation where the macroscopic reaction term determined from the Glauber part is bistable and balanced."],"url":"http://arxiv.org/abs/2404.18364v1","category":"math.PR"}
{"created":"2024-04-29 02:02:33","title":"Physics-informed Convolutional Neural Network for Microgrid Economic Dispatch","abstract":"The variability of renewable energy generation and the unpredictability of electricity demand create a need for real-time economic dispatch (ED) of assets in microgrids. However, solving numerical optimization problems in real-time can be incredibly challenging. This study proposes using a convolutional neural network (CNN) based on deep learning to address these challenges. Compared to traditional methods, CNN is more efficient, delivers more dependable results, and has a shorter response time when dealing with uncertainties. While CNN has shown promising results, it does not extract explainable knowledge from the data. To address this limitation, a physics-inspired CNN model is developed by incorporating constraints of the ED problem into the CNN training to ensure that the model follows physical laws while fitting the data. The proposed method can significantly accelerate real-time economic dispatch of microgrids without compromising the accuracy of numerical optimization techniques. The effectiveness of the proposed data-driven approach for optimal allocation of microgrid resources in real-time is verified through a comprehensive comparison with conventional numerical optimization approaches.","sentences":["The variability of renewable energy generation and the unpredictability of electricity demand create a need for real-time economic dispatch (ED) of assets in microgrids.","However, solving numerical optimization problems in real-time can be incredibly challenging.","This study proposes using a convolutional neural network (CNN) based on deep learning to address these challenges.","Compared to traditional methods, CNN is more efficient, delivers more dependable results, and has a shorter response time when dealing with uncertainties.","While CNN has shown promising results, it does not extract explainable knowledge from the data.","To address this limitation, a physics-inspired CNN model is developed by incorporating constraints of the ED problem into the CNN training to ensure that the model follows physical laws while fitting the data.","The proposed method can significantly accelerate real-time economic dispatch of microgrids without compromising the accuracy of numerical optimization techniques.","The effectiveness of the proposed data-driven approach for optimal allocation of microgrid resources in real-time is verified through a comprehensive comparison with conventional numerical optimization approaches."],"url":"http://arxiv.org/abs/2404.18362v1","category":"eess.SY"}
{"created":"2024-04-29 01:51:52","title":"Perovskite topological exciton-polariton disclination laser at room temperature","abstract":"Topologically nontrivial systems can be protected by band topology in momentum space, as seen in topological insulators and semimetals, or real-space topology, such as in lattice deformations known as topological disclinations (TDs). TDs, with inherent chiral symmetry, can support localized states pinned spectrally to the middle of the topological gap, preventing hybridization with bulk bands, and making them promising for topological lasers. Here, we experimentally realize a C4v symmetric TD laser based on perovskite exciton-polariton lattices at room temperature. Protected by the chiral and point group symmetries of the lattice, the TD state emerges in the middle of the gap and at the core of the perovskite lattice. Under a non-resonant pulsed excitation, coherent polariton lasing occurs precisely at the TD state with a low threshold of 9.5 uJ/cm2, as confirmed by momentum space and real space spectra measurements. This study not only introduces a class of symmetry-protected topological lasers, but also expands the landscape for exploring exciton-polariton light-matter interactions with novel topological structures.","sentences":["Topologically nontrivial systems can be protected by band topology in momentum space, as seen in topological insulators and semimetals, or real-space topology, such as in lattice deformations known as topological disclinations (TDs).","TDs, with inherent chiral symmetry, can support localized states pinned spectrally to the middle of the topological gap, preventing hybridization with bulk bands, and making them promising for topological lasers.","Here, we experimentally realize a C4v symmetric TD laser based on perovskite exciton-polariton lattices at room temperature.","Protected by the chiral and point group symmetries of the lattice, the TD state emerges in the middle of the gap and at the core of the perovskite lattice.","Under a non-resonant pulsed excitation, coherent polariton lasing occurs precisely at the TD state with a low threshold of 9.5 uJ/cm2, as confirmed by momentum space and real space spectra measurements.","This study not only introduces a class of symmetry-protected topological lasers, but also expands the landscape for exploring exciton-polariton light-matter interactions with novel topological structures."],"url":"http://arxiv.org/abs/2404.18360v1","category":"physics.optics"}
{"created":"2024-04-29 01:17:36","title":"Maximal Ideals in Commutative Rings and the Axiom of Choice","abstract":"It is well-known that within Zermelo-Fraenkel set theory (ZF), the Axiom of Choice (AC) implies the Maximal Ideal Theorem (MIT), namely that every commutative ring has a maximal ideal. The converse implication MIT $\\Rightarrow$ AC was first proved by Hodges, with subsequent proofs given by Banaschewski and Ern\\'e.   Here we give another derivation of MIT $\\Rightarrow$ AC, aiming to make the exposition self-contained and accessible to non-experts with only introductory familiarity with commutative ring theory and naive set theory.","sentences":["It is well-known that within Zermelo-Fraenkel set theory (ZF), the Axiom of Choice (AC) implies the Maximal Ideal Theorem (MIT), namely that every commutative ring has a maximal ideal.","The converse implication MIT $\\Rightarrow$ AC was first proved by Hodges, with subsequent proofs given by Banaschewski and Ern\\'e.   ","Here we give another derivation of MIT $\\Rightarrow$ AC, aiming to make the exposition self-contained and accessible to non-experts with only introductory familiarity with commutative ring theory and naive set theory."],"url":"http://arxiv.org/abs/2404.18351v1","category":"math.AC"}
{"created":"2024-04-29 01:05:53","title":"Bilinear optimal control for the Stokes-Brinkman equations: a priori and a posteriori error analyses","abstract":"We analyze a bilinear optimal control problem for the Stokes--Brinkman equations: the control variable enters the state equations as a coefficient. In two- and three-dimensional Lipschitz domains, we perform a complete continuous analysis that includes the existence of solutions and first- and second-order optimality conditions. We also develop two finite element methods that differ fundamentally in whether the admissible control set is discretized or not. For each of the proposed methods, we perform a convergence analysis and derive a priori error estimates; the latter under the assumption that the domain is convex. Finally, assuming that the domain is Lipschitz, we develop an a posteriori error estimator for each discretization scheme and obtain a global reliability bound.","sentences":["We analyze a bilinear optimal control problem for the Stokes--Brinkman equations: the control variable enters the state equations as a coefficient.","In two- and three-dimensional Lipschitz domains, we perform a complete continuous analysis that includes the existence of solutions and first- and second-order optimality conditions.","We also develop two finite element methods that differ fundamentally in whether the admissible control set is discretized or not.","For each of the proposed methods, we perform a convergence analysis and derive a priori error estimates; the latter under the assumption that the domain is convex.","Finally, assuming that the domain is Lipschitz, we develop an a posteriori error estimator for each discretization scheme and obtain a global reliability bound."],"url":"http://arxiv.org/abs/2404.18348v1","category":"math.NA"}
{"created":"2024-04-29 00:57:07","title":"Constraints on Tsallis cosmology using recent low redshift measurements","abstract":"Recently Tsallis cosmology has been presented as a novel proposal for alleviating both $H_0$ and $\\sigma_8$ tensions. Hence a universe filled with matter and radiation as perfect fluids and considering the Tsallis entropy is confronted using recent cosmological measurements coming from cosmic chronometers, type Ia supernovae, hydrogen II galaxies, quasars, and baryon acoustic oscillations. Following a Bayesian Markov Chain Monte Carlo analysis and combining the samples, we constrain the main characteristic parameters $\\alpha = 1.031^{+0.054}_{-0.051}$ and $\\delta = 1.005^{+0.001}_{-0.001}$ where the uncertainties are $1\\sigma$ confidence level. Additionally, we estimate the today deceleration parameter $q_0=-0.530^{+0.018}_{-0.017}$, the deceleration-acceleration transition redshift $z_T=0.632^{+0.028}_{-0.028}$ and the age of the universe $\\tau_U = 12.637^{+0.067}_{-0.066}\\,\\rm{Gyrs}$ which are in agreement with the standard cosmology ($\\Lambda$CDM) within $1.5\\sigma$. Furthermore, we find that the dark energy equation of state is consistent with both phantom and quintessence behaviors within $1\\sigma$ in the past and converging to $\\Lambda$CDM in the future. Finally, Tsallis cosmology is preferred over $\\Lambda$CDM by the combined data, suggesting further studies at the perturbation level.","sentences":["Recently Tsallis cosmology has been presented as a novel proposal for alleviating both $H_0$ and $\\sigma_8$ tensions.","Hence a universe filled with matter and radiation as perfect fluids and considering the Tsallis entropy is confronted using recent cosmological measurements coming from cosmic chronometers, type Ia supernovae, hydrogen II galaxies, quasars, and baryon acoustic oscillations.","Following a Bayesian Markov Chain Monte Carlo analysis and combining the samples, we constrain the main characteristic parameters $\\alpha = 1.031^{+0.054}_{-0.051}$ and $\\delta = 1.005^{+0.001}_{-0.001}$ where the uncertainties are $1\\sigma$ confidence level.","Additionally, we estimate the today deceleration parameter $q_0=-0.530^{+0.018}_{-0.017}$, the deceleration-acceleration transition redshift $z_T=0.632^{+0.028}_{-0.028}$ and the age of the universe $\\tau_U = 12.637^{+0.067}_{-0.066}\\,\\rm{Gyrs}$ which are in agreement with the standard cosmology ($\\Lambda$CDM) within $1.5\\sigma$. Furthermore, we find that the dark energy equation of state is consistent with both phantom and quintessence behaviors within $1\\sigma$ in the past and converging to $\\Lambda$CDM in the future.","Finally, Tsallis cosmology is preferred over $\\Lambda$CDM by the combined data, suggesting further studies at the perturbation level."],"url":"http://arxiv.org/abs/2404.18346v1","category":"astro-ph.CO"}
{"created":"2024-04-28 23:33:47","title":"Numerical investigation of fluid-structure interaction in a pilot-operated microfluidic valve","abstract":"The present paper is concerned with numerical investigation of the performance of a pilot-operated control valve based on shape memory alloy actuation control. The valve under investigation can be integrated into miniaturized hydraulic systems and is developed to perform precise dispensing, mixing, or dosing tasks while being able to withstand relatively high pressure differences. The study evaluates the valve's response under the current ON/OFF and the desired proportional control regimes using numerical methods for fluid-structure interaction. The computational model replicates the operation of the valve, which requires an understanding of the complex interactions between the fluid flow with the pressurized valve and the contact with the valve seat during the opening and closing processes. In addition, the model leverages advanced numerical techniques to overcome several complexities arising mainly from the geometrical, material, and contact nonlinearities, and to mitigate the shortcomings of the partitioned fluid-structure interaction approach. Several simulations are conducted to examine the valve's structural and flow behavior under varying pressure conditions. Results indicate that the valve is adequate for ON/OFF actuation control but is susceptible to flow-induced vibrations during the proportional control regime that occurs due to the sharp pressure drop in the valve-seat gap and the ensuing Venturi effect, which counteract the opening of the main valve. The fluid-structure-interaction simulations provide insight into the mechanism underlying the flow-induced vibrations, which can serve to improve the design and enhance the performance of the valve in microfluidic applications.","sentences":["The present paper is concerned with numerical investigation of the performance of a pilot-operated control valve based on shape memory alloy actuation control.","The valve under investigation can be integrated into miniaturized hydraulic systems and is developed to perform precise dispensing, mixing, or dosing tasks while being able to withstand relatively high pressure differences.","The study evaluates the valve's response under the current ON/OFF and the desired proportional control regimes using numerical methods for fluid-structure interaction.","The computational model replicates the operation of the valve, which requires an understanding of the complex interactions between the fluid flow with the pressurized valve and the contact with the valve seat during the opening and closing processes.","In addition, the model leverages advanced numerical techniques to overcome several complexities arising mainly from the geometrical, material, and contact nonlinearities, and to mitigate the shortcomings of the partitioned fluid-structure interaction approach.","Several simulations are conducted to examine the valve's structural and flow behavior under varying pressure conditions.","Results indicate that the valve is adequate for ON/OFF actuation control but is susceptible to flow-induced vibrations during the proportional control regime that occurs due to the sharp pressure drop in the valve-seat gap and the ensuing Venturi effect, which counteract the opening of the main valve.","The fluid-structure-interaction simulations provide insight into the mechanism underlying the flow-induced vibrations, which can serve to improve the design and enhance the performance of the valve in microfluidic applications."],"url":"http://arxiv.org/abs/2404.18335v1","category":"physics.flu-dyn"}
{"created":"2024-04-28 22:52:03","title":"Studies of stationary features in jets: BL Lacertae II. Trajectory reversals and superluminal speeds on sub-parsec scales","abstract":"High resolution VLBI observations revealed a quasi-stationary component (QSC) in the relativistic jets of many blazars, which represents a standing recollimation shock. The VLBA monitoring of the BL Lacertae jet at 15~GHz shows the QSC at a projected distance of about 0.26~mas from the radio core. We study the trajectory and kinematics of the QSC in BL Lacertae on sub-parsec scales using 15~GHz VLBA maps of 164 observations over 20 years from the MOJAVE program and 2~cm VLBA Survey. To reconstruct the QSC's intrinsic trajectory, we use moving average and trajectory refinement procedures to smooth out the effects of core displacement and account for QSC positioning errors. We identified 22 QSC reversal patterns on spatial scales ranging from 0.01~mas to 0.05~mas and with a frequency of $\\sim 1.5$ per year. Most reversals have an acute angle $<90\\degr$ at the turning point, and few have a loop-shaped or arc-shaped trajectory. The directions of reversals (clockwise or counterclockwise) appear to be random. Combinations of reversals show reversible and quasi-periodic motion. We propose a model in which the reverse motion of the QSC is due to the passage of a relativistic transverse jet wave through a fixed location of the QSC, similar to the transverse motion of a seagull on a wave. The QSC motion is governed by the amplitude, velocity, and tilt of the relativistic transverse wave. According to the model, relativistic waves are generated upstream of QSC. In the active state of the jet, the directions of the twisting waves are random, similar to the behaviour of the wave in a high-pressure hose, while in the jet stable state, the wave makes quasi-periodic oscillations with regular twisting. In this model the transverse speed of the QSC in the host galaxy frame is subluminal ($<0.3\\,c$), but to an observer it appears superluminal ($\\sim 2\\,c$).","sentences":["High resolution VLBI observations revealed a quasi-stationary component (QSC) in the relativistic jets of many blazars, which represents a standing recollimation shock.","The VLBA monitoring of the BL Lacertae jet at 15~GHz shows the QSC at a projected distance of about 0.26~mas from the radio core.","We study the trajectory and kinematics of the QSC in BL Lacertae on sub-parsec scales using 15~GHz VLBA maps of 164 observations over 20 years from the MOJAVE program and 2~cm VLBA Survey.","To reconstruct the QSC's intrinsic trajectory, we use moving average and trajectory refinement procedures to smooth out the effects of core displacement and account for QSC positioning errors.","We identified 22 QSC reversal patterns on spatial scales ranging from 0.01~mas to 0.05~mas and with a frequency of $\\sim 1.5$ per year.","Most reversals have an acute angle $<90\\degr$ at the turning point, and few have a loop-shaped or arc-shaped trajectory.","The directions of reversals (clockwise or counterclockwise) appear to be random.","Combinations of reversals show reversible and quasi-periodic motion.","We propose a model in which the reverse motion of the QSC is due to the passage of a relativistic transverse jet wave through a fixed location of the QSC, similar to the transverse motion of a seagull on a wave.","The QSC motion is governed by the amplitude, velocity, and tilt of the relativistic transverse wave.","According to the model, relativistic waves are generated upstream of QSC.","In the active state of the jet, the directions of the twisting waves are random, similar to the behaviour of the wave in a high-pressure hose, while in the jet stable state, the wave makes quasi-periodic oscillations with regular twisting.","In this model the transverse speed of the QSC in the host galaxy frame is subluminal ($<0.3\\,c$), but to an observer it appears superluminal ($\\sim 2\\,c$)."],"url":"http://arxiv.org/abs/2404.18330v1","category":"astro-ph.GA"}
{"created":"2024-04-28 21:41:53","title":"Casimir force within Ising chain with competing interactions","abstract":"We derive exact results for the critical Casimir force (CCF) within the one-dimensional Ising model with periodic boundary conditions (PBC's) and long-range equivalent-neighbor ferromagnetic interactions of strength $J_{l}/N>0$ superimposed on the nearest-neighbor interactions of strength $J_{s}$ which could be either ferromagnetic ($J_{s}>0$) or antiferromagnetic ($J_{s}<0$). In the infinite system limit the model, also known as the Nagle-Kardar model, exhibits in the plane $(K_s=\\beta J_s,K_l=\\beta J_l)$ a critical line $2 K_l=\\exp{\\left(-2 K_s\\right)}, K_s>-\\ln3/4$, which ends at a tricritical point $(K_l=-\\sqrt{3}/2, K_s=-\\ln3/4)$. The critical Casimir amplitudes are: $\\Delta_{\\rm Cas}^{\\rm (cr)}=1/4$ at the critical line, and $\\Delta_{\\rm Cas}^{\\rm (tr)}=1/3$ at the tricritical point. Quite unexpectedly, with the imposed PBC's the CCF exhibits very unusual behavior as a function of temperature and magnetic field. It is repulsive near the critical line and tricritical point, decaying rapidly with separation from those two singular regimes fast away from them and becoming attractive, displaying in which the maximum amplitude of the attraction exceeds the maximum amplitude of repulsion. This represents a violation of the widely-accepted \"boundary condition rule\", which holds that the CCF is attractive for equivalent BC's and repulsive for conflicting BC's independently of the actual bulk universality class of the phase transition under investigation.","sentences":["We derive exact results for the critical Casimir force (CCF) within the one-dimensional Ising model with periodic boundary conditions (PBC's) and long-range equivalent-neighbor ferromagnetic interactions of strength $J_{l}/N>0$ superimposed on the nearest-neighbor interactions of strength $J_{s}$ which could be either ferromagnetic ($J_{s}>0$) or antiferromagnetic ($J_{s}<0$).","In the infinite system limit the model, also known as the Nagle-Kardar model, exhibits in the plane $(K_s=\\beta J_s,K_l=\\beta J_l)$ a critical line $2 K_l=\\exp{\\left(-2 K_s\\right)}, K_s>-\\ln3/4$, which ends at a tricritical point $(K_l=-\\sqrt{3}/2, K_s=-\\ln3/4)$.","The critical Casimir amplitudes are: $\\Delta_{\\rm Cas}^{\\rm (cr)}=1/4$ at the critical line, and $\\Delta_{\\rm Cas}^{\\rm (tr)}=1/3$ at the tricritical point.","Quite unexpectedly, with the imposed PBC's the CCF exhibits very unusual behavior as a function of temperature and magnetic field.","It is repulsive near the critical line and tricritical point, decaying rapidly with separation from those two singular regimes fast away from them and becoming attractive, displaying in which the maximum amplitude of the attraction exceeds the maximum amplitude of repulsion.","This represents a violation of the widely-accepted \"boundary condition rule\", which holds that the CCF is attractive for equivalent BC's and repulsive for conflicting BC's independently of the actual bulk universality class of the phase transition under investigation."],"url":"http://arxiv.org/abs/2404.18324v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-28 21:18:45","title":"Riemannian Optimization for Active Mapping with Robot Teams","abstract":"Autonomous exploration of unknown environments using a team of mobile robots demands distributed perception and planning strategies to enable efficient and scalable performance. Ideally, each robot should update its map and plan its motion not only relying on its own observations, but also considering the observations of its peers. Centralized solutions to multi-robot coordination are susceptible to central node failure and require a sophisticated communication infrastructure for reliable operation. Current decentralized active mapping methods consider simplistic robot models with linear-Gaussian observations and Euclidean robot states. In this work, we present a distributed multi-robot mapping and planning method, called Riemannian Optimization for Active Mapping (ROAM). We formulate an optimization problem over a graph with node variables belonging to a Riemannian manifold and a consensus constraint requiring feasible solutions to agree on the node variables. We develop a distributed Riemannian optimization algorithm that relies only on one-hop communication to solve the problem with consensus and optimality guarantees. We show that multi-robot active mapping can be achieved via two applications of our distributed Riemannian optimization over different manifolds: distributed estimation of a 3-D semantic map and distributed planning of SE(3) trajectories that minimize map uncertainty. We demonstrate the performance of ROAM in simulation and real-world experiments using a team of robots with RGB-D cameras.","sentences":["Autonomous exploration of unknown environments using a team of mobile robots demands distributed perception and planning strategies to enable efficient and scalable performance.","Ideally, each robot should update its map and plan its motion not only relying on its own observations, but also considering the observations of its peers.","Centralized solutions to multi-robot coordination are susceptible to central node failure and require a sophisticated communication infrastructure for reliable operation.","Current decentralized active mapping methods consider simplistic robot models with linear-Gaussian observations and Euclidean robot states.","In this work, we present a distributed multi-robot mapping and planning method, called Riemannian Optimization for Active Mapping (ROAM).","We formulate an optimization problem over a graph with node variables belonging to a Riemannian manifold and a consensus constraint requiring feasible solutions to agree on the node variables.","We develop a distributed Riemannian optimization algorithm that relies only on one-hop communication to solve the problem with consensus and optimality guarantees.","We show that multi-robot active mapping can be achieved via two applications of our distributed Riemannian optimization over different manifolds: distributed estimation of a 3-D semantic map and distributed planning of SE(3) trajectories that minimize map uncertainty.","We demonstrate the performance of ROAM in simulation and real-world experiments using a team of robots with RGB-D cameras."],"url":"http://arxiv.org/abs/2404.18321v1","category":"cs.RO"}
{"created":"2024-04-28 21:09:52","title":"User Welfare Optimization in Recommender Systems with Competing Content Creators","abstract":"Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms. This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform. However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare. To encourage creators to best serve a broad user population with relevant content, it becomes the platform's responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators.   In this study, we perform system-side user welfare optimization under a competitive game setting among content creators. We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content. These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators' content production strategies. To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1. a proof-of-concept negative example illustrating how creators' strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on a leading short-video recommendation platform.","sentences":["Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms.","This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform.","However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare.","To encourage creators to best serve a broad user population with relevant content, it becomes the platform's responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators.   ","In this study, we perform system-side user welfare optimization under a competitive game setting among content creators.","We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content.","These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators' content production strategies.","To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1.","a proof-of-concept negative example illustrating how creators' strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on a leading short-video recommendation platform."],"url":"http://arxiv.org/abs/2404.18319v1","category":"cs.IR"}
{"created":"2024-04-28 20:57:55","title":"Position paper: Do not explain (vision models) without context","abstract":"Does the stethoscope in the picture make the adjacent person a doctor or a patient? This, of course, depends on the contextual relationship of the two objects. If it is obvious, why don not explanation methods for vision models use contextual information? In this paper, we (1) review the most popular methods of explaining computer vision models by pointing out that they do not take into account context information, (2) provide examples of real-world use cases where spatial context plays a significant role, (3) propose new research directions that may lead to better use of context information in explaining computer vision models, (4) argue that a change in approach to explanations is needed from 'where' to 'how'.","sentences":["Does the stethoscope in the picture make the adjacent person a doctor or a patient?","This, of course, depends on the contextual relationship of the two objects.","If it is obvious, why don not explanation methods for vision models use contextual information?","In this paper, we (1) review the most popular methods of explaining computer vision models by pointing out that they do not take into account context information, (2) provide examples of real-world use cases where spatial context plays a significant role, (3) propose new research directions that may lead to better use of context information in explaining computer vision models, (4) argue that a change in approach to explanations is needed from 'where' to 'how'."],"url":"http://arxiv.org/abs/2404.18316v1","category":"cs.CV"}
{"created":"2024-04-28 20:22:46","title":"Data-Driven Dynamic State Estimation of Photovoltaic Systems via Sparse Regression Unscented Kalman Filter","abstract":"Dynamic state estimation (DSE) is vital in modern power systems with numerous inverter-based distributed energy resources including solar and wind, ensuring real-time accuracy for tracking system variables and optimizing grid stability. This paper proposes a data-driven DSE approach designed for photovoltaic (PV) energy conversion systems (single stage and two stage) that are subjected to both process and measurement noise. The proposed framework follows a two-phase methodology encompassing ``data-driven model identification\" and ``state-estimation.\" In the initial model identification phase, state feedback is gathered to elucidate the dynamics of the photovoltaic systems using nonlinear sparse regression technique. Following the identification of the PV dynamics, the nonlinear data-driven model will be utilized to estimate the dynamics of the PV system for monitoring and protection purposes. To account for incomplete measurements, inherent uncertainties, and noise, we employ an ``unscented Kalman filter,\" which facilitates state estimation by processing the noisy output data. Ultimately, the paper substantiates the efficacy of the proposed sparse regression-based unscented Kalman filter through simulation results, providing a comparative analysis with a physics-based DSE.","sentences":["Dynamic state estimation (DSE) is vital in modern power systems with numerous inverter-based distributed energy resources including solar and wind, ensuring real-time accuracy for tracking system variables and optimizing grid stability.","This paper proposes a data-driven DSE approach designed for photovoltaic (PV) energy conversion systems (single stage and two stage) that are subjected to both process and measurement noise.","The proposed framework follows a two-phase methodology encompassing ``data-driven model identification\" and ``state-estimation.\"","In the initial model identification phase, state feedback is gathered to elucidate the dynamics of the photovoltaic systems using nonlinear sparse regression technique.","Following the identification of the PV dynamics, the nonlinear data-driven model will be utilized to estimate the dynamics of the PV system for monitoring and protection purposes.","To account for incomplete measurements, inherent uncertainties, and noise, we employ an ``unscented Kalman filter,\" which facilitates state estimation by processing the noisy output data.","Ultimately, the paper substantiates the efficacy of the proposed sparse regression-based unscented Kalman filter through simulation results, providing a comparative analysis with a physics-based DSE."],"url":"http://arxiv.org/abs/2404.18305v1","category":"cs.SY"}
{"created":"2024-04-28 20:08:01","title":"GNarsil: Splitting Stabilizers into Gauges","abstract":"Quantum subsystem codes have been shown to improve error-correction performance, ease the implementation of logical operations on codes, and make stabilizer measurements easier by decomposing stabilizers into smaller-weight gauge operators. In this paper, we present two algorithms that produce new subsystem codes from a \"seed\" CSS code. They replace some stabilizers of a given CSS code with smaller-weight gauge operators that split the remaining stabilizers, while being compatible with the logical Pauli operators of the code. The algorithms recover the well-known Bacon-Shor code computationally as well as produce a new $\\left[\\left[ 9,1,2,2 \\right]\\right]$ rotated surface subsystem code with weight-$3$ gauges and weight-$4$ stabilizers. We illustrate using a $\\left[\\left[ 100,25,3 \\right]\\right]$ subsystem hypergraph product (SHP) code that the algorithms can produce more efficient gauge operators than the closed-form expressions of the SHP construction. However, we observe that the stabilizers of the lifted product quantum LDPC codes are more challenging to split into small-weight gauge operators. Hence, we introduce the subsystem lifted product (SLP) code construction and develop a new $\\left[\\left[ 775, 124, 20 \\right]\\right]$ code from Tanner's classical quasi-cyclic LDPC code. The code has high-weight stabilizers but all gauge operators that split stabilizers have weight $5$, except one. In contrast, the LP stabilizer code from Tanner's code has parameters $\\left[\\left[ 1054, 124, 20 \\right]\\right]$. This serves as a novel example of new subsystem codes that outperform stabilizer versions of them. Finally, based on our experiments, we share some general insights about non-locality's effects on the performance of splitting stabilizers into small-weight gauges.","sentences":["Quantum subsystem codes have been shown to improve error-correction performance, ease the implementation of logical operations on codes, and make stabilizer measurements easier by decomposing stabilizers into smaller-weight gauge operators.","In this paper, we present two algorithms that produce new subsystem codes from a \"seed\" CSS code.","They replace some stabilizers of a given CSS code with smaller-weight gauge operators that split the remaining stabilizers, while being compatible with the logical Pauli operators of the code.","The algorithms recover the well-known Bacon-Shor code computationally as well as produce a new $\\left[\\left[ 9,1,2,2 \\right]\\right]$ rotated surface subsystem code with weight-$3$ gauges and weight-$4$ stabilizers.","We illustrate using a $\\left[\\left[ 100,25,3 \\right]\\right]$ subsystem hypergraph product (SHP) code that the algorithms can produce more efficient gauge operators than the closed-form expressions of the SHP construction.","However, we observe that the stabilizers of the lifted product quantum LDPC codes are more challenging to split into small-weight gauge operators.","Hence, we introduce the subsystem lifted product (SLP) code construction and develop a new $\\left[\\left[ 775, 124, 20 \\right]\\right]$ code from Tanner's classical quasi-cyclic LDPC code.","The code has high-weight stabilizers but all gauge operators that split stabilizers have weight $5$, except one.","In contrast, the LP stabilizer code from Tanner's code has parameters $\\left[\\left[ 1054, 124, 20 \\right]\\right]$. This serves as a novel example of new subsystem codes that outperform stabilizer versions of them.","Finally, based on our experiments, we share some general insights about non-locality's effects on the performance of splitting stabilizers into small-weight gauges."],"url":"http://arxiv.org/abs/2404.18302v1","category":"quant-ph"}
{"created":"2024-04-28 18:57:24","title":"Monitoring Real-Time Systems under Parametric Delay","abstract":"Online monitoring of embedded real-time systems can be achieved by reduction of an adequate property language, like Metric Interval Temporal Logic, to timed automata and symbolic execution of the resulting automata on the trace observed from the system. This direct construction however only is faithful if observation of the trace is immediate in the sense that the monitor can assign exact time stamps to the actions it observes, which is rarely true in practice due to the substantial and fluctuating parametric delays introduced by the circuitry connecting the observed system to its monitoring device. We present a purely zone-based online monitoring procedure and its implementation which handle such parametric delays exactly without recurrence to costly verification procedures for parametric timed automata.","sentences":["Online monitoring of embedded real-time systems can be achieved by reduction of an adequate property language, like Metric Interval Temporal Logic, to timed automata and symbolic execution of the resulting automata on the trace observed from the system.","This direct construction however only is faithful if observation of the trace is immediate in the sense that the monitor can assign exact time stamps to the actions it observes, which is rarely true in practice due to the substantial and fluctuating parametric delays introduced by the circuitry connecting the observed system to its monitoring device.","We present a purely zone-based online monitoring procedure and its implementation which handle such parametric delays exactly without recurrence to costly verification procedures for parametric timed automata."],"url":"http://arxiv.org/abs/2404.18282v1","category":"cs.FL"}
{"created":"2024-04-28 18:29:41","title":"Minimally Extended Current Algebras of Toroidal Conformal Field Theories","abstract":"It is well-known that families of two-dimensional toroidal conformal field theories possess a dense subset of rational toroidal conformal field theories, which makes such families an interesting testing ground about rationality of conformal field theories in families in general. Rational toroidal conformal field theories possess an extended chiral and anti-chiral algebra known as W-algebras. Their partition functions decompose into a finite sum of products of holomorphic and anti-holomorphic characters of these W-algebras. Instead of considering these characters, we decompose the partition functions into products of characters of minimal extensions of $\\widehat{\\mathfrak{u}}(1)$ current algebras, which already appear for rational conformal field theories with target space $S^1$. We present an explicit construction that determines such decompositions. While these decompositions are not unique, they are universal in the sense that any rational toroidal conformal field theory with a target space torus of arbitrary dimension admits such decompositions. We illustrate these decompositions with a few representative examples of rational toroidal conformal field theories with two- and three-dimensional target space tori.","sentences":["It is well-known that families of two-dimensional toroidal conformal field theories possess a dense subset of rational toroidal conformal field theories, which makes such families an interesting testing ground about rationality of conformal field theories in families in general.","Rational toroidal conformal field theories possess an extended chiral and anti-chiral algebra known as W-algebras.","Their partition functions decompose into a finite sum of products of holomorphic and anti-holomorphic characters of these W-algebras.","Instead of considering these characters, we decompose the partition functions into products of characters of minimal extensions of $\\widehat{\\mathfrak{u}}(1)$ current algebras, which already appear for rational conformal field theories with target space $S^1$.","We present an explicit construction that determines such decompositions.","While these decompositions are not unique, they are universal in the sense that any rational toroidal conformal field theory with a target space torus of arbitrary dimension admits such decompositions.","We illustrate these decompositions with a few representative examples of rational toroidal conformal field theories with two- and three-dimensional target space tori."],"url":"http://arxiv.org/abs/2404.18269v1","category":"hep-th"}
{"created":"2024-04-28 18:18:14","title":"Optimal Treatment Allocation under Constraints","abstract":"In optimal policy problems where treatment effects vary at the individual level, optimally allocating treatments to recipients is complex even when potential outcomes are known. We present an algorithm for multi-arm treatment allocation problems that is guaranteed to find the optimal allocation in strongly polynomial time, and which is able to handle arbitrary potential outcomes as well as constraints on treatment requirement and capacity. Further, starting from an arbitrary allocation, we show how to optimally re-allocate treatments in a Pareto-improving manner. To showcase our results, we use data from Danish nurse home visiting for infants. We estimate nurse specific treatment effects for children born 1959-1967 in Copenhagen, comparing nurses against each other. We exploit random assignment of newborn children to nurses within a district to obtain causal estimates of nurse-specific treatment effects using causal machine learning. Using these estimates, and treating the Danish nurse home visiting program as a case of an optimal treatment allocation problem (where a treatment is a nurse), we document room for significant productivity improvements by optimally re-allocating nurses to children. Our estimates suggest that optimal allocation of nurses to children could have improved average yearly earnings by USD 1,815 and length of education by around two months.","sentences":["In optimal policy problems where treatment effects vary at the individual level, optimally allocating treatments to recipients is complex even when potential outcomes are known.","We present an algorithm for multi-arm treatment allocation problems that is guaranteed to find the optimal allocation in strongly polynomial time, and which is able to handle arbitrary potential outcomes as well as constraints on treatment requirement and capacity.","Further, starting from an arbitrary allocation, we show how to optimally re-allocate treatments in a Pareto-improving manner.","To showcase our results, we use data from Danish nurse home visiting for infants.","We estimate nurse specific treatment effects for children born 1959-1967 in Copenhagen, comparing nurses against each other.","We exploit random assignment of newborn children to nurses within a district to obtain causal estimates of nurse-specific treatment effects using causal machine learning.","Using these estimates, and treating the Danish nurse home visiting program as a case of an optimal treatment allocation problem (where a treatment is a nurse), we document room for significant productivity improvements by optimally re-allocating nurses to children.","Our estimates suggest that optimal allocation of nurses to children could have improved average yearly earnings by USD 1,815 and length of education by around two months."],"url":"http://arxiv.org/abs/2404.18268v1","category":"econ.EM"}
{"created":"2024-04-28 18:16:58","title":"LINOCS: Lookahead Inference of Networked Operators for Continuous Stability","abstract":"Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior. For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems. Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging. Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process. Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators. Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data. LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions. We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples.","sentences":["Identifying latent interactions within complex systems is key to unlocking deeper insights into their operational dynamics, including how their elements affect each other and contribute to the overall system behavior.","For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizing the interactions among populations is key for understanding complex ecosystems.","Such systems, often modeled as dynamical systems, typically exhibit noisy high-dimensional and non-stationary temporal behavior that renders their identification challenging.","Existing dynamical system identification methods often yield operators that accurately capture short-term behavior but fail to predict long-term trends, suggesting an incomplete capture of the underlying process.","Methods that consider extended forecasts (e.g., recurrent neural networks) lack explicit representations of element-wise interactions and require substantial training data, thereby failing to capture interpretable network operators.","Here we introduce Lookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS), a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data.","LINOCS integrates several multi-step predictions with adaptive weights during training to recover dynamical operators that can yield accurate long-term predictions.","We demonstrate LINOCS' ability to recover the ground truth dynamical operators underlying synthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems' decomposition, and regularized linear time-varying systems) as well as its capability to produce meaningful operators with robust reconstructions through various real-world examples."],"url":"http://arxiv.org/abs/2404.18267v1","category":"eess.SY"}
{"created":"2024-04-28 18:10:39","title":"Terawatt-level three-stage pulse compression for all-attosecond pump-probe spectroscopy","abstract":"The generation of terawatt (TW) near-single-cycle laser pulses is of high interest for applications including attosecond science. Here we demonstrate a three-stage post-compression scheme in a non-guided geometry using He as the nonlinear medium, resulting in the generation of multi-mJ pulses with a duration of 3.7 fs. Key features of this approach are its simplicity, robustness and high stability, making it ideally suited for highly demanding applications such as attosecond-pump attosecond-probe spectroscopy (APAPS). This is demonstrated by performing two-color APAPS in Ar and Ne, where both simultaneous and sequential two-photon absorption are observed. Our approach is scalable to multi-TW powers.","sentences":["The generation of terawatt (TW) near-single-cycle laser pulses is of high interest for applications including attosecond science.","Here we demonstrate a three-stage post-compression scheme in a non-guided geometry using He as the nonlinear medium, resulting in the generation of multi-mJ pulses with a duration of 3.7 fs.","Key features of this approach are its simplicity, robustness and high stability, making it ideally suited for highly demanding applications such as attosecond-pump attosecond-probe spectroscopy (APAPS).","This is demonstrated by performing two-color APAPS in Ar and Ne, where both simultaneous and sequential two-photon absorption are observed.","Our approach is scalable to multi-TW powers."],"url":"http://arxiv.org/abs/2404.18266v1","category":"physics.optics"}
{"created":"2024-04-28 17:50:58","title":"Align, Minimize and Diversify: A Source-Free Unsupervised Domain Adaptation Method for Handwritten Text Recognition","abstract":"This paper serves to introduce the Align, Minimize and Diversify (AMD) method, a Source-Free Unsupervised Domain Adaptation approach for Handwritten Text Recognition (HTR). This framework decouples the adaptation process from the source data, thus not only sidestepping the resource-intensive retraining process but also making it possible to leverage the wealth of pre-trained knowledge encoded in modern Deep Learning architectures. Our method explicitly eliminates the need to revisit the source data during adaptation by incorporating three distinct regularization terms: the Align term, which reduces the feature distribution discrepancy between source and target data, ensuring the transferability of the pre-trained representation; the Minimize term, which encourages the model to make assertive predictions, pushing the outputs towards one-hot-like distributions in order to minimize prediction uncertainty, and finally, the Diversify term, which safeguards against the degeneracy in predictions by promoting varied and distinctive sequences throughout the target data, preventing informational collapse. Experimental results from several benchmarks demonstrated the effectiveness and robustness of AMD, showing it to be competitive and often outperforming DA methods in HTR.","sentences":["This paper serves to introduce the Align, Minimize and Diversify (AMD) method, a Source-Free Unsupervised Domain Adaptation approach for Handwritten Text Recognition (HTR).","This framework decouples the adaptation process from the source data, thus not only sidestepping the resource-intensive retraining process but also making it possible to leverage the wealth of pre-trained knowledge encoded in modern Deep Learning architectures.","Our method explicitly eliminates the need to revisit the source data during adaptation by incorporating three distinct regularization terms: the Align term, which reduces the feature distribution discrepancy between source and target data, ensuring the transferability of the pre-trained representation; the Minimize term, which encourages the model to make assertive predictions, pushing the outputs towards one-hot-like distributions in order to minimize prediction uncertainty, and finally, the Diversify term, which safeguards against the degeneracy in predictions by promoting varied and distinctive sequences throughout the target data, preventing informational collapse.","Experimental results from several benchmarks demonstrated the effectiveness and robustness of AMD, showing it to be competitive and often outperforming DA methods in HTR."],"url":"http://arxiv.org/abs/2404.18260v1","category":"cs.CV"}
{"created":"2024-04-28 17:18:41","title":"Fisher Information Improved Training-Free Conditional Diffusion Model","abstract":"Recently, the diffusion model with the training-free methods has succeeded in conditional image generation tasks. However, there is an efficiency problem because it requires calculating the gradient with high computational cost, and previous methods make strong assumptions to solve it, sacrificing generalization. In this work, we propose the Fisher information guided diffusion model (FIGD). Concretely, we introduce the Fisher information to estimate the gradient without making any additional assumptions to reduce computation cost. Meanwhile, we demonstrate that the Fisher information ensures the generalization of FIGD and provides new insights for training-free methods based on the information theory. The experimental results demonstrate that FIGD could achieve different conditional generations more quickly while maintaining high quality.","sentences":["Recently, the diffusion model with the training-free methods has succeeded in conditional image generation tasks.","However, there is an efficiency problem because it requires calculating the gradient with high computational cost, and previous methods make strong assumptions to solve it, sacrificing generalization.","In this work, we propose the Fisher information guided diffusion model (FIGD).","Concretely, we introduce the Fisher information to estimate the gradient without making any additional assumptions to reduce computation cost.","Meanwhile, we demonstrate that the Fisher information ensures the generalization of FIGD and provides new insights for training-free methods based on the information theory.","The experimental results demonstrate that FIGD could achieve different conditional generations more quickly while maintaining high quality."],"url":"http://arxiv.org/abs/2404.18252v1","category":"cs.CV"}
{"created":"2024-04-28 17:08:04","title":"Observing Supernova Neutrino Light Curves with Super-Kamiokande. V. Distance Estimation with Neutrinos Alone","abstract":"Neutrinos are pivotal signals in multi-messenger observations of supernovae. Recent advancements in the analysis method of supernova neutrinos, especially in quantitative analysis, have significantly broadened scientific possibilities. This study demonstrates the feasibility of estimating distances to supernovae using quantitative analysis techniques for supernova neutrinos. This estimation utilizes the direct relationship between the radius of a neutron star and the distance to the supernova. The radius of a neutron star is determined with an approximate uncertainty of 10% through observations such as X-rays and gravitational waves. By integrating this information, the distance to the supernova can be estimated with an uncertainty of within 15% at a 95% confidence level. It has been established that neutrinos can pinpoint the direction of supernovae, and when combined with distance estimates, three-dimensional localization becomes achievable. This capability is vital for follow-up observations using multi-messenger approaches. Moreover, more precise distance determinations to supernovae through follow-up observations, such as optical observations, allow for accurate measurements of neutron star radii. This data, via the neutron star mass-radius relationship, could provide various insights into nuclear physics.","sentences":["Neutrinos are pivotal signals in multi-messenger observations of supernovae.","Recent advancements in the analysis method of supernova neutrinos, especially in quantitative analysis, have significantly broadened scientific possibilities.","This study demonstrates the feasibility of estimating distances to supernovae using quantitative analysis techniques for supernova neutrinos.","This estimation utilizes the direct relationship between the radius of a neutron star and the distance to the supernova.","The radius of a neutron star is determined with an approximate uncertainty of 10% through observations such as X-rays and gravitational waves.","By integrating this information, the distance to the supernova can be estimated with an uncertainty of within 15% at a 95% confidence level.","It has been established that neutrinos can pinpoint the direction of supernovae, and when combined with distance estimates, three-dimensional localization becomes achievable.","This capability is vital for follow-up observations using multi-messenger approaches.","Moreover, more precise distance determinations to supernovae through follow-up observations, such as optical observations, allow for accurate measurements of neutron star radii.","This data, via the neutron star mass-radius relationship, could provide various insights into nuclear physics."],"url":"http://arxiv.org/abs/2404.18248v1","category":"astro-ph.HE"}
{"created":"2024-04-28 16:47:52","title":"Uniform in time bounds for a stochastic hybrid system with fast periodic sampling and small white-noise","abstract":"We study the asymptotic behavior, uniform-in-time, of a nonlinear dynamical system under the combined effects of fast periodic sampling with period $\\delta$ and small white noise of size $\\varepsilon,\\thinspace 0<\\varepsilon,\\delta \\ll 1$. The dynamics depend on both the current and recent measurements of the state, and as such it is not Markovian. Our main results can be interpreted as Law of Large Numbers (LLN) and Central Limit Theorem (CLT) type results. LLN type result shows that the resulting stochastic process is close to an ordinary differential equation (ODE) uniformly in time as $\\varepsilon,\\delta \\searrow 0.$ Further, in regards to CLT, we provide quantitative and uniform-in-time control of the fluctuations process. The interaction of the small parameters provides an additional drift term in the limiting fluctuations, which captures both the sampling and noise effects. As a consequence, we obtain a first-order perturbation expansion of the stochastic process along with time-independent estimates on the remainder. The zeroth- and first-order terms in the expansion are given by an ODE and SDE, respectively. Simulation studies that illustrate and supplement the theoretical results are also provided.","sentences":["We study the asymptotic behavior, uniform-in-time, of a nonlinear dynamical system under the combined effects of fast periodic sampling with period $\\delta$ and small white noise of size $\\varepsilon,\\thinspace 0<\\varepsilon,\\delta \\ll 1$.","The dynamics depend on both the current and recent measurements of the state, and as such it is not Markovian.","Our main results can be interpreted as Law of Large Numbers (LLN) and Central Limit Theorem (CLT) type results.","LLN type result shows that the resulting stochastic process is close to an ordinary differential equation (ODE) uniformly in time as $\\varepsilon,\\delta \\searrow 0.$ Further, in regards to CLT, we provide quantitative and uniform-in-time control of the fluctuations process.","The interaction of the small parameters provides an additional drift term in the limiting fluctuations, which captures both the sampling and noise effects.","As a consequence, we obtain a first-order perturbation expansion of the stochastic process along with time-independent estimates on the remainder.","The zeroth- and first-order terms in the expansion are given by an ODE and SDE, respectively.","Simulation studies that illustrate and supplement the theoretical results are also provided."],"url":"http://arxiv.org/abs/2404.18242v1","category":"math.PR"}
{"created":"2024-04-28 16:31:32","title":"SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning","abstract":"Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning. While interest in studying LLM unlearning is growing,the impact of the optimizer choice for LLM unlearning remains under-explored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between {second-order optimization} and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method. SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning.","sentences":["Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices.","LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility out of the scope of unlearning.","While interest in studying LLM unlearning is growing,the impact of the optimizer choice for LLM unlearning remains under-explored.","In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between {second-order optimization} and influence unlearning (a classical approach using influence functions to update the model for data influence removal).","This insight propels us to develop a second-order unlearning framework, termed SOUL, built upon the second-order clipped stochastic optimization (Sophia)-based LLM training method.","SOUL extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process.","Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, suggesting the promise of second-order optimization in providing a scalable and easily implementable solution for LLM unlearning."],"url":"http://arxiv.org/abs/2404.18239v1","category":"cs.LG"}
{"created":"2024-04-28 15:57:04","title":"A cautious approach to constraint-based causal model selection","abstract":"We study the data-driven selection of causal graphical models using constraint-based algorithms, which determine the existence or non-existence of edges (causal connections) in a graph based on testing a series of conditional independence hypotheses. In settings where the ultimate scientific goal is to use the selected graph to inform estimation of some causal effect of interest (e.g., by selecting a valid and sufficient set of adjustment variables), we argue that a \"cautious\" approach to graph selection should control the probability of falsely removing edges and prefer dense, rather than sparse, graphs. We propose a simple inversion of the usual conditional independence testing procedure: to remove an edge, test the null hypothesis of conditional association greater than some user-specified threshold, rather than the null of independence. This equivalence testing formulation to testing independence constraints leads to a procedure with desriable statistical properties and behaviors that better match the inferential goals of certain scientific studies, for example observational epidemiological studies that aim to estimate causal effects in the face of causal model uncertainty. We illustrate our approach on a data example from environmental epidemiology.","sentences":["We study the data-driven selection of causal graphical models using constraint-based algorithms, which determine the existence or non-existence of edges (causal connections) in a graph based on testing a series of conditional independence hypotheses.","In settings where the ultimate scientific goal is to use the selected graph to inform estimation of some causal effect of interest (e.g., by selecting a valid and sufficient set of adjustment variables), we argue that a \"cautious\" approach to graph selection should control the probability of falsely removing edges and prefer dense, rather than sparse, graphs.","We propose a simple inversion of the usual conditional independence testing procedure: to remove an edge, test the null hypothesis of conditional association greater than some user-specified threshold, rather than the null of independence.","This equivalence testing formulation to testing independence constraints leads to a procedure with desriable statistical properties and behaviors that better match the inferential goals of certain scientific studies, for example observational epidemiological studies that aim to estimate causal effects in the face of causal model uncertainty.","We illustrate our approach on a data example from environmental epidemiology."],"url":"http://arxiv.org/abs/2404.18232v1","category":"stat.ME"}
{"created":"2024-04-28 15:51:10","title":"Symmetry reduction, gauge reduction, backreaction and consistent higher order perturbation theory","abstract":"For interacting classical field theories such as general relativity exact solutions typically can only be found by imposing physically motivated (Killing) {\\it symmetry} assumptions. Such highly symmetric solutions are then often used as {\\it backgrounds} in a {\\it perturbative} approach to more general non-symmetric solutions.   If the theory is in addition a {\\it gauge} theory such as general relativity, the issue arises how to consistently combine the perturbative expansion with the gauge reduction. For instance it is not granted that the corresponding constraints expanded to a given order still close under Poisson brackets with respect to the non-symmetric degrees of freedom up to higher order.   If one is interested in the problem of {\\it backreaction} between symmetric and non-symmetric dgrees of freedom, then one also must consider the symmetric degrees of freedom as dynamical variables which supply additional terms in Poisson brackets with respect to the symmetric degrees of freedom and the just mentioned consistency problem becomes even more complicated.   In this paper we show for a general theory how to consistently combine all of these notions. The idea is to {\\it first} perform the {\\it exact} gauge reduction on the {\\it full} phase space which results in the reduced phase space of observables and physical Hamiltonian respectively and {\\it secondly} expand that physical Hamiltonian perturbatively. Surprisingly, this strategy is not only practically feasible but also avoids the above mentioned tensions.   We also show how to perform the partial reduction with respect to only the asymmetric constraints but that theory is not quantisable at finite orders unless there is only one symmetric constraint.","sentences":["For interacting classical field theories such as general relativity exact solutions typically can only be found by imposing physically motivated (Killing) {\\it symmetry} assumptions.","Such highly symmetric solutions are then often used as {\\it backgrounds} in a {\\it perturbative} approach to more general non-symmetric solutions.   ","If the theory is in addition a {\\it gauge} theory such as general relativity, the issue arises how to consistently combine the perturbative expansion with the gauge reduction.","For instance it is not granted that the corresponding constraints expanded to a given order still close under Poisson brackets with respect to the non-symmetric degrees of freedom up to higher order.   ","If one is interested in the problem of {\\it backreaction} between symmetric and non-symmetric dgrees of freedom, then one also must consider the symmetric degrees of freedom as dynamical variables which supply additional terms in Poisson brackets with respect to the symmetric degrees of freedom and the just mentioned consistency problem becomes even more complicated.   ","In this paper we show for a general theory how to consistently combine all of these notions.","The idea is to {\\it first} perform the {\\it exact} gauge reduction on the {\\it full} phase space which results in the reduced phase space of observables and physical Hamiltonian respectively and {\\it secondly} expand that physical Hamiltonian perturbatively.","Surprisingly, this strategy is not only practically feasible but also avoids the above mentioned tensions.   ","We also show how to perform the partial reduction with respect to only the asymmetric constraints but that theory is not quantisable at finite orders unless there is only one symmetric constraint."],"url":"http://arxiv.org/abs/2404.18230v1","category":"gr-qc"}
{"created":"2024-04-28 15:37:48","title":"Gordan-Rankin-Cohen operators on superstrings","abstract":"We distinguish two classifications of bidifferential operators: between (A) spaces of modular forms and (B) spaces of weighted densities.   (A) The invariant under the projective action of $\\text{SL}(2;\\mathbb{Z})$ binary differential operators between spaces of modular forms of integer or half-integer weight on the 1-dimensional manifold were found by Gordan (called transvectants), rediscovered and classified by Rankin and Cohen (called brackets), and, in still another context, by Janson and Peetre. The invariant under the algebraic supergroup $\\text{OSp}(1|2; \\mathbb{Z})$ super modular forms of integer and half-integer weight on $(1|1)$-dimensional superstrings with contact structure were introduced, bidifferential operators between them classified and further studied by Gieres-Theisen, Cohen-Manin-Zagier, and Gargoubi-Ovsienko.   (B) For any complex weights, we classify the analogs of Gordan-Rankin-Cohen (briefly: GRC) binary differential operators between spaces of weighted densities invariant under $\\mathfrak{pgl}(2)$. For any complex weights, we classify the analogs of GRC-operators between spaces of weighted densities invariant under the Lie superalgebra $\\mathfrak{osp}(1|2)$. In the case of $(1|1)$-dimensional superstring without any additional structure, we also classify the analogs of GRC-operators between spaces of any weighted densities invariant under the Lie superalgebra $\\mathfrak{pgl}(1|2)$.","sentences":["We distinguish two classifications of bidifferential operators: between (A) spaces of modular forms and (B) spaces of weighted densities.   ","(A) The invariant under the projective action of $\\text{SL}(2;\\mathbb{Z})$ binary differential operators between spaces of modular forms of integer or half-integer weight on the 1-dimensional manifold were found by Gordan (called transvectants), rediscovered and classified by Rankin and Cohen (called brackets), and, in still another context, by Janson and Peetre.","The invariant under the algebraic supergroup $\\text{OSp}(1|2; \\mathbb{Z})$ super modular forms of integer and half-integer weight on $(1|1)$-dimensional superstrings with contact structure were introduced, bidifferential operators between them classified and further studied by Gieres-Theisen, Cohen-Manin-Zagier, and Gargoubi-Ovsienko.   ","(B)","For any complex weights, we classify the analogs of Gordan-Rankin-Cohen (briefly: GRC) binary differential operators between spaces of weighted densities invariant under $\\mathfrak{pgl}(2)$. For any complex weights, we classify the analogs of GRC-operators between spaces of weighted densities invariant under the Lie superalgebra $\\mathfrak{osp}(1|2)$. In the case of $(1|1)$-dimensional superstring without any additional structure, we also classify the analogs of GRC-operators between spaces of any weighted densities invariant under the Lie superalgebra $\\mathfrak{pgl}(1|2)$."],"url":"http://arxiv.org/abs/2404.18222v1","category":"math.RT"}
{"created":"2024-04-28 15:31:35","title":"Asymptotically safe -- canonical quantum gravity junction","abstract":"The canonical (CQG) and asymptotically safe (ASQG) approach to quantum gravity share to be both non-perturbative programmes. However, apart from that they seem to differ in several aspects such as: 1. Signature: CQG is Lorentzian while ASQG is mostly Euclidian. 2. Background Independence (BI): CQG is manifesly BI while ASQG is apparently not. 3. Truncations: CQG is apparently free of truncations while ASQG makes heavy use of them.   The purpose of the present work is to either overcome actual differences or to explain why apparent differences are actually absent. Thereby we intend to enhance the contact and communication between the two communities. The focus of this contribution is on conceptual issues rather than deep technical details such has high order truncations. On the other hand the paper tries to be self-contained in order to be useful to researchers from both communities.   The point of contact is the path integral formulation of Lorentzian CQG in its reduced phase space formulation which yields the formal generating functional of physical (i.e. gauge invariant) either Schwinger or Feynman N-point functions for (relational) observables. The corresponding effective actions of these generating functionals can then be subjected to the ASQG Wetterich type flow equations which serve in particular to find the rigorous generating fuctionals via the inverse Legendre transform of the fixed pointed effective action.","sentences":["The canonical (CQG) and asymptotically safe (ASQG) approach to quantum gravity share to be both non-perturbative programmes.","However, apart from that they seem to differ in several aspects such as: 1.","Signature: CQG is Lorentzian while ASQG is mostly Euclidian.","2. Background Independence (BI): CQG is manifesly BI while ASQG is apparently not.","3. Truncations: CQG is apparently free of truncations while ASQG makes heavy use of them.   ","The purpose of the present work is to either overcome actual differences or to explain why apparent differences are actually absent.","Thereby we intend to enhance the contact and communication between the two communities.","The focus of this contribution is on conceptual issues rather than deep technical details such has high order truncations.","On the other hand the paper tries to be self-contained in order to be useful to researchers from both communities.   ","The point of contact is the path integral formulation of Lorentzian CQG in its reduced phase space formulation which yields the formal generating functional of physical (i.e. gauge invariant) either Schwinger or Feynman N-point functions for (relational) observables.","The corresponding effective actions of these generating functionals can then be subjected to the ASQG Wetterich type flow equations which serve in particular to find the rigorous generating fuctionals via the inverse Legendre transform of the fixed pointed effective action."],"url":"http://arxiv.org/abs/2404.18220v1","category":"hep-th"}
{"created":"2024-04-28 15:31:20","title":"BUFF: Boosted Decision Tree based Ultra-Fast Flow matching","abstract":"Tabular data stands out as one of the most frequently encountered types in high energy physics. Unlike commonly homogeneous data such as pixelated images, simulating high-dimensional tabular data and accurately capturing their correlations are often quite challenging, even with the most advanced architectures. Based on the findings that tree-based models surpass the performance of deep learning models for tasks specific to tabular data, we adopt the very recent generative modeling class named conditional flow matching and employ different techniques to integrate the usage of Gradient Boosted Trees. The performances are evaluated for various tasks on different analysis level with several public datasets. We demonstrate the training and inference time of most high-level simulation tasks can achieve speedup by orders of magnitude. The application can be extended to low-level feature simulation and conditioned generations with competitive performance.","sentences":["Tabular data stands out as one of the most frequently encountered types in high energy physics.","Unlike commonly homogeneous data such as pixelated images, simulating high-dimensional tabular data and accurately capturing their correlations are often quite challenging, even with the most advanced architectures.","Based on the findings that tree-based models surpass the performance of deep learning models for tasks specific to tabular data, we adopt the very recent generative modeling class named conditional flow matching and employ different techniques to integrate the usage of Gradient Boosted Trees.","The performances are evaluated for various tasks on different analysis level with several public datasets.","We demonstrate the training and inference time of most high-level simulation tasks can achieve speedup by orders of magnitude.","The application can be extended to low-level feature simulation and conditioned generations with competitive performance."],"url":"http://arxiv.org/abs/2404.18219v1","category":"physics.ins-det"}
{"created":"2024-04-28 15:03:49","title":"ROS 2 on a Chip, Achieving Brain-Like Speeds and Efficiency in Robotic Networking","abstract":"The Robot Operating System (ROS) pubsub model played a pivotal role in developing sophisticated robotic applications. However, the complexities and real-time demands of modern robotics necessitate more efficient communication solutions that are deterministic and isochronous. This article introduces a groundbreaking approach: embedding ROS 2 message-passing infrastructure directly onto a specialized hardware chip, significantly enhancing speed and efficiency in robotic communications. Our FPGA prototypes of the chip design can send or receive packages in less than 2.5 microseconds, accelerating networking communications by more than 62x on average and improving energy consumption by more than 500x when compared to traditional ROS 2 software implementations on modern CPUs. Additionally, it dramatically reduces maximum latency in ROS 2 networking communication by more than 30,000x. In situations of peak latency, our design guarantees an isochronous response within 11 microseconds, a stark improvement over the potential hundreds of milliseconds reported by modern CPU systems under similar conditions.","sentences":["The Robot Operating System (ROS) pubsub model played a pivotal role in developing sophisticated robotic applications.","However, the complexities and real-time demands of modern robotics necessitate more efficient communication solutions that are deterministic and isochronous.","This article introduces a groundbreaking approach: embedding ROS 2 message-passing infrastructure directly onto a specialized hardware chip, significantly enhancing speed and efficiency in robotic communications.","Our FPGA prototypes of the chip design can send or receive packages in less than 2.5 microseconds, accelerating networking communications by more than 62x on average and improving energy consumption by more than 500x when compared to traditional ROS 2 software implementations on modern CPUs.","Additionally, it dramatically reduces maximum latency in ROS 2 networking communication by more than 30,000x.","In situations of peak latency, our design guarantees an isochronous response within 11 microseconds, a stark improvement over the potential hundreds of milliseconds reported by modern CPU systems under similar conditions."],"url":"http://arxiv.org/abs/2404.18208v1","category":"cs.RO"}
{"created":"2024-04-28 14:51:49","title":"Gate tunable edge magnetoplasmon resonators","abstract":"Quantum Hall systems are platforms of choice when it comes to study topological properties of condensed matter systems and anyonic exchange statistics. In this work we have developed a tunable radiofrequency edge magnetoplasmonic resonator meant to serve as a versatile platform for future interferometric devices. The resonance frequency of the system is controlled by both the magnetic field and a set of electrostatic gates. The gates allow us to change both the size of the resonant cavity and the electronic density of the two-dimensional electron gas. We show that we can continuously control the frequency response of our resonator, making it possible to develop an edge magnetoplasmon interferometer. As we reach smaller sizes of our resonator, finite size effects caused by the measurement probes manifest. We present a theoretical description of the system taking into account the spatial extension of the probing gates. In the future, such device will be a valuable tool to investigate the properties of non-abelian anyons in the fractional quantum Hall regime.","sentences":["Quantum Hall systems are platforms of choice when it comes to study topological properties of condensed matter systems and anyonic exchange statistics.","In this work we have developed a tunable radiofrequency edge magnetoplasmonic resonator meant to serve as a versatile platform for future interferometric devices.","The resonance frequency of the system is controlled by both the magnetic field and a set of electrostatic gates.","The gates allow us to change both the size of the resonant cavity and the electronic density of the two-dimensional electron gas.","We show that we can continuously control the frequency response of our resonator, making it possible to develop an edge magnetoplasmon interferometer.","As we reach smaller sizes of our resonator, finite size effects caused by the measurement probes manifest.","We present a theoretical description of the system taking into account the spatial extension of the probing gates.","In the future, such device will be a valuable tool to investigate the properties of non-abelian anyons in the fractional quantum Hall regime."],"url":"http://arxiv.org/abs/2404.18204v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-28 14:39:14","title":"Mean Field Game of High-Frequency Anticipatory Trading","abstract":"The interactions between a large population of high-frequency traders (HFTs) and a large trader (LT) who executes a certain amount of assets at discrete time points are studied. HFTs are faster in the sense that they trade continuously and predict the transactions of LT. A jump process is applied to model the transition of HFTs' attitudes towards inventories and the equilibrium is solved through the mean field game approach. When the crowd of HFTs is averse to running (ending) inventories, they first take then supply liquidity at each transaction of LT (throughout the whole execution period). Inventory-averse HFTs lower LT's costs if the market temporary impact is relatively large to the permanent one. What's more, the repeated liquidity consuming-supplying behavior of HFTs makes LT's optimal strategy close to uniform trading.","sentences":["The interactions between a large population of high-frequency traders (HFTs) and a large trader (LT) who executes a certain amount of assets at discrete time points are studied.","HFTs are faster in the sense that they trade continuously and predict the transactions of LT.","A jump process is applied to model the transition of HFTs' attitudes towards inventories and the equilibrium is solved through the mean field game approach.","When the crowd of HFTs is averse to running (ending) inventories, they first take then supply liquidity at each transaction of LT (throughout the whole execution period).","Inventory-averse HFTs lower LT's costs if the market temporary impact is relatively large to the permanent one.","What's more, the repeated liquidity consuming-supplying behavior of HFTs makes LT's optimal strategy close to uniform trading."],"url":"http://arxiv.org/abs/2404.18200v1","category":"q-fin.MF"}
{"created":"2024-04-28 14:19:40","title":"Curse of Dimensionality on Persistence Diagrams","abstract":"The stability of persistent homology has led to wide applications of the persistence diagram as a trusted topological descriptor in the presence of noise. However, with the increasing demand for high-dimension and low-sample-size data processing in modern science, it is questionable whether persistence diagrams retain their reliability in the presence of high-dimensional noise. This work aims to study the reliability of persistence diagrams in the high-dimension low-sample-size data setting. By analyzing the asymptotic behavior of persistence diagrams for high-dimensional random data, we show that persistence diagrams are no longer reliable descriptors of low-sample-size data under high-dimensional noise perturbations. We refer to this loss of reliability of persistence diagrams in such data settings as the curse of dimensionality on persistence diagrams. Next, we investigate the possibility of using normalized principal component analysis as a method for reducing the dimensionality of the high-dimensional observed data to resolve the curse of dimensionality. We show that this method can mitigate the curse of dimensionality on persistence diagrams. Our results shed some new light on the challenges of processing high-dimension low-sample-size data by persistence diagrams and provide a starting point for future research in this area.","sentences":["The stability of persistent homology has led to wide applications of the persistence diagram as a trusted topological descriptor in the presence of noise.","However, with the increasing demand for high-dimension and low-sample-size data processing in modern science, it is questionable whether persistence diagrams retain their reliability in the presence of high-dimensional noise.","This work aims to study the reliability of persistence diagrams in the high-dimension low-sample-size data setting.","By analyzing the asymptotic behavior of persistence diagrams for high-dimensional random data, we show that persistence diagrams are no longer reliable descriptors of low-sample-size data under high-dimensional noise perturbations.","We refer to this loss of reliability of persistence diagrams in such data settings as the curse of dimensionality on persistence diagrams.","Next, we investigate the possibility of using normalized principal component analysis as a method for reducing the dimensionality of the high-dimensional observed data to resolve the curse of dimensionality.","We show that this method can mitigate the curse of dimensionality on persistence diagrams.","Our results shed some new light on the challenges of processing high-dimension low-sample-size data by persistence diagrams and provide a starting point for future research in this area."],"url":"http://arxiv.org/abs/2404.18194v1","category":"math.ST"}
{"created":"2024-04-28 13:13:42","title":"Auto-Optimized Maximum Torque Per Ampere Control of IPMSM Using Dual Control for Exploration and Exploitation","abstract":"In this paper, a maximum torque per ampere (MTPA) control strategy for the interior permanent magnet synchronous motor (IPMSM) using dual control for exploration and exploitation (DCEE). In the proposed method, the permanent magnet flux and the difference between the $d$- and $q$-axis inductance are identified by multiple estimators using the recursive least square method. The initial values of the estimated parameters in different estimators are different. By using multiple estimators, exploration of the operational environment to reduce knowledge uncertainty can be realized. Compared to those MTPA control strategies based on the extremum-seeking method, the proposed method has better dynamic performance when speed or load varies. The effectiveness of the proposed method is verified by simulations.","sentences":["In this paper, a maximum torque per ampere (MTPA) control strategy for the interior permanent magnet synchronous motor (IPMSM) using dual control for exploration and exploitation (DCEE).","In the proposed method, the permanent magnet flux and the difference between the $d$- and $q$-axis inductance are identified by multiple estimators using the recursive least square method.","The initial values of the estimated parameters in different estimators are different.","By using multiple estimators, exploration of the operational environment to reduce knowledge uncertainty can be realized.","Compared to those MTPA control strategies based on the extremum-seeking method, the proposed method has better dynamic performance when speed or load varies.","The effectiveness of the proposed method is verified by simulations."],"url":"http://arxiv.org/abs/2404.18176v1","category":"eess.SY"}
{"created":"2024-04-28 12:53:13","title":"Monotonicity rules for the ratio of power series","abstract":"In this paper, we present some monotonicity rules for the ratio of two power series $x\\mapsto \\sum_{k=0}^\\infty a_k x^k / \\sum_{k=0}^\\infty b_k x^k$ under the assumption that the monotonicity of the sequence ${a_k/b_k}$ changes twice. Additionally, we introduce a local monotonicity rule in this paper.","sentences":["In this paper, we present some monotonicity rules for the ratio of two power series $x\\mapsto \\sum_{k=0}^\\infty a_k","x^k / \\sum_{k=0}^\\infty b_k","x^k$ under the assumption that the monotonicity of the sequence ${a_k/b_k}$ changes twice.","Additionally, we introduce a local monotonicity rule in this paper."],"url":"http://arxiv.org/abs/2404.18168v1","category":"math.CA"}
{"created":"2024-04-28 12:43:56","title":"Empirical approximation to invariant measures of mean-field Langevin dynamics","abstract":"This paper is concerned with the approximation to invariant measures for Langevin dynamics of McKean--Vlasov type. Under dissipativity and Lipschitz conditions, we prove that the empirical measures of both the mean-field and self-interacting Langevin dynamics converge to the invariant measure in the Wasserstein distance. Numerical experiments are conducted to illustrate theoretical results.","sentences":["This paper is concerned with the approximation to invariant measures for Langevin dynamics of McKean--Vlasov type.","Under dissipativity and Lipschitz conditions, we prove that the empirical measures of both the mean-field and self-interacting Langevin dynamics converge to the invariant measure in the Wasserstein distance.","Numerical experiments are conducted to illustrate theoretical results."],"url":"http://arxiv.org/abs/2404.18164v1","category":"math.PR"}
{"created":"2024-04-28 12:36:35","title":"Thermodynamic uncertainty relation for quantum entropy production","abstract":"In quantum thermodynamics, entropy production is usually defined in terms of the quantum relative entropy between two states. We derive a lower bound for the quantum entropy production in terms of the mean and variance of quantum observables, which we will refer to as a thermodynamic uncertainty relation (TUR) for the entropy production. In the absence of coherence between the states, our result reproduces classic TURs in stochastic thermodynamics. For the derivation of the TUR, we introduce a lower bound for a quantum generalization of the $\\chi^2$ divergence between two states and discuss its implications for stochastic and quantum thermodynamics, as well as the limiting case where it reproduces the quantum Cram\\'er-Rao inequality.","sentences":["In quantum thermodynamics, entropy production is usually defined in terms of the quantum relative entropy between two states.","We derive a lower bound for the quantum entropy production in terms of the mean and variance of quantum observables, which we will refer to as a thermodynamic uncertainty relation (TUR) for the entropy production.","In the absence of coherence between the states, our result reproduces classic TURs in stochastic thermodynamics.","For the derivation of the TUR, we introduce a lower bound for a quantum generalization of the $\\chi^2$ divergence between two states and discuss its implications for stochastic and quantum thermodynamics, as well as the limiting case where it reproduces the quantum Cram\\'er-Rao inequality."],"url":"http://arxiv.org/abs/2404.18163v1","category":"quant-ph"}
{"created":"2024-04-28 12:12:08","title":"ShapeMoir\u00e9: Channel-Wise Shape-Guided Network for Image Demoir\u00e9ing","abstract":"Photographing optoelectronic displays often introduces unwanted moir\\'e patterns due to analog signal interference between the pixel grids of the display and the camera sensor arrays. This work identifies two problems that are largely ignored by existing image demoir\\'eing approaches: 1) moir\\'e patterns vary across different channels (RGB); 2) repetitive patterns are constantly observed. However, employing conventional convolutional (CNN) layers cannot address these problems. Instead, this paper presents the use of our recently proposed Shape concept. It was originally employed to model consistent features from fragmented regions, particularly when identical or similar objects coexist in an RGB-D image. Interestingly, we find that the Shape information effectively captures the moir\\'e patterns in artifact images. Motivated by this discovery, we propose a ShapeMoir\\'e method to aid in image demoir\\'eing. Beyond modeling shape features at the patch-level, we further extend this to the global image-level and design a novel Shape-Architecture. Consequently, our proposed method, equipped with both ShapeConv and Shape-Architecture, can be seamlessly integrated into existing approaches without introducing additional parameters or computation overhead during inference. We conduct extensive experiments on four widely used datasets, and the results demonstrate that our ShapeMoir\\'e achieves state-of-the-art performance, particularly in terms of the PSNR metric. We then apply our method across four popular architectures to showcase its generalization capabilities. Moreover, our ShapeMoir\\'e is robust and viable under real-world demoir\\'eing scenarios involving smartphone photographs.","sentences":["Photographing optoelectronic displays often introduces unwanted moir\\'e patterns due to analog signal interference between the pixel grids of the display and the camera sensor arrays.","This work identifies two problems that are largely ignored by existing image demoir\\'eing approaches: 1) moir\\'e patterns vary across different channels (RGB); 2) repetitive patterns are constantly observed.","However, employing conventional convolutional (CNN) layers cannot address these problems.","Instead, this paper presents the use of our recently proposed Shape concept.","It was originally employed to model consistent features from fragmented regions, particularly when identical or similar objects coexist in an RGB-D image.","Interestingly, we find that the Shape information effectively captures the moir\\'e patterns in artifact images.","Motivated by this discovery, we propose a ShapeMoir\\'e method to aid in image demoir\\'eing.","Beyond modeling shape features at the patch-level, we further extend this to the global image-level and design a novel Shape-Architecture.","Consequently, our proposed method, equipped with both ShapeConv and Shape-Architecture, can be seamlessly integrated into existing approaches without introducing additional parameters or computation overhead during inference.","We conduct extensive experiments on four widely used datasets, and the results demonstrate that our ShapeMoir\\'e achieves state-of-the-art performance, particularly in terms of the PSNR metric.","We then apply our method across four popular architectures to showcase its generalization capabilities.","Moreover, our ShapeMoir\\'e is robust and viable under real-world demoir\\'eing scenarios involving smartphone photographs."],"url":"http://arxiv.org/abs/2404.18155v1","category":"cs.CV"}
{"created":"2024-04-28 12:11:34","title":"Explaining vague language","abstract":"Why is language vague? Vagueness may be explained and rationalized if it can be shown that vague language is more useful to speaker and hearer than precise language. In a well-known paper, Lipman proposes a game-theoretic account of vagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot be strictly better than precision at equilibrium. More recently, \\'Egr\\'e, Spector, Mortier and Verheyen have put forward a Bayesian account of vagueness establishing that using vague words can be strictly more informative than using precise words. This paper proposes to compare both results and to explain why they are not in contradiction. Lipman's definition of vagueness relies exclusively on a property of signaling strategies, without making any assumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of semantic content. We argue that the semantic account of vagueness is needed, and more adequate and explanatory of vagueness.","sentences":["Why is language vague?","Vagueness may be explained and rationalized if it can be shown that vague language is more useful to speaker and hearer than precise language.","In a well-known paper, Lipman proposes a game-theoretic account of vagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot be strictly better than precision at equilibrium.","More recently, \\'Egr\\'e, Spector, Mortier and Verheyen have put forward a Bayesian account of vagueness establishing that using vague words can be strictly more informative than using precise words.","This paper proposes to compare both results and to explain why they are not in contradiction.","Lipman's definition of vagueness relies exclusively on a property of signaling strategies, without making any assumptions about the lexicon, whereas \\'Egr\\'e et al.'s involves a layer of semantic content.","We argue that the semantic account of vagueness is needed, and more adequate and explanatory of vagueness."],"url":"http://arxiv.org/abs/2404.18154v1","category":"cs.CL"}
{"created":"2024-04-28 12:01:23","title":"Decidability of Graph Neural Networks via Logical Characterizations","abstract":"We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic. We use a family of recently-discovered decidable logics involving \"Presburger quantifiers\". We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs. We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs. We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems.","sentences":["We present results concerning the expressiveness and decidability of a popular graph learning formalism, graph neural networks (GNNs), exploiting connections with logic.","We use a family of recently-discovered decidable logics involving \"Presburger quantifiers\".","We show how to use these logics to measure the expressiveness of classes of GNNs, in some cases getting exact correspondences between the expressiveness of logics and GNNs.","We also employ the logics, and the techniques used to analyze them, to obtain decision procedures for verification problems over GNNs.","We complement this with undecidability results for static analysis problems involving the logics, as well as for GNN verification problems."],"url":"http://arxiv.org/abs/2404.18151v1","category":"cs.LO"}
{"created":"2024-04-28 11:42:54","title":"Decentralized Peer Review in Open Science: A Mechanism Proposal","abstract":"Peer review is a laborious, yet essential, part of academic publishing with crucial impact on the scientific endeavor. The current lack of incentives and transparency harms the credibility of this process. Researchers are neither rewarded for superior nor penalized for bad reviews. Additionally, confidential reports cause a loss of insights and make the review process vulnerable to scientific misconduct. We propose a community-owned and -governed system that 1) remunerates reviewers for their efforts, 2) publishes the (anonymized) reports for scrutiny by the community, 3) tracks reputation of reviewers and 4) provides digital certificates. Automated by transparent smart-contract blockchain technology, the system aims to increase quality and speed of peer review while lowering the chance and impact of erroneous judgements.","sentences":["Peer review is a laborious, yet essential, part of academic publishing with crucial impact on the scientific endeavor.","The current lack of incentives and transparency harms the credibility of this process.","Researchers are neither rewarded for superior nor penalized for bad reviews.","Additionally, confidential reports cause a loss of insights and make the review process vulnerable to scientific misconduct.","We propose a community-owned and -governed system that 1) remunerates reviewers for their efforts, 2) publishes the (anonymized) reports for scrutiny by the community, 3) tracks reputation of reviewers and 4) provides digital certificates.","Automated by transparent smart-contract blockchain technology, the system aims to increase quality and speed of peer review while lowering the chance and impact of erroneous judgements."],"url":"http://arxiv.org/abs/2404.18148v1","category":"cs.GT"}
{"created":"2024-04-28 10:43:59","title":"Numerical dissipation induced by the low-pass filtering in nonlinear gyrokinetic simulations","abstract":"De-aliasing is an essential procedure for eliminating the aliasing error in nonlinear simulations, such as nonlinear gyrokinetic turbulence simulations. An ideal approach to de-aliasing in the periodic dimension is Fourier truncation. Finite difference low-pass filtering applied in the non-periodic direction strongly dampens aliasing modes. At the same time, it induces numerical dissipation in the region of the physically realistic solution. It is shown analytically that the long-wave dissipation coefficient is proportional to the (Np-3) power of the wavenumber under desirable constraints satisfying the highest order accuracy, where Np is the number of filter points. Numerical results after applying the optimised low-pass filtering to the nonlinear gyrokinetic turbulence simulation suggest that the nine-point format preserves intact mesoscopic zonal structures in Tokamak plasmas, and is therefore suitable for long-time nonlinear turbulence simulations.","sentences":["De-aliasing is an essential procedure for eliminating the aliasing error in nonlinear simulations, such as nonlinear gyrokinetic turbulence simulations.","An ideal approach to de-aliasing in the periodic dimension is Fourier truncation.","Finite difference low-pass filtering applied in the non-periodic direction strongly dampens aliasing modes.","At the same time, it induces numerical dissipation in the region of the physically realistic solution.","It is shown analytically that the long-wave dissipation coefficient is proportional to the (Np-3) power of the wavenumber under desirable constraints satisfying the highest order accuracy, where Np is the number of filter points.","Numerical results after applying the optimised low-pass filtering to the nonlinear gyrokinetic turbulence simulation suggest that the nine-point format preserves intact mesoscopic zonal structures in Tokamak plasmas, and is therefore suitable for long-time nonlinear turbulence simulations."],"url":"http://arxiv.org/abs/2404.18139v1","category":"physics.plasm-ph"}
{"created":"2024-04-28 10:35:23","title":"Domar aggregation under nonneutral elasticity of substitution","abstract":"The characteristics inherent in a Domar aggregation, when all sectoral productions embody a common nonneutral elasticity of substitution, is examined. There, the general equilibrium propagation of productivity changes entails structural transformation that brings nonlinearities in their aggregation into price indices. We show that negative singularity, such that a finite productivity decrease induces an infinitely large price, is possible in an inelastic economy, while positive singularity, such that a finite productivity increase induces a zero price, is possible in an elastic economy. Regarding the aggregate outputs, two independent productivity changes will have synergism in an elastic economy, whereas negative synergism will be prevalent in an inelastic economy. Neither issue is of concern in a Cobb-Douglas economy where the elasticity of substitution is everywhere neutral.","sentences":["The characteristics inherent in a Domar aggregation, when all sectoral productions embody a common nonneutral elasticity of substitution, is examined.","There, the general equilibrium propagation of productivity changes entails structural transformation that brings nonlinearities in their aggregation into price indices.","We show that negative singularity, such that a finite productivity decrease induces an infinitely large price, is possible in an inelastic economy, while positive singularity, such that a finite productivity increase induces a zero price, is possible in an elastic economy.","Regarding the aggregate outputs, two independent productivity changes will have synergism in an elastic economy, whereas negative synergism will be prevalent in an inelastic economy.","Neither issue is of concern in a Cobb-Douglas economy where the elasticity of substitution is everywhere neutral."],"url":"http://arxiv.org/abs/2404.18137v1","category":"econ.TH"}
{"created":"2024-04-28 09:52:58","title":"Universality of the thermodynamics of a quantum-mechanically radiating black hole departing from thermality","abstract":"Mathur and Mehta won the third prize in the 2023 Gravity Research Foundation Essay Competition for proving the universality of black hole (BH) thermodynamics. Specifically, they demonstrated that any Extremely Compact Object (ECO) must have the same BH thermodynamic properties regardless of whether or not the ECO possesses an event horizon. The result is remarkable, but it was obtained under the approximation according to which the BH emission spectrum has an exactly thermal character. In fact, strong arguments based on energy conservation and BH back reaction imply that the spectrum of the Hawking radiation cannot be exactly thermal. In this work the result of Mathur and Mehta will be extended to the case where the radiation spectrum is not exactly thermal using the concept of BH dynamical state.","sentences":["Mathur and Mehta won the third prize in the 2023 Gravity Research Foundation Essay Competition for proving the universality of black hole (BH) thermodynamics.","Specifically, they demonstrated that any Extremely Compact Object (ECO) must have the same BH thermodynamic properties regardless of whether or not the ECO possesses an event horizon.","The result is remarkable, but it was obtained under the approximation according to which the BH emission spectrum has an exactly thermal character.","In fact, strong arguments based on energy conservation and BH back reaction imply that the spectrum of the Hawking radiation cannot be exactly thermal.","In this work the result of Mathur and Mehta will be extended to the case where the radiation spectrum is not exactly thermal using the concept of BH dynamical state."],"url":"http://arxiv.org/abs/2404.18128v1","category":"gr-qc"}
{"created":"2024-04-28 09:21:36","title":"Power laws and logarithmic oscillations in diffusion processes on infinite countable ultrametric spaces","abstract":"It is shown that if the initial condition of the Cauchy problem for the diffusion equation on a general infinite countable ultrametric space is spherically symmetric with respect to some point, then this problem has an exact analytical solution. A general solution of this problem is presented for pure ultrametric diffusion, as well as for ultrametric diffusion with a reaction sink concentrated at the center of spherical symmetry. Conditions on the ultrametric and the distribution of the number of states in ultrametric spheres are found that lead at large times to the asymptotic behavior of the solutions obtained in the form of a power law modulated by a bounded function that is log-periodic under some additional conditions.","sentences":["It is shown that if the initial condition of the Cauchy problem for the diffusion equation on a general infinite countable ultrametric space is spherically symmetric with respect to some point, then this problem has an exact analytical solution.","A general solution of this problem is presented for pure ultrametric diffusion, as well as for ultrametric diffusion with a reaction sink concentrated at the center of spherical symmetry.","Conditions on the ultrametric and the distribution of the number of states in ultrametric spheres are found that lead at large times to the asymptotic behavior of the solutions obtained in the form of a power law modulated by a bounded function that is log-periodic under some additional conditions."],"url":"http://arxiv.org/abs/2404.18123v1","category":"math-ph"}
{"created":"2024-04-28 08:58:56","title":"Performance advantage of quantum hypothesis testing for partially coherent optical sources","abstract":"Determining the presence of a potential optical source in the interest region is important for an imaging system and can be achieved by using hypothesis testing. The previous studies assume that the potential source is completely incoherent. In this paper, this problem is generalized to the scenario with partially coherent sources and any prior probabilities. We compare the error probability limit given by the quantum Helstrom bound with the error probability given by direct decision based on the prior probability. On this basis, the quantum-optimal detection advantage and detection-useless region are analyzed. For practical purposes, we propose a specific detection strategy using binary spatial-mode demultiplexing, which can be used in the scenarios without any prior information. This strategy shows superior detection performance and the results hold prospects for achieving super-resolved microscopic and astronomical imaging.","sentences":["Determining the presence of a potential optical source in the interest region is important for an imaging system and can be achieved by using hypothesis testing.","The previous studies assume that the potential source is completely incoherent.","In this paper, this problem is generalized to the scenario with partially coherent sources and any prior probabilities.","We compare the error probability limit given by the quantum Helstrom bound with the error probability given by direct decision based on the prior probability.","On this basis, the quantum-optimal detection advantage and detection-useless region are analyzed.","For practical purposes, we propose a specific detection strategy using binary spatial-mode demultiplexing, which can be used in the scenarios without any prior information.","This strategy shows superior detection performance and the results hold prospects for achieving super-resolved microscopic and astronomical imaging."],"url":"http://arxiv.org/abs/2404.18120v1","category":"quant-ph"}
{"created":"2024-04-28 08:52:37","title":"Products of commutators in matrix rings","abstract":"Let $R$ be a ring and let $n\\ge 2$. We discuss the question of whether every element in the matrix ring $M_n(R)$ is a product of (additive) commutators $[x,y]=xy-yx$, for $x,y\\in M_n(R)$. An example showing that this does not always hold, even when $R$ is commutative, is provided. If, however, $R$ has Bass stable rank one, then under various additional conditions every element in $M_n(R)$ is a product of three commutators. Further, if $R$ is a division ring with infinite center, then every element in $M_n(R)$ is a product of two commutators. If $R$ is a field and $a\\in M_n(R)$, then every element in $M_n(R)$ is a sum of elements of the form $[a,x][a,y]$ with $x,y\\in M_n(R)$ if and only if the degree of the minimal polynomial of $a$ is greater than $2$.","sentences":["Let $R$ be a ring and let $n\\ge 2$.","We discuss the question of whether every element in the matrix ring $M_n(R)$ is a product of (additive) commutators","$[x,y]=xy-yx$, for $x,y\\in M_n(R)$.","An example showing that this does not always hold, even when $R$ is commutative, is provided.","If, however, $R$ has Bass stable rank one, then under various additional conditions every element in $M_n(R)$ is a product of three commutators.","Further, if $R$ is a division ring with infinite center, then every element in $M_n(R)$ is a product of two commutators.","If $R$ is a field and $a\\in M_n(R)$, then every element in $M_n(R)$ is a sum of elements of the form $[a,x][a,y]$ with $x,y\\in M_n(R)$ if and only if the degree of the minimal polynomial of $a$ is greater than $2$."],"url":"http://arxiv.org/abs/2404.18116v1","category":"math.RA"}
{"created":"2024-04-28 08:44:28","title":"Deep Boosting Learning: A Brand-new Cooperative Approach for Image-Text Matching","abstract":"Image-text matching remains a challenging task due to heterogeneous semantic diversity across modalities and insufficient distance separability within triplets. Different from previous approaches focusing on enhancing multi-modal representations or exploiting cross-modal correspondence for more accurate retrieval, in this paper we aim to leverage the knowledge transfer between peer branches in a boosting manner to seek a more powerful matching model. Specifically, we propose a brand-new Deep Boosting Learning (DBL) algorithm, where an anchor branch is first trained to provide insights into the data properties, with a target branch gaining more advanced knowledge to develop optimal features and distance metrics. Concretely, an anchor branch initially learns the absolute or relative distance between positive and negative pairs, providing a foundational understanding of the particular network and data distribution. Building upon this knowledge, a target branch is concurrently tasked with more adaptive margin constraints to further enlarge the relative distance between matched and unmatched samples. Extensive experiments validate that our DBL can achieve impressive and consistent improvements based on various recent state-of-the-art models in the image-text matching field, and outperform related popular cooperative strategies, e.g., Conventional Distillation, Mutual Learning, and Contrastive Learning. Beyond the above, we confirm that DBL can be seamlessly integrated into their training scenarios and achieve superior performance under the same computational costs, demonstrating the flexibility and broad applicability of our proposed method. Our code is publicly available at: https://github.com/Paranioar/DBL.","sentences":["Image-text matching remains a challenging task due to heterogeneous semantic diversity across modalities and insufficient distance separability within triplets.","Different from previous approaches focusing on enhancing multi-modal representations or exploiting cross-modal correspondence for more accurate retrieval, in this paper we aim to leverage the knowledge transfer between peer branches in a boosting manner to seek a more powerful matching model.","Specifically, we propose a brand-new Deep Boosting Learning (DBL) algorithm, where an anchor branch is first trained to provide insights into the data properties, with a target branch gaining more advanced knowledge to develop optimal features and distance metrics.","Concretely, an anchor branch initially learns the absolute or relative distance between positive and negative pairs, providing a foundational understanding of the particular network and data distribution.","Building upon this knowledge, a target branch is concurrently tasked with more adaptive margin constraints to further enlarge the relative distance between matched and unmatched samples.","Extensive experiments validate that our DBL can achieve impressive and consistent improvements based on various recent state-of-the-art models in the image-text matching field, and outperform related popular cooperative strategies, e.g., Conventional Distillation, Mutual Learning, and Contrastive Learning.","Beyond the above, we confirm that DBL can be seamlessly integrated into their training scenarios and achieve superior performance under the same computational costs, demonstrating the flexibility and broad applicability of our proposed method.","Our code is publicly available at: https://github.com/Paranioar/DBL."],"url":"http://arxiv.org/abs/2404.18114v1","category":"cs.CV"}
{"created":"2024-04-29 17:16:22","title":"Spivavtor: An Instruction Tuned Ukrainian Text Editing Model","abstract":"We introduce Spivavtor, a dataset, and instruction-tuned models for text editing focused on the Ukrainian language. Spivavtor is the Ukrainian-focused adaptation of the English-only CoEdIT model. Similar to CoEdIT, Spivavtor performs text editing tasks by following instructions in Ukrainian. This paper describes the details of the Spivavtor-Instruct dataset and Spivavtor models. We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as Grammatical Error Correction (GEC), Text Simplification, Coherence, and Paraphrasing, and demonstrate its superior performance on all of them. We publicly release our best-performing models and data as resources to the community to advance further research in this space.","sentences":["We introduce Spivavtor, a dataset, and instruction-tuned models for text editing focused on the Ukrainian language.","Spivavtor is the Ukrainian-focused adaptation of the English-only CoEdIT model.","Similar to CoEdIT, Spivavtor performs text editing tasks by following instructions in Ukrainian.","This paper describes the details of the Spivavtor-Instruct dataset and Spivavtor models.","We evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as Grammatical Error Correction (GEC), Text Simplification, Coherence, and Paraphrasing, and demonstrate its superior performance on all of them.","We publicly release our best-performing models and data as resources to the community to advance further research in this space."],"url":"http://arxiv.org/abs/2404.18880v1","category":"cs.CL"}
{"created":"2024-04-29 16:42:58","title":"MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection","abstract":"In this paper, we present a different way to use two modalities, in which either one modality or the other is seen by a single model. This can be useful when adapting an unimodal model to leverage more information while respecting a limited computational budget. This would mean having a single model that is able to deal with any modalities. To describe this, we coined the term anymodal learning. An example of this, is a use case where, surveillance in a room when the lights are off would be much more valuable using an infrared modality while a visible one would provide more discriminative information when lights are on. This work investigates how to efficiently leverage visible and infrared/thermal modalities for transformer-based object detection backbone to create an anymodal architecture. Our work does not create any inference overhead during the testing while exploring an effective way to exploit the two modalities during the training. To accomplish such a task, we introduce the novel anymodal training technique: Mixed Patches (MiPa), in conjunction with a patch-wise domain agnostic module, which is responsible of learning the best way to find a common representation of both modalities. This approach proves to be able to balance modalities by reaching competitive results on individual modality benchmarks with the alternative of using an unimodal architecture on three different visible-infrared object detection datasets. Finally, our proposed method, when used as a regularization for the strongest modality, can beat the performance of multimodal fusion methods while only requiring a single modality during inference. Notably, MiPa became the state-of-the-art on the LLVIP visible/infrared benchmark. Code: https://github.com/heitorrapela/MiPa","sentences":["In this paper, we present a different way to use two modalities, in which either one modality or the other is seen by a single model.","This can be useful when adapting an unimodal model to leverage more information while respecting a limited computational budget.","This would mean having a single model that is able to deal with any modalities.","To describe this, we coined the term anymodal learning.","An example of this, is a use case where, surveillance in a room when the lights are off would be much more valuable using an infrared modality while a visible one would provide more discriminative information when lights are on.","This work investigates how to efficiently leverage visible and infrared/thermal modalities for transformer-based object detection backbone to create an anymodal architecture.","Our work does not create any inference overhead during the testing while exploring an effective way to exploit the two modalities during the training.","To accomplish such a task, we introduce the novel anymodal training technique: Mixed Patches (MiPa), in conjunction with a patch-wise domain agnostic module, which is responsible of learning the best way to find a common representation of both modalities.","This approach proves to be able to balance modalities by reaching competitive results on individual modality benchmarks with the alternative of using an unimodal architecture on three different visible-infrared object detection datasets.","Finally, our proposed method, when used as a regularization for the strongest modality, can beat the performance of multimodal fusion methods while only requiring a single modality during inference.","Notably, MiPa became the state-of-the-art on the LLVIP visible/infrared benchmark.","Code: https://github.com/heitorrapela/MiPa"],"url":"http://arxiv.org/abs/2404.18849v1","category":"cs.CV"}
{"created":"2024-04-29 16:26:27","title":"Accurate adaptive deep learning method for solving elliptic problems","abstract":"Deep learning method is of great importance in solving partial differential equations. In this paper, inspired by the failure-informed idea proposed by Gao et.al. (SIAM Journal on Scientific Computing 45(4)(2023)) and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems. Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling. Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy. Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases.","sentences":["Deep learning method is of great importance in solving partial differential equations.","In this paper, inspired by the failure-informed idea proposed by Gao et.al.","(SIAM Journal on Scientific Computing 45(4)(2023))","and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems.","Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling.","Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy.","Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases."],"url":"http://arxiv.org/abs/2404.18838v1","category":"math.NA"}
{"created":"2024-04-29 15:40:40","title":"A Partial Replication of MaskFormer in TensorFlow on TPUs for the TensorFlow Model Garden","abstract":"This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs). Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet the specifications of the MaskFormer model. We address key challenges encountered during the replication, non-convergence issues, slow training, adaptation of loss functions, and the integration of TPU-specific functionalities. We verify our reproduced implementation and present qualitative results on the COCO dataset. Although our implementation meets some of the objectives for end-to-end reproducibility, we encountered challenges in replicating the PyTorch version of MaskFormer in TensorFlow. This replication process is not straightforward and requires substantial engineering efforts. Specifically, it necessitates the customization of various components within the TFMG, alongside thorough verification and hyper-parameter tuning. The replication is available at: https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer","sentences":["This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs).","Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet the specifications of the MaskFormer model.","We address key challenges encountered during the replication, non-convergence issues, slow training, adaptation of loss functions, and the integration of TPU-specific functionalities.","We verify our reproduced implementation and present qualitative results on the COCO dataset.","Although our implementation meets some of the objectives for end-to-end reproducibility, we encountered challenges in replicating the PyTorch version of MaskFormer in TensorFlow.","This replication process is not straightforward and requires substantial engineering efforts.","Specifically, it necessitates the customization of various components within the TFMG, alongside thorough verification and hyper-parameter tuning.","The replication is available at: https://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer"],"url":"http://arxiv.org/abs/2404.18801v1","category":"cs.CV"}
{"created":"2024-04-29 13:23:38","title":"How Deep Is Your Gaze? Leveraging Distance in Image-Based Gaze Analysis","abstract":"Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data. They are typically extracted with a constant size, approximating the foveated area. As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques. In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance. Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area. We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths. Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization. We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization.","sentences":["Image thumbnails are a valuable data source for fixation filtering, scanpath classification, and visualization of eye tracking data.","They are typically extracted with a constant size, approximating the foveated area.","As a consequence, the focused area of interest in the scene becomes less prominent in the thumbnail with increasing distance, affecting image-based analysis techniques.","In this work, we propose depth-adaptive thumbnails, a method for varying image size according to the eye-to-object distance.","Adjusting the visual angle relative to the distance leads to a zoom effect on the focused area.","We evaluate our approach on recordings in augmented reality, investigating the similarity of thumbnails and scanpaths.","Our quantitative findings suggest that considering the eye-to-object distance improves the quality of data analysis and visualization.","We demonstrate the utility of depth-adaptive thumbnails for applications in scanpath comparison and visualization."],"url":"http://arxiv.org/abs/2404.18680v1","category":"cs.HC"}
{"created":"2024-04-29 13:15:22","title":"Exact symmetry conservation and automatic mesh refinement in discrete initial boundary value problems","abstract":"We present a novel solution procedure for initial boundary value problems. The procedure is based on an action principle, in which coordinate maps are included as dynamical degrees of freedom. This reparametrization invariant action is formulated in an abstract parameter space and an energy density scale associated with the space-time coordinates separates the dynamics of the coordinate maps and of the propagating fields. Treating coordinates as dependent, i.e. dynamical quantities, offers the opportunity to discretize the action while retaining all space-time symmetries and also provides the basis for automatic adaptive mesh refinement (AMR). The presence of unbroken space-time symmetries after discretization also ensures that the associated continuum Noether charges remain exactly conserved. The presence of coordinate maps in addition provides new freedom in the choice of boundary conditions. An explicit numerical example for wave propagation in $1+1$ dimensions is provided, using recently developed regularized summation-by-parts finite difference operators.","sentences":["We present a novel solution procedure for initial boundary value problems.","The procedure is based on an action principle, in which coordinate maps are included as dynamical degrees of freedom.","This reparametrization invariant action is formulated in an abstract parameter space and an energy density scale associated with the space-time coordinates separates the dynamics of the coordinate maps and of the propagating fields.","Treating coordinates as dependent, i.e. dynamical quantities, offers the opportunity to discretize the action while retaining all space-time symmetries and also provides the basis for automatic adaptive mesh refinement (AMR).","The presence of unbroken space-time symmetries after discretization also ensures that the associated continuum Noether charges remain exactly conserved.","The presence of coordinate maps in addition provides new freedom in the choice of boundary conditions.","An explicit numerical example for wave propagation in $1+1$ dimensions is provided, using recently developed regularized summation-by-parts finite difference operators."],"url":"http://arxiv.org/abs/2404.18676v1","category":"math.NA"}
{"created":"2024-04-29 10:59:07","title":"The link between hyperuniformity, Coulomb energy, and Wasserstein distance to Lebesgue for two-dimensional point processes","abstract":"We investigate the interplay between three possible properties of stationary point processes: i) Finite Coulomb energy with short-scale regularization, ii) Finite $2$-Wasserstein transportation distance to the Lebesgue measure and iii) Hyperuniformity. In dimension $2$, we prove that i) implies ii), which is known to imply iii), and we provide simple counter-examples to both converse implications. However, we prove that ii) implies i) for processes with a uniformly bounded density of points, and that i) - finiteness of the regularized Coulomb energy - is equivalent to a certain property of quantitative hyperuniformity that is just slightly stronger than hyperuniformity itself.   Our proof relies on the classical link between $H^{-1}$-norm and $2$-Wasserstein distance between measures, on the screening construction for Coulomb gases (of which we present an adaptation to $2$-Wasserstein space which might be of independent interest), and on recent necessary and sufficient conditions for the existence of stationary \"electric\" fields compatible with a given stationary point process.","sentences":["We investigate the interplay between three possible properties of stationary point processes: i) Finite Coulomb energy with short-scale regularization, ii) Finite $2$-Wasserstein transportation distance to the Lebesgue measure and iii) Hyperuniformity.","In dimension $2$, we prove that i) implies ii), which is known to imply iii), and we provide simple counter-examples to both converse implications.","However, we prove that ii) implies i) for processes with a uniformly bounded density of points, and that i) - finiteness of the regularized Coulomb energy - is equivalent to a certain property of quantitative hyperuniformity that is just slightly stronger than hyperuniformity itself.   ","Our proof relies on the classical link between $H^{-1}$-norm and $2$-Wasserstein distance between measures, on the screening construction for Coulomb gases (of which we present an adaptation to $2$-Wasserstein space which might be of independent interest), and on recent necessary and sufficient conditions for the existence of stationary \"electric\" fields compatible with a given stationary point process."],"url":"http://arxiv.org/abs/2404.18588v1","category":"math.PR"}
{"created":"2024-04-29 09:34:25","title":"Time Machine GPT","abstract":"Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.","sentences":["Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata.","This approach is not aligned with the evolving nature of language.","Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data.","This paper presents a new approach: a series of point-in-time LLMs called Time Machine GPT (TiMaGPT), specifically designed to be nonprognosticative.","This ensures they remain uninformed about future factual information and linguistic changes.","This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic.","We provide access to both the models and training datasets."],"url":"http://arxiv.org/abs/2404.18543v1","category":"cs.CL"}
{"created":"2024-04-29 06:44:33","title":"Clicks2Line: Using Lines for Interactive Image Segmentation","abstract":"For click-based interactive segmentation methods, reducing the number of clicks required to obtain a desired segmentation result is essential. Although recent click-based methods yield decent segmentation results, we observe that substantial amount of clicks are required to segment elongated regions. To reduce the amount of user-effort required, we propose using lines instead of clicks for such cases. In this paper, an interactive segmentation algorithm which adaptively adopts either clicks or lines as input is proposed. Experimental results demonstrate that using lines can generate better segmentation results than clicks for several cases.","sentences":["For click-based interactive segmentation methods, reducing the number of clicks required to obtain a desired segmentation result is essential.","Although recent click-based methods yield decent segmentation results, we observe that substantial amount of clicks are required to segment elongated regions.","To reduce the amount of user-effort required, we propose using lines instead of clicks for such cases.","In this paper, an interactive segmentation algorithm which adaptively adopts either clicks or lines as input is proposed.","Experimental results demonstrate that using lines can generate better segmentation results than clicks for several cases."],"url":"http://arxiv.org/abs/2404.18461v1","category":"cs.CV"}
{"created":"2024-04-29 06:35:34","title":"Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild","abstract":"Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training. However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks. Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists. In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios. To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability. We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks. Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches. Codes are available at https://github.com/GitGyun/chameleon.","sentences":["Large language models have evolved data-efficient generalists, benefiting from the universal language interface and large-scale pre-training.","However, constructing a data-efficient generalist for dense visual prediction presents a distinct challenge due to the variation in label structures across different tasks.","Consequently, generalization to unseen dense prediction tasks in the low-data regime is not straightforward and has received less attention from previous vision generalists.","In this study, we explore a universal model that can flexibly adapt to unseen dense label structures with a few examples, enabling it to serve as a data-efficient vision generalist in diverse real-world scenarios.","To this end, we base our method on a powerful meta-learning framework and explore several axes to improve its performance and versatility for real-world problems, such as flexible adaptation mechanisms and scalability.","We evaluate our model across a spectrum of unseen real-world scenarios where low-shot learning is desirable, including video, 3D, medical, biological, and user-interactive tasks.","Equipped with a generic architecture and an effective adaptation mechanism, our model flexibly adapts to all of these tasks with at most 50 labeled images, showcasing a significant advancement over existing data-efficient generalist approaches.","Codes are available at https://github.com/GitGyun/chameleon."],"url":"http://arxiv.org/abs/2404.18459v1","category":"cs.CV"}
{"created":"2024-04-29 06:32:28","title":"Autonomous Quality and Hallucination Assessment for Virtual Tissue Staining and Digital Pathology","abstract":"Histopathological staining of human tissue is essential in the diagnosis of various diseases. The recent advances in virtual tissue staining technologies using AI alleviate some of the costly and tedious steps involved in the traditional histochemical staining process, permitting multiplexed rapid staining of label-free tissue without using staining reagents, while also preserving tissue. However, potential hallucinations and artifacts in these virtually stained tissue images pose concerns, especially for the clinical utility of these approaches. Quality assessment of histology images is generally performed by human experts, which can be subjective and depends on the training level of the expert. Here, we present an autonomous quality and hallucination assessment method (termed AQuA), mainly designed for virtual tissue staining, while also being applicable to histochemical staining. AQuA achieves 99.8% accuracy when detecting acceptable and unacceptable virtually stained tissue images without access to ground truth, also presenting an agreement of 98.5% with the manual assessments made by board-certified pathologists. Besides, AQuA achieves super-human performance in identifying realistic-looking, virtually stained hallucinatory images that would normally mislead human diagnosticians by deceiving them into diagnosing patients that never existed. We further demonstrate the wide adaptability of AQuA across various virtually and histochemically stained tissue images and showcase its strong external generalization to detect unseen hallucination patterns of virtual staining network models as well as artifacts observed in the traditional histochemical staining workflow. This framework creates new opportunities to enhance the reliability of virtual staining and will provide quality assurance for various image generation and transformation tasks in digital pathology and computational imaging.","sentences":["Histopathological staining of human tissue is essential in the diagnosis of various diseases.","The recent advances in virtual tissue staining technologies using AI alleviate some of the costly and tedious steps involved in the traditional histochemical staining process, permitting multiplexed rapid staining of label-free tissue without using staining reagents, while also preserving tissue.","However, potential hallucinations and artifacts in these virtually stained tissue images pose concerns, especially for the clinical utility of these approaches.","Quality assessment of histology images is generally performed by human experts, which can be subjective and depends on the training level of the expert.","Here, we present an autonomous quality and hallucination assessment method (termed AQuA), mainly designed for virtual tissue staining, while also being applicable to histochemical staining.","AQuA achieves 99.8% accuracy when detecting acceptable and unacceptable virtually stained tissue images without access to ground truth, also presenting an agreement of 98.5% with the manual assessments made by board-certified pathologists.","Besides, AQuA achieves super-human performance in identifying realistic-looking, virtually stained hallucinatory images that would normally mislead human diagnosticians by deceiving them into diagnosing patients that never existed.","We further demonstrate the wide adaptability of AQuA across various virtually and histochemically stained tissue images and showcase its strong external generalization to detect unseen hallucination patterns of virtual staining network models as well as artifacts observed in the traditional histochemical staining workflow.","This framework creates new opportunities to enhance the reliability of virtual staining and will provide quality assurance for various image generation and transformation tasks in digital pathology and computational imaging."],"url":"http://arxiv.org/abs/2404.18458v1","category":"eess.IV"}
{"created":"2024-04-29 05:22:23","title":"Metasurface-based Toroidal Lenslet Array Design for Addressing Laser Guide Star Elongation","abstract":"The Giant Magellan Telescope will use laser tomography adaptive optics to correct for atmospheric turbulence using artificial guide stars created in the sodium layer of the atmosphere (altitude ~95km). The sodium layer has appreciable thickness (~11km) and this results in the laser guide star being an elongated cylinder shape. Wavefront sensing with a Shack-Hartmann is challenging, as subapertures located further away from the laser launch position image an increasingly elongated perspective of the laser guide star. Large detectors can be used to adequately pack and sample the images on the detector, however, this increases readout noise and limits the design space available for the wavefront sensor. To tackle this challenge, we propose an original solution based on nano-engineered meta-optics tailored to produce a spatially varying anamorphic image scale compression. We present meta-lenslet array designs that can deliver ~100% of the full anamorphic image size reduction required for focal lengths down to 8mm, and greater than 50% image size reduction for focal lengths down to 2mm. This will allow greatly improved sampling of the available information across the whole wavefront sensor, while still being a viable design within the limits of current-generation fabrication facilities.","sentences":["The Giant Magellan Telescope will use laser tomography adaptive optics to correct for atmospheric turbulence using artificial guide stars created in the sodium layer of the atmosphere (altitude ~95km).","The sodium layer has appreciable thickness (~11km) and this results in the laser guide star being an elongated cylinder shape.","Wavefront sensing with a Shack-Hartmann is challenging, as subapertures located further away from the laser launch position image an increasingly elongated perspective of the laser guide star.","Large detectors can be used to adequately pack and sample the images on the detector, however, this increases readout noise and limits the design space available for the wavefront sensor.","To tackle this challenge, we propose an original solution based on nano-engineered meta-optics tailored to produce a spatially varying anamorphic image scale compression.","We present meta-lenslet array designs that can deliver ~100% of the full anamorphic image size reduction required for focal lengths down to 8mm, and greater than 50% image size reduction for focal lengths down to 2mm.","This will allow greatly improved sampling of the available information across the whole wavefront sensor, while still being a viable design within the limits of current-generation fabrication facilities."],"url":"http://arxiv.org/abs/2404.18435v1","category":"astro-ph.IM"}
{"created":"2024-04-29 03:11:13","title":"Dflow, a Python framework for constructing cloud-native AI-for-Science workflows","abstract":"In the AI-for-science era, scientific computing scenarios such as concurrent learning and high-throughput computing demand a new generation of infrastructure that supports scalable computing resources and automated workflow management on both cloud and high-performance supercomputers. Here we introduce Dflow, an open-source Python toolkit designed for scientists to construct workflows with simple programming interfaces. It enables complex process control and task scheduling across a distributed, heterogeneous infrastructure, leveraging containers and Kubernetes for flexibility. Dflow is highly observable and can scale to thousands of concurrent nodes per workflow, enhancing the efficiency of complex scientific computing tasks. The basic unit in Dflow, known as an Operation (OP), is reusable and independent of the underlying infrastructure or context. Dozens of workflow projects have been developed based on Dflow, spanning a wide range of projects. We anticipate that the reusability of Dflow and its components will encourage more scientists to publish their workflows and OP components. These components, in turn, can be adapted and reused in various contexts, fostering greater collaboration and innovation in the scientific community.","sentences":["In the AI-for-science era, scientific computing scenarios such as concurrent learning and high-throughput computing demand a new generation of infrastructure that supports scalable computing resources and automated workflow management on both cloud and high-performance supercomputers.","Here we introduce Dflow, an open-source Python toolkit designed for scientists to construct workflows with simple programming interfaces.","It enables complex process control and task scheduling across a distributed, heterogeneous infrastructure, leveraging containers and Kubernetes for flexibility.","Dflow is highly observable and can scale to thousands of concurrent nodes per workflow, enhancing the efficiency of complex scientific computing tasks.","The basic unit in Dflow, known as an Operation (OP), is reusable and independent of the underlying infrastructure or context.","Dozens of workflow projects have been developed based on Dflow, spanning a wide range of projects.","We anticipate that the reusability of Dflow and its components will encourage more scientists to publish their workflows and OP components.","These components, in turn, can be adapted and reused in various contexts, fostering greater collaboration and innovation in the scientific community."],"url":"http://arxiv.org/abs/2404.18392v1","category":"cs.DC"}
{"created":"2024-04-29 03:04:37","title":"Critical grid method: An extensible Smoothed Particle Hydrodynamics fluid general interpolation method for Fluid-Structure Interaction surface coupling based on preCICE","abstract":"Solving Fluid-Structure Interaction (FSI) problems using traditional methods is a big challenge in the field of numerical simulation. As a powerful multi-physical field coupled library, preCICE has a bright application prospect for solving FSI, which supports many open/closed source software and commercial CFD solvers to solve FSI problems in the form of a black box. However, this library currently only supports mesh-based coupling schemes. This paper proposes a critical grid (mesh) as an intermediate medium for the particle method to connect a bidirectional coupling tool named preCICE. The particle and critical mesh are used to interpolate the displacement and force so that the pure Lagrangian Smoothed Particle Hydrodynamic (SPH) method can also solve the FSI problem. This method is called the particle mesh coupling (PMC) method, which theoretically solves the mesh mismatch problem based on the particle method to connect preCICE. In addition, we conduct experiments to verify the performance of the PMC method, in which the fluid and the structure is discretized by SPH and the Finite Element Method (FEM), respectively. The results show that the PMC method given in this paper is effective for solving FSI problems. Finally, our source code for the SPH fluid adapter is open-source and available on GitHub for further developing preCICE compatibility with more meshless methods.","sentences":["Solving Fluid-Structure Interaction (FSI) problems using traditional methods is a big challenge in the field of numerical simulation.","As a powerful multi-physical field coupled library, preCICE has a bright application prospect for solving FSI, which supports many open/closed source software and commercial CFD solvers to solve FSI problems in the form of a black box.","However, this library currently only supports mesh-based coupling schemes.","This paper proposes a critical grid (mesh) as an intermediate medium for the particle method to connect a bidirectional coupling tool named preCICE.","The particle and critical mesh are used to interpolate the displacement and force so that the pure Lagrangian Smoothed Particle Hydrodynamic (SPH) method can also solve the FSI problem.","This method is called the particle mesh coupling (PMC) method, which theoretically solves the mesh mismatch problem based on the particle method to connect preCICE.","In addition, we conduct experiments to verify the performance of the PMC method, in which the fluid and the structure is discretized by SPH and the Finite Element Method (FEM), respectively.","The results show that the PMC method given in this paper is effective for solving FSI problems.","Finally, our source code for the SPH fluid adapter is open-source and available on GitHub for further developing preCICE compatibility with more meshless methods."],"url":"http://arxiv.org/abs/2404.18390v1","category":"math.NA"}
{"created":"2024-04-28 21:23:40","title":"BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models","abstract":"The growing demand for Large Language Models (LLMs) across diverse applications has prompted a paradigm shift in the design of deep learning serving systems. Deploying LLMs, especially in multi-tenant environments, presents considerable challenges due to their high computational and memory demands. We present BlockLLM, a serving system that exploits the potential of sharing components among fine-tuned LLM models to offer an efficient and flexible solution for LLM workloads. BlockLLM partitions the models into finer-grained blocks to enable the reuse of model components and independent provisioning to improve the computation efficiency. BlockLLM consists of an offline block zoo, for storing the blocks, and an online system to serve the requests through chains of blocks. It offers multi-fold flexibility: (1) Adaptive assembly of block chains on-the-fly is achieved with the help of equivalence evaluation among blocks in the zoo. (2) We enable per-block batch size and configure best-effort KV cache coordination at individual block level. (3) We adopt speculative execution and locality-aware block placement to mitigate the communication costs from dynamic block resource allocation. Our evaluation demonstrates that BlockLLM reduces memory and storage footprints and improves computation efficiency, outperforming existing serving approach in 95\\%ile latency and GPU utilization by 33.5\\% and 20.1\\%, respectively.","sentences":["The growing demand for Large Language Models (LLMs) across diverse applications has prompted a paradigm shift in the design of deep learning serving systems.","Deploying LLMs, especially in multi-tenant environments, presents considerable challenges due to their high computational and memory demands.","We present BlockLLM, a serving system that exploits the potential of sharing components among fine-tuned LLM models to offer an efficient and flexible solution for LLM workloads.","BlockLLM partitions the models into finer-grained blocks to enable the reuse of model components and independent provisioning to improve the computation efficiency.","BlockLLM consists of an offline block zoo, for storing the blocks, and an online system to serve the requests through chains of blocks.","It offers multi-fold flexibility: (1) Adaptive assembly of block chains on-the-fly is achieved with the help of equivalence evaluation among blocks in the zoo.","(2) We enable per-block batch size and configure best-effort KV cache coordination at individual block level.","(3) We adopt speculative execution and locality-aware block placement to mitigate the communication costs from dynamic block resource allocation.","Our evaluation demonstrates that BlockLLM reduces memory and storage footprints and improves computation efficiency, outperforming existing serving approach in 95\\%ile latency and GPU utilization by 33.5\\% and 20.1\\%, respectively."],"url":"http://arxiv.org/abs/2404.18322v1","category":"cs.DC"}
{"created":"2024-04-28 21:04:46","title":"Reconstructing random graphs from distance queries","abstract":"We estimate the minimum number of distance queries that is sufficient to reconstruct the binomial random graph $G(n,p)$ with constant diameter with high probability. We get a tight (up to a constant factor) answer for all $p>n^{-1+o(1)}$ outside \"threshold windows\" around $n^{-k/(k+1)+o(1)}$, $k\\in\\mathbb{Z}_{>0}$: with high probability the query complexity equals $\\Theta(n^{4-d}p^{2-d})$, where $d$ is the diameter of the random graph. This demonstrates the following non-monotone behaviour: the query complexity jumps down at moments when the diameter gets larger; yet, between these moments the query complexity grows. We also show that there exists a non-adaptive algorithm that reconstructs the random graph with $O(n^{4-d}p^{2-d}\\ln n)$ distance queries with high probability, and this is best possible.","sentences":["We estimate the minimum number of distance queries that is sufficient to reconstruct the binomial random graph $G(n,p)$ with constant diameter with high probability.","We get a tight (up to a constant factor) answer for all $p>n^{-1+o(1)}$ outside \"threshold windows\" around $n^{-k/(k+1)+o(1)}$, $k\\in\\mathbb{Z}_{>0}$: with high probability the query complexity equals $\\Theta(n^{4-d}p^{2-d})$, where $d$ is the diameter of the random graph.","This demonstrates the following non-monotone behaviour: the query complexity jumps down at moments when the diameter gets larger; yet, between these moments the query complexity grows.","We also show that there exists a non-adaptive algorithm that reconstructs the random graph with $O(n^{4-d}p^{2-d}\\ln n)$ distance queries with high probability, and this is best possible."],"url":"http://arxiv.org/abs/2404.18318v1","category":"math.CO"}
{"created":"2024-04-28 17:41:07","title":"Semiparametric causal mediation analysis in cluster-randomized experiments","abstract":"In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators. In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments--the natural indirect effect, natural direct effect, and spillover mediation effect. We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case. Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments.","sentences":["In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome.","Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators.","In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments--the natural indirect effect, natural direct effect, and spillover mediation effect.","We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator.","We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case.","Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments."],"url":"http://arxiv.org/abs/2404.18256v1","category":"stat.ME"}
{"created":"2024-04-28 16:58:53","title":"AdaFSNet: Time Series Classification Based on Convolutional Network with a Adaptive and Effective Kernel Size Configuration","abstract":"Time series classification is one of the most critical and challenging problems in data mining, existing widely in various fields and holding significant research importance. Despite extensive research and notable achievements with successful real-world applications, addressing the challenge of capturing the appropriate receptive field (RF) size from one-dimensional or multi-dimensional time series of varying lengths remains a persistent issue, which greatly impacts performance and varies considerably across different datasets. In this paper, we propose an Adaptive and Effective Full-Scope Convolutional Neural Network (AdaFSNet) to enhance the accuracy of time series classification. This network includes two Dense Blocks. Particularly, it can dynamically choose a range of kernel sizes that effectively encompass the optimal RF size for various datasets by incorporating multiple prime numbers corresponding to the time series length. We also design a TargetDrop block, which can reduce redundancy while extracting a more effective RF. To assess the effectiveness of the AdaFSNet network, comprehensive experiments were conducted using the UCR and UEA datasets, which include one-dimensional and multi-dimensional time series data, respectively. Our model surpassed baseline models in terms of classification accuracy, underscoring the AdaFSNet network's efficiency and effectiveness in handling time series classification tasks.","sentences":["Time series classification is one of the most critical and challenging problems in data mining, existing widely in various fields and holding significant research importance.","Despite extensive research and notable achievements with successful real-world applications, addressing the challenge of capturing the appropriate receptive field (RF) size from one-dimensional or multi-dimensional time series of varying lengths remains a persistent issue, which greatly impacts performance and varies considerably across different datasets.","In this paper, we propose an Adaptive and Effective Full-Scope Convolutional Neural Network (AdaFSNet) to enhance the accuracy of time series classification.","This network includes two Dense Blocks.","Particularly, it can dynamically choose a range of kernel sizes that effectively encompass the optimal RF size for various datasets by incorporating multiple prime numbers corresponding to the time series length.","We also design a TargetDrop block, which can reduce redundancy while extracting a more effective RF.","To assess the effectiveness of the AdaFSNet network, comprehensive experiments were conducted using the UCR and UEA datasets, which include one-dimensional and multi-dimensional time series data, respectively.","Our model surpassed baseline models in terms of classification accuracy, underscoring the AdaFSNet network's efficiency and effectiveness in handling time series classification tasks."],"url":"http://arxiv.org/abs/2404.18246v1","category":"cs.LG"}
{"created":"2024-04-28 15:44:57","title":"TextGram: Towards a better domain-adaptive pretraining","abstract":"For green AI, it is crucial to measure and reduce the carbon footprint emitted during the training of large language models. In NLP, performing pre-training on Transformer models requires significant computational resources. This pre-training involves using a large amount of text data to gain prior knowledge for performing downstream tasks. Thus, it is important that we select the correct data in the form of domain-specific data from this vast corpus to achieve optimum results aligned with our domain-specific tasks. While training on large unsupervised data is expensive, it can be optimized by performing a data selection step before pretraining. Selecting important data reduces the space overhead and the substantial amount of time required to pre-train the model while maintaining constant accuracy. We investigate the existing selection strategies and propose our own domain-adaptive data selection method - TextGram - that effectively selects essential data from large corpora. We compare and evaluate the results of finetuned models for text classification task with and without data selection. We show that the proposed strategy works better compared to other selection methods.","sentences":["For green AI, it is crucial to measure and reduce the carbon footprint emitted during the training of large language models.","In NLP, performing pre-training on Transformer models requires significant computational resources.","This pre-training involves using a large amount of text data to gain prior knowledge for performing downstream tasks.","Thus, it is important that we select the correct data in the form of domain-specific data from this vast corpus to achieve optimum results aligned with our domain-specific tasks.","While training on large unsupervised data is expensive, it can be optimized by performing a data selection step before pretraining.","Selecting important data reduces the space overhead and the substantial amount of time required to pre-train the model while maintaining constant accuracy.","We investigate the existing selection strategies and propose our own domain-adaptive data selection method - TextGram - that effectively selects essential data from large corpora.","We compare and evaluate the results of finetuned models for text classification task with and without data selection.","We show that the proposed strategy works better compared to other selection methods."],"url":"http://arxiv.org/abs/2404.18228v1","category":"cs.CL"}
{"created":"2024-04-28 15:07:48","title":"A survey of dynamic graph neural networks","abstract":"Graph neural networks (GNNs) have emerged as a powerful tool for effectively mining and learning from graph-structured data, with applications spanning numerous domains. However, most research focuses on static graphs, neglecting the dynamic nature of real-world networks where topologies and attributes evolve over time. By integrating sequence modeling modules into traditional GNN architectures, dynamic GNNs aim to bridge this gap, capturing the inherent temporal dependencies of dynamic graphs for a more authentic depiction of complex networks. This paper provides a comprehensive review of the fundamental concepts, key techniques, and state-of-the-art dynamic GNN models. We present the mainstream dynamic GNN models in detail and categorize models based on how temporal information is incorporated. We also discuss large-scale dynamic GNNs and pre-training techniques. Although dynamic GNNs have shown superior performance, challenges remain in scalability, handling heterogeneous information, and lack of diverse graph datasets. The paper also discusses possible future directions, such as adaptive and memory-enhanced models, inductive learning, and theoretical analysis.","sentences":["Graph neural networks (GNNs) have emerged as a powerful tool for effectively mining and learning from graph-structured data, with applications spanning numerous domains.","However, most research focuses on static graphs, neglecting the dynamic nature of real-world networks where topologies and attributes evolve over time.","By integrating sequence modeling modules into traditional GNN architectures, dynamic GNNs aim to bridge this gap, capturing the inherent temporal dependencies of dynamic graphs for a more authentic depiction of complex networks.","This paper provides a comprehensive review of the fundamental concepts, key techniques, and state-of-the-art dynamic GNN models.","We present the mainstream dynamic GNN models in detail and categorize models based on how temporal information is incorporated.","We also discuss large-scale dynamic GNNs and pre-training techniques.","Although dynamic GNNs have shown superior performance, challenges remain in scalability, handling heterogeneous information, and lack of diverse graph datasets.","The paper also discusses possible future directions, such as adaptive and memory-enhanced models, inductive learning, and theoretical analysis."],"url":"http://arxiv.org/abs/2404.18211v1","category":"cs.LG"}
{"created":"2024-04-28 14:37:10","title":"Rethinking Attention Gated with Hybrid Dual Pyramid Transformer-CNN for Generalized Segmentation in Medical Imaging","abstract":"Inspired by the success of Transformers in Computer vision, Transformers have been widely investigated for medical imaging segmentation. However, most of Transformer architecture are using the recent transformer architectures as encoder or as parallel encoder with the CNN encoder. In this paper, we introduce a novel hybrid CNN-Transformer segmentation architecture (PAG-TransYnet) designed for efficiently building a strong CNN-Transformer encoder. Our approach exploits attention gates within a Dual Pyramid hybrid encoder. The contributions of this methodology can be summarized into three key aspects: (i) the utilization of Pyramid input for highlighting the prominent features at different scales, (ii) the incorporation of a PVT transformer to capture long-range dependencies across various resolutions, and (iii) the implementation of a Dual-Attention Gate mechanism for effectively fusing prominent features from both CNN and Transformer branches. Through comprehensive evaluation across different segmentation tasks including: abdominal multi-organs segmentation, infection segmentation (Covid-19 and Bone Metastasis), microscopic tissues segmentation (Gland and Nucleus). The proposed approach demonstrates state-of-the-art performance and exhibits remarkable generalization capabilities. This research represents a significant advancement towards addressing the pressing need for efficient and adaptable segmentation solutions in medical imaging applications.","sentences":["Inspired by the success of Transformers in Computer vision, Transformers have been widely investigated for medical imaging segmentation.","However, most of Transformer architecture are using the recent transformer architectures as encoder or as parallel encoder with the CNN encoder.","In this paper, we introduce a novel hybrid CNN-Transformer segmentation architecture (PAG-TransYnet) designed for efficiently building a strong CNN-Transformer encoder.","Our approach exploits attention gates within a Dual Pyramid hybrid encoder.","The contributions of this methodology can be summarized into three key aspects: (i) the utilization of Pyramid input for highlighting the prominent features at different scales, (ii) the incorporation of a PVT transformer to capture long-range dependencies across various resolutions, and (iii) the implementation of a Dual-Attention Gate mechanism for effectively fusing prominent features from both CNN and Transformer branches.","Through comprehensive evaluation across different segmentation tasks including: abdominal multi-organs segmentation, infection segmentation (Covid-19 and Bone Metastasis), microscopic tissues segmentation (Gland and Nucleus).","The proposed approach demonstrates state-of-the-art performance and exhibits remarkable generalization capabilities.","This research represents a significant advancement towards addressing the pressing need for efficient and adaptable segmentation solutions in medical imaging applications."],"url":"http://arxiv.org/abs/2404.18199v1","category":"eess.IV"}
{"created":"2024-04-28 12:13:34","title":"Event-based Video Frame Interpolation with Edge Guided Motion Refinement","abstract":"Video frame interpolation, the process of synthesizing intermediate frames between sequential video frames, has made remarkable progress with the use of event cameras. These sensors, with microsecond-level temporal resolution, fill information gaps between frames by providing precise motion cues. However, contemporary Event-Based Video Frame Interpolation (E-VFI) techniques often neglect the fact that event data primarily supply high-confidence features at scene edges during multi-modal feature fusion, thereby diminishing the role of event signals in optical flow (OF) estimation and warping refinement. To address this overlooked aspect, we introduce an end-to-end E-VFI learning method (referred to as EGMR) to efficiently utilize edge features from event signals for motion flow and warping enhancement. Our method incorporates an Edge Guided Attentive (EGA) module, which rectifies estimated video motion through attentive aggregation based on the local correlation of multi-modal features in a coarse-to-fine strategy. Moreover, given that event data can provide accurate visual references at scene edges between consecutive frames, we introduce a learned visibility map derived from event data to adaptively mitigate the occlusion problem in the warping refinement process. Extensive experiments on both synthetic and real datasets show the effectiveness of the proposed approach, demonstrating its potential for higher quality video frame interpolation.","sentences":["Video frame interpolation, the process of synthesizing intermediate frames between sequential video frames, has made remarkable progress with the use of event cameras.","These sensors, with microsecond-level temporal resolution, fill information gaps between frames by providing precise motion cues.","However, contemporary Event-Based Video Frame Interpolation (E-VFI) techniques often neglect the fact that event data primarily supply high-confidence features at scene edges during multi-modal feature fusion, thereby diminishing the role of event signals in optical flow (OF) estimation and warping refinement.","To address this overlooked aspect, we introduce an end-to-end E-VFI learning method (referred to as EGMR) to efficiently utilize edge features from event signals for motion flow and warping enhancement.","Our method incorporates an Edge Guided Attentive (EGA) module, which rectifies estimated video motion through attentive aggregation based on the local correlation of multi-modal features in a coarse-to-fine strategy.","Moreover, given that event data can provide accurate visual references at scene edges between consecutive frames, we introduce a learned visibility map derived from event data to adaptively mitigate the occlusion problem in the warping refinement process.","Extensive experiments on both synthetic and real datasets show the effectiveness of the proposed approach, demonstrating its potential for higher quality video frame interpolation."],"url":"http://arxiv.org/abs/2404.18156v1","category":"cs.CV"}
{"created":"2024-04-28 10:16:35","title":"SafePaint: Anti-forensic Image Inpainting with Domain Adaptation","abstract":"Existing image inpainting methods have achieved remarkable accomplishments in generating visually appealing results, often accompanied by a trend toward creating more intricate structural textures. However, while these models excel at creating more realistic image content, they often leave noticeable traces of tampering, posing a significant threat to security. In this work, we take the anti-forensic capabilities into consideration, firstly proposing an end-to-end training framework for anti-forensic image inpainting named SafePaint. Specifically, we innovatively formulated image inpainting as two major tasks: semantically plausible content completion and region-wise optimization. The former is similar to current inpainting methods that aim to restore the missing regions of corrupted images. The latter, through domain adaptation, endeavors to reconcile the discrepancies between the inpainted region and the unaltered area to achieve anti-forensic goals. Through comprehensive theoretical analysis, we validate the effectiveness of domain adaptation for anti-forensic performance. Furthermore, we meticulously crafted a region-wise separated attention (RWSA) module, which not only aligns with our objective of anti-forensics but also enhances the performance of the model. Extensive qualitative and quantitative evaluations show our approach achieves comparable results to existing image inpainting methods while offering anti-forensic capabilities not available in other methods.","sentences":["Existing image inpainting methods have achieved remarkable accomplishments in generating visually appealing results, often accompanied by a trend toward creating more intricate structural textures.","However, while these models excel at creating more realistic image content, they often leave noticeable traces of tampering, posing a significant threat to security.","In this work, we take the anti-forensic capabilities into consideration, firstly proposing an end-to-end training framework for anti-forensic image inpainting named SafePaint.","Specifically, we innovatively formulated image inpainting as two major tasks: semantically plausible content completion and region-wise optimization.","The former is similar to current inpainting methods that aim to restore the missing regions of corrupted images.","The latter, through domain adaptation, endeavors to reconcile the discrepancies between the inpainted region and the unaltered area to achieve anti-forensic goals.","Through comprehensive theoretical analysis, we validate the effectiveness of domain adaptation for anti-forensic performance.","Furthermore, we meticulously crafted a region-wise separated attention (RWSA) module, which not only aligns with our objective of anti-forensics but also enhances the performance of the model.","Extensive qualitative and quantitative evaluations show our approach achieves comparable results to existing image inpainting methods while offering anti-forensic capabilities not available in other methods."],"url":"http://arxiv.org/abs/2404.18136v1","category":"cs.CV"}
{"created":"2024-04-28 10:11:36","title":"Dexterous Grasp Transformer","abstract":"In this work, we propose a novel discriminative framework for dexterous grasp generation, named Dexterous Grasp TRansformer (DGTR), capable of predicting a diverse set of feasible grasp poses by processing the object point cloud with only one forward pass. We formulate dexterous grasp generation as a set prediction task and design a transformer-based grasping model for it. However, we identify that this set prediction paradigm encounters several optimization challenges in the field of dexterous grasping and results in restricted performance. To address these issues, we propose progressive strategies for both the training and testing phases. First, the dynamic-static matching training (DSMT) strategy is presented to enhance the optimization stability during the training phase. Second, we introduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasping quality during the testing phase. Experimental results on the DexGraspNet dataset demonstrate the capability of DGTR to predict dexterous grasp poses with both high quality and diversity. Notably, while keeping high quality, the diversity of grasp poses predicted by DGTR significantly outperforms previous works in multiple metrics without any data pre-processing. Codes are available at https://github.com/iSEE-Laboratory/DGTR .","sentences":["In this work, we propose a novel discriminative framework for dexterous grasp generation, named Dexterous Grasp TRansformer (DGTR), capable of predicting a diverse set of feasible grasp poses by processing the object point cloud with only one forward pass.","We formulate dexterous grasp generation as a set prediction task and design a transformer-based grasping model for it.","However, we identify that this set prediction paradigm encounters several optimization challenges in the field of dexterous grasping and results in restricted performance.","To address these issues, we propose progressive strategies for both the training and testing phases.","First, the dynamic-static matching training (DSMT) strategy is presented to enhance the optimization stability during the training phase.","Second, we introduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasping quality during the testing phase.","Experimental results on the DexGraspNet dataset demonstrate the capability of DGTR to predict dexterous grasp poses with both high quality and diversity.","Notably, while keeping high quality, the diversity of grasp poses predicted by DGTR significantly outperforms previous works in multiple metrics without any data pre-processing.","Codes are available at https://github.com/iSEE-Laboratory/DGTR ."],"url":"http://arxiv.org/abs/2404.18135v1","category":"cs.RO"}
{"created":"2024-04-28 07:32:00","title":"Advancing Supervised Learning with the Wave Loss Function: A Robust and Smooth Approach","abstract":"Loss function plays a vital role in supervised learning frameworks. The selection of the appropriate loss function holds the potential to have a substantial impact on the proficiency attained by the acquired model. The training of supervised learning algorithms inherently adheres to predetermined loss functions during the optimization process. In this paper, we present a novel contribution to the realm of supervised machine learning: an asymmetric loss function named wave loss. It exhibits robustness against outliers, insensitivity to noise, boundedness, and a crucial smoothness property. Theoretically, we establish that the proposed wave loss function manifests the essential characteristic of being classification-calibrated. Leveraging this breakthrough, we incorporate the proposed wave loss function into the least squares setting of support vector machines (SVM) and twin support vector machines (TSVM), resulting in two robust and smooth models termed Wave-SVM and Wave-TSVM, respectively. To address the optimization problem inherent in Wave-SVM, we utilize the adaptive moment estimation (Adam) algorithm. It is noteworthy that this paper marks the first instance of the Adam algorithm application to solve an SVM model. Further, we devise an iterative algorithm to solve the optimization problems of Wave-TSVM. To empirically showcase the effectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on benchmark UCI and KEEL datasets (with and without feature noise) from diverse domains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical domain, we evaluate it on the Alzheimer Disease Neuroimaging Initiative (ADNI) dataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM and Wave-TSVM in achieving superior prediction accuracy against the baseline models.","sentences":["Loss function plays a vital role in supervised learning frameworks.","The selection of the appropriate loss function holds the potential to have a substantial impact on the proficiency attained by the acquired model.","The training of supervised learning algorithms inherently adheres to predetermined loss functions during the optimization process.","In this paper, we present a novel contribution to the realm of supervised machine learning: an asymmetric loss function named wave loss.","It exhibits robustness against outliers, insensitivity to noise, boundedness, and a crucial smoothness property.","Theoretically, we establish that the proposed wave loss function manifests the essential characteristic of being classification-calibrated.","Leveraging this breakthrough, we incorporate the proposed wave loss function into the least squares setting of support vector machines (SVM) and twin support vector machines (TSVM), resulting in two robust and smooth models termed Wave-SVM and Wave-TSVM, respectively.","To address the optimization problem inherent in Wave-SVM, we utilize the adaptive moment estimation (Adam) algorithm.","It is noteworthy that this paper marks the first instance of the Adam algorithm application to solve an SVM model.","Further, we devise an iterative algorithm to solve the optimization problems of Wave-TSVM.","To empirically showcase the effectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on benchmark UCI and KEEL datasets (with and without feature noise) from diverse domains.","Moreover, to exemplify the applicability of Wave-SVM in the biomedical domain, we evaluate it on the Alzheimer Disease Neuroimaging Initiative (ADNI) dataset.","The experimental outcomes unequivocally reveal the prowess of Wave-SVM and Wave-TSVM in achieving superior prediction accuracy against the baseline models."],"url":"http://arxiv.org/abs/2404.18101v1","category":"cs.LG"}
{"created":"2024-04-28 07:08:44","title":"Parameterized Dynamic Logic -- Towards A Cyclic Logical Framework for Program Verification via Operational Semantics","abstract":"Dynamic logic and its variations, because of their good expressive forms capturing program specifications clearly by isolating programs from logical formulas, have been used as a formalism in program reasoning for decades and have many applications in different areas. The program models of traditional dynamic logics are in explicit forms. With a clearly-defined syntactic structure, compositional verification is made possible, in which a deduction step transfers proving a program into proving its sub-programs. This structure-based reasoning forms the basis of many dynamic logics and popular Hoare-style logics. However, structural rules induce a major drawback that for different target programs, different rules have to be proposed to adapt different program structures. Moreover, there exist programs that does not support (or not entirely support) a structure-based reasoning. In this paper, we propose a parameterized `dynamic-logic-like' logic called DLp with general forms of program models and formulas, and propose a cyclic proof system for this logic. Program reasoning in DLp is directly based on symbolic executions of programs according to their operational semantics. This reduces the burden of designing a large set of rules when specializing a logic theory to a specific domain, and facilitates verifying programs without a suitable structure for direct reasoning. Without reasoning by dissolving program structures, DLp can cause an infinite proof structure. To solve this, we build a cyclic preproof structure for the proof system of DLp and prove its soundness. Case studies are analyzed to show how DLp works for reasoning about different types of programs.","sentences":["Dynamic logic and its variations, because of their good expressive forms capturing program specifications clearly by isolating programs from logical formulas, have been used as a formalism in program reasoning for decades and have many applications in different areas.","The program models of traditional dynamic logics are in explicit forms.","With a clearly-defined syntactic structure, compositional verification is made possible, in which a deduction step transfers proving a program into proving its sub-programs.","This structure-based reasoning forms the basis of many dynamic logics and popular Hoare-style logics.","However, structural rules induce a major drawback that for different target programs, different rules have to be proposed to adapt different program structures.","Moreover, there exist programs that does not support (or not entirely support) a structure-based reasoning.","In this paper, we propose a parameterized `dynamic-logic-like' logic called DLp with general forms of program models and formulas, and propose a cyclic proof system for this logic.","Program reasoning in DLp is directly based on symbolic executions of programs according to their operational semantics.","This reduces the burden of designing a large set of rules when specializing a logic theory to a specific domain, and facilitates verifying programs without a suitable structure for direct reasoning.","Without reasoning by dissolving program structures, DLp can cause an infinite proof structure.","To solve this, we build a cyclic preproof structure for the proof system of DLp and prove its soundness.","Case studies are analyzed to show how DLp works for reasoning about different types of programs."],"url":"http://arxiv.org/abs/2404.18098v1","category":"cs.LO"}
{"created":"2024-04-28 07:01:55","title":"Snake with Shifted Window: Learning to Adapt Vessel Pattern for OCTA Segmentation","abstract":"Segmenting specific targets or structures in optical coherence tomography angiography (OCTA) images is fundamental for conducting further pathological studies. The retinal vascular layers are rich and intricate, and such vascular with complex shapes can be captured by the widely-studied OCTA images. In this paper, we thus study how to use OCTA images with projection vascular layers to segment retinal structures. To this end, we propose the SSW-OCTA model, which integrates the advantages of deformable convolutions suited for tubular structures and the swin-transformer for global feature extraction, adapting to the characteristics of OCTA modality images. Our model underwent testing and comparison on the OCTA-500 dataset, achieving state-of-the-art performance. The code is available at: https://github.com/ShellRedia/Snake-SWin-OCTA.","sentences":["Segmenting specific targets or structures in optical coherence tomography angiography (OCTA) images is fundamental for conducting further pathological studies.","The retinal vascular layers are rich and intricate, and such vascular with complex shapes can be captured by the widely-studied OCTA images.","In this paper, we thus study how to use OCTA images with projection vascular layers to segment retinal structures.","To this end, we propose the SSW-OCTA model, which integrates the advantages of deformable convolutions suited for tubular structures and the swin-transformer for global feature extraction, adapting to the characteristics of OCTA modality images.","Our model underwent testing and comparison on the OCTA-500 dataset, achieving state-of-the-art performance.","The code is available at: https://github.com/ShellRedia/Snake-SWin-OCTA."],"url":"http://arxiv.org/abs/2404.18096v1","category":"eess.IV"}
{"created":"2024-04-28 06:16:44","title":"An open-source solver for finding global solutions to constrained derivative-free optimization problems","abstract":"In this work, we propose a heuristic based open source solver for finding global solution to constrained derivative-free optimization (DFO) problems. Our solver named Global optimization using Surrogates for Derivative-free Optimization (GSDO) relies on surrogate approximation to the original problem. In the proposed algorithm, an initial feasible point is first generated. This point is subsequently used to generate well spaced feasible points for formulating better radial basis function based surrogate approximations to original objective and constraint functions. Finally, these surrogates are used to solve the derivative-free global optimization problems. The proposed solver is capable of handling quantifiable and nonquantifiable as well as relaxable and unrelaxable constraints. We compared the performance of proposed solver with state of the art solvers like Nonlinear Optimization by Mesh Adaptive Direct Search (NOMAD), differential evolution (DE) and Simplicial Homology Global Optimization (SHGO) on standard test problems. The numerical results clearly demonstrate that the performance of our method is competitive with respect to other solvers.","sentences":["In this work, we propose a heuristic based open source solver for finding global solution to constrained derivative-free optimization (DFO) problems.","Our solver named Global optimization using Surrogates for Derivative-free Optimization (GSDO) relies on surrogate approximation to the original problem.","In the proposed algorithm, an initial feasible point is first generated.","This point is subsequently used to generate well spaced feasible points for formulating better radial basis function based surrogate approximations to original objective and constraint functions.","Finally, these surrogates are used to solve the derivative-free global optimization problems.","The proposed solver is capable of handling quantifiable and nonquantifiable as well as relaxable and unrelaxable constraints.","We compared the performance of proposed solver with state of the art solvers like Nonlinear Optimization by Mesh Adaptive Direct Search (NOMAD), differential evolution (DE) and Simplicial Homology Global Optimization (SHGO) on standard test problems.","The numerical results clearly demonstrate that the performance of our method is competitive with respect to other solvers."],"url":"http://arxiv.org/abs/2404.18080v1","category":"math.OC"}
{"created":"2024-04-28 03:28:27","title":"Prompt Customization for Continual Learning","abstract":"Contemporary continual learning approaches typically select prompts from a pool, which function as supplementary inputs to a pre-trained model. However, this strategy is hindered by the inherent noise of its selection approach when handling increasing tasks. In response to these challenges, we reformulate the prompting approach for continual learning and propose the prompt customization (PC) method. PC mainly comprises a prompt generation module (PGM) and a prompt modulation module (PMM). In contrast to conventional methods that employ hard prompt selection, PGM assigns different coefficients to prompts from a fixed-sized pool of prompts and generates tailored prompts. Moreover, PMM further modulates the prompts by adaptively assigning weights according to the correlations between input data and corresponding prompts. We evaluate our method on four benchmark datasets for three diverse settings, including the class, domain, and task-agnostic incremental learning tasks. Experimental results demonstrate consistent improvement (by up to 16.2\\%), yielded by the proposed method, over the state-of-the-art (SOTA) techniques.","sentences":["Contemporary continual learning approaches typically select prompts from a pool, which function as supplementary inputs to a pre-trained model.","However, this strategy is hindered by the inherent noise of its selection approach when handling increasing tasks.","In response to these challenges, we reformulate the prompting approach for continual learning and propose the prompt customization (PC) method.","PC mainly comprises a prompt generation module (PGM) and a prompt modulation module (PMM).","In contrast to conventional methods that employ hard prompt selection, PGM assigns different coefficients to prompts from a fixed-sized pool of prompts and generates tailored prompts.","Moreover, PMM further modulates the prompts by adaptively assigning weights according to the correlations between input data and corresponding prompts.","We evaluate our method on four benchmark datasets for three diverse settings, including the class, domain, and task-agnostic incremental learning tasks.","Experimental results demonstrate consistent improvement (by up to 16.2\\%), yielded by the proposed method, over the state-of-the-art (SOTA) techniques."],"url":"http://arxiv.org/abs/2404.18060v1","category":"cs.CV"}
{"created":"2024-04-28 02:59:58","title":"Mass-preserving Spatio-temporal adaptive PINN for Cahn-Hilliard equations with strong nonlinearity and singularity","abstract":"As one kind important phase field equations, Cahn-Hilliard equations contain spatial high order derivatives, strong nonlinearities, and even singularities. When using the physics informed neural network (PINN) to simulate the long time evolution, it is necessary to decompose the time domain to capture the transition of solutions in different time. Moreover, the baseline PINN can't maintain the mass conservation property for the equations. We propose a mass-preserving spatio-temporal adaptive PINN. This method adaptively dividing the time domain according to the rate of energy decrease, and solves the Cahn-Hilliard equation in each time step using an independent neural network. To improve the prediction accuracy, spatial adaptive sampling is employed in the subdomain to select points with large residual value and add them to the training samples. Additionally, a mass constraint is added to the loss function to compensate the mass degradation problem of the PINN method in solving the Cahn-Hilliard equations. The mass-preserving spatio-temporal adaptive PINN is employed to solve a series of numerical examples. These include the Cahn-Hilliard equations with different bulk potentials, the three dimensional Cahn-Hilliard equation with singularities, and the set of Cahn-Hilliard equations. The numerical results demonstrate the effectiveness of the proposed algorithm.","sentences":["As one kind important phase field equations, Cahn-Hilliard equations contain spatial high order derivatives, strong nonlinearities, and even singularities.","When using the physics informed neural network (PINN) to simulate the long time evolution, it is necessary to decompose the time domain to capture the transition of solutions in different time.","Moreover, the baseline PINN can't maintain the mass conservation property for the equations.","We propose a mass-preserving spatio-temporal adaptive PINN.","This method adaptively dividing the time domain according to the rate of energy decrease, and solves the Cahn-Hilliard equation in each time step using an independent neural network.","To improve the prediction accuracy, spatial adaptive sampling is employed in the subdomain to select points with large residual value and add them to the training samples.","Additionally, a mass constraint is added to the loss function to compensate the mass degradation problem of the PINN method in solving the Cahn-Hilliard equations.","The mass-preserving spatio-temporal adaptive PINN is employed to solve a series of numerical examples.","These include the Cahn-Hilliard equations with different bulk potentials, the three dimensional Cahn-Hilliard equation with singularities, and the set of Cahn-Hilliard equations.","The numerical results demonstrate the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2404.18054v1","category":"math.NA"}
{"created":"2024-04-28 01:10:09","title":"Pose-aware 3D Beamwidth Adaptation for Mobile Extended Reality","abstract":"This paper presents a sensor-aided pose-aware beamwidth adaptation design for a conceptual extended reality (XR) Head-Mounted Display (HMD) equipped with a 2D planar array. The beam is tracked and adapted on the user side by leveraging HMD orientation estimates. The beamwidth adaptation scheme is effected by selective deactivation of elements in the 2D antenna array, employing the angular estimation covariance matrix to overlap the beam with the estimation confidence interval. The proposed method utilizes the estimation correlations to adapt the beamwidth along the confidence interval of these estimates. Compared to a beamwidth adaptation without leveraging estimation correlations, the proposed method demonstrates the gain of leveraging estimation correlations by improving the coverage area for a given outage probability threshold by approximately 16%, or equivalently increasing the power efficiency up to 18%.","sentences":["This paper presents a sensor-aided pose-aware beamwidth adaptation design for a conceptual extended reality (XR) Head-Mounted Display (HMD) equipped with a 2D planar array.","The beam is tracked and adapted on the user side by leveraging HMD orientation estimates.","The beamwidth adaptation scheme is effected by selective deactivation of elements in the 2D antenna array, employing the angular estimation covariance matrix to overlap the beam with the estimation confidence interval.","The proposed method utilizes the estimation correlations to adapt the beamwidth along the confidence interval of these estimates.","Compared to a beamwidth adaptation without leveraging estimation correlations, the proposed method demonstrates the gain of leveraging estimation correlations by improving the coverage area for a given outage probability threshold by approximately 16%, or equivalently increasing the power efficiency up to 18%."],"url":"http://arxiv.org/abs/2404.18042v1","category":"cs.IT"}
{"created":"2024-04-29 17:58:19","title":"Analytically weak solutions to stochastic heat equations with spatially rough noise","abstract":"In [HHL+17] the authors showed existence and uniqueness of solutions to the nonlinear one-dimensional stochastic heat equation driven by a Gaussian noise that is white in time and rougher than white in space (in particular, its covariance is not a measure). Here we present a simple alternative to derive such results by considering the equations in the analytically weak sense, using either the variational approach or Krylov's $L^p$-theory. Various improvements are obtained as corollaries.","sentences":["In [HHL+17] the authors showed existence and uniqueness of solutions to the nonlinear one-dimensional stochastic heat equation driven by a Gaussian noise that is white in time and rougher than white in space (in particular, its covariance is not a measure).","Here we present a simple alternative to derive such results by considering the equations in the analytically weak sense, using either the variational approach or Krylov's $L^p$-theory.","Various improvements are obtained as corollaries."],"url":"http://arxiv.org/abs/2404.18920v1","category":"math.PR"}
{"created":"2024-04-29 17:57:11","title":"Strong solutions to McKean-Vlasov SDEs associated to a class of degenerate Fokker-Planck equations with coefficients of Nemytskii-type","abstract":"While the nondegenerate case is well-known, there are only few results on the existence of strong solutions to McKean-Vlasov SDEs with coefficients of Nemytskii-type in the degenerate case. We consider a broad class of degenerate nonlinear Fokker-Planck(-Kolmogorov) equations with coefficients of Nemytskii-type. This includes, in particular, the classical porous medium equation perturbed by a first-order term with initial datum in a subset of probability densities, which is dense with respect to the topology inherited from $L^1$, and, in the one-dimensional setting, the classical porous medium equation with initial datum in an arbitrary point $x\\in\\mathbb{R}$. For these kind of equations the existence of a Schwartz-distributional solution $u$ is well-known. We show that there exists a unique strong solution to the associated degenerate McKean-Vlasov SDE with time marginal law densities $u$. In particular, every weak solution to this equation with time marginal law densities $u$ can be written as a functional of the driving Brownian motion.   Moreover, plugging any Brownian motion into this very functional yields a weak solution with time marginal law densities $u$.","sentences":["While the nondegenerate case is well-known, there are only few results on the existence of strong solutions to McKean-Vlasov SDEs with coefficients of Nemytskii-type in the degenerate case.","We consider a broad class of degenerate nonlinear Fokker-Planck(-Kolmogorov) equations with coefficients of Nemytskii-type.","This includes, in particular, the classical porous medium equation perturbed by a first-order term with initial datum in a subset of probability densities, which is dense with respect to the topology inherited from $L^1$, and, in the one-dimensional setting, the classical porous medium equation with initial datum in an arbitrary point $x\\in\\mathbb{R}$.","For these kind of equations the existence of a Schwartz-distributional solution $u$ is well-known.","We show that there exists a unique strong solution to the associated degenerate McKean-Vlasov SDE with time marginal law densities $u$. In particular, every weak solution to this equation with time marginal law densities $u$ can be written as a functional of the driving Brownian motion.   ","Moreover, plugging any Brownian motion into this very functional yields a weak solution with time marginal law densities $u$."],"url":"http://arxiv.org/abs/2404.18918v1","category":"math.PR"}
{"created":"2024-04-29 17:55:27","title":"On the behavior of pressure in a low Mach number flow","abstract":"In our recent works, we proposed a theory of turbulence via the mean field effect of an intermolecular potential, which in part relies on an empirically observed \"equilibrated\" behavior of the pressure variable in a low Mach number flow -- that is, while other variables may exhibit considerable variations at low Mach numbers, the pressure is (nearly) constant. At the same time, conventional kinetic theory does not offer a satisfactory explanation for such a behavior of the pressure variable, instead leading to an isentropic flow in the form of the usual compressible Euler equations.   In the current work, we introduce a novel correction term into the pair correlation function of the BBGKY closure for the collision integral, which adjusts the density of incident or recedent pairs of particles depending on the macroscopic compression or expansion rate of the gas. Remarkably, this term does not affect the density and momentum transport equations, and manifests solely in the pressure transport equation. We also find that the effect of the novel term on the pressure dynamics matches the observed behavior -- that is, it tends to attenuate the acoustic waves and smooth out the pressure solution. It appears that the missing piece of the low Mach number puzzle has finally been identified.","sentences":["In our recent works, we proposed a theory of turbulence via the mean field effect of an intermolecular potential, which in part relies on an empirically observed \"equilibrated\" behavior of the pressure variable in a low Mach number flow -- that is, while other variables may exhibit considerable variations at low Mach numbers, the pressure is (nearly) constant.","At the same time, conventional kinetic theory does not offer a satisfactory explanation for such a behavior of the pressure variable, instead leading to an isentropic flow in the form of the usual compressible Euler equations.   ","In the current work, we introduce a novel correction term into the pair correlation function of the BBGKY closure for the collision integral, which adjusts the density of incident or recedent pairs of particles depending on the macroscopic compression or expansion rate of the gas.","Remarkably, this term does not affect the density and momentum transport equations, and manifests solely in the pressure transport equation.","We also find that the effect of the novel term on the pressure dynamics matches the observed behavior -- that is, it tends to attenuate the acoustic waves and smooth out the pressure solution.","It appears that the missing piece of the low Mach number puzzle has finally been identified."],"url":"http://arxiv.org/abs/2404.18914v1","category":"physics.flu-dyn"}
{"created":"2024-04-29 17:31:00","title":"RSCaMa: Remote Sensing Image Change Captioning with State Space Model","abstract":"Remote Sensing Image Change Captioning (RSICC) aims to identify surface changes in multi-temporal remote sensing images and describe them in natural language. Current methods typically rely on an encoder-decoder architecture and focus on designing a sophisticated neck to process bi-temporal features extracted by the backbone. Recently, State Space Models (SSMs), especially Mamba, have demonstrated outstanding performance in many fields, owing to their efficient feature-selective modelling capability. However, their potential in the RSICC task remains unexplored. In this paper, we introduce Mamba into RSICC and propose a novel approach called RSCaMa (Remote Sensing Change Captioning Mamba). Specifically, we utilize Siamese backbones to extract bi-temporal features, which are then processed through multiple CaMa layers consisting of Spatial Difference-guided SSM (SD-SSM) and Temporal Traveling SSM (TT-SSM). SD-SSM uses differential features to enhance change perception, while TT-SSM promotes bitemporal interactions in a token-wise cross-scanning manner. Experimental results validate the effectiveness of CaMa layers and demonstrate the superior performance of RSCaMa, as well as the potential of Mamba in the RSICC task. Additionally, we systematically compare the effects of three language decoders, including Mamba, GPT-style decoder with causal attention mechanism, and Transformer decoder with cross-attention mechanism. This provides valuable insights for future RSICC research. The code will be available at https://github.com/Chen-Yang-Liu/RSCaMa","sentences":["Remote Sensing Image Change Captioning (RSICC) aims to identify surface changes in multi-temporal remote sensing images and describe them in natural language.","Current methods typically rely on an encoder-decoder architecture and focus on designing a sophisticated neck to process bi-temporal features extracted by the backbone.","Recently, State Space Models (SSMs), especially Mamba, have demonstrated outstanding performance in many fields, owing to their efficient feature-selective modelling capability.","However, their potential in the RSICC task remains unexplored.","In this paper, we introduce Mamba into RSICC and propose a novel approach called RSCaMa","(Remote Sensing Change Captioning Mamba).","Specifically, we utilize Siamese backbones to extract bi-temporal features, which are then processed through multiple CaMa layers consisting of Spatial Difference-guided SSM (SD-SSM) and Temporal Traveling SSM (TT-SSM).","SD-SSM uses differential features to enhance change perception, while TT-SSM promotes bitemporal interactions in a token-wise cross-scanning manner.","Experimental results validate the effectiveness of CaMa layers and demonstrate the superior performance of RSCaMa, as well as the potential of Mamba in the RSICC task.","Additionally, we systematically compare the effects of three language decoders, including Mamba, GPT-style decoder with causal attention mechanism, and Transformer decoder with cross-attention mechanism.","This provides valuable insights for future RSICC research.","The code will be available at https://github.com/Chen-Yang-Liu/RSCaMa"],"url":"http://arxiv.org/abs/2404.18895v1","category":"cs.CV"}
{"created":"2024-04-29 17:19:07","title":"A Thom Isotopy Theorem for nonproper semialgebraic maps","abstract":"We prove a version of the Thom Isotopy Theorem for nonproper semialgebraic maps $f\\colon X\\rightarrow \\mathbb{R}^m$, where $X \\subset\\mathbb{R}^n$ is a semialgebraic set and $f$ is the restriction to $X$ of a smooth semialgebraic map $F:\\mathbb{R}^n\\to \\mathbb{R}^m$.","sentences":["We prove a version of the Thom Isotopy Theorem for nonproper semialgebraic maps $f\\colon X\\rightarrow \\mathbb{R}^m$, where $X \\subset\\mathbb{R}^n$ is a semialgebraic set and $f$ is the restriction to $X$ of a smooth semialgebraic map $F:\\mathbb{R}^n\\to \\mathbb{R}^m$."],"url":"http://arxiv.org/abs/2404.18883v1","category":"math.DG"}
{"created":"2024-04-29 17:14:08","title":"High-Energy Reaction Dynamics of N$_{3}$","abstract":"The atom-exchange and atomization dissociation dynamics for the N($^4$S) + N$_2(^1 \\Sigma_{\\rm g}^+)$ reaction is studied using a reproducing kernel Hilbert space (RKHS)-based, global potential energy surface (PES) at the MRCI-F12/aug-cc-pVTZ-F12 level of theory. For the atom exchange reaction $({\\rm N_A N_B} + {\\rm N_C} \\rightarrow {\\rm   N_A N_C} + {\\rm N_B}$), computed thermal rates and their temperature dependence from quasi-classical trajectory (QCT) simulations agree to within error bars with the available experiments. Companion QCT simulations using a recently published CASPT2-based PES confirm these findings. For the atomization reaction, leading to three N$(^4{\\rm   S})$ atoms, the computed rates from the RKHS-PES overestimate the experimentally reported rates by one order of magnitude whereas those from the PIP-PES agree favourably, and the $T$-dependence of both computations is consistent with experiment. These differences can be traced back to the different methods and basis sets used. The lifetime of the metastable N$_3$ molecule is estimated to be $\\sim 200$ fs depending on the initial state of the reactants. Finally, neural network-based exhaustive state-to-distribution models are presented using both PESs for the atom exchange reaction. These models will be instrumental for a broader exploration of the reaction dynamics of air.","sentences":["The atom-exchange and atomization dissociation dynamics for the N($^4$S)","+","N$_2(^1 \\Sigma_{\\rm g}^+)$ reaction is studied using a reproducing kernel Hilbert space (RKHS)-based, global potential energy surface (PES) at the MRCI-F12/aug-cc-pVTZ-F12 level of theory.","For the atom exchange reaction $({\\rm N_A N_B} + {\\rm N_C} \\rightarrow {\\rm   N_A N_C} + {\\rm N_B}$), computed thermal rates and their temperature dependence from quasi-classical trajectory (QCT) simulations agree to within error bars with the available experiments.","Companion QCT simulations using a recently published CASPT2-based PES confirm these findings.","For the atomization reaction, leading to three N$(^4{\\rm   S})$ atoms, the computed rates from the RKHS-PES overestimate the experimentally reported rates by one order of magnitude whereas those from the PIP-PES agree favourably, and the $T$-dependence of both computations is consistent with experiment.","These differences can be traced back to the different methods and basis sets used.","The lifetime of the metastable N$_3$ molecule is estimated to be $\\sim 200$ fs depending on the initial state of the reactants.","Finally, neural network-based exhaustive state-to-distribution models are presented using both PESs for the atom exchange reaction.","These models will be instrumental for a broader exploration of the reaction dynamics of air."],"url":"http://arxiv.org/abs/2404.18877v1","category":"physics.chem-ph"}
{"created":"2024-04-29 16:25:07","title":"Continuity of attractors of parabolic equations with nonlinear boundary conditions and rapidly varying boundaries. The case of a Lipschitz deformation","abstract":"In this paper we obtain the continuity of attractors for nonlinear parabolic equations with nonlinear boundary conditions when the boundary of the domain varies very rapidly as a parameter $\\epsilon$ goes to zero. We want to consider the case which the boundary of the domain presents a highly oscillatory behavior as $\\epsilon$ goes to zero. For the case where we have a Lipschitz deformation of the boundary with the Lipschitz constant uniformly bounded in $\\epsilon$ but the boundaries do not approach in a Lipschitz sense, the solutions of these equations $E$-converge to the solution of a limit parabolic equation of the same type, where the boundary condition has a factor that captures the oscillations of the boundary. To address this problem, it is necessary to consider the notion of convergence of functions defined in varying domains and the convergence of a family of operators defined in different Banach spaces. Since the problems have nonlinear terms at the boundary, then it is necessary to extend these concepts to the case of spaces with negative exponents and to operators defined between these spaces.","sentences":["In this paper we obtain the continuity of attractors for nonlinear parabolic equations with nonlinear boundary conditions when the boundary of the domain varies very rapidly as a parameter $\\epsilon$ goes to zero.","We want to consider the case which the boundary of the domain presents a highly oscillatory behavior as $\\epsilon$ goes to zero.","For the case where we have a Lipschitz deformation of the boundary with the Lipschitz constant uniformly bounded in $\\epsilon$ but the boundaries do not approach in a Lipschitz sense, the solutions of these equations $E$-converge to the solution of a limit parabolic equation of the same type, where the boundary condition has a factor that captures the oscillations of the boundary.","To address this problem, it is necessary to consider the notion of convergence of functions defined in varying domains and the convergence of a family of operators defined in different Banach spaces.","Since the problems have nonlinear terms at the boundary, then it is necessary to extend these concepts to the case of spaces with negative exponents and to operators defined between these spaces."],"url":"http://arxiv.org/abs/2404.18836v1","category":"math.AP"}
{"created":"2024-04-29 16:04:49","title":"Non-parametric estimation for the stochastic wave equation","abstract":"The spatially dependent wave speed of a stochastic wave equation driven by space-time white noise is estimated using the local observation scheme. Given a fixed time horizon, we prove asymptotic normality for an augmented maximum likelihood estimator as the resolution level of the observations tends to zero. We show that the expectation and variance of the observed Fisher information are intrinsically related to the kinetic energy within an associated deterministic wave equation and prove an asymptotic equipartition of energy principle using the notion of asymptotic Riemann-Lebesgue operators.","sentences":["The spatially dependent wave speed of a stochastic wave equation driven by space-time white noise is estimated using the local observation scheme.","Given a fixed time horizon, we prove asymptotic normality for an augmented maximum likelihood estimator as the resolution level of the observations tends to zero.","We show that the expectation and variance of the observed Fisher information are intrinsically related to the kinetic energy within an associated deterministic wave equation and prove an asymptotic equipartition of energy principle using the notion of asymptotic Riemann-Lebesgue operators."],"url":"http://arxiv.org/abs/2404.18823v1","category":"math.ST"}
{"created":"2024-04-29 14:44:24","title":"Differential Inclusions Involving the Curl Operator","abstract":"In this article, we study the existence of $\\eta\\in W_0^{1,\\infty}(\\Omega;\\mathbb R^n)$ satisfying $$\\textrm{curl} \\ \\eta\\in E \\textrm{ a.e. in }\\Omega,$$ where $n\\in \\mathbb N, \\Omega\\subseteq \\mathbb R^n$ is open, bounded and $E\\subseteq \\Lambda^2.$","sentences":["In this article, we study the existence of $\\eta\\in W_0^{1,\\infty}(\\Omega;\\mathbb R^n)$ satisfying $$\\textrm{curl} \\ \\eta\\in E \\textrm{ a.e. in }\\Omega,$$ where $n\\in \\mathbb N, \\Omega\\subseteq \\mathbb R^n$ is open, bounded and $E\\subseteq \\Lambda^2.$"],"url":"http://arxiv.org/abs/2404.18744v1","category":"math.AP"}
{"created":"2024-04-29 13:41:35","title":"Private graph colouring with limited defectiveness","abstract":"Differential privacy is the gold standard in the problem of privacy preserving data analysis, which is crucial in a wide range of disciplines. Vertex colouring is one of the most fundamental questions about a graph. In this paper, we study the vertex colouring problem in the differentially private setting.   To be edge-differentially private, a colouring algorithm needs to be defective: a colouring is d-defective if a vertex can share a colour with at most d of its neighbours. Without defectiveness, the only differentially private colouring algorithm needs to assign n different colours to the n different vertices. We show the following lower bound for the defectiveness: a differentially private c-edge colouring algorithm of a graph of maximum degree {\\Delta} > 0 has defectiveness at least d = {\\Omega} (log n / (log c+log {\\Delta})).   We also present an {\\epsilon}-differentially private algorithm to {\\Theta} ( {\\Delta} / log n + 1 / {\\epsilon})-colour a graph with defectiveness at most {\\Theta}(log n).","sentences":["Differential privacy is the gold standard in the problem of privacy preserving data analysis, which is crucial in a wide range of disciplines.","Vertex colouring is one of the most fundamental questions about a graph.","In this paper, we study the vertex colouring problem in the differentially private setting.   ","To be edge-differentially private, a colouring algorithm needs to be defective: a colouring is d-defective if a vertex can share a colour with at most d of its neighbours.","Without defectiveness, the only differentially private colouring algorithm needs to assign n different colours to the n different vertices.","We show the following lower bound for the defectiveness: a differentially private c-edge colouring algorithm of a graph of maximum degree {\\Delta} > 0 has defectiveness at least d = {\\Omega} (log n / (log c+log {\\Delta})).   ","We also present an {\\epsilon}-differentially private algorithm to {\\Theta} ( {\\Delta} / log n + 1 / {\\epsilon})-colour a graph with defectiveness at most {\\Theta}(log n)."],"url":"http://arxiv.org/abs/2404.18692v1","category":"cs.DS"}
{"created":"2024-04-29 12:19:31","title":"On a conjecture of Levesque and Waldschmidt II","abstract":"Related to Shank's notion of simplest cubic fields, the family of parametrised Diophantine equations,   \\[ x^3 - (n-1) x^2 y - (n+2) xy^2 - 1 = \\left( x - \\lambda_0 y\\right) \\left(x-\\lambda_1 y\\right) \\left(x - \\lambda_2 y\\right) = \\pm 1,   \\] was studied and solved effectively by Thomas and later solved completely by Mignotte.   An open conjecture of Levesque and Waldschmidt states that taking these parametrised Diophantine equations and twisting them not only once but twice, in the sense that we look at   \\[ f_{n,s,t}(x,y) = \\left( x - \\lambda_0^s \\lambda_1^t y \\right) \\left( x - \\lambda_1^s\\lambda_2^t y \\right) \\left( x - \\lambda_2^s\\lambda_0^t y \\right) = \\pm 1,   \\] retains a result similar to what Thomas obtained in the original or Levesque and Waldschidt in the once-twisted ($t = 0$) case; namely, that non-trivial solutions can only appear in equations where the parameters are small. We confirm this conjecture, given that the absolute values of the exponents $s, t$ are not too large compared to the base parameter $n$.","sentences":["Related to Shank's notion of simplest cubic fields, the family of parametrised Diophantine equations,   \\[ x^3 - (n-1) x^2 y - (n+2) xy^2 - 1 = \\left( x - \\lambda_0 y\\right) \\left(x-\\lambda_1 y\\right) \\left(x - \\lambda_2 y\\right) = \\pm 1,   \\] was studied and solved effectively by Thomas and later solved completely by Mignotte.   ","An open conjecture of Levesque and Waldschmidt states that taking these parametrised Diophantine equations and twisting them not only once but twice, in the sense that we look at   \\[ f_{n,s,t}(x,y) = \\left( x - \\lambda_0^s \\lambda_1^t y \\right) \\left( x - \\lambda_1^s\\lambda_2^t y \\right) \\left( x - \\lambda_2^s\\lambda_0^t y \\right) = \\pm 1,   \\] retains a result similar to what Thomas obtained in the original or Levesque and Waldschidt in the once-twisted ($t = 0$) case; namely, that non-trivial solutions can only appear in equations where the parameters are small.","We confirm this conjecture, given that the absolute values of the exponents $s, t$ are not too large compared to the base parameter $n$."],"url":"http://arxiv.org/abs/2404.18642v1","category":"math.NT"}
{"created":"2024-04-29 12:12:12","title":"A Stochastic Reconstruction Theorem on Rectangular Increments with an Application to a Mixed Hyperbolic SPDE","abstract":"We extend the stochastic reconstruction theorem to a setting where the underlying family of distributions satisfies some natural conditions involving rectangular increments. This allows us to prove the well-posedness of a new class of mixed stochastic partial differential of hyperbolic type which combines standard Walsh stochastic integration and Young products.","sentences":["We extend the stochastic reconstruction theorem to a setting where the underlying family of distributions satisfies some natural conditions involving rectangular increments.","This allows us to prove the well-posedness of a new class of mixed stochastic partial differential of hyperbolic type which combines standard Walsh stochastic integration and Young products."],"url":"http://arxiv.org/abs/2404.18634v1","category":"math.PR"}
{"created":"2024-04-29 11:19:59","title":"Interference with (Pseudo) Thermal Light; The Hanbury Brown and Twiss Effect","abstract":"The correlation of light from two sources leads to an interference pattern if they belong to a specific time interval known as the coherence time, denoted as $\\Delta \\tau$. The relationship governing this phenomenon is $\\Delta \\tau \\Delta \\nu \\approx 1$, where $\\Delta \\nu$ represents the bandwidth of the light. This requirement is not satisfied, and hence, interference fringes are not observable in the case of ordinary (thermal) light. In the 1950s, Robert Hanbury Brown and Richard Q. Twiss explored interference phenomena using a narrow bandwidth of thermal light. This investigation led to the discovery of the Hanbury-Brown and Twiss effect (or the HBT effect in short), which has since found applications in various fields, particularly stellar observations and quantum optics. This article briefly traces the history of the HBT effect and its applications in various fields, including stellar observations. More importantly, it outlines the basic theoretical framework of this effect, followed by the design and results of the correlation in intensity fluctuation of a pseudo-thermal light in a college laboratory setting (Michelson interferometer).","sentences":["The correlation of light from two sources leads to an interference pattern if they belong to a specific time interval known as the coherence time, denoted as $\\Delta \\tau$.","The relationship governing this phenomenon is $\\Delta \\tau \\Delta \\nu \\approx 1$, where $\\Delta \\nu$ represents the bandwidth of the light.","This requirement is not satisfied, and hence, interference fringes are not observable in the case of ordinary (thermal) light.","In the 1950s, Robert Hanbury Brown and Richard Q. Twiss explored interference phenomena using a narrow bandwidth of thermal light.","This investigation led to the discovery of the Hanbury-Brown and Twiss effect (or the HBT effect in short), which has since found applications in various fields, particularly stellar observations and quantum optics.","This article briefly traces the history of the HBT effect and its applications in various fields, including stellar observations.","More importantly, it outlines the basic theoretical framework of this effect, followed by the design and results of the correlation in intensity fluctuation of a pseudo-thermal light in a college laboratory setting (Michelson interferometer)."],"url":"http://arxiv.org/abs/2404.18606v1","category":"astro-ph.IM"}
{"created":"2024-04-29 10:12:12","title":"Classification of Affinely Homogeneous Hessian Rank 2 Hypersurfaces S^3 in R^4","abstract":"We determine all affinely homogeneous hypersurfaces S^3 in R^4 whose Hessian is (invariantly) of constant rank 2, including the simply transitive ones.   We find 34 inequivalent terminal branches yielding each to a nonempty moduli space of homogeneous models of hypersurfaces S^3 in R^4, sometimes parametrized by a certain complicated algebraic variety, especially for the 15 (over 34) families of models which are simply transitive.   We employ the power series method of equivalence, which captures invariants at the origin, creates branches, and infinitesimalizes calculations.   In Lie's original classification spirit, we describe the found homogeneous models by listing explicit Lie algebras of infinitesimal transformations, sometimes parametrized by absolute invariants satisfying certain algebraic equations.","sentences":["We determine all affinely homogeneous hypersurfaces S^3 in R^4 whose Hessian is (invariantly) of constant rank 2, including the simply transitive ones.   ","We find 34 inequivalent terminal branches yielding each to a nonempty moduli space of homogeneous models of hypersurfaces S^3 in R^4, sometimes parametrized by a certain complicated algebraic variety, especially for the 15 (over 34) families of models which are simply transitive.   ","We employ the power series method of equivalence, which captures invariants at the origin, creates branches, and infinitesimalizes calculations.   ","In Lie's original classification spirit, we describe the found homogeneous models by listing explicit Lie algebras of infinitesimal transformations, sometimes parametrized by absolute invariants satisfying certain algebraic equations."],"url":"http://arxiv.org/abs/2404.18565v1","category":"math.DG"}
{"created":"2024-04-29 09:42:56","title":"High Fidelity simulations of the multi-species Vlasov equation in the electro-static, collisional-less limit","abstract":"The accurate prediction of occurrence and strength of kinetic instabilities in plasmas remains a significant challenge in nuclear fusion research. To accurately capture the plasmas dynamics one is required to solve the Vlasov equation for several species which, however, comes with a number of challenges as high dimensionality of the model as well as turbulence and development of fine but relevant structures in the distribution function. The predominantly employed Particle-in-Cell (PIC) method often lacks the accuracy to resolve the dynamics correctly, which can only be remedied by going to higher resolutions but at a prohibitorily high cost due to the high-dimensionality. Thus in this work we discuss the usage of the Numerical Flow Iteration (NuFI) as high fidelity approach, in contrast to e.g. PIC or grid-based approaches, to solve the multi-species Vlasov equation in modes leading to kinetic instabilities.","sentences":["The accurate prediction of occurrence and strength of kinetic instabilities in plasmas remains a significant challenge in nuclear fusion research.","To accurately capture the plasmas dynamics one is required to solve the Vlasov equation for several species which, however, comes with a number of challenges as high dimensionality of the model as well as turbulence and development of fine","but relevant structures in the distribution function.","The predominantly employed Particle-in-Cell (PIC) method often lacks the accuracy to resolve the dynamics correctly, which can only be remedied by going to higher resolutions but at a prohibitorily high cost due to the high-dimensionality.","Thus in this work we discuss the usage of the Numerical Flow Iteration (NuFI) as high fidelity approach, in contrast to e.g. PIC or grid-based approaches, to solve the multi-species Vlasov equation in modes leading to kinetic instabilities."],"url":"http://arxiv.org/abs/2404.18549v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 09:14:42","title":"Predicting PDEs Fast and Efficiently with Equivariant Extreme Learning Machines","abstract":"We utilize extreme learning machines for the prediction of partial differential equations (PDEs). Our method splits the state space into multiple windows that are predicted individually using a single model. Despite requiring only few data points (in some cases, our method can learn from a single full-state snapshot), it still achieves high accuracy and can predict the flow of PDEs over long time horizons. Moreover, we show how additional symmetries can be exploited to increase sample efficiency and to enforce equivariance.","sentences":["We utilize extreme learning machines for the prediction of partial differential equations (PDEs).","Our method splits the state space into multiple windows that are predicted individually using a single model.","Despite requiring only few data points (in some cases, our method can learn from a single full-state snapshot), it still achieves high accuracy and can predict the flow of PDEs over long time horizons.","Moreover, we show how additional symmetries can be exploited to increase sample efficiency and to enforce equivariance."],"url":"http://arxiv.org/abs/2404.18530v1","category":"cs.LG"}
{"created":"2024-04-29 06:51:39","title":"MRIC: Model-Based Reinforcement-Imitation Learning with Mixture-of-Codebooks for Autonomous Driving Simulation","abstract":"Accurately simulating diverse behaviors of heterogeneous agents in various scenarios is fundamental to autonomous driving simulation. This task is challenging due to the multi-modality of behavior distribution, the high-dimensionality of driving scenarios, distribution shift, and incomplete information. Our first insight is to leverage state-matching through differentiable simulation to provide meaningful learning signals and achieve efficient credit assignment for the policy. This is demonstrated by revealing the existence of gradient highways and interagent gradient pathways. However, the issues of gradient explosion and weak supervision in low-density regions are discovered. Our second insight is that these issues can be addressed by applying dual policy regularizations to narrow the function space. Further considering diversity, our third insight is that the behaviors of heterogeneous agents in the dataset can be effectively compressed as a series of prototype vectors for retrieval. These lead to our model-based reinforcement-imitation learning framework with temporally abstracted mixture-of-codebooks (MRIC). MRIC introduces the open-loop modelbased imitation learning regularization to stabilize training, and modelbased reinforcement learning (RL) regularization to inject domain knowledge. The RL regularization involves differentiable Minkowskidifference-based collision avoidance and projection-based on-road and traffic rule compliance rewards. A dynamic multiplier mechanism is further proposed to eliminate the interference from the regularizations while ensuring their effectiveness. Experimental results using the largescale Waymo open motion dataset show that MRIC outperforms state-ofthe-art baselines on diversity, behavioral realism, and distributional realism, with large margins on some key metrics (e.g., collision rate, minSADE, and time-to-collision JSD).","sentences":["Accurately simulating diverse behaviors of heterogeneous agents in various scenarios is fundamental to autonomous driving simulation.","This task is challenging due to the multi-modality of behavior distribution, the high-dimensionality of driving scenarios, distribution shift, and incomplete information.","Our first insight is to leverage state-matching through differentiable simulation to provide meaningful learning signals and achieve efficient credit assignment for the policy.","This is demonstrated by revealing the existence of gradient highways and interagent gradient pathways.","However, the issues of gradient explosion and weak supervision in low-density regions are discovered.","Our second insight is that these issues can be addressed by applying dual policy regularizations to narrow the function space.","Further considering diversity, our third insight is that the behaviors of heterogeneous agents in the dataset can be effectively compressed as a series of prototype vectors for retrieval.","These lead to our model-based reinforcement-imitation learning framework with temporally abstracted mixture-of-codebooks (MRIC).","MRIC introduces the open-loop modelbased imitation learning regularization to stabilize training, and modelbased reinforcement learning (RL) regularization to inject domain knowledge.","The RL regularization involves differentiable Minkowskidifference-based collision avoidance and projection-based on-road and traffic rule compliance rewards.","A dynamic multiplier mechanism is further proposed to eliminate the interference from the regularizations while ensuring their effectiveness.","Experimental results using the largescale Waymo open motion dataset show that MRIC outperforms state-ofthe-art baselines on diversity, behavioral realism, and distributional realism, with large margins on some key metrics (e.g., collision rate, minSADE, and time-to-collision JSD)."],"url":"http://arxiv.org/abs/2404.18464v1","category":"cs.RO"}
{"created":"2024-04-29 06:24:32","title":"3D Gaussian Splatting with Deferred Reflection","abstract":"The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting.","sentences":["The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis.","However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately.","We present a deferred shading method to effectively render specular reflection with Gaussian splatting.","The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients.","We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects.","Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting."],"url":"http://arxiv.org/abs/2404.18454v1","category":"cs.CV"}
{"created":"2024-04-29 05:29:26","title":"$\u03bd$-DBA: Neural Implicit Dense Bundle Adjustment Enables Image-Only Driving Scene Reconstruction","abstract":"The joint optimization of the sensor trajectory and 3D map is a crucial characteristic of bundle adjustment (BA), essential for autonomous driving. This paper presents $\\nu$-DBA, a novel framework implementing geometric dense bundle adjustment (DBA) using 3D neural implicit surfaces for map parametrization, which optimizes both the map surface and trajectory poses using geometric error guided by dense optical flow prediction. Additionally, we fine-tune the optical flow model with per-scene self-supervision to further improve the quality of the dense mapping. Our experimental results on multiple driving scene datasets demonstrate that our method achieves superior trajectory optimization and dense reconstruction accuracy. We also investigate the influences of photometric error and different neural geometric priors on the performance of surface reconstruction and novel view synthesis. Our method stands as a significant step towards leveraging neural implicit representations in dense bundle adjustment for more accurate trajectories and detailed environmental mapping.","sentences":["The joint optimization of the sensor trajectory and 3D map is a crucial characteristic of bundle adjustment (BA), essential for autonomous driving.","This paper presents $\\nu$-DBA, a novel framework implementing geometric dense bundle adjustment (DBA) using 3D neural implicit surfaces for map parametrization, which optimizes both the map surface and trajectory poses using geometric error guided by dense optical flow prediction.","Additionally, we fine-tune the optical flow model with per-scene self-supervision to further improve the quality of the dense mapping.","Our experimental results on multiple driving scene datasets demonstrate that our method achieves superior trajectory optimization and dense reconstruction accuracy.","We also investigate the influences of photometric error and different neural geometric priors on the performance of surface reconstruction and novel view synthesis.","Our method stands as a significant step towards leveraging neural implicit representations in dense bundle adjustment for more accurate trajectories and detailed environmental mapping."],"url":"http://arxiv.org/abs/2404.18439v1","category":"cs.CV"}
{"created":"2024-04-29 04:57:09","title":"Well-defined $f(Q)$ Gravity, Reconstruction of FLRW Spacetime and Unification of Inflation with Dark Energy Epoch","abstract":"We formulate the convenient and mimetic $f(Q)$ gravities in terms of the metric and four scalar fields with the $Q$ being the non-metricity scalar. As a result, it is shown that the obtained field equations are well-defined. By using the field equations, we show how the $f(Q)$ models which realise any given FLRW spacetime may be reconstructed. We construct convenient $f(Q)$ gravity and mimetic $f(Q)$ gravity, which describe the inflation and dark energy epochs and even unify the inflation and dark energy. Moreover, a radiation-dominated epoch and early dark energy may be easily included in the above theories as is explicitly shown. It is shown that the inflationary spectral index $n_s$ and the tensor-to-scalar ratio $r$ are consistent with Planck data for some models of $f(Q)$ gravity. Corresponding asymptotic forms of $f(Q)$ and mimetic $f(Q)$ are given. Finally, some remarks on possible reconstruction for spherically symmetric spacetime are given.","sentences":["We formulate the convenient and mimetic $f(Q)$ gravities in terms of the metric and four scalar fields with the $Q$ being the non-metricity scalar.","As a result, it is shown that the obtained field equations are well-defined.","By using the field equations, we show how the $f(Q)$ models which realise any given FLRW spacetime may be reconstructed.","We construct convenient $f(Q)$ gravity and mimetic $f(Q)$ gravity, which describe the inflation and dark energy epochs and even unify the inflation and dark energy.","Moreover, a radiation-dominated epoch and early dark energy may be easily included in the above theories as is explicitly shown.","It is shown that the inflationary spectral index $n_s$ and the tensor-to-scalar ratio $r$ are consistent with Planck data for some models of $f(Q)$ gravity.","Corresponding asymptotic forms of $f(Q)$ and mimetic $f(Q)$ are given.","Finally, some remarks on possible reconstruction for spherically symmetric spacetime are given."],"url":"http://arxiv.org/abs/2404.18427v1","category":"gr-qc"}
{"created":"2024-04-29 04:10:22","title":"Learning a Sparse Neural Network using IHT","abstract":"The core of a good model is in its ability to focus only on important information that reflects the basic patterns and consistencies, thus pulling out a clear, noise-free signal from the dataset. This necessitates using a simplified model defined by fewer parameters. The importance of theoretical foundations becomes clear in this context, as this paper relies on established results from the domain of advanced sparse optimization, particularly those addressing nonlinear differentiable functions. The need for such theoretical foundations is further highlighted by the trend that as computational power for training NNs increases, so does the complexity of the models in terms of a higher number of parameters. In practical scenarios, these large models are often simplified to more manageable versions with fewer parameters.   Understanding why these simplified models with less number of parameters remain effective raises a crucial question. Understanding why these simplified models with fewer parameters remain effective raises an important question. This leads to the broader question of whether there is a theoretical framework that can clearly explain these empirical observations. Recent developments, such as establishing necessary conditions for the convergence of iterative hard thresholding (IHT) to a sparse local minimum (a sparse method analogous to gradient descent) are promising. The remarkable capacity of the IHT algorithm to accurately identify and learn the locations of nonzero parameters underscores its practical effectiveness and utility.   This paper aims to investigate whether the theoretical prerequisites for such convergence are applicable in the realm of neural network (NN) training by providing justification for all the necessary conditions for convergence. Then, these conditions are validated by experiments on a single-layer NN, using the IRIS dataset as a testbed.","sentences":["The core of a good model is in its ability to focus only on important information that reflects the basic patterns and consistencies, thus pulling out a clear, noise-free signal from the dataset.","This necessitates using a simplified model defined by fewer parameters.","The importance of theoretical foundations becomes clear in this context, as this paper relies on established results from the domain of advanced sparse optimization, particularly those addressing nonlinear differentiable functions.","The need for such theoretical foundations is further highlighted by the trend that as computational power for training NNs increases, so does the complexity of the models in terms of a higher number of parameters.","In practical scenarios, these large models are often simplified to more manageable versions with fewer parameters.   ","Understanding why these simplified models with less number of parameters remain effective raises a crucial question.","Understanding why these simplified models with fewer parameters remain effective raises an important question.","This leads to the broader question of whether there is a theoretical framework that can clearly explain these empirical observations.","Recent developments, such as establishing necessary conditions for the convergence of iterative hard thresholding (IHT) to a sparse local minimum (a sparse method analogous to gradient descent) are promising.","The remarkable capacity of the IHT algorithm to accurately identify and learn the locations of nonzero parameters underscores its practical effectiveness and utility.   ","This paper aims to investigate whether the theoretical prerequisites for such convergence are applicable in the realm of neural network (NN) training by providing justification for all the necessary conditions for convergence.","Then, these conditions are validated by experiments on a single-layer NN, using the IRIS dataset as a testbed."],"url":"http://arxiv.org/abs/2404.18414v1","category":"cs.LG"}
{"created":"2024-04-29 03:41:49","title":"Deep generative modelling of canonical ensemble with differentiable thermal properties","abstract":"We propose a variational modelling method with differentiable temperature for canonical ensembles. Using a deep generative model, the free energy is estimated and minimized simultaneously in a continuous temperature range. At optimal, this generative model is a Boltzmann distribution with temperature dependence. The training process requires no dataset, and works with arbitrary explicit density generative models. We applied our method to study the phase transitions (PT) in the Ising and XY models, and showed that the direct-sampling simulation of our model is as accurate as the Markov Chain Monte Carlo (MCMC) simulation, but more efficient. Moreover, our method can give thermodynamic quantities as differentiable functions of temperature akin to an analytical solution. The free energy aligns closely with the exact one to the second-order derivative, so this inclusion of temperature dependence enables the otherwise biased variational model to capture the subtle thermal effects at the PTs. These findings shed light on the direct simulation of physical systems using deep generative models","sentences":["We propose a variational modelling method with differentiable temperature for canonical ensembles.","Using a deep generative model, the free energy is estimated and minimized simultaneously in a continuous temperature range.","At optimal, this generative model is a Boltzmann distribution with temperature dependence.","The training process requires no dataset, and works with arbitrary explicit density generative models.","We applied our method to study the phase transitions (PT) in the Ising and XY models, and showed that the direct-sampling simulation of our model is as accurate as the Markov Chain Monte Carlo (MCMC) simulation, but more efficient.","Moreover, our method can give thermodynamic quantities as differentiable functions of temperature akin to an analytical solution.","The free energy aligns closely with the exact one to the second-order derivative, so this inclusion of temperature dependence enables the otherwise biased variational model to capture the subtle thermal effects at the PTs.","These findings shed light on the direct simulation of physical systems using deep generative models"],"url":"http://arxiv.org/abs/2404.18404v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-29 03:15:21","title":"Mesh-based Photorealistic and Real-time 3D Mapping for Robust Visual Perception of Autonomous Underwater Vehicle","abstract":"This paper proposes a photorealistic real-time dense 3D mapping system that utilizes a learning-based image enhancement method and mesh-based map representation. Due to the characteristics of the underwater environment, where problems such as hazing and low contrast occur, it is hard to apply conventional simultaneous localization and mapping (SLAM) methods. Furthermore, for sensitive tasks like inspecting cracks, photorealistic mapping is very important. However, the behavior of Autonomous Underwater Vehicle (AUV) is computationally constrained. In this paper, we utilize a neural network-based image enhancement method to improve pose estimation and mapping quality and apply a sliding window-based mesh expansion method to enable lightweight, fast, and photorealistic mapping. To validate our results, we utilize real-world and indoor synthetic datasets. We performed qualitative validation with the real-world dataset and quantitative validation by modeling images from the indoor synthetic dataset as underwater scenes.","sentences":["This paper proposes a photorealistic real-time dense 3D mapping system that utilizes a learning-based image enhancement method and mesh-based map representation.","Due to the characteristics of the underwater environment, where problems such as hazing and low contrast occur, it is hard to apply conventional simultaneous localization and mapping (SLAM) methods.","Furthermore, for sensitive tasks like inspecting cracks, photorealistic mapping is very important.","However, the behavior of Autonomous Underwater Vehicle (AUV) is computationally constrained.","In this paper, we utilize a neural network-based image enhancement method to improve pose estimation and mapping quality and apply a sliding window-based mesh expansion method to enable lightweight, fast, and photorealistic mapping.","To validate our results, we utilize real-world and indoor synthetic datasets.","We performed qualitative validation with the real-world dataset and quantitative validation by modeling images from the indoor synthetic dataset as underwater scenes."],"url":"http://arxiv.org/abs/2404.18395v1","category":"cs.RO"}
{"created":"2024-04-29 03:11:31","title":"Machine Learning Interatomic Potentials with Keras API","abstract":"A neural network is used to train, predict, and evaluate a model to calculate the energies of 3-dimensional systems composed of Ti and O atoms. Python classes are implemented to quantify atomic interactions through symmetry functions and to specify prediction algorithms. The hyperparameters of the model are optimised by minimising validation RMSE, which then produced a model that is accurate to within 100 eV. The model could be improved by proper testing of symmetry function calculations and addressing properties of features and targets.","sentences":["A neural network is used to train, predict, and evaluate a model to calculate the energies of 3-dimensional systems composed of Ti and O atoms.","Python classes are implemented to quantify atomic interactions through symmetry functions and to specify prediction algorithms.","The hyperparameters of the model are optimised by minimising validation RMSE, which then produced a model that is accurate to within 100 eV. The model could be improved by proper testing of symmetry function calculations and addressing properties of features and targets."],"url":"http://arxiv.org/abs/2404.18393v1","category":"physics.comp-ph"}
{"created":"2024-04-29 02:33:40","title":"Object Registration in Neural Fields","abstract":"Neural fields provide a continuous scene representation of 3D geometry and appearance in a way which has great promise for robotics applications. One functionality that unlocks unique use-cases for neural fields in robotics is object 6-DoF registration. In this paper, we provide an expanded analysis of the recent Reg-NF neural field registration method and its use-cases within a robotics context. We showcase the scenario of determining the 6-DoF pose of known objects within a scene using scene and object neural field models. We show how this may be used to better represent objects within imperfectly modelled scenes and generate new scenes by substituting object neural field models into the scene.","sentences":["Neural fields provide a continuous scene representation of 3D geometry and appearance in a way which has great promise for robotics applications.","One functionality that unlocks unique use-cases for neural fields in robotics is object 6-DoF registration.","In this paper, we provide an expanded analysis of the recent Reg-NF neural field registration method and its use-cases within a robotics context.","We showcase the scenario of determining the 6-DoF pose of known objects within a scene using scene and object neural field models.","We show how this may be used to better represent objects within imperfectly modelled scenes and generate new scenes by substituting object neural field models into the scene."],"url":"http://arxiv.org/abs/2404.18381v1","category":"cs.RO"}
{"created":"2024-04-29 02:28:47","title":"On a fully nonlinear k-Hessian system of Lane-Emden type","abstract":"In this manuscript we prove the existence of solutions to a fully nonlinear system of (degenerate) elliptic equations of Lane-Emden type and discuss a inhomogeneous generalization.","sentences":["In this manuscript we prove the existence of solutions to a fully nonlinear system of (degenerate) elliptic equations of Lane-Emden type and discuss a inhomogeneous generalization."],"url":"http://arxiv.org/abs/2404.18379v1","category":"math.AP"}
{"created":"2024-04-29 02:28:09","title":"Overflying Nilpotent Horizons","abstract":"We study solutions of Einstein equations with negative cosmological constant in five dimensions that describe black holes whose event horizons are homogeneous, anisotropic spaces. We focus on the case where the constant-time slices of the horizon are the Nil geometry, the Thurston geometry associated to the Heisenberg group. For such spaces, we analyze the symmetries both in the asymptotic region and in the near horizon region. We compute the associated conserved charges, which turn out to be finite and admit a sensible physical interpretation. We analyze the thermodynamics of the Nil black hole, and we present a stationary spinning generalization of it in the slowly rotating approximation.","sentences":["We study solutions of Einstein equations with negative cosmological constant in five dimensions that describe black holes whose event horizons are homogeneous, anisotropic spaces.","We focus on the case where the constant-time slices of the horizon are the Nil geometry, the Thurston geometry associated to the Heisenberg group.","For such spaces, we analyze the symmetries both in the asymptotic region and in the near horizon region.","We compute the associated conserved charges, which turn out to be finite and admit a sensible physical interpretation.","We analyze the thermodynamics of the Nil black hole, and we present a stationary spinning generalization of it in the slowly rotating approximation."],"url":"http://arxiv.org/abs/2404.18378v1","category":"hep-th"}
{"created":"2024-04-29 02:20:12","title":"Integrable semi-discretization for a modified Camassa-Holm equation with cubic nonlinearity","abstract":"In the present paper, an integrable semi-discretization of the modified Camassa-Holm (mCH) equation with cubic nonlinearity is presented. The key points of the construction are based on the discrete Kadomtsev-Petviashvili (KP) equation and appropriate definition of discrete reciprocal transformations. First, we demonstrate that these bilinear equations and their determinant solutions can be derived from the discrete KP equation through Miwa transformation and some reductions. Then, by scrutinizing the reduction process, we obtain a set of semi-discrete bilinear equations and their general soliton solutions in the Gram-type determinant form. Finally, we obtain an integrable semi-discrete analog of the mCH equation by introducing dependent variables and discrete reciprocal transformation. It is also shown that the semi-discrete mCH equation converges to the continuous one in the continuum limit.","sentences":["In the present paper, an integrable semi-discretization of the modified Camassa-Holm (mCH) equation with cubic nonlinearity is presented.","The key points of the construction are based on the discrete Kadomtsev-Petviashvili (KP) equation and appropriate definition of discrete reciprocal transformations.","First, we demonstrate that these bilinear equations and their determinant solutions can be derived from the discrete KP equation through Miwa transformation and some reductions.","Then, by scrutinizing the reduction process, we obtain a set of semi-discrete bilinear equations and their general soliton solutions in the Gram-type determinant form.","Finally, we obtain an integrable semi-discrete analog of the mCH equation by introducing dependent variables and discrete reciprocal transformation.","It is also shown that the semi-discrete mCH equation converges to the continuous one in the continuum limit."],"url":"http://arxiv.org/abs/2404.18372v1","category":"nlin.SI"}
{"created":"2024-04-29 00:56:45","title":"Some Computational Results on Koszul-Vinberg Cochain Complexes","abstract":"An affine connection is said to be flat if its curvature tensor vanishes identically. Koszul-Vinberg (KV for abbreviation) cohomology has been invoked to study the deformation theory of flat and torsion-free affine connections on tangent bundle. In this Note, we compute explicitly the differentials of various specific KV cochains, and study their relation to classical objects in information geometry, including deformations associated with projective and dual-projective transformations of a flat and torsion-free affine connection. As an application, we also give a simple yet non-trivial example of a KV algebra of which second cohomology group does not vanish.","sentences":["An affine connection is said to be flat if its curvature tensor vanishes identically.","Koszul-Vinberg (KV for abbreviation) cohomology has been invoked to study the deformation theory of flat and torsion-free affine connections on tangent bundle.","In this Note, we compute explicitly the differentials of various specific KV cochains, and study their relation to classical objects in information geometry, including deformations associated with projective and dual-projective transformations of a flat and torsion-free affine connection.","As an application, we also give a simple yet non-trivial example of a KV algebra of which second cohomology group does not vanish."],"url":"http://arxiv.org/abs/2404.18344v1","category":"math.DG"}
{"created":"2024-04-28 20:54:57","title":"DIRESA, a distance-preserving nonlinear dimension reduction technique based on regularized autoencoders","abstract":"In meteorology, finding similar weather patterns or analogs in historical datasets can be useful for data assimilation, forecasting, and postprocessing. In climate science, analogs in historical and climate projection data are used for attribution and impact studies. However, most of the time, those large weather and climate datasets are nearline. They must be downloaded, which takes a lot of bandwidth and disk space, before the computationally expensive search can be executed. We propose a dimension reduction technique based on autoencoder (AE) neural networks to compress those datasets and perform the search in an interpretable, compressed latent space. A distance-regularized Siamese twin autoencoder (DIRESA) architecture is designed to preserve distance in latent space while capturing the nonlinearities in the datasets. Using conceptual climate models of different complexities, we show that the latent components thus obtained provide physical insight into the dominant modes of variability in the system. Compressing datasets with DIRESA reduces the online storage and keeps the latent components uncorrelated, while the distance (ordering) preservation and reconstruction fidelity robustly outperform Principal Component Analysis (PCA) and other dimension reduction techniques such as UMAP or variational autoencoders.","sentences":["In meteorology, finding similar weather patterns or analogs in historical datasets can be useful for data assimilation, forecasting, and postprocessing.","In climate science, analogs in historical and climate projection data are used for attribution and impact studies.","However, most of the time, those large weather and climate datasets are nearline.","They must be downloaded, which takes a lot of bandwidth and disk space, before the computationally expensive search can be executed.","We propose a dimension reduction technique based on autoencoder (AE) neural networks to compress those datasets and perform the search in an interpretable, compressed latent space.","A distance-regularized Siamese twin autoencoder (DIRESA) architecture is designed to preserve distance in latent space while capturing the nonlinearities in the datasets.","Using conceptual climate models of different complexities, we show that the latent components thus obtained provide physical insight into the dominant modes of variability in the system.","Compressing datasets with DIRESA reduces the online storage and keeps the latent components uncorrelated, while the distance (ordering) preservation and reconstruction fidelity robustly outperform Principal Component Analysis (PCA) and other dimension reduction techniques such as UMAP or variational autoencoders."],"url":"http://arxiv.org/abs/2404.18314v1","category":"cs.LG"}
{"created":"2024-04-28 20:49:21","title":"Application of Iterative LQR on a Mobile Robot With Simple Dynamics","abstract":"The aim in this paper is to apply the iLQR, iterative Linear Quadratic Regulator, to control the movement of a mobile robot following an already defined trajectory. This control strategy has proven its utility for nonlinear systems. As follows up, this work intends to concertize this statement and to evaluate the extent to which the performance is comparatively improved against the ordinary, non-iterative LQR. The method is applied to a differential robot with non-holonomic constraints. The mathematical equations, resulting description and the implementation of this method are explicitly explained, and the simulation studies are conducted in the Matlab and Simulink environment.","sentences":["The aim in this paper is to apply the iLQR, iterative Linear Quadratic Regulator, to control the movement of a mobile robot following an already defined trajectory.","This control strategy has proven its utility for nonlinear systems.","As follows up, this work intends to concertize this statement and to evaluate the extent to which the performance is comparatively improved against the ordinary, non-iterative LQR.","The method is applied to a differential robot with non-holonomic constraints.","The mathematical equations, resulting description and the implementation of this method are explicitly explained, and the simulation studies are conducted in the Matlab and Simulink environment."],"url":"http://arxiv.org/abs/2404.18312v1","category":"eess.SY"}
{"created":"2024-04-28 20:00:04","title":"VoroTO: Multiscale Topology Optimization of Voronoi Structures using Surrogate Neural Networks","abstract":"Cellular structures found in nature exhibit remarkable properties such as high strength, high energy absorption, excellent thermal/acoustic insulation, and fluid transfusion. Many of these structures are Voronoi-like; therefore researchers have proposed Voronoi multi-scale designs for a wide variety of engineering applications. However, designing such structures can be computationally prohibitive due to the multi-scale nature of the underlying analysis and optimization. In this work, we propose the use of a neural network (NN) to carry out efficient topology optimization (TO) of multi-scale Voronoi structures. The NN is first trained using Voronoi parameters (cell site locations, thickness, orientation, and anisotropy) to predict the homogenized constitutive properties. This network is then integrated into a conventional TO framework to minimize structural compliance subject to a volume constraint. Special considerations are given for ensuring positive definiteness of the constitutive matrix and promoting macroscale connectivity. Several numerical examples are provided to showcase the proposed method.","sentences":["Cellular structures found in nature exhibit remarkable properties such as high strength, high energy absorption, excellent thermal/acoustic insulation, and fluid transfusion.","Many of these structures are Voronoi-like; therefore researchers have proposed Voronoi multi-scale designs for a wide variety of engineering applications.","However, designing such structures can be computationally prohibitive due to the multi-scale nature of the underlying analysis and optimization.","In this work, we propose the use of a neural network (NN) to carry out efficient topology optimization (TO) of multi-scale Voronoi structures.","The NN is first trained using Voronoi parameters (cell site locations, thickness, orientation, and anisotropy) to predict the homogenized constitutive properties.","This network is then integrated into a conventional TO framework to minimize structural compliance subject to a volume constraint.","Special considerations are given for ensuring positive definiteness of the constitutive matrix and promoting macroscale connectivity.","Several numerical examples are provided to showcase the proposed method."],"url":"http://arxiv.org/abs/2404.18300v1","category":"cs.CE"}
{"created":"2024-04-28 19:02:54","title":"S3-SLAM: Sparse Tri-plane Encoding for Neural Implicit SLAM","abstract":"With the emergence of Neural Radiance Fields (NeRF), neural implicit representations have gained widespread applications across various domains, including simultaneous localization and mapping. However, current neural implicit SLAM faces a challenging trade-off problem between performance and the number of parameters. To address this problem, we propose sparse tri-plane encoding, which efficiently achieves scene reconstruction at resolutions up to 512 using only 2~4% of the commonly used tri-plane parameters (reduced from 100MB to 2~4MB). On this basis, we design S3-SLAM to achieve rapid and high-quality tracking and mapping through sparsifying plane parameters and integrating orthogonal features of tri-plane. Furthermore, we develop hierarchical bundle adjustment to achieve globally consistent geometric structures and reconstruct high-resolution appearance. Experimental results demonstrate that our approach achieves competitive tracking and scene reconstruction with minimal parameters on three datasets. Source code will soon be available.","sentences":["With the emergence of Neural Radiance Fields (NeRF), neural implicit representations have gained widespread applications across various domains, including simultaneous localization and mapping.","However, current neural implicit SLAM faces a challenging trade-off problem between performance and the number of parameters.","To address this problem, we propose sparse tri-plane encoding, which efficiently achieves scene reconstruction at resolutions up to 512 using only 2~4% of the commonly used tri-plane parameters (reduced from 100MB to 2~4MB).","On this basis, we design S3-SLAM to achieve rapid and high-quality tracking and mapping through sparsifying plane parameters and integrating orthogonal features of tri-plane.","Furthermore, we develop hierarchical bundle adjustment to achieve globally consistent geometric structures and reconstruct high-resolution appearance.","Experimental results demonstrate that our approach achieves competitive tracking and scene reconstruction with minimal parameters on three datasets.","Source code will soon be available."],"url":"http://arxiv.org/abs/2404.18284v1","category":"cs.CV"}
{"created":"2024-04-28 18:47:08","title":"An automated pipeline for computation and analysis of functional ventilation and perfusion lung MRI with matrix pencil decomposition: TrueLung","abstract":"Purpose: To introduce and evaluate TrueLung, an automated pipeline for computation and analysis of free-breathing and contrast-agent free pulmonary functional MRI.   Material and Methods: time-resolved ultra-fast bSSFP acquisitions are transferred to TrueLung, which includes image quality checks, image registration, and computation of perfusion and ventilation maps with matrix pencil decomposition. Neural network whole-lung and lobar segmentations allow quantification of impaired relative perfusion (RQ) and fractional ventilation (RFV). TrueLung delivers functional maps and quantitative outcomes, reported for clinicians in concise documents. We evaluated the pipeline in 75 cystic fibrosis children. Whole-lung and lobar segmentations were manually refined when necessary, and the impact on RQ and RFV was quantified.   Results: Functional imaging was performed at 7.9 $\\pm$ 1.8 (mean $\\pm$ SD) coronal slice positions per patient, totaling on average 6min 20s scan time per patient. The whole pipeline required 20min calculation time per subject. TrueLung delivered the functional maps of all the subjects for radiological assessment. Quality controlling maps and segmentations lasted 1min 12s per patient. The automated segmentations and quantification of whole-lung defects were satisfying in 88% of patients (97% of slices) and the lobar quantification in 73% (93% of slices). The segmentations refinements required 16s per patient for the whole-lung, and 2min 10s for the lobe masks. The relative differences in RFV and RQ between fully-automated and manually refined data were marginal.   Conclusions: TrueLung quickly delivers functional maps and quantitative outcomes in an objective and standardized way, suitable for radiological and pneumological assessment with minimal manual input. TrueLung can be used for clinical research in cystic fibrosis and might be applied across various lung diseases.","sentences":["Purpose: To introduce and evaluate TrueLung, an automated pipeline for computation and analysis of free-breathing and contrast-agent free pulmonary functional MRI.   Material and Methods: time-resolved ultra-fast bSSFP acquisitions are transferred to TrueLung, which includes image quality checks, image registration, and computation of perfusion and ventilation maps with matrix pencil decomposition.","Neural network whole-lung and lobar segmentations allow quantification of impaired relative perfusion (RQ) and fractional ventilation (RFV).","TrueLung delivers functional maps and quantitative outcomes, reported for clinicians in concise documents.","We evaluated the pipeline in 75 cystic fibrosis children.","Whole-lung and lobar segmentations were manually refined when necessary, and the impact on RQ and RFV was quantified.   ","Results:","Functional imaging was performed at 7.9 $\\pm$ 1.8 (mean $\\pm$ SD) coronal slice positions per patient, totaling on average 6min 20s scan time per patient.","The whole pipeline required 20min calculation time per subject.","TrueLung delivered the functional maps of all the subjects for radiological assessment.","Quality controlling maps and segmentations lasted 1min 12s per patient.","The automated segmentations and quantification of whole-lung defects were satisfying in 88% of patients (97% of slices) and the lobar quantification in 73% (93% of slices).","The segmentations refinements required 16s per patient for the whole-lung, and 2min 10s for the lobe masks.","The relative differences in RFV and RQ between fully-automated and manually refined data were marginal.   ","Conclusions: TrueLung quickly delivers functional maps and quantitative outcomes in an objective and standardized way, suitable for radiological and pneumological assessment with minimal manual input.","TrueLung can be used for clinical research in cystic fibrosis and might be applied across various lung diseases."],"url":"http://arxiv.org/abs/2404.18275v1","category":"physics.med-ph"}
{"created":"2024-04-28 18:36:59","title":"Parameter-Efficient Tuning Large Language Models for Graph Representation Learning","abstract":"Text-rich graphs, which exhibit rich textual information on nodes and edges, are prevalent across a wide range of real-world business applications. Large Language Models (LLMs) have demonstrated remarkable abilities in understanding text, which also introduced the potential for more expressive modeling in text-rich graphs. Despite these capabilities, efficiently applying LLMs to representation learning on graphs presents significant challenges. Recently, parameter-efficient fine-tuning methods for LLMs have enabled efficient new task generalization with minimal time and memory consumption. Inspired by this, we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel approach for efficient graph representation learning with LLMs on text-rich graphs. Specifically, we utilize a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt. This prompt is then inserted at the beginning of the text sequence. To improve the quality of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting the next token in the node text. Compared with existing joint GNN and LMs, our method directly generate the node embeddings from large language models with an affordable fine-tuning cost. We validate our approach through comprehensive experiments conducted on 8 different text-rich graphs, observing an average improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction evaluations. Our results demonstrate the efficacy and efficiency of our model, showing that it can be smoothly integrated with various large language models, including OPT, LLaMA and Falcon.","sentences":["Text-rich graphs, which exhibit rich textual information on nodes and edges, are prevalent across a wide range of real-world business applications.","Large Language Models (LLMs) have demonstrated remarkable abilities in understanding text, which also introduced the potential for more expressive modeling in text-rich graphs.","Despite these capabilities, efficiently applying LLMs to representation learning on graphs presents significant challenges.","Recently, parameter-efficient fine-tuning methods for LLMs have enabled efficient new task generalization with minimal time and memory consumption.","Inspired by this, we introduce Graph-aware Parameter-Efficient Fine-Tuning - GPEFT, a novel approach for efficient graph representation learning with LLMs on text-rich graphs.","Specifically, we utilize a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt.","This prompt is then inserted at the beginning of the text sequence.","To improve the quality of graph prompts, we pre-trained the GNN to assist the frozen LLM in predicting the next token in the node text.","Compared with existing joint GNN and LMs, our method directly generate the node embeddings from large language models with an affordable fine-tuning cost.","We validate our approach through comprehensive experiments conducted on 8 different text-rich graphs, observing an average improvement of 2% in hit@1 and Mean Reciprocal Rank (MRR) in link prediction evaluations.","Our results demonstrate the efficacy and efficiency of our model, showing that it can be smoothly integrated with various large language models, including OPT, LLaMA and Falcon."],"url":"http://arxiv.org/abs/2404.18271v1","category":"cs.CL"}
{"created":"2024-04-28 17:02:24","title":"Classical integrability in the presence of a cosmological constant: analytic and machine learning results","abstract":"We study the integrability of two-dimensional theories that are obtained by a dimensional reduction of certain four-dimensional gravitational theories describing the coupling of Maxwell fields and neutral scalar fields to gravity in the presence of a potential for the neutral scalar fields. By focusing on a certain solution subspace, we show that a subset of the equations of motion in two dimensions are the compatibility conditions for a modified version of the Breitenlohner-Maison linear system. Subsequently, we study the Liouville integrability of the 2D models encoding the chosen 4D solution subspace from a one-dimensional point of view by constructing Lax pair matrices. In this endeavour, we successfully employ a linear neural network to search for Lax pair matrices for these models, thereby illustrating how machine learning approaches can be effectively implemented to augment the identification of integrable structures in classical systems.","sentences":["We study the integrability of two-dimensional theories that are obtained by a dimensional reduction of certain four-dimensional gravitational theories describing the coupling of Maxwell fields and neutral scalar fields to gravity in the presence of a potential for the neutral scalar fields.","By focusing on a certain solution subspace, we show that a subset of the equations of motion in two dimensions are the compatibility conditions for a modified version of the Breitenlohner-Maison linear system.","Subsequently, we study the Liouville integrability of the 2D models encoding the chosen 4D solution subspace from a one-dimensional point of view by constructing Lax pair matrices.","In this endeavour, we successfully employ a linear neural network to search for Lax pair matrices for these models, thereby illustrating how machine learning approaches can be effectively implemented to augment the identification of integrable structures in classical systems."],"url":"http://arxiv.org/abs/2404.18247v1","category":"hep-th"}
{"created":"2024-04-28 16:54:02","title":"Estimating Bethe roots with VQE","abstract":"Bethe equations, whose solutions determine exact eigenvalues and eigenstates of corresponding integrable Hamiltonians, are generally hard to solve. We implement a Variational Quantum Eigensolver (VQE) approach to estimating Bethe roots of the spin-1/2 XXZ quantum spin chain, by using Bethe states as trial states, and treating Bethe roots as variational parameters. In numerical simulations of systems of size up to 6, we obtain estimates for Bethe roots corresponding to both ground states and excited states with up to 5 down-spins, for both the closed and open XXZ chains. This approach is not limited to real Bethe roots.","sentences":["Bethe equations, whose solutions determine exact eigenvalues and eigenstates of corresponding integrable Hamiltonians, are generally hard to solve.","We implement a Variational Quantum Eigensolver (VQE) approach to estimating Bethe roots of the spin-1/2 XXZ quantum spin chain, by using Bethe states as trial states, and treating Bethe roots as variational parameters.","In numerical simulations of systems of size up to 6, we obtain estimates for Bethe roots corresponding to both ground states and excited states with up to 5 down-spins, for both the closed and open XXZ chains.","This approach is not limited to real Bethe roots."],"url":"http://arxiv.org/abs/2404.18244v1","category":"quant-ph"}
{"created":"2024-04-28 16:47:44","title":"The Second Picard iteration of NLS on the $2d$ sphere does not regularize Gaussian random initial data","abstract":"We consider the Wick ordered cubic Schr\\\"odinger equation (NLS) posed on the two-dimensional sphere, with initial data distributed according to a Gaussian measure. We show that the second Picard iteration does not improve the regularity of the initial data in the scale of the classical Sobolev spaces. This is in sharp contrast with the Wick ordered NLS on the two-dimensional tori, a model for which we know from the work of Bourgain that the second Picard iteration gains one half derivative. Our proof relies on identifying a singular part of the nonlinearity. We show that this singular part is responsible for a concentration phenomenon on a large circle (i.e. a stable closed geodesic), which prevents any regularization in the second Picard iteration.","sentences":["We consider the Wick ordered cubic Schr\\\"odinger equation (NLS) posed on the two-dimensional sphere, with initial data distributed according to a Gaussian measure.","We show that the second Picard iteration does not improve the regularity of the initial data in the scale of the classical Sobolev spaces.","This is in sharp contrast with the Wick ordered NLS on the two-dimensional tori, a model for which we know from the work of Bourgain that the second Picard iteration gains one half derivative.","Our proof relies on identifying a singular part of the nonlinearity.","We show that this singular part is responsible for a concentration phenomenon on a large circle (i.e. a stable closed geodesic), which prevents any regularization in the second Picard iteration."],"url":"http://arxiv.org/abs/2404.18241v1","category":"math.AP"}
{"created":"2024-04-28 15:48:44","title":"Probabilistic well-posedeness for the nonlinear Schr\u00f6dinger equation on the $2d$ sphere I: positive regularities","abstract":"We establish the probabilistic well-posedness of the nonlinear Schr\\\"odinger equation on the $2d$ sphere $\\mathbb{S}^{2}$. The initial data are distributed according to Gaussian measures with typical regularity $H^{s}(\\mathbb{S}^{2})$, for $s>0$. This level of regularity goes significantly beyond existing deterministic results, in a regime where the flow map cannot be extended uniformly continuously.","sentences":["We establish the probabilistic well-posedness of the nonlinear Schr\\\"odinger equation on the $2d$ sphere $\\mathbb{S}^{2}$. The initial data are distributed according to Gaussian measures with typical regularity $H^{s}(\\mathbb{S}^{2})$, for $s>0$. This level of regularity goes significantly beyond existing deterministic results, in a regime where the flow map cannot be extended uniformly continuously."],"url":"http://arxiv.org/abs/2404.18229v1","category":"math.AP"}
{"created":"2024-04-28 15:40:13","title":"Relational Lorentzian Asymptotically Safe Quantum Gravity: Showcase model","abstract":"In a recent contribution we identified possible points of contact between the asymptotically safe and canonical approach to quantum gravity. The idea is to start from the reduced phase space (often called relational) formulation of canonical quantum gravity which provides a reduced (or physical) Hamiltonian for the true (observable) degrees of freedom. The resulting reduced phase space is then canonically quantised and one can construct the generating functional of time ordered Wightman (i.e. Feynman) or Schwinger distributions respectively from the corresponding time translation unitary group or contraction semigroup respectively as a path integral. For the unitary choice that path integral can be rewritten in terms of the Lorentzian Einstein Hilbert action plus observable matter action and a ghost action. The ghost action depends on the Hilbert space representation chosen for the canonical quantisation and a reduction term that encodes the reduction of the full phase space to the phase space of observavbles. This path integral can then be treated with the methods of asymptically safe quantum gravity in its {\\it Lorentzian} version.   We also exemplified the procedure using a concrete, minimalistic example namely Einstein-Klein-Gordon theory with as many neutral and massless scalar fields as there are spacetime dimensions. However, no explicit calculations were performed. In this paper we fill in the missing steps. Particular care is needed due to the necessary switch to Lorentzian signature which has strong impact on the convergence of ``heat'' kernel time integrals in the heat kernel expansion of the trace involved in the Wetterich equation and which requires different cut-off functions than in the Euclidian version. As usual we truncate at relatively low order and derive and solve the resulting flow equations in that approximation.","sentences":["In a recent contribution we identified possible points of contact between the asymptotically safe and canonical approach to quantum gravity.","The idea is to start from the reduced phase space (often called relational) formulation of canonical quantum gravity which provides a reduced (or physical) Hamiltonian for the true (observable) degrees of freedom.","The resulting reduced phase space is then canonically quantised and one can construct the generating functional of time ordered Wightman (i.e. Feynman) or Schwinger distributions respectively from the corresponding time translation unitary group or contraction semigroup respectively as a path integral.","For the unitary choice that path integral can be rewritten in terms of the Lorentzian Einstein Hilbert action plus observable matter action and a ghost action.","The ghost action depends on the Hilbert space representation chosen for the canonical quantisation and a reduction term that encodes the reduction of the full phase space to the phase space of observavbles.","This path integral can then be treated with the methods of asymptically safe quantum gravity in its {\\it Lorentzian} version.   ","We also exemplified the procedure using a concrete, minimalistic example namely Einstein-Klein-Gordon theory with as many neutral and massless scalar fields as there are spacetime dimensions.","However, no explicit calculations were performed.","In this paper we fill in the missing steps.","Particular care is needed due to the necessary switch to Lorentzian signature which has strong impact on the convergence of ``heat'' kernel time integrals in the heat kernel expansion of the trace involved in the Wetterich equation and which requires different cut-off functions than in the Euclidian version.","As usual we truncate at relatively low order and derive and solve the resulting flow equations in that approximation."],"url":"http://arxiv.org/abs/2404.18224v1","category":"hep-th"}
{"created":"2024-04-28 15:22:24","title":"Exploring Transport Properties of Quark-Gluon Plasma with a Machine-Learning assisted Holographic Approach","abstract":"Based on the holographic model, which incorporates the equation of state (EoS) and baryon number susceptibility for different flavors, we calculate the drag force, jet quenching parameter, and diffusion coefficient of the heavy quark at finite temperature and chemical potential. The holographic results for the diffusion coefficient align with lattice data for $N_f = 0$ and $N_f = 2+1$, falling within their error margins. The holographic diffusion coefficient for heavy quark in the systems of different flavors agrees with the fitting results of ALICE data well. The jet quenching parameter in our model demonstrates strong consistency with the results from both RHIC and LHC for different flavors. By comparing the experimental data with other models, we can confirm the validity of our holographic model calculation. The work reinforces the potential of bottom-up holographic model in advancing our understanding of transport properties of hot and dense quark-gluon plasma.","sentences":["Based on the holographic model, which incorporates the equation of state (EoS) and baryon number susceptibility for different flavors, we calculate the drag force, jet quenching parameter, and diffusion coefficient of the heavy quark at finite temperature and chemical potential.","The holographic results for the diffusion coefficient align with lattice data for $N_f = 0$ and $N_f = 2+1$, falling within their error margins.","The holographic diffusion coefficient for heavy quark in the systems of different flavors agrees with the fitting results of ALICE data well.","The jet quenching parameter in our model demonstrates strong consistency with the results from both RHIC and LHC for different flavors.","By comparing the experimental data with other models, we can confirm the validity of our holographic model calculation.","The work reinforces the potential of bottom-up holographic model in advancing our understanding of transport properties of hot and dense quark-gluon plasma."],"url":"http://arxiv.org/abs/2404.18217v1","category":"hep-ph"}
{"created":"2024-04-28 14:59:46","title":"Testing for Asymmetric Information in Insurance with Deep Learning","abstract":"The positive correlation test for asymmetric information developed by Chiappori and Salanie (2000) has been applied in many insurance markets. Most of the literature focuses on the special case of constant correlation; it also relies on restrictive parametric specifications for the choice of coverage and the occurrence of claims. We relax these restrictions by estimating conditional covariances and correlations using deep learning methods. We test the positive correlation property by using the intersection test of Chernozhukov, Lee, and Rosen (2013) and the \"sorted groups\" test of Chernozhukov, Demirer, Duflo, and Fernandez-Val (2023). Our results confirm earlier findings that the correlation between risk and coverage is small. Random forests and gradient boosting trees produce similar results to neural networks.","sentences":["The positive correlation test for asymmetric information developed by Chiappori and Salanie (2000) has been applied in many insurance markets.","Most of the literature focuses on the special case of constant correlation; it also relies on restrictive parametric specifications for the choice of coverage and the occurrence of claims.","We relax these restrictions by estimating conditional covariances and correlations using deep learning methods.","We test the positive correlation property by using the intersection test of Chernozhukov, Lee, and Rosen (2013) and the \"sorted groups\" test of Chernozhukov, Demirer, Duflo, and Fernandez-Val (2023).","Our results confirm earlier findings that the correlation between risk and coverage is small.","Random forests and gradient boosting trees produce similar results to neural networks."],"url":"http://arxiv.org/abs/2404.18207v1","category":"econ.EM"}
{"created":"2024-04-28 13:28:57","title":"CConnect: Synergistic Convolutional Regularization for Cartesian T2* Mapping","abstract":"Magnetic resonance imaging (MRI) is fundamental for the assessment of many diseases, due to its excellent tissue contrast characterization. This is based on quantitative techniques, such as T1 , T2 , and T2* mapping. Quantitative MRI requires the acquisition of several contrast-weighed images followed by a fitting to an exponential model or dictionary matching, which results in undesirably long acquisition times. Undersampling reconstruction techniques are commonly employed to speed up the scan, with the drawback of introducing aliasing artifacts. However, most undersampling reconstruction techniques require long computational times or do not exploit redundancies across the different contrast-weighted images. This study introduces a new regularization technique to overcome aliasing artifacts, namely CConnect, which uses an innovative regularization term that leverages several trained convolutional neural networks (CNNs) to connect and exploit information across image contrasts in a latent space. We validate our method using in-vivo T2* mapping of the brain, with retrospective undersampling factors of 4, 5 and 6, demonstrating its effectiveness in improving reconstruction in comparison to state-of-the-art techniques. Comparisons against joint total variation, nuclear low rank and a deep learning (DL) de-aliasing post-processing method, with respect to structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR) metrics are presented.","sentences":["Magnetic resonance imaging (MRI) is fundamental for the assessment of many diseases, due to its excellent tissue contrast characterization.","This is based on quantitative techniques, such as T1 , T2 , and T2* mapping.","Quantitative MRI requires the acquisition of several contrast-weighed images followed by a fitting to an exponential model or dictionary matching, which results in undesirably long acquisition times.","Undersampling reconstruction techniques are commonly employed to speed up the scan, with the drawback of introducing aliasing artifacts.","However, most undersampling reconstruction techniques require long computational times or do not exploit redundancies across the different contrast-weighted images.","This study introduces a new regularization technique to overcome aliasing artifacts, namely CConnect, which uses an innovative regularization term that leverages several trained convolutional neural networks (CNNs) to connect and exploit information across image contrasts in a latent space.","We validate our method using in-vivo T2* mapping of the brain, with retrospective undersampling factors of 4, 5 and 6, demonstrating its effectiveness in improving reconstruction in comparison to state-of-the-art techniques.","Comparisons against joint total variation, nuclear low rank and a deep learning (DL) de-aliasing post-processing method, with respect to structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR) metrics are presented."],"url":"http://arxiv.org/abs/2404.18182v1","category":"physics.med-ph"}
{"created":"2024-04-28 13:26:53","title":"Learning to Move Objects with Fluid Streams in a Differentiable Simulation","abstract":"We introduce a method for manipulating objects in three-dimensional space using controlled fluid streams. To achieve this, we train a neural network controller in a differentiable simulation and evaluate it in a simulated environment consisting of an 8x8 grid of vertical emitters. By carrying out various horizontal displacement tasks such as moving objects to specific positions while reacting to external perturbations, we demonstrate that a controller, trained with a limited number of iterations, can generalise to longer episodes and learn the complex dynamics of fluid-solid interactions. Importantly, our approach requires only the observation of the manipulated object's state, paving the way for the development of physical systems that enable contactless manipulation of objects using air streams.","sentences":["We introduce a method for manipulating objects in three-dimensional space using controlled fluid streams.","To achieve this, we train a neural network controller in a differentiable simulation and evaluate it in a simulated environment consisting of an 8x8 grid of vertical emitters.","By carrying out various horizontal displacement tasks such as moving objects to specific positions while reacting to external perturbations, we demonstrate that a controller, trained with a limited number of iterations, can generalise to longer episodes and learn the complex dynamics of fluid-solid interactions.","Importantly, our approach requires only the observation of the manipulated object's state, paving the way for the development of physical systems that enable contactless manipulation of objects using air streams."],"url":"http://arxiv.org/abs/2404.18181v1","category":"cs.RO"}
{"created":"2024-04-28 12:05:23","title":"Llarull's theorem on odd dimensional manifolds: the noncompact case","abstract":"Let $(M,g^{TM})$ be an odd dimensional ($\\dim M\\geq 3$) connected oriented noncompact complete spin Riemannian manifold. Let $k^{TM}$ be the associated scalar curvature. Let $f:M\\to S^{\\dim M}(1)$ be a smooth area decreasing map which is locally constant near infinity and of nonzero degree. Suppose $k^{TM}\\geq ({\\dim M})({\\dim M}-1)$ on the support of ${\\rm d}f$, we show that $\\inf(k^{TM})<0$. This answers a question of Gromov.","sentences":["Let $(M,g^{TM})$ be an odd dimensional ($\\dim M\\geq 3$) connected oriented noncompact complete spin Riemannian manifold.","Let $k^{TM}$ be the associated scalar curvature.","Let $f:M\\to S^{\\dim M}(1)$ be a smooth area decreasing map which is locally constant near infinity and of nonzero degree.","Suppose $k^{TM}\\geq ({\\dim M})({\\dim M}-1)$ on the support of ${\\rm d}f$, we show that $\\inf(k^{TM})<0$. This answers a question of Gromov."],"url":"http://arxiv.org/abs/2404.18153v1","category":"math.DG"}
{"created":"2024-04-28 11:42:01","title":"The Cosmological Impact of Brane-Chern-Simons Massive Gravity","abstract":"In this paper, we present a novel extension of massive gravity theory; the Brane-Chern-Simons massive gravity theory. We explore the cosmological implications of this theory by deriving the background equations and demonstrating the existence of self-accelerating solutions. Interestingly, our theory suggests the existence of self-accelerating mechanisms that originate from an effective cosmological constant, leading to intriguing possibilities for understanding the nature of cosmic acceleration. Furthermore, we perform a tensor perturbation analysis to investigate the propagation of gravitational waves in this framework. We derive the dispersion relation for gravitational waves and study their behavior in the Friedmann-Lema\\^itre-Roberson-Waker cosmology within the context of Brane-Chen-Simons massive gravity. Utilizing the latest Union2 type Ia supernova dataset comprising $557$ SNIa events, we provide observational support for our theoretical framework, indicating that the Brane-Chern-Simons massive gravity theory is consistent with cosmological observations.","sentences":["In this paper, we present a novel extension of massive gravity theory; the Brane-Chern-Simons massive gravity theory.","We explore the cosmological implications of this theory by deriving the background equations and demonstrating the existence of self-accelerating solutions.","Interestingly, our theory suggests the existence of self-accelerating mechanisms that originate from an effective cosmological constant, leading to intriguing possibilities for understanding the nature of cosmic acceleration.","Furthermore, we perform a tensor perturbation analysis to investigate the propagation of gravitational waves in this framework.","We derive the dispersion relation for gravitational waves and study their behavior in the Friedmann-Lema\\^itre-Roberson-Waker cosmology within the context of Brane-Chen-Simons massive gravity.","Utilizing the latest Union2 type Ia supernova dataset comprising $557$ SNIa events, we provide observational support for our theoretical framework, indicating that the Brane-Chern-Simons massive gravity theory is consistent with cosmological observations."],"url":"http://arxiv.org/abs/2404.18147v1","category":"gr-qc"}
{"created":"2024-04-28 09:30:37","title":"An Arbitrarily High-Order Fully Well-balanced Hybrid Finite Element-Finite Volume Method for a One-dimensional Blood Flow Model","abstract":"In this paper, we propose an arbitrarily high-order accurate fully well-balanced numerical method for the one-dimensional blood flow model. The developed method is based on a continuous representation of the solution and a natural combination of the conservative and primitive formulations of the studied PDEs. The degrees of freedom are defined as point values at cell interfaces and moments of the conservative variables inside the cell, drawing inspiration from the discontinuous Galerkin method. The well-balanced property, in the sense of an exact preservation of both the zero and non-zero velocity equilibria, is achieved by a well-balanced approximation of the source term in the conservative formulation and a well-balanced residual computation in the primitive formulation. To lowest (3rd) order this method reduces to the method developed in [Abgrall and Liu, A New Approach for Designing Well-Balanced Schemes for the Shallow Water Equations: A Combination of Conservative and Primitive Formulations, arXiv preprint, arXiv:2304.07809]. Several numerical tests are shown to prove its well-balanced and high-order accuracy properties.","sentences":["In this paper, we propose an arbitrarily high-order accurate fully well-balanced numerical method for the one-dimensional blood flow model.","The developed method is based on a continuous representation of the solution and a natural combination of the conservative and primitive formulations of the studied PDEs.","The degrees of freedom are defined as point values at cell interfaces and moments of the conservative variables inside the cell, drawing inspiration from the discontinuous Galerkin method.","The well-balanced property, in the sense of an exact preservation of both the zero and non-zero velocity equilibria, is achieved by a well-balanced approximation of the source term in the conservative formulation and a well-balanced residual computation in the primitive formulation.","To lowest (3rd) order this method reduces to the method developed in [Abgrall and Liu, A New Approach for Designing Well-Balanced Schemes for the Shallow Water Equations: A Combination of Conservative and Primitive Formulations, arXiv preprint, arXiv:2304.07809].","Several numerical tests are shown to prove its well-balanced and high-order accuracy properties."],"url":"http://arxiv.org/abs/2404.18124v1","category":"math.NA"}
{"created":"2024-04-28 08:37:31","title":"Strong generalized holomorphic principal bundles","abstract":"We introduce the notion of a strong generalized holomorphic (SGH) fiber bundle and develop connection and curvature theory for an SGH principal $G$-bundle over a regular generalized complex (GC) manifold, where $G$ is a complex Lie group. We develop a de Rham cohomology for regular GC manifolds, and a Dolbeault cohomology for SGH vector bundles. Moreover, we establish a Chern-Weil theory for SGH principal $G$-bundles under certain mild assumptions on the leaf space of the GC structure. We also present a Hodge theory along with associated dualities and vanishing theorems for SGH vector bundles. Several examples of SGH fiber bundles are given.","sentences":["We introduce the notion of a strong generalized holomorphic (SGH) fiber bundle and develop connection and curvature theory for an SGH principal $G$-bundle over a regular generalized complex (GC) manifold, where $G$ is a complex Lie group.","We develop a de Rham cohomology for regular GC manifolds, and a Dolbeault cohomology for SGH vector bundles.","Moreover, we establish a Chern-Weil theory for SGH principal $G$-bundles under certain mild assumptions on the leaf space of the GC structure.","We also present a Hodge theory along with associated dualities and vanishing theorems for SGH vector bundles.","Several examples of SGH fiber bundles are given."],"url":"http://arxiv.org/abs/2404.18113v1","category":"math.DG"}
{"created":"2024-04-28 08:13:06","title":"Some three dimensional smooth transonic flows for the steady Euler equations with an external force","abstract":"We establish the existence and uniqueness of some smooth accelerating transonic flows governed by the steady compressible Euler equations with an external force in cylinders with arbitrary cross sections, which include both irrotational flows and Beltrami flows with nonuniform proportionality factors. One of the key ingredients in the analysis of smooth transonic irrotational flows is the well-posedness theory for a linear elliptic-hyperbolic mixed second order differential equation of Keldysh type in cylinders, which is achieved by extending the problem to an auxiliary linear elliptic-hyberbolic-elliptic mixed problem in a longer cylinder where the governing equation becomes elliptic at the exit of the new cylinder, so that one can use the multiplier method and the cut-off techniques to derive the $H^2$ and higher order estimates in transonic regions. It is further shown that the energy estimate can be closed in the $H^4$ framework. For smooth transonic Beltrami flows, we solve a transport equation for the proportionality factor and a type-changing enlarged deformation-curl system with mixed boundary conditions. The compatibility conditions for the $H^4$ estimate to the enlarged deformation-curl system near the intersection between the entrance and the cylinder wall play a crucial role in the analysis.","sentences":["We establish the existence and uniqueness of some smooth accelerating transonic flows governed by the steady compressible Euler equations with an external force in cylinders with arbitrary cross sections, which include both irrotational flows and Beltrami flows with nonuniform proportionality factors.","One of the key ingredients in the analysis of smooth transonic irrotational flows is the well-posedness theory for a linear elliptic-hyperbolic mixed second order differential equation of Keldysh type in cylinders, which is achieved by extending the problem to an auxiliary linear elliptic-hyberbolic-elliptic mixed problem in a longer cylinder where the governing equation becomes elliptic at the exit of the new cylinder, so that one can use the multiplier method and the cut-off techniques to derive the $H^2$ and higher order estimates in transonic regions.","It is further shown that the energy estimate can be closed in the $H^4$ framework.","For smooth transonic Beltrami flows, we solve a transport equation for the proportionality factor and a type-changing enlarged deformation-curl system with mixed boundary conditions.","The compatibility conditions for the $H^4$ estimate to the enlarged deformation-curl system near the intersection between the entrance and the cylinder wall play a crucial role in the analysis."],"url":"http://arxiv.org/abs/2404.18110v1","category":"math.AP"}
{"created":"2024-04-28 08:02:26","title":"Weak-strong uniqueness and high-friction limit for Euler-Riesz systems","abstract":"In this work we employ the relative energy method to obtain a weak-strong uniqueness principle for a Euler-Riesz system, as well as to establish its convergence in the high-friction limit towards a gradient flow equation. The main technical challenge in our analysis is addressed using a specific case of a Hardy-Littlewood-Sobolev inequality for Riesz potentials.","sentences":["In this work we employ the relative energy method to obtain a weak-strong uniqueness principle for a Euler-Riesz system, as well as to establish its convergence in the high-friction limit towards a gradient flow equation.","The main technical challenge in our analysis is addressed using a specific case of a Hardy-Littlewood-Sobolev inequality for Riesz potentials."],"url":"http://arxiv.org/abs/2404.18108v1","category":"math.AP"}
{"created":"2024-04-28 07:43:22","title":"Non-abelian symmetric critical gravitating vortices on a sphere","abstract":"We produce examples of solutions to the non-abelian gravitating vortex equations, which are a dimensional reduction of the K\\\"aher-Yang-Mills- Higgs equations. These are equations for a K\\\"ahler metric and a metric on a vector bundle. We consider a symmetric situation on a sphere with a relationship between the parameters involved (criticality), and perform a non-trivial reduction of the problem to a system of ordinary differential equations on the real line with complicated boundary conditions at infinity. This system involves a parameter whose dependence on the volume of the K\\\"ahler metric is non-explicit. We prove existence to this system using the method of continuity. We then prove that the parameter can be varied to make sure that all possible admissible volumes are attained.","sentences":["We produce examples of solutions to the non-abelian gravitating vortex equations, which are a dimensional reduction of the K\\\"aher-Yang-Mills- Higgs equations.","These are equations for a K\\\"ahler metric and a metric on a vector bundle.","We consider a symmetric situation on a sphere with a relationship between the parameters involved (criticality), and perform a non-trivial reduction of the problem to a system of ordinary differential equations on the real line with complicated boundary conditions at infinity.","This system involves a parameter whose dependence on the volume of the K\\\"ahler metric is non-explicit.","We prove existence to this system using the method of continuity.","We then prove that the parameter can be varied to make sure that all possible admissible volumes are attained."],"url":"http://arxiv.org/abs/2404.18103v1","category":"math.DG"}
{"created":"2024-04-28 07:39:18","title":"Quasi-interpolation projectors for Subdivision Surfaces","abstract":"Subdivision surfaces are considered as an extension of splines to accommodate models with complex topologies, making them useful for addressing PDEs on models with complex topologies in isogeometric analysis. This has generated a lot of interest in the field of subdivision space approximation. The quasi-interpolation offers a highly efficient approach for spline approximation, eliminating the necessity of solving large linear systems of equations. Nevertheless, the lack of analytical expressions at extraordinary points on subdivision surfaces makes traditional techniques for creating B-spline quasi-interpolants inappropriate for subdivision spaces. To address this obstacle, this paper innovatively reframes the evaluation issue associated with subdivision surfaces as a correlation between subdivision matrices and limit points, offering a thorough method for quasi-interpolation specifically designed for subdivision surfaces. This developed quasi-interpolant, termed the subdivision space projection operator, accurately reproduces the subdivision space. We provide explicit quasi-interpolation formulas for various typical subdivision schemes. Numerical experiments demonstrate that the quasi-interpolants for Catmull-Clark and Loop subdivision exhibit third-order approximation in the (L_2) norm and second-order in the (L_\\infty) norm. Furthermore, the modified Loop subdivision quasi-interpolant achieves optimal approximation rates in both the (L_2) and (L_\\infty) norms.","sentences":["Subdivision surfaces are considered as an extension of splines to accommodate models with complex topologies, making them useful for addressing PDEs on models with complex topologies in isogeometric analysis.","This has generated a lot of interest in the field of subdivision space approximation.","The quasi-interpolation offers a highly efficient approach for spline approximation, eliminating the necessity of solving large linear systems of equations.","Nevertheless, the lack of analytical expressions at extraordinary points on subdivision surfaces makes traditional techniques for creating B-spline quasi-interpolants inappropriate for subdivision spaces.","To address this obstacle, this paper innovatively reframes the evaluation issue associated with subdivision surfaces as a correlation between subdivision matrices and limit points, offering a thorough method for quasi-interpolation specifically designed for subdivision surfaces.","This developed quasi-interpolant, termed the subdivision space projection operator, accurately reproduces the subdivision space.","We provide explicit quasi-interpolation formulas for various typical subdivision schemes.","Numerical experiments demonstrate that the quasi-interpolants for Catmull-Clark and Loop subdivision exhibit third-order approximation in the (L_2) norm and second-order in the (L_\\infty) norm.","Furthermore, the modified Loop subdivision quasi-interpolant achieves optimal approximation rates in both the (L_2) and (L_\\infty) norms."],"url":"http://arxiv.org/abs/2404.18102v1","category":"math.NA"}
{"created":"2024-04-28 05:57:38","title":"Spectral Kernels and Holomorphic Morse Inequalities for Sequence of Line Bundles","abstract":"Given a sequence of Hermitian holomorphic line bundles $(L_k,h_k)$ over a complex manifold $M$ which may not be compact, we generalize the scaling method in arXiv:2310.08048 to study the asymptotic behavior of the Bergman kernels and spectral kernels with respect to the space of global holomorphic sections of $L_k$ with $(0,q)$-forms. We derive the leading term of the Bergman and spectral kernels under the local convergence assumption in the sequence of Chern curvatures $c_1(L_k,h_k)$, inspired by arXiv:2012.12019. The manifold $M$ may be non-K\\\"ahler and $c_1(L_k,h_k)$ may be negative or degenerate. Moreover, we establish the $L_k$-asymptotic version of Demailly's holomorphic Morse inequalities as an application to compact complex manifolds.","sentences":["Given a sequence of Hermitian holomorphic line bundles $(L_k,h_k)$ over a complex manifold $M$ which may not be compact, we generalize the scaling method in arXiv:2310.08048 to study the asymptotic behavior of the Bergman kernels and spectral kernels with respect to the space of global holomorphic sections of $L_k$ with $(0,q)$-forms.","We derive the leading term of the Bergman and spectral kernels under the local convergence assumption in the sequence of Chern curvatures $c_1(L_k,h_k)$, inspired by arXiv:2012.12019.","The manifold $M$ may be non-K\\\"ahler and $c_1(L_k,h_k)$ may be negative or degenerate.","Moreover, we establish the $L_k$-asymptotic version of Demailly's holomorphic Morse inequalities as an application to compact complex manifolds."],"url":"http://arxiv.org/abs/2404.18079v1","category":"math.CV"}
{"created":"2024-04-28 05:49:50","title":"On some linear equations associated with dispersionless integrable systems","abstract":"We use a recently proposed scheme of matrix extension of dispersionless integrable systems for the Abelian case, in which it leads to linear equations, connected with the initial dispersionless system. In the examples considered, these equations can be interpreted in terms of Abelian gauge fields on the geometric background defined by the dispersionless system. They are also connected with the linearisation of initial systems. We construct solutions to these linear equations in terms of wave functions of the Lax pair for dispersionless system, which is represented in terms of some vector fields.","sentences":["We use a recently proposed scheme of matrix extension of dispersionless integrable systems for the Abelian case, in which it leads to linear equations, connected with the initial dispersionless system.","In the examples considered, these equations can be interpreted in terms of Abelian gauge fields on the geometric background defined by the dispersionless system.","They are also connected with the linearisation of initial systems.","We construct solutions to these linear equations in terms of wave functions of the Lax pair for dispersionless system, which is represented in terms of some vector fields."],"url":"http://arxiv.org/abs/2404.18078v1","category":"nlin.SI"}
{"created":"2024-04-28 05:05:22","title":"Calabi-Yau metrics of Calabi type with polynomial rate of convergence","abstract":"We present new complete Calabi-Yau metrics defined on the complement of a smooth anticanonical divisor with ample normal bundle, approaching the Calabi model space at a polynomial rate. Moreover, we establish the uniqueness of this type of Calabi-Yau metric within a fixed cohomology class.","sentences":["We present new complete Calabi-Yau metrics defined on the complement of a smooth anticanonical divisor with ample normal bundle, approaching the Calabi model space at a polynomial rate.","Moreover, we establish the uniqueness of this type of Calabi-Yau metric within a fixed cohomology class."],"url":"http://arxiv.org/abs/2404.18070v1","category":"math.DG"}
{"created":"2024-04-28 04:59:08","title":"Local Discontinuous Galerkin method for fractional Korteweg-de Vries equation","abstract":"We propose a local discontinuous Galerkin (LDG) method for fractional Korteweg-de Vries equation involving the fractional Laplacian with exponent $\\alpha\\in (1,2)$ in one and two space dimensions. By decomposing the fractional Laplacian into a first order derivative and a fractional integral, we prove $L^2$-stability of the semi-discrete LDG scheme incorporating suitable interface and boundary fluxes. We analyze the error estimate by considering linear convection term and utilizing the estimate, we derive the error estimate for general nonlinear flux and demonstrate an order of convergence $\\mathcal{O}(h^{k+1/2})$. Moreover, the stability and error analysis have been extended to multiple space dimensional case. Additionally, we devise a fully discrete LDG scheme using the four-stage fourth-order Runge-Kutta method. We prove that the scheme is strongly stable under an appropriate time step constraint by establishing a \\emph{three-step strong stability} estimate. Numerical illustrations are shown to demonstrate the efficiency of the scheme by obtaining an optimal order of convergence.","sentences":["We propose a local discontinuous Galerkin (LDG) method for fractional Korteweg-de Vries equation involving the fractional Laplacian with exponent $\\alpha\\in (1,2)$ in one and two space dimensions.","By decomposing the fractional Laplacian into a first order derivative and a fractional integral, we prove $L^2$-stability of the semi-discrete LDG scheme incorporating suitable interface and boundary fluxes.","We analyze the error estimate by considering linear convection term and utilizing the estimate, we derive the error estimate for general nonlinear flux and demonstrate an order of convergence $\\mathcal{O}(h^{k+1/2})$. Moreover, the stability and error analysis have been extended to multiple space dimensional case.","Additionally, we devise a fully discrete LDG scheme using the four-stage fourth-order Runge-Kutta method.","We prove that the scheme is strongly stable under an appropriate time step constraint by establishing a \\emph{three-step strong stability} estimate.","Numerical illustrations are shown to demonstrate the efficiency of the scheme by obtaining an optimal order of convergence."],"url":"http://arxiv.org/abs/2404.18069v1","category":"math.NA"}
{"created":"2024-04-28 03:47:48","title":"Compressed Image Captioning using CNN-based Encoder-Decoder Framework","abstract":"In today's world, image processing plays a crucial role across various fields, from scientific research to industrial applications. But one particularly exciting application is image captioning. The potential impact of effective image captioning is vast. It can significantly boost the accuracy of search engines, making it easier to find relevant information. Moreover, it can greatly enhance accessibility for visually impaired individuals, providing them with a more immersive experience of digital content. However, despite its promise, image captioning presents several challenges. One major hurdle is extracting meaningful visual information from images and transforming it into coherent language. This requires bridging the gap between the visual and linguistic domains, a task that demands sophisticated algorithms and models. Our project is focused on addressing these challenges by developing an automatic image captioning architecture that combines the strengths of convolutional neural networks (CNNs) and encoder-decoder models. The CNN model is used to extract the visual features from images, and later, with the help of the encoder-decoder framework, captions are generated. We also did a performance comparison where we delved into the realm of pre-trained CNN models, experimenting with multiple architectures to understand their performance variations. In our quest for optimization, we also explored the integration of frequency regularization techniques to compress the \"AlexNet\" and \"EfficientNetB0\" model. We aimed to see if this compressed model could maintain its effectiveness in generating image captions while being more resource-efficient.","sentences":["In today's world, image processing plays a crucial role across various fields, from scientific research to industrial applications.","But one particularly exciting application is image captioning.","The potential impact of effective image captioning is vast.","It can significantly boost the accuracy of search engines, making it easier to find relevant information.","Moreover, it can greatly enhance accessibility for visually impaired individuals, providing them with a more immersive experience of digital content.","However, despite its promise, image captioning presents several challenges.","One major hurdle is extracting meaningful visual information from images and transforming it into coherent language.","This requires bridging the gap between the visual and linguistic domains, a task that demands sophisticated algorithms and models.","Our project is focused on addressing these challenges by developing an automatic image captioning architecture that combines the strengths of convolutional neural networks (CNNs) and encoder-decoder models.","The CNN model is used to extract the visual features from images, and later, with the help of the encoder-decoder framework, captions are generated.","We also did a performance comparison where we delved into the realm of pre-trained CNN models, experimenting with multiple architectures to understand their performance variations.","In our quest for optimization, we also explored the integration of frequency regularization techniques to compress the \"AlexNet\" and \"EfficientNetB0\" model.","We aimed to see if this compressed model could maintain its effectiveness in generating image captions while being more resource-efficient."],"url":"http://arxiv.org/abs/2404.18062v1","category":"cs.CV"}
{"created":"2024-04-28 03:11:44","title":"Joint Reference Frame Synthesis and Post Filter Enhancement for Versatile Video Coding","abstract":"This paper presents the joint reference frame synthesis (RFS) and post-processing filter enhancement (PFE) for Versatile Video Coding (VVC), aiming to explore the combination of different neural network-based video coding (NNVC) tools to better utilize the hierarchical bi-directional coding structure of VVC. Both RFS and PFE utilize the Space-Time Enhancement Network (STENet), which receives two input frames with artifacts and produces two enhanced frames with suppressed artifacts, along with an intermediate synthesized frame. STENet comprises two pipelines, the synthesis pipeline and the enhancement pipeline, tailored for different purposes. During RFS, two reconstructed frames are sent into STENet's synthesis pipeline to synthesize a virtual reference frame, similar to the current to-be-coded frame. The synthesized frame serves as an additional reference frame inserted into the reference picture list (RPL). During PFE, two reconstructed frames are fed into STENet's enhancement pipeline to alleviate their artifacts and distortions, resulting in enhanced frames with reduced artifacts and distortions. To reduce inference complexity, we propose joint inference of RFS and PFE (JISE), achieved through a single execution of STENet. Integrated into the VVC reference software VTM-15.0, RFS, PFE, and JISE are coordinated within a novel Space-Time Enhancement Window (STEW) under Random Access (RA) configuration. The proposed method could achieve -7.34%/-17.21%/-16.65% PSNR-based BD-rate on average for three components under RA configuration.","sentences":["This paper presents the joint reference frame synthesis (RFS) and post-processing filter enhancement (PFE) for Versatile Video Coding (VVC), aiming to explore the combination of different neural network-based video coding (NNVC) tools to better utilize the hierarchical bi-directional coding structure of VVC.","Both RFS and PFE utilize the Space-Time Enhancement Network (STENet), which receives two input frames with artifacts and produces two enhanced frames with suppressed artifacts, along with an intermediate synthesized frame.","STENet comprises two pipelines, the synthesis pipeline and the enhancement pipeline, tailored for different purposes.","During RFS, two reconstructed frames are sent into STENet's synthesis pipeline to synthesize a virtual reference frame, similar to the current to-be-coded frame.","The synthesized frame serves as an additional reference frame inserted into the reference picture list (RPL).","During PFE, two reconstructed frames are fed into STENet's enhancement pipeline to alleviate their artifacts and distortions, resulting in enhanced frames with reduced artifacts and distortions.","To reduce inference complexity, we propose joint inference of RFS and PFE (JISE), achieved through a single execution of STENet.","Integrated into the VVC reference software VTM-15.0, RFS, PFE, and JISE are coordinated within a novel Space-Time Enhancement Window (STEW) under Random Access (RA) configuration.","The proposed method could achieve -7.34%/-17.21%/-16.65% PSNR-based BD-rate on average for three components under RA configuration."],"url":"http://arxiv.org/abs/2404.18058v1","category":"eess.IV"}
{"created":"2024-04-28 03:09:37","title":"The rigidity of biconservative surfaces in Sol3","abstract":"We consider biconservative surfaces in Sol3, find their local equations, and then show that all biharmonic surfaces in this space are minimal.","sentences":["We consider biconservative surfaces in Sol3, find their local equations, and then show that all biharmonic surfaces in this space are minimal."],"url":"http://arxiv.org/abs/2404.18056v1","category":"math.DG"}
{"created":"2024-04-28 02:55:34","title":"Liouville type theorems for the 3D stationary MHD and Hall-MHD equations with non-zero constant vectors at infinity","abstract":"In this paper, we investigate Liouville type theorems for the three-dimensional steady-state MHD or Hall-MHD system under some asymptotic assumptions at infinity. Firstly, for the Hall-MHD system we obtain that $u$ and $B$ are constant vectors for any fluid viscosity, magnetic resistivity or Hall-coefficient when the magnetic field $B$ tends to a non-zero constant vector at infinity while the velocity field $u$ tends to $0$. Secondly, it also follows that $u$ and $B$ are constant for the Hall-MHD system when the velocity field tends to a constant vector at infinity while the magnetic field tends to $0$ without any assumptions on viscosity, magnetic resistivity or Hall-coefficient. One main difficulty lies in the Hall term, and we obtain the $L^p$ estimates of a generalized Oseen system with some supercritical terms via Lizorkin's theory and prove that the operator is stable by exploring Kato's stability theorem. Moreover, some similar results for the degenerate fluid viscosity or magnetic resistivity for the MHD system are also obtained, which is independent of interest.","sentences":["In this paper, we investigate Liouville type theorems for the three-dimensional steady-state MHD or Hall-MHD system under some asymptotic assumptions at infinity.","Firstly, for the Hall-MHD system we obtain that $u$ and $B$ are constant vectors for any fluid viscosity, magnetic resistivity or Hall-coefficient when the magnetic field $B$ tends to a non-zero constant vector at infinity while the velocity field $u$ tends to $0$. Secondly, it also follows that $u$ and $B$ are constant for the Hall-MHD system when the velocity field tends to a constant vector at infinity while the magnetic field tends to $0$ without any assumptions on viscosity, magnetic resistivity or Hall-coefficient.","One main difficulty lies in the Hall term, and we obtain the $L^p$ estimates of a generalized Oseen system with some supercritical terms via Lizorkin's theory and prove that the operator is stable by exploring Kato's stability theorem.","Moreover, some similar results for the degenerate fluid viscosity or magnetic resistivity for the MHD system are also obtained, which is independent of interest."],"url":"http://arxiv.org/abs/2404.18051v1","category":"math.AP"}
{"created":"2024-04-28 00:57:17","title":"Fashion Recommendation: Outfit Compatibility using GNN","abstract":"Numerous industries have benefited from the use of machine learning and fashion in industry is no exception. By gaining a better understanding of what makes a good outfit, companies can provide useful product recommendations to their users. In this project, we follow two existing approaches that employ graphs to represent outfits and use modified versions of the Graph neural network (GNN) frameworks. Both Node-wise Graph Neural Network (NGNN) and Hypergraph Neural Network aim to score a set of items according to the outfit compatibility of items. The data used is the Polyvore Dataset which consists of curated outfits with product images and text descriptions for each product in an outfit. We recreate the analysis on a subset of this data and compare the two existing models on their performance on two tasks Fill in the blank (FITB): finding an item that completes an outfit, and Compatibility prediction: estimating compatibility of different items grouped as an outfit. We can replicate the results directionally and find that HGNN does have a slightly better performance on both tasks. On top of replicating the results of the two papers we also tried to use embeddings generated from a vision transformer and witness enhanced prediction accuracy across the board","sentences":["Numerous industries have benefited from the use of machine learning and fashion in industry is no exception.","By gaining a better understanding of what makes a good outfit, companies can provide useful product recommendations to their users.","In this project, we follow two existing approaches that employ graphs to represent outfits and use modified versions of the Graph neural network (GNN) frameworks.","Both Node-wise Graph Neural Network (NGNN) and Hypergraph Neural Network aim to score a set of items according to the outfit compatibility of items.","The data used is the Polyvore Dataset which consists of curated outfits with product images and text descriptions for each product in an outfit.","We recreate the analysis on a subset of this data and compare the two existing models on their performance on two tasks Fill in the blank (FITB): finding an item that completes an outfit, and Compatibility prediction: estimating compatibility of different items grouped as an outfit.","We can replicate the results directionally and find that HGNN does have a slightly better performance on both tasks.","On top of replicating the results of the two papers we also tried to use embeddings generated from a vision transformer and witness enhanced prediction accuracy across the board"],"url":"http://arxiv.org/abs/2404.18040v1","category":"cs.CL"}
{"created":"2024-04-28 00:55:58","title":"Implicit Update of the Moment Equations for a Multi-Species, Homogeneous BGK Model","abstract":"A simple iterative approach for solving a set of implicit kinetic moment equations is proposed. This implicit solve is a key component in the IMEX discretization of the multi-species Bhatnagar-Gross-Krook (M-BGK) model with nontrivial collision frequencies depending on individual species temperatures. We prove that under mild time step restrictions, the iterative method generates a contraction mapping. Numerical simulations are provided to illustrate results of the IMEX scheme using the implicit moment solver.","sentences":["A simple iterative approach for solving a set of implicit kinetic moment equations is proposed.","This implicit solve is a key component in the IMEX discretization of the multi-species Bhatnagar-Gross-Krook (M-BGK) model with nontrivial collision frequencies depending on individual species temperatures.","We prove that under mild time step restrictions, the iterative method generates a contraction mapping.","Numerical simulations are provided to illustrate results of the IMEX scheme using the implicit moment solver."],"url":"http://arxiv.org/abs/2404.18039v1","category":"math.NA"}
{"created":"2024-04-28 00:50:48","title":"Comparison between a priori and a posteriori slope limiters for high-order finite volume schemes","abstract":"High-order finite volume and finite element methods offer impressive accuracy and cost efficiency when solving hyperbolic conservation laws with smooth solutions. However, if the solution contains discontinuities, these high-order methods can introduce unphysical oscillations and severe overshoots/undershoots. Slope limiters are an effective remedy, combating these oscillations by preserving monotonicity. Some limiters can even maintain a strict maximum principle in the numerical solution. They can be classified into one of two categories: \\textit{a priori} and \\textit{a posteriori} limiters. The former revises the high-order solution based only on data at the current time $t^n$, while the latter involves computing a candidate solution at $t^{n+1}$ and iteratively recomputing it until some conditions are satisfied. These two limiting paradigms are available for both finite volume and finite element methods.   In this work, we develop a methodology to compare \\textit{a priori} and \\textit{a posteriori} limiters for finite volume solvers at arbitrarily high order. We select the maximum principle preserving scheme presented in \\cite{zhang2011maximum, zhang2010maximum} as our \\textit{a priori} limited scheme. For \\textit{a posteriori} limiting, we adopt the methodology presented in \\cite{clain2011high} and search for so-called \\textit{troubled cells} in the candidate solution. We revise them with a robust MUSCL fallback scheme. The linear advection equation is solved in both one and two dimensions and we compare variations of these limited schemes based on their ability to maintain a maximum principle, solution quality over long time integration and computational cost.   ...","sentences":["High-order finite volume and finite element methods offer impressive accuracy and cost efficiency when solving hyperbolic conservation laws with smooth solutions.","However, if the solution contains discontinuities, these high-order methods can introduce unphysical oscillations and severe overshoots/undershoots.","Slope limiters are an effective remedy, combating these oscillations by preserving monotonicity.","Some limiters can even maintain a strict maximum principle in the numerical solution.","They can be classified into one of two categories: \\textit{a priori} and \\textit{a posteriori} limiters.","The former revises the high-order solution based only on data at the current time $t^n$, while the latter involves computing a candidate solution at $t^{n+1}$ and iteratively recomputing it until some conditions are satisfied.","These two limiting paradigms are available for both finite volume and finite element methods.   ","In this work, we develop a methodology to compare \\textit{a priori} and \\textit{a posteriori} limiters for finite volume solvers at arbitrarily high order.","We select the maximum principle preserving scheme presented in \\cite{zhang2011maximum, zhang2010maximum} as our \\textit{a priori} limited scheme.","For \\textit{a posteriori} limiting, we adopt the methodology presented in \\cite{clain2011high} and search for so-called \\textit{troubled cells} in the candidate solution.","We revise them with a robust MUSCL fallback scheme.","The linear advection equation is solved in both one and two dimensions and we compare variations of these limited schemes based on their ability to maintain a maximum principle, solution quality over long time integration and computational cost.   ..."],"url":"http://arxiv.org/abs/2404.18037v1","category":"math.NA"}
{"created":"2024-04-29 17:59:11","title":"Point Cloud Models Improve Visual Robustness in Robotic Learners","abstract":"Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies. Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts. Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training. Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners. Project Webpage: https://pvskand.github.io/projects/PCWM","sentences":["Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences.","In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies.","To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cloud World Model (PCWM) and point cloud based control policies.","Our experiments show that policies that explicitly encode point clouds are significantly more robust than their RGB-D counterparts.","Further, we find our proposed PCWM significantly outperforms prior works in terms of sample efficiency during training.","Taken together, these results suggest reasoning about the 3D scene through point clouds can improve performance, reduce learning time, and increase robustness for robotic learners.","Project Webpage: https://pvskand.github.io/projects/PCWM"],"url":"http://arxiv.org/abs/2404.18926v1","category":"cs.RO"}
{"created":"2024-04-29 15:19:25","title":"DNA Calorimetric Force Spectroscopy at Single Base Pair Resolution","abstract":"DNA hybridization is a fundamental reaction with wide-ranging applications in biotechnology. The nearest-neighbor (NN) model provides the most reliable description of the energetics of duplex formation. Most DNA thermodynamics studies have been done in melting experiments in bulk, of limited resolution due to ensemble averaging. In contrast, single-molecule methods have reached the maturity to derive DNA thermodynamics with unprecedented accuracy. We combine single-DNA mechanical unzipping experiments using a temperature jump optical trap with machine learning methods and derive the temperature-dependent DNA energy parameters of the NN model. In particular, we measure the previously unknown ten heat-capacity change parameters $\\Delta C_p$, relevant for thermodynamical predictions throughout the DNA stability range. Calorimetric force spectroscopy establishes a groundbreaking methodology to accurately study nucleic acids, from chemically modified DNA to RNA and DNA/RNA hybrid structures.","sentences":["DNA hybridization is a fundamental reaction with wide-ranging applications in biotechnology.","The nearest-neighbor (NN) model provides the most reliable description of the energetics of duplex formation.","Most DNA thermodynamics studies have been done in melting experiments in bulk, of limited resolution due to ensemble averaging.","In contrast, single-molecule methods have reached the maturity to derive DNA thermodynamics with unprecedented accuracy.","We combine single-DNA mechanical unzipping experiments using a temperature jump optical trap with machine learning methods and derive the temperature-dependent DNA energy parameters of the NN model.","In particular, we measure the previously unknown ten heat-capacity change parameters $\\Delta C_p$, relevant for thermodynamical predictions throughout the DNA stability range.","Calorimetric force spectroscopy establishes a groundbreaking methodology to accurately study nucleic acids, from chemically modified DNA to RNA and DNA/RNA hybrid structures."],"url":"http://arxiv.org/abs/2404.18785v1","category":"q-bio.BM"}
{"created":"2024-04-29 15:08:24","title":"A Universal Metric of Dataset Similarity for Cross-silo Federated Learning","abstract":"Federated Learning is increasingly used in domains such as healthcare to facilitate collaborative model training without data-sharing. However, datasets located in different sites are often non-identically distributed, leading to degradation of model performance in FL. Most existing methods for assessing these distribution shifts are limited by being dataset or task-specific. Moreover, these metrics can only be calculated by exchanging data, a practice restricted in many FL scenarios. To address these challenges, we propose a novel metric for assessing dataset similarity. Our metric exhibits several desirable properties for FL: it is dataset-agnostic, is calculated in a privacy-preserving manner, and is computationally efficient, requiring no model training. In this paper, we first establish a theoretical connection between our metric and training dynamics in FL. Next, we extensively evaluate our metric on a range of datasets including synthetic, benchmark, and medical imaging datasets. We demonstrate that our metric shows a robust and interpretable relationship with model performance and can be calculated in privacy-preserving manner. As the first federated dataset similarity metric, we believe this metric can better facilitate successful collaborations between sites.","sentences":["Federated Learning is increasingly used in domains such as healthcare to facilitate collaborative model training without data-sharing.","However, datasets located in different sites are often non-identically distributed, leading to degradation of model performance in FL.","Most existing methods for assessing these distribution shifts are limited by being dataset or task-specific.","Moreover, these metrics can only be calculated by exchanging data, a practice restricted in many FL scenarios.","To address these challenges, we propose a novel metric for assessing dataset similarity.","Our metric exhibits several desirable properties for FL: it is dataset-agnostic, is calculated in a privacy-preserving manner, and is computationally efficient, requiring no model training.","In this paper, we first establish a theoretical connection between our metric and training dynamics in FL.","Next, we extensively evaluate our metric on a range of datasets including synthetic, benchmark, and medical imaging datasets.","We demonstrate that our metric shows a robust and interpretable relationship with model performance and can be calculated in privacy-preserving manner.","As the first federated dataset similarity metric, we believe this metric can better facilitate successful collaborations between sites."],"url":"http://arxiv.org/abs/2404.18773v1","category":"cs.LG"}
{"created":"2024-04-29 14:12:33","title":"Exploring Chebyshev Polynomial Approximations: Error Estimates for Functions of Bounded Variation","abstract":"Approximation theory plays a central role in numerical analysis, undergoing continuous evolution through a spectrum of methodologies. Notably, Lebesgue, Weierstrass, Fourier, and Chebyshev approximations stand out among these methods. However, each technique possesses inherent limitations, underscoring the critical importance of selecting an appropriate approximation method tailored to specific problem domains. This article delves into the utilization of Chebyshev polynomials at Chebyshev nodes for approximation. For sufficiently smooth functions, the partial sum of Chebyshev series expansion offers optimal polynomial approximation, rendering it a preferred choice in various applications such as digital signal processing and graph filters due to its computational efficiency. In this article, we focus on functions of bounded variation, which find numerous applications across mathematical physics, hyperbolic conservations, and optimization. We present two optimal error estimations associated with Chebyshev polynomial approximations tailored for such functions. To validate our theoretical assertions, we conduct numerical experiments. Additionally, we delineate promising future avenues aligned with this research, particularly within the realms of machine learning and related fields.","sentences":["Approximation theory plays a central role in numerical analysis, undergoing continuous evolution through a spectrum of methodologies.","Notably, Lebesgue, Weierstrass, Fourier, and Chebyshev approximations stand out among these methods.","However, each technique possesses inherent limitations, underscoring the critical importance of selecting an appropriate approximation method tailored to specific problem domains.","This article delves into the utilization of Chebyshev polynomials at Chebyshev nodes for approximation.","For sufficiently smooth functions, the partial sum of Chebyshev series expansion offers optimal polynomial approximation, rendering it a preferred choice in various applications such as digital signal processing and graph filters due to its computational efficiency.","In this article, we focus on functions of bounded variation, which find numerous applications across mathematical physics, hyperbolic conservations, and optimization.","We present two optimal error estimations associated with Chebyshev polynomial approximations tailored for such functions.","To validate our theoretical assertions, we conduct numerical experiments.","Additionally, we delineate promising future avenues aligned with this research, particularly within the realms of machine learning and related fields."],"url":"http://arxiv.org/abs/2404.18723v1","category":"math.NA"}
{"created":"2024-04-29 09:00:32","title":"A Systematic Evaluation of Adversarial Attacks against Speech Emotion Recognition Models","abstract":"Speech emotion recognition (SER) is constantly gaining attention in recent years due to its potential applications in diverse fields and thanks to the possibility offered by deep learning technologies. However, recent studies have shown that deep learning models can be vulnerable to adversarial attacks. In this paper, we systematically assess this problem by examining the impact of various adversarial white-box and black-box attacks on different languages and genders within the context of SER. We first propose a suitable methodology for audio data processing, feature extraction, and CNN-LSTM architecture. The observed outcomes highlighted the significant vulnerability of CNN-LSTM models to adversarial examples (AEs). In fact, all the considered adversarial attacks are able to significantly reduce the performance of the constructed models. Furthermore, when assessing the efficacy of the attacks, minor differences were noted between the languages analyzed as well as between male and female speech. In summary, this work contributes to the understanding of the robustness of CNN-LSTM models, particularly in SER scenarios, and the impact of AEs. Interestingly, our findings serve as a baseline for a) developing more robust algorithms for SER, b) designing more effective attacks, c) investigating possible defenses, d) improved understanding of the vocal differences between different languages and genders, and e) overall, enhancing our comprehension of the SER task.","sentences":["Speech emotion recognition (SER) is constantly gaining attention in recent years due to its potential applications in diverse fields and thanks to the possibility offered by deep learning technologies.","However, recent studies have shown that deep learning models can be vulnerable to adversarial attacks.","In this paper, we systematically assess this problem by examining the impact of various adversarial white-box and black-box attacks on different languages and genders within the context of SER.","We first propose a suitable methodology for audio data processing, feature extraction, and CNN-LSTM architecture.","The observed outcomes highlighted the significant vulnerability of CNN-LSTM models to adversarial examples (AEs).","In fact, all the considered adversarial attacks are able to significantly reduce the performance of the constructed models.","Furthermore, when assessing the efficacy of the attacks, minor differences were noted between the languages analyzed as well as between male and female speech.","In summary, this work contributes to the understanding of the robustness of CNN-LSTM models, particularly in SER scenarios, and the impact of AEs.","Interestingly, our findings serve as a baseline for a) developing more robust algorithms for SER, b) designing more effective attacks, c) investigating possible defenses, d) improved understanding of the vocal differences between different languages and genders, and e) overall, enhancing our comprehension of the SER task."],"url":"http://arxiv.org/abs/2404.18514v1","category":"cs.SD"}
{"created":"2024-04-29 08:52:52","title":"Explainability of Machine Learning Approaches in Forensic Linguistics: A Case Study in Geolinguistic Authorship Profiling","abstract":"Forensic authorship profiling uses linguistic markers to infer characteristics about an author of a text. This task is paralleled in dialect classification, where a prediction is made about the linguistic variety of a text based on the text itself. While there have been significant advances in the last years in variety classification (Jauhiainen et al., 2019) and state-of-the-art approaches reach accuracies of up to 100% depending on the similarity of varieties and the scope of prediction (e.g., Milne et al., 2012; Blodgett et al., 2017), forensic linguistics rarely relies on these approaches due to their lack of transparency (see Nini, 2023), amongst other reasons. In this paper we therefore explore explainability of machine learning approaches considering the forensic context. We focus on variety classification as a means of geolinguistic profiling of unknown texts. For this we work with an approach proposed by Xie et al. (2024) to extract the lexical items most relevant to the variety classifications. We find that the extracted lexical features are indeed representative of their respective varieties and note that the trained models also rely on place names for classifications.","sentences":["Forensic authorship profiling uses linguistic markers to infer characteristics about an author of a text.","This task is paralleled in dialect classification, where a prediction is made about the linguistic variety of a text based on the text itself.","While there have been significant advances in the last years in variety classification (Jauhiainen et al., 2019) and state-of-the-art approaches reach accuracies of up to 100% depending on the similarity of varieties and the scope of prediction (e.g., Milne et al., 2012; Blodgett et al., 2017), forensic linguistics rarely relies on these approaches due to their lack of transparency (see Nini, 2023), amongst other reasons.","In this paper we therefore explore explainability of machine learning approaches considering the forensic context.","We focus on variety classification as a means of geolinguistic profiling of unknown texts.","For this we work with an approach proposed by Xie et al. (2024) to extract the lexical items most relevant to the variety classifications.","We find that the extracted lexical features are indeed representative of their respective varieties and note that the trained models also rely on place names for classifications."],"url":"http://arxiv.org/abs/2404.18510v1","category":"cs.CL"}
{"created":"2024-04-29 07:07:58","title":"HFT: Half Fine-Tuning for Large Language Models","abstract":"Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences. However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data. In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge. Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge. We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term. Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks. Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time.","sentences":["Large language models (LLMs) with one or more fine-tuning phases have become a necessary step to unlock various capabilities, enabling LLMs to follow natural language instructions or align with human preferences.","However, it carries the risk of catastrophic forgetting during sequential training, the parametric knowledge or the ability learned in previous stages may be overwhelmed by incoming training data.","In this paper, we find that by regularly resetting partial parameters, LLMs can restore some of the original knowledge.","Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge.","We provide a feasibility analysis from the perspective of optimization and interpret the parameter selection operation as a regularization term.","Without changing the model architecture, HFT could be seamlessly integrated into existing fine-tuning frameworks.","Extensive experiments and analysis on supervised fine-tuning, direct preference optimization, and continual learning consistently demonstrate the effectiveness, robustness, and efficiency of HFT.","Compared with FFT, HFT not only significantly alleviates the forgetting problem, but also achieves the best performance in a series of downstream benchmarks, with an approximately 30% reduction in training time."],"url":"http://arxiv.org/abs/2404.18466v1","category":"cs.CL"}
{"created":"2024-04-29 06:46:25","title":"Self-supervised contrastive learning of radio data for source detection, classification and peculiar object discovery","abstract":"New advancements in radio data post-processing are underway within the SKA precursor community, aiming to facilitate the extraction of scientific results from survey images through a semi-automated approach. Several of these developments leverage deep learning (DL) methodologies for diverse tasks, including source detection, object or morphology classification, and anomaly detection. Despite substantial progress, the full potential of these methods often remains untapped due to challenges associated with training large supervised models, particularly in the presence of small and class-unbalanced labelled datasets. Self-supervised learning has recently established itself as a powerful methodology to deal with some of the aforementioned challenges, by directly learning a lower-dimensional representation from large samples of unlabelled data. The resulting model and data representation can then be used for data inspection and various downstream tasks if a small subset of labelled data is available. In this work, we explored contrastive learning methods to learn suitable radio data representation from unlabelled images taken from the ASKAP EMU and SARAO MeerKAT GPS surveys. We evaluated trained models and the obtained data representation over smaller labelled datasets, also taken from different radio surveys, in selected analysis tasks: source detection and classification, and search for objects with peculiar morphology. For all explored downstream tasks, we reported and discussed the benefits brought by self-supervised foundational models built on radio data.","sentences":["New advancements in radio data post-processing are underway within the SKA precursor community, aiming to facilitate the extraction of scientific results from survey images through a semi-automated approach.","Several of these developments leverage deep learning (DL) methodologies for diverse tasks, including source detection, object or morphology classification, and anomaly detection.","Despite substantial progress, the full potential of these methods often remains untapped due to challenges associated with training large supervised models, particularly in the presence of small and class-unbalanced labelled datasets.","Self-supervised learning has recently established itself as a powerful methodology to deal with some of the aforementioned challenges, by directly learning a lower-dimensional representation from large samples of unlabelled data.","The resulting model and data representation can then be used for data inspection and various downstream tasks if a small subset of labelled data is available.","In this work, we explored contrastive learning methods to learn suitable radio data representation from unlabelled images taken from the ASKAP EMU and SARAO MeerKAT GPS surveys.","We evaluated trained models and the obtained data representation over smaller labelled datasets, also taken from different radio surveys, in selected analysis tasks: source detection and classification, and search for objects with peculiar morphology.","For all explored downstream tasks, we reported and discussed the benefits brought by self-supervised foundational models built on radio data."],"url":"http://arxiv.org/abs/2404.18462v1","category":"astro-ph.IM"}
{"created":"2024-04-29 03:52:56","title":"Low surface brightness galaxies from BASS+MzLS with Machine Learning","abstract":"From $\\sim$ 5000 deg$^{2}$ of the combination of the Beijing-Arizona Sky Survey (BASS) and Mayall $z$-band Legacy Survey (MzLS) which is also the northern sky region of the Dark Energy Spectroscopic Instrument (DESI) Legacy Imaging Surveys, we selected a sample of 31,825 candidates of low surface brightness galaxies (LSBGs) with the mean effective surface brightness 24.2 $< \\bar{\\mu}_{\\rm eff,g} <$ 28.8 mag arcsec$^{\\rm -2}$ and the half-light radius 2.5$^{\\prime\\prime}$ $< r_{\\rm eff} <$ 20$^{\\prime\\prime}$ based on the released photometric catalogue and the machine learning model. The distribution of the LSBGs is of bimodality in the $g$ - $r$ color, indicating the two distinct populations of the blue ($g$ - $r <$ 0.60) and the red ($g$ - $r >$ 0.60) LSBGs. The blue LSBGs appear spiral, disk or irregular while the red LSBGs are spheroidal or ellipitcal and spatially clustered. This trend shows that the color has a strong correlation with galaxy morphology for LSBGs. In the spatial distribution, the blue LSBGs are more uniformly distributed while the red ones are highly clustered, indicating that red LSBGs preferentially populated denser environment than the blue LSBGs. Besides, both populations have consistent distribution of ellipticity (median $\\epsilon\\sim$ 0.3), half-light radius (median $r_{\\rm eff} \\sim$ 4$^{\\prime\\prime}$), and Sersic index (median $n$ = 1), implying the dominance of the full sample by the round and disk galaxies. This sample has definitely extended the studies of LSBGs to a regime of lower surface brightness, fainter magnitude, and broader other properties than the previously SDSS-based samples.","sentences":["From $\\sim$ 5000 deg$^{2}$ of the combination of the Beijing-Arizona Sky Survey (BASS) and Mayall $z$-band Legacy Survey (MzLS) which is also the northern sky region of the Dark Energy Spectroscopic Instrument (DESI)","Legacy Imaging Surveys, we selected a sample of 31,825 candidates of low surface brightness galaxies (LSBGs) with the mean effective surface brightness 24.2 $< \\bar{\\mu}_{\\rm eff,g} <$ 28.8 mag arcsec$^{\\rm -2}$ and the half-light radius 2.5$^{\\prime\\prime}$","$< r_{\\rm eff} <$ 20$^{\\prime\\prime}$ based on the released photometric catalogue and the machine learning model.","The distribution of the LSBGs is of bimodality in the $g$ - $r$ color, indicating the two distinct populations of the blue ($g$ - $r <$ 0.60) and the red ($g$ - $r >$ 0.60) LSBGs.","The blue LSBGs appear spiral, disk or irregular while the red LSBGs are spheroidal or ellipitcal and spatially clustered.","This trend shows that the color has a strong correlation with galaxy morphology for LSBGs.","In the spatial distribution, the blue LSBGs are more uniformly distributed while the red ones are highly clustered, indicating that red LSBGs preferentially populated denser environment than the blue LSBGs.","Besides, both populations have consistent distribution of ellipticity (median $\\epsilon\\sim$ 0.3), half-light radius (median $r_{\\rm eff} \\sim$ 4$^{\\prime\\prime}$), and Sersic index (median $n$ = 1), implying the dominance of the full sample by the round and disk galaxies.","This sample has definitely extended the studies of LSBGs to a regime of lower surface brightness, fainter magnitude, and broader other properties than the previously SDSS-based samples."],"url":"http://arxiv.org/abs/2404.18408v2","category":"astro-ph.GA"}
{"created":"2024-04-29 03:19:39","title":"MM-TTS: A Unified Framework for Multimodal, Prompt-Induced Emotional Text-to-Speech Synthesis","abstract":"Emotional Text-to-Speech (E-TTS) synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction. However, current E-TTS approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs. To address these limitations, we propose the Multimodal Emotional Text-to-Speech System (MM-TTS), a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech. MM-TTS consists of two key components: (1) the Emotion Prompt Alignment Module (EP-Align), which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information; and (2) the Emotion Embedding-Induced TTS (EMI-TTS), which integrates the aligned emotional embeddings with state-of-the-art TTS models to synthesize speech that accurately reflects the intended emotions. Extensive evaluations across diverse datasets demonstrate the superior performance of MM-TTS compared to traditional E-TTS models. Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with MM-TTS achieving scores of 7.35% and 3.07%, respectively. Subjective assessments further validate that MM-TTS generates speech with emotional fidelity and naturalness comparable to human speech. Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214","sentences":["Emotional Text-to-Speech (E-TTS) synthesis has gained significant attention in recent years due to its potential to enhance human-computer interaction.","However, current E-TTS approaches often struggle to capture the complexity of human emotions, primarily relying on oversimplified emotional labels or single-modality inputs.","To address these limitations, we propose the Multimodal Emotional Text-to-Speech System (MM-TTS), a unified framework that leverages emotional cues from multiple modalities to generate highly expressive and emotionally resonant speech.","MM-TTS consists of two key components: (1) the Emotion Prompt Alignment Module (EP-Align), which employs contrastive learning to align emotional features across text, audio, and visual modalities, ensuring a coherent fusion of multimodal information; and (2) the Emotion Embedding-Induced TTS (EMI-TTS), which integrates the aligned emotional embeddings with state-of-the-art TTS models to synthesize speech that accurately reflects the intended emotions.","Extensive evaluations across diverse datasets demonstrate the superior performance of MM-TTS compared to traditional E-TTS models.","Objective metrics, including Word Error Rate (WER) and Character Error Rate (CER), show significant improvements on ESD dataset, with MM-TTS achieving scores of 7.35% and 3.07%, respectively.","Subjective assessments further validate that MM-TTS generates speech with emotional fidelity and naturalness comparable to human speech.","Our code and pre-trained models are publicly available at https://anonymous.4open.science/r/MMTTS-D214"],"url":"http://arxiv.org/abs/2404.18398v1","category":"cs.CL"}
{"created":"2024-04-29 03:13:09","title":"Reconstructing Satellites in 3D from Amateur Telescope Images","abstract":"This paper proposes a framework for the 3D reconstruction of satellites in low-Earth orbit, utilizing videos captured by small amateur telescopes. The video data obtained from these telescopes differ significantly from data for standard 3D reconstruction tasks, characterized by intense motion blur, atmospheric turbulence, pervasive background light pollution, extended focal length and constrained observational perspectives. To address these challenges, our approach begins with a comprehensive pre-processing workflow that encompasses deep learning-based image restoration, feature point extraction and camera pose initialization. We proceed with the application of an improved 3D Gaussian splatting algorithm for reconstructing the 3D model. Our technique supports simultaneous 3D Gaussian training and pose estimation, enabling the robust generation of intricate 3D point clouds from sparse, noisy data. The procedure is further bolstered by a post-editing phase designed to eliminate noise points inconsistent with our prior knowledge of a satellite's geometric constraints. We validate our approach using both synthetic datasets and actual observations of China's Space Station, showcasing its significant advantages over existing methods in reconstructing 3D space objects from ground-based observations.","sentences":["This paper proposes a framework for the 3D reconstruction of satellites in low-Earth orbit, utilizing videos captured by small amateur telescopes.","The video data obtained from these telescopes differ significantly from data for standard 3D reconstruction tasks, characterized by intense motion blur, atmospheric turbulence, pervasive background light pollution, extended focal length and constrained observational perspectives.","To address these challenges, our approach begins with a comprehensive pre-processing workflow that encompasses deep learning-based image restoration, feature point extraction and camera pose initialization.","We proceed with the application of an improved 3D Gaussian splatting algorithm for reconstructing the 3D model.","Our technique supports simultaneous 3D Gaussian training and pose estimation, enabling the robust generation of intricate 3D point clouds from sparse, noisy data.","The procedure is further bolstered by a post-editing phase designed to eliminate noise points inconsistent with our prior knowledge of a satellite's geometric constraints.","We validate our approach using both synthetic datasets and actual observations of China's Space Station, showcasing its significant advantages over existing methods in reconstructing 3D space objects from ground-based observations."],"url":"http://arxiv.org/abs/2404.18394v1","category":"cs.CV"}
{"created":"2024-04-29 02:42:02","title":"A Framework for Learning and Reusing Robotic Skills","abstract":"In this paper, we present our work in progress towards creating a library of motion primitives. This library facilitates easier and more intuitive learning and reusing of robotic skills. Users can teach robots complex skills through Learning from Demonstration, which is automatically segmented into primitives and stored in clusters of similar skills. We propose a novel multimodal segmentation method as well as a novel trajectory clustering method. Then, when needed for reuse, we transform primitives into new environments using trajectory editing. We present simulated results for our framework with demonstrations taken on real-world robots.","sentences":["In this paper, we present our work in progress towards creating a library of motion primitives.","This library facilitates easier and more intuitive learning and reusing of robotic skills.","Users can teach robots complex skills through Learning from Demonstration, which is automatically segmented into primitives and stored in clusters of similar skills.","We propose a novel multimodal segmentation method as well as a novel trajectory clustering method.","Then, when needed for reuse, we transform primitives into new environments using trajectory editing.","We present simulated results for our framework with demonstrations taken on real-world robots."],"url":"http://arxiv.org/abs/2404.18383v1","category":"cs.RO"}
{"created":"2024-04-28 21:53:42","title":"MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion Recognition","abstract":"This paper presents a novel approach to processing multimodal data for dynamic emotion recognition, named as the Multimodal Masked Autoencoder for Dynamic Emotion Recognition (MultiMAE-DER). The MultiMAE-DER leverages the closely correlated representation information within spatiotemporal sequences across visual and audio modalities. By utilizing a pre-trained masked autoencoder model, the MultiMAEDER is accomplished through simple, straightforward finetuning. The performance of the MultiMAE-DER is enhanced by optimizing six fusion strategies for multimodal input sequences. These strategies address dynamic feature correlations within cross-domain data across spatial, temporal, and spatiotemporal sequences. In comparison to state-of-the-art multimodal supervised learning models for dynamic emotion recognition, MultiMAE-DER enhances the weighted average recall (WAR) by 4.41% on the RAVDESS dataset and by 2.06% on the CREMAD. Furthermore, when compared with the state-of-the-art model of multimodal self-supervised learning, MultiMAE-DER achieves a 1.86% higher WAR on the IEMOCAP dataset.","sentences":["This paper presents a novel approach to processing multimodal data for dynamic emotion recognition, named as the Multimodal Masked Autoencoder for Dynamic Emotion Recognition (MultiMAE-DER).","The MultiMAE-DER leverages the closely correlated representation information within spatiotemporal sequences across visual and audio modalities.","By utilizing a pre-trained masked autoencoder model, the MultiMAEDER is accomplished through simple, straightforward finetuning.","The performance of the MultiMAE-DER is enhanced by optimizing six fusion strategies for multimodal input sequences.","These strategies address dynamic feature correlations within cross-domain data across spatial, temporal, and spatiotemporal sequences.","In comparison to state-of-the-art multimodal supervised learning models for dynamic emotion recognition, MultiMAE-DER enhances the weighted average recall (WAR) by 4.41% on the RAVDESS dataset and by 2.06% on the CREMAD.","Furthermore, when compared with the state-of-the-art model of multimodal self-supervised learning, MultiMAE-DER achieves a 1.86% higher WAR on the IEMOCAP dataset."],"url":"http://arxiv.org/abs/2404.18327v1","category":"cs.CV"}
{"created":"2024-04-28 21:36:40","title":"Flow-based Nonperturbative Simulation of First-order Phase Transitions","abstract":"We present a flow-based method for simulating and calculating nucleation rates of first-order phase transitions in scalar field theory on a lattice. Motivated by recent advancements in machine learning tools, particularly normalizing flows for lattice field theory, we propose the ``partitioning flow-based Markov chain Monte Carlo (PFMCMC) sampling\" method to address two challenges encountered in normalizing flow applications for lattice field theory: the ``mode-collapse\" and ``rare-event sampling\" problems. Using a (2+1)-dimensional real scalar model as an example, we demonstrate the effectiveness of our PFMCMC method in modeling highly hierarchical order parameter probability distributions and simulating critical bubble configurations. These simulations are then used to facilitate the calculation of nucleation rates. We anticipate the application of this method to (3+1)-dimensional theories for studying realistic cosmological phase transitions.","sentences":["We present a flow-based method for simulating and calculating nucleation rates of first-order phase transitions in scalar field theory on a lattice.","Motivated by recent advancements in machine learning tools, particularly normalizing flows for lattice field theory, we propose the ``partitioning flow-based Markov chain Monte Carlo (PFMCMC) sampling\" method to address two challenges encountered in normalizing flow applications for lattice field theory: the ``mode-collapse\" and ``rare-event sampling\" problems.","Using a (2+1)-dimensional real scalar model as an example, we demonstrate the effectiveness of our PFMCMC method in modeling highly hierarchical order parameter probability distributions and simulating critical bubble configurations.","These simulations are then used to facilitate the calculation of nucleation rates.","We anticipate the application of this method to (3+1)-dimensional theories for studying realistic cosmological phase transitions."],"url":"http://arxiv.org/abs/2404.18323v1","category":"hep-lat"}
{"created":"2024-04-28 19:41:40","title":"Quantum-enhanced learning with a controllable bosonic variational sensor network","abstract":"The emergence of quantum sensor networks has presented opportunities for enhancing complex sensing tasks, while simultaneously introducing significant challenges in designing and analyzing quantum sensing protocols due to the intricate nature of entanglement and physical processes. Supervised learning assisted by an entangled sensor network (SLAEN) [Phys. Rev. X 9, 041023 (2019)] represents a promising paradigm for automating sensor-network design through variational quantum machine learning. However, the original SLAEN, constrained by the Gaussian nature of quantum circuits, is limited to learning linearly separable data. Leveraging the universal quantum control available in cavity-QED experiments, we propose a generalized SLAEN capable of handling nonlinear data classification tasks. We establish a theoretical framework for physical-layer data classification to underpin our approach. Through training quantum probes and measurements, we uncover a threshold phenomenon in classification error across various tasks -- when the energy of probes exceeds a certain threshold, the error drastically diminishes to zero, providing a significant improvement over the Gaussian SLAEN. Despite the non-Gaussian nature of the problem, we offer analytical insights into determining the threshold and residual error in the presence of noise. Our findings carry implications for radio-frequency photonic sensors and microwave dark matter haloscopes.","sentences":["The emergence of quantum sensor networks has presented opportunities for enhancing complex sensing tasks, while simultaneously introducing significant challenges in designing and analyzing quantum sensing protocols due to the intricate nature of entanglement and physical processes.","Supervised learning assisted by an entangled sensor network (SLAEN)","[Phys. Rev. X 9, 041023 (2019)] represents a promising paradigm for automating sensor-network design through variational quantum machine learning.","However, the original SLAEN, constrained by the Gaussian nature of quantum circuits, is limited to learning linearly separable data.","Leveraging the universal quantum control available in cavity-QED experiments, we propose a generalized SLAEN capable of handling nonlinear data classification tasks.","We establish a theoretical framework for physical-layer data classification to underpin our approach.","Through training quantum probes and measurements, we uncover a threshold phenomenon in classification error across various tasks -- when the energy of probes exceeds a certain threshold, the error drastically diminishes to zero, providing a significant improvement over the Gaussian SLAEN.","Despite the non-Gaussian nature of the problem, we offer analytical insights into determining the threshold and residual error in the presence of noise.","Our findings carry implications for radio-frequency photonic sensors and microwave dark matter haloscopes."],"url":"http://arxiv.org/abs/2404.18293v1","category":"quant-ph"}
{"created":"2024-04-28 19:24:58","title":"Joint Energy and Latency Optimization in Federated Learning over Cell-Free Massive MIMO Networks","abstract":"Federated learning (FL) is a distributed learning paradigm wherein users exchange FL models with a server instead of raw datasets, thereby preserving data privacy and reducing communication overhead. However, the increased number of FL users may hinder completing large-scale FL over wireless networks due to high imposed latency. Cell-free massive multiple-input multiple-output~(CFmMIMO) is a promising architecture for implementing FL because it serves many users on the same time/frequency resources. While CFmMIMO enhances energy efficiency through spatial multiplexing and collaborative beamforming, it remains crucial to meticulously allocate uplink transmission powers to the FL users. In this paper, we propose an uplink power allocation scheme in FL over CFmMIMO by considering the effect of each user's power on the energy and latency of other users to jointly minimize the users' uplink energy and the latency of FL training. The proposed solution algorithm is based on the coordinate gradient descent method. Numerical results show that our proposed method outperforms the well-known max-sum rate by increasing up to~$27$\\% and max-min energy efficiency of the Dinkelbach method by increasing up to~$21$\\% in terms of test accuracy while having limited uplink energy and latency budget for FL over CFmMIMO.","sentences":["Federated learning (FL) is a distributed learning paradigm wherein users exchange FL models with a server instead of raw datasets, thereby preserving data privacy and reducing communication overhead.","However, the increased number of FL users may hinder completing large-scale FL over wireless networks due to high imposed latency.","Cell-free massive multiple-input multiple-output~(CFmMIMO) is a promising architecture for implementing FL because it serves many users on the same time/frequency resources.","While CFmMIMO enhances energy efficiency through spatial multiplexing and collaborative beamforming, it remains crucial to meticulously allocate uplink transmission powers to the FL users.","In this paper, we propose an uplink power allocation scheme in FL over CFmMIMO by considering the effect of each user's power on the energy and latency of other users to jointly minimize the users' uplink energy and the latency of FL training.","The proposed solution algorithm is based on the coordinate gradient descent method.","Numerical results show that our proposed method outperforms the well-known max-sum rate by increasing up to~$27$\\% and max-min energy efficiency of the Dinkelbach method by increasing up to~$21$\\% in terms of test accuracy while having limited uplink energy and latency budget for FL over CFmMIMO."],"url":"http://arxiv.org/abs/2404.18287v1","category":"cs.LG"}
{"created":"2024-04-28 19:00:13","title":"Fast \\textit{ab initio} design of high-entropy magnetic thin films","abstract":"We show that the magnetic properties of high-entropy alloys (HEAs) can be captured by \\textit{ab initio} calculations within the coherent potential approximation, where the atomic details of the high-entropy mixing are considered as an effective medium that possesses the translational symmetry of the lattice. This is demonstrated using the face-centered cubic (FCC) phase of $\\textrm{FeCoNiMnCu}$ and the $L1_0$ phase of $\\textrm{(FeCoNiMnCu)Pt}$ by comparing the density functional theory (DFT) results with the experimental values. Working within the first Brillouin zone and the primitive unit cell, we show that DFT can capture the smooth profile of magnetic properties such as the saturation magnetization, the Curie temperature and the magnetic anisotropy, using only a sparse set of sampling points in the vast compositional space. The smooth profiles given by DFT indeed follow the experimental trend, demonstrating the promising potential of using machine learning to explore the magnetic properties of HEAs, by establishing reasonably large datasets with high-throughput calculations using density-functional theory.","sentences":["We show that the magnetic properties of high-entropy alloys (HEAs) can be captured by \\textit{ab initio} calculations within the coherent potential approximation, where the atomic details of the high-entropy mixing are considered as an effective medium that possesses the translational symmetry of the lattice.","This is demonstrated using the face-centered cubic (FCC) phase of $\\textrm{FeCoNiMnCu}$ and the $L1_0$ phase of $\\textrm{(FeCoNiMnCu)Pt}$ by comparing the density functional theory (DFT) results with the experimental values.","Working within the first Brillouin zone and the primitive unit cell, we show that DFT can capture the smooth profile of magnetic properties such as the saturation magnetization, the Curie temperature and the magnetic anisotropy, using only a sparse set of sampling points in the vast compositional space.","The smooth profiles given by DFT indeed follow the experimental trend, demonstrating the promising potential of using machine learning to explore the magnetic properties of HEAs, by establishing reasonably large datasets with high-throughput calculations using density-functional theory."],"url":"http://arxiv.org/abs/2404.18283v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-28 18:51:32","title":"Out-of-distribution Detection in Medical Image Analysis: A survey","abstract":"Computer-aided diagnostics has benefited from the development of deep learning-based computer vision techniques in these years. Traditional supervised deep learning methods assume that the test sample is drawn from the identical distribution as the training data. However, it is possible to encounter out-of-distribution samples in real-world clinical scenarios, which may cause silent failure in deep learning-based medical image analysis tasks. Recently, research has explored various out-of-distribution (OOD) detection situations and techniques to enable a trustworthy medical AI system. In this survey, we systematically review the recent advances in OOD detection in medical image analysis. We first explore several factors that may cause a distributional shift when using a deep-learning-based model in clinic scenarios, with three different types of distributional shift well defined on top of these factors. Then a framework is suggested to categorize and feature existing solutions, while the previous studies are reviewed based on the methodology taxonomy. Our discussion also includes evaluation protocols and metrics, as well as the challenge and a research direction lack of exploration.","sentences":["Computer-aided diagnostics has benefited from the development of deep learning-based computer vision techniques in these years.","Traditional supervised deep learning methods assume that the test sample is drawn from the identical distribution as the training data.","However, it is possible to encounter out-of-distribution samples in real-world clinical scenarios, which may cause silent failure in deep learning-based medical image analysis tasks.","Recently, research has explored various out-of-distribution (OOD) detection situations and techniques to enable a trustworthy medical AI system.","In this survey, we systematically review the recent advances in OOD detection in medical image analysis.","We first explore several factors that may cause a distributional shift when using a deep-learning-based model in clinic scenarios, with three different types of distributional shift well defined on top of these factors.","Then a framework is suggested to categorize and feature existing solutions, while the previous studies are reviewed based on the methodology taxonomy.","Our discussion also includes evaluation protocols and metrics, as well as the challenge and a research direction lack of exploration."],"url":"http://arxiv.org/abs/2404.18279v1","category":"cs.CV"}
{"created":"2024-04-28 18:44:10","title":"Kernel Corrector LSTM","abstract":"Forecasting methods are affected by data quality issues in two ways: 1. they are hard to predict, and 2. they may affect the model negatively when it is updated with new data. The latter issue is usually addressed by pre-processing the data to remove those issues. An alternative approach has recently been proposed, Corrector LSTM (cLSTM), which is a Read \\& Write Machine Learning (RW-ML) algorithm that changes the data while learning to improve its predictions. Despite promising results being reported, cLSTM is computationally expensive, as it uses a meta-learner to monitor the hidden states of the LSTM. We propose a new RW-ML algorithm, Kernel Corrector LSTM (KcLSTM), that replaces the meta-learner of cLSTM with a simpler method: Kernel Smoothing. We empirically evaluate the forecasting accuracy and the training time of the new algorithm and compare it with cLSTM and LSTM. Results indicate that it is able to decrease the training time while maintaining a competitive forecasting accuracy.","sentences":["Forecasting methods are affected by data quality issues in two ways: 1.","they are hard to predict, and 2.","they may affect the model negatively when it is updated with new data.","The latter issue is usually addressed by pre-processing the data to remove those issues.","An alternative approach has recently been proposed, Corrector LSTM (cLSTM), which is a Read \\& Write Machine Learning (RW-ML) algorithm that changes the data while learning to improve its predictions.","Despite promising results being reported, cLSTM is computationally expensive, as it uses a meta-learner to monitor the hidden states of the LSTM.","We propose a new RW-ML algorithm, Kernel Corrector LSTM (KcLSTM), that replaces the meta-learner of cLSTM with a simpler method: Kernel Smoothing.","We empirically evaluate the forecasting accuracy and the training time of the new algorithm and compare it with cLSTM and LSTM.","Results indicate that it is able to decrease the training time while maintaining a competitive forecasting accuracy."],"url":"http://arxiv.org/abs/2404.18273v1","category":"cs.LG"}
{"created":"2024-04-28 17:20:08","title":"Efficient Remote Sensing with Harmonized Transfer Learning and Modality Alignment","abstract":"With the rise of Visual and Language Pretraining (VLP), an increasing number of downstream tasks are adopting the paradigm of pretraining followed by fine-tuning. Although this paradigm has demonstrated potential in various multimodal downstream tasks, its implementation in the remote sensing domain encounters some obstacles. Specifically, the tendency for same-modality embeddings to cluster together impedes efficient transfer learning. To tackle this issue, we review the aim of multimodal transfer learning for downstream tasks from a unified perspective, and rethink the optimization process based on three distinct objectives. We propose \"Harmonized Transfer Learning and Modality Alignment (HarMA)\", a method that simultaneously satisfies task constraints, modality alignment, and single-modality uniform alignment, while minimizing training overhead through parameter-efficient fine-tuning. Remarkably, without the need for external data for training, HarMA achieves state-of-the-art performance in two popular multimodal retrieval tasks in the field of remote sensing. Our experiments reveal that HarMA achieves competitive and even superior performance to fully fine-tuned models with only minimal adjustable parameters. Due to its simplicity, HarMA can be integrated into almost all existing multimodal pretraining models. We hope this method can facilitate the efficient application of large models to a wide range of downstream tasks while significantly reducing the resource consumption. Code is available at https://github.com/seekerhuang/HarMA.","sentences":["With the rise of Visual and Language Pretraining (VLP), an increasing number of downstream tasks are adopting the paradigm of pretraining followed by fine-tuning.","Although this paradigm has demonstrated potential in various multimodal downstream tasks, its implementation in the remote sensing domain encounters some obstacles.","Specifically, the tendency for same-modality embeddings to cluster together impedes efficient transfer learning.","To tackle this issue, we review the aim of multimodal transfer learning for downstream tasks from a unified perspective, and rethink the optimization process based on three distinct objectives.","We propose \"Harmonized Transfer Learning and Modality Alignment (HarMA)\", a method that simultaneously satisfies task constraints, modality alignment, and single-modality uniform alignment, while minimizing training overhead through parameter-efficient fine-tuning.","Remarkably, without the need for external data for training, HarMA achieves state-of-the-art performance in two popular multimodal retrieval tasks in the field of remote sensing.","Our experiments reveal that HarMA achieves competitive and even superior performance to fully fine-tuned models with only minimal adjustable parameters.","Due to its simplicity, HarMA can be integrated into almost all existing multimodal pretraining models.","We hope this method can facilitate the efficient application of large models to a wide range of downstream tasks while significantly reducing the resource consumption.","Code is available at https://github.com/seekerhuang/HarMA."],"url":"http://arxiv.org/abs/2404.18253v1","category":"cs.CV"}
{"created":"2024-04-28 17:18:08","title":"Machine Learning for Blockchain Data Analysis: Progress and Opportunities","abstract":"Blockchain technology has rapidly emerged to mainstream attention, while its publicly accessible, heterogeneous, massive-volume, and temporal data are reminiscent of the complex dynamics encountered during the last decade of big data. Unlike any prior data source, blockchain datasets encompass multiple layers of interactions across real-world entities, e.g., human users, autonomous programs, and smart contracts. Furthermore, blockchain's integration with cryptocurrencies has introduced financial aspects of unprecedented scale and complexity such as decentralized finance, stablecoins, non-fungible tokens, and central bank digital currencies. These unique characteristics present both opportunities and challenges for machine learning on blockchain data.   On one hand, we examine the state-of-the-art solutions, applications, and future directions associated with leveraging machine learning for blockchain data analysis critical for the improvement of blockchain technology such as e-crime detection and trends prediction. On the other hand, we shed light on the pivotal role of blockchain by providing vast datasets and tools that can catalyze the growth of the evolving machine learning ecosystem. This paper serves as a comprehensive resource for researchers, practitioners, and policymakers, offering a roadmap for navigating this dynamic and transformative field.","sentences":["Blockchain technology has rapidly emerged to mainstream attention, while its publicly accessible, heterogeneous, massive-volume, and temporal data are reminiscent of the complex dynamics encountered during the last decade of big data.","Unlike any prior data source, blockchain datasets encompass multiple layers of interactions across real-world entities, e.g., human users, autonomous programs, and smart contracts.","Furthermore, blockchain's integration with cryptocurrencies has introduced financial aspects of unprecedented scale and complexity such as decentralized finance, stablecoins, non-fungible tokens, and central bank digital currencies.","These unique characteristics present both opportunities and challenges for machine learning on blockchain data.   ","On one hand, we examine the state-of-the-art solutions, applications, and future directions associated with leveraging machine learning for blockchain data analysis critical for the improvement of blockchain technology such as e-crime detection and trends prediction.","On the other hand, we shed light on the pivotal role of blockchain by providing vast datasets and tools that can catalyze the growth of the evolving machine learning ecosystem.","This paper serves as a comprehensive resource for researchers, practitioners, and policymakers, offering a roadmap for navigating this dynamic and transformative field."],"url":"http://arxiv.org/abs/2404.18251v1","category":"cs.CR"}
{"created":"2024-04-28 17:10:17","title":"Tenspiler: A Verified Lifting-Based Compiler for Tensor Operations","abstract":"Tensor processing infrastructures such as deep learning frameworks and specialized hardware accelerators have revolutionized how computationally intensive code from domains such as deep learning and image processing is executed and optimized. These infrastructures provide powerful and expressive abstractions while ensuring high performance. However, to utilize them, code must be written specifically using the APIs / ISAs of such software frameworks or hardware accelerators. Importantly, given the fast pace of innovation in these domains, code written today quickly becomes legacy as new frameworks and accelerators are developed, and migrating such legacy code manually is a considerable effort.   To enable developers in leveraging such DSLs while preserving their current programming paradigm, we introduce Tenspiler, a verified lifting-based compiler that uses program synthesis to translate sequential programs written in general-purpose programming languages (e.g., C++ or Python code) into tensor operations. Central to Tenspiler is our carefully crafted yet simple intermediate language, named TensIR, that expresses tensor operations. TensIR enables efficient lifting, verification, and code generation.   Currently, Tenspiler already supports \\textbf{six} DSLs, spanning a broad spectrum of software and hardware environments. Furthermore, we show that new backends can be easily supported by Tenspiler by adding simple pattern-matching rules for TensIR. Using 10 real-world code benchmark suites, our experimental evaluation shows that by translating code to be executed on \\textbf{6} different software frameworks and hardware devices, Tenspiler offers on average 105$\\times$ kernel and 9.65$\\times$ end-to-end execution time improvement over the fully-optimized sequential implementation of the same benchmarks.","sentences":["Tensor processing infrastructures such as deep learning frameworks and specialized hardware accelerators have revolutionized how computationally intensive code from domains such as deep learning and image processing is executed and optimized.","These infrastructures provide powerful and expressive abstractions while ensuring high performance.","However, to utilize them, code must be written specifically using the APIs / ISAs of such software frameworks or hardware accelerators.","Importantly, given the fast pace of innovation in these domains, code written today quickly becomes legacy as new frameworks and accelerators are developed, and migrating such legacy code manually is a considerable effort.   ","To enable developers in leveraging such DSLs while preserving their current programming paradigm, we introduce Tenspiler, a verified lifting-based compiler that uses program synthesis to translate sequential programs written in general-purpose programming languages (e.g., C++ or Python code) into tensor operations.","Central to Tenspiler is our carefully crafted yet simple intermediate language, named TensIR, that expresses tensor operations.","TensIR enables efficient lifting, verification, and code generation.   ","Currently, Tenspiler already supports \\textbf{six} DSLs, spanning a broad spectrum of software and hardware environments.","Furthermore, we show that new backends can be easily supported by Tenspiler by adding simple pattern-matching rules for TensIR.","Using 10 real-world code benchmark suites, our experimental evaluation shows that by translating code to be executed on \\textbf{6} different software frameworks and hardware devices, Tenspiler offers on average 105$\\times$ kernel and 9.65$\\times$ end-to-end execution time improvement over the fully-optimized sequential implementation of the same benchmarks."],"url":"http://arxiv.org/abs/2404.18249v1","category":"cs.PL"}
{"created":"2024-04-28 16:55:44","title":"FAD-SAR: A Novel Fishing Activity Detection System via Synthetic Aperture Radar Images Based on Deep Learning Method","abstract":"Illegal, unreported, and unregulated (IUU) fishing seriously affects various aspects of human life. However, current methods for detecting and monitoring IUU activities at sea have limitations. While Synthetic Aperture Radar (SAR) can complement existing vessel detection systems, extracting useful information from SAR images using traditional methods, especially for IUU fishing identification, poses challenges. This paper proposes a deep learning-based system for detecting fishing activities. We implemented this system on the xView3 dataset using six classical object detection models: Faster R-CNN, Cascade R-CNN, SSD, RetinaNet, FSAF, and FCOS. We applied improvement methods to enhance the performance of the Faster R-CNN model. Specifically, training the Faster R-CNN model using Online Hard Example Mining (OHEM) strategy improved the Avg-F1 value from 0.212 to 0.216, representing a 1.96% improvement.","sentences":["Illegal, unreported, and unregulated (IUU) fishing seriously affects various aspects of human life.","However, current methods for detecting and monitoring IUU activities at sea have limitations.","While Synthetic Aperture Radar (SAR) can complement existing vessel detection systems, extracting useful information from SAR images using traditional methods, especially for IUU fishing identification, poses challenges.","This paper proposes a deep learning-based system for detecting fishing activities.","We implemented this system on the xView3 dataset using six classical object detection models: Faster R-CNN, Cascade R-CNN, SSD, RetinaNet, FSAF, and FCOS.","We applied improvement methods to enhance the performance of the Faster R-CNN model.","Specifically, training the Faster R-CNN model using Online Hard Example Mining (OHEM) strategy improved the Avg-F1 value from 0.212 to 0.216, representing a 1.96% improvement."],"url":"http://arxiv.org/abs/2404.18245v1","category":"cs.CV"}
{"created":"2024-04-28 16:24:28","title":"Bayesian optimization for state engineering of quantum gases","abstract":"State engineering of quantum objects is a central requirement in most implementations. In the cases where the quantum dynamics can be described by analytical solutions or simple approximation models, optimal state preparation protocols have been theoretically proposed and experimentally realized. For more complex systems, however, such as multi-component quantum gases, simplifying assumptions do not apply anymore and the optimization techniques become computationally impractical. Here, we propose Bayesian optimization based on multi-output Gaussian processes to learn the quantum state's physical properties from few simulations only. We evaluate its performance on an optimization study case of diabatically transporting a Bose-Einstein condensate while keeping it in its ground state, and show that within only few hundreds of executions of the underlying physics simulation, we reach a competitive performance with other protocols. While restricting this benchmarking to well known approximations for straightforward comparisons, we expect a similar performance when employing more involving models, which are computationally more challenging. This paves the way to efficient state engineering of complex quantum systems.","sentences":["State engineering of quantum objects is a central requirement in most implementations.","In the cases where the quantum dynamics can be described by analytical solutions or simple approximation models, optimal state preparation protocols have been theoretically proposed and experimentally realized.","For more complex systems, however, such as multi-component quantum gases, simplifying assumptions do not apply anymore and the optimization techniques become computationally impractical.","Here, we propose Bayesian optimization based on multi-output Gaussian processes to learn the quantum state's physical properties from few simulations only.","We evaluate its performance on an optimization study case of diabatically transporting a Bose-Einstein condensate while keeping it in its ground state, and show that within only few hundreds of executions of the underlying physics simulation, we reach a competitive performance with other protocols.","While restricting this benchmarking to well known approximations for straightforward comparisons, we expect a similar performance when employing more involving models, which are computationally more challenging.","This paves the way to efficient state engineering of complex quantum systems."],"url":"http://arxiv.org/abs/2404.18234v1","category":"quant-ph"}
{"created":"2024-04-28 16:14:31","title":"A Note on Asynchronous Challenges: Unveiling Formulaic Bias and Data Loss in the Hayashi-Yoshida Estimator","abstract":"The Hayashi-Yoshida (\\HY)-estimator exhibits an intrinsic, telescoping property that leads to an often overlooked computational bias, which we denote,formulaic or intrinsic bias. This formulaic bias results in data loss by cancelling out potentially relevant data points, the nonextant data points. This paper attempts to formalize and quantify the data loss arising from this bias. In particular, we highlight the existence of nonextant data points via a concrete example, and prove necessary and sufficient conditions for the telescoping property to induce this type of formulaic bias.Since this type of bias is nonexistent when inputs, i.e., observation times, $\\Pi^{(1)} :=(t_i^{(1)})_{i=0,1,\\ldots}$ and $\\Pi^{(2)} :=(t_j^{(2)})_{j=0,1,\\ldots}$, are synchronous, we introduce the (a,b)-asynchronous adversary. This adversary generates inputs $\\Pi^{(1)}$ and $\\Pi^{(2)}$ according to two independent homogenous Poisson processes with rates a>0 and b>0, respectively. We address the foundational questions regarding cumulative minimal (or least) average data point loss, and determine the values for a and b. We prove that for equal rates a=b, the minimal average cumulative data loss over both inputs is attained and amounts to 25\\%. We present an algorithm, which is based on our theorem, for computing the exact number of nonextant data points given inputs $\\Pi^{(1)}$ and $\\Pi^{(2)}$, and suggest alternative methods. Finally, we use simulated data to empirically compare the (cumulative) average data loss of the (\\HY)-estimator.","sentences":["The Hayashi-Yoshida (\\HY)-estimator exhibits an intrinsic, telescoping property that leads to an often overlooked computational bias, which we denote,formulaic or intrinsic bias.","This formulaic bias results in data loss by cancelling out potentially relevant data points, the nonextant data points.","This paper attempts to formalize and quantify the data loss arising from this bias.","In particular, we highlight the existence of nonextant data points via a concrete example, and prove necessary and sufficient conditions for the telescoping property to induce this type of formulaic bias.","Since this type of bias is nonexistent when inputs, i.e., observation times, $\\Pi^{(1)} :=(","t_i^{(1)})_{i=0,1,\\ldots}$ and $\\Pi^{(2)} :=(t_j^{(2)})_{j=0,1,\\ldots}$, are synchronous, we introduce the (a,b)-asynchronous adversary.","This adversary generates inputs $\\Pi^{(1)}$ and $\\Pi^{(2)}$ according to two independent homogenous Poisson processes with rates a>0 and b>0, respectively.","We address the foundational questions regarding cumulative minimal (or least) average data point loss, and determine the values for a and b.","We prove that for equal rates a=b, the minimal average cumulative data loss over both inputs is attained and amounts to 25\\%.","We present an algorithm, which is based on our theorem, for computing the exact number of nonextant data points given inputs $\\Pi^{(1)}$ and $\\Pi^{(2)}$, and suggest alternative methods.","Finally, we use simulated data to empirically compare the (cumulative) average data loss of the (\\HY)-estimator."],"url":"http://arxiv.org/abs/2404.18233v1","category":"stat.ML"}
{"created":"2024-04-28 15:41:05","title":"Quadruped robot traversing 3D complex environments with limited perception","abstract":"Traversing 3-D complex environments has always been a significant challenge for legged locomotion. Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information. However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly. This task is undeniably challenging. Our research finds that methods based on collision detection can enhance a robot's perception of environmental obstacles. In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing. This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robot's traversability in complex environments. We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments.","sentences":["Traversing 3-D complex environments has always been a significant challenge for legged locomotion.","Existing methods typically rely on external sensors such as vision and lidar to preemptively react to obstacles by acquiring environmental information.","However, in scenarios like nighttime or dense forests, external sensors often fail to function properly, necessitating robots to rely on proprioceptive sensors to perceive diverse obstacles in the environment and respond promptly.","This task is undeniably challenging.","Our research finds that methods based on collision detection can enhance a robot's perception of environmental obstacles.","In this work, we propose an end-to-end learning-based quadruped robot motion controller that relies solely on proprioceptive sensing.","This controller can accurately detect, localize, and agilely respond to collisions in unknown and complex 3D environments, thereby improving the robot's traversability in complex environments.","We demonstrate in both simulation and real-world experiments that our method enables quadruped robots to successfully traverse challenging obstacles in various complex environments."],"url":"http://arxiv.org/abs/2404.18225v2","category":"cs.RO"}
{"created":"2024-04-28 15:20:45","title":"L3Cube-MahaNews: News-based Short Text and Long Document Classification Datasets in Marathi","abstract":"The availability of text or topic classification datasets in the low-resource Marathi language is limited, typically consisting of fewer than 4 target labels, with some achieving nearly perfect accuracy. In this work, we introduce L3Cube-MahaNews, a Marathi text classification corpus that focuses on News headlines and articles. This corpus stands out as the largest supervised Marathi Corpus, containing over 1.05L records classified into a diverse range of 12 categories. To accommodate different document lengths, MahaNews comprises three supervised datasets specifically designed for short text, long documents, and medium paragraphs. The consistent labeling across these datasets facilitates document length-based analysis. We provide detailed data statistics and baseline results on these datasets using state-of-the-art pre-trained BERT models. We conduct a comparative analysis between monolingual and multilingual BERT models, including MahaBERT, IndicBERT, and MuRIL. The monolingual MahaBERT model outperforms all others on every dataset. These resources also serve as Marathi topic classification datasets or models and are publicly available at https://github.com/l3cube-pune/MarathiNLP .","sentences":["The availability of text or topic classification datasets in the low-resource Marathi language is limited, typically consisting of fewer than 4 target labels, with some achieving nearly perfect accuracy.","In this work, we introduce L3Cube-MahaNews, a Marathi text classification corpus that focuses on News headlines and articles.","This corpus stands out as the largest supervised Marathi Corpus, containing over 1.05L records classified into a diverse range of 12 categories.","To accommodate different document lengths, MahaNews comprises three supervised datasets specifically designed for short text, long documents, and medium paragraphs.","The consistent labeling across these datasets facilitates document length-based analysis.","We provide detailed data statistics and baseline results on these datasets using state-of-the-art pre-trained BERT models.","We conduct a comparative analysis between monolingual and multilingual BERT models, including MahaBERT, IndicBERT, and MuRIL.","The monolingual MahaBERT model outperforms all others on every dataset.","These resources also serve as Marathi topic classification datasets or models and are publicly available at https://github.com/l3cube-pune/MarathiNLP ."],"url":"http://arxiv.org/abs/2404.18216v1","category":"cs.CL"}
{"created":"2024-04-28 14:58:54","title":"Enhancing Action Recognition from Low-Quality Skeleton Data via Part-Level Knowledge Distillation","abstract":"Skeleton-based action recognition is vital for comprehending human-centric videos and has applications in diverse domains. One of the challenges of skeleton-based action recognition is dealing with low-quality data, such as skeletons that have missing or inaccurate joints. This paper addresses the issue of enhancing action recognition using low-quality skeletons through a general knowledge distillation framework. The proposed framework employs a teacher-student model setup, where a teacher model trained on high-quality skeletons guides the learning of a student model that handles low-quality skeletons. To bridge the gap between heterogeneous high-quality and lowquality skeletons, we present a novel part-based skeleton matching strategy, which exploits shared body parts to facilitate local action pattern learning. An action-specific part matrix is developed to emphasize critical parts for different actions, enabling the student model to distill discriminative part-level knowledge. A novel part-level multi-sample contrastive loss achieves knowledge transfer from multiple high-quality skeletons to low-quality ones, which enables the proposed knowledge distillation framework to include training low-quality skeletons that lack corresponding high-quality matches. Comprehensive experiments conducted on the NTU-RGB+D, Penn Action, and SYSU 3D HOI datasets demonstrate the effectiveness of the proposed knowledge distillation framework.","sentences":["Skeleton-based action recognition is vital for comprehending human-centric videos and has applications in diverse domains.","One of the challenges of skeleton-based action recognition is dealing with low-quality data, such as skeletons that have missing or inaccurate joints.","This paper addresses the issue of enhancing action recognition using low-quality skeletons through a general knowledge distillation framework.","The proposed framework employs a teacher-student model setup, where a teacher model trained on high-quality skeletons guides the learning of a student model that handles low-quality skeletons.","To bridge the gap between heterogeneous high-quality and lowquality skeletons, we present a novel part-based skeleton matching strategy, which exploits shared body parts to facilitate local action pattern learning.","An action-specific part matrix is developed to emphasize critical parts for different actions, enabling the student model to distill discriminative part-level knowledge.","A novel part-level multi-sample contrastive loss achieves knowledge transfer from multiple high-quality skeletons to low-quality ones, which enables the proposed knowledge distillation framework to include training low-quality skeletons that lack corresponding high-quality matches.","Comprehensive experiments conducted on the NTU-RGB+D, Penn Action, and SYSU 3D HOI datasets demonstrate the effectiveness of the proposed knowledge distillation framework."],"url":"http://arxiv.org/abs/2404.18206v1","category":"cs.CV"}
{"created":"2024-04-28 14:39:29","title":"What Foundation Models can Bring for Robot Learning in Manipulation : A Survey","abstract":"The realization of universal robots is an ultimate goal of researchers. However, a key hurdle in achieving this goal lies in the robots' ability to manipulate objects in their unstructured surrounding environments according to different tasks. The learning-based approach is considered an effective way to address generalization. The impressive performance of foundation models in the fields of computer vision and natural language suggests the potential of embedding foundation models into manipulation tasks as a viable path toward achieving general manipulation capability. However, we believe achieving general manipulation capability requires an overarching framework akin to auto driving. This framework should encompass multiple functional modules, with different foundation models assuming distinct roles in facilitating general manipulation capability. This survey focuses on the contributions of foundation models to robot learning for manipulation. We propose a comprehensive framework and detail how foundation models can address challenges in each module of the framework. What's more, we examine current approaches, outline challenges, suggest future research directions, and identify potential risks associated with integrating foundation models into this domain.","sentences":["The realization of universal robots is an ultimate goal of researchers.","However, a key hurdle in achieving this goal lies in the robots' ability to manipulate objects in their unstructured surrounding environments according to different tasks.","The learning-based approach is considered an effective way to address generalization.","The impressive performance of foundation models in the fields of computer vision and natural language suggests the potential of embedding foundation models into manipulation tasks as a viable path toward achieving general manipulation capability.","However, we believe achieving general manipulation capability requires an overarching framework akin to auto driving.","This framework should encompass multiple functional modules, with different foundation models assuming distinct roles in facilitating general manipulation capability.","This survey focuses on the contributions of foundation models to robot learning for manipulation.","We propose a comprehensive framework and detail how foundation models can address challenges in each module of the framework.","What's more, we examine current approaches, outline challenges, suggest future research directions, and identify potential risks associated with integrating foundation models into this domain."],"url":"http://arxiv.org/abs/2404.18201v1","category":"cs.RO"}
{"created":"2024-04-28 14:04:58","title":"Naive Bayes Classifiers and One-hot Encoding of Categorical Variables","abstract":"This paper investigates the consequences of encoding a $K$-valued categorical variable incorrectly as $K$ bits via one-hot encoding, when using a Na\\\"{\\i}ve Bayes classifier. This gives rise to a product-of-Bernoullis (PoB) assumption, rather than the correct categorical Na\\\"{\\i}ve Bayes classifier. The differences between the two classifiers are analysed mathematically and experimentally. In our experiments using probability vectors drawn from a Dirichlet distribution, the two classifiers are found to agree on the maximum a posteriori class label for most cases, although the posterior probabilities are usually greater for the PoB case.","sentences":["This paper investigates the consequences of encoding a $K$-valued categorical variable incorrectly as $K$ bits via one-hot encoding, when using a Na\\\"{\\i}ve Bayes classifier.","This gives rise to a product-of-Bernoullis (PoB) assumption, rather than the correct categorical Na\\\"{\\i}ve Bayes classifier.","The differences between the two classifiers are analysed mathematically and experimentally.","In our experiments using probability vectors drawn from a Dirichlet distribution, the two classifiers are found to agree on the maximum a posteriori class label for most cases, although the posterior probabilities are usually greater for the PoB case."],"url":"http://arxiv.org/abs/2404.18190v1","category":"cs.LG"}
{"created":"2024-04-28 13:25:11","title":"EkoHate: Abusive Language and Hate Speech Detection for Code-switched Political Discussions on Nigerian Twitter","abstract":"Nigerians have a notable online presence and actively discuss political and topical matters. This was particularly evident throughout the 2023 general election, where Twitter was used for campaigning, fact-checking and verification, and even positive and negative discourse. However, little or none has been done in the detection of abusive language and hate speech in Nigeria. In this paper, we curated code-switched Twitter data directed at three musketeers of the governorship election on the most populous and economically vibrant state in Nigeria; Lagos state, with the view to detect offensive speech in political discussions. We developed EkoHate -- an abusive language and hate speech dataset for political discussions between the three candidates and their followers using a binary (normal vs offensive) and fine-grained four-label annotation scheme. We analysed our dataset and provided an empirical evaluation of state-of-the-art methods across both supervised and cross-lingual transfer learning settings. In the supervised setting, our evaluation results in both binary and four-label annotation schemes show that we can achieve 95.1 and 70.3 F1 points respectively. Furthermore, we show that our dataset adequately transfers very well to three publicly available offensive datasets (OLID, HateUS2020, and FountaHate), generalizing to political discussions in other regions like the US.","sentences":["Nigerians have a notable online presence and actively discuss political and topical matters.","This was particularly evident throughout the 2023 general election, where Twitter was used for campaigning, fact-checking and verification, and even positive and negative discourse.","However, little or none has been done in the detection of abusive language and hate speech in Nigeria.","In this paper, we curated code-switched Twitter data directed at three musketeers of the governorship election on the most populous and economically vibrant state in Nigeria; Lagos state, with the view to detect offensive speech in political discussions.","We developed EkoHate -- an abusive language and hate speech dataset for political discussions between the three candidates and their followers using a binary (normal vs offensive) and fine-grained four-label annotation scheme.","We analysed our dataset and provided an empirical evaluation of state-of-the-art methods across both supervised and cross-lingual transfer learning settings.","In the supervised setting, our evaluation results in both binary and four-label annotation schemes show that we can achieve 95.1 and 70.3 F1 points respectively.","Furthermore, we show that our dataset adequately transfers very well to three publicly available offensive datasets (OLID, HateUS2020, and FountaHate), generalizing to political discussions in other regions like the US."],"url":"http://arxiv.org/abs/2404.18180v1","category":"cs.CL"}
{"created":"2024-04-28 12:46:36","title":"Behavior-Contextualized Item Preference Modeling for Multi-Behavior Recommendation","abstract":"In recommender systems, multi-behavior methods have demonstrated their effectiveness in mitigating issues like data sparsity, a common challenge in traditional single-behavior recommendation approaches. These methods typically infer user preferences from various auxiliary behaviors and apply them to the target behavior for recommendations. However, this direct transfer can introduce noise to the target behavior in recommendation, due to variations in user attention across different behaviors. To address this issue, this paper introduces a novel approach, Behavior-Contextualized Item Preference Modeling (BCIPM), for multi-behavior recommendation. Our proposed Behavior-Contextualized Item Preference Network discerns and learns users' specific item preferences within each behavior. It then considers only those preferences relevant to the target behavior for final recommendations, significantly reducing noise from auxiliary behaviors. These auxiliary behaviors are utilized solely for training the network parameters, thereby refining the learning process without compromising the accuracy of the target behavior recommendations. To further enhance the effectiveness of BCIPM, we adopt a strategy of pre-training the initial embeddings. This step is crucial for enriching the item-aware preferences, particularly in scenarios where data related to the target behavior is sparse. Comprehensive experiments conducted on four real-world datasets demonstrate BCIPM's superior performance compared to several leading state-of-the-art models, validating the robustness and efficiency of our proposed approach.","sentences":["In recommender systems, multi-behavior methods have demonstrated their effectiveness in mitigating issues like data sparsity, a common challenge in traditional single-behavior recommendation approaches.","These methods typically infer user preferences from various auxiliary behaviors and apply them to the target behavior for recommendations.","However, this direct transfer can introduce noise to the target behavior in recommendation, due to variations in user attention across different behaviors.","To address this issue, this paper introduces a novel approach, Behavior-Contextualized Item Preference Modeling (BCIPM), for multi-behavior recommendation.","Our proposed Behavior-Contextualized Item Preference Network discerns and learns users' specific item preferences within each behavior.","It then considers only those preferences relevant to the target behavior for final recommendations, significantly reducing noise from auxiliary behaviors.","These auxiliary behaviors are utilized solely for training the network parameters, thereby refining the learning process without compromising the accuracy of the target behavior recommendations.","To further enhance the effectiveness of BCIPM, we adopt a strategy of pre-training the initial embeddings.","This step is crucial for enriching the item-aware preferences, particularly in scenarios where data related to the target behavior is sparse.","Comprehensive experiments conducted on four real-world datasets demonstrate BCIPM's superior performance compared to several leading state-of-the-art models, validating the robustness and efficiency of our proposed approach."],"url":"http://arxiv.org/abs/2404.18166v1","category":"cs.IR"}
{"created":"2024-04-28 07:47:52","title":"Semi-supervised Text-based Person Search","abstract":"Text-based person search (TBPS) aims to retrieve images of a specific person from a large image gallery based on a natural language description. Existing methods rely on massive annotated image-text data to achieve satisfactory performance in fully-supervised learning. It poses a significant challenge in practice, as acquiring person images from surveillance videos is relatively easy, while obtaining annotated texts is challenging. The paper undertakes a pioneering initiative to explore TBPS under the semi-supervised setting, where only a limited number of person images are annotated with textual descriptions while the majority of images lack annotations. We present a two-stage basic solution based on generation-then-retrieval for semi-supervised TBPS. The generation stage enriches annotated data by applying an image captioning model to generate pseudo-texts for unannotated images. Later, the retrieval stage performs fully-supervised retrieval learning using the augmented data. Significantly, considering the noise interference of the pseudo-texts on retrieval learning, we propose a noise-robust retrieval framework that enhances the ability of the retrieval model to handle noisy data. The framework integrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refine the model architecture, and Noise-Guided Progressive Training (NP-Train) to enhance the training process. PC-Mask performs masking on the input data at both the patch-level and the channel-level to prevent overfitting noisy supervision. NP-Train introduces a progressive training schedule based on the noise level of pseudo-texts to facilitate noise-robust learning. Extensive experiments on multiple TBPS benchmarks show that the proposed framework achieves promising performance under the semi-supervised setting.","sentences":["Text-based person search (TBPS) aims to retrieve images of a specific person from a large image gallery based on a natural language description.","Existing methods rely on massive annotated image-text data to achieve satisfactory performance in fully-supervised learning.","It poses a significant challenge in practice, as acquiring person images from surveillance videos is relatively easy, while obtaining annotated texts is challenging.","The paper undertakes a pioneering initiative to explore TBPS under the semi-supervised setting, where only a limited number of person images are annotated with textual descriptions while the majority of images lack annotations.","We present a two-stage basic solution based on generation-then-retrieval for semi-supervised TBPS.","The generation stage enriches annotated data by applying an image captioning model to generate pseudo-texts for unannotated images.","Later, the retrieval stage performs fully-supervised retrieval learning using the augmented data.","Significantly, considering the noise interference of the pseudo-texts on retrieval learning, we propose a noise-robust retrieval framework that enhances the ability of the retrieval model to handle noisy data.","The framework integrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refine the model architecture, and Noise-Guided Progressive Training (NP-Train) to enhance the training process.","PC-Mask performs masking on the input data at both the patch-level and the channel-level to prevent overfitting noisy supervision.","NP-Train introduces a progressive training schedule based on the noise level of pseudo-texts to facilitate noise-robust learning.","Extensive experiments on multiple TBPS benchmarks show that the proposed framework achieves promising performance under the semi-supervised setting."],"url":"http://arxiv.org/abs/2404.18106v1","category":"cs.CV"}
{"created":"2024-04-28 07:29:18","title":"Accurate and Unbiased Reconstruction of CMB B Mode using Deep Learning","abstract":"An ingeniously designed autoencoder (PrimeNet) using simulated observations of future generation ECHO satellite mission recovers CMB B mode map, angular spectrum for multipoles $\\ell \\lesssim 9$ and tensor to scalar ratio $r$ {\\it limited only by cosmic variance down to $r= 0.0001$ and below}. We use diverse, realistically complex and detailed foreground models. PrimeNet predicts accurate results even when data with $r=0$ are tested which were not used in training, implying robust and efficient predictive power. The work eliminates a major bottleneck of weak CMB B mode reconstruction and takes a leap forward for understanding fundamental physics of the primordial Universe.","sentences":["An ingeniously designed autoencoder (PrimeNet) using simulated observations of future generation ECHO satellite mission recovers CMB B mode map, angular spectrum for multipoles $\\ell \\lesssim 9$ and tensor to scalar ratio $r$ {\\it limited only by cosmic variance down to $r= 0.0001$ and below}.","We use diverse, realistically complex and detailed foreground models.","PrimeNet predicts accurate results even when data with $r=0$ are tested which were not used in training, implying robust and efficient predictive power.","The work eliminates a major bottleneck of weak CMB B mode reconstruction and takes a leap forward for understanding fundamental physics of the primordial Universe."],"url":"http://arxiv.org/abs/2404.18100v1","category":"astro-ph.CO"}
{"created":"2024-04-29 17:55:40","title":"Colloidal dispersions of sterically and electrostatically stabilized PbS quantum dots: the effect of stabilization mechanism on structure factors, second virial coefficients, and film-forming properties","abstract":"Electrostatically stabilized nanocrystals (NCs) and, in particular, quantum dots (QDs) hold promise for forming strongly coupled superlattices due to their compact and electronically conductive surface ligands. However, studies on the colloidal dispersion and interparticle interactions of electrostatically stabilized sub-10 nm NCs have been limited, hindering the optimization of colloidal stability and self-assembly. In this study, we employed small-angle X ray scattering (SAXS) experiments to investigate the interparticle interactions and arrangement of PbS QDs with thiostannate ligands (PbS-Sn2S64-) in polar solvents. The study reveals significant deviations from ideal solution behavior in electrostatically stabilized QD dispersions. Our results demonstrate that PbS-Sn2S64- QDs exhibit long-range interactions within the solvent, in contrast to the short-range steric repulsion characteristic of PbS QDs with oleate ligands (PbS-OA). Introducing highly charged multivalent electrolytes screens electrostatic interactions between charged QDs, reducing the length scale of the repulsive interactions. Furthermore, we calculate the second virial (B2) coefficients from SAXS data, providing insights into how surface chemistry, solvent, and size influence pair potentials. Finally, we explore the influence of long-range interparticle interactions of PbS-Sn2S64- QDs on the morphology of films produced by drying or spin-coating colloidal solutions. The long-range repulsive term of PbS-Sn2S64- QDs promotes the formation of amorphous films, and screening the electrostatic repulsion by addition of an electrolyte enables the formation of a crystalline film. These findings highlight the critical role of NC-NC interactions in tailoring the properties of functional nanomaterials.","sentences":["Electrostatically stabilized nanocrystals (NCs) and, in particular, quantum dots (QDs) hold promise for forming strongly coupled superlattices due to their compact and electronically conductive surface ligands.","However, studies on the colloidal dispersion and interparticle interactions of electrostatically stabilized sub-10 nm NCs have been limited, hindering the optimization of colloidal stability and self-assembly.","In this study, we employed small-angle X ray scattering (SAXS) experiments to investigate the interparticle interactions and arrangement of PbS QDs with thiostannate ligands (PbS-Sn2S64-) in polar solvents.","The study reveals significant deviations from ideal solution behavior in electrostatically stabilized QD dispersions.","Our results demonstrate that PbS-Sn2S64- QDs exhibit long-range interactions within the solvent, in contrast to the short-range steric repulsion characteristic of PbS QDs with oleate ligands (PbS-OA).","Introducing highly charged multivalent electrolytes screens electrostatic interactions between charged QDs, reducing the length scale of the repulsive interactions.","Furthermore, we calculate the second virial (B2) coefficients from SAXS data, providing insights into how surface chemistry, solvent, and size influence pair potentials.","Finally, we explore the influence of long-range interparticle interactions of PbS-Sn2S64- QDs on the morphology of films produced by drying or spin-coating colloidal solutions.","The long-range repulsive term of PbS-Sn2S64- QDs promotes the formation of amorphous films, and screening the electrostatic repulsion by addition of an electrolyte enables the formation of a crystalline film.","These findings highlight the critical role of NC-NC interactions in tailoring the properties of functional nanomaterials."],"url":"http://arxiv.org/abs/2404.18915v1","category":"cond-mat.soft"}
{"created":"2024-04-29 17:35:05","title":"High-performance gate-controlled superconducting switches: large output voltage and reproducibility","abstract":"Logic circuits consist of devices that can be controlled between two distinct states. The recent demonstration that a superconducting current flowing in a constriction can be controlled via a gate voltage ($V_G$) - can lead to superconducting logic with better performance than existing logics. However, before such logic is developed, high reproducibility in the functioning of GCS devices and optimization of their performance must be achieved. Here, we report an investigation of gated Nb devices showing GCS with unprecedently-high reproducibility. Based on the investigation of a statistically-significant number of devices, we demonstrate that the GCS is independent of the constriction width, in contrast with previous reports, and confirm a strong correlation between the GCS and the leakage current ($I_{leak}$) induced by $V_G$. We also achieve a voltage output in our devices larger than the typical values reported to date by at least one order of magnitude, which is relevant for the future interconnection of devices, and show that $I_{leak}$ can be used as a tool to modulate the operational $V_G$ of devices on a $SiO_2$ substrates. These results altogether represent an important step forward towards the optimization of reproducibility and performance of GCS devices, and the future development of a GCS-based logic.","sentences":["Logic circuits consist of devices that can be controlled between two distinct states.","The recent demonstration that a superconducting current flowing in a constriction can be controlled via a gate voltage ($V_G$) - can lead to superconducting logic with better performance than existing logics.","However, before such logic is developed, high reproducibility in the functioning of GCS devices and optimization of their performance must be achieved.","Here, we report an investigation of gated Nb devices showing GCS with unprecedently-high reproducibility.","Based on the investigation of a statistically-significant number of devices, we demonstrate that the GCS is independent of the constriction width, in contrast with previous reports, and confirm a strong correlation between the GCS and the leakage current ($I_{leak}$) induced by $V_G$. We also achieve a voltage output in our devices larger than the typical values reported to date by at least one order of magnitude, which is relevant for the future interconnection of devices, and show that $I_{leak}$ can be used as a tool to modulate the operational $V_G$ of devices on a $SiO_2$ substrates.","These results altogether represent an important step forward towards the optimization of reproducibility and performance of GCS devices, and the future development of a GCS-based logic."],"url":"http://arxiv.org/abs/2404.18899v1","category":"cond-mat.supr-con"}
{"created":"2024-04-29 17:10:41","title":"A Multilevel Strategy to Improve People Tracking in a Real-World Scenario","abstract":"The Pal\\'acio do Planalto, office of the President of Brazil, was invaded by protesters on January 8, 2023. Surveillance videos taken from inside the building were subsequently released by the Brazilian Supreme Court for public scrutiny. We used segments of such footage to create the UFPR-Planalto801 dataset for people tracking and re-identification in a real-world scenario. This dataset consists of more than 500,000 images. This paper presents a tracking approach targeting this dataset. The method proposed in this paper relies on the use of known state-of-the-art trackers combined in a multilevel hierarchy to correct the ID association over the trajectories. We evaluated our method using IDF1, MOTA, MOTP and HOTA metrics. The results show improvements for every tracker used in the experiments, with IDF1 score increasing by a margin up to 9.5%.","sentences":["The Pal\\'acio do Planalto, office of the President of Brazil, was invaded by protesters on January 8, 2023.","Surveillance videos taken from inside the building were subsequently released by the Brazilian Supreme Court for public scrutiny.","We used segments of such footage to create the UFPR-Planalto801 dataset for people tracking and re-identification in a real-world scenario.","This dataset consists of more than 500,000 images.","This paper presents a tracking approach targeting this dataset.","The method proposed in this paper relies on the use of known state-of-the-art trackers combined in a multilevel hierarchy to correct the ID association over the trajectories.","We evaluated our method using IDF1, MOTA, MOTP and HOTA metrics.","The results show improvements for every tracker used in the experiments, with IDF1 score increasing by a margin up to 9.5%."],"url":"http://arxiv.org/abs/2404.18876v1","category":"cs.CV"}
{"created":"2024-04-29 15:45:50","title":"Architecture for fast implementation of qLDPC codes with optimized Rydberg gates","abstract":"We propose an implementation of bivariate bicycle codes (Nature {\\bf 627}, 778 (2024)) based on long-range Rydberg gates between stationary neutral atom qubits. An optimized layout of data and ancilla qubits reduces the maximum Euclidean communication distance needed for non-local parity check operators. An optimized Rydberg gate pulse design enables $\\sf CZ$ entangling operations with fidelity ${\\mathcal F}>0.999$ at a distance greater than $12~\\mu\\rm m$. The combination of optimized layout and gate design leads to a quantum error correction cycle time of $\\sim 1.2~\\rm ms$ for a $[[144,12,12]]$ code, an order of magnitude improvement over previous designs.","sentences":["We propose an implementation of bivariate bicycle codes (Nature {\\bf 627}, 778 (2024)) based on long-range Rydberg gates between stationary neutral atom qubits.","An optimized layout of data and ancilla qubits reduces the maximum Euclidean communication distance needed for non-local parity check operators.","An optimized Rydberg gate pulse design enables $\\sf CZ$ entangling operations with fidelity ${\\mathcal F}>0.999$ at a distance greater than $12~\\mu\\rm m$.","The combination of optimized layout and gate design leads to a quantum error correction cycle time of $\\sim 1.2~\\rm ms$ for a $[[144,12,12]]$ code, an order of magnitude improvement over previous designs."],"url":"http://arxiv.org/abs/2404.18809v1","category":"quant-ph"}
{"created":"2024-04-29 15:01:30","title":"Enhanced and Robust Contrast in CEST MRI: Saturation Pulse Shape Design via Optimal Control","abstract":"Purpose: To employ optimal control for the numerical design of CEST saturation pulses to maximize contrast and stability against $B_0$ inhomogeneities.   Theory and Methods: We applied an optimal control framework for the design pulse shapes for CEST saturation pulse trains. The cost functional minimized both the pulse energy and the discrepancy between the corresponding CEST spectrum and the target spectrum based on a continuous RF pulse. The optimization is subject to hardware limitations. In measurements on a 7 T preclinical scanner, the optimal control pulses were compared to continuous-wave and Gaussian saturation methods. We conducted a comparison of the optimal control pulses were compared to with Gaussian, block pulse trains, and adiabatic spin-lock pulses.   Results: The optimal control pulse train demonstrated saturation levels comparable to continuous-wave saturation and surpassed Gaussian saturation by up to 50 \\% in phantom measurements. In phantom measurements at 3 T the optimized pulses not only showcased the highest CEST contrast, but also the highest stability against field inhomogeneities. In contrast, block pulse saturation resulted in severe artifacts. Dynamic Bloch-McConnell simulations were employed to identify the source of these artifacts, and underscore the $B_0$ robustness of the optimized pulses.   Conclusion: In this work, it was shown that a substantial improvement in pulsed saturation CEST imaging can be achieved by using Optimal Control design principles. It is possible to overcome the sensitivity of saturation to B0 inhomogeneities while achieving CEST contrast close to continuous wave saturation.","sentences":["Purpose: To employ optimal control for the numerical design of CEST saturation pulses to maximize contrast and stability against $B_0$ inhomogeneities.   ","Theory and Methods: We applied an optimal control framework for the design pulse shapes for CEST saturation pulse trains.","The cost functional minimized both the pulse energy and the discrepancy between the corresponding CEST spectrum and the target spectrum based on a continuous RF pulse.","The optimization is subject to hardware limitations.","In measurements on a 7 T preclinical scanner, the optimal control pulses were compared to continuous-wave and Gaussian saturation methods.","We conducted a comparison of the optimal control pulses were compared to with Gaussian, block pulse trains, and adiabatic spin-lock pulses.   ","Results:","The optimal control pulse train demonstrated saturation levels comparable to continuous-wave saturation and surpassed Gaussian saturation by up to 50 \\% in phantom measurements.","In phantom measurements at 3 T the optimized pulses not only showcased the highest CEST contrast, but also the highest stability against field inhomogeneities.","In contrast, block pulse saturation resulted in severe artifacts.","Dynamic Bloch-McConnell simulations were employed to identify the source of these artifacts, and underscore the $B_0$ robustness of the optimized pulses.   ","Conclusion: In this work, it was shown that a substantial improvement in pulsed saturation CEST imaging can be achieved by using Optimal Control design principles.","It is possible to overcome the sensitivity of saturation to B0 inhomogeneities while achieving CEST contrast close to continuous wave saturation."],"url":"http://arxiv.org/abs/2404.18764v1","category":"physics.med-ph"}
{"created":"2024-04-29 07:28:51","title":"Mobile Networks on the Move: Optimizing Moving Base Stations Dynamics in Urban Scenarios","abstract":"Base station densification is one of the key approaches for delivering high capacity in radio access networks. However, current static deployments are often impractical and financially unsustainable, as they increase both capital and operational expenditures of the network. An alternative paradigm is the moving base stations (MBSs) approach, by which part of base stations are installed on vehicles. However, to the best of our knowledge, it is still unclear if and up to which point MBSs allow decreasing the number of static base stations (BSs) deployed in urban settings. In this work, we start tackling this issue by proposing a modeling approach for a first-order evaluation of potential infrastructure savings enabled by the MBSs paradigm. Starting from a set of stochastic geometry results, and a traffic demand profile over time, we formulate an optimization problem for the derivation of the optimal combination of moving and static BSs which minimizes the overall amount of BSs deployed, while guaranteeing a target mean QoS for users. Initial results on a two-district scenario with measurement-based network traffic profiles suggest that substantial infrastructure savings are achievable. We show that these results are robust against different values of user density.","sentences":["Base station densification is one of the key approaches for delivering high capacity in radio access networks.","However, current static deployments are often impractical and financially unsustainable, as they increase both capital and operational expenditures of the network.","An alternative paradigm is the moving base stations (MBSs) approach, by which part of base stations are installed on vehicles.","However, to the best of our knowledge, it is still unclear if and up to which point MBSs allow decreasing the number of static base stations (BSs) deployed in urban settings.","In this work, we start tackling this issue by proposing a modeling approach for a first-order evaluation of potential infrastructure savings enabled by the MBSs paradigm.","Starting from a set of stochastic geometry results, and a traffic demand profile over time, we formulate an optimization problem for the derivation of the optimal combination of moving and static BSs which minimizes the overall amount of BSs deployed, while guaranteeing a target mean QoS for users.","Initial results on a two-district scenario with measurement-based network traffic profiles suggest that substantial infrastructure savings are achievable.","We show that these results are robust against different values of user density."],"url":"http://arxiv.org/abs/2404.18476v1","category":"cs.NI"}
{"created":"2024-04-29 05:26:51","title":"Two classes of constacyclic codes with a square-root-like lower bound","abstract":"Constacyclic codes over finite fields are an important class of linear codes as they contain distance-optimal codes and linear codes with best known parameters. They are interesting in theory and practice, as they have the constacyclic structure. In this paper, an infinite class of $q$-ary negacyclic codes of length $(q^m-1)/2$ and an infinite class of $q$-ary constacyclic codes of length $(q^m-1)/(q-1)$ are constructed and analyzed. As a by-product, two infinite classes of ternary negacyclic self-dual codes with a square-root-like lower bound on their minimum distances are presented.","sentences":["Constacyclic codes over finite fields are an important class of linear codes as they contain distance-optimal codes and linear codes with best known parameters.","They are interesting in theory and practice, as they have the constacyclic structure.","In this paper, an infinite class of $q$-ary negacyclic codes of length $(q^m-1)/2$ and an infinite class of $q$-ary constacyclic codes of length $(q^m-1)/(q-1)$ are constructed and analyzed.","As a by-product, two infinite classes of ternary negacyclic self-dual codes with a square-root-like lower bound on their minimum distances are presented."],"url":"http://arxiv.org/abs/2404.18438v1","category":"cs.IT"}
{"created":"2024-04-29 05:26:07","title":"A family of self-orthogonal divisible codes with locality 2","abstract":"Linear codes are widely studied due to their applications in communication, cryptography, quantum codes, distributed storage and many other fields. In this paper, we use the trace and norm functions over finite fields to construct a family of linear codes. The weight distributions of the codes are determined in three cases via Gaussian sums. The codes are shown to be self-orthogonal divisible codes with only three, four or five nonzero weights in these cases. In particular, we prove that this family of linear codes has locality 2. Several optimal or almost optimal linear codes and locally recoverable codes are derived. In particular, an infinite family of distance-optimal binary linear codes with respect to the sphere-packing bound is obtained. The self-orthogonal codes derived in this paper can be used to construct lattices and have nice application in distributed storage.","sentences":["Linear codes are widely studied due to their applications in communication, cryptography, quantum codes, distributed storage and many other fields.","In this paper, we use the trace and norm functions over finite fields to construct a family of linear codes.","The weight distributions of the codes are determined in three cases via Gaussian sums.","The codes are shown to be self-orthogonal divisible codes with only three, four or five nonzero weights in these cases.","In particular, we prove that this family of linear codes has locality 2.","Several optimal or almost optimal linear codes and locally recoverable codes are derived.","In particular, an infinite family of distance-optimal binary linear codes with respect to the sphere-packing bound is obtained.","The self-orthogonal codes derived in this paper can be used to construct lattices and have nice application in distributed storage."],"url":"http://arxiv.org/abs/2404.18437v1","category":"cs.IT"}
{"created":"2024-04-29 05:17:51","title":"The augmented codes of a family of linear codes with locality 2","abstract":"In this paper, we first generalize the class of linear codes by Ding and Ding (IEEE TIT, 61(11), pp. 5835-5842, 2015). Then we mainly study the augmented codes of this generalized class of linear codes. For one thing, we use Gaussian sums to determine the parameters and weight distributions of the augmented codes in some cases. It is shown that the augmented codes are self-orthogonal and have only a few nonzero weights. For another thing, the locality of the augmented codes is proved to be 2, which indicates the augmented codes are useful in distributed storage. Besides, the augmented codes are projective as the minimum distance of their duals is proved to be 3. In particular, we obtain several (almost) optimal linear codes and locally recoverable codes.","sentences":["In this paper, we first generalize the class of linear codes by Ding and Ding (IEEE TIT, 61(11), pp. 5835-5842, 2015).","Then we mainly study the augmented codes of this generalized class of linear codes.","For one thing, we use Gaussian sums to determine the parameters and weight distributions of the augmented codes in some cases.","It is shown that the augmented codes are self-orthogonal and have only a few nonzero weights.","For another thing, the locality of the augmented codes is proved to be 2, which indicates the augmented codes are useful in distributed storage.","Besides, the augmented codes are projective as the minimum distance of their duals is proved to be 3.","In particular, we obtain several (almost) optimal linear codes and locally recoverable codes."],"url":"http://arxiv.org/abs/2404.18434v1","category":"cs.IT"}
{"created":"2024-04-29 03:52:02","title":"Movable Antenna-Enhanced Wireless Powered Mobile Edge Computing Systems","abstract":"In this paper, we propose a movable antenna (MA) enhanced scheme for wireless powered mobile edge computing (WP-MEC) system, where the hybrid access point (HAP) equipped with multiple MAs first emits wireless energy to charge wireless devices (WDs), and then receives the offloaded tasks from the WDs for edge computing. The MAs deployed at the HAP enhance the spatial degrees of freedom (DoFs) by flexibly adjusting the positions of MAs within an available region, thereby improving the efficiency of both downlink wireless energy transfer (WPT) and uplink task offloading. To balance the performance enhancement against the implementation intricacy, we further propose three types of MA positioning configurations, i.e., dynamic MA positioning, semi-dynamic MA positioning, and static MA positioning. In addition, the non-linear power conversion of energy harvesting (EH) circuits at the WDs and the finite computing capability at the edge server are taken into account. Our objective is to maximize the sum computational rate (SCR) by jointly optimizing the time allocation, positions of MAs, energy beamforming matrix, receive combing vectors, and offloading strategies of WDs. To solve the non-convex problems, efficient alternating optimization (AO) frameworks are proposed. Moreover, we propose a hybrid algorithm of particle swarm optimization with variable local search (PSO-VLS) to solve the sub-problem of MA positioning. Numerical results validate the superiority of exploiting MAs over the fixed-position antennas (FPAs) for enhancing the SCR performance of WP-MEC systems.","sentences":["In this paper, we propose a movable antenna (MA) enhanced scheme for wireless powered mobile edge computing (WP-MEC) system, where the hybrid access point (HAP) equipped with multiple MAs first emits wireless energy to charge wireless devices (WDs), and then receives the offloaded tasks from the WDs for edge computing.","The MAs deployed at the HAP enhance the spatial degrees of freedom (DoFs) by flexibly adjusting the positions of MAs within an available region, thereby improving the efficiency of both downlink wireless energy transfer (WPT) and uplink task offloading.","To balance the performance enhancement against the implementation intricacy, we further propose three types of MA positioning configurations, i.e., dynamic MA positioning, semi-dynamic MA positioning, and static MA positioning.","In addition, the non-linear power conversion of energy harvesting (EH) circuits at the WDs and the finite computing capability at the edge server are taken into account.","Our objective is to maximize the sum computational rate (SCR) by jointly optimizing the time allocation, positions of MAs, energy beamforming matrix, receive combing vectors, and offloading strategies of WDs.","To solve the non-convex problems, efficient alternating optimization (AO) frameworks are proposed.","Moreover, we propose a hybrid algorithm of particle swarm optimization with variable local search (PSO-VLS) to solve the sub-problem of MA positioning.","Numerical results validate the superiority of exploiting MAs over the fixed-position antennas (FPAs) for enhancing the SCR performance of WP-MEC systems."],"url":"http://arxiv.org/abs/2404.18406v1","category":"cs.IT"}
{"created":"2024-04-29 03:21:05","title":"Semantic Line Combination Detector","abstract":"A novel algorithm, called semantic line combination detector (SLCD), to find an optimal combination of semantic lines is proposed in this paper. It processes all lines in each line combination at once to assess the overall harmony of the lines. First, we generate various line combinations from reliable lines. Second, we estimate the score of each line combination and determine the best one. Experimental results demonstrate that the proposed SLCD outperforms existing semantic line detectors on various datasets. Moreover, it is shown that SLCD can be applied effectively to three vision tasks of vanishing point detection, symmetry axis detection, and composition-based image retrieval. Our codes are available at https://github.com/Jinwon-Ko/SLCD.","sentences":["A novel algorithm, called semantic line combination detector (SLCD), to find an optimal combination of semantic lines is proposed in this paper.","It processes all lines in each line combination at once to assess the overall harmony of the lines.","First, we generate various line combinations from reliable lines.","Second, we estimate the score of each line combination and determine the best one.","Experimental results demonstrate that the proposed SLCD outperforms existing semantic line detectors on various datasets.","Moreover, it is shown that SLCD can be applied effectively to three vision tasks of vanishing point detection, symmetry axis detection, and composition-based image retrieval.","Our codes are available at https://github.com/Jinwon-Ko/SLCD."],"url":"http://arxiv.org/abs/2404.18399v1","category":"cs.CV"}
{"created":"2024-04-29 02:57:59","title":"Diffusion Limit with Optimal Convergence Rate of Classical Solutions to the Vlasov-Maxwell-Boltzmann System","abstract":"We study the diffusion limit of the strong solution to the Vlasov-Maxwell-Boltzmann (VMB) system with initial data near a global Maxwellian. By introducing a new decomposition of the solution to identify the essential components for generating the initial layer, we prove the convergence and establish the opitmal convergence rate of the classical solution to the VMB system to the solution of the Navier-Stokes-Maxwell system based on the spectral analysis.","sentences":["We study the diffusion limit of the strong solution to the Vlasov-Maxwell-Boltzmann (VMB) system with initial data near a global Maxwellian.","By introducing a new decomposition of the solution to identify the essential components for generating the initial layer, we prove the convergence and establish the opitmal convergence rate of the classical solution to the VMB system to the solution of the Navier-Stokes-Maxwell system based on the spectral analysis."],"url":"http://arxiv.org/abs/2404.18389v1","category":"math.AP"}
{"created":"2024-04-29 02:46:09","title":"Network Intent Decomposition and Optimization for Energy-Aware Radio Access Network","abstract":"With recent advancements in the sixth generation (6G) communication technologies, more vertical industries have encountered diverse network services. How to reduce energy consumption is critical to meet the expectation of the quality of diverse network services. In particular, the number of base stations in 6G is huge with coupled adjustable network parameters. However, the problem is complex with multiple network objectives and parameters. Network intents are difficult to map to individual network elements and require enhanced automation capabilities. In this paper, we present a network intent decomposition and optimization mechanism in an energy-aware radio access network scenario. By characterizing the intent ontology with a standard template, we present a generic network intent representation framework. Then we propose a novel intent modeling method using Knowledge Acquisition in automated Specification language, which can model the network ontology. To clarify the number and types of network objectives and energy-saving operations, we develop a Softgoal Interdependency Graph-based network intent decomposition model, and thus, a network intent decomposition algorithm is presented. Simulation results demonstrate that the proposed algorithm outperforms without conflict analysis in intent decomposition time. Moreover, we design a deep Q-network-assisted intent optimization scheme to validate the performance gain.","sentences":["With recent advancements in the sixth generation (6G) communication technologies, more vertical industries have encountered diverse network services.","How to reduce energy consumption is critical to meet the expectation of the quality of diverse network services.","In particular, the number of base stations in 6G is huge with coupled adjustable network parameters.","However, the problem is complex with multiple network objectives and parameters.","Network intents are difficult to map to individual network elements and require enhanced automation capabilities.","In this paper, we present a network intent decomposition and optimization mechanism in an energy-aware radio access network scenario.","By characterizing the intent ontology with a standard template, we present a generic network intent representation framework.","Then we propose a novel intent modeling method using Knowledge Acquisition in automated Specification language, which can model the network ontology.","To clarify the number and types of network objectives and energy-saving operations, we develop a Softgoal Interdependency Graph-based network intent decomposition model, and thus, a network intent decomposition algorithm is presented.","Simulation results demonstrate that the proposed algorithm outperforms without conflict analysis in intent decomposition time.","Moreover, we design a deep Q-network-assisted intent optimization scheme to validate the performance gain."],"url":"http://arxiv.org/abs/2404.18386v1","category":"cs.NI"}
{"created":"2024-04-29 02:32:06","title":"Dynamic Global Feedback Stabilization: why do the twist?","abstract":"We investigate global dynamic feedback stabilization from a topological viewpoint. In particular, we consider the general case of dynamic feedback systems, whereby the total space (which includes the state space of the system and of the controller) is a fibre bundle, and derive conditions on the topology of the bundle that are necessary for various notions of global stabilization to hold. This point of view highlight the importance of distinguishing trivial bundles and twisted bundles in the study of global dynamic feedback stabilization, as we show that dynamic feedback defined on a twisted bundle can stabilize systems that dynamic feedback on trivial bundles cannot.","sentences":["We investigate global dynamic feedback stabilization from a topological viewpoint.","In particular, we consider the general case of dynamic feedback systems, whereby the total space (which includes the state space of the system and of the controller) is a fibre bundle, and derive conditions on the topology of the bundle that are necessary for various notions of global stabilization to hold.","This point of view highlight the importance of distinguishing trivial bundles and twisted bundles in the study of global dynamic feedback stabilization, as we show that dynamic feedback defined on a twisted bundle can stabilize systems that dynamic feedback on trivial bundles cannot."],"url":"http://arxiv.org/abs/2404.18380v1","category":"math.OC"}
{"created":"2024-04-29 02:14:45","title":"A SAT Scalpel for Lattice Surgery: Representation and Synthesis of Subroutines for Surface-Code Fault-Tolerant Quantum Computing","abstract":"Quantum error correction is necessary for large-scale quantum computing. A promising quantum error correcting code is the surface code. For this code, fault-tolerant quantum computing (FTQC) can be performed via lattice surgery, i.e., splitting and merging patches of code. Given the frequent use of certain lattice-surgery subroutines (LaS), it becomes crucial to optimize their design in order to minimize the overall spacetime volume of FTQC. In this study, we define the variables to represent LaS and the constraints on these variables. Leveraging this formulation, we develop a synthesizer for LaS, LaSsynth, that encodes a LaS construction problem into a SAT instance, subsequently querying SAT solvers for a solution. Starting from a baseline design, we can gradually invoke the solver with shrinking spacetime volume to derive more compact designs. Due to our foundational formulation and the use of SAT solvers, LaSsynth can exhaustively explore the design space, yielding optimal designs in volume. For example, it achieves 8% and 18% volume reduction respectively over two states-of-the-art human designs for the 15-to-1 T-factory, a bottleneck in FTQC.","sentences":["Quantum error correction is necessary for large-scale quantum computing.","A promising quantum error correcting code is the surface code.","For this code, fault-tolerant quantum computing (FTQC) can be performed via lattice surgery, i.e., splitting and merging patches of code.","Given the frequent use of certain lattice-surgery subroutines (LaS), it becomes crucial to optimize their design in order to minimize the overall spacetime volume of FTQC.","In this study, we define the variables to represent LaS and the constraints on these variables.","Leveraging this formulation, we develop a synthesizer for LaS, LaSsynth, that encodes a LaS construction problem into a SAT instance, subsequently querying SAT solvers for a solution.","Starting from a baseline design, we can gradually invoke the solver with shrinking spacetime volume to derive more compact designs.","Due to our foundational formulation and the use of SAT solvers, LaSsynth can exhaustively explore the design space, yielding optimal designs in volume.","For example, it achieves 8% and 18% volume reduction respectively over two states-of-the-art human designs for the 15-to-1 T-factory, a bottleneck in FTQC."],"url":"http://arxiv.org/abs/2404.18369v1","category":"quant-ph"}
{"created":"2024-04-29 01:54:08","title":"Improving Multi-Instance GPU Efficiency via Sub-Entry Sharing TLB Design","abstract":"NVIDIA's Multi-Instance GPU (MIG) technology enables partitioning GPU computing power and memory into separate hardware instances, providing complete isolation including compute resources, caches, and memory. However, prior work identifies that MIG does not extend to partitioning the last-level TLB (i.e., L3 TLB), which remains shared among all instances. To enhance TLB reach, NVIDIA GPUs reorganized the TLB structure with 16 sub-entries in each L3 TLB entry that have a one-to-one mapping to the address translations for 16 pages of size 64KB located within the same 1MB aligned range. Our comprehensive investigation of address translation efficiency in MIG identifies two main issues caused by L3 TLB sharing interference: (i) it results in performance degradation for co-running applications, and (ii) TLB sub-entries are not fully utilized before eviction. Based on this observation, we propose STAR to improve the utilization of TLB sub-entries through dynamic sharing of TLB entries across multiple base addresses. STAR evaluates TLB entries based on their sub-entry utilization to optimize address translation storage, dynamically adjusting between a shared and non-shared status to cater to current demand. We show that STAR improves overall performance by an average of 30.2% across various multi-tenant workloads.","sentences":["NVIDIA's Multi-Instance GPU (MIG) technology enables partitioning GPU computing power and memory into separate hardware instances, providing complete isolation including compute resources, caches, and memory.","However, prior work identifies that MIG does not extend to partitioning the last-level TLB (i.e., L3 TLB), which remains shared among all instances.","To enhance TLB reach, NVIDIA GPUs reorganized the TLB structure with 16 sub-entries in each L3 TLB entry that have a one-to-one mapping to the address translations for 16 pages of size 64KB located within the same 1MB aligned range.","Our comprehensive investigation of address translation efficiency in MIG identifies two main issues caused by L3 TLB sharing interference: (i) it results in performance degradation for co-running applications, and (ii) TLB sub-entries are not fully utilized before eviction.","Based on this observation, we propose STAR to improve the utilization of TLB sub-entries through dynamic sharing of TLB entries across multiple base addresses.","STAR evaluates TLB entries based on their sub-entry utilization to optimize address translation storage, dynamically adjusting between a shared and non-shared status to cater to current demand.","We show that STAR improves overall performance by an average of 30.2% across various multi-tenant workloads."],"url":"http://arxiv.org/abs/2404.18361v1","category":"cs.DC"}
{"created":"2024-04-29 00:56:48","title":"Parameter optimization of Josephson parametric amplifiers using a heuristic search algorithm for axion haloscope search","abstract":"The cavity haloscope is among the most widely adopted experimental platforms designed to detect dark matter axions with its principle relying on the conversion of axions into microwave photons in the presence of a strong magnetic field. The Josephson parametric amplifier (JPA), known for its quantum-limited noise characteristics, has been incorporated in the detection system to capture the weakly interacting axion signals. However, the performance of the JPA can be influenced by its environment, leading to potential unreliability of a predefined parameter set obtained in a specific laboratory setting. Furthermore, conducting a broadband search requires consecutive characterization of the amplifier across different tuning frequencies. To ensure more reliable measurements, we utilize the Nelder-Mead technique as a numerical search method to dynamically determine the optimal operating conditions. This heuristic search algorithm explores the multidimensional parameter space of the JPA, optimizing critical characteristics such as gain and noise temperature to maximize signal-to-noise ratios for a given experimental setup. Our study presents a comprehensive analysis of the properties of a flux-driven JPA to demonstrate the effectiveness of the algorithm. This approach contributes to ongoing efforts in axion dark matter research by offering an efficient method to enhance axion detection sensitivity through the optimized utilization of JPAs.","sentences":["The cavity haloscope is among the most widely adopted experimental platforms designed to detect dark matter axions with its principle relying on the conversion of axions into microwave photons in the presence of a strong magnetic field.","The Josephson parametric amplifier (JPA), known for its quantum-limited noise characteristics, has been incorporated in the detection system to capture the weakly interacting axion signals.","However, the performance of the JPA can be influenced by its environment, leading to potential unreliability of a predefined parameter set obtained in a specific laboratory setting.","Furthermore, conducting a broadband search requires consecutive characterization of the amplifier across different tuning frequencies.","To ensure more reliable measurements, we utilize the Nelder-Mead technique as a numerical search method to dynamically determine the optimal operating conditions.","This heuristic search algorithm explores the multidimensional parameter space of the JPA, optimizing critical characteristics such as gain and noise temperature to maximize signal-to-noise ratios for a given experimental setup.","Our study presents a comprehensive analysis of the properties of a flux-driven JPA to demonstrate the effectiveness of the algorithm.","This approach contributes to ongoing efforts in axion dark matter research by offering an efficient method to enhance axion detection sensitivity through the optimized utilization of JPAs."],"url":"http://arxiv.org/abs/2404.18345v1","category":"physics.ins-det"}
{"created":"2024-04-29 00:54:38","title":"G-Refine: A General Quality Refiner for Text-to-Image Generation","abstract":"With the evolution of Text-to-Image (T2I) models, the quality defects of AI-Generated Images (AIGIs) pose a significant barrier to their widespread adoption. In terms of both perception and alignment, existing models cannot always guarantee high-quality results. To mitigate this limitation, we introduce G-Refine, a general image quality refiner designed to enhance low-quality images without compromising the integrity of high-quality ones. The model is composed of three interconnected modules: a perception quality indicator, an alignment quality indicator, and a general quality enhancement module. Based on the mechanisms of the Human Visual System (HVS) and syntax trees, the first two indicators can respectively identify the perception and alignment deficiencies, and the last module can apply targeted quality enhancement accordingly. Extensive experimentation reveals that when compared to alternative optimization methods, AIGIs after G-Refine outperform in 10+ quality metrics across 4 databases. This improvement significantly contributes to the practical application of contemporary T2I models, paving the way for their broader adoption. The code will be released on https://github.com/Q-Future/Q-Refine.","sentences":["With the evolution of Text-to-Image (T2I) models, the quality defects of AI-Generated Images (AIGIs) pose a significant barrier to their widespread adoption.","In terms of both perception and alignment, existing models cannot always guarantee high-quality results.","To mitigate this limitation, we introduce G-Refine, a general image quality refiner designed to enhance low-quality images without compromising the integrity of high-quality ones.","The model is composed of three interconnected modules: a perception quality indicator, an alignment quality indicator, and a general quality enhancement module.","Based on the mechanisms of the Human Visual System (HVS) and syntax trees, the first two indicators can respectively identify the perception and alignment deficiencies, and the last module can apply targeted quality enhancement accordingly.","Extensive experimentation reveals that when compared to alternative optimization methods, AIGIs after G-Refine outperform in 10+ quality metrics across 4 databases.","This improvement significantly contributes to the practical application of contemporary T2I models, paving the way for their broader adoption.","The code will be released on https://github.com/Q-Future/Q-Refine."],"url":"http://arxiv.org/abs/2404.18343v1","category":"cs.MM"}
{"created":"2024-04-29 00:35:59","title":"Additive Spanner Lower Bounds with Optimal Inner Graph Structure","abstract":"We construct $n$-node graphs on which any $O(n)$-size spanner has additive error at least $+\\Omega(n^{3/17})$, improving on the previous best lower bound of $\\Omega(n^{1/7})$ [Bodwin-Hoppenworth FOCS '22]. Our construction completes the first two steps of a particular three-step research program, introduced in prior work and overviewed here, aimed at producing tight bounds for the problem by aligning aspects of the upper and lower bound constructions. More specifically, we develop techniques that enable the use of inner graphs in the lower bound framework whose technical properties are provably tight with the corresponding assumptions made in the upper bounds. As an additional application of our techniques, we improve the corresponding lower bound for $O(n)$-size additive emulators to $+\\Omega(n^{1/14})$.","sentences":["We construct $n$-node graphs on which any $O(n)$-size spanner has additive error at least $+\\Omega(n^{3/17})$, improving on the previous best lower bound of $\\Omega(n^{1/7})$ [Bodwin-Hoppenworth FOCS '22].","Our construction completes the first two steps of a particular three-step research program, introduced in prior work and overviewed here, aimed at producing tight bounds for the problem by aligning aspects of the upper and lower bound constructions.","More specifically, we develop techniques that enable the use of inner graphs in the lower bound framework whose technical properties are provably tight with the corresponding assumptions made in the upper bounds.","As an additional application of our techniques, we improve the corresponding lower bound for $O(n)$-size additive emulators to $+\\Omega(n^{1/14})$."],"url":"http://arxiv.org/abs/2404.18337v1","category":"cs.DS"}
{"created":"2024-04-28 22:53:52","title":"Moment-SOS relaxations for moment and tensor recovery problems","abstract":"This paper studies moment and tensor recovery problems whose decomposing vectors are contained in some given semialgebraic sets. We propose Moment-SOS relaxations with generic objectives for recovering moments and tensors, whose decomposition lengths are expected to be low. This kind of problems have broad applications in various tensor decomposition questions. Numerical experiments are provided to demonstrate the efficiency of this approach.","sentences":["This paper studies moment and tensor recovery problems whose decomposing vectors are contained in some given semialgebraic sets.","We propose Moment-SOS relaxations with generic objectives for recovering moments and tensors, whose decomposition lengths are expected to be low.","This kind of problems have broad applications in various tensor decomposition questions.","Numerical experiments are provided to demonstrate the efficiency of this approach."],"url":"http://arxiv.org/abs/2404.18332v1","category":"math.OC"}
{"created":"2024-04-28 22:53:03","title":"Multi-Robot Object SLAM using Distributed Variational Inference","abstract":"Multi-robot simultaneous localization and mapping (SLAM) enables a robot team to achieve coordinated tasks relying on a common map. However, centralized processing of robot observations is undesirable because it creates a single point of failure and requires pre-existing infrastructure and significant multi-hop communication throughput. This paper formulates multi-robot object SLAM as a variational inference problem over a communication graph. We impose a consensus constraint on the objects maintained by different nodes to ensure agreement on a common map. To solve the problem, we develop a distributed mirror descent algorithm with a regularization term enforcing consensus. Using Gaussian distributions in the algorithm, we derive a distributed multi-state constraint Kalman filter (MSCKF) for multi-robot object SLAM. Experiments on real and simulated data show that our method improves the trajectory and object estimates, compared to individual-robot SLAM, while achieving better scaling to large robot teams, compared to centralized multi-robot SLAM. Code is available at https://github.com/intrepidChw/distributed_msckf.","sentences":["Multi-robot simultaneous localization and mapping (SLAM) enables a robot team to achieve coordinated tasks relying on a common map.","However, centralized processing of robot observations is undesirable because it creates a single point of failure and requires pre-existing infrastructure and significant multi-hop communication throughput.","This paper formulates multi-robot object SLAM as a variational inference problem over a communication graph.","We impose a consensus constraint on the objects maintained by different nodes to ensure agreement on a common map.","To solve the problem, we develop a distributed mirror descent algorithm with a regularization term enforcing consensus.","Using Gaussian distributions in the algorithm, we derive a distributed multi-state constraint Kalman filter (MSCKF) for multi-robot object SLAM.","Experiments on real and simulated data show that our method improves the trajectory and object estimates, compared to individual-robot SLAM, while achieving better scaling to large robot teams, compared to centralized multi-robot SLAM.","Code is available at https://github.com/intrepidChw/distributed_msckf."],"url":"http://arxiv.org/abs/2404.18331v1","category":"cs.RO"}
{"created":"2024-04-28 20:52:08","title":"Multi-Link Operation and Wireless Digital Twin to Support Enhanced Roaming in Next-Gen Wi-Fi","abstract":"The next generation of Wi-Fi is meant to achieve ultra-high reliability for wireless communication. Several approaches are available to this extent, some of which are being considered for inclusion in standards specifications, including coordination of access points to reduce interference.   In this paper, we propose a centralized architecture based on digital twins, called WiTwin, with the aim of supporting wireless stations in selecting the optimal association according to a set of parameters. Unlike prior works, we assume that Wi-Fi 7 features like multi-link operation (MLO) are available. Moreover, one of the main goals of this architecture is to preserve communication quality in the presence of mobility, by helping stations to perform reassociation at the right time and in the best way.","sentences":["The next generation of Wi-Fi is meant to achieve ultra-high reliability for wireless communication.","Several approaches are available to this extent, some of which are being considered for inclusion in standards specifications, including coordination of access points to reduce interference.   ","In this paper, we propose a centralized architecture based on digital twins, called WiTwin, with the aim of supporting wireless stations in selecting the optimal association according to a set of parameters.","Unlike prior works, we assume that Wi-Fi 7 features like multi-link operation (MLO) are available.","Moreover, one of the main goals of this architecture is to preserve communication quality in the presence of mobility, by helping stations to perform reassociation at the right time and in the best way."],"url":"http://arxiv.org/abs/2404.18313v1","category":"cs.NI"}
{"created":"2024-04-28 19:57:00","title":"The $\\ell_r$-Levy-Grothendieck problem and $r\\rightarrow p$ norms of Levy matrices","abstract":"Given an $n\\times n$ matrix $A_n$ and $1\\leq r, p \\leq\\infty$, consider the following quadratic optimization problem referred to as the $\\ell_r$-Grothendieck problem: \\begin{align}M_r(A_n)\\coloneqq\\max_{\\boldsymbol{x}\\in\\mathbb{R}^n:\\|\\boldsymbol{x}\\|_r\\leq1}\\boldsymbol{x}^{\\top} A_n \\boldsymbol{x},\\end{align} as well as the $r\\rightarrow p$ operator norm of the matrix $A_n$, defined as \\begin{align}\\|A_n\\|_{r \\rightarrow p}\\coloneqq \\sup _{\\boldsymbol{x}\\in\\mathbb{R}^n:\\|\\boldsymbol{x}\\|_r \\leq 1}\\|A_n \\boldsymbol{x}\\|_p,\\end{align} where $\\|\\boldsymbol{x}\\|_r$ denotes the $\\ell_r$-norm of the vector $\\boldsymbol{x}$. This work analyzes high-dimensional asymptotics of these quantities when $A_n$ are symmetric random matrices with independent and identically distributed heavy-tailed upper-triangular entries with index $\\alpha$. When $1\\leq r\\leq 2$ (respectively, $1\\leq r\\leq p$) and $\\alpha\\in(0,2)$, suitably scaled versions of $M_r(A_n)$ and $\\|A_n\\|_{r\\rightarrow p}$ are shown to converge to a Fr\\'echet distribution as $n\\rightarrow\\infty$. In contrast, when $2< r<\\infty$ (respectively, $1\\leq p< r$), it is shown that there exists $\\alpha_*\\in(1,2)$ such that for every $\\alpha\\in(0,\\alpha_*)$, suitably scaled versions of $M_r(A_n)$ and $\\|A_n\\|_{r\\rightarrow p}$ converge to the power of a stable distribution. Furthermore, it is shown that there exists $\\bar\\alpha_*>\\alpha_*$ such that when $\\alpha\\in(\\alpha_*,\\bar\\alpha_*)$, the latter convergence result holds only when the matrix entries are centered; when the entries have non-zero mean, a different limit arises after additional centering and scaling. As a corollary, these results yield a characterization of the limiting ground state of the Levy spin glass when $\\alpha \\in (0,1)$. The analysis uses a combination of tools from the theory of heavy-tailed distributions, the nonlinear power method and concentration inequalities.","sentences":["Given an $n\\times n$ matrix $A_n$ and $1\\leq r, p \\leq\\infty$, consider the following quadratic optimization problem referred to as the $\\ell_r$-Grothendieck problem: \\begin{align}M_r(A_n)\\coloneqq\\max_{\\boldsymbol{x}\\in\\mathbb{R}^n:\\|\\boldsymbol{x}\\|_r\\leq1}\\boldsymbol{x}^{\\top} A_n \\boldsymbol{x},\\end{align} as well as the $r\\rightarrow p$ operator norm of the matrix $A_n$, defined as \\begin{align}\\|A_n\\|_{r \\rightarrow p}\\coloneqq \\sup _{\\boldsymbol{x}\\in\\mathbb{R}^n:\\|\\boldsymbol{x}\\|_r \\leq 1}\\|A_n \\boldsymbol{x}\\|_p,\\end{align} where $\\|\\boldsymbol{x}\\|_r$ denotes the $\\ell_r$-norm of the vector $\\boldsymbol{x}$. This work analyzes high-dimensional asymptotics of these quantities when $A_n$ are symmetric random matrices with independent and identically distributed heavy-tailed upper-triangular entries with index $\\alpha$. When $1\\leq r\\leq 2$ (respectively, $1\\leq r\\leq p$) and $\\alpha\\in(0,2)$, suitably scaled versions of $M_r(A_n)$ and $\\|A_n\\|_{r\\rightarrow p}$ are shown to converge to a Fr\\'echet distribution as $n\\rightarrow\\infty$. In contrast, when $2< r<\\infty$ (respectively, $1\\leq p< r$), it is shown that there exists $\\alpha_*\\in(1,2)$ such that for every $\\alpha\\in(0,\\alpha_*)$, suitably scaled versions of $M_r(A_n)$ and $\\|A_n\\|_{r\\rightarrow p}$ converge to the power of a stable distribution.","Furthermore, it is shown that there exists $\\bar\\alpha_*>\\alpha_*$ such that when $\\alpha\\in(\\alpha_*,\\bar\\alpha_*)$, the latter convergence result holds only when the matrix entries are centered; when the entries have non-zero mean, a different limit arises after additional centering and scaling.","As a corollary, these results yield a characterization of the limiting ground state of the Levy spin glass when $\\alpha \\in (0,1)$.","The analysis uses a combination of tools from the theory of heavy-tailed distributions, the nonlinear power method and concentration inequalities."],"url":"http://arxiv.org/abs/2404.18299v1","category":"math.PR"}
{"created":"2024-04-28 19:47:24","title":"Coordination Capacity for Classical-Quantum Correlations","abstract":"Network coordination is considered in three basic settings, characterizing the generation of separable and classical-quantum correlations among multiple parties. First, we consider the simulation of a classical-quantum state between two nodes with rate-limited common randomness (CR) and communication. Furthermore, we study the preparation of a separable state between multiple nodes with rate-limited CR and no communication. At last, we consider a broadcast setting, where a sender and two receivers simulate a classical-quantum-quantum state using rate-limited CR and communication. We establish the optimal tradeoff between communication and CR rates in each setting.","sentences":["Network coordination is considered in three basic settings, characterizing the generation of separable and classical-quantum correlations among multiple parties.","First, we consider the simulation of a classical-quantum state between two nodes with rate-limited common randomness (CR) and communication.","Furthermore, we study the preparation of a separable state between multiple nodes with rate-limited CR and no communication.","At last, we consider a broadcast setting, where a sender and two receivers simulate a classical-quantum-quantum state using rate-limited CR and communication.","We establish the optimal tradeoff between communication and CR rates in each setting."],"url":"http://arxiv.org/abs/2404.18297v1","category":"quant-ph"}
{"created":"2024-04-28 19:44:43","title":"Spectral analysis and best decay rate of the wave propagator on the tadpole graph","abstract":"We consider the damped wave semigroup on the tadpole graph ${\\mathcal R}$. We first give a meticulous spectral analysis, followed by a judicious decomposition of the resolvent's kernel. As a consequence, and by showing that the generalized eigenfunctions form a Riesz basis of some subspace of the energy space $\\mathcal{H}$, we establish the exponential decay of the corresponding energy, with the optimal decay rate dictated by the spectral abscissa of the relevant operator.","sentences":["We consider the damped wave semigroup on the tadpole graph ${\\mathcal R}$.","We first give a meticulous spectral analysis, followed by a judicious decomposition of the resolvent's kernel.","As a consequence, and by showing that the generalized eigenfunctions form a Riesz basis of some subspace of the energy space $\\mathcal{H}$, we establish the exponential decay of the corresponding energy, with the optimal decay rate dictated by the spectral abscissa of the relevant operator."],"url":"http://arxiv.org/abs/2404.18295v1","category":"math.AP"}
{"created":"2024-04-28 19:43:21","title":"PSTAIC regularization for 2D spatiotemporal image reconstruction","abstract":"We propose a model for restoration of spatio-temporal TIRF images based on infimal decomposition regularization model named STAIC proposed earlier. We propose to strengthen the STAIC algorithm by enabling it to estimate the relative weights in the regularization term by incorporating it as part of the optimization problem. We also design an iterative scheme which alternatively minimizes the weight and image sub-problems. We demonstrate the restoration quality of this regularization scheme against other restoration models enabled by similar weight estimation schemes.","sentences":["We propose a model for restoration of spatio-temporal TIRF images based on infimal decomposition regularization model named STAIC proposed earlier.","We propose to strengthen the STAIC algorithm by enabling it to estimate the relative weights in the regularization term by incorporating it as part of the optimization problem.","We also design an iterative scheme which alternatively minimizes the weight and image sub-problems.","We demonstrate the restoration quality of this regularization scheme against other restoration models enabled by similar weight estimation schemes."],"url":"http://arxiv.org/abs/2404.18294v1","category":"eess.IV"}
{"created":"2024-04-28 18:51:12","title":"The Gravitational Lensing Imprints of DES Y3 Superstructures on the CMB: A Matched Filtering Approach","abstract":"$ $Low density cosmic voids gravitationally lens the cosmic microwave background (CMB), leaving a negative imprint on the CMB convergence $\\kappa$. This effect provides insight into the distribution of matter within voids, and can also be used to study the growth of structure. We measure this lensing imprint by cross-correlating the Planck CMB lensing convergence map with voids identified in the Dark Energy Survey Year 3 data set, covering approximately 4,200 deg$^2$ of the sky. We use two distinct void-finding algorithms: a 2D void-finder which operates on the projected galaxy density field in thin redshift shells, and a new code, Voxel, which operates on the full 3D map of galaxy positions. We employ an optimal matched filtering method for cross-correlation, using the MICE N-body simulation both to establish the template for the matched filter and to calibrate detection significances. Using the DES Y3 photometric luminous red galaxy sample, we measure $A_\\kappa$, the amplitude of the observed lensing signal relative to the simulation template, obtaining $A_\\kappa = 1.03 \\pm 0.22$ ($4.6\\sigma$ significance) for Voxel and $A_\\kappa = 1.02 \\pm 0.17$ ($5.9\\sigma$ significance) for 2D voids, both consistent with $\\Lambda$CDM expectations. We additionally invert the 2D void-finding process to identify superclusters in the projected density field, for which we measure $A_\\kappa = 0.87 \\pm 0.15$ ($5.9\\sigma$ significance). The leading source of noise in our measurements is Planck noise, implying that future data from the Atacama Cosmology Telescope (ACT), South Pole Telescope (SPT) and CMB-S4 will increase sensitivity and allow for more precise measurements.","sentences":["$ $Low density cosmic voids gravitationally lens the cosmic microwave background (CMB), leaving a negative imprint on the CMB convergence $\\kappa$.","This effect provides insight into the distribution of matter within voids, and can also be used to study the growth of structure.","We measure this lensing imprint by cross-correlating the Planck CMB lensing convergence map with voids identified in the Dark Energy Survey Year 3 data set, covering approximately 4,200 deg$^2$ of the sky.","We use two distinct void-finding algorithms: a 2D void-finder which operates on the projected galaxy density field in thin redshift shells, and a new code, Voxel, which operates on the full 3D map of galaxy positions.","We employ an optimal matched filtering method for cross-correlation, using the MICE N-body simulation both to establish the template for the matched filter and to calibrate detection significances.","Using the DES Y3 photometric luminous red galaxy sample, we measure $A_\\kappa$, the amplitude of the observed lensing signal relative to the simulation template, obtaining $A_\\kappa = 1.03 \\pm 0.22$ ($4.6\\sigma$ significance) for Voxel and $A_\\kappa = 1.02 \\pm 0.17$ ($5.9\\sigma$ significance) for 2D voids, both consistent with $\\Lambda$CDM expectations.","We additionally invert the 2D void-finding process to identify superclusters in the projected density field, for which we measure $A_\\kappa = 0.87 \\pm 0.15$ ($5.9\\sigma$ significance).","The leading source of noise in our measurements is Planck noise, implying that future data from the Atacama Cosmology Telescope (ACT), South Pole Telescope (SPT) and CMB-S4 will increase sensitivity and allow for more precise measurements."],"url":"http://arxiv.org/abs/2404.18278v1","category":"astro-ph.CO"}
{"created":"2024-04-28 18:07:13","title":"Quantum signatures of the mixed classical phase space for three interacting particles in a circular trap","abstract":"We study theoretically two consequences of the mixed classical phase space for three repulsively-interacting bosonic particles in a circular trap. First, we show that the energy levels of the corresponding quantum system are well described by a Berry-Robnik distribution. Second, we identify stationary quantum states whose density is enhanced along the stable classical periodic trajectories, and calculate their energies and wavefunctions using the semiclassical Einstein-Brillouin-Keller (EBK) theory. Our EBK results are in excellent agreement with our full-fledged finite-element numerics. We discuss the impact of discrete symmetries, including bosonic exchange symmetry, on these classically localized states. They are within experimental reach, and occur in the same range of energies as the quantum scar reported in our previous work [Phys. Rev. A 107, 022217 (2023)].","sentences":["We study theoretically two consequences of the mixed classical phase space for three repulsively-interacting bosonic particles in a circular trap.","First, we show that the energy levels of the corresponding quantum system are well described by a Berry-Robnik distribution.","Second, we identify stationary quantum states whose density is enhanced along the stable classical periodic trajectories, and calculate their energies and wavefunctions using the semiclassical Einstein-Brillouin-Keller (EBK) theory.","Our EBK results are in excellent agreement with our full-fledged finite-element numerics.","We discuss the impact of discrete symmetries, including bosonic exchange symmetry, on these classically localized states.","They are within experimental reach, and occur in the same range of energies as the quantum scar reported in our previous work [","Phys. Rev. A 107, 022217 (2023)]."],"url":"http://arxiv.org/abs/2404.18265v1","category":"quant-ph"}
{"created":"2024-04-28 15:42:37","title":"A quantum compiler design method by using linear combinations of permutations","abstract":"A matrix can be converted into a doubly stochastic matrix by using two diagonal matrices. And a doubly stochastic matrix can be written as a sum of permutation matrices. In this paper, we describe a method to write a given generic matrix in terms of quantum gates based on the block encoding.   In particular, we first show how to convert a matrix into doubly stochastic matrices and by using Birkhoff's algorithm, we express that matrix in terms of a linear combination of permutations which can be mapped to quantum circuits. We then discuss a few optimization techniques that can be applied in a possibly future quantum compiler software based on the method described here.","sentences":["A matrix can be converted into a doubly stochastic matrix by using two diagonal matrices.","And a doubly stochastic matrix can be written as a sum of permutation matrices.","In this paper, we describe a method to write a given generic matrix in terms of quantum gates based on the block encoding.   ","In particular, we first show how to convert a matrix into doubly stochastic matrices and by using Birkhoff's algorithm, we express that matrix in terms of a linear combination of permutations which can be mapped to quantum circuits.","We then discuss a few optimization techniques that can be applied in a possibly future quantum compiler software based on the method described here."],"url":"http://arxiv.org/abs/2404.18226v1","category":"quant-ph"}
{"created":"2024-04-28 15:39:02","title":"On the suitability of single-edge notch tension (SENT) testing for assessing hydrogen-assisted cracking susceptibility","abstract":"Combined experiments and computational modelling are used to increase understanding of the suitability of the Single-Edge Notch Tension (SENT) test for assessing hydrogen embrittlement susceptibility. The SENT tests were designed to provide the mode I threshold stress intensity factor ($K_{\\text{th}}$) for hydrogen-assisted cracking of a C110 steel in two corrosive environments. These were accompanied by hydrogen permeation experiments to relate the environments to the absorbed hydrogen concentrations. A coupled phase-field-based deformation-diffusion-fracture model is then employed to simulate the SENT tests, predicting $K_{\\text{th}}$ in good agreement with the experimental results and providing insights into the hydrogen absorption-diffusion-cracking interactions. The suitability of SENT testing and its optimal characteristics (e.g., test duration) are discussed in terms of the various simultaneous active time-dependent phenomena, triaxiality dependencies, and regimes of hydrogen embrittlement susceptibility.","sentences":["Combined experiments and computational modelling are used to increase understanding of the suitability of the Single-Edge Notch Tension (SENT) test for assessing hydrogen embrittlement susceptibility.","The SENT tests were designed to provide the mode I threshold stress intensity factor ($K_{\\text{th}}$) for hydrogen-assisted cracking of a C110 steel in two corrosive environments.","These were accompanied by hydrogen permeation experiments to relate the environments to the absorbed hydrogen concentrations.","A coupled phase-field-based deformation-diffusion-fracture model is then employed to simulate the SENT tests, predicting $K_{\\text{th}}$ in good agreement with the experimental results and providing insights into the hydrogen absorption-diffusion-cracking interactions.","The suitability of SENT testing and its optimal characteristics (e.g., test duration) are discussed in terms of the various simultaneous active time-dependent phenomena, triaxiality dependencies, and regimes of hydrogen embrittlement susceptibility."],"url":"http://arxiv.org/abs/2404.18223v1","category":"cs.CE"}
{"created":"2024-04-28 14:19:52","title":"Power-Efficiency Constraint for Biological rotary Motor Driven by Chemical Gradient","abstract":"Biological motors, as counterpart of heat engines, are driven by a chemical gradient to output mechanical work and play critical roles in biological functions, such as the ATP synthesis. Its thermodynamic efficiency, along with power, are two vital criteria for evaluating the performance of these motors. In this letter, we investigate a model of a microscopic chemical engine and assess the constraint relation between power and efficiency for the cycle with finite operation time. Our model demonstrates a general constraint relation between power and efficiency, and predicts that the efficiency at maximum power in the large friction regime is half of the maximum quasi-static efficiency, i.e., 1/2. These findings shall provide new direction to optimize the operation of microscopic biological motors.","sentences":["Biological motors, as counterpart of heat engines, are driven by a chemical gradient to output mechanical work and play critical roles in biological functions, such as the ATP synthesis.","Its thermodynamic efficiency, along with power, are two vital criteria for evaluating the performance of these motors.","In this letter, we investigate a model of a microscopic chemical engine and assess the constraint relation between power and efficiency for the cycle with finite operation time.","Our model demonstrates a general constraint relation between power and efficiency, and predicts that the efficiency at maximum power in the large friction regime is half of the maximum quasi-static efficiency, i.e., 1/2.","These findings shall provide new direction to optimize the operation of microscopic biological motors."],"url":"http://arxiv.org/abs/2404.18195v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-28 14:18:41","title":"Thermodynamic Stability Versus Chaos Bound Violation in D-dimensional RN Black Holes: Angular Momentum Effects and Phase Transitions","abstract":"We compute the Lyapunov exponents for test particles orbiting in unstable circular trajectories around D-dimensional Reissner-Nordstr\\\"om (RN) black holes, scrutinizing instances of the chaos bound violation. Notably, we discover that an increase in particle angular momentum exacerbates the breach of the chaos bound. Our research centrally investigates the correlation between black hole thermodynamic phase transitions and the breaking of the chaos limit. Findings suggest that the chaos bound can only be transgressed within thermodynamically stable phases of black holes. Specifically, in the four-dimensional scenario, the critical point of the thermodynamic phase transition aligns with the threshold condition that delineates the onset of chaos bound violation. These outcomes underscore a deep-rooted link between the thermodynamic stability of black holes and the constraints imposed by the chaos bound on particle dynamics.","sentences":["We compute the Lyapunov exponents for test particles orbiting in unstable circular trajectories around D-dimensional Reissner-Nordstr\\\"om (RN) black holes, scrutinizing instances of the chaos bound violation.","Notably, we discover that an increase in particle angular momentum exacerbates the breach of the chaos bound.","Our research centrally investigates the correlation between black hole thermodynamic phase transitions and the breaking of the chaos limit.","Findings suggest that the chaos bound can only be transgressed within thermodynamically stable phases of black holes.","Specifically, in the four-dimensional scenario, the critical point of the thermodynamic phase transition aligns with the threshold condition that delineates the onset of chaos bound violation.","These outcomes underscore a deep-rooted link between the thermodynamic stability of black holes and the constraints imposed by the chaos bound on particle dynamics."],"url":"http://arxiv.org/abs/2404.18193v1","category":"hep-th"}
{"created":"2024-04-28 14:14:35","title":"Block-Map-Based Localization in Large-Scale Environment","abstract":"Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments. Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services. To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps. Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information. Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose. Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking. Comparison experiments are performed on publicly available large-scale datasets. Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT and M2DGR.","sentences":["Accurate localization is an essential technology for the flexible navigation of robots in large-scale environments.","Both SLAM-based and map-based localization will increase the computing load due to the increase in map size, which will affect downstream tasks such as robot navigation and services.","To this end, we propose a localization system based on Block Maps (BMs) to reduce the computational load caused by maintaining large-scale maps.","Firstly, we introduce a method for generating block maps and the corresponding switching strategies, ensuring that the robot can estimate the state in large-scale environments by loading local map information.","Secondly, global localization according to Branch-and-Bound Search (BBS) in the 3D map is introduced to provide the initial pose.","Finally, a graph-based optimization method is adopted with a dynamic sliding window that determines what factors are being marginalized whether a robot is exposed to a BM or switching to another one, which maintains the accuracy and efficiency of pose tracking.","Comparison experiments are performed on publicly available large-scale datasets.","Results show that the proposed method can track the robot pose even though the map scale reaches more than 6 kilometers, while efficient and accurate localization is still guaranteed on NCLT and M2DGR."],"url":"http://arxiv.org/abs/2404.18192v1","category":"cs.RO"}
{"created":"2024-04-28 13:58:32","title":"Single-Photon-Subtracted-Squeezed-Vacuum-State Based Postselected Weak Measurement and its Applications","abstract":"In this paper, we study the effects of postselected von Neumann measurement on the nonclassicality of the Single-Photon-Subtracted-Squeezed-Vacuum-State (SPSSVS). We calculate the squeezing effect, Mandel factor, Wigner function, signal-to-noise ratio (SNR)s and state distance function.We found that postselected von Neumann measurement has positive effects on the optimization of SPSSVS. In particular, by properly choosing the anomalous weak value, the nonclassical inherent features of SPSSVS such as squeezing, photon statistics and phase space distribution can be optimized significantly. The advantages of postselected weak measurement on improving the SNR compared to non-postselected measurement scheme is also confirmed. The superiority of SPSSVS based postselected weak measurement in quantum state optimization may have potential applications of in the associated quantum information processing.","sentences":["In this paper, we study the effects of postselected von Neumann measurement on the nonclassicality of the Single-Photon-Subtracted-Squeezed-Vacuum-State (SPSSVS).","We calculate the squeezing effect, Mandel factor, Wigner function, signal-to-noise ratio (SNR)s and state distance function.","We found that postselected von Neumann measurement has positive effects on the optimization of SPSSVS.","In particular, by properly choosing the anomalous weak value, the nonclassical inherent features of SPSSVS such as squeezing, photon statistics and phase space distribution can be optimized significantly.","The advantages of postselected weak measurement on improving the SNR compared to non-postselected measurement scheme is also confirmed.","The superiority of SPSSVS based postselected weak measurement in quantum state optimization may have potential applications of in the associated quantum information processing."],"url":"http://arxiv.org/abs/2404.18189v1","category":"quant-ph"}
{"created":"2024-04-28 13:43:00","title":"Joint Spectrum Partitioning and Power Allocation for Energy Efficient Semi-Integrated Sensing and Communications","abstract":"With spectrum resources becoming congested and the emergence of sensing-enabled wireless applications, conventional resource allocation methods need a revamp to support communications-only, sensing-only, and integrated sensing and communication (ISaC) services together. In this letter, we propose two joint spectrum partitioning (SP) and power allocation (PA) schemes to maximize the aggregate sensing and communication performance as well as corresponding energy efficiency (EE) of a semi-ISaC system that supports all three services in a unified manner. The proposed framework captures the priority of the distinct services, impact of target clutters, power budget and bandwidth constraints, and sensing and communication quality-of-service (QoS) requirements. We reveal that the former problem is jointly convex and the latter is a non-convex problem that can be solved optimally by exploiting fractional and parametric programming techniques. Numerical results verify the effectiveness of proposed schemes and extract novel insights related to the impact of the priority and QoS requirements of distinct services on the performance of semi-ISaC networks.","sentences":["With spectrum resources becoming congested and the emergence of sensing-enabled wireless applications, conventional resource allocation methods need a revamp to support communications-only, sensing-only, and integrated sensing and communication (ISaC) services together.","In this letter, we propose two joint spectrum partitioning (SP) and power allocation (PA) schemes to maximize the aggregate sensing and communication performance as well as corresponding energy efficiency (EE) of a semi-ISaC system that supports all three services in a unified manner.","The proposed framework captures the priority of the distinct services, impact of target clutters, power budget and bandwidth constraints, and sensing and communication quality-of-service (QoS) requirements.","We reveal that the former problem is jointly convex and the latter is a non-convex problem that can be solved optimally by exploiting fractional and parametric programming techniques.","Numerical results verify the effectiveness of proposed schemes and extract novel insights related to the impact of the priority and QoS requirements of distinct services on the performance of semi-ISaC networks."],"url":"http://arxiv.org/abs/2404.18187v1","category":"cs.IT"}
{"created":"2024-04-28 11:16:20","title":"Revisiting Majumdar-Ghosh spin chain model and Max-cut problem using variational quantum algorithms","abstract":"In this work, energy levels of the Majumdar-Ghosh model (MGM) are analyzed up to 15 spins chain in the noisy intermediate-scale quantum framework using noisy simulations. This is a useful model whose exact solution is known for a particular choice of interaction coefficients. We have solved this model for interaction coefficients other than that required for the exactly solvable conditions as this solution can be of help in understanding the quantum phase transitions in complex spin chain models. The solutions are obtained using quantum approximate optimization algorithms (QAOA), and variational quantum eigensolver (VQE). To obtain the solutions, the one-dimensional lattice network is mapped to a Hamiltonian that corresponds to the required interaction coefficients among spins. Then, the ground states energy eigenvalue of this Hamiltonian is found using QAOA and VQE. Further, the validity of the Lieb-Schultz-Mattis theorem in the context of MGM is established by employing variational quantum deflation to find the first excited energy of MGM. Solution for an unweighted Max-cut graph for 17 nodes is also obtained using QAOA and VQE to know which one of these two techniques performs better in a combinatorial optimization problem. Since the variational quantum algorithms used here to revisit the Max-cut problem and MGM are hybrid algorithms, they require classical optimization. Consequently, the results obtained using different types of classical optimizers are compared to reveal that the QNSPSA optimizer improves the convergence of QAOA in comparison to the SPSA optimizer. However, VQE with EfficientSU2 ansatz using the SPSA optimizer yields the best results.","sentences":["In this work, energy levels of the Majumdar-Ghosh model (MGM) are analyzed up to 15 spins chain in the noisy intermediate-scale quantum framework using noisy simulations.","This is a useful model whose exact solution is known for a particular choice of interaction coefficients.","We have solved this model for interaction coefficients other than that required for the exactly solvable conditions as this solution can be of help in understanding the quantum phase transitions in complex spin chain models.","The solutions are obtained using quantum approximate optimization algorithms (QAOA), and variational quantum eigensolver (VQE).","To obtain the solutions, the one-dimensional lattice network is mapped to a Hamiltonian that corresponds to the required interaction coefficients among spins.","Then, the ground states energy eigenvalue of this Hamiltonian is found using QAOA and VQE.","Further, the validity of the Lieb-Schultz-Mattis theorem in the context of MGM is established by employing variational quantum deflation to find the first excited energy of MGM.","Solution for an unweighted Max-cut graph for 17 nodes is also obtained using QAOA and VQE to know which one of these two techniques performs better in a combinatorial optimization problem.","Since the variational quantum algorithms used here to revisit the Max-cut problem and MGM are hybrid algorithms, they require classical optimization.","Consequently, the results obtained using different types of classical optimizers are compared to reveal that the QNSPSA optimizer improves the convergence of QAOA in comparison to the SPSA optimizer.","However, VQE with EfficientSU2 ansatz using the SPSA optimizer yields the best results."],"url":"http://arxiv.org/abs/2404.18142v1","category":"quant-ph"}
{"created":"2024-04-28 09:07:28","title":"Research on the Evaluation Index System of Enterprise Production Efficiency","abstract":"This paper focuses on studying the evaluation index system for the production efficiency of tobacco enterprises. Considering the limitations of existing evaluation methods in accurately assessing the production quality of cigarette enterprises, a mathematical model based on the Analytic Hierarchy Process (AHP) is established. This model constructs an evaluation framework for the production efficiency of cigarette enterprises and subsequently analyzes the significance of each index within this framework. To comprehensively analyze the multi-index and feasibility aspects of the selected projects, the AHP method is employed to establish a comprehensive feasibility research and evaluation structure model. The result of this feasibility study provides the conclusion that the construction of an evaluation index system for the production efficiency of cigarette enterprises can indeed promote the enhancement of their production efficiency.","sentences":["This paper focuses on studying the evaluation index system for the production efficiency of tobacco enterprises.","Considering the limitations of existing evaluation methods in accurately assessing the production quality of cigarette enterprises, a mathematical model based on the Analytic Hierarchy Process (AHP) is established.","This model constructs an evaluation framework for the production efficiency of cigarette enterprises and subsequently analyzes the significance of each index within this framework.","To comprehensively analyze the multi-index and feasibility aspects of the selected projects, the AHP method is employed to establish a comprehensive feasibility research and evaluation structure model.","The result of this feasibility study provides the conclusion that the construction of an evaluation index system for the production efficiency of cigarette enterprises can indeed promote the enhancement of their production efficiency."],"url":"http://arxiv.org/abs/2404.18121v1","category":"math.OC"}
{"created":"2024-04-28 07:47:45","title":"Tightly-Coupled VLP/INS Integrated Navigation by Inclination Estimation and Blockage Handling","abstract":"Visible Light Positioning (VLP) has emerged as a promising technology capable of delivering indoor localization with high accuracy. In VLP systems that use Photodiodes (PDs) as light receivers, the Received Signal Strength (RSS) is affected by the incidence angle of light, making the inclination of PDs a critical parameter in the positioning model. Currently, most studies assume the inclination to be constant, limiting the applications and positioning accuracy. Additionally, light blockages may severely interfere with the RSS measurements but the literature has not explored blockage detection in real-world experiments. To address these problems, we propose a tightly coupled VLP/INS (Inertial Navigation System) integrated navigation system that uses graph optimization to account for varying PD inclinations and VLP blockages. We also discussed the possibility of simultaneously estimating the robot's pose and the locations of some unknown LEDs. Simulations and two groups of real-world experiments demonstrate the efficiency of our approach, achieving an average positioning accuracy of 10 cm during movement and inclination accuracy within 1 degree despite inclination changes and blockages.","sentences":["Visible Light Positioning (VLP) has emerged as a promising technology capable of delivering indoor localization with high accuracy.","In VLP systems that use Photodiodes (PDs) as light receivers, the Received Signal Strength (RSS) is affected by the incidence angle of light, making the inclination of PDs a critical parameter in the positioning model.","Currently, most studies assume the inclination to be constant, limiting the applications and positioning accuracy.","Additionally, light blockages may severely interfere with the RSS measurements but the literature has not explored blockage detection in real-world experiments.","To address these problems, we propose a tightly coupled VLP/INS (Inertial Navigation System) integrated navigation system that uses graph optimization to account for varying PD inclinations and VLP blockages.","We also discussed the possibility of simultaneously estimating the robot's pose and the locations of some unknown LEDs.","Simulations and two groups of real-world experiments demonstrate the efficiency of our approach, achieving an average positioning accuracy of 10 cm during movement and inclination accuracy within 1 degree despite inclination changes and blockages."],"url":"http://arxiv.org/abs/2404.18105v1","category":"cs.RO"}
{"created":"2024-04-28 07:07:34","title":"Approximations of Rockafellians, Lagrangians, and Dual Functions","abstract":"Solutions of an optimization problem are sensitive to changes caused by approximations or parametric perturbations, especially in the nonconvex setting. This paper investigates the ability of substitute problems, constructed from Rockafellian functions, to provide robustness against such perturbations. Unlike classical stability analysis focused on local changes around (local) minimizers, we employ epi-convergence to examine whether the approximating problems suitably approach the actual one globally. We show that under natural assumptions the substitute problems can be well-behaved in the sense of epi-convergence even though the actual one is not. We further quantify the rates of convergence that often lead to Lipschitz-kind stability properties for the substitute problems.","sentences":["Solutions of an optimization problem are sensitive to changes caused by approximations or parametric perturbations, especially in the nonconvex setting.","This paper investigates the ability of substitute problems, constructed from Rockafellian functions, to provide robustness against such perturbations.","Unlike classical stability analysis focused on local changes around (local) minimizers, we employ epi-convergence to examine whether the approximating problems suitably approach the actual one globally.","We show that under natural assumptions the substitute problems can be well-behaved in the sense of epi-convergence even though the actual one is not.","We further quantify the rates of convergence that often lead to Lipschitz-kind stability properties for the substitute problems."],"url":"http://arxiv.org/abs/2404.18097v1","category":"math.OC"}
{"created":"2024-04-28 06:44:17","title":"Contaminated Online Convex Optimization","abstract":"In the field of online convex optimization, some efficient algorithms have been designed for each of the individual classes of objective functions, e.g., convex, strongly convex, and exp-concave. However, existing regret analyses, including those of universal algorithms, are limited to cases in which the objective functions in all rounds belong to the same class, and cannot be applied to cases in which the property of objective functions may change in each time step. To address such cases, this paper proposes a new regime which we refer to as \\textit{contaminated} online convex optimization. In the contaminated case, regret is bounded by $O(\\log T+\\sqrt{k\\log T})$ when some universal algorithms are used, and bounded by $O(\\log T+\\sqrt{k})$ when our proposed algorithms are used, where $k$ represents how contaminated objective functions are. We also present a matching lower bound of $\\Omega(\\log T + \\sqrt{k})$. These are intermediate bounds between a convex case and a strongly convex or exp-concave case.","sentences":["In the field of online convex optimization, some efficient algorithms have been designed for each of the individual classes of objective functions, e.g., convex, strongly convex, and exp-concave.","However, existing regret analyses, including those of universal algorithms, are limited to cases in which the objective functions in all rounds belong to the same class, and cannot be applied to cases in which the property of objective functions may change in each time step.","To address such cases, this paper proposes a new regime which we refer to as \\textit{contaminated} online convex optimization.","In the contaminated case, regret is bounded by $O(\\log T+\\sqrt{k\\log T})$ when some universal algorithms are used, and bounded by $O(\\log T+\\sqrt{k})$ when our proposed algorithms are used, where $k$ represents how contaminated objective functions are.","We also present a matching lower bound of $\\Omega(\\log T +","\\sqrt{k})$. These are intermediate bounds between a convex case and a strongly convex or exp-concave case."],"url":"http://arxiv.org/abs/2404.18093v1","category":"math.OC"}
{"created":"2024-04-28 05:26:12","title":"Can Perplexity Predict Fine-Tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali","abstract":"Recent language models use subwording mechanisms to handle Out-of-Vocabulary(OOV) words seen during test time and, their generation capacity is generally measured using perplexity, an intrinsic metric. It is known that increasing the subword granularity results in a decrease of perplexity value. However, the study of how subwording affects the understanding capacity of language models has been very few and only limited to a handful of languages. To reduce this gap we used 6 different tokenization schemes to pretrain relatively small language models in Nepali and used the representations learned to finetune on several downstream tasks. Although byte-level BPE algorithm has been used in recent models like GPT, RoBERTa we show that on average they are sub-optimal in comparison to algorithms such as SentencePiece in finetuning performances for Nepali. Additionally, similar recent studies have focused on the Bert-based language model. We, however, pretrain and finetune sequential transformer-based language models.","sentences":["Recent language models use subwording mechanisms to handle Out-of-Vocabulary(OOV) words seen during test time and, their generation capacity is generally measured using perplexity, an intrinsic metric.","It is known that increasing the subword granularity results in a decrease of perplexity value.","However, the study of how subwording affects the understanding capacity of language models has been very few and only limited to a handful of languages.","To reduce this gap we used 6 different tokenization schemes to pretrain relatively small language models in Nepali and used the representations learned to finetune on several downstream tasks.","Although byte-level BPE algorithm has been used in recent models like GPT, RoBERTa we show that on average they are sub-optimal in comparison to algorithms such as SentencePiece in finetuning performances for Nepali.","Additionally, similar recent studies have focused on the Bert-based language model.","We, however, pretrain and finetune sequential transformer-based language models."],"url":"http://arxiv.org/abs/2404.18071v1","category":"cs.CL"}
{"created":"2024-04-28 00:58:28","title":"Variational Optimization for Quantum Problems using Deep Generative Networks","abstract":"Optimization is one of the keystones of modern science and engineering. Its applications in quantum technology and machine learning helped nurture variational quantum algorithms and generative AI respectively. We propose a general approach to design variational optimization algorithms based on generative models: the Variational Generative Optimization Network (VGON). To demonstrate its broad applicability, we apply VGON to three quantum tasks: finding the best state in an entanglement-detection protocol, finding the ground state of a 1D quantum spin model with variational quantum circuits, and generating degenerate ground states of many-body quantum Hamiltonians. For the first task, VGON greatly reduces the optimization time compared to stochastic gradient descent while generating nearly optimal quantum states. For the second task, VGON alleviates the barren plateau problem in variational quantum circuits. For the final task, VGON can identify the degenerate ground state spaces after a single stage of training and generate a variety of states therein.","sentences":["Optimization is one of the keystones of modern science and engineering.","Its applications in quantum technology and machine learning helped nurture variational quantum algorithms and generative AI respectively.","We propose a general approach to design variational optimization algorithms based on generative models: the Variational Generative Optimization Network (VGON).","To demonstrate its broad applicability, we apply VGON to three quantum tasks: finding the best state in an entanglement-detection protocol, finding the ground state of a 1D quantum spin model with variational quantum circuits, and generating degenerate ground states of many-body quantum Hamiltonians.","For the first task, VGON greatly reduces the optimization time compared to stochastic gradient descent while generating nearly optimal quantum states.","For the second task, VGON alleviates the barren plateau problem in variational quantum circuits.","For the final task, VGON can identify the degenerate ground state spaces after a single stage of training and generate a variety of states therein."],"url":"http://arxiv.org/abs/2404.18041v1","category":"quant-ph"}
{"created":"2024-04-28 00:35:38","title":"Fast Monte Carlo Analysis for 6-DoF Powered-Descent Guidance via GPU-Accelerated Sequential Convex Programming","abstract":"We introduce a GPU-accelerated Monte Carlo framework for nonconvex, free-final-time trajectory optimization problems. This framework makes use of the prox-linear method, which belongs to the larger family of sequential convex programming (SCP) algorithms, in conjunction with a constraint reformulation that guarantees inter-sample constraint satisfaction. Key features of this framework are: (1) continuous-time constraint satisfaction; (2) a matrix-inverse-free solution method; (3) the use of the proportional-integral projected gradient (PIPG) method, a first-order convex optimization solver, customized to the convex subproblem at hand; and, (4) an end-to-end, library-free implementation of the algorithm. We demonstrate this GPU-based framework on the 6-DoF powered-descent guidance problem, and show that it is faster than an equivalent serial CPU implementation for Monte Carlo simulations with over 1000 runs. To the best of our knowledge, this is the first GPU-based implementation of a general-purpose nonconvex trajectory optimization solver.","sentences":["We introduce a GPU-accelerated Monte Carlo framework for nonconvex, free-final-time trajectory optimization problems.","This framework makes use of the prox-linear method, which belongs to the larger family of sequential convex programming (SCP) algorithms, in conjunction with a constraint reformulation that guarantees inter-sample constraint satisfaction.","Key features of this framework are: (1) continuous-time constraint satisfaction; (2) a matrix-inverse-free solution method; (3) the use of the proportional-integral projected gradient (PIPG) method, a first-order convex optimization solver, customized to the convex subproblem at hand; and, (4) an end-to-end, library-free implementation of the algorithm.","We demonstrate this GPU-based framework on the 6-DoF powered-descent guidance problem, and show that it is faster than an equivalent serial CPU implementation for Monte Carlo simulations with over 1000 runs.","To the best of our knowledge, this is the first GPU-based implementation of a general-purpose nonconvex trajectory optimization solver."],"url":"http://arxiv.org/abs/2404.18034v1","category":"math.OC"}
{"created":"2024-04-29 17:29:31","title":"Comprehensive Synthesis of Magnetic Tornado: Co-spatial Incidence of Chromospheric Swirls and EUV Brightening","abstract":"Magnetic tornadoes, characterized as impulsive Alfven waves initiated by photospheric vortices in intergranular lanes, are considered efficient energy channels to the corona. Despite their acknowledged importance for solar coronal heating, their observational counterparts from the corona have not been well understood. To address this issue, we use a radiative MHD simulation of a coronal loop with footpoints rooted in the upper convection zone, and synthesize the chromospheric and coronal emissions corresponding to a magnetic tornado. Considering SDO/AIA 171 A and Solar Orbiter/EUI 174 A channels, our synthesis reveals that the coronal response to magnetic tornadoes can be observed as an EUV brightening of which width is ~2 Mm. This brightening is located above the synthesized chromospheric swirl observed in Ca II 8542 A, Ca II K, and Mg II k lines, which can be detected by instruments such as SST/CRISP, GST/FISS, and IRIS. Considering the height correspondence of the synthesized brightening, magnetic tornadoes can be an alternative mechanism for the small-scale EUV brightenings such as the solar \"campfires''. Our findings indicate that coordinated observations encompassing the chromosphere to the corona are indispensable for comprehending the origin of coronal EUV brightenings.","sentences":["Magnetic tornadoes, characterized as impulsive Alfven waves initiated by photospheric vortices in intergranular lanes, are considered efficient energy channels to the corona.","Despite their acknowledged importance for solar coronal heating, their observational counterparts from the corona have not been well understood.","To address this issue, we use a radiative MHD simulation of a coronal loop with footpoints rooted in the upper convection zone, and synthesize the chromospheric and coronal emissions corresponding to a magnetic tornado.","Considering SDO/AIA 171 A and Solar Orbiter/EUI 174 A channels, our synthesis reveals that the coronal response to magnetic tornadoes can be observed as an EUV brightening of which width is ~2","Mm.","This brightening is located above the synthesized chromospheric swirl observed in Ca II 8542 A, Ca II K, and Mg II k lines, which can be detected by instruments such as SST/CRISP, GST/FISS, and IRIS.","Considering the height correspondence of the synthesized brightening, magnetic tornadoes can be an alternative mechanism for the small-scale EUV brightenings such as the solar \"campfires''.","Our findings indicate that coordinated observations encompassing the chromosphere to the corona are indispensable for comprehending the origin of coronal EUV brightenings."],"url":"http://arxiv.org/abs/2404.18892v1","category":"astro-ph.SR"}
{"created":"2024-04-29 17:04:33","title":"Twist analysis of the spin-orbit correlation in QCD","abstract":"We present a QCD analysis of the twist-three parton distribution functions associated with the spin-orbit correlation of quarks and gluons in the nucleon. We derive a novel longitudinal momentum sum rule which may be regarded as the momentum version of the Jaffe-Manohar spin sum rule. The result is also applicable to spinless hadrons and nuclei with trivial modifications.","sentences":["We present a QCD analysis of the twist-three parton distribution functions associated with the spin-orbit correlation of quarks and gluons in the nucleon.","We derive a novel longitudinal momentum sum rule which may be regarded as the momentum version of the Jaffe-Manohar spin sum rule.","The result is also applicable to spinless hadrons and nuclei with trivial modifications."],"url":"http://arxiv.org/abs/2404.18872v1","category":"hep-ph"}
{"created":"2024-04-29 16:50:14","title":"Correlated Electron Effects in Chromium Trihalide Hetostructures with Graphene: A Tight-Binding Model Perspective","abstract":"In this study, we present an effective tight-binding model for an accurate description of the lowest energy quadruplet of conduction band in a ferromagnetic CrX$_3$ monolayer, tuned to the complementary \\textit{ab initio} density functional theory simulations. This model, based on a minimum number of chromium orbitals, captures a distinctively flat dispersion in those bands but requires taking into account hoppings beyond nearest neighbours, revealing ligand-mediated electron pathways connecting remote chromium sites. Doping of states in the lowest conduction band of CrX$_3$ requires charge transfer, which, according to recent studies, can occur in graphene(G)/CrX$_3$ heterostructures. Here, we use the detailed description of the lowest conduction band in CrI$_3$ to show that G/CrI$_3$/G and G/CrI$_3$ are type-II heterostructures where light holes in graphene would coexist with heavy electrons in the magnetic layer, where the latter can be characterised by Wigner parameter $r_s\\sim 15-20$ (as estimated for hBN-encapsulated structures).","sentences":["In this study, we present an effective tight-binding model for an accurate description of the lowest energy quadruplet of conduction band in a ferromagnetic CrX$_3$ monolayer, tuned to the complementary \\textit{ab initio} density functional theory simulations.","This model, based on a minimum number of chromium orbitals, captures a distinctively flat dispersion in those bands but requires taking into account hoppings beyond nearest neighbours, revealing ligand-mediated electron pathways connecting remote chromium sites.","Doping of states in the lowest conduction band of CrX$_3$ requires charge transfer, which, according to recent studies, can occur in graphene(G)/CrX$_3$ heterostructures.","Here, we use the detailed description of the lowest conduction band in CrI$_3$ to show that G/CrI$_3$/G and G/CrI$_3$ are type-II heterostructures where light holes in graphene would coexist with heavy electrons in the magnetic layer, where the latter can be characterised by Wigner parameter $r_s\\sim 15-20$ (as estimated for hBN-encapsulated structures)."],"url":"http://arxiv.org/abs/2404.18858v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 16:32:33","title":"$\u03b2$-WLZZ models from $\u03b2$-ensemble integrals directly","abstract":"Recently, we performed a two $\\beta$-ensemble realization of the series of $\\beta$-deformed WLZZ matrix models involving $\\beta$-deformed Harish-Chandra-Itzykson-Zuber integrals. The realization was derived and studied by using Ward identities, which do not allow one to fix integration contours, these latter were chosen to be real axis for one $\\beta$-ensemble and imaginary axis for the other one basing on some particular checks. Here, we evaluate the $\\beta$-ensemble integrals directly using a conjecture by I.G. Macdonald, and explain that another choice of integration contours is also possible.","sentences":["Recently, we performed a two $\\beta$-ensemble realization of the series of $\\beta$-deformed WLZZ matrix models involving $\\beta$-deformed Harish-Chandra-Itzykson-Zuber integrals.","The realization was derived and studied by using Ward identities, which do not allow one to fix integration contours, these latter were chosen to be real axis for one $\\beta$-ensemble and imaginary axis for the other one basing on some particular checks.","Here, we evaluate the $\\beta$-ensemble integrals directly using a conjecture by I.G. Macdonald, and explain that another choice of integration contours is also possible."],"url":"http://arxiv.org/abs/2404.18843v1","category":"hep-th"}
{"created":"2024-04-29 15:23:08","title":"Measurements of the CKM angle $\u03b3$ and parameters related to mixing and CP violation in the charm at LHCb","abstract":"A recent combination of measurements performed by the LHCb collaboration determined that $\\gamma=(63.8^{+3.5}_{-3.7})^\\circ$. The fit combined the results of $\\gamma$ and charm measurements, which resulted in precision improvements for the strong-phase difference between $D^0 \\to K^-\\pi^+$ and $D^0 \\to K^+\\pi^-$ decays, $\\delta_{K\\pi}^{D}$, and the $D^0-\\bar{D}^0$ mixing parameter, $y$. In addition, new LHCb measurements of the CKM angle $\\gamma$ using $B^0 \\to D K^{*0}$ and $B^- \\to D^{*} K^-$ decays with the $D\\to K_{S}^0 h^+ h^-$ final state (where $h=\\pi,K$) are presented.","sentences":["A recent combination of measurements performed by the LHCb collaboration determined that $\\gamma=(63.8^{+3.5}_{-3.7})^\\circ$. The fit combined the results of $\\gamma$ and charm measurements, which resulted in precision improvements for the strong-phase difference between $D^0 \\to K^-\\pi^+$ and $D^0 \\to K^+\\pi^-$ decays, $\\delta_{K\\pi}^{D}$, and the $D^0-\\bar{D}^0$ mixing parameter, $y$. In addition, new LHCb measurements of the CKM angle $\\gamma$ using $B^0 \\to D K^{*0}$ and $B^- \\to D^{*} K^-$ decays with the $D\\to K_{S}^0 h^+ h^-$ final state (where $h=\\pi,K$) are presented."],"url":"http://arxiv.org/abs/2404.18789v1","category":"hep-ex"}
{"created":"2024-04-29 15:15:55","title":"Semiparametric fiducial inference","abstract":"R. A. Fisher introduced the concept of fiducial as a potential replacement for the Bayesian posterior distribution in the 1930s. During the past century, fiducial approaches have been explored in various parametric and nonparametric settings. However, to the best of our knowledge, no fiducial inference has been developed in the realm of semiparametric statistics. In this paper, we propose a novel fiducial approach for semiparametric models. To streamline our presentation, we use the Cox proportional hazards model, which is the most popular model for the analysis of survival data, as a running example. Other models and extensions are also discussed. In our experiments, we find our method to perform well especially in situations when the maximum likelihood estimator fails.","sentences":["R. A. Fisher introduced the concept of fiducial as a potential replacement for the Bayesian posterior distribution in the 1930s.","During the past century, fiducial approaches have been explored in various parametric and nonparametric settings.","However, to the best of our knowledge, no fiducial inference has been developed in the realm of semiparametric statistics.","In this paper, we propose a novel fiducial approach for semiparametric models.","To streamline our presentation, we use the Cox proportional hazards model, which is the most popular model for the analysis of survival data, as a running example.","Other models and extensions are also discussed.","In our experiments, we find our method to perform well especially in situations when the maximum likelihood estimator fails."],"url":"http://arxiv.org/abs/2404.18779v1","category":"stat.ME"}
{"created":"2024-04-29 15:11:05","title":"\"Rosenbluth\" separation of the $J/\u03c8$ near-threshold photoproduction -- an access to the gluon Gravitational Form Factors at high $t$","abstract":"We perform analysis of the near-threshold $J/\\psi $ photoproduction data off the proton based on the high skewness expansion ( $\\xi \\to 1$) of the cross-section in Ref. [1]. This expansion allows to separate the corresponding gluonic form factors, in much the same way as this is done in the electro-magnetic case with the Rosenbluth separation. We examine the independence of the extracted form factors with the photon beam energy. In Ref. [1] these form factors, in the leading-moment approximation, are related to the proton's gluon Gravitational Form Factors (gGFF). Using this approximation, we compare the extracted data points with lattice calculations in the region of momentum transfer squared of $1<|t|<2$ GeV$^{2}$, where the data and lattice results overlap. Such analysis demonstrates the possibility of extracting some combinations of the gGFFs from the data at high $t$, complementary to the lattice calculations available in the low $t$ region. However, higher statistics is needed to more accurately check the predicted $\\xi$-scaling behavior of the data and compare with the lattice results, thus testing the theoretical assumptions used in Ref. [1].","sentences":["We perform analysis of the near-threshold $J/\\psi $ photoproduction data off the proton based on the high skewness expansion ( $\\xi \\to 1$) of the cross-section in Ref.","[1].","This expansion allows to separate the corresponding gluonic form factors, in much the same way as this is done in the electro-magnetic case with the Rosenbluth separation.","We examine the independence of the extracted form factors with the photon beam energy.","In Ref.","[1] these form factors, in the leading-moment approximation, are related to the proton's gluon Gravitational Form Factors (gGFF).","Using this approximation, we compare the extracted data points with lattice calculations in the region of momentum transfer squared of $1<|t|<2$ GeV$^{2}$, where the data and lattice results overlap.","Such analysis demonstrates the possibility of extracting some combinations of the gGFFs from the data at high $t$, complementary to the lattice calculations available in the low $t$ region.","However, higher statistics is needed to more accurately check the predicted $\\xi$-scaling behavior of the data and compare with the lattice results, thus testing the theoretical assumptions used in Ref.","[1]."],"url":"http://arxiv.org/abs/2404.18776v1","category":"nucl-ex"}
{"created":"2024-04-29 14:48:26","title":"$\u03a0^0_4$ conservation of the Ordered Variable Word theorem","abstract":"A left-variable word over an alphabet~$A$ is a word over~$A \\cup \\{\\star\\}$ whose first letter is the distinguished symbol~$\\star$ standing for a placeholder. The Ordered Variable Word theorem ($\\mathsf{OVW}$), also known as Carlson-Simpson's theorem, is a tree partition theorem, stating that for every finite alphabet~$A$ and every finite coloring of the words over~$A$, there exists a word $c_0$ and an infinite sequence of left-variable words $w_1, w_2, \\dots$ such that $\\{ c_0 \\cdot w_1[a_1] \\cdot \\dots \\cdot w_k[a_k] : k \\in \\mathbb{N}, a_1, \\dots, a_k \\in A \\}$ is monochromatic.   In this article, we prove that $\\mathsf{OVW}$ is $\\Pi^0_4$-conservative over~$\\mathsf{RCA}_0 + \\mathsf{B}\\Sigma^0_2$. This implies in particular that $\\mathsf{OVW}$ does not imply $\\mathsf{ACA}_0$ over~$\\mathsf{RCA}_0$. This is the first principle for which the only known separation from~$\\mathsf{ACA}_0$ involves non-standard models.","sentences":["A left-variable word over an alphabet~$A$ is a word over~$A \\cup \\{\\star\\}$ whose first letter is the distinguished symbol~$\\star$ standing for a placeholder.","The Ordered Variable Word theorem ($\\mathsf{OVW}$), also known as Carlson-Simpson's theorem, is a tree partition theorem, stating that for every finite alphabet~$A$ and every finite coloring of the words over~$A$, there exists a word $c_0$ and an infinite sequence of left-variable words $w_1, w_2, \\dots$ such that $\\{ c_0 \\cdot w_1[a_1] \\cdot \\dots \\cdot w_k[a_k] : k \\in \\mathbb{N}, a_1, \\dots, a_k","\\in A \\}$ is monochromatic.   ","In this article, we prove that $\\mathsf{OVW}$ is $\\Pi^0_4$-conservative over~$\\mathsf{RCA}_0 + \\mathsf{B}\\Sigma^0_2$.","This implies in particular that $\\mathsf{OVW}$ does not imply $\\mathsf{ACA}_0$ over~$\\mathsf{RCA}_0$. This is the first principle for which the only known separation from~$\\mathsf{ACA}_0$ involves non-standard models."],"url":"http://arxiv.org/abs/2404.18749v1","category":"math.LO"}
{"created":"2024-04-29 14:07:32","title":"Diffusion coefficient matrix for multiple conserved charges: a Kubo approach","abstract":"The strongly interacting matter created in relativistic heavy-ion collisions possesses several conserved quantum numbers, such as baryon number, strangeness, and electric charge. The diffusion process of these charges can be characterized by a diffusion matrix that describes the mutual influence of the diffusion of various charges. We derive the Kubo relations for evaluating diffusion coefficients as elements of a diffusion matrix. We further demonstrate that in the weak coupling limit, the diffusion matrix elements obtained through Kubo relations reduce to those obtained from kinetic theory with an appropriate identification of the relaxation times. We illustrate this evaluation in a toy model of two interacting scalar fields with two conserved charges.","sentences":["The strongly interacting matter created in relativistic heavy-ion collisions possesses several conserved quantum numbers, such as baryon number, strangeness, and electric charge.","The diffusion process of these charges can be characterized by a diffusion matrix that describes the mutual influence of the diffusion of various charges.","We derive the Kubo relations for evaluating diffusion coefficients as elements of a diffusion matrix.","We further demonstrate that in the weak coupling limit, the diffusion matrix elements obtained through Kubo relations reduce to those obtained from kinetic theory with an appropriate identification of the relaxation times.","We illustrate this evaluation in a toy model of two interacting scalar fields with two conserved charges."],"url":"http://arxiv.org/abs/2404.18718v1","category":"hep-ph"}
{"created":"2024-04-29 14:05:27","title":"Electrically tunable layer-hybridized trions in doped WSe$_2$ bilayers","abstract":"Doped van der Waals heterostructures host layer-hybridized trions, i.e. charged excitons with layer-delocalized constituents holding promise for highly controllable optoelectronics. Combining a microscopic theory with photoluminescence (PL) experiments, we demonstrate the electrical tunability of the trion energy landscape in naturally stacked WSe$_2$ bilayers. We show that an out-of-plane electric field modifies the energetic ordering of the lowest lying trion states, which consist of layer-hybridized $\\Lambda$-point electrons and layer-localized K-point holes. At small fields, intralayer-like trions yield distinct PL signatures in opposite doping regimes characterized by weak Stark shifts in both cases. Above a doping-asymmetric critical field, interlayer-like species are energetically favored and produce PL peaks with a pronounced Stark red-shift and a counter-intuitively large intensity arising from efficient phonon-assisted recombination. Our work presents an important step forward in the microscopic understanding of layer-hybridized trions in van der Waals heterostructures and paves the way towards optoelectronic applications based on electrically controllable atomically-thin semiconductors.","sentences":["Doped van der Waals heterostructures host layer-hybridized trions, i.e. charged excitons with layer-delocalized constituents holding promise for highly controllable optoelectronics.","Combining a microscopic theory with photoluminescence (PL) experiments, we demonstrate the electrical tunability of the trion energy landscape in naturally stacked WSe$_2$ bilayers.","We show that an out-of-plane electric field modifies the energetic ordering of the lowest lying trion states, which consist of layer-hybridized $\\Lambda$-point electrons and layer-localized K-point holes.","At small fields, intralayer-like trions yield distinct PL signatures in opposite doping regimes characterized by weak Stark shifts in both cases.","Above a doping-asymmetric critical field, interlayer-like species are energetically favored and produce PL peaks with a pronounced Stark red-shift and a counter-intuitively large intensity arising from efficient phonon-assisted recombination.","Our work presents an important step forward in the microscopic understanding of layer-hybridized trions in van der Waals heterostructures and paves the way towards optoelectronic applications based on electrically controllable atomically-thin semiconductors."],"url":"http://arxiv.org/abs/2404.18716v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-29 13:59:56","title":"Classical Casimir pressure in the presence of axion dark matter","abstract":"We study the effects of an oscillating axion field on the pressure between two metallic plates. We consider the situation where a magnetic field parallel to the plates is present and show that the electric field induced by the coupling of the axion to photons leads to resonances. When the boundary plates are perfect conductors, the resonances are infinitely thin whilst they are broadened when the conductivity of the boundary plates is taken into account. The resonances take place at the tower of distances close to dn = (2n+1){\\pi}/m where m is the axion mass and have a finite width and height depending on the conductivity. The resulting resonant pressure on the plate depends on the induced polarisation at the surface of the plates. We investigate the reach of future Casimir experiments in terms of the axion mass and the conductivity of the boundary plates. We find that for large enough conductivities, the axion-induced pressure could be larger than the quantum Casimir effect between the plates.","sentences":["We study the effects of an oscillating axion field on the pressure between two metallic plates.","We consider the situation where a magnetic field parallel to the plates is present and show that the electric field induced by the coupling of the axion to photons leads to resonances.","When the boundary plates are perfect conductors, the resonances are infinitely thin whilst they are broadened when the conductivity of the boundary plates is taken into account.","The resonances take place at the tower of distances close to dn = (2n+1){\\pi}/m where m is the axion mass and have a finite width and height depending on the conductivity.","The resulting resonant pressure on the plate depends on the induced polarisation at the surface of the plates.","We investigate the reach of future Casimir experiments in terms of the axion mass and the conductivity of the boundary plates.","We find that for large enough conductivities, the axion-induced pressure could be larger than the quantum Casimir effect between the plates."],"url":"http://arxiv.org/abs/2404.18710v1","category":"hep-ph"}
{"created":"2024-04-29 13:54:51","title":"Gravitational wave detection of DFSZ axion models","abstract":"As a promising dark matter candidate, the axion particle can naturally solve the strong CP problem of the standard model through the U(1) Peccei-Quinn symmetry-breaking process. However, detecting this high-energy process poses a significant challenge for colliders. We precisely calculate the phase transition gravitational wave signals of this symmetry-breaking process in the popular DFSZ axion model. By comparing the signals with the expected sensitivity of Cosmic Explorer and Einstein Telescope, we demonstrate that Cosmic Explorer could detect this process with a signal-to-noise ratio higher than 8 for the benchmark points. Furthermore, by performing Fisher analysis, we find that if signals are observed, the bubble wall velocity will be the first phase transition parameter to be determined. The future gravitational wave detectors are expected to provide a new approach to exploring concrete axion models.","sentences":["As a promising dark matter candidate, the axion particle can naturally solve the strong CP problem of the standard model through the U(1) Peccei-Quinn symmetry-breaking process.","However, detecting this high-energy process poses a significant challenge for colliders.","We precisely calculate the phase transition gravitational wave signals of this symmetry-breaking process in the popular DFSZ axion model.","By comparing the signals with the expected sensitivity of Cosmic Explorer and Einstein Telescope, we demonstrate that Cosmic Explorer could detect this process with a signal-to-noise ratio higher than 8 for the benchmark points.","Furthermore, by performing Fisher analysis, we find that if signals are observed, the bubble wall velocity will be the first phase transition parameter to be determined.","The future gravitational wave detectors are expected to provide a new approach to exploring concrete axion models."],"url":"http://arxiv.org/abs/2404.18703v1","category":"hep-ph"}
{"created":"2024-04-29 12:14:54","title":"Citizen Science in European Research Infrastructures","abstract":"Major European Union-funded research infrastructure and open science projects have traditionally included dissemination work, for mostly one-way communication of the research activities. Here we present and review our radical re-envisioning of this work, by directly engaging citizen science volunteers into the research. We summarise the citizen science in the Horizon-funded projects ASTERICS (Astronomy ESFRI and Research Infrastructure Clusters) and ESCAPE (European Science Cluster of Astronomy and Particle Physics ESFRI Research Infrastructures), engaging hundreds of thousands of volunteers in providing millions of data mining classifications. Not only does this have enormously more scientific and societal impact than conventional dissemination, but it facilitates the direct research involvement of what is often arguably the most neglected stakeholder group in Horizon projects, the science-inclined public. We conclude with recommendations and opportunities for deploying crowdsourced data mining in the physical sciences, noting that the primary goal is always the fundamental research question; if public engagement is the primary goal to optimise, then other, more targeted approaches may be more effective.","sentences":["Major European Union-funded research infrastructure and open science projects have traditionally included dissemination work, for mostly one-way communication of the research activities.","Here we present and review our radical re-envisioning of this work, by directly engaging citizen science volunteers into the research.","We summarise the citizen science in the Horizon-funded projects ASTERICS (Astronomy ESFRI and Research Infrastructure Clusters) and ESCAPE (European Science Cluster of Astronomy and Particle Physics ESFRI Research Infrastructures), engaging hundreds of thousands of volunteers in providing millions of data mining classifications.","Not only does this have enormously more scientific and societal impact than conventional dissemination, but it facilitates the direct research involvement of what is often arguably the most neglected stakeholder group in Horizon projects, the science-inclined public.","We conclude with recommendations and opportunities for deploying crowdsourced data mining in the physical sciences, noting that the primary goal is always the fundamental research question; if public engagement is the primary goal to optimise, then other, more targeted approaches may be more effective."],"url":"http://arxiv.org/abs/2404.18635v1","category":"astro-ph.IM"}
{"created":"2024-04-29 11:43:44","title":"Elliptic Sombor energy of a graph","abstract":"Let $G$ be a simple graph with vertex set $V(G) = \\{v_1, v_2,\\ldots, v_n\\}$. The elliptic Sombor matrix of $G$, denoted by $A_{ESO}(G)$, is defined as the $n\\times n$ matrix whose $(i,j)$-entry is $(d_i+d_j)\\sqrt{d_i^2+d_j^2}$ if $v_i$ and $v_j$ are adjacent and $0$ for another cases. Let the eigenvalues of the elliptic Sombor matrix $A_{ESO}(G)$ be $\\rho_1\\geq \\rho_2\\geq \\ldots\\geq \\rho_n$ which are the roots of the elliptic Sombor characteristic polynomial $\\prod_{i=1}^n (\\rho-\\rho_i)$. The elliptic Sombor energy ${E_{ESO}}$ of $G$ is the sum of absolute values of the eigenvalues of $A_{ESO}(G)$. In this paper, we compute the elliptic Sombor characteristic polynomial and the elliptic Sombor energy for some graph classes. We compute the elliptic Sombor energy of cubic graphs of order $10$ and as a consequence, we see that two $k$-regular graphs of the same order may have different elliptic Sombor energy.","sentences":["Let $G$ be a simple graph with vertex set $V(G) = \\{v_1, v_2,\\ldots,","v_n\\}$.","The elliptic Sombor matrix of $G$, denoted by $A_{ESO}(G)$, is defined as the $n\\times n$ matrix whose $(i,j)$-entry is $(d_i+d_j)\\sqrt{d_i^2+d_j^2}$ if $v_i$ and $v_j$ are adjacent and $0$ for another cases.","Let the eigenvalues of the elliptic Sombor matrix $A_{ESO}(G)$ be $\\rho_1\\geq \\rho_2\\geq \\ldots\\geq \\rho_n$ which are the roots of the elliptic Sombor characteristic polynomial $\\prod_{i=1}^n (\\rho-\\rho_i)$. The elliptic Sombor energy ${E_{ESO}}$ of $G$ is the sum of absolute values of the eigenvalues of $A_{ESO}(G)$. In this paper, we compute the elliptic Sombor characteristic polynomial and the elliptic Sombor energy for some graph classes.","We compute the elliptic Sombor energy of cubic graphs of order $10$ and as a consequence, we see that two $k$-regular graphs of the same order may have different elliptic Sombor energy."],"url":"http://arxiv.org/abs/2404.18622v1","category":"math.CO"}
{"created":"2024-04-29 11:11:00","title":"The population of Galactic supernova remnants in the TeV range","abstract":"Supernova remnants (SNRs) are likely to be significant sources of cosmic rays up to the knee of the local cosmic-ray (CR) spectrum. They produce gamma-rays in the very-high-energy (VHE) ($E>0.1$ TeV) range via: hadronic interactions with the interstellar medium and leptonic interactions with soft photons. Current observations have lead to the detection of about a dozen of VHE SNRs and future instruments should increase this number. The details of particle acceleration at SNRs, and of the mechanisms producing VHE gamma-rays at SNRs are poorly understood. We aim to study the population of SNRs detected in the TeV range and its properties, and to address fundamental questions of particle acceleration at SNR shocks: What is the spectrum of accelerated particles? What is the efficiency of acceleration? Is the VHE emission dominated by hadronic or leptonic interactions? By means of Monte Carlo methods, we simulate the population of SNRs in the VHE domain and confront our simulations to H.E.S.S. Galactic Plane Survey (HGPS). We explore the parameter space: the slope of accelerated particles $\\alpha$, the electron-to-proton ratio $K_{\\rm ep}$, and the efficiency of particle acceleration $\\xi $. We found sets of parameters for which $\\gtrsim 90$\\% of realisations are found in agreements with the HGPS data. These parameters are found $ 4.2 \\gtrsim \\alpha \\gtrsim 4.1 $, $10^{-5} \\lesssim K_{\\rm ep} \\lesssim 10^{-4.5}$, and $0.03 \\lesssim \\xi\\lesssim 0.1 $ . We were able to strongly argue against some regions of the parameter space: $\\alpha \\lesssim 4.05$, $\\alpha \\gtrsim 4.35$, or $K_{\\rm ep} \\gtrsim 10^{-3}$. Our model is so far able to explain the SNR population of the HGPS. Our approach, confronted to the results of future systematic surveys, will help remove degeneracy in the solutions, and to better understand particle acceleration at SNR shocks.","sentences":["Supernova remnants (SNRs) are likely to be significant sources of cosmic rays up to the knee of the local cosmic-ray (CR) spectrum.","They produce gamma-rays in the very-high-energy (VHE) ($E>0.1$ TeV) range via: hadronic interactions with the interstellar medium and leptonic interactions with soft photons.","Current observations have lead to the detection of about a dozen of VHE SNRs and future instruments should increase this number.","The details of particle acceleration at SNRs, and of the mechanisms producing VHE gamma-rays at SNRs are poorly understood.","We aim to study the population of SNRs detected in the TeV range and its properties, and to address fundamental questions of particle acceleration at SNR shocks: What is the spectrum of accelerated particles?","What is the efficiency of acceleration?","Is the VHE emission dominated by hadronic or leptonic interactions?","By means of Monte Carlo methods, we simulate the population of SNRs in the VHE domain and confront our simulations to H.E.S.S. Galactic Plane Survey (HGPS).","We explore the parameter space: the slope of accelerated particles $\\alpha$, the electron-to-proton ratio $K_{\\rm ep}$, and the efficiency of particle acceleration $\\xi $.","We found sets of parameters for which $\\gtrsim 90$\\% of realisations are found in agreements with the HGPS data.","These parameters are found $ 4.2 \\gtrsim \\alpha \\gtrsim 4.1 $, $10^{-5} \\lesssim K_{\\rm ep} \\lesssim 10^{-4.5}$, and $0.03 \\lesssim \\xi\\lesssim 0.1 $ .","We were able to strongly argue against some regions of the parameter space: $\\alpha \\lesssim 4.05$, $\\alpha \\gtrsim 4.35$, or $K_{\\rm ep} \\gtrsim 10^{-3}$.","Our model is so far able to explain the SNR population of the HGPS.","Our approach, confronted to the results of future systematic surveys, will help remove degeneracy in the solutions, and to better understand particle acceleration at SNR shocks."],"url":"http://arxiv.org/abs/2404.18595v1","category":"astro-ph.HE"}
{"created":"2024-04-29 10:11:17","title":"Cool matter distribution in inner solar corona from 2023 total solar eclipse observation","abstract":"Solar corona has been judged to consist of free electrons and highly ionized ions with extremely high temperature as a widely accepted knowledge. This view is changed by our eclipse observations. Distributions of cool matter represented by neutral iron atoms in hot inner solar corona are presented via derived global maps of solar Fraunhofer(F-) and Emission(E-) coronae, compared with those of continuum(Kontinuierlich, K-) corona formed by free electrons. The maps are obtained from simultaneous observations of dual filtering bands centered respectively at 659.4nm and 660.1nm, performed from twin telescopes during the total solar eclipse on April 20, 2023 at Com town of East Timor, assisted for judgement via spectral images obtained by a portable spectrograph. They show respectively presences of these neutral iron atoms yielding 659.3nm and 659.4nm lines in both the quiet sun and active regions. The distribution of the cool matter in form of line depression forms an inner F-corona, different from that of the cool matter in form of line enhancement. Both the distributions show a crucial difference from that of the free electrons represented by the K-corona map. It is also found that intensities of the F-corona and the E-corona induced by these neutral atoms are only small fractions of the K-corona, and the diffusion can be seen clearly in all these maps. They uncover also that the coronal heating resources do not distribute pervasively but likely form a thermodynamic griddle where minor photospheric neutral atoms can escape from the heating into the corona globally.","sentences":["Solar corona has been judged to consist of free electrons and highly ionized ions with extremely high temperature as a widely accepted knowledge.","This view is changed by our eclipse observations.","Distributions of cool matter represented by neutral iron atoms in hot inner solar corona are presented via derived global maps of solar Fraunhofer(F-) and Emission(E-) coronae, compared with those of continuum(Kontinuierlich, K-) corona formed by free electrons.","The maps are obtained from simultaneous observations of dual filtering bands centered respectively at 659.4nm and 660.1nm, performed from twin telescopes during the total solar eclipse on April 20, 2023 at Com town of East Timor, assisted for judgement via spectral images obtained by a portable spectrograph.","They show respectively presences of these neutral iron atoms yielding 659.3nm and 659.4nm lines in both the quiet sun and active regions.","The distribution of the cool matter in form of line depression forms an inner F-corona, different from that of the cool matter in form of line enhancement.","Both the distributions show a crucial difference from that of the free electrons represented by the K-corona map.","It is also found that intensities of the F-corona and the E-corona induced by these neutral atoms are only small fractions of the K-corona, and the diffusion can be seen clearly in all these maps.","They uncover also that the coronal heating resources do not distribute pervasively but likely form a thermodynamic griddle where minor photospheric neutral atoms can escape from the heating into the corona globally."],"url":"http://arxiv.org/abs/2404.18563v1","category":"astro-ph.SR"}
{"created":"2024-04-29 10:04:53","title":"Controlling the Flavour Changing Neutral Couplings of Multi-Higgs Doublets Models through Unitary Matrices","abstract":"In this paper, we introduce Unitary Flavour Violation to produce Multi-Higgs Doublets Models where all flavour parameters are contained within three unitary matrices. After that, we identify two of its subclasses, the left and right models, which have naturally suppressed tree-level Flavour Changing Neutral Couplings that easily avoid the experimental constraints derived from neutral meson mixing. Then, we show that left models can accomodate spontaneous CP violation when all quarks have Flavour Changing Neutral Couplings. Finally, we illustrate these concepts by considering a specific implementation with three Higgs doublets.","sentences":["In this paper, we introduce Unitary Flavour Violation to produce Multi-Higgs Doublets Models where all flavour parameters are contained within three unitary matrices.","After that, we identify two of its subclasses, the left and right models, which have naturally suppressed tree-level Flavour Changing Neutral Couplings that easily avoid the experimental constraints derived from neutral meson mixing.","Then, we show that left models can accomodate spontaneous CP violation when all quarks have Flavour Changing Neutral Couplings.","Finally, we illustrate these concepts by considering a specific implementation with three Higgs doublets."],"url":"http://arxiv.org/abs/2404.18559v1","category":"hep-ph"}
{"created":"2024-04-29 09:48:53","title":"Absolute light yield measurement of NaI:Tl crystals for dark matter search","abstract":"The NaI:Tl crystals were early investigated and used for wide application fields due to high light yield and crystal growth advantages. So far, the absolute light yields of NaI:Tl crystal have typically been known to be 40 ph/keV. However, it varies widely, far from the theoretical estimation. Since the high light yield and better sensitivity of NaI:Tl crystal is important for low mass dark matter search. Therefore, it is necessary to use high light NaI:Tl crystal, and absolute light yield should be measured with accuracy. In this work, we use the single photoelectron technique for measuring the absolute light yield of 35 NaI:Tl crystals with various sizes from different vendors. There are several high-quality crystals from the COSINE-100 experiment and commercial companies in these crystals. The theoretical estimation and GEANT4 optical simulation have been studied to investigate the PMT optics. Results show the essential role of this correction in avoiding overrated light yield values. The SPE technique using different PMT was compared to the photodiode and avalanche photodiode methods. A 10% systematic error was obtained. Our results show the excellent absolute light yield of NaI:Tl, at 59.4 +- 5.9 ph/keV, while the theoretical predicted light yield is around 70 ph/keV. The evaluation with NaI:Tl crystals in the COSINE-100 experiment has been performed. The six crystals in the COSINE-100 experiment have a high light yield. Based on our results, the light loss of encapsulation needs to be improved, especially for the big-size crystals.","sentences":["The NaI:Tl crystals were early investigated and used for wide application fields due to high light yield and crystal growth advantages.","So far, the absolute light yields of NaI:Tl crystal have typically been known to be 40 ph/keV.","However, it varies widely, far from the theoretical estimation.","Since the high light yield and better sensitivity of NaI:Tl crystal is important for low mass dark matter search.","Therefore, it is necessary to use high light NaI:Tl crystal, and absolute light yield should be measured with accuracy.","In this work, we use the single photoelectron technique for measuring the absolute light yield of 35 NaI:Tl crystals with various sizes from different vendors.","There are several high-quality crystals from the COSINE-100 experiment and commercial companies in these crystals.","The theoretical estimation and GEANT4 optical simulation have been studied to investigate the PMT optics.","Results show the essential role of this correction in avoiding overrated light yield values.","The SPE technique using different PMT was compared to the photodiode and avalanche photodiode methods.","A 10% systematic error was obtained.","Our results show the excellent absolute light yield of NaI:Tl, at 59.4 +- 5.9 ph/keV, while the theoretical predicted light yield is around 70 ph/keV.","The evaluation with NaI:Tl crystals in the COSINE-100 experiment has been performed.","The six crystals in the COSINE-100 experiment have a high light yield.","Based on our results, the light loss of encapsulation needs to be improved, especially for the big-size crystals."],"url":"http://arxiv.org/abs/2404.18551v1","category":"hep-ex"}
{"created":"2024-04-29 09:39:45","title":"Light tetraquark states with exotic quantum numbers $J^{PC}=2^{+-}$","abstract":"We study the masses of light tetraquark states $ud\\bar{u}\\bar{d}$ , $us\\bar{u}\\bar{s}$ and $ss\\bar{s}\\bar{s}$ with exotic quantum numbers $J^{PC}=2^{+-}$ using the method of QCD sum rules. It is found that there is no tetraquark operator with two Lorentz indices coupling to the $2^{+-}$ quantum numbers. To investigate such tetraquark states, we construct the interpolating tetraquark currents with three Lorentz indices and without derivative operator. We calculate the correlation functions up to dimension 10 condensates, and extract the $2^{+-}$ invariant functions via the projector operator. Our results show that the masses of the $ud\\bar{u}\\bar{d}$, $us\\bar{u}\\bar{s}$ and $ss\\bar{s}\\bar{s}$ tetraquark states with $J^{PC}=2^{+-}$ are about $3.3-3.5 ~\\mathrm{GeV}$, $3.5-3.6 ~\\mathrm{GeV}$ and $3.6 ~\\mathrm{GeV}$, respectively. We further discuss the strong decays of these light tetraquarks into the two-meson and baryon-antibaryon final states, and suggest to search for them in the $\\rho\\pi, \\omega\\pi, \\phi\\pi$, $b_{1}\\pi$, $h_{1}\\pi$, $K\\bar K^\\ast, K\\bar{K}_{1}$, $\\Delta\\bar{\\Delta}$, $\\Sigma^{\\ast} \\bar{\\Sigma }^{\\ast}$, $\\Xi^{\\ast} \\bar{\\Xi }^{\\ast}$, $\\Omega\\bar{\\Omega }$ channels in the future.","sentences":["We study the masses of light tetraquark states $ud\\bar{u}\\bar{d}$ , $us\\bar{u}\\bar{s}$ and $ss\\bar{s}\\bar{s}$ with exotic quantum numbers $J^{PC}=2^{+-}$ using the method of QCD sum rules.","It is found that there is no tetraquark operator with two Lorentz indices coupling to the $2^{+-}$ quantum numbers.","To investigate such tetraquark states, we construct the interpolating tetraquark currents with three Lorentz indices and without derivative operator.","We calculate the correlation functions up to dimension 10 condensates, and extract the $2^{+-}$ invariant functions via the projector operator.","Our results show that the masses of the $ud\\bar{u}\\bar{d}$, $us\\bar{u}\\bar{s}$ and $ss\\bar{s}\\bar{s}$ tetraquark states with $J^{PC}=2^{+-}$ are about $3.3-3.5 ~\\mathrm{GeV}$, $3.5-3.6 ~\\mathrm{GeV}$ and $3.6 ~\\mathrm{GeV}$, respectively.","We further discuss the strong decays of these light tetraquarks into the two-meson and baryon-antibaryon final states, and suggest to search for them in the $\\rho\\pi, \\omega\\pi, \\phi\\pi$, $b_{1}\\pi$, $h_{1}\\pi$, $K\\bar K^\\ast, K\\bar{K}_{1}$, $\\Delta\\bar{\\Delta}$, $\\Sigma^{\\ast} \\bar{\\Sigma }^{\\ast}$, $\\Xi^{\\ast} \\bar{\\Xi }^{\\ast}$, $\\Omega\\bar{\\Omega }$ channels in the future."],"url":"http://arxiv.org/abs/2404.18547v1","category":"hep-ph"}
{"created":"2024-04-29 09:07:08","title":"Analytical approach to the design of RF photoinjector","abstract":"The objective of this thesis is to ascertain the dimensions of an RF 2.856GHz photoinjector through a combination of analytical and computational approaches. The phase velocity within a single cavity exceeds 'c', rendering it inadequate for storing the requisite energy for beam acceleration. To surmount this limitation, we aim to devise a multi-celled cavity design. However, the alterations in electromagnetic fields and resonant frequency within the multi-celled cavity are intricate and sensitive, presenting challenges in obtaining precise dimensions solely via computer simulations. Prior to numerical methods, it is essential to analyze the photoinjector using theoretical frameworks. We employ perturbation theory and the construction of an equivalent circuit to elucidate the underlying physics of the photoinjector and the electrical oscillations within the cell structure. Detailed analytical methods for the equivalent circuit are explored. Through theoretical analysis, the dimensions and simulation outcomes can be determined quantitatively.","sentences":["The objective of this thesis is to ascertain the dimensions of an RF 2.856GHz photoinjector through a combination of analytical and computational approaches.","The phase velocity within a single cavity exceeds 'c', rendering it inadequate for storing the requisite energy for beam acceleration.","To surmount this limitation, we aim to devise a multi-celled cavity design.","However, the alterations in electromagnetic fields and resonant frequency within the multi-celled cavity are intricate and sensitive, presenting challenges in obtaining precise dimensions solely via computer simulations.","Prior to numerical methods, it is essential to analyze the photoinjector using theoretical frameworks.","We employ perturbation theory and the construction of an equivalent circuit to elucidate the underlying physics of the photoinjector and the electrical oscillations within the cell structure.","Detailed analytical methods for the equivalent circuit are explored.","Through theoretical analysis, the dimensions and simulation outcomes can be determined quantitatively."],"url":"http://arxiv.org/abs/2404.18520v1","category":"physics.plasm-ph"}
{"created":"2024-04-29 08:55:42","title":"Origin of Ferroelectricity and Superconductivity with Nontrivial Electronic Topology in Fluorinated Nb2N","abstract":"Two-dimensional (2D) intrinsic superconductors with nontrivial topological band and vertical ferroelectricity exhibit fascinating characteristics to achieving electrostatic control of quantum phases. While, only a few such 2D materials have been theoretically predicted. In this work, based on first principles calculations, we explore the superconductivity and ferroelectric properties in fluorinated 2D Nb2N. In the stable Nb2NF2, H3-Nb2NF2 breaks the spatial inversion symmetry, exhibiting vertical ferroelectric. More interestingly, it not only possesses intrinsic superconductivity with superconducting transition temperatures (Tc) of 10 K, but also exhibits nontrivial band topology. While, H1-Nb2NF2 shows topological band and superconductivity with Tc of 32 K, surpassing most of 2D conventional topological superconductors' candidates. Our research has enriched 2D superconducting materials with nontrivial band topology and ferroelectric properties, and provided a theoretical basis for the preparation of devices switching between superconducting and ferroelectric states with external electric field.","sentences":["Two-dimensional (2D) intrinsic superconductors with nontrivial topological band and vertical ferroelectricity exhibit fascinating characteristics to achieving electrostatic control of quantum phases.","While, only a few such 2D materials have been theoretically predicted.","In this work, based on first principles calculations, we explore the superconductivity and ferroelectric properties in fluorinated 2D Nb2N. In the stable Nb2NF2,","H3-Nb2NF2 breaks the spatial inversion symmetry, exhibiting vertical ferroelectric.","More interestingly, it not only possesses intrinsic superconductivity with superconducting transition temperatures (Tc) of 10 K, but also exhibits nontrivial band topology.","While, H1-Nb2NF2 shows topological band and superconductivity with Tc of 32 K, surpassing most of 2D conventional topological superconductors' candidates.","Our research has enriched 2D superconducting materials with nontrivial band topology and ferroelectric properties, and provided a theoretical basis for the preparation of devices switching between superconducting and ferroelectric states with external electric field."],"url":"http://arxiv.org/abs/2404.18511v2","category":"cond-mat.supr-con"}
{"created":"2024-04-29 08:19:26","title":"A new hybrid gadolinium nanoparticles-loaded polymeric material for neutron detection in rare event searches","abstract":"Experiments aimed at direct searches for WIMP dark matter require highly effective reduction of backgrounds and control of any residual radioactive contamination. In particular, neutrons interacting with atomic nuclei represent an important class of backgrounds due to the expected similarity of a WIMP-nucleon interaction, so that such experiments often feature a dedicated neutron detector surrounding the active target volume. In the context of the development of DarkSide-20k detector at INFN Gran Sasso National Laboratory (LNGS), several R&D projects were conceived and developed for the creation of a new hybrid material rich in both hydrogen and gadolinium nuclei to be employed as an essential element of the neutron detector. Thanks to its very high cross-section for neutron capture, gadolinium is one of the most widely used elements in neutron detectors, while the hydrogen-rich material is instrumental in efficiently moderating the neutrons. In this paper results from one of the R&Ds are presented. In this effort the new hybrid material was obtained as a poly(methyl methacrylate) (PMMA) matrix, loaded with gadolinium oxide in the form of nanoparticles. We describe its realization, including all phases of design, purification, construction, characterization, and determination of mechanical properties of the new material.","sentences":["Experiments aimed at direct searches for WIMP dark matter require highly effective reduction of backgrounds and control of any residual radioactive contamination.","In particular, neutrons interacting with atomic nuclei represent an important class of backgrounds due to the expected similarity of a WIMP-nucleon interaction, so that such experiments often feature a dedicated neutron detector surrounding the active target volume.","In the context of the development of DarkSide-20k detector at INFN Gran Sasso National Laboratory (LNGS), several R&D projects were conceived and developed for the creation of a new hybrid material rich in both hydrogen and gadolinium nuclei to be employed as an essential element of the neutron detector.","Thanks to its very high cross-section for neutron capture, gadolinium is one of the most widely used elements in neutron detectors, while the hydrogen-rich material is instrumental in efficiently moderating the neutrons.","In this paper results from one of the R&Ds are presented.","In this effort the new hybrid material was obtained as a poly(methyl methacrylate) (PMMA) matrix, loaded with gadolinium oxide in the form of nanoparticles.","We describe its realization, including all phases of design, purification, construction, characterization, and determination of mechanical properties of the new material."],"url":"http://arxiv.org/abs/2404.18492v1","category":"physics.ins-det"}
{"created":"2024-04-29 07:44:20","title":"Increasing resolution and instability for linear inverse scattering problems","abstract":"In this work we study the increasing resolution of linear inverse scattering problems at a large fixed frequency. We consider the problem of recovering the density of a Herglotz wave function, and the linearized inverse scattering problem for a potential. It is shown that the number of features that can be stably recovered (stable region) becomes larger as the frequency increases, whereas one has strong instability for the rest of the features (unstable region). To show this rigorously, we prove that the singular values of the forward operator stay roughly constant in the stable region and decay exponentially in the unstable region. The arguments are based on structural properties of the problems and they involve the Courant min-max principle for singular values, quantitative Agmon-H\\\"ormander estimates, and a Schwartz kernel computation based on the coarea formula.","sentences":["In this work we study the increasing resolution of linear inverse scattering problems at a large fixed frequency.","We consider the problem of recovering the density of a Herglotz wave function, and the linearized inverse scattering problem for a potential.","It is shown that the number of features that can be stably recovered (stable region) becomes larger as the frequency increases, whereas one has strong instability for the rest of the features (unstable region).","To show this rigorously, we prove that the singular values of the forward operator stay roughly constant in the stable region and decay exponentially in the unstable region.","The arguments are based on structural properties of the problems and they involve the Courant min-max principle for singular values, quantitative Agmon-H\\\"ormander estimates, and a Schwartz kernel computation based on the coarea formula."],"url":"http://arxiv.org/abs/2404.18482v1","category":"math.AP"}
{"created":"2024-04-29 07:18:09","title":"Harmonic locus and Calogero-Moser spaces","abstract":"We study the harmonic locus consisting of the monodromy-free Schr\\\"odinger operators with rational potential and quadratic growth at infinity. It is known after Oblomkov that it can be identified with the set of all partitions via the Wronskian map for Hermite polynomials. We show that the harmonic locus can also be identified with the subset of the Calogero--Moser space introduced by Wilson, which is fixed by the symplectic action of $\\mathbb C^\\times.$ As a corollary, for the multiplicity-free part of the locus we effectively solve the inverse problem for the Wronskian map by describing the partition in terms of the spectrum of the corresponding Moser matrix. We also compute the characters of the $\\mathbb C^\\times$-action at the fixed points, proving, in particular, a conjecture of Conti and Masoero.","sentences":["We study the harmonic locus consisting of the monodromy-free Schr\\\"odinger operators with rational potential and quadratic growth at infinity.","It is known after Oblomkov that it can be identified with the set of all partitions via the Wronskian map for Hermite polynomials.","We show that the harmonic locus can also be identified with the subset of the Calogero--Moser space introduced by Wilson, which is fixed by the symplectic action of $\\mathbb C^\\times.$ As a corollary, for the multiplicity-free part of the locus we effectively solve the inverse problem for the Wronskian map by describing the partition in terms of the spectrum of the corresponding Moser matrix.","We also compute the characters of the $\\mathbb C^\\times$-action at the fixed points, proving, in particular, a conjecture of Conti and Masoero."],"url":"http://arxiv.org/abs/2404.18471v1","category":"math-ph"}
{"created":"2024-04-29 06:03:49","title":"Pre-peak Emission in Tidal Disruption Events","abstract":"The rising part of a tidal disruption event light curve provides unique insight into early emission and the onset of accretion. Various mechanisms are proposed to explain the pre-peak emission, including shocks from debris interaction and reprocessing of disk emission. We study the pre-peak emission and its influence on the gas circularization by a series of gray radiation hydrodynamic simulations with varying black hole mass. We find that given a super-Eddington fallback rate of 10\\dot{M}_{Edd}, the stream-stream collision can occur multiple times and drive strong outflows of up to 9\\dot{M}_{Edd}. By dispersing gas to \\gtrsim 100rs, the outflow can delay gas circularization and leads to sub-Eddington accretion rates during the first few stream-stream collisions. The stream-stream collision shock and circularization shock can sustain a luminosity of ~10^{44}erg/s for days. The luminosity is generally sub-Eddington and shows a weak correlation with accretion rate at early time. The outflow is optically thick, yielding a reprocessing layer with a size of ~10^{14} cm and photospheric temperature of ~4\\times10^{4}K.","sentences":["The rising part of a tidal disruption event light curve provides unique insight into early emission and the onset of accretion.","Various mechanisms are proposed to explain the pre-peak emission, including shocks from debris interaction and reprocessing of disk emission.","We study the pre-peak emission and its influence on the gas circularization by a series of gray radiation hydrodynamic simulations with varying black hole mass.","We find that given a super-Eddington fallback rate of 10\\dot{M}_{Edd}, the stream-stream collision can occur multiple times and drive strong outflows of up to 9\\dot{M}_{Edd}.","By dispersing gas to \\gtrsim 100rs, the outflow can delay gas circularization and leads to sub-Eddington accretion rates during the first few stream-stream collisions.","The stream-stream collision shock and circularization shock can sustain a luminosity of ~10^{44}erg/s for days.","The luminosity is generally sub-Eddington and shows a weak correlation with accretion rate at early time.","The outflow is optically thick, yielding a reprocessing layer with a size of ~10^{14} cm and photospheric temperature of ~4\\times10^{4}K."],"url":"http://arxiv.org/abs/2404.18446v1","category":"astro-ph.HE"}
{"created":"2024-04-29 05:39:52","title":"Witten index of BMN matrix quantum mechanics","abstract":"We compute the Witten index of the Berenstein-Maldacena-Nastase matrix quantum mechanics, which counts the number of ground states as well as the difference between the numbers of bosonic and fermionic BPS states with nonzero spins. The Witten index sets a lower bound on the entropy, which exhibits an $N^2$ growth that predicts the existence of BPS black holes in M-theory, asymptotic to the plane wave geometry. We also discuss a relation between the Witten index in the infinite $N$ limit and the superconformal index of the Aharony-Bergman-Jafferis-Maldacena theory.","sentences":["We compute the Witten index of the Berenstein-Maldacena-Nastase matrix quantum mechanics, which counts the number of ground states as well as the difference between the numbers of bosonic and fermionic BPS states with nonzero spins.","The Witten index sets a lower bound on the entropy, which exhibits an $N^2$ growth that predicts the existence of BPS black holes in M-theory, asymptotic to the plane wave geometry.","We also discuss a relation between the Witten index in the infinite $N$ limit and the superconformal index of the Aharony-Bergman-Jafferis-Maldacena theory."],"url":"http://arxiv.org/abs/2404.18442v1","category":"hep-th"}
{"created":"2024-04-29 05:36:59","title":"Non-Hermitian Lattice Fermions in 2D GNY Model","abstract":"We work the lattice fermions and non-Hermitian formulation in the 2D GNY model and demonstrate the numerical implementation for two flavors by the Hybrid Monte Carlo. Our approach has a notable advantage in dealing with chiral symmetry on a lattice by avoiding the Nielsen-Ninomiya theorem, due to the non-symmetrized finite-difference operator. We restore the hypercubic symmetry by averaging over all possible orientations with the proper continuum limit. Our study is the first simulation for the interacting fermion formulated in a non-hermitian way. We compare the numerical solution with the one-loop resummation. The resummation results matches with the numerical solution in $\\langle\\phi\\rangle$, $\\langle\\phi^2\\rangle$, $\\langle\\mathrm{Tr}(\\bar{\\psi}_1\\psi_1+\\bar{\\psi}_2\\psi_2)/2\\rangle$, and $\\langle\\mathrm{Tr}(\\bar{\\psi}_1\\psi_1+\\bar{\\psi}_2\\psi_2)\\phi/2\\rangle$. We also used the one-loop resummation to provide the RG flow and asymptotic safety in the 2D GNY model.","sentences":["We work the lattice fermions and non-Hermitian formulation in the 2D GNY model and demonstrate the numerical implementation for two flavors by the Hybrid Monte Carlo.","Our approach has a notable advantage in dealing with chiral symmetry on a lattice by avoiding the Nielsen-Ninomiya theorem, due to the non-symmetrized finite-difference operator.","We restore the hypercubic symmetry by averaging over all possible orientations with the proper continuum limit.","Our study is the first simulation for the interacting fermion formulated in a non-hermitian way.","We compare the numerical solution with the one-loop resummation.","The resummation results matches with the numerical solution in $\\langle\\phi\\rangle$, $\\langle\\phi^2\\rangle$, $\\langle\\mathrm{Tr}(\\bar{\\psi}_1\\psi_1+\\bar{\\psi}_2\\psi_2)/2\\rangle$, and $\\langle\\mathrm{Tr}(\\bar{\\psi}_1\\psi_1+\\bar{\\psi}_2\\psi_2)\\phi/2\\rangle$. We also used the one-loop resummation to provide the RG flow and asymptotic safety in the 2D GNY model."],"url":"http://arxiv.org/abs/2404.18441v1","category":"hep-th"}
{"created":"2024-04-29 04:32:12","title":"Probing the topological phase transition in the Su-Schrieffer-Heeger model using Rydberg-atom synthetic dimensions","abstract":"We simulate the the Su-Schrieffer-Heeger (SSH) model using Rydberg-atom synthetic dimensions constructed, in a single atom, from a ladder of six neighboring $n\\:^3S_1$ Rydberg states in which adjacent states are coupled with two-photon transitions using microwave fields. Alternating strong/weak tunneling rates, controlled by adjusting the microwave amplitudes, are varied to map out the topological phase transition as a function of the ratio of the tunneling rates.   For each ratio, quench dynamics experiments, in which the system is initially prepared in one of the bulk Rydberg states and then subjected to the microwave fields, are performed to measure the population evolution of the system. From the dynamics measurements, we extract the mean chiral displacement and verify that its long-time average value converges towards the system winding number.   The topological phase transition is also examined by probing the energy spectrum of the system in steady state and observing the disappearance of the zero-energy edge states. The results show that even a system with as few as six levels can demonstrate the essential characteristics of the SSH Hamiltonian.","sentences":["We simulate the the Su-Schrieffer-Heeger (SSH) model using Rydberg-atom synthetic dimensions constructed, in a single atom, from a ladder of six neighboring $n\\:^3S_1$ Rydberg states in which adjacent states are coupled with two-photon transitions using microwave fields.","Alternating strong/weak tunneling rates, controlled by adjusting the microwave amplitudes, are varied to map out the topological phase transition as a function of the ratio of the tunneling rates.   ","For each ratio, quench dynamics experiments, in which the system is initially prepared in one of the bulk Rydberg states and then subjected to the microwave fields, are performed to measure the population evolution of the system.","From the dynamics measurements, we extract the mean chiral displacement and verify that its long-time average value converges towards the system winding number.   ","The topological phase transition is also examined by probing the energy spectrum of the system in steady state and observing the disappearance of the zero-energy edge states.","The results show that even a system with as few as six levels can demonstrate the essential characteristics of the SSH Hamiltonian."],"url":"http://arxiv.org/abs/2404.18420v1","category":"quant-ph"}
{"created":"2024-04-29 03:07:49","title":"Broad and Bi-directional narrow quasi-periodic fast-propagating wave trains associated with a filament-driven halo CME on 2023 April 21","abstract":"This paper presents three distinct wave trains that occurred on 2023 April 21: a broad quasi-periodic fast-propagating (QFP) wave train and a bi-directional narrow QFP wave train. The broad QFP wave train expands outward in a circular wavefront, while bi-directional narrow QFP wave trains propagate in the northward and southward directions, respectively. The concurrent presence of the wave trains offers a remarkable opportunity to investigate their respective triggering mechanisms. Measurement shows that the broad QFP wave train's speed is 300- 1100 km/s in different propagating directions. There is a significant difference in the speed of the bi-directional narrow QFP wave trains: the southward propagation achieves 1400 km/s, while the northward propagation only reaches about 550 km/s accompanied by a deceleration of about 1- 2 kms-2. Using the wavelet analysis, we find that the periodicity of the propagating wave trains in the southward and northward directions closely matches the quasi-periodic pulsations (QPPs) exhibited by the flares. Based on these results, the narrow QFP wave trains were most likely excited by the intermittent energy release in the accompanying flare. In contrast, the broad QFP wave train had a tight relationship with the erupting filament, probably attributed to the unwinding motion of the erupting filament or the leakage of the fast sausage wave train inside the filament body.","sentences":["This paper presents three distinct wave trains that occurred on 2023 April 21: a broad quasi-periodic fast-propagating (QFP) wave train and a bi-directional narrow QFP wave train.","The broad QFP wave train expands outward in a circular wavefront, while bi-directional narrow QFP wave trains propagate in the northward and southward directions, respectively.","The concurrent presence of the wave trains offers a remarkable opportunity to investigate their respective triggering mechanisms.","Measurement shows that the broad QFP wave train's speed is 300- 1100 km/s in different propagating directions.","There is a significant difference in the speed of the bi-directional narrow QFP wave trains: the southward propagation achieves 1400 km/s, while the northward propagation only reaches about 550 km/s accompanied by a deceleration of about 1- 2 kms-2.","Using the wavelet analysis, we find that the periodicity of the propagating wave trains in the southward and northward directions closely matches the quasi-periodic pulsations (QPPs) exhibited by the flares.","Based on these results, the narrow QFP wave trains were most likely excited by the intermittent energy release in the accompanying flare.","In contrast, the broad QFP wave train had a tight relationship with the erupting filament, probably attributed to the unwinding motion of the erupting filament or the leakage of the fast sausage wave train inside the filament body."],"url":"http://arxiv.org/abs/2404.18391v1","category":"astro-ph.SR"}
{"created":"2024-04-29 02:43:23","title":"Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions","abstract":"Language models can hallucinate when performing complex and detailed mathematical reasoning. Physics provides a rich domain for assessing mathematical reasoning capabilities where physical context imbues the use of symbols which needs to satisfy complex semantics (\\textit{e.g.,} units, tensorial order), leading to instances where inference may be algebraically coherent, yet unphysical. In this work, we assess the ability of Language Models (LMs) to perform fine-grained mathematical and physical reasoning using a curated dataset encompassing multiple notations and Physics subdomains. We improve zero-shot scores using synthetic in-context examples, and demonstrate non-linear degradation of derivation quality with perturbation strength via the progressive omission of supporting premises. We find that the models' mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions.","sentences":["Language models can hallucinate when performing complex and detailed mathematical reasoning.","Physics provides a rich domain for assessing mathematical reasoning capabilities where physical context imbues the use of symbols which needs to satisfy complex semantics (\\textit{e.g.,} units, tensorial order), leading to instances where inference may be algebraically coherent, yet unphysical.","In this work, we assess the ability of Language Models (LMs) to perform fine-grained mathematical and physical reasoning using a curated dataset encompassing multiple notations and Physics subdomains.","We improve zero-shot scores using synthetic in-context examples, and demonstrate non-linear degradation of derivation quality with perturbation strength via the progressive omission of supporting premises.","We find that the models' mathematical reasoning is not physics-informed in this setting, where physical context is predominantly ignored in favour of reverse-engineering solutions."],"url":"http://arxiv.org/abs/2404.18384v1","category":"cs.CL"}
{"created":"2024-04-29 02:11:14","title":"Anomalous Phonon in Charge-Density-Wave Phase of Kagome Metal CsV3Sb5","abstract":"CsV3Sb5, a notable compound within the kagome family, is renowned for its topological and superconducting properties, as well as its detection of local magnetic field and anomalous Hall effect in experiments. However, the origin of this local magnetic field is still veiled. In this study, we employ the first-principles calculations to investigate the atomic vibration in both the pristine and the charge-density-wave phases of CsV$_3$Sb$_5$. Our analysis reveals the presence of ``anomalous phonons\" in these structures, these phonon induce the circular vibration of atoms, contributing to the phonon magnetic moments and subsequently to the observed the local magnetic fields. Additionally, we observe that lattice distortion in the charge-density-wave phase amplifies these circular vibrations, resulting in a stronger local magnetic field, particularly from the vanadium atoms. This investigation not only reveals the potential relation between lattice distortion and atomic polarization but also offers a novel idea to understand the origin of local magnetic moment in CsV3Sb5.","sentences":["CsV3Sb5, a notable compound within the kagome family, is renowned for its topological and superconducting properties, as well as its detection of local magnetic field and anomalous Hall effect in experiments.","However, the origin of this local magnetic field is still veiled.","In this study, we employ the first-principles calculations to investigate the atomic vibration in both the pristine and the charge-density-wave phases of CsV$_3$Sb$_5$.","Our analysis reveals the presence of ``anomalous phonons\" in these structures, these phonon induce the circular vibration of atoms, contributing to the phonon magnetic moments and subsequently to the observed the local magnetic fields.","Additionally, we observe that lattice distortion in the charge-density-wave phase amplifies these circular vibrations, resulting in a stronger local magnetic field, particularly from the vanadium atoms.","This investigation not only reveals the potential relation between lattice distortion and atomic polarization but also offers a novel idea to understand the origin of local magnetic moment in CsV3Sb5."],"url":"http://arxiv.org/abs/2404.18366v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-29 01:37:15","title":"CP violation in the CKM mixing for degenerate quark masses","abstract":"CP violation in the CKM mixing is discussed for the case of quark mass degeneracy that is approximate in the quark mass hierarchy limit. Differing from the traditional understanding of CP vanishing for degenerate masses, we find degenerate symmetry plays a non-trivial role in CP violation. The minimal flavor structure model is reviewed to demonstrate the role of degenerate symmetry in quark flavor mixing, particularly in CP violation. This relation between mass hierarchy and CP violation helps us understand the origin of CP violation and assists the construction of the flavor model.","sentences":["CP violation in the CKM mixing is discussed for the case of quark mass degeneracy that is approximate in the quark mass hierarchy limit.","Differing from the traditional understanding of CP vanishing for degenerate masses, we find degenerate symmetry plays a non-trivial role in CP violation.","The minimal flavor structure model is reviewed to demonstrate the role of degenerate symmetry in quark flavor mixing, particularly in CP violation.","This relation between mass hierarchy and CP violation helps us understand the origin of CP violation and assists the construction of the flavor model."],"url":"http://arxiv.org/abs/2404.18358v1","category":"hep-ph"}
{"created":"2024-04-29 01:35:56","title":"Narrow linewidth exciton-polariton laser","abstract":"We present a high-resolution spectroscopic investigation of the energy and linewidth of an exciton-polariton laser in the single-mode regime, which derives its coherent emission from an optically pumped and confined exciton-polariton condensate. We report an ultra-narrow linewidth of 56~MHz or 0.24~$\\mu$eV, corresponding to a coherence time of 5.7~ns. The narrow linewidth is consistently achieved by using an exciton-polariton condensate with a high photonic content confined in an optically induced trap. Contrary to previous studies, we show that the excitonic reservoir created by the pump and responsible for creating the trap does not strongly affect the emission linewidth as long as the condensate is trapped and the pump power is well above the condensation (lasing) threshold. Our results open up opportunities for manipulating the macroscopic quantum state of the system owing to its long coherence time.","sentences":["We present a high-resolution spectroscopic investigation of the energy and linewidth of an exciton-polariton laser in the single-mode regime, which derives its coherent emission from an optically pumped and confined exciton-polariton condensate.","We report an ultra-narrow linewidth of 56~MHz or 0.24~$\\mu$eV, corresponding to a coherence time of 5.7~ns.","The narrow linewidth is consistently achieved by using an exciton-polariton condensate with a high photonic content confined in an optically induced trap.","Contrary to previous studies, we show that the excitonic reservoir created by the pump and responsible for creating the trap does not strongly affect the emission linewidth as long as the condensate is trapped and the pump power is well above the condensation (lasing) threshold.","Our results open up opportunities for manipulating the macroscopic quantum state of the system owing to its long coherence time."],"url":"http://arxiv.org/abs/2404.18354v1","category":"physics.optics"}
{"created":"2024-04-29 01:07:00","title":"Turbulence statistics of HI clouds entrained in the Milky Way's nuclear wind","abstract":"The interstellar medium (ISM) is ubiquitously turbulent across many physically distinct environments within the Galaxy. Turbulence is key in controlling the structure and dynamics of the ISM, regulating star formation, and transporting metals within the Galaxy. We present the first observational measurements of turbulence in neutral hydrogen entrained in the hot nuclear wind of the Milky Way. Using recent MeerKAT observations of two extra-planar HI clouds above (gal. lat.$\\,\\sim7.0^{\\circ}$) and below (gal. lat.$\\,\\sim-3.9^{\\circ}$) the Galactic disc, we analyse centroid velocity and column density maps to estimate the velocity dispersion ($\\sigma_{v,\\mathrm{3D}}$), the turbulent sonic Mach number ($\\mathcal{M}$), the volume density dispersion ($\\sigma_{\\rho/\\rho_0}$), and the turbulence driving parameter ($b$). We also present a new prescription for estimating the spatial temperature variations of HI in the presence of related molecular gas. We measure these turbulence quantities on the global scale of each cloud, but also spatially map their variation across the plane-of-sky extent of each cloud by using a roving kernel method. We find that the two clouds share very similar characteristics of their internal turbulence, despite their varying latitudes. Both clouds are in the sub-to-trans-sonic Mach regime, and have primarily compressively-driven ($b\\sim1$) turbulence. Given that there is no known active star-formation present in these clouds, this may be indicative of the way the cloud-wind interaction injects energy into the entrained atomic material on parsec scales.","sentences":["The interstellar medium (ISM) is ubiquitously turbulent across many physically distinct environments within the Galaxy.","Turbulence is key in controlling the structure and dynamics of the ISM, regulating star formation, and transporting metals within the Galaxy.","We present the first observational measurements of turbulence in neutral hydrogen entrained in the hot nuclear wind of the Milky Way.","Using recent MeerKAT observations of two extra-planar HI clouds above (gal.","lat.$\\,\\sim7.0^{\\circ}$) and below (gal.","lat.$\\,\\sim-3.9^{\\circ}$) the Galactic disc, we analyse centroid velocity and column density maps to estimate the velocity dispersion ($\\sigma_{v,\\mathrm{3D}}$), the turbulent sonic Mach number ($\\mathcal{M}$), the volume density dispersion ($\\sigma_{\\rho/\\rho_0}$), and the turbulence driving parameter ($b$).","We also present a new prescription for estimating the spatial temperature variations of HI in the presence of related molecular gas.","We measure these turbulence quantities on the global scale of each cloud, but also spatially map their variation across the plane-of-sky extent of each cloud by using a roving kernel method.","We find that the two clouds share very similar characteristics of their internal turbulence, despite their varying latitudes.","Both clouds are in the sub-to-trans-sonic Mach regime, and have primarily compressively-driven ($b\\sim1$) turbulence.","Given that there is no known active star-formation present in these clouds, this may be indicative of the way the cloud-wind interaction injects energy into the entrained atomic material on parsec scales."],"url":"http://arxiv.org/abs/2404.18349v1","category":"astro-ph.GA"}
{"created":"2024-04-29 00:41:13","title":"Interfacial Rheology of Lanthanide Binding Peptide Surfactants at the Air-Water Interface","abstract":"Peptide surfactants (PEPS) are studied to capture and retain rare earth elements (REEs) at air-water interfaces to enable REE separations. Peptide sequences, designed to selectively bind REEs, depend crucially on the position of ligands within their binding loop domain. These ligands form a coordination sphere that wraps and retains the cation. We study variants of lanthanide binding tags (LBTs) designed to complex strongly with Tb$^{3+}$. The peptide LBT$^{5-}$ (with net charge -5) is known to bind Tb$^{3+}$ and adsorb with more REE cations than peptide molecules, suggesting that undesired non-specific Coulombic interactions occur. Rheological characterization of interfaces of LBT$^{5-}$ and Tb$^{3+}$ solutions reveal the formation of an interfacial gel. To probe whether this gelation reflects chelation among intact adsorbed LBT$^{5-}$:Tb$^{3+}$ complexes or destruction of the binding loop, we study a variant, LBT$^{3-}$, designed to form net neutral LBT$^{3-}$:Tb$^{3+}$ complexes. Solutions of LBT$^{3-}$ and Tb$^{3+}$ form purely viscous layers in the presence of excess Tb$^{3+}$, indicating that each peptide binds a single REE in an intact coordination sphere. We introduce the variant RR-LBT$^{3-}$ with net charge -3 and anionic ligands outside of the coordination sphere. We find that such exposed ligands promote interfacial gelation. Thus, a nuanced requirement for interfacial selectivity of PEPS is proposed: that anionic ligands outside of the coordination sphere must be avoided to prevent the non-selective recruitment of REE cations. This view is supported by simulation, including interfacial molecular dynamics simulations, and interfacial metadynamics simulations of the free energy landscape of the binding loop conformational space.","sentences":["Peptide surfactants (PEPS) are studied to capture and retain rare earth elements (REEs) at air-water interfaces to enable REE separations.","Peptide sequences, designed to selectively bind REEs, depend crucially on the position of ligands within their binding loop domain.","These ligands form a coordination sphere that wraps and retains the cation.","We study variants of lanthanide binding tags (LBTs) designed to complex strongly with Tb$^{3+}$. The peptide LBT$^{5-}$ (with net charge -5) is known to bind Tb$^{3+}$ and adsorb with more REE cations than peptide molecules, suggesting that undesired non-specific Coulombic interactions occur.","Rheological characterization of interfaces of LBT$^{5-}$ and Tb$^{3+}$ solutions reveal the formation of an interfacial gel.","To probe whether this gelation reflects chelation among intact adsorbed LBT$^{5-}$:Tb$^{3+}$ complexes or destruction of the binding loop, we study a variant, LBT$^{3-}$, designed to form net neutral LBT$^{3-}$:Tb$^{3+}$ complexes.","Solutions of LBT$^{3-}$ and Tb$^{3+}$ form purely viscous layers in the presence of excess Tb$^{3+}$, indicating that each peptide binds a single REE in an intact coordination sphere.","We introduce the variant RR-LBT$^{3-}$ with net charge -3 and anionic ligands outside of the coordination sphere.","We find that such exposed ligands promote interfacial gelation.","Thus, a nuanced requirement for interfacial selectivity of PEPS is proposed: that anionic ligands outside of the coordination sphere must be avoided to prevent the non-selective recruitment of REE cations.","This view is supported by simulation, including interfacial molecular dynamics simulations, and interfacial metadynamics simulations of the free energy landscape of the binding loop conformational space."],"url":"http://arxiv.org/abs/2404.18340v1","category":"cond-mat.soft"}
{"created":"2024-04-28 20:40:00","title":"Electrophilic scalar hair from rotating magnetized stars and effects of cosmic neutrino background","abstract":"Ultralight electrophilic scalar field can mediate a long-range force or radiate from a pulsar or a magnetar if the scalar field has a coupling with the Goldreich-Julian charge density or the net electron charge density of the star. The interaction of the electron with the long-range scalar profile results in a spatial variation of the electron mass. A scalar induced magnetic field is created due to such interaction. The mass of the scalar in such cases is constrained by the radius of the star. The scalar field can also radiate from a binary system or an isolated star if the mass of the scalar is less than the orbital frequency and the spin frequency respectively. The electrophilic scalar radiation can contribute to the orbital period loss of binary systems and pulsar spin-down. Comparing with existing and projected experimental sensitivities, we obtain constraints on scalar coupling with ultralight mass. Some of these bounds are stronger than the existing fifth force constraints. The constraints on the scalar coupling can be significantly screened if the scalar has a coupling with the ubiquitous cosmic neutrino background. Improvements in experimental sensitivity and observations of compact objects with stronger magnetic fields and higher angular velocities could further refine these bounds.","sentences":["Ultralight electrophilic scalar field can mediate a long-range force or radiate from a pulsar or a magnetar if the scalar field has a coupling with the Goldreich-Julian charge density or the net electron charge density of the star.","The interaction of the electron with the long-range scalar profile results in a spatial variation of the electron mass.","A scalar induced magnetic field is created due to such interaction.","The mass of the scalar in such cases is constrained by the radius of the star.","The scalar field can also radiate from a binary system or an isolated star if the mass of the scalar is less than the orbital frequency and the spin frequency respectively.","The electrophilic scalar radiation can contribute to the orbital period loss of binary systems and pulsar spin-down.","Comparing with existing and projected experimental sensitivities, we obtain constraints on scalar coupling with ultralight mass.","Some of these bounds are stronger than the existing fifth force constraints.","The constraints on the scalar coupling can be significantly screened if the scalar has a coupling with the ubiquitous cosmic neutrino background.","Improvements in experimental sensitivity and observations of compact objects with stronger magnetic fields and higher angular velocities could further refine these bounds."],"url":"http://arxiv.org/abs/2404.18309v1","category":"hep-ph"}
{"created":"2024-04-28 17:47:36","title":"Around-the-world seismic echo as a trigger for aftershocks and the main shock of an earthquake","abstract":"The essence of the cumulative effect of a round-the-world seismic echo is that the echo can serve as a trigger for a second tremors in the epicentral zone of the earthquake that gave rise to the echo. According to the classification of triggers, the round-the-world echo is an endogenous force mechanical additive trigger. The round-the-world echo excited by the main shock as a trigger for aftershocks has been studied in detail previously. In this work, the question of whether a foreshock can excite a round-the-world echo, which will turn out to be a trigger for the main shock, is posed and experimentally studied. In the course of the study, the classification of the so-called triads of earthquakes was considered. When studying one of the six types of triads, indirect signs were found that the answer to the question posed may be positive Key words: earthquake source, main rupture, foreshock, triad of earthquakes, cumulative effect, free vibrations of the Earth.","sentences":["The essence of the cumulative effect of a round-the-world seismic echo is that the echo can serve as a trigger for a second tremors in the epicentral zone of the earthquake that gave rise to the echo.","According to the classification of triggers, the round-the-world echo is an endogenous force mechanical additive trigger.","The round-the-world echo excited by the main shock as a trigger for aftershocks has been studied in detail previously.","In this work, the question of whether a foreshock can excite a round-the-world echo, which will turn out to be a trigger for the main shock, is posed and experimentally studied.","In the course of the study, the classification of the so-called triads of earthquakes was considered.","When studying one of the six types of triads, indirect signs were found that the answer to the question posed may be positive Key words: earthquake source, main rupture, foreshock, triad of earthquakes, cumulative effect, free vibrations of the Earth."],"url":"http://arxiv.org/abs/2404.18258v1","category":"physics.geo-ph"}
