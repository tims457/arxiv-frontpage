{"created":"2024-01-30 18:59:56","title":"A simple, strong baseline for building damage detection on the xBD dataset","abstract":"We construct a strong baseline method for building damage detection by starting with the highly-engineered winning solution of the xView2 competition, and gradually stripping away components. This way, we obtain a much simpler method, while retaining adequate performance. We expect the simplified solution to be more widely and easily applicable. This expectation is based on the reduced complexity, as well as the fact that we choose hyperparameters based on simple heuristics, that transfer to other datasets. We then re-arrange the xView2 dataset splits such that the test locations are not seen during training, contrary to the competition setup. In this setting, we find that both the complex and the simplified model fail to generalize to unseen locations. Analyzing the dataset indicates that this failure to generalize is not only a model-based problem, but that the difficulty might also be influenced by the unequal class distributions between events.   Code, including the baseline model, is available under https://github.com/PaulBorneP/Xview2_Strong_Baseline","sentences":["We construct a strong baseline method for building damage detection by starting with the highly-engineered winning solution of the xView2 competition, and gradually stripping away components.","This way, we obtain a much simpler method, while retaining adequate performance.","We expect the simplified solution to be more widely and easily applicable.","This expectation is based on the reduced complexity, as well as the fact that we choose hyperparameters based on simple heuristics, that transfer to other datasets.","We then re-arrange the xView2 dataset splits such that the test locations are not seen during training, contrary to the competition setup.","In this setting, we find that both the complex and the simplified model fail to generalize to unseen locations.","Analyzing the dataset indicates that this failure to generalize is not only a model-based problem, but that the difficulty might also be influenced by the unequal class distributions between events.   ","Code, including the baseline model, is available under https://github.com/PaulBorneP/Xview2_Strong_Baseline"],"url":"http://arxiv.org/abs/2401.17271v1","category":"cs.CV"}
{"created":"2024-01-30 18:59:38","title":"YOLO-World: Real-Time Open-Vocabulary Object Detection","abstract":"The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However, their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation.","sentences":["The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools.","However, their reliance on predefined and trained object categories limits their applicability in open scenarios.","Addressing this limitation, we introduce YOLO-World, an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets.","Specifically, we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information.","Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency.","On the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on V100, which outperforms many state-of-the-art methods in terms of both accuracy and speed.","Furthermore, the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks, including object detection and open-vocabulary instance segmentation."],"url":"http://arxiv.org/abs/2401.17270v1","category":"cs.CV"}
{"created":"2024-01-30 18:58:46","title":"Effect of Weight Quantization on Learning Models by Typical Case Analysis","abstract":"This paper examines the quantization methods used in large-scale data analysis models and their hyperparameter choices. The recent surge in data analysis scale has significantly increased computational resource requirements. To address this, quantizing model weights has become a prevalent practice in data analysis applications such as deep learning. Quantization is particularly vital for deploying large models on devices with limited computational resources. However, the selection of quantization hyperparameters, like the number of bits and value range for weight quantization, remains an underexplored area. In this study, we employ the typical case analysis from statistical physics, specifically the replica method, to explore the impact of hyperparameters on the quantization of simple learning models. Our analysis yields three key findings: (i) an unstable hyperparameter phase, known as replica symmetry breaking, occurs with a small number of bits and a large quantization width; (ii) there is an optimal quantization width that minimizes error; and (iii) quantization delays the onset of overparameterization, helping to mitigate overfitting as indicated by the double descent phenomenon. We also discover that non-uniform quantization can enhance stability. Additionally, we develop an approximate message-passing algorithm to validate our theoretical results.","sentences":["This paper examines the quantization methods used in large-scale data analysis models and their hyperparameter choices.","The recent surge in data analysis scale has significantly increased computational resource requirements.","To address this, quantizing model weights has become a prevalent practice in data analysis applications such as deep learning.","Quantization is particularly vital for deploying large models on devices with limited computational resources.","However, the selection of quantization hyperparameters, like the number of bits and value range for weight quantization, remains an underexplored area.","In this study, we employ the typical case analysis from statistical physics, specifically the replica method, to explore the impact of hyperparameters on the quantization of simple learning models.","Our analysis yields three key findings: (i) an unstable hyperparameter phase, known as replica symmetry breaking, occurs with a small number of bits and a large quantization width; (ii) there is an optimal quantization width that minimizes error; and (iii) quantization delays the onset of overparameterization, helping to mitigate overfitting as indicated by the double descent phenomenon.","We also discover that non-uniform quantization can enhance stability.","Additionally, we develop an approximate message-passing algorithm to validate our theoretical results."],"url":"http://arxiv.org/abs/2401.17269v1","category":"stat.ML"}
{"created":"2024-01-30 18:58:43","title":"Weaver: Foundation Models for Creative Writing","abstract":"This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.","sentences":["This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation.","Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models.","We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation.","The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost.","Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them.","Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes.","Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage).","We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance.","Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs."],"url":"http://arxiv.org/abs/2401.17268v1","category":"cs.CL"}
{"created":"2024-01-30 18:57:08","title":"ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models","abstract":"Chemical reactivity models are developed to predict chemical reaction outcomes in the form of classification (success/failure) or regression (product yield) tasks. The vast majority of the reported models are trained solely on chemical information such as reactants, products, reagents, and solvents, but not on the details of a synthetic protocol. Herein incorporation of procedural text with the aim to augment the Graphormer reactivity model and improve its accuracy is presented. Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the discernment of unpromising reactions, thereby providing more accurate models with improved specificity.","sentences":["Chemical reactivity models are developed to predict chemical reaction outcomes in the form of classification (success/failure) or regression (product yield) tasks.","The vast majority of the reported models are trained solely on chemical information such as reactants, products, reagents, and solvents, but not on the details of a synthetic protocol.","Herein incorporation of procedural text with the aim to augment the Graphormer reactivity model and improve its accuracy is presented.","Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA).","Both methodologies enhance the discernment of unpromising reactions, thereby providing more accurate models with improved specificity."],"url":"http://arxiv.org/abs/2401.17267v1","category":"cs.LG"}
{"created":"2024-01-30 18:56:40","title":"Partially Law-Invariant Risk Measures","abstract":"We introduce the concept of partial law invariance, generalizing the concept of law-invariant risk measures widely used in statistical and financial applications. This new concept is motivated by practical considerations of decision-making under uncertainty, thus connecting the literature on decision theory and that on financial risk management. We fully characterize partially law-invariant coherent risk measures via a novel representation formula, which, surprisingly, has little resemblance to the classical formula for law-invariant coherent risk measures. A notion of strong partial law invariance is introduced, allowing for a representation formula akin to the classical one. We propose a few classes of new risk measures, including partially law-invariant versions of the Expected Shortfall and the entropic risk measures, and illustrate their applications in risk assessment under model uncertainty.","sentences":["We introduce the concept of partial law invariance, generalizing the concept of law-invariant risk measures widely used in statistical and financial applications.","This new concept is motivated by practical considerations of decision-making under uncertainty, thus connecting the literature on decision theory and that on financial risk management.","We fully characterize partially law-invariant coherent risk measures via a novel representation formula, which, surprisingly, has little resemblance to the classical formula for law-invariant coherent risk measures.","A notion of strong partial law invariance is introduced, allowing for a representation formula akin to the classical one.","We propose a few classes of new risk measures, including partially law-invariant versions of the Expected Shortfall and the entropic risk measures, and illustrate their applications in risk assessment under model uncertainty."],"url":"http://arxiv.org/abs/2401.17265v1","category":"q-fin.RM"}
{"created":"2024-01-30 18:56:22","title":"Proactive Detection of Voice Cloning with Localized Watermarking","abstract":"In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator/detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed - achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.","sentences":["In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning.","We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech.","AudioSeal employs a generator/detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility.","AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics.","Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed - achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications."],"url":"http://arxiv.org/abs/2401.17264v1","category":"cs.SD"}
{"created":"2024-01-30 18:56:08","title":"Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks","abstract":"Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4 from 92% to 6%.","sentences":["Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior.","While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical.","To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs.","This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks.","In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4 from 92% to 6%."],"url":"http://arxiv.org/abs/2401.17263v1","category":"cs.LG"}
{"created":"2024-01-30 18:55:02","title":"Post-inflationary Dark Matter production and Leptogenesis: Metric versus Palatini formalism","abstract":"We investigate production of non-thermal dark matter particle and heavy sterile neutrino from inflaton during the reheating era which is preceded by a slow-roll inflationary epoch with a quartic potential and non-minimal coupling ($\\xi$) between the inflaton and the gravity. We compare our analysis between metric and Palatini formalism. For the latter with $\\xi=0.5$ and number of $e$-folds $\\sim 60$, $r$ can be as small as $\\sim {\\cal O}\\left(10^{-3}\\right)$ which may be validated at $1-\\sigma$ CL of prospective future reaches of upcoming CMB observation such as CMB-S4~etc. We identify that permissible range of Yukawa coupling $y_\\chi$ between inflaton and fermionic DM $\\chi$, to be ${\\cal O}\\left(10^{-3.5}\\right)\\gtrsim y_\\chi \\gtrsim {\\cal O}\\left(10^{-20}\\right)$ for metric formalism and ${\\cal O}\\left(10^{-4}\\right)\\gtrsim y_\\chi \\gtrsim {\\cal O}\\left(10^{-11}\\right)$ for Palatini formalism which is consistent with current PLANCK data and also be within the reach of future CMB experiments. For the scenario of leptogenesis via the decay of sterile neutrino produced from inflaton decay, we also investigate the parameter space of heavy neutrino mass $m_{N_1}$ and Yukawa coupling $y_{N_1}$ of sterile neutrino with inflaton, which are consistent with current CMB data and successful generation of the observed baryon asymmetry of the universe via leptogenesis. In contrast to metric formalism, in the case of Palatini formalism for successful leptogenesis to occur we find that $y_{N_1}$ has a very narrow allowable range and is severely constrained from the consistency with CMB predictions.","sentences":["We investigate production of non-thermal dark matter particle and heavy sterile neutrino from inflaton during the reheating era which is preceded by a slow-roll inflationary epoch with a quartic potential and non-minimal coupling ($\\xi$) between the inflaton and the gravity.","We compare our analysis between metric and Palatini formalism.","For the latter with $\\xi=0.5$ and number of $e$-folds $\\sim 60$, $r$ can be as small as $\\sim {\\cal O}\\left(10^{-3}\\right)$ which may be validated at $1-\\sigma$ CL of prospective future reaches of upcoming CMB observation such as CMB-S4~etc.","We identify that permissible range of Yukawa coupling $y_\\chi$ between inflaton and fermionic DM $\\chi$, to be ${\\cal O}\\left(10^{-3.5}\\right)\\gtrsim y_\\chi \\gtrsim {\\cal O}\\left(10^{-20}\\right)$ for metric formalism and ${\\cal O}\\left(10^{-4}\\right)\\gtrsim y_\\chi \\gtrsim {\\cal O}\\left(10^{-11}\\right)$ for Palatini formalism which is consistent with current PLANCK data and also be within the reach of future CMB experiments.","For the scenario of leptogenesis via the decay of sterile neutrino produced from inflaton decay, we also investigate the parameter space of heavy neutrino mass $m_{N_1}$ and Yukawa coupling $y_{N_1}$ of sterile neutrino with inflaton, which are consistent with current CMB data and successful generation of the observed baryon asymmetry of the universe via leptogenesis.","In contrast to metric formalism, in the case of Palatini formalism for successful leptogenesis to occur we find that $y_{N_1}$ has a very narrow allowable range and is severely constrained from the consistency with CMB predictions."],"url":"http://arxiv.org/abs/2401.17262v1","category":"astro-ph.CO"}
{"created":"2024-01-30 18:52:54","title":"Dark Matter Searches on a Photonic Chip","abstract":"Dark matter (DM) with masses of order an electronvolt or below can have a non-zero coupling to electromagnetism. In these models, the ambient DM behaves as a new classical source in Maxwell's equations, which can excite potentially detectable electromagnetic (EM) fields in the laboratory. We describe a new proposal for using integrated photonics to search for such DM candidates with masses in the 0.1 eV - few eV range. This approach offers a wide range of wavelength-scale devices like resonators and waveguides that can enable a novel and exciting experimental program. In particular, we show how refractive index-modulated resonators, such as grooved or periodically-poled microrings, or patterned slabs, support EM modes with efficient coupling to DM. When excited by the DM, these modes can be read out by coupling the resonators to a waveguide that terminates on a micron-scale-sized single photon detector, such as a single pixel of an ultra-quiet charge-coupled device or a superconducting nanowire. We then estimate the sensitivity of this experimental concept in the context of axion-like particle and dark photon models of DM, showing that the scaling and confinement advantages of nanophotonics may enable exploration of new DM parameter space.","sentences":["Dark matter (DM) with masses of order an electronvolt or below can have a non-zero coupling to electromagnetism.","In these models, the ambient DM behaves as a new classical source in Maxwell's equations, which can excite potentially detectable electromagnetic (EM) fields in the laboratory.","We describe a new proposal for using integrated photonics to search for such DM candidates with masses in the 0.1 eV - few eV range.","This approach offers a wide range of wavelength-scale devices like resonators and waveguides that can enable a novel and exciting experimental program.","In particular, we show how refractive index-modulated resonators, such as grooved or periodically-poled microrings, or patterned slabs, support EM modes with efficient coupling to DM.","When excited by the DM, these modes can be read out by coupling the resonators to a waveguide that terminates on a micron-scale-sized single photon detector, such as a single pixel of an ultra-quiet charge-coupled device or a superconducting nanowire.","We then estimate the sensitivity of this experimental concept in the context of axion-like particle and dark photon models of DM, showing that the scaling and confinement advantages of nanophotonics may enable exploration of new DM parameter space."],"url":"http://arxiv.org/abs/2401.17260v1","category":"hep-ph"}
{"created":"2024-01-30 18:49:44","title":"You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation","abstract":"In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step. We propose a novel scale distillation approach to train our SR model. Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher. We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training. This process is repeated iteratively until we reach the target scale factor of the final model. The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve. We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference. Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it. We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step.","sentences":["In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach for image super-resolution that yields state-of-the-art results using only a single DDIM step.","We propose a novel scale distillation approach to train our SR model.","Instead of directly training our SR model on the scale factor of interest, we start by training a teacher model on a smaller magnification scale, thereby making the SR problem simpler for the teacher.","We then train a student model for a higher magnification scale, using the predictions of the teacher as a target during the training.","This process is repeated iteratively until we reach the target scale factor of the final model.","The rationale behind our scale distillation is that the teacher aids the student diffusion model training by i) providing a target adapted to the current noise level rather than using the same target coming from ground truth data for all noise levels and ii) providing an accurate target as the teacher has a simpler task to solve.","We empirically show that the distilled model significantly outperforms the model trained for high scales directly, specifically with few steps during inference.","Having a strong diffusion model that requires only one step allows us to freeze the U-Net and fine-tune the decoder on top of it.","We show that the combination of spatially distilled U-Net and fine-tuned decoder outperforms state-of-the-art methods requiring 200 steps with only one single step."],"url":"http://arxiv.org/abs/2401.17258v1","category":"cs.CV"}
{"created":"2024-01-30 18:48:37","title":"Weak-to-Strong Jailbreaking on Large Language Models","abstract":"Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding. Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations. This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs. The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations. Our study reveals a previously unnoticed yet efficient way of jailbreaking, exposing an urgent safety issue that needs to be considered when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong","sentences":["Although significant efforts have been dedicated to aligning large language models (LLMs), red-teaming reports suggest that these carefully aligned LLMs could still be jailbroken through adversarial prompts, tuning, or decoding.","Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that the decoding distributions of jailbroken and aligned models differ only in the initial generations.","This observation motivates us to propose the weak-to-strong jailbreaking attack, where adversaries can utilize smaller unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly larger aligned LLMs (e.g., 70B).","To jailbreak, one only needs to additionally decode two smaller LLMs once, which involves minimal computation and latency compared to decoding the larger LLMs.","The efficacy of this attack is demonstrated through experiments conducted on five models from three different organizations.","Our study reveals a previously unnoticed yet efficient way of jailbreaking, exposing an urgent safety issue that needs to be considered when aligning LLMs.","As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging.","The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong"],"url":"http://arxiv.org/abs/2401.17256v1","category":"cs.CL"}
{"created":"2024-01-30 18:46:30","title":"Towards Quantum Simulation of Non-Markovian Open Quantum Dynamics: A Universal and Compact Theory","abstract":"As quantum technologies continue to advance, the simulation of open quantum dynamics using quantum algorithms has garnered increasing attention. In this paper, we present a universal and compact theory, the dissipaton-embedded quantum master equation in second quantization (DQME-SQ), for simulating non-Markovian open quantum dynamics. The DQME-SQ theory is not only inprinciple exact for both bosonic and fermionic environments that satisfy Gaussian statistics, but also possesses a compact form that facilitates quantum simulations. To demonstrate the practicality of the DQME-SQ theory, we conduct digital quantum simulations of spin-boson and Anderson impurity models, highlighting the significant non-Markovian dynamical effects. The proposed theoretical framework establishes a solid foundation for the accurate and efficient simulation of complex open quantum systems.","sentences":["As quantum technologies continue to advance, the simulation of open quantum dynamics using quantum algorithms has garnered increasing attention.","In this paper, we present a universal and compact theory, the dissipaton-embedded quantum master equation in second quantization (DQME-SQ), for simulating non-Markovian open quantum dynamics.","The DQME-SQ theory is not only inprinciple exact for both bosonic and fermionic environments that satisfy Gaussian statistics, but also possesses a compact form that facilitates quantum simulations.","To demonstrate the practicality of the DQME-SQ theory, we conduct digital quantum simulations of spin-boson and Anderson impurity models, highlighting the significant non-Markovian dynamical effects.","The proposed theoretical framework establishes a solid foundation for the accurate and efficient simulation of complex open quantum systems."],"url":"http://arxiv.org/abs/2401.17255v1","category":"quant-ph"}
{"created":"2024-01-30 18:44:41","title":"Quantum $X$-Secure $B$-Byzantine $T$-Colluding Private Information Retrieval","abstract":"We consider the problems arising from the presence of Byzantine servers in a quantum private information retrieval (QPIR) setting. This is the first work to precisely define what the capabilities of Byzantine servers could be in a QPIR context. We show that quantum Byzantine servers have more capabilities than their classical counterparts due to the possibilities created by the quantum encoding procedure. We focus on quantum Byzantine servers that can apply any reversible operations on their individual qudits. In this case, the Byzantine servers can generate any error, i.e., this covers \\emph{all} possible single qudit operations that can be done by the Byzantine servers on their qudits. We design a scheme that is resilient to these kinds of manipulations. We show that the scheme designed achieves superdense coding gain in all cases, i.e., $R_Q= \\max \\left\\{0,\\min\\left\\{1,2\\left(1-\\frac{X+T+2B}{N}\\right)\\right\\}\\right\\}$.","sentences":["We consider the problems arising from the presence of Byzantine servers in a quantum private information retrieval (QPIR) setting.","This is the first work to precisely define what the capabilities of Byzantine servers could be in a QPIR context.","We show that quantum Byzantine servers have more capabilities than their classical counterparts due to the possibilities created by the quantum encoding procedure.","We focus on quantum Byzantine servers that can apply any reversible operations on their individual qudits.","In this case, the Byzantine servers can generate any error, i.e., this covers \\emph{all} possible single qudit operations that can be done by the Byzantine servers on their qudits.","We design a scheme that is resilient to these kinds of manipulations.","We show that the scheme designed achieves superdense coding gain in all cases, i.e., $R_Q= \\max \\left\\{0,\\min\\left\\{1,2\\left(1-\\frac{X+T+2B}{N}\\right)\\right\\}\\right\\}$."],"url":"http://arxiv.org/abs/2401.17252v1","category":"cs.IT"}
{"created":"2024-01-30 18:40:35","title":"Semantic Forwarding for Next Generation Relay Networks","abstract":"We consider cooperative semantic text communications facilitated by a relay node. We propose two types of semantic forwarding: semantic lossy forwarding (SLF) and semantic predict-and-forward (SPF). Both are machine learning aided approaches, and, in particular, utilize attention mechanisms at the relay to establish a dynamic semantic state, updated upon receiving a new source signal. In the SLF model, the semantic state is used to decode the received source signal; whereas in the SPF model, it is used to predict the next source signal, enabling proactive forwarding. Our proposed forwarding schemes do not need any channel state information and exhibit consistent performance regardless of the relay's position. Our results demonstrate that the proposed semantic forwarding techniques outperform conventional semantic-agnostic baselines.","sentences":["We consider cooperative semantic text communications facilitated by a relay node.","We propose two types of semantic forwarding: semantic lossy forwarding (SLF) and semantic predict-and-forward (SPF).","Both are machine learning aided approaches, and, in particular, utilize attention mechanisms at the relay to establish a dynamic semantic state, updated upon receiving a new source signal.","In the SLF model, the semantic state is used to decode the received source signal; whereas in the SPF model, it is used to predict the next source signal, enabling proactive forwarding.","Our proposed forwarding schemes do not need any channel state information and exhibit consistent performance regardless of the relay's position.","Our results demonstrate that the proposed semantic forwarding techniques outperform conventional semantic-agnostic baselines."],"url":"http://arxiv.org/abs/2401.17247v1","category":"cs.IT"}
{"created":"2024-01-30 18:37:45","title":"LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation","abstract":"Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from mixed data sources. Additionally, LLaMP substantially reduces the hallucinated volumetric strain in a diamond cubic silicon structure from 66.3% to 0. The proposed framework offers an intuitive and nearly hallucination-free approach to exploring materials informatics and establishes a pathway for knowledge distillation and fine-tuning other language models. We envision the framework as a valuable component for scientific hypotheses and a foundation for future autonomous laboratories where multiple LLM agents communicate and cooperate with robotics to drive material synthesis and chemical reactions without hard-coded human logic and intervention.","sentences":["Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial.","However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data.","Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP).","Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis.","We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from mixed data sources.","Additionally, LLaMP substantially reduces the hallucinated volumetric strain in a diamond cubic silicon structure from 66.3% to 0.","The proposed framework offers an intuitive and nearly hallucination-free approach to exploring materials informatics and establishes a pathway for knowledge distillation and fine-tuning other language models.","We envision the framework as a valuable component for scientific hypotheses and a foundation for future autonomous laboratories where multiple LLM agents communicate and cooperate with robotics to drive material synthesis and chemical reactions without hard-coded human logic and intervention."],"url":"http://arxiv.org/abs/2401.17244v1","category":"cs.CL"}
{"created":"2024-01-30 18:24:50","title":"Explicit Good Codes Approaching Distance 1 in Ulam Metric","abstract":"The Ulam distance of two permutations on $[n]$ is $n$ minus the length of their longest common subsequence. In this paper, we show that for every $\\varepsilon>0$, there exists some $\\alpha>0$, and an infinite set $\\Gamma\\subseteq \\mathbb{N}$, such that for all $n\\in\\Gamma$, there is an explicit set $C_n$ of $(n!)^{\\alpha}$ many permutations on $[n]$, such that every pair of permutations in $C_n$ has pairwise Ulam distance at least $(1-\\varepsilon)\\cdot n$. Moreover, we can compute the $i^{\\text{th}}$ permutation in $C_n$ in poly$(n)$ time and can also decode in poly$(n)$ time, a permutation $\\pi$ on $[n]$ to its closest permutation $\\pi^*$ in $C_n$, if the Ulam distance of $\\pi$ and $\\pi^*$ is less than $ \\frac{(1-\\varepsilon)\\cdot n}{4} $.   Previously, it was implicitly known by combining works of Goldreich and Wigderson [Israel Journal of Mathematics'23] and Farnoud, Skachek, and Milenkovic [IEEE Transactions on Information Theory'13] in a black-box manner, that it is possible to explicitly construct $(n!)^{\\Omega(1)}$ many permutations on $[n]$, such that every pair of them have pairwise Ulam distance at least $\\frac{n}{6}\\cdot (1-\\varepsilon)$, for any $\\varepsilon>0$, and the bound on the distance can be improved to $\\frac{n}{4}\\cdot (1-\\varepsilon)$ if the construction of Goldreich and Wigderson is directly analyzed in the Ulam metric.","sentences":["The Ulam distance of two permutations on $[n]$ is $n$ minus the length of their longest common subsequence.","In this paper, we show that for every $\\varepsilon>0$, there exists some $\\alpha>0$, and an infinite set $\\Gamma\\subseteq \\mathbb{N}$, such that for all $n\\in\\Gamma$, there is an explicit set $C_n$ of $(n!)^{\\alpha}$ many permutations on $[n]$, such that every pair of permutations in $C_n$ has pairwise Ulam distance at least $(1-\\varepsilon)\\cdot n$. Moreover, we can compute the $i^{\\text{th}}$ permutation in $C_n$ in poly$(n)$ time and can also decode in poly$(n)$ time, a permutation $\\pi$ on $[n]$ to its closest permutation $\\pi^*$ in $C_n$, if the Ulam distance of $\\pi$ and $\\pi^*$ is less than $ \\frac{(1-\\varepsilon)\\cdot n}{4} $.   ","Previously, it was implicitly known by combining works of Goldreich and Wigderson","[Israel Journal of Mathematics'23] and Farnoud, Skachek, and Milenkovic [IEEE Transactions on Information Theory'13] in a black-box manner, that it is possible to explicitly construct $(n!)^{\\Omega(1)}$ many permutations on $[n]$, such that every pair of them have pairwise Ulam distance at least $\\frac{n}{6}\\cdot (1-\\varepsilon)$, for any $\\varepsilon>0$, and the bound on the distance can be improved to $\\frac{n}{4}\\cdot (1-\\varepsilon)$ if the construction of Goldreich and Wigderson is directly analyzed in the Ulam metric."],"url":"http://arxiv.org/abs/2401.17235v1","category":"cs.IT"}
{"created":"2024-01-30 18:23:28","title":"Asynchronous Distributed Genetic Algorithms with Javascript and JSON","abstract":"In a connected world, spare CPU cycles are up for grabs, if you only make its obtention easy enough. In this paper we present a distributed evolutionary computation system that uses the computational capabilities of the ubiquituous web browser. Using Asynchronous Javascript and JSON (Javascript Object Notation, a serialization protocol) allows anybody with a web browser (that is, mostly everybody connected to the Internet) to participate in a genetic algorithm experiment with little effort, or none at all. Since, in this case, computing becomes a social activity and is inherently impredictable, in this paper we will explore the performance of this kind of virtual computer by solving simple problems such as the Royal Road function and analyzing how many machines and evaluations it yields. We will also examine possible performance bottlenecks and how to solve them, and, finally, issue some advice on how to set up this kind of experiments to maximize turnout and, thus, performance.","sentences":["In a connected world, spare CPU cycles are up for grabs, if you only make its obtention easy enough.","In this paper we present a distributed evolutionary computation system that uses the computational capabilities of the ubiquituous web browser.","Using Asynchronous Javascript and JSON (Javascript Object Notation, a serialization protocol) allows anybody with a web browser (that is, mostly everybody connected to the Internet) to participate in a genetic algorithm experiment with little effort, or none at all.","Since, in this case, computing becomes a social activity and is inherently impredictable, in this paper we will explore the performance of this kind of virtual computer by solving simple problems such as the Royal Road function and analyzing how many machines and evaluations it yields.","We will also examine possible performance bottlenecks and how to solve them, and, finally, issue some advice on how to set up this kind of experiments to maximize turnout and, thus, performance."],"url":"http://arxiv.org/abs/2401.17234v1","category":"cs.NE"}
{"created":"2024-01-30 18:18:41","title":"ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment","abstract":"Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, \"Re(presentational)Al(ignment)net\", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visual representational patterns across object categories and different neural data modalities. Furthermore, we discover that alignment with human brain representations improves the model's adversarial robustness. Our findings suggest that ReAlnet sets a new precedent in the field, bridging the gap between artificial and human vision, and paving the way for more brain-like artificial intelligence systems.","sentences":["Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains.","Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models.","Addressing this gap, we present, for the first time, \"Re(presentational)Al(ignment)net\", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations.","Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visual representational patterns across object categories and different neural data modalities.","Furthermore, we discover that alignment with human brain representations improves the model's adversarial robustness.","Our findings suggest that ReAlnet sets a new precedent in the field, bridging the gap between artificial and human vision, and paving the way for more brain-like artificial intelligence systems."],"url":"http://arxiv.org/abs/2401.17231v1","category":"cs.CV"}
{"created":"2024-01-30 18:18:27","title":"ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models","abstract":"This paper introduces ESPnet-SPK, a toolkit designed with several objectives for training speaker embedding extractors. First, we provide an open-source platform for researchers in the speaker recognition community to effortlessly build models. We provide several models, ranging from x-vector to recent SKA-TDNN. Through the modularized architecture design, variants can be developed easily. We also aspire to bridge developed models with other domains, facilitating the broad research community to effortlessly incorporate state-of-the-art embedding extractors. Pre-trained embedding extractors can be accessed in an off-the-shelf manner and we demonstrate the toolkit's versatility by showcasing its integration with two tasks. Another goal is to integrate with diverse self-supervised learning features. We release a reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O evaluation protocol using WavLM-Large with ECAPA-TDNN.","sentences":["This paper introduces ESPnet-SPK, a toolkit designed with several objectives for training speaker embedding extractors.","First, we provide an open-source platform for researchers in the speaker recognition community to effortlessly build models.","We provide several models, ranging from x-vector to recent SKA-TDNN.","Through the modularized architecture design, variants can be developed easily.","We also aspire to bridge developed models with other domains, facilitating the broad research community to effortlessly incorporate state-of-the-art embedding extractors.","Pre-trained embedding extractors can be accessed in an off-the-shelf manner and we demonstrate the toolkit's versatility by showcasing its integration with two tasks.","Another goal is to integrate with diverse self-supervised learning features.","We release a reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O evaluation protocol using WavLM-Large with ECAPA-TDNN."],"url":"http://arxiv.org/abs/2401.17230v1","category":"cs.SD"}
{"created":"2024-01-30 18:15:25","title":"Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning","abstract":"Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms. However, existing works often treat morality as binary, ranging from right to wrong. This simplistic view does not capture the nuances of moral judgment. Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment. In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach. We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively. Our results show that a pluralist approach to morality can be captured in an embedding space. However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels.","sentences":["Recent advances in NLP show that language models retain a discernible level of knowledge in deontological ethics and moral norms.","However, existing works often treat morality as binary, ranging from right to wrong.","This simplistic view does not capture the nuances of moral judgment.","Pluralist moral philosophers argue that human morality can be deconstructed into a finite number of elements, respecting individual differences in moral judgment.","In line with this view, we build a pluralist moral sentence embedding space via a state-of-the-art contrastive learning approach.","We systematically investigate the embedding space by studying the emergence of relationships among moral elements, both quantitatively and qualitatively.","Our results show that a pluralist approach to morality can be captured in an embedding space.","However, moral pluralism is challenging to deduce via self-supervision alone and requires a supervised approach with human labels."],"url":"http://arxiv.org/abs/2401.17228v1","category":"cs.CL"}
{"created":"2024-01-30 18:14:07","title":"Knowledge Problems in Protocol Analysis: Extending the Notion of Subterm Convergent","abstract":"We introduce a new form of restricted term rewrite system, the graph-embedded term rewrite system. These systems, and thus the name, are inspired by the graph minor relation and are more flexible extensions of the well-known homeomorphic-embedded property of term rewrite systems. As a motivating application area, we consider the symbolic analysis of security protocols, and more precisely the two knowledge problems defined by the deduction problem and the static equivalence problem. In this field restricted term rewrite systems, such as subterm convergent ones, have proven useful since the knowledge problems are decidable for such systems. Many of the same decision procedures still work for examples of systems which are \"beyond subterm convergent\". However, the applicability of the corresponding decision procedures to these examples must often be proven on an individual basis. This is due to the problem that they don't fit into an existing syntactic definition for which the procedures are known to work. Here we show that many of these systems belong to a particular subclass of graph-embedded convergent systems, called contracting convergent systems. On the one hand, we show that the knowledge problems are decidable for the subclass of contracting convergent systems. On the other hand, we show that the knowledge problems are undecidable for the class of graph-embedded systems. Going further, we compare and contrast these graph embedded systems with several notions and properties already known in the protocol analysis literature. Finally, we provide several combination results, both for the combination of multiple contracting convergent systems, and then for the combination of contracting convergent systems with particular permutative equational theories.","sentences":["We introduce a new form of restricted term rewrite system, the graph-embedded term rewrite system.","These systems, and thus the name, are inspired by the graph minor relation and are more flexible extensions of the well-known homeomorphic-embedded property of term rewrite systems.","As a motivating application area, we consider the symbolic analysis of security protocols, and more precisely the two knowledge problems defined by the deduction problem and the static equivalence problem.","In this field restricted term rewrite systems, such as subterm convergent ones, have proven useful since the knowledge problems are decidable for such systems.","Many of the same decision procedures still work for examples of systems which are \"beyond subterm convergent\".","However, the applicability of the corresponding decision procedures to these examples must often be proven on an individual basis.","This is due to the problem that they don't fit into an existing syntactic definition for which the procedures are known to work.","Here we show that many of these systems belong to a particular subclass of graph-embedded convergent systems, called contracting convergent systems.","On the one hand, we show that the knowledge problems are decidable for the subclass of contracting convergent systems.","On the other hand, we show that the knowledge problems are undecidable for the class of graph-embedded systems.","Going further, we compare and contrast these graph embedded systems with several notions and properties already known in the protocol analysis literature.","Finally, we provide several combination results, both for the combination of multiple contracting convergent systems, and then for the combination of contracting convergent systems with particular permutative equational theories."],"url":"http://arxiv.org/abs/2401.17226v1","category":"cs.LO"}
{"created":"2024-01-30 18:11:31","title":"Evolvable Agents, a Fine Grained Approach for Distributed Evolutionary Computing: Walking towards the Peer-to-Peer Computing Frontiers","abstract":"In this work we propose a fine grained approach with self-adaptive migration rate for distributed evolutionary computation. Our target is to gain some insights on the effects caused by communication when the algorithm scales. To this end, we consider a set of basic topologies in order to avoid the overlapping of algorithmic effects between communication and topological structures. We analyse the approach viability by comparing how solution quality and algorithm speed change when the number of processors increases and compare it with an Island model based implementation. A finer-grained approach implies a better chance of achieving a larger scalable system; such a feature is crucial concerning large-scale parallel architectures such as Peer-to-Peer systems. In order to check scalability, we perform a threefold experimental evaluation of this model: First, we concentrate on the algorithmic results when the problem scales up to eight nodes in comparison with how it does following the Island model. Second, we analyse the computing time speedup of the approach while scaling. Finally, we analyse the network performance with the proposed self-adaptive migration rate policy that depends on the link latency and bandwidth. With this experimental setup, our approach shows better scalability than the Island model and a equivalent robustness on the average of the three test functions under study.","sentences":["In this work we propose a fine grained approach with self-adaptive migration rate for distributed evolutionary computation.","Our target is to gain some insights on the effects caused by communication when the algorithm scales.","To this end, we consider a set of basic topologies in order to avoid the overlapping of algorithmic effects between communication and topological structures.","We analyse the approach viability by comparing how solution quality and algorithm speed change when the number of processors increases and compare it with an Island model based implementation.","A finer-grained approach implies a better chance of achieving a larger scalable system; such a feature is crucial concerning large-scale parallel architectures such as Peer-to-Peer systems.","In order to check scalability, we perform a threefold experimental evaluation of this model:","First, we concentrate on the algorithmic results when the problem scales up to eight nodes in comparison with how it does following the Island model.","Second, we analyse the computing time speedup of the approach while scaling.","Finally, we analyse the network performance with the proposed self-adaptive migration rate policy that depends on the link latency and bandwidth.","With this experimental setup, our approach shows better scalability than the Island model and a equivalent robustness on the average of the three test functions under study."],"url":"http://arxiv.org/abs/2401.17224v1","category":"cs.NE"}
{"created":"2024-01-30 18:09:11","title":"MouSi: Poly-Visual-Expert Vision-Language Models","abstract":"Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations. For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1. Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated. We have open-sourced the training code used in this report. All of these resources can be found on our project website.","sentences":["Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens.","These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information.","Addressing these challenges is crucial for enhancing the performance and applicability of VLMs.","This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc.","This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs.","In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length limitations.","For instance, in our implementation, this technique significantly reduces the positional occupancy in models like SAM, from a substantial 4096 to a more efficient and manageable 64 or even down to 1.","Experimental results demonstrate that VLMs with multiple experts exhibit consistently superior performance over isolated visual encoders and mark a significant performance boost as more experts are integrated.","We have open-sourced the training code used in this report.","All of these resources can be found on our project website."],"url":"http://arxiv.org/abs/2401.17221v1","category":"cs.CV"}
{"created":"2024-01-30 18:02:44","title":"GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual AI for Smart Eyewear","abstract":"Multimodal large language models (LMMs) excel in world knowledge and problem-solving abilities. Through the use of a world-facing camera and contextual AI, emerging smart accessories aim to provide a seamless interface between humans and LMMs. Yet, these wearable computing systems lack an understanding of the user's attention. We introduce GazeGPT as a new user interaction paradigm for contextual AI. GazeGPT uses eye tracking to help the LMM understand which object in the world-facing camera view a user is paying attention to. Using extensive user evaluations, we show that this gaze-contingent mechanism is a faster and more accurate pointing mechanism than alternatives; that it augments human capabilities by significantly improving their accuracy in a dog-breed classification task; and that it is consistently ranked as more natural than head- or body-driven selection mechanisms for contextual AI. Moreover, we prototype a variety of application scenarios that suggest GazeGPT could be of significant value to users as part of future AI-driven personal assistants.","sentences":["Multimodal large language models (LMMs) excel in world knowledge and problem-solving abilities.","Through the use of a world-facing camera and contextual AI, emerging smart accessories aim to provide a seamless interface between humans and LMMs.","Yet, these wearable computing systems lack an understanding of the user's attention.","We introduce GazeGPT as a new user interaction paradigm for contextual AI.","GazeGPT uses eye tracking to help the LMM understand which object in the world-facing camera view a user is paying attention to.","Using extensive user evaluations, we show that this gaze-contingent mechanism is a faster and more accurate pointing mechanism than alternatives; that it augments human capabilities by significantly improving their accuracy in a dog-breed classification task; and that it is consistently ranked as more natural than head- or body-driven selection mechanisms for contextual AI.","Moreover, we prototype a variety of application scenarios that suggest GazeGPT could be of significant value to users as part of future AI-driven personal assistants."],"url":"http://arxiv.org/abs/2401.17217v1","category":"cs.HC"}
{"created":"2024-01-30 18:00:42","title":"Multi-FLEX: An Automatic Task Sequence Execution Framework to Enable Reactive Motion Planning for Multi-Robot Applications","abstract":"In this letter, an integrated task planning and reactive motion planning framework termed Multi-FLEX is presented that targets real-world, industrial multi-robot applications. Reactive motion planning has been attractive for the purposes of collision avoidance, particularly when there are sources of uncertainty and variation. Most industrial applications, though, typically require parts of motion to be at least partially non-reactive in order to achieve functional objectives. Multi-FLEX resolves this dissonance and enables such applications to take advantage of reactive motion planning. The Multi-FLEX framework achieves 1) coordination of motion requests to resolve task-level conflicts and overlaps, 2) incorporation of application-specific task constraints into online motion planning using the new concepts of task dependency accommodation, task decomposition, and task bundling, and 3) online generation of robot trajectories using a custom, online reactive motion planner. This planner combines fast-to-create, sparse dynamic roadmaps (to find a complete path to the goal) with fast-to-execute, short-horizon, online, optimization-based local planning (for collision avoidance and high performance). To demonstrate, we use two six-degree-of-freedom, high-speed industrial robots in a deburring application to show the ability of this approach to not just handle collision avoidance and task variations, but to also achieve industrial applications.","sentences":["In this letter, an integrated task planning and reactive motion planning framework termed Multi-FLEX is presented that targets real-world, industrial multi-robot applications.","Reactive motion planning has been attractive for the purposes of collision avoidance, particularly when there are sources of uncertainty and variation.","Most industrial applications, though, typically require parts of motion to be at least partially non-reactive in order to achieve functional objectives.","Multi-FLEX resolves this dissonance and enables such applications to take advantage of reactive motion planning.","The Multi-FLEX framework achieves 1) coordination of motion requests to resolve task-level conflicts and overlaps, 2) incorporation of application-specific task constraints into online motion planning using the new concepts of task dependency accommodation, task decomposition, and task bundling, and 3) online generation of robot trajectories using a custom, online reactive motion planner.","This planner combines fast-to-create, sparse dynamic roadmaps (to find a complete path to the goal) with fast-to-execute, short-horizon, online, optimization-based local planning (for collision avoidance and high performance).","To demonstrate, we use two six-degree-of-freedom, high-speed industrial robots in a deburring application to show the ability of this approach to not just handle collision avoidance and task variations, but to also achieve industrial applications."],"url":"http://arxiv.org/abs/2401.17214v1","category":"cs.RO"}
{"created":"2024-01-30 17:57:46","title":"ContactGen: Contact-Guided Interactive 3D Human Generation for Partners","abstract":"Among various interactions between humans, such as eye contact and gestures, physical interactions by contact can act as an essential moment in understanding human behaviors. Inspired by this fact, given a 3D partner human with the desired interaction label, we introduce a new task of 3D human generation in terms of physical contact. Unlike previous works of interacting with static objects or scenes, a given partner human can have diverse poses and different contact regions according to the type of interaction. To handle this challenge, we propose a novel method of generating interactive 3D humans for a given partner human based on a guided diffusion framework. Specifically, we newly present a contact prediction module that adaptively estimates potential contact regions between two input humans according to the interaction label. Using the estimated potential contact regions as complementary guidances, we dynamically enforce ContactGen to generate interactive 3D humans for a given partner human within a guided diffusion model. We demonstrate ContactGen on the CHI3D dataset, where our method generates physically plausible and diverse poses compared to comparison methods.","sentences":["Among various interactions between humans, such as eye contact and gestures, physical interactions by contact can act as an essential moment in understanding human behaviors.","Inspired by this fact, given a 3D partner human with the desired interaction label, we introduce a new task of 3D human generation in terms of physical contact.","Unlike previous works of interacting with static objects or scenes, a given partner human can have diverse poses and different contact regions according to the type of interaction.","To handle this challenge, we propose a novel method of generating interactive 3D humans for a given partner human based on a guided diffusion framework.","Specifically, we newly present a contact prediction module that adaptively estimates potential contact regions between two input humans according to the interaction label.","Using the estimated potential contact regions as complementary guidances, we dynamically enforce ContactGen to generate interactive 3D humans for a given partner human within a guided diffusion model.","We demonstrate ContactGen on the CHI3D dataset, where our method generates physically plausible and diverse poses compared to comparison methods."],"url":"http://arxiv.org/abs/2401.17212v1","category":"cs.CV"}
{"created":"2024-01-30 17:55:37","title":"Quantum dynamics in one and two dimensions via recursion method","abstract":"We report an implementation of the recursion method that addresses quantum many-body dynamics in the nonperturbative regime. The implementation has two key ingredients: a computer-algebraic routine for symbolic calculation of nested commutators and a procedure to extrapolate the sequence of Lanczos coefficients according to the universal operator growth hypothesis. We apply the method to calculate infinite-temperature correlation functions for spin-$1/2$ systems on one- and two-dimensional lattices. In two dimensions the accessible timescale is large enough to essentially embrace the relaxation to equilibrium. The method allows one to accurately calculate transport coefficients. As an illustration, we compute the diffusion constant for the transverse-field Ising model on a square lattice.","sentences":["We report an implementation of the recursion method that addresses quantum many-body dynamics in the nonperturbative regime.","The implementation has two key ingredients: a computer-algebraic routine for symbolic calculation of nested commutators and a procedure to extrapolate the sequence of Lanczos coefficients according to the universal operator growth hypothesis.","We apply the method to calculate infinite-temperature correlation functions for spin-$1/2$ systems on one- and two-dimensional lattices.","In two dimensions the accessible timescale is large enough to essentially embrace the relaxation to equilibrium.","The method allows one to accurately calculate transport coefficients.","As an illustration, we compute the diffusion constant for the transverse-field Ising model on a square lattice."],"url":"http://arxiv.org/abs/2401.17211v1","category":"cond-mat.str-el"}
{"created":"2024-01-30 17:49:53","title":"Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI","abstract":"A comprehensive understanding of the organizational principles in the human brain requires, among other factors, well-quantifiable descriptors of nerve fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a microscopic imaging technique that enables insights into the fine-grained organization of myelinated nerve fibers with high resolution. Descriptors characterizing the fiber architecture observed in 3D-PLI would enable downstream analysis tasks such as multimodal correlation studies, clustering, and mapping. However, best practices for observer-independent characterization of fiber architecture in 3D-PLI are not yet available. To this end, we propose the application of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images using self-supervised representation learning. We introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning. We combine this sampling strategy with specifically designed image augmentations to gain robustness to typical variations in 3D-PLI parameter maps. The approach is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey brain. We show that extracted features are highly sensitive to different configurations of nerve fibers, yet robust to variations between consecutive brain sections arising from histological processing. We demonstrate their practical applicability for retrieving clusters of homogeneous fiber architecture and performing data mining for interactively selected templates of specific components of fiber architecture such as U-fibers.","sentences":["A comprehensive understanding of the organizational principles in the human brain requires, among other factors, well-quantifiable descriptors of nerve fiber architecture.","Three-dimensional polarized light imaging (3D-PLI) is a microscopic imaging technique that enables insights into the fine-grained organization of myelinated nerve fibers with high resolution.","Descriptors characterizing the fiber architecture observed in 3D-PLI would enable downstream analysis tasks such as multimodal correlation studies, clustering, and mapping.","However, best practices for observer-independent characterization of fiber architecture in 3D-PLI are not yet available.","To this end, we propose the application of a fully data-driven approach to characterize nerve fiber architecture in 3D-PLI images using self-supervised representation learning.","We introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the spatial neighborhood of texture examples across histological brain sections of a 3D reconstructed volume to sample positive pairs for contrastive learning.","We combine this sampling strategy with specifically designed image augmentations to gain robustness to typical variations in 3D-PLI parameter maps.","The approach is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey brain.","We show that extracted features are highly sensitive to different configurations of nerve fibers, yet robust to variations between consecutive brain sections arising from histological processing.","We demonstrate their practical applicability for retrieving clusters of homogeneous fiber architecture and performing data mining for interactively selected templates of specific components of fiber architecture such as U-fibers."],"url":"http://arxiv.org/abs/2401.17207v1","category":"cs.CV"}
{"created":"2024-01-30 17:47:07","title":"Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT Semantic Embeddings K-Means-Infused CRF Model","abstract":"Named Entity Recognition (NER) is a sub-task of Natural Language Processing (NLP) that distinguishes entities from unorganized text into predefined categorization. In recent years, a lot of Bangla NLP subtasks have received quite a lot of attention; but Named Entity Recognition in Bangla still lags behind. In this research, we explored the existing state of research in Bangla Named Entity Recognition. We tried to figure out the limitations that current techniques and datasets face, and we would like to address these limitations in our research. Additionally, We developed a Gazetteer that has the ability to significantly boost the performance of NER. We also proposed a new NER solution by taking advantage of state-of-the-art NLP tools that outperform conventional techniques.","sentences":["Named Entity Recognition (NER) is a sub-task of Natural Language Processing (NLP) that distinguishes entities from unorganized text into predefined categorization.","In recent years, a lot of Bangla NLP subtasks have received quite a lot of attention; but Named Entity Recognition in Bangla still lags behind.","In this research, we explored the existing state of research in Bangla Named Entity Recognition.","We tried to figure out the limitations that current techniques and datasets face, and we would like to address these limitations in our research.","Additionally, We developed a Gazetteer that has the ability to significantly boost the performance of NER.","We also proposed a new NER solution by taking advantage of state-of-the-art NLP tools that outperform conventional techniques."],"url":"http://arxiv.org/abs/2401.17206v1","category":"cs.CL"}
{"created":"2024-01-30 17:45:47","title":"Adaptive Experiment Design with Synthetic Controls","abstract":"Clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients. However, patients in large populations rarely respond the same way to the same treatment. This heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations - especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation. Motivated by this need, we propose Syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations. Syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations. We validate the performance of Syntax and provide insights into when it might have an advantage over conventional trial designs through experiments.","sentences":["Clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients.","However, patients in large populations rarely respond the same way to the same treatment.","This heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations - especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation.","Motivated by this need, we propose Syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations.","Syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations.","We validate the performance of Syntax and provide insights into when it might have an advantage over conventional trial designs through experiments."],"url":"http://arxiv.org/abs/2401.17205v1","category":"stat.ML"}
{"created":"2024-01-30 17:38:48","title":"CPR++: Object Localization via Single Coarse Point Supervision","abstract":"Point-based object localization (POL), which pursues high-performance object sensing under low-cost data annotation, has attracted increased attention. However, the point annotation mode inevitably introduces semantic variance due to the inconsistency of annotated points. Existing POL heavily rely on strict annotation rules, which are difficult to define and apply, to handle the problem. In this study, we propose coarse point refinement (CPR), which to our best knowledge is the first attempt to alleviate semantic variance from an algorithmic perspective. CPR reduces the semantic variance by selecting a semantic centre point in a neighbourhood region to replace the initial annotated point. Furthermore, We design a sampling region estimation module to dynamically compute a sampling region for each object and use a cascaded structure to achieve end-to-end optimization. We further integrate a variance regularization into the structure to concentrate the predicted scores, yielding CPR++. We observe that CPR++ can obtain scale information and further reduce the semantic variance in a global region, thus guaranteeing high-performance object localization. Extensive experiments on four challenging datasets validate the effectiveness of both CPR and CPR++. We hope our work can inspire more research on designing algorithms rather than annotation rules to address the semantic variance problem in POL. The dataset and code will be public at github.com/ucas-vg/PointTinyBenchmark.","sentences":["Point-based object localization (POL), which pursues high-performance object sensing under low-cost data annotation, has attracted increased attention.","However, the point annotation mode inevitably introduces semantic variance due to the inconsistency of annotated points.","Existing POL heavily rely on strict annotation rules, which are difficult to define and apply, to handle the problem.","In this study, we propose coarse point refinement (CPR), which to our best knowledge is the first attempt to alleviate semantic variance from an algorithmic perspective.","CPR reduces the semantic variance by selecting a semantic centre point in a neighbourhood region to replace the initial annotated point.","Furthermore, We design a sampling region estimation module to dynamically compute a sampling region for each object and use a cascaded structure to achieve end-to-end optimization.","We further integrate a variance regularization into the structure to concentrate the predicted scores, yielding CPR++.","We observe that CPR++ can obtain scale information and further reduce the semantic variance in a global region, thus guaranteeing high-performance object localization.","Extensive experiments on four challenging datasets validate the effectiveness of both CPR and CPR++.","We hope our work can inspire more research on designing algorithms rather than annotation rules to address the semantic variance problem in POL.","The dataset and code will be public at github.com/ucas-vg/PointTinyBenchmark."],"url":"http://arxiv.org/abs/2401.17203v1","category":"cs.CV"}
{"created":"2024-01-30 17:35:00","title":"Optimal quantum teleportation of collaboration","abstract":"We consider a network of three spatially separated labs of Alice, Bob, and Charlie, with a two-qubit state shared between Alice-Bob and Bob-Charlie, and all of them can collaborate through LOCC. We focus on the problem of optimal and deterministic distribution of a quantum teleportation channel (QTC) between Alice and Charlie. This involves distributing a two-qubit entangled state between Alice and Charlie with an optimized fully entangled fraction (FEF) over all three-party trace-preserving (TP) LOCC, exceeding the classical bound. However, we find that the optimal distribution of QTC generally has no one-to-one correspondence with the optimal distribution of entanglement. For some specific class of pre-shared two-qubit states, we identify the set of sufficient TP LOCC strategies that optimally distribute QTC. In this context, the mentioned set is restricted, with Bob initiating operations and subsequently sharing the outcomes with Alice and Charlie. Following Bob's contribution and after it is discarded, Alice and Charlie have the freedom of local post-processing. It seems that if one of the pre-shared entangled states is noisy, the optimal distribution may not necessarily require the other one to be most resourceful, i.e., a maximally entangled state (MES). Furthermore, when both of the pre-shared entangled states are noisy, there are instances where an efficient Bob-assisted protocol (generally a suboptimal protocol distributing a channel with FEF larger than the classical bound) necessarily requires Bob's joint measurement to be either performing projective measurement (PVM) in partially entangled pure states or performing POVM. In this regard, our study also reveals that the RPBES protocol introduced in Ref. [Phys. Rev. Lett. 93. 260501] for efficient entanglement distribution (even optimally for some cases), is not an efficient protocol in general.","sentences":["We consider a network of three spatially separated labs of Alice, Bob, and Charlie, with a two-qubit state shared between Alice-Bob and Bob-Charlie, and all of them can collaborate through LOCC.","We focus on the problem of optimal and deterministic distribution of a quantum teleportation channel (QTC) between Alice and Charlie.","This involves distributing a two-qubit entangled state between Alice and Charlie with an optimized fully entangled fraction (FEF) over all three-party trace-preserving (TP) LOCC, exceeding the classical bound.","However, we find that the optimal distribution of QTC generally has no one-to-one correspondence with the optimal distribution of entanglement.","For some specific class of pre-shared two-qubit states, we identify the set of sufficient TP LOCC strategies that optimally distribute QTC.","In this context, the mentioned set is restricted, with Bob initiating operations and subsequently sharing the outcomes with Alice and Charlie.","Following Bob's contribution and after it is discarded, Alice and Charlie have the freedom of local post-processing.","It seems that if one of the pre-shared entangled states is noisy, the optimal distribution may not necessarily require the other one to be most resourceful, i.e., a maximally entangled state (MES).","Furthermore, when both of the pre-shared entangled states are noisy, there are instances where an efficient Bob-assisted protocol (generally a suboptimal protocol distributing a channel with FEF larger than the classical bound) necessarily requires Bob's joint measurement to be either performing projective measurement (PVM) in partially entangled pure states or performing POVM.","In this regard, our study also reveals that the RPBES protocol introduced in Ref.","[Phys. Rev. Lett.","93.","260501] for efficient entanglement distribution (even optimally for some cases), is not an efficient protocol in general."],"url":"http://arxiv.org/abs/2401.17201v1","category":"quant-ph"}
{"created":"2024-01-30 17:33:35","title":"NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble Techniques","abstract":"This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods. Our research brings three significant contributions. Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability. Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods. Lastly, we provide a library, facilitating the practical implementation of XAI ensembling, thus promoting the adoption of transparent and interpretable deep learning models.","sentences":["This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods.","Our research brings three significant contributions.","Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability.","Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods.","Lastly, we provide a library, facilitating the practical implementation of XAI ensembling, thus promoting the adoption of transparent and interpretable deep learning models."],"url":"http://arxiv.org/abs/2401.17200v1","category":"cs.LG"}
{"created":"2024-01-30 17:31:55","title":"A Mixed Linear and Graded Logic: Proofs, Terms, and Models","abstract":"Graded modal logics generalise standard modal logics via families of modalities indexed by an algebraic structure whose operations mediate between the different modalities. The graded \"of-course\" modality $!_r$ captures how many times a proposition is used and has an analogous interpretation to the of-course modality from linear logic; the of-course modality from linear logic can be modelled by a linear exponential comonad and graded of-course can be modelled by a graded linear exponential comonad. Benton showed in his seminal paper on Linear/Non-Linear logic that the of-course modality can be split into two modalities connecting intuitionistic logic with linear logic, forming a symmetric monoidal adjunction. Later, Fujii et al. demonstrated that every graded comonad can be decomposed into an adjunction and a 'strict action'. We give a similar result to Benton, leveraging Fujii et al.'s decomposition, showing that graded modalities can be split into two modalities connecting a graded logic with a graded linear logic. We propose a sequent calculus, its proof theory and categorical model, and a natural deduction system which we show is isomorphic to the sequent calculus. Interestingly, our system can also be understood as Linear/Non-Linear logic composed with an action that adds the grading, further illuminating the shared principles between linear logic and a class of graded modal logics.","sentences":["Graded modal logics generalise standard modal logics via families of modalities indexed by an algebraic structure whose operations mediate between the different modalities.","The graded \"of-course\" modality $!_r$ captures how many times a proposition is used and has an analogous interpretation to the of-course modality from linear logic; the of-course modality from linear logic can be modelled by a linear exponential comonad and graded of-course can be modelled by a graded linear exponential comonad.","Benton showed in his seminal paper on Linear/Non-Linear logic that the of-course modality can be split into two modalities connecting intuitionistic logic with linear logic, forming a symmetric monoidal adjunction.","Later, Fujii et al. demonstrated that every graded comonad can be decomposed into an adjunction and a 'strict action'.","We give a similar result to Benton, leveraging Fujii et al.'s decomposition, showing that graded modalities can be split into two modalities connecting a graded logic with a graded linear logic.","We propose a sequent calculus, its proof theory and categorical model, and a natural deduction system which we show is isomorphic to the sequent calculus.","Interestingly, our system can also be understood as Linear/Non-Linear logic composed with an action that adds the grading, further illuminating the shared principles between linear logic and a class of graded modal logics."],"url":"http://arxiv.org/abs/2401.17199v1","category":"cs.LO"}
{"created":"2024-01-30 17:31:25","title":"Euler transformation for multiple $q$-hypergeometric series from wall-crossing formula of $K$-theoretic vortex partition function","abstract":"We show that transformation formulas of multiple $q$-hypergeometric series agree with wall-crossing formulas of $K$-theoretic vortex partition functions obtained by Hwang, Yi and the author \\cite{Hwang:2017kmk}. For the vortex partition function in 3d $\\mathcal{N}=2$ gauge theory, we show that the wall-crossing formula agrees with the Kajihara transformation \\cite{kajihara2004euler}. For the vortex partition function in 3d $\\mathcal{N}=4$ gauge theory, we show that the wall-crossing formula agrees with the transformation formula by Halln\\\"as, Langmann, Noumi and Rosengren \\cite{Halln_s_2022}. Since the $K$-theoretic vortex partition functions are related with indices such as the $\\chi_t$-genus of the handsaw quiver variety, we discuss geometric interpretation of Euler transformations in terms of wall-crossing formulas of handsaw quiver variety.","sentences":["We show that transformation formulas of multiple $q$-hypergeometric series agree with wall-crossing formulas of $K$-theoretic vortex partition functions obtained by Hwang, Yi and the author \\cite{Hwang:2017kmk}.","For the vortex partition function in 3d $\\mathcal{N}=2$ gauge theory, we show that the wall-crossing formula agrees with the Kajihara transformation \\cite{kajihara2004euler}.","For the vortex partition function in 3d $\\mathcal{N}=4$ gauge theory, we show that the wall-crossing formula agrees with the transformation formula by Halln\\\"as, Langmann, Noumi and Rosengren \\cite{Halln_s_2022}.","Since the $K$-theoretic vortex partition functions are related with indices such as the $\\chi_t$-genus of the handsaw quiver variety, we discuss geometric interpretation of Euler transformations in terms of wall-crossing formulas of handsaw quiver variety."],"url":"http://arxiv.org/abs/2401.17198v1","category":"hep-th"}
{"created":"2024-01-30 17:31:19","title":"Data-efficient Fine-tuning for LLM-based Recommendation","abstract":"Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance. To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. Empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.","sentences":["Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation.","However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application.","To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data.","We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning.","While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   ","To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process.","To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples.","Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance.","To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score.","Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs.","Empirical results on three real-world datasets validate the effectiveness of our proposed method.","In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%."],"url":"http://arxiv.org/abs/2401.17197v1","category":"cs.IR"}
{"created":"2024-01-30 17:30:44","title":"Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers","abstract":"In text classification, creating an adversarial example means subtly perturbing a few words in a sentence without changing its meaning, causing it to be misclassified by a classifier. A concerning observation is that a significant portion of adversarial examples generated by existing methods change only one word. This single-word perturbation vulnerability represents a significant weakness in classifiers, which malicious users can exploit to efficiently create a multitude of adversarial examples. This paper studies this problem and makes the following key contributions: (1) We introduce a novel metric \\r{ho} to quantitatively assess a classifier's robustness against single-word perturbation. (2) We present the SP-Attack, designed to exploit the single-word perturbation vulnerability, achieving a higher attack success rate, better preserving sentence meaning, while reducing computation costs compared to state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims to improve \\r{ho} by applying data augmentation in learning. Experimental results on 4 datasets and BERT and distilBERT classifiers show that SP-Defense improves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the attack success rate of existing attack methods that involve multiple-word perturbations.","sentences":["In text classification, creating an adversarial example means subtly perturbing a few words in a sentence without changing its meaning, causing it to be misclassified by a classifier.","A concerning observation is that a significant portion of adversarial examples generated by existing methods change only one word.","This single-word perturbation vulnerability represents a significant weakness in classifiers, which malicious users can exploit to efficiently create a multitude of adversarial examples.","This paper studies this problem and makes the following key contributions: (1) We introduce a novel metric \\r{ho} to quantitatively assess a classifier's robustness against single-word perturbation.","(2) We present the SP-Attack, designed to exploit the single-word perturbation vulnerability, achieving a higher attack success rate, better preserving sentence meaning, while reducing computation costs compared to state-of-the-art adversarial methods.","(3) We propose SP-Defense, which aims to improve \\r{ho} by applying data augmentation in learning.","Experimental results on 4 datasets and BERT and distilBERT classifiers show that SP-Defense improves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the attack success rate of existing attack methods that involve multiple-word perturbations."],"url":"http://arxiv.org/abs/2401.17196v1","category":"cs.CL"}
{"created":"2024-01-30 17:30:30","title":"The point scatterer approximation for wave dynamics","abstract":"Given an open, bounded and connected set $\\Omega\\subset\\mathbb{R}^{3}$ and its rescaling $\\Omega_{\\varepsilon}$ of size $\\varepsilon\\ll 1$, we consider the solutions of the Cauchy problem for the inhomogeneous wave equation $$ (\\varepsilon^{-2}\\chi_{\\Omega_{\\varepsilon}}+\\chi_{\\mathbb{R}^{3}\\backslash\\Omega_{\\varepsilon}})\\partial_{tt}u=\\Delta u+f $$ with initial data and source supported outside $\\Omega_{\\varepsilon}$; here, $\\chi_{S}$ denotes the characteristic function of a set $S$. We provide the first-order $\\varepsilon$-corrections with respect to the solutions of the inhomogeneous free wave equation and give space-time estimates on the remainders in the $L^{\\infty}((0,1/\\varepsilon^{\\tau}),L^{2}(\\mathbb{R}^{3})) $-norm. Such corrections are explicitly expressed in terms of the eigenvalues and eigenfunctions of the Newton potential operator in $L^{2}(\\Omega)$ and provide an effective dynamics describing a legitimate point scatterer approximation in the time domain.","sentences":["Given an open, bounded and connected set $\\Omega\\subset\\mathbb{R}^{3}$ and its rescaling $\\Omega_{\\varepsilon}$ of size $\\varepsilon\\ll 1$, we consider the solutions of the Cauchy problem for the inhomogeneous wave equation $$ (\\varepsilon^{-2}\\chi_{\\Omega_{\\varepsilon}}+\\chi_{\\mathbb{R}^{3}\\backslash\\Omega_{\\varepsilon}})\\partial_{tt}u=\\Delta u+f $$ with initial data and source supported outside $\\Omega_{\\varepsilon}$; here, $\\chi_{S}$ denotes the characteristic function of a set $S$. We provide the first-order $\\varepsilon$-corrections with respect to the solutions of the inhomogeneous free wave equation and give space-time estimates on the remainders in the $L^{\\infty}((0,1/\\varepsilon^{\\tau}),L^{2}(\\mathbb{R}^{3}))","$-norm.","Such corrections are explicitly expressed in terms of the eigenvalues and eigenfunctions of the Newton potential operator in $L^{2}(\\Omega)$ and provide an effective dynamics describing a legitimate point scatterer approximation in the time domain."],"url":"http://arxiv.org/abs/2401.17195v1","category":"math-ph"}
{"created":"2024-01-30 17:29:58","title":"Mixed State Variational Quantum Eigensolver for the Estimation of Expectation Values at Finite Temperature","abstract":"We introduce a novel hybrid quantum-classical algorithm for the near-term computation of expectation values in quantum systems at finite temperatures. This is based on two stages: on the first one, a mixed state approximating a fiducial truncated density matrix is prepared through Variational Quantum Eigensolving (VQE) techniques; this is then followed by a reweighting stage where the expectation values for observables of interest are computed. These two stages can then be iterated again with different hyperparameters to achieve arbitrary accuracy. Resource and time scalability of the algorithm is discussed with a near-term perspective.","sentences":["We introduce a novel hybrid quantum-classical algorithm for the near-term computation of expectation values in quantum systems at finite temperatures.","This is based on two stages: on the first one, a mixed state approximating a fiducial truncated density matrix is prepared through Variational Quantum Eigensolving (VQE) techniques; this is then followed by a reweighting stage where the expectation values for observables of interest are computed.","These two stages can then be iterated again with different hyperparameters to achieve arbitrary accuracy.","Resource and time scalability of the algorithm is discussed with a near-term perspective."],"url":"http://arxiv.org/abs/2401.17194v1","category":"quant-ph"}
{"created":"2024-01-30 17:24:44","title":"Semantic Belief Behavior Graph: Enabling Autonomous Robot Inspection in Unknown Environments","abstract":"This paper addresses the problem of autonomous robotic inspection in complex and unknown environments. This capability is crucial for efficient and precise inspections in various real-world scenarios, even when faced with perceptual uncertainty and lack of prior knowledge of the environment. Existing methods for real-world autonomous inspections typically rely on predefined targets and waypoints and often fail to adapt to dynamic or unknown settings. In this work, we introduce the Semantic Belief Behavior Graph (SB2G) framework as a novel approach to semantic-aware autonomous robot inspection. SB2G generates a control policy for the robot, featuring behavior nodes that encapsulate various semantic-based policies designed for inspecting different classes of objects. We design an active semantic search behavior to guide the robot in locating objects for inspection while reducing semantic information uncertainty. The edges in the SB2G encode transitions between these behaviors. We validate our approach through simulation and real-world urban inspections using a legged robotic platform. Our results show that SB2G enables a more efficient inspection policy, exhibiting performance comparable to human-operated inspections.","sentences":["This paper addresses the problem of autonomous robotic inspection in complex and unknown environments.","This capability is crucial for efficient and precise inspections in various real-world scenarios, even when faced with perceptual uncertainty and lack of prior knowledge of the environment.","Existing methods for real-world autonomous inspections typically rely on predefined targets and waypoints and often fail to adapt to dynamic or unknown settings.","In this work, we introduce the Semantic Belief Behavior Graph (SB2G) framework as a novel approach to semantic-aware autonomous robot inspection.","SB2G generates a control policy for the robot, featuring behavior nodes that encapsulate various semantic-based policies designed for inspecting different classes of objects.","We design an active semantic search behavior to guide the robot in locating objects for inspection while reducing semantic information uncertainty.","The edges in the SB2G encode transitions between these behaviors.","We validate our approach through simulation and real-world urban inspections using a legged robotic platform.","Our results show that SB2G enables a more efficient inspection policy, exhibiting performance comparable to human-operated inspections."],"url":"http://arxiv.org/abs/2401.17191v1","category":"cs.RO"}
{"created":"2024-01-30 17:20:37","title":"Improving robustness of quantum feedback control with reinforcement learning","abstract":"Obtaining reliable state preparation protocols is a key step towards practical implementation of many quantum technologies, and one of the main tasks in quantum control. In this work, different reinforcement learning approaches are used to derive a feedback law for state preparation of a desired state in a target system. In particular, we focus on the robustness of the obtained strategies with respect to different types and amount of noise. Comparing the results indicates that the learned controls are more robust to unmodeled perturbations with respect to simple feedback strategy based on optimized population transfer, and that training on simulated nominal model retain the same advantages displayed by controllers trained on real data. The possibility of effective off-line training of robust controllers promises significant advantages towards practical implementation.","sentences":["Obtaining reliable state preparation protocols is a key step towards practical implementation of many quantum technologies, and one of the main tasks in quantum control.","In this work, different reinforcement learning approaches are used to derive a feedback law for state preparation of a desired state in a target system.","In particular, we focus on the robustness of the obtained strategies with respect to different types and amount of noise.","Comparing the results indicates that the learned controls are more robust to unmodeled perturbations with respect to simple feedback strategy based on optimized population transfer, and that training on simulated nominal model retain the same advantages displayed by controllers trained on real data.","The possibility of effective off-line training of robust controllers promises significant advantages towards practical implementation."],"url":"http://arxiv.org/abs/2401.17190v1","category":"quant-ph"}
{"created":"2024-01-30 17:20:34","title":"Exceptional points and ground-state entanglement spectrum for a fermionic extension of the Swanson oscillator","abstract":"Motivated by the structure of the Swanson oscillator, which is a well-known example of a non-hermitian quantum system consisting of a general representation of a quadratic Hamiltonian, we propose a fermionic extension of such a scheme which incorporates two fermionic oscillators, together with bilinear-coupling terms that do not conserve particle number. We determine the eigenvalues and eigenvectors, and expose the appearance of exceptional points where two of the eigenstates coalesce with the corresponding eigenvectors exhibiting the self-orthogonality relation. The model exhibits a quantum phase transition due to the presence of a ground-state crossing. We compute the entanglement spectrum and entanglement entropy of the ground state.","sentences":["Motivated by the structure of the Swanson oscillator, which is a well-known example of a non-hermitian quantum system consisting of a general representation of a quadratic Hamiltonian, we propose a fermionic extension of such a scheme which incorporates two fermionic oscillators, together with bilinear-coupling terms that do not conserve particle number.","We determine the eigenvalues and eigenvectors, and expose the appearance of exceptional points where two of the eigenstates coalesce with the corresponding eigenvectors exhibiting the self-orthogonality relation.","The model exhibits a quantum phase transition due to the presence of a ground-state crossing.","We compute the entanglement spectrum and entanglement entropy of the ground state."],"url":"http://arxiv.org/abs/2401.17189v1","category":"quant-ph"}
{"created":"2024-01-30 17:17:43","title":"Nested Construction of Polar Codes via Transformers","abstract":"Tailoring polar code construction for decoding algorithms beyond successive cancellation has remained a topic of significant interest in the field. However, despite the inherent nested structure of polar codes, the use of sequence models in polar code construction is understudied. In this work, we propose using a sequence modeling framework to iteratively construct a polar code for any given length and rate under various channel conditions. Simulations show that polar codes designed via sequential modeling using transformers outperform both 5G-NR sequence and Density Evolution based approaches for both AWGN and Rayleigh fading channels.","sentences":["Tailoring polar code construction for decoding algorithms beyond successive cancellation has remained a topic of significant interest in the field.","However, despite the inherent nested structure of polar codes, the use of sequence models in polar code construction is understudied.","In this work, we propose using a sequence modeling framework to iteratively construct a polar code for any given length and rate under various channel conditions.","Simulations show that polar codes designed via sequential modeling using transformers outperform both 5G-NR sequence and Density Evolution based approaches for both AWGN and Rayleigh fading channels."],"url":"http://arxiv.org/abs/2401.17188v1","category":"cs.IT"}
{"created":"2024-01-30 17:16:32","title":"Formal Synthesis of Uncertainty Reduction Controllers","abstract":"In its quest for approaches to taming uncertainty in self-adaptive systems (SAS), the research community has largely focused on solutions that adapt the SAS architecture or behaviour in response to uncertainty. By comparison, solutions that reduce the uncertainty affecting SAS (other than through the blanket monitoring of their components and environment) remain underexplored. Our paper proposes a more nuanced, adaptive approach to SAS uncertainty reduction. To that end, we introduce a SAS architecture comprising an uncertainty reduction controller that drives the adaptive acquisition of new information within the SAS adaptation loop, and a tool-supported method that uses probabilistic model checking to synthesise such controllers. The controllers generated by our method deliver optimal trade-offs between SAS uncertainty reduction benefits and new information acquisition costs. We illustrate the use and evaluate the effectiveness of our approach for mobile robot navigation and server infrastructure management SAS.","sentences":["In its quest for approaches to taming uncertainty in self-adaptive systems (SAS), the research community has largely focused on solutions that adapt the SAS architecture or behaviour in response to uncertainty.","By comparison, solutions that reduce the uncertainty affecting SAS (other than through the blanket monitoring of their components and environment) remain underexplored.","Our paper proposes a more nuanced, adaptive approach to SAS uncertainty reduction.","To that end, we introduce a SAS architecture comprising an uncertainty reduction controller that drives the adaptive acquisition of new information within the SAS adaptation loop, and a tool-supported method that uses probabilistic model checking to synthesise such controllers.","The controllers generated by our method deliver optimal trade-offs between SAS uncertainty reduction benefits and new information acquisition costs.","We illustrate the use and evaluate the effectiveness of our approach for mobile robot navigation and server infrastructure management SAS."],"url":"http://arxiv.org/abs/2401.17187v1","category":"cs.SE"}
{"created":"2024-01-30 17:14:05","title":"Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning","abstract":"While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts. To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training. We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance. Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently. Our code and data are available at \\url{https://github.com/yangbang18/CLFM}.","sentences":["While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities.","To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability.","In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF).","We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment.","Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences.","It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts.","To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training.","We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance.","Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently.","Our code and data are available at \\url{https://github.com/yangbang18/CLFM}."],"url":"http://arxiv.org/abs/2401.17186v1","category":"cs.CV"}
{"created":"2024-01-30 17:13:29","title":"Multi-Camera Asynchronous Ball Localization and Trajectory Prediction with Factor Graphs and Human Poses","abstract":"The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins. The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground. In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization. Additionally, we estimate hidden states like velocity and spin for trajectory prediction. Furthermore, to enhance spin inference early in the ball's flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph. This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction. Our result shows the trained TCN can predict the spin priors with RMSE of 5.27 Hz. Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter.","sentences":["The rapid and precise localization and prediction of a ball are critical for developing agile robots in ball sports, particularly in sports like tennis characterized by high-speed ball movements and powerful spins.","The Magnus effect induced by spin adds complexity to trajectory prediction during flight and bounce dynamics upon contact with the ground.","In this study, we introduce an innovative approach that combines a multi-camera system with factor graphs for real-time and asynchronous 3D tennis ball localization.","Additionally, we estimate hidden states like velocity and spin for trajectory prediction.","Furthermore, to enhance spin inference early in the ball's flight, where limited observations are available, we integrate human pose data using a temporal convolutional network (TCN) to compute spin priors within the factor graph.","This refinement provides more accurate spin priors at the beginning of the factor graph, leading to improved early-stage hidden state inference for prediction.","Our result shows the trained TCN can predict the spin priors with RMSE of 5.27 Hz.","Integrating TCN into the factor graph reduces the prediction error of landing positions by over 63.6% compared to a baseline method that utilized an adaptive extended Kalman filter."],"url":"http://arxiv.org/abs/2401.17185v1","category":"cs.RO"}
{"created":"2024-01-30 17:12:56","title":"Rigorous Error Analysis for Logarithmic Number Systems","abstract":"Logarithmic Number Systems (LNS) hold considerable promise in helping reduce the number of bits needed to represent a high dynamic range of real-numbers with finite precision, and also efficiently support multiplication and division. However, under LNS, addition and subtraction turn into non-linear functions that must be approximated - typically using precomputed table-based functions. Additionally, multiple layers of error correction are typically needed to improve result accuracy. Unfortunately, previous efforts have not characterized the resulting error bound. We provide the first rigorous analysis of LNS, covering detailed techniques such as co-transformation that are crucial to implementing subtraction with reasonable accuracy. We provide theorems capturing the error due to table interpolations, the finite precision of pre-computed values in the tables, and the error introduced by fix-point multiplications involved in LNS implementations. We empirically validate our analysis using a Python implementation, showing that our analytical bounds are tight, and that our testing campaign generates inputs diverse-enough to almost match (but not exceed) the analytical bounds. We close with discussions on how to adapt our analysis to LNS systems with different bases and also discuss many pragmatic ramifications of our work in the broader arena of scientific computing and machine learning.","sentences":["Logarithmic Number Systems (LNS) hold considerable promise in helping reduce the number of bits needed to represent a high dynamic range of real-numbers with finite precision, and also efficiently support multiplication and division.","However, under LNS, addition and subtraction turn into non-linear functions that must be approximated - typically using precomputed table-based functions.","Additionally, multiple layers of error correction are typically needed to improve result accuracy.","Unfortunately, previous efforts have not characterized the resulting error bound.","We provide the first rigorous analysis of LNS, covering detailed techniques such as co-transformation that are crucial to implementing subtraction with reasonable accuracy.","We provide theorems capturing the error due to table interpolations, the finite precision of pre-computed values in the tables, and the error introduced by fix-point multiplications involved in LNS implementations.","We empirically validate our analysis using a Python implementation, showing that our analytical bounds are tight, and that our testing campaign generates inputs diverse-enough to almost match (but not exceed) the analytical bounds.","We close with discussions on how to adapt our analysis to LNS systems with different bases and also discuss many pragmatic ramifications of our work in the broader arena of scientific computing and machine learning."],"url":"http://arxiv.org/abs/2401.17184v1","category":"cs.MS"}
{"created":"2024-01-30 17:12:12","title":"Detailed Error Analysis of the HHL Algorithm","abstract":"We reiterate the contribution made by Harrow, Hassidim, and Llyod to the quantum matrix equation solver with the emphasis on the algorithm description and the error analysis derivation details. Moreover, the behavior of the amplitudes of the phase register on the completion of the Quantum Phase Estimation is studied. This study is beneficial for the comprehension of the choice of the phase register size and its interrelation with the Hamiltonian simulation duration in the algorithm setup phase.","sentences":["We reiterate the contribution made by Harrow, Hassidim, and Llyod to the quantum matrix equation solver with the emphasis on the algorithm description and the error analysis derivation details.","Moreover, the behavior of the amplitudes of the phase register on the completion of the Quantum Phase Estimation is studied.","This study is beneficial for the comprehension of the choice of the phase register size and its interrelation with the Hamiltonian simulation duration in the algorithm setup phase."],"url":"http://arxiv.org/abs/2401.17182v1","category":"quant-ph"}
{"created":"2024-01-30 17:11:56","title":"Transfer Learning for Text Diffusion Models","abstract":"In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff''. We begin by establishing a strong baseline setup for training text diffusion models. Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks. Building on this finding, we test various transfer learning setups for text diffusion models. On machine translation, we find that text diffusion underperforms the standard AR approach. However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases. We also observe quality gains from AR2Diff -- adapting AR models to use diffusion decoding. These results are promising given that text diffusion is relatively underexplored and can be significantly faster than AR decoding for long text generation.","sentences":["In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs).","We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff''.","We begin by establishing a strong baseline setup for training text diffusion models.","Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks.","Building on this finding, we test various transfer learning setups for text diffusion models.","On machine translation, we find that text diffusion underperforms the standard AR approach.","However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases.","We also observe quality gains from AR2Diff -- adapting AR models to use diffusion decoding.","These results are promising given that text diffusion is relatively underexplored and can be significantly faster than AR decoding for long text generation."],"url":"http://arxiv.org/abs/2401.17181v1","category":"cs.CL"}
{"created":"2024-01-30 17:11:04","title":"GraphViz2Vec: A Structure-aware Feature Generation Model to Improve Classification in GNNs","abstract":"GNNs are widely used to solve various tasks including node classification and link prediction. Most of the GNN architectures assume the initial embedding to be random or generated from popular distributions. These initial embeddings require multiple layers of transformation to converge into a meaningful latent representation. While number of layers allow accumulation of larger neighbourhood of a node it also introduce the problem of over-smoothing. In addition, GNNs are inept at representing structural information. For example, the output embedding of a node does not capture its triangles participation. In this paper, we presented a novel feature extraction methodology GraphViz2Vec that can capture the structural information of a node's local neighbourhood to create meaningful initial embeddings for a GNN model. These initial embeddings helps existing models achieve state-of-the-art results in various classification tasks. Further, these initial embeddings help the model to produce desired results with only two layers which in turn reduce the problem of over-smoothing. The initial encoding of a node is obtained from an image classification model trained on multiple energy diagrams of its local neighbourhood. These energy diagrams are generated with the induced sub-graph of the nodes traversed by multiple random walks. The generated encodings increase the performance of existing models on classification tasks (with a mean increase of $4.65\\%$ and $2.58\\%$ for the node and link classification tasks, respectively), with some models achieving state-of-the-art results.","sentences":["GNNs are widely used to solve various tasks including node classification and link prediction.","Most of the GNN architectures assume the initial embedding to be random or generated from popular distributions.","These initial embeddings require multiple layers of transformation to converge into a meaningful latent representation.","While number of layers allow accumulation of larger neighbourhood of a node it also introduce the problem of over-smoothing.","In addition, GNNs are inept at representing structural information.","For example, the output embedding of a node does not capture its triangles participation.","In this paper, we presented a novel feature extraction methodology GraphViz2Vec that can capture the structural information of a node's local neighbourhood to create meaningful initial embeddings for a GNN model.","These initial embeddings helps existing models achieve state-of-the-art results in various classification tasks.","Further, these initial embeddings help the model to produce desired results with only two layers which in turn reduce the problem of over-smoothing.","The initial encoding of a node is obtained from an image classification model trained on multiple energy diagrams of its local neighbourhood.","These energy diagrams are generated with the induced sub-graph of the nodes traversed by multiple random walks.","The generated encodings increase the performance of existing models on classification tasks (with a mean increase of $4.65\\%$ and $2.58\\%$ for the node and link classification tasks, respectively), with some models achieving state-of-the-art results."],"url":"http://arxiv.org/abs/2401.17178v1","category":"cs.LG"}
{"created":"2024-01-30 17:10:42","title":"A Hamilton-Jacobi approach to nonlocal kinetic equations","abstract":"Highly concentrated patterns have been observed in a spatially heterogeneous, nonlocal, model of BGK type implementing a velocity-jump process.   We study both a linear and a nonlinear case and describe the concentration profile. In particular, we analyse a hyperbolic (or high frequency) regime that can be interpreted both as a local (microscopic) or as a nonlocal (macroscopic) rescaling. We consider a Hopf-Cole transform and derive a Hamilton-Jacobi equation. The concentrations are then explained as a consequence of the stationary points of the Hamiltonian that is spatially heterogeneous like the velocity-jump process. After revising the classical hydrodynamic limits for the aggregate quantities and the eikonal equation that can be derived from those with a Hopf-Cole transform, we find that the Hamilton-Jacobi equation is a second order approximation of the eikonal equation in the limit of small diffusivity. For nonlinear turning kernels, the Hopf-Cole transform allows to study the stability of the possible homogeneous configurations and of patterns and the results of a linear stability analysis previously obtained are found and extended to a nonlinear regime. In particular, it is shown that instability (pattern formation) occurs when the Hamiltonian is convex-concave.","sentences":["Highly concentrated patterns have been observed in a spatially heterogeneous, nonlocal, model of BGK type implementing a velocity-jump process.   ","We study both a linear and a nonlinear case and describe the concentration profile.","In particular, we analyse a hyperbolic (or high frequency) regime that can be interpreted both as a local (microscopic) or as a nonlocal (macroscopic) rescaling.","We consider a Hopf-Cole transform and derive a Hamilton-Jacobi equation.","The concentrations are then explained as a consequence of the stationary points of the Hamiltonian that is spatially heterogeneous like the velocity-jump process.","After revising the classical hydrodynamic limits for the aggregate quantities and the eikonal equation that can be derived from those with a Hopf-Cole transform, we find that the Hamilton-Jacobi equation is a second order approximation of the eikonal equation in the limit of small diffusivity.","For nonlinear turning kernels, the Hopf-Cole transform allows to study the stability of the possible homogeneous configurations and of patterns and the results of a linear stability analysis previously obtained are found and extended to a nonlinear regime.","In particular, it is shown that instability (pattern formation) occurs when the Hamiltonian is convex-concave."],"url":"http://arxiv.org/abs/2401.17176v1","category":"math-ph"}
{"created":"2024-01-30 17:10:42","title":"Data-Driven Discovery of PDEs via the Adjoint Method","abstract":"In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even though the presented adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.","sentences":["In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data.","The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data.","Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner.","In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived.","Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy.","However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017).","Even though the presented adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter."],"url":"http://arxiv.org/abs/2401.17177v1","category":"math.OC"}
{"created":"2024-01-30 17:08:22","title":"Integrable Frame Fields using Odeco Tensors","abstract":"We propose a method for computing integrable orthogonal frame fields on planar surfaces. Frames and their symmetries are implicitly represented using orthogonally decomposable (odeco) tensors. To formulate an integrability criterion, we express the frame field's Lie bracket solely in terms of the tensor representation; this is made possible by studying the sensitivity of the frame with respect to perturbations in the tensor. We construct an energy formulation that computes smooth and integrable frame fields, in both isotropic and anisotropic settings. The user can prescribe any size and orientation constraints in input, and the solver creates and places the singularities required to fit the constraints with the correct topology. The computed frame field can be integrated to a seamless parametrization that is aligned with the frame field.","sentences":["We propose a method for computing integrable orthogonal frame fields on planar surfaces.","Frames and their symmetries are implicitly represented using orthogonally decomposable (odeco) tensors.","To formulate an integrability criterion, we express the frame field's Lie bracket solely in terms of the tensor representation; this is made possible by studying the sensitivity of the frame with respect to perturbations in the tensor.","We construct an energy formulation that computes smooth and integrable frame fields, in both isotropic and anisotropic settings.","The user can prescribe any size and orientation constraints in input, and the solver creates and places the singularities required to fit the constraints with the correct topology.","The computed frame field can be integrated to a seamless parametrization that is aligned with the frame field."],"url":"http://arxiv.org/abs/2401.17175v1","category":"cs.CG"}
{"created":"2024-01-30 17:06:25","title":"A large dataset curation and benchmark for drug target interaction","abstract":"Bioactivity data plays a key role in drug discovery and repurposing. The resource-demanding nature of \\textit{in vitro} and \\textit{in vivo} experiments, as well as the recent advances in data-driven computational biochemistry research, highlight the importance of \\textit{in silico} drug target interaction (DTI) prediction approaches. While numerous large public bioactivity data sources exist, research in the field could benefit from better standardization of existing data resources. At present, different research works that share similar goals are often difficult to compare properly because of different choices of data sources and train/validation/test split strategies. Additionally, many works are based on small data subsets, leading to results and insights of possible limited validity. In this paper we propose a way to standardize and represent efficiently a very large dataset curated from multiple public sources, split the data into train, validation and test sets based on different meaningful strategies, and provide a concrete evaluation protocol to accomplish a benchmark. We analyze the proposed data curation, prove its usefulness and validate the proposed benchmark through experimental studies based on an existing neural network model.","sentences":["Bioactivity data plays a key role in drug discovery and repurposing.","The resource-demanding nature of \\textit{in vitro} and \\textit{in vivo} experiments, as well as the recent advances in data-driven computational biochemistry research, highlight the importance of \\textit{in silico} drug target interaction (DTI) prediction approaches.","While numerous large public bioactivity data sources exist, research in the field could benefit from better standardization of existing data resources.","At present, different research works that share similar goals are often difficult to compare properly because of different choices of data sources and train/validation/test split strategies.","Additionally, many works are based on small data subsets, leading to results and insights of possible limited validity.","In this paper we propose a way to standardize and represent efficiently a very large dataset curated from multiple public sources, split the data into train, validation and test sets based on different meaningful strategies, and provide a concrete evaluation protocol to accomplish a benchmark.","We analyze the proposed data curation, prove its usefulness and validate the proposed benchmark through experimental studies based on an existing neural network model."],"url":"http://arxiv.org/abs/2401.17174v1","category":"q-bio.BM"}
{"created":"2024-01-30 17:04:47","title":"Zero-Shot Reinforcement Learning via Function Encoders","abstract":"Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encoder task representation.","sentences":["Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge.","The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks.","To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions.","By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation.","Thus, the agent is able to achieve transfer between related tasks at run time with no additional training.","We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encoder task representation."],"url":"http://arxiv.org/abs/2401.17173v1","category":"cs.LG"}
{"created":"2024-01-30 17:00:22","title":"Learning Domain-Independent Green's Function For Elliptic Partial Differential Equations","abstract":"Green's function characterizes a partial differential equation (PDE) and maps its solution in the entire domain as integrals. Finding the analytical form of Green's function is a non-trivial exercise, especially for a PDE defined on a complex domain or a PDE with variable coefficients. In this paper, we propose a novel boundary integral network to learn the domain-independent Green's function, referred to as BIN-G. We evaluate the Green's function in the BIN-G using a radial basis function (RBF) kernel-based neural network. We train the BIN-G by minimizing the residual of the PDE and the mean squared errors of the solutions to the boundary integral equations for prescribed test functions. By leveraging the symmetry of the Green's function and controlling refinements of the RBF kernel near the singularity of the Green function, we demonstrate that our numerical scheme enables fast training and accurate evaluation of the Green's function for PDEs with variable coefficients. The learned Green's function is independent of the domain geometries, forcing terms, and boundary conditions in the boundary integral formulation. Numerical experiments verify the desired properties of the method and the expected accuracy for the two-dimensional Poisson and Helmholtz equations with variable coefficients.","sentences":["Green's function characterizes a partial differential equation (PDE) and maps its solution in the entire domain as integrals.","Finding the analytical form of Green's function is a non-trivial exercise, especially for a PDE defined on a complex domain or a PDE with variable coefficients.","In this paper, we propose a novel boundary integral network to learn the domain-independent Green's function, referred to as BIN-G. We evaluate the Green's function in the BIN-G using a radial basis function (RBF) kernel-based neural network.","We train the BIN-G by minimizing the residual of the PDE and the mean squared errors of the solutions to the boundary integral equations for prescribed test functions.","By leveraging the symmetry of the Green's function and controlling refinements of the RBF kernel near the singularity of the Green function, we demonstrate that our numerical scheme enables fast training and accurate evaluation of the Green's function for PDEs with variable coefficients.","The learned Green's function is independent of the domain geometries, forcing terms, and boundary conditions in the boundary integral formulation.","Numerical experiments verify the desired properties of the method and the expected accuracy for the two-dimensional Poisson and Helmholtz equations with variable coefficients."],"url":"http://arxiv.org/abs/2401.17172v1","category":"physics.comp-ph"}
{"created":"2024-01-30 16:56:54","title":"Conditional and Modal Reasoning in Large Language Models","abstract":"The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.","sentences":["The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science.","In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones.","We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king').","These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning.","Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans.","Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals.","Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals."],"url":"http://arxiv.org/abs/2401.17169v1","category":"cs.CL"}
{"created":"2024-01-30 16:56:32","title":"Stale Profile Matching","abstract":"Profile-guided optimizations rely on profile data for directing compilers to generate optimized code. To achieve the maximum performance boost, profile data needs to be collected on the same version of the binary that is being optimized. In practice however, there is typically a gap between the profile collection and the release, which makes a portion of the profile invalid for optimizations. This phenomenon is known as profile staleness, and it is a serious practical problem for data-center workloads both for compilers and binary optimizers.   In this paper we thoroughly study the staleness problem and propose the first practical solution for utilizing profiles collected on binaries built from several revisions behind the release. Our algorithm is developed and implemented in a mainstream open-source post-link optimizer, BOLT. An extensive evaluation on a variety of standalone benchmarks and production services indicates that the new method recovers up to $0.8$ of the maximum BOLT benefit, even when most of the input profile data is stale and would have been discarded by the optimizer otherwise.","sentences":["Profile-guided optimizations rely on profile data for directing compilers to generate optimized code.","To achieve the maximum performance boost, profile data needs to be collected on the same version of the binary that is being optimized.","In practice however, there is typically a gap between the profile collection and the release, which makes a portion of the profile invalid for optimizations.","This phenomenon is known as profile staleness, and it is a serious practical problem for data-center workloads both for compilers and binary optimizers.   ","In this paper we thoroughly study the staleness problem and propose the first practical solution for utilizing profiles collected on binaries built from several revisions behind the release.","Our algorithm is developed and implemented in a mainstream open-source post-link optimizer, BOLT.","An extensive evaluation on a variety of standalone benchmarks and production services indicates that the new method recovers up to $0.8$ of the maximum BOLT benefit, even when most of the input profile data is stale and would have been discarded by the optimizer otherwise."],"url":"http://arxiv.org/abs/2401.17168v1","category":"cs.PL"}
{"created":"2024-01-30 16:52:56","title":"Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios","abstract":"The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools. However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization. To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios. UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks. It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving. A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps. Thus, unlike previous work, it eliminates the restriction of pre-defined toolset during planning. Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field. The benchmark is publicly available at https://github.com/JoeYing1019/UltraTool.","sentences":["The recent trend of using Large Language Models (LLMs) as intelligent agents in real-world applications underscores the necessity for comprehensive evaluations of their capabilities, particularly in complex scenarios involving planning, creating, and using tools.","However, existing benchmarks typically focus on simple synthesized queries that do not reflect real-world complexity, thereby offering limited perspectives in evaluating tool utilization.","To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs' ability in tool utilization within real-world scenarios.","UltraTool focuses on the entire process of using tools - from planning and creating to applying them in complex tasks.","It emphasizes real-world complexities, demanding accurate, multi-step planning for effective problem-solving.","A key feature of UltraTool is its independent evaluation of planning with natural language, which happens before tool usage and simplifies the task solving by mapping out the intermediate steps.","Thus, unlike previous work, it eliminates the restriction of pre-defined toolset during planning.","Through extensive experiments on various LLMs, we offer novel insights into the evaluation of capabilities of LLMs in tool utilization, thereby contributing a fresh perspective to this rapidly evolving field.","The benchmark is publicly available at https://github.com/JoeYing1019/UltraTool."],"url":"http://arxiv.org/abs/2401.17167v1","category":"cs.CL"}
{"created":"2024-01-30 16:49:50","title":"Learning Agent-based Modeling with LLM Companions: Experiences of Novices and Experts Using ChatGPT & NetLogo Chat","abstract":"Large Language Models (LLMs) have the potential to fundamentally change the way people engage in computer programming. Agent-based modeling (ABM) has become ubiquitous in natural and social sciences and education, yet no prior studies have explored the potential of LLMs to assist it. We designed NetLogo Chat to support the learning and practice of NetLogo, a programming language for ABM. To understand how users perceive, use, and need LLM-based interfaces, we interviewed 30 participants from global academia, industry, and graduate schools. Experts reported more perceived benefits than novices and were more inclined to adopt LLMs in their workflow. We found significant differences between experts and novices in their perceptions, behaviors, and needs for human-AI collaboration. We surfaced a knowledge gap between experts and novices as a possible reason for the benefit gap. We identified guidance, personalization, and integration as major needs for LLM-based interfaces to support the programming of ABM.","sentences":["Large Language Models (LLMs) have the potential to fundamentally change the way people engage in computer programming.","Agent-based modeling (ABM) has become ubiquitous in natural and social sciences and education, yet no prior studies have explored the potential of LLMs to assist it.","We designed NetLogo Chat to support the learning and practice of NetLogo, a programming language for ABM.","To understand how users perceive, use, and need LLM-based interfaces, we interviewed 30 participants from global academia, industry, and graduate schools.","Experts reported more perceived benefits than novices and were more inclined to adopt LLMs in their workflow.","We found significant differences between experts and novices in their perceptions, behaviors, and needs for human-AI collaboration.","We surfaced a knowledge gap between experts and novices as a possible reason for the benefit gap.","We identified guidance, personalization, and integration as major needs for LLM-based interfaces to support the programming of ABM."],"url":"http://arxiv.org/abs/2401.17163v1","category":"cs.HC"}
{"created":"2024-01-30 16:47:41","title":"Hybrid Tendon and Ball Chain Continuum Robots for Enhanced Dexterity in Medical Interventions","abstract":"A hybrid continuum robot design is introduced that combines a proximal tendon-actuated section with a distal telescoping section comprised of permanent-magnet spheres actuated using an external magnet. While, individually, each section can approach a point in its workspace from one or at most several orientations, the two-section combination possesses a dexterous workspace. The paper describes kinematic modeling of the hybrid design and provides a description of the dexterous workspace. We present experimental validation which shows that a simplified kinematic model produces tip position mean and maximum errors of 3% and 7% of total robot length, respectively.","sentences":["A hybrid continuum robot design is introduced that combines a proximal tendon-actuated section with a distal telescoping section comprised of permanent-magnet spheres actuated using an external magnet.","While, individually, each section can approach a point in its workspace from one or at most several orientations, the two-section combination possesses a dexterous workspace.","The paper describes kinematic modeling of the hybrid design and provides a description of the dexterous workspace.","We present experimental validation which shows that a simplified kinematic model produces tip position mean and maximum errors of 3% and 7% of total robot length, respectively."],"url":"http://arxiv.org/abs/2401.17161v1","category":"cs.RO"}
{"created":"2024-01-30 16:47:30","title":"Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis","abstract":"Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.   In this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decision-making process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-of-the-art (SOTA) SMT solvers. We implement our method, dubbed Z3alpha, as part of the Z3 SMT solver. Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT, the default Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging QF_BV benchmark set, Z3alpha solves 42.7% more instances than the default strategy in the Z3 SMT solver.","sentences":["Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case.","However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.   ","In this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method.","Our method treats strategy synthesis as a sequential decision-making process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space.","The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search.","These novel approaches allow for a deeper and more efficient exploration of the strategy space, enabling us to synthesize more effective strategies than the default ones in state-of-the-art (SOTA) SMT solvers.","We implement our method, dubbed Z3alpha, as part of the Z3 SMT solver.","Through extensive evaluations across 6 important SMT logics, Z3alpha demonstrates superior performance compared to the SOTA synthesis tool FastSMT, the default Z3 solver, and the CVC5 solver on most benchmarks.","Remarkably, on a challenging QF_BV benchmark set, Z3alpha solves 42.7% more instances than the default strategy in the Z3 SMT solver."],"url":"http://arxiv.org/abs/2401.17159v1","category":"cs.AI"}
{"created":"2024-01-30 16:36:58","title":"Real-time Contact State Estimation in Shape Control of Deformable Linear Objects under Small Environmental Constraints","abstract":"Controlling the shape of deformable linear objects using robots and constraints provided by environmental fixtures has diverse industrial applications. In order to establish robust contacts with these fixtures, accurate estimation of the contact state is essential for preventing and rectifying potential anomalies. However, this task is challenging due to the small sizes of fixtures, the requirement for real-time performances, and the infinite degrees of freedom of the deformable linear objects. In this paper, we propose a real-time approach for estimating both contact establishment and subsequent changes by leveraging the dependency between the applied and detected contact force on the deformable linear objects. We seamlessly integrate this method into the robot control loop and achieve an adaptive shape control framework which avoids, detects and corrects anomalies automatically. Real-world experiments validate the robustness and effectiveness of our contact estimation approach across various scenarios, significantly increasing the success rate of shape control processes.","sentences":["Controlling the shape of deformable linear objects using robots and constraints provided by environmental fixtures has diverse industrial applications.","In order to establish robust contacts with these fixtures, accurate estimation of the contact state is essential for preventing and rectifying potential anomalies.","However, this task is challenging due to the small sizes of fixtures, the requirement for real-time performances, and the infinite degrees of freedom of the deformable linear objects.","In this paper, we propose a real-time approach for estimating both contact establishment and subsequent changes by leveraging the dependency between the applied and detected contact force on the deformable linear objects.","We seamlessly integrate this method into the robot control loop and achieve an adaptive shape control framework which avoids, detects and corrects anomalies automatically.","Real-world experiments validate the robustness and effectiveness of our contact estimation approach across various scenarios, significantly increasing the success rate of shape control processes."],"url":"http://arxiv.org/abs/2401.17154v1","category":"cs.RO"}
{"created":"2024-01-30 16:32:37","title":"An Open Software Suite for Event-Based Video","abstract":"While traditional video representations are organized around discrete image frames, event-based video is a new paradigm that forgoes image frames altogether. Rather, pixel samples are temporally asynchronous and independent of one another. Until now, researchers have lacked a cohesive software framework for exploring the representation, compression, and applications of event-based video. I present the AD$\\Delta$ER software suite to fill this gap. This framework includes utilities for transcoding framed and multimodal event-based video sources to a common representation, rate control mechanisms, lossy compression, application support, and an interactive GUI for transcoding and playback. In this paper, I describe these various software components and their usage.","sentences":["While traditional video representations are organized around discrete image frames, event-based video is a new paradigm that forgoes image frames altogether.","Rather, pixel samples are temporally asynchronous and independent of one another.","Until now, researchers have lacked a cohesive software framework for exploring the representation, compression, and applications of event-based video.","I present the AD$\\Delta$ER software suite to fill this gap.","This framework includes utilities for transcoding framed and multimodal event-based video sources to a common representation, rate control mechanisms, lossy compression, application support, and an interactive GUI for transcoding and playback.","In this paper, I describe these various software components and their usage."],"url":"http://arxiv.org/abs/2401.17151v1","category":"cs.MM"}
{"created":"2024-01-30 16:31:48","title":"GAISSALabel: A tool for energy labeling of ML models","abstract":"Background: The increasing environmental impact of Information Technologies, particularly in Machine Learning (ML), highlights the need for sustainable practices in software engineering. The escalating complexity and energy consumption of ML models need tools for assessing and improving their energy efficiency. Goal: This paper introduces GAISSALabel, a web-based tool designed to evaluate and label the energy efficiency of ML models. Method: GAISSALabel is a technology transfer development from a former research on energy efficiency classification of ML, consisting of a holistic tool for assessing both the training and inference phases of ML models, considering various metrics such as power draw, model size efficiency, CO2e emissions and more. Results: GAISSALabel offers a labeling system for energy efficiency, akin to labels on consumer appliances, making it accessible to ML stakeholders of varying backgrounds. The tool's adaptability allows for customization in the proposed labeling system, ensuring its relevance in the rapidly evolving ML field. Conclusions: GAISSALabel represents a significant step forward in sustainable software engineering, offering a solution for balancing high-performance ML models with environmental impacts. The tool's effectiveness and market relevance will be further assessed through planned evaluations using the Technology Acceptance Model.","sentences":["Background: The increasing environmental impact of Information Technologies, particularly in Machine Learning (ML), highlights the need for sustainable practices in software engineering.","The escalating complexity and energy consumption of ML models need tools for assessing and improving their energy efficiency.","Goal:","This paper introduces GAISSALabel, a web-based tool designed to evaluate and label the energy efficiency of ML models.","Method: GAISSALabel is a technology transfer development from a former research on energy efficiency classification of ML, consisting of a holistic tool for assessing both the training and inference phases of ML models, considering various metrics such as power draw, model size efficiency, CO2e emissions and more.","Results: GAISSALabel offers a labeling system for energy efficiency, akin to labels on consumer appliances, making it accessible to ML stakeholders of varying backgrounds.","The tool's adaptability allows for customization in the proposed labeling system, ensuring its relevance in the rapidly evolving ML field.","Conclusions: GAISSALabel represents a significant step forward in sustainable software engineering, offering a solution for balancing high-performance ML models with environmental impacts.","The tool's effectiveness and market relevance will be further assessed through planned evaluations using the Technology Acceptance Model."],"url":"http://arxiv.org/abs/2401.17150v1","category":"cs.SE"}
{"created":"2024-01-30 16:31:24","title":"Optical Tactile Sensing for Aerial Multi-Contact Interaction: Design, Integration, and Evaluation","abstract":"Distributed tactile sensing for multi-force detection is crucial for various aerial robot interaction tasks. However, current contact sensing solutions on drones only exploit single end-effector sensors and cannot provide distributed multi-contact sensing. Designed to be easily mounted at the bottom of a drone, we propose an optical tactile sensor that features a large and curved soft sensing surface, a hollow structure and a new illumination system. Even when spaced only 2 cm apart, multiple contacts can be detected simultaneously using our software pipeline, which provides real-world quantities of 3D contact locations (mm) and 3D force vectors (N), with an accuracy of 1.5 mm and 0.17 N respectively. We demonstrate the sensor's applicability and reliability onboard and in real-time with two demos related to i) the estimation of the compliance of different perches and subsequent re-alignment and landing on the stiffer one, and ii) the mapping of sparse obstacles. The implementation of our distributed tactile sensor represents a significant step towards attaining the full potential of drones as versatile robots capable of interacting with and navigating within complex environments.","sentences":["Distributed tactile sensing for multi-force detection is crucial for various aerial robot interaction tasks.","However, current contact sensing solutions on drones only exploit single end-effector sensors and cannot provide distributed multi-contact sensing.","Designed to be easily mounted at the bottom of a drone, we propose an optical tactile sensor that features a large and curved soft sensing surface, a hollow structure and a new illumination system.","Even when spaced only 2 cm apart, multiple contacts can be detected simultaneously using our software pipeline, which provides real-world quantities of 3D contact locations (mm) and 3D force vectors (N), with an accuracy of 1.5 mm and 0.17 N respectively.","We demonstrate the sensor's applicability and reliability onboard and in real-time with two demos related to i) the estimation of the compliance of different perches and subsequent re-alignment and landing on the stiffer one, and ii) the mapping of sparse obstacles.","The implementation of our distributed tactile sensor represents a significant step towards attaining the full potential of drones as versatile robots capable of interacting with and navigating within complex environments."],"url":"http://arxiv.org/abs/2401.17149v1","category":"cs.RO"}
{"created":"2024-01-30 16:27:44","title":"Dependency-Aware Online Caching","abstract":"We consider a variant of the online caching problem where the items exhibit dependencies among each other: an item can reside in the cache only if all its dependent items are also in the cache. The dependency relations can form any directed acyclic graph. These requirements arise e.g., in systems such as CacheFlow (SOSR 2016) that cache forwarding rules for packet classification in IP-based communication networks.   First, we present an optimal randomized online caching algorithm which accounts for dependencies among the items. Our randomized algorithm is $O( \\log k)$-competitive, where $k$ is the size of the cache, meaning that our algorithm never incurs the cost of $O(\\log k)$ times higher than even an optimal algorithm that knows the future input sequence.   Second, we consider the bypassing model, where requests can be served at a fixed price without fetching the item and its dependencies into the cache -- a variant of caching with dependencies introduced by Bienkowski et al. at SPAA 2017. For this setting, we give an $O( \\sqrt{k \\cdot \\log k})$-competitive algorithm, which significantly improves the best known competitiveness. We conduct a small case study, to find out that our algorithm incurs on average 2x lower cost.","sentences":["We consider a variant of the online caching problem where the items exhibit dependencies among each other: an item can reside in the cache only if all its dependent items are also in the cache.","The dependency relations can form any directed acyclic graph.","These requirements arise e.g., in systems such as CacheFlow (SOSR 2016) that cache forwarding rules for packet classification in IP-based communication networks.   ","First, we present an optimal randomized online caching algorithm which accounts for dependencies among the items.","Our randomized algorithm is $O( \\log k)$-competitive, where $k$ is the size of the cache, meaning that our algorithm never incurs the cost of $O(\\log k)$ times higher than even an optimal algorithm that knows the future input sequence.   ","Second, we consider the bypassing model, where requests can be served at a fixed price without fetching the item and its dependencies into the cache -- a variant of caching with dependencies introduced by Bienkowski et al.","at SPAA 2017.","For this setting, we give an $O( \\sqrt{k \\cdot \\log k})$-competitive algorithm, which significantly improves the best known competitiveness.","We conduct a small case study, to find out that our algorithm incurs on average 2x lower cost."],"url":"http://arxiv.org/abs/2401.17146v1","category":"cs.DS"}
{"created":"2024-01-30 16:25:22","title":"Integrable Holographic Defect CFTs","abstract":"We review a class of integrable, supersymmetric defect conformal field theories which have holographic duals in the form of probe brane models. Our main examples are defect versions of N=4 SYM and ABJM theory, both involving a domain wall with Nahm pole boundary conditions, and the case of a 't Hooft line embedded in N=4 SYM. The field theory defect respectively the probe D-brane can be described as an integrable boundary state of the integrable system underlying the AdS/CFT correspondence.","sentences":["We review a class of integrable, supersymmetric defect conformal field theories which have holographic duals in the form of probe brane models.","Our main examples are defect versions of N=4 SYM and ABJM theory, both involving a domain wall with Nahm pole boundary conditions, and the case of a 't Hooft line embedded in N=4 SYM.","The field theory defect respectively the probe D-brane can be described as an integrable boundary state of the integrable system underlying the AdS/CFT correspondence."],"url":"http://arxiv.org/abs/2401.17144v1","category":"hep-th"}
{"created":"2024-01-30 16:21:32","title":"Realization of unitary representations of the Lorentz group on de Sitter space","abstract":"This paper builds on our previous work in which we showed that, for all connected semisimple linear Lie groups $G$ acting on a non-compactly causal symmetric space $M = G/H$, every irreducible unitary representation of $G$ can be realized by boundary value maps of holomorphic extensions in distributional sections of a vector bundle over $M$. In the present paper we discuss this procedure for the connected Lorentz group $G = SO_{1,d}(R)_e$ acting on de Sitter space $M = dS^d$. We show in particular that the previously constructed nets of real subspaces satisfy the locality condition. Following ideas of Bros and Moschella from the 1990's, we show that the matrix-valued spherical function that corresponds to our extension process extends analytically to a large domain $G_C^{cut}$ in the complexified group $G_C = \\SO_{1,d}(C)$, which for $d = 1$ specializes to the complex cut plane $C \\setminus (-\\infinity, 0]$. A number of special situations is discussed specifically: (a) The case $d = 1$, which closely corresponds to standard subspaces in Hilbert spaces, (b) the case of scalar-valued functions, which for $d > 2$ is the case of spherical representations, for which we also describe the jump singularities of the holomorphic extensions on the cut in de Sitter space, (c) the case $d = 3$, where we obtain rather explicit formulas for the matrix-valued spherical functions.","sentences":["This paper builds on our previous work in which we showed that, for all connected semisimple linear Lie groups $G$ acting on a non-compactly causal symmetric space $M = G/H$, every irreducible unitary representation of $G$ can be realized by boundary value maps of holomorphic extensions in distributional sections of a vector bundle over $M$. In the present paper we discuss this procedure for the connected Lorentz group $G = SO_{1,d}(R)_e$ acting on de Sitter space $M = dS^d$.","We show in particular that the previously constructed nets of real subspaces satisfy the locality condition.","Following ideas of Bros and Moschella from the 1990's, we show that the matrix-valued spherical function that corresponds to our extension process extends analytically to a large domain $G_C^{cut}$ in the complexified group $G_C = \\SO_{1,d}(C)$, which for $d = 1$ specializes to the complex cut plane $C \\setminus (-\\infinity, 0]$.","A number of special situations is discussed specifically: (a) The case $d = 1$, which closely corresponds to standard subspaces in Hilbert spaces, (b) the case of scalar-valued functions, which for $d > 2$ is the case of spherical representations, for which we also describe the jump singularities of the holomorphic extensions on the cut in de Sitter space, (c) the case $d = 3$, where we obtain rather explicit formulas for the matrix-valued spherical functions."],"url":"http://arxiv.org/abs/2401.17140v1","category":"math-ph"}
{"created":"2024-01-30 16:19:55","title":"Large Language Model Evaluation via Matrix Entropy","abstract":"Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing alignment quality and we find that modern large multi-modal models exhibit great alignment performance.","sentences":["Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains.","Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   ","In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs.","It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability.","Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings.","For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law.","For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing alignment quality and we find that modern large multi-modal models exhibit great alignment performance."],"url":"http://arxiv.org/abs/2401.17139v1","category":"cs.LG"}
{"created":"2024-01-30 16:17:48","title":"Nuclear scattering via quantum computing","abstract":"We propose a hybrid quantum-classical framework to solve the elastic scattering phase shift of two well-bound nuclei in an uncoupled channel. Within this framework, we develop a many-body formalism in which the continuum scattering states of the two colliding nuclei are regulated by a weak external harmonic oscillator potential with varying strength. Based on our formalism, we propose an approach to compute the eigenenergies of the low-lying scattering states of the relative motion of the colliding nuclei as a function of the oscillator strength of the confining potential. Utilizing the modified effective range expansion, we extrapolate the elastic scattering phase shift of the colliding nuclei from these eigenenergies to the limit when the external potential vanishes. In our hybrid approach, we leverage the advantage of quantum computing to solve for these eigenenergies from a set of many-nucleon Hamiltonian eigenvalue problems. These eigenenergies are inputs to classical computers to obtain the phase shift. We demonstrate our framework with two simple problems, where we implement the rodeo algorithm to solve the relevant eigenenergies with the IBM Qiskit quantum simulator. The results of both the spectra and the elastic scattering phase shifts agree well with other theoretical results.","sentences":["We propose a hybrid quantum-classical framework to solve the elastic scattering phase shift of two well-bound nuclei in an uncoupled channel.","Within this framework, we develop a many-body formalism in which the continuum scattering states of the two colliding nuclei are regulated by a weak external harmonic oscillator potential with varying strength.","Based on our formalism, we propose an approach to compute the eigenenergies of the low-lying scattering states of the relative motion of the colliding nuclei as a function of the oscillator strength of the confining potential.","Utilizing the modified effective range expansion, we extrapolate the elastic scattering phase shift of the colliding nuclei from these eigenenergies to the limit when the external potential vanishes.","In our hybrid approach, we leverage the advantage of quantum computing to solve for these eigenenergies from a set of many-nucleon Hamiltonian eigenvalue problems.","These eigenenergies are inputs to classical computers to obtain the phase shift.","We demonstrate our framework with two simple problems, where we implement the rodeo algorithm to solve the relevant eigenenergies with the IBM Qiskit quantum simulator.","The results of both the spectra and the elastic scattering phase shifts agree well with other theoretical results."],"url":"http://arxiv.org/abs/2401.17138v1","category":"nucl-th"}
{"created":"2024-01-30 16:15:55","title":"Systematically Assessing the Security Risks of AI/ML-enabled Connected Healthcare Systems","abstract":"The adoption of machine-learning-enabled systems in the healthcare domain is on the rise. While the use of ML in healthcare has several benefits, it also expands the threat surface of medical systems. We show that the use of ML in medical systems, particularly connected systems that involve interfacing the ML engine with multiple peripheral devices, has security risks that might cause life-threatening damage to a patient's health in case of adversarial interventions. These new risks arise due to security vulnerabilities in the peripheral devices and communication channels. We present a case study where we demonstrate an attack on an ML-enabled blood glucose monitoring system by introducing adversarial data points during inference. We show that an adversary can achieve this by exploiting a known vulnerability in the Bluetooth communication channel connecting the glucose meter with the ML-enabled app. We further show that state-of-the-art risk assessment techniques are not adequate for identifying and assessing these new risks. Our study highlights the need for novel risk analysis methods for analyzing the security of AI-enabled connected health devices.","sentences":["The adoption of machine-learning-enabled systems in the healthcare domain is on the rise.","While the use of ML in healthcare has several benefits, it also expands the threat surface of medical systems.","We show that the use of ML in medical systems, particularly connected systems that involve interfacing the ML engine with multiple peripheral devices, has security risks that might cause life-threatening damage to a patient's health in case of adversarial interventions.","These new risks arise due to security vulnerabilities in the peripheral devices and communication channels.","We present a case study where we demonstrate an attack on an ML-enabled blood glucose monitoring system by introducing adversarial data points during inference.","We show that an adversary can achieve this by exploiting a known vulnerability in the Bluetooth communication channel connecting the glucose meter with the ML-enabled app.","We further show that state-of-the-art risk assessment techniques are not adequate for identifying and assessing these new risks.","Our study highlights the need for novel risk analysis methods for analyzing the security of AI-enabled connected health devices."],"url":"http://arxiv.org/abs/2401.17136v1","category":"cs.CR"}
{"created":"2024-01-30 16:11:31","title":"Wrist movement classification for adaptive mobile phone based rehabilitation of children with motor skill impairments","abstract":"Rehabilitation exercises performed by children with cerebral palsy are tedious and repetitive. To make them more engaging, we propose to use an exergame approach, where an adaptive application can help the child remain stimulated and interested during exercises. In this paper, we describe how the mobile phone sensors can be used to classify wrist movements of the user during the rehabilitation exercises to detect if the user is performing the correct exercise and illustrate the use of our approach in an actual mobile phone application. We also show how an adaptive difficulty system was added to the application to allow the system to adjust to the user. We present experimental results from a pilot with healthy subjects that were constrained to simulate restricted wrist movements, as well as from tests with a target group of children with cerebral palsy. Our results show that wrist movement classification is successfully achieved and results in improved interactions.","sentences":["Rehabilitation exercises performed by children with cerebral palsy are tedious and repetitive.","To make them more engaging, we propose to use an exergame approach, where an adaptive application can help the child remain stimulated and interested during exercises.","In this paper, we describe how the mobile phone sensors can be used to classify wrist movements of the user during the rehabilitation exercises to detect if the user is performing the correct exercise and illustrate the use of our approach in an actual mobile phone application.","We also show how an adaptive difficulty system was added to the application to allow the system to adjust to the user.","We present experimental results from a pilot with healthy subjects that were constrained to simulate restricted wrist movements, as well as from tests with a target group of children with cerebral palsy.","Our results show that wrist movement classification is successfully achieved and results in improved interactions."],"url":"http://arxiv.org/abs/2401.17134v1","category":"cs.HC"}
{"created":"2024-01-30 16:07:44","title":"A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion","abstract":"Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for singing voices compared to ordinary speech voices. To enhance the transferability, we propose to utilize a frame-level interaction reduction-based loss. We demonstrate the prevention effectiveness, utility, and robustness of SongBsAb on three SVC models and two datasets using both objective and human study-based subjective metrics. Our work fosters an emerging research direction for mitigating illegal automated song covers.","sentences":["Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody.","However, it raises serious concerns about copyright and civil right infringements to multiple entities.","This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers.","SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices.","SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics.","To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for singing voices compared to ordinary speech voices.","To enhance the transferability, we propose to utilize a frame-level interaction reduction-based loss.","We demonstrate the prevention effectiveness, utility, and robustness of SongBsAb on three SVC models and two datasets using both objective and human study-based subjective metrics.","Our work fosters an emerging research direction for mitigating illegal automated song covers."],"url":"http://arxiv.org/abs/2401.17133v1","category":"cs.SD"}
{"created":"2024-01-30 16:01:54","title":"Diagonals and Block-Ordered Relations","abstract":"More than 70 years ago, Jaques Riguet suggested the existence of an ``analogie frappante'' (striking analogy) between so-called ``relations de Ferrers'' and a class of difunctional relations, members of which we call ``diagonals''. Inspired by his suggestion, we formulate an ``analogie frappante'' linking the notion of a block-ordered relation and the notion of the diagonal of a relation. We formulate several novel properties of the core/index of a diagonal, and use these properties to rephrase our ``analogie frappante''. Loosely speaking, we show that a block-ordered relation is a provisional ordering up to isomorphism and reduction to its core. (Our theorems make this informal statement precise.) Unlike Riguet (and others who follow his example), we avoid almost entirely the use of nested complements to express and reason about properties of these notions: we use factors (aka residuals) instead. The only (and inevitable) exception to this is to show that our definition of a ``staircase'' relation is equivalent to Riguet's definition of a ``relation de Ferrers''. Our ``analogie frappante'' also makes it obvious that a ``staircase'' relation is not necessarily block-ordered, in spite of the mental picture of such a relation presented by Riguet.","sentences":["More than 70 years ago, Jaques Riguet suggested the existence of an ``analogie frappante'' (striking analogy) between so-called ``relations de Ferrers'' and a class of difunctional relations, members of which we call ``diagonals''.","Inspired by his suggestion, we formulate an ``analogie frappante'' linking the notion of a block-ordered relation and the notion of the diagonal of a relation.","We formulate several novel properties of the core/index of a diagonal, and use these properties to rephrase our ``analogie frappante''.","Loosely speaking, we show that a block-ordered relation is a provisional ordering up to isomorphism and reduction to its core.","(Our theorems make this informal statement precise.)","Unlike Riguet (and others who follow his example), we avoid almost entirely the use of nested complements to express and reason about properties of these notions: we use factors (aka residuals) instead.","The only (and inevitable) exception to this is to show that our definition of a ``staircase'' relation is equivalent to Riguet's definition of a ``relation de Ferrers''.","Our ``analogie frappante'' also makes it obvious that a ``staircase'' relation is not necessarily block-ordered, in spite of the mental picture of such a relation presented by Riguet."],"url":"http://arxiv.org/abs/2401.17130v1","category":"cs.LO"}
{"created":"2024-01-30 16:00:14","title":"Personalized Differential Privacy for Ridge Regression","abstract":"The increased application of machine learning (ML) in sensitive domains requires protecting the training data through privacy frameworks, such as differential privacy (DP). DP requires to specify a uniform privacy level $\\varepsilon$ that expresses the maximum privacy loss that each data point in the entire dataset is willing to tolerate. Yet, in practice, different data points often have different privacy requirements. Having to set one uniform privacy level is usually too restrictive, often forcing a learner to guarantee the stringent privacy requirement, at a large cost to accuracy. To overcome this limitation, we introduce our novel Personalized-DP Output Perturbation method (PDP-OP) that enables to train Ridge regression models with individual per data point privacy levels. We provide rigorous privacy proofs for our PDP-OP as well as accuracy guarantees for the resulting model. This work is the first to provide such theoretical accuracy guarantees when it comes to personalized DP in machine learning, whereas previous work only provided empirical evaluations. We empirically evaluate PDP-OP on synthetic and real datasets and with diverse privacy distributions. We show that by enabling each data point to specify their own privacy requirement, we can significantly improve the privacy-accuracy trade-offs in DP. We also show that PDP-OP outperforms the personalized privacy techniques of Jorgensen et al. (2015).","sentences":["The increased application of machine learning (ML) in sensitive domains requires protecting the training data through privacy frameworks, such as differential privacy (DP).","DP requires to specify a uniform privacy level $\\varepsilon$ that expresses the maximum privacy loss that each data point in the entire dataset is willing to tolerate.","Yet, in practice, different data points often have different privacy requirements.","Having to set one uniform privacy level is usually too restrictive, often forcing a learner to guarantee the stringent privacy requirement, at a large cost to accuracy.","To overcome this limitation, we introduce our novel Personalized-DP Output Perturbation method (PDP-OP) that enables to train Ridge regression models with individual per data point privacy levels.","We provide rigorous privacy proofs for our PDP-OP as well as accuracy guarantees for the resulting model.","This work is the first to provide such theoretical accuracy guarantees when it comes to personalized DP in machine learning, whereas previous work only provided empirical evaluations.","We empirically evaluate PDP-OP on synthetic and real datasets and with diverse privacy distributions.","We show that by enabling each data point to specify their own privacy requirement, we can significantly improve the privacy-accuracy trade-offs in DP.","We also show that PDP-OP outperforms the personalized privacy techniques of Jorgensen et al. (2015)."],"url":"http://arxiv.org/abs/2401.17127v1","category":"cs.LG"}
{"created":"2024-01-30 15:57:53","title":"Characterising resource management performance in Kubernetes","abstract":"A key challenge for supporting elastic behaviour in cloud systems is to achieve a good performance in automated (de-)provisioning and scheduling of computing resources. One of the key aspects that can be significant is the overheads associated with deploying, terminating and maintaining resources. Therefore, due to their lower start up and termination overhead, containers are rapidly replacing Virtual Machines (VMs) in many cloud deployments, as the computation instance of choice. In this paper, we analyse the performance of Kubernetes achieved through a Petri net-based performance model. Kubernetes is a container management system for a distributed cluster environment. Our model can be characterised using data from a Kubernetes deployment, and can be exploited for supporting capacity planning and designing Kubernetes-based elastic applications.","sentences":["A key challenge for supporting elastic behaviour in cloud systems is to achieve a good performance in automated (de-)provisioning and scheduling of computing resources.","One of the key aspects that can be significant is the overheads associated with deploying, terminating and maintaining resources.","Therefore, due to their lower start up and termination overhead, containers are rapidly replacing Virtual Machines (VMs) in many cloud deployments, as the computation instance of choice.","In this paper, we analyse the performance of Kubernetes achieved through a Petri net-based performance model.","Kubernetes is a container management system for a distributed cluster environment.","Our model can be characterised using data from a Kubernetes deployment, and can be exploited for supporting capacity planning and designing Kubernetes-based elastic applications."],"url":"http://arxiv.org/abs/2401.17125v1","category":"cs.DC"}
{"created":"2024-01-30 15:54:25","title":"Physical Priors Augmented Event-Based 3D Reconstruction","abstract":"3D neural implicit representations play a significant component in many robotic applications. However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available. In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training. The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs. Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries. More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields. The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d.","sentences":["3D neural implicit representations play a significant component in many robotic applications.","However, reconstructing neural radiance fields (NeRF) from realistic event data remains a challenge due to the sparsities and the lack of information when only event streams are available.","In this paper, we utilize motion, geometry, and density priors behind event data to impose strong physical constraints to augment NeRF training.","The proposed novel pipeline can directly benefit from those priors to reconstruct 3D scenes without additional inputs.","Moreover, we present a novel density-guided patch-based sampling strategy for robust and efficient learning, which not only accelerates training procedures but also conduces to expressions of local geometries.","More importantly, we establish the first large dataset for event-based 3D reconstruction, which contains 101 objects with various materials and geometries, along with the groundtruth of images and depth maps for all camera viewpoints, which significantly facilitates other research in the related fields.","The code and dataset will be publicly available at https://github.com/Mercerai/PAEv3d."],"url":"http://arxiv.org/abs/2401.17121v1","category":"cs.RO"}
{"created":"2024-01-30 15:53:42","title":"PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering","abstract":"Landscape renderings are realistic images of landscape sites, allowing stakeholders to perceive better and evaluate design ideas. While recent advances in Generative Artificial Intelligence (GAI) enable automated generation of landscape renderings, the end-to-end methods are not compatible with common design processes, leading to insufficient alignment with design idealizations and limited cohesion of iterative landscape design. Informed by a formative study for comprehending design requirements, we present PlantoGraphy, an iterative design system that allows for interactive configuration of GAI models to accommodate human-centered design practice. A two-stage pipeline is incorporated: first, concretization module transforms conceptual ideas into concrete scene layouts with a domain-oriented large language model; and second, illustration module converts scene layouts into realistic landscape renderings using a fine-tuned low-rank adaptation diffusion model. PlantoGraphy has undergone a series of performance evaluations and user studies, demonstrating its effectiveness in landscape rendering generation and the high recognition of its interactive functionality.","sentences":["Landscape renderings are realistic images of landscape sites, allowing stakeholders to perceive better and evaluate design ideas.","While recent advances in Generative Artificial Intelligence (GAI) enable automated generation of landscape renderings, the end-to-end methods are not compatible with common design processes, leading to insufficient alignment with design idealizations and limited cohesion of iterative landscape design.","Informed by a formative study for comprehending design requirements, we present PlantoGraphy, an iterative design system that allows for interactive configuration of GAI models to accommodate human-centered design practice.","A two-stage pipeline is incorporated: first, concretization module transforms conceptual ideas into concrete scene layouts with a domain-oriented large language model; and second, illustration module converts scene layouts into realistic landscape renderings using a fine-tuned low-rank adaptation diffusion model.","PlantoGraphy has undergone a series of performance evaluations and user studies, demonstrating its effectiveness in landscape rendering generation and the high recognition of its interactive functionality."],"url":"http://arxiv.org/abs/2401.17120v1","category":"cs.HC"}
{"created":"2024-01-30 15:53:07","title":"Explainable data-driven modeling via mixture of experts: towards effective blending of grey and black-box models","abstract":"Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases. Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints. Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity. To address these issues, we propose a comprehensive framework based on a \"mixture of experts\" rationale. This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors. Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms. To enhance interpretability, we penalize abrupt variations in the expert's combination. Experimental results validate the effectiveness of our approach in producing an interpretable combination of models closely resembling the target phenomena.","sentences":["Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases.","Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints.","Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity.","To address these issues, we propose a comprehensive framework based on a \"mixture of experts\" rationale.","This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors.","Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms.","To enhance interpretability, we penalize abrupt variations in the expert's combination.","Experimental results validate the effectiveness of our approach in producing an interpretable combination of models closely resembling the target phenomena."],"url":"http://arxiv.org/abs/2401.17118v1","category":"cs.LG"}
{"created":"2024-01-30 15:52:56","title":"A Bearing-Angle Approach for Unknown Target Motion Analysis Based on Visual Measurements","abstract":"Vision-based estimation of the motion of a moving target is usually formulated as a bearing-only estimation problem where the visual measurement is modeled as a bearing vector. Although the bearing-only approach has been studied for decades, a fundamental limitation of this approach is that it requires extra lateral motion of the observer to enhance the target's observability. Unfortunately, the extra lateral motion conflicts with the desired motion of the observer in many tasks. It is well-known that, once a target has been detected in an image, a bounding box that surrounds the target can be obtained. Surprisingly, this common visual measurement especially its size information has not been well explored up to now. In this paper, we propose a new bearing-angle approach to estimate the motion of a target by modeling its image bounding box as bearing-angle measurements. Both theoretical analysis and experimental results show that this approach can significantly enhance the observability without relying on additional lateral motion of the observer. The benefit of the bearing-angle approach comes with no additional cost because a bounding box is a standard output of object detection algorithms. The approach simply exploits the information that has not been fully exploited in the past. No additional sensing devices or special detection algorithms are required.","sentences":["Vision-based estimation of the motion of a moving target is usually formulated as a bearing-only estimation problem where the visual measurement is modeled as a bearing vector.","Although the bearing-only approach has been studied for decades, a fundamental limitation of this approach is that it requires extra lateral motion of the observer to enhance the target's observability.","Unfortunately, the extra lateral motion conflicts with the desired motion of the observer in many tasks.","It is well-known that, once a target has been detected in an image, a bounding box that surrounds the target can be obtained.","Surprisingly, this common visual measurement especially its size information has not been well explored up to now.","In this paper, we propose a new bearing-angle approach to estimate the motion of a target by modeling its image bounding box as bearing-angle measurements.","Both theoretical analysis and experimental results show that this approach can significantly enhance the observability without relying on additional lateral motion of the observer.","The benefit of the bearing-angle approach comes with no additional cost because a bounding box is a standard output of object detection algorithms.","The approach simply exploits the information that has not been fully exploited in the past.","No additional sensing devices or special detection algorithms are required."],"url":"http://arxiv.org/abs/2401.17117v1","category":"cs.RO"}
{"created":"2024-01-30 15:50:06","title":"Quantum error mitigation and correction mediated by Yang-Baxter equation and artificial neural network","abstract":"Quantum computing shows great potential, but errors pose a significant challenge. This study explores new strategies for mitigating quantum errors using artificial neural networks (ANN) and the Yang-Baxter equation (YBE). Unlike traditional error correction methods, which are computationally intensive, we investigate artificial error mitigation. The manuscript introduces the basics of quantum error sources and explores the potential of using classical computation for error mitigation. The Yang-Baxter equation plays a crucial role, allowing us to compress time dynamics simulations into constant-depth circuits. By introducing controlled noise through the YBE, we enhance the dataset for error mitigation. We train an ANN model on partial data from quantum simulations, demonstrating its effectiveness in correcting errors in time-evolving quantum states.","sentences":["Quantum computing shows great potential, but errors pose a significant challenge.","This study explores new strategies for mitigating quantum errors using artificial neural networks (ANN) and the Yang-Baxter equation (YBE).","Unlike traditional error correction methods, which are computationally intensive, we investigate artificial error mitigation.","The manuscript introduces the basics of quantum error sources and explores the potential of using classical computation for error mitigation.","The Yang-Baxter equation plays a crucial role, allowing us to compress time dynamics simulations into constant-depth circuits.","By introducing controlled noise through the YBE, we enhance the dataset for error mitigation.","We train an ANN model on partial data from quantum simulations, demonstrating its effectiveness in correcting errors in time-evolving quantum states."],"url":"http://arxiv.org/abs/2401.17116v1","category":"quant-ph"}
{"created":"2024-01-30 15:50:05","title":"Identifying Quality Mersenne Twister Streams For Parallel Stochastic Simulations","abstract":"The Mersenne Twister (MT) is a pseudo-random number generator (PRNG) widely used in High Performance Computing for parallel stochastic simulations. We aim to assess the quality of common parallelization techniques used to generate large streams of MT pseudo-random numbers. We compare three techniques: sequence splitting, random spacing and MT indexed sequence. The TestU01 Big Crush battery is used to evaluate the quality of 4096 streams for each technique on three different hardware configurations. Surprisingly, all techniques exhibited almost 30% of defects with no technique showing better quality than the others. While all 106 Big Crush tests showed failures, the failure rate was limited to a small number of tests (maximum of 6 tests failed per stream, resulting in over 94% success rate). Thanks to 33 CPU years, high-quality streams identified are given. They can be used for sensitive parallel simulations such as nuclear medicine and precise high-energy physics applications.","sentences":["The Mersenne Twister (MT) is a pseudo-random number generator (PRNG) widely used in High Performance Computing for parallel stochastic simulations.","We aim to assess the quality of common parallelization techniques used to generate large streams of MT pseudo-random numbers.","We compare three techniques: sequence splitting, random spacing and MT indexed sequence.","The TestU01 Big Crush battery is used to evaluate the quality of 4096 streams for each technique on three different hardware configurations.","Surprisingly, all techniques exhibited almost 30% of defects with no technique showing better quality than the others.","While all 106 Big Crush tests showed failures, the failure rate was limited to a small number of tests (maximum of 6 tests failed per stream, resulting in over 94% success rate).","Thanks to 33 CPU years, high-quality streams identified are given.","They can be used for sensitive parallel simulations such as nuclear medicine and precise high-energy physics applications."],"url":"http://arxiv.org/abs/2401.17115v1","category":"cs.DC"}
{"created":"2024-01-30 15:47:28","title":"n-dimensional non-commutative GUP quantization and application to the Bianchi I model","abstract":"We analyse a n-dimensional Generalized Uncertainty Principle (GUP) quantization framework, characterized by a non-commutative nature of the configurational variables. First, we identify a set of states which are maximally localized only along a single direction, at the expense of being less localized in all the other ones. Subsequently, in order to recover information about localization on the whole configuration space, we use the only state of the theory which exhibits maximal localization simultaneously in every direction to construct a satisfactory quasi-position representation, by virtue of a suitable translational operator. The resultant quantum framework is then applied to model the dynamics of the Bianchi I cosmology. The corresponding Wheeler-DeWitt equation is reduced to Schr\\\"odinger dynamics for the two anisotropy degrees of freedom, using a WKB representation for the volume-like variable of the Universe, in accordance with the Vilenkin scenario. The main result of our cosmological implementation of the constructed quantum theory demonstrates how the dynamics of a wave packet peaked at some point in the configuration space represented in the quasi-position variables, favours as the most probable configuration exactly the initial one for a relatively long time, if compared with the ordinary quantum theory. This preference arises from the distinct behavioral dynamics exhibited by wave packets in the two quantum theories.","sentences":["We analyse a n-dimensional Generalized Uncertainty Principle (GUP) quantization framework, characterized by a non-commutative nature of the configurational variables.","First, we identify a set of states which are maximally localized only along a single direction, at the expense of being less localized in all the other ones.","Subsequently, in order to recover information about localization on the whole configuration space, we use the only state of the theory which exhibits maximal localization simultaneously in every direction to construct a satisfactory quasi-position representation, by virtue of a suitable translational operator.","The resultant quantum framework is then applied to model the dynamics of the Bianchi I cosmology.","The corresponding Wheeler-DeWitt equation is reduced to Schr\\\"odinger dynamics for the two anisotropy degrees of freedom, using a WKB representation for the volume-like variable of the Universe, in accordance with the Vilenkin scenario.","The main result of our cosmological implementation of the constructed quantum theory demonstrates how the dynamics of a wave packet peaked at some point in the configuration space represented in the quasi-position variables, favours as the most probable configuration exactly the initial one for a relatively long time, if compared with the ordinary quantum theory.","This preference arises from the distinct behavioral dynamics exhibited by wave packets in the two quantum theories."],"url":"http://arxiv.org/abs/2401.17113v1","category":"gr-qc"}
{"created":"2024-01-30 15:45:30","title":"Evaluation in Neural Style Transfer: A Review","abstract":"The field of Neural Style Transfer (NST) has witnessed remarkable progress in the past few years, with approaches being able to synthesize artistic and photorealistic images and videos of exceptional quality. To evaluate such results, a diverse landscape of evaluation methods and metrics is used, including authors' opinions based on side-by-side comparisons, human evaluation studies that quantify the subjective judgements of participants, and a multitude of quantitative computational metrics which objectively assess the different aspects of an algorithm's performance. However, there is no consensus regarding the most suitable and effective evaluation procedure that can guarantee the reliability of the results. In this review, we provide an in-depth analysis of existing evaluation techniques, identify the inconsistencies and limitations of current evaluation methods, and give recommendations for standardized evaluation practices. We believe that the development of a robust evaluation framework will not only enable more meaningful and fairer comparisons among NST methods but will also enhance the comprehension and interpretation of research findings in the field.","sentences":["The field of Neural Style Transfer (NST) has witnessed remarkable progress in the past few years, with approaches being able to synthesize artistic and photorealistic images and videos of exceptional quality.","To evaluate such results, a diverse landscape of evaluation methods and metrics is used, including authors' opinions based on side-by-side comparisons, human evaluation studies that quantify the subjective judgements of participants, and a multitude of quantitative computational metrics which objectively assess the different aspects of an algorithm's performance.","However, there is no consensus regarding the most suitable and effective evaluation procedure that can guarantee the reliability of the results.","In this review, we provide an in-depth analysis of existing evaluation techniques, identify the inconsistencies and limitations of current evaluation methods, and give recommendations for standardized evaluation practices.","We believe that the development of a robust evaluation framework will not only enable more meaningful and fairer comparisons among NST methods but will also enhance the comprehension and interpretation of research findings in the field."],"url":"http://arxiv.org/abs/2401.17109v1","category":"cs.CV"}
{"created":"2024-01-30 15:45:02","title":"Joint Semantic Communication and Target Sensing for 6G Communication System","abstract":"This paper investigates the secure resource allocation for a downlink integrated sensing and communication system with multiple legal users and potential eavesdroppers. In the considered model, the base station (BS) simultaneously transmits sensing and communication signals through beamforming design, where the sensing signals can be viewed as artificial noise to enhance the security of communication signals. To further enhance the security in the semantic layer, the semantic information is extracted from the original information before transmission. The user side can only successfully recover the received information with the help of the knowledge base shared with the BS, which is stored in advance. Our aim is to maximize the sum semantic secrecy rate of all users while maintaining the minimum quality of service for each user and guaranteeing overall sensing performance. To solve this sum semantic secrecy rate maximization problem, an iterative algorithm is proposed using the alternating optimization method. The simulation results demonstrate the superiority of the proposed algorithm in terms of secure semantic communication and reliable detection.","sentences":["This paper investigates the secure resource allocation for a downlink integrated sensing and communication system with multiple legal users and potential eavesdroppers.","In the considered model, the base station (BS) simultaneously transmits sensing and communication signals through beamforming design, where the sensing signals can be viewed as artificial noise to enhance the security of communication signals.","To further enhance the security in the semantic layer, the semantic information is extracted from the original information before transmission.","The user side can only successfully recover the received information with the help of the knowledge base shared with the BS, which is stored in advance.","Our aim is to maximize the sum semantic secrecy rate of all users while maintaining the minimum quality of service for each user and guaranteeing overall sensing performance.","To solve this sum semantic secrecy rate maximization problem, an iterative algorithm is proposed using the alternating optimization method.","The simulation results demonstrate the superiority of the proposed algorithm in terms of secure semantic communication and reliable detection."],"url":"http://arxiv.org/abs/2401.17108v1","category":"cs.IT"}
{"created":"2024-01-30 15:44:37","title":"A Novel Method for Calculating Deflection Angle with Finite-Distance Correction","abstract":"In recent study in Ref.~\\cite{Dashi} (arXiv: 2401.12525), we have introduced a method aimed at calculating the weak-field asymptotic deflection angle. This method offers an efficient computational approach that avoids the complexities of integration and cumbersome iterative procedures typically associated with deflection angle calculations. In the present paper, we expand upon this method to encompass finite-distance deflection scenarios, wherein it is postulated that both the distances from the source to the lens and from the observer to the lens are finite. Importantly, this extension naturally encompasses the case of a lens in asymptotically non-flat spacetime. As an illustrative example, we apply this method to compute the gravitational deflection angle of massive particles in Kerr spacetime while accounting for the effects of finite distance.","sentences":["In recent study in Ref.~\\cite{Dashi} (arXiv: 2401.12525), we have introduced a method aimed at calculating the weak-field asymptotic deflection angle.","This method offers an efficient computational approach that avoids the complexities of integration and cumbersome iterative procedures typically associated with deflection angle calculations.","In the present paper, we expand upon this method to encompass finite-distance deflection scenarios, wherein it is postulated that both the distances from the source to the lens and from the observer to the lens are finite.","Importantly, this extension naturally encompasses the case of a lens in asymptotically non-flat spacetime.","As an illustrative example, we apply this method to compute the gravitational deflection angle of massive particles in Kerr spacetime while accounting for the effects of finite distance."],"url":"http://arxiv.org/abs/2401.17107v1","category":"gr-qc"}
{"created":"2024-01-30 15:42:17","title":"Two Regimes of Asymptotic Fall-off of a Massive Scalar Field in the Schwarzschild-de Sitter Spacetime","abstract":"The decay behavior of a massless scalar field in the Schwarzschild-de Sitter spacetime is well-known to follow an exponential law at asymptotically late times $t \\rightarrow \\infty$. In contrast, a massive scalar field in the asymptotically flat Schwarzschild background exhibits a decay with oscillatory (sinusoidal) tails enveloped by a power law. We demonstrate that the asymptotic decay of a massive scalar field in the Schwarzschild-de Sitter spacetime is exponential. Specifically, if $\\mu M \\gg 1$, where $\\mu$ and $M$ represent the mass of the field and the black hole, respectively, the exponential decay is also oscillatory. Conversely, in the regime of small $\\mu M$, the decay is purely exponential without oscillations. This distinction in decay regimes underscores the fact that, for asymptotically de Sitter spacetimes, a particular branch of quasinormal modes, instead of a ``tail'', governs the decay at asymptotically late times. There are two branches of quasinormal modes for the Schwarzschild-de Sitter spacetime: the modes of an asymptotically flat black hole corrected by a non-zero $\\Lambda$-term, and the modes of an empty de Sitter spacetime corrected by the presence of a black hole. We show that the latter branch is responsible for the asymptotic decay. When $\\mu M$ is small, the modes of pure de Sitter spacetime are purely imaginary (non-oscillatory), while at intermediate and large $\\mu M$ they have both real and imaginary parts, what produces the two pictures of the asymptotic decay. In addition, we show that asymptotic decay is similar in nature in higher dimensional spacetimes.","sentences":["The decay behavior of a massless scalar field in the Schwarzschild-de Sitter spacetime is well-known to follow an exponential law at asymptotically late times $t \\rightarrow \\infty$. In contrast, a massive scalar field in the asymptotically flat Schwarzschild background exhibits a decay with oscillatory (sinusoidal) tails enveloped by a power law.","We demonstrate that the asymptotic decay of a massive scalar field in the Schwarzschild-de Sitter spacetime is exponential.","Specifically, if $\\mu M \\gg 1$, where $\\mu$ and $M$ represent the mass of the field and the black hole, respectively, the exponential decay is also oscillatory.","Conversely, in the regime of small $\\mu M$, the decay is purely exponential without oscillations.","This distinction in decay regimes underscores the fact that, for asymptotically de Sitter spacetimes, a particular branch of quasinormal modes, instead of a ``tail'', governs the decay at asymptotically late times.","There are two branches of quasinormal modes for the Schwarzschild-de Sitter spacetime: the modes of an asymptotically flat black hole corrected by a non-zero $\\Lambda$-term, and the modes of an empty de Sitter spacetime corrected by the presence of a black hole.","We show that the latter branch is responsible for the asymptotic decay.","When $\\mu M$ is small, the modes of pure de Sitter spacetime are purely imaginary (non-oscillatory), while at intermediate and large $\\mu M$ they have both real and imaginary parts, what produces the two pictures of the asymptotic decay.","In addition, we show that asymptotic decay is similar in nature in higher dimensional spacetimes."],"url":"http://arxiv.org/abs/2401.17106v1","category":"gr-qc"}
{"created":"2024-01-30 15:35:49","title":"Bridging the gap between cutting-edge science and school: non-formal education in high-energy physics","abstract":"We report mainly on the global flagship outreach activity in particle physics: the International Particle Physics Masterclasses. It is illustrated on the example of Slovakia and the Czech Republic. The Masterclasses are described and their long-term impact is studied with the help of a survey among former participants. The positive effect of Masterclasses in shifting the attitude towards science is shown. We also discuss the modification of Masterclasses for a pandemic situation. Finally, we present CASCADE projects as a way to foster the interest in a field which has been strongly enhanced by previous experience, e.g., at Masterclasses.","sentences":["We report mainly on the global flagship outreach activity in particle physics: the International Particle Physics Masterclasses.","It is illustrated on the example of Slovakia and the Czech Republic.","The Masterclasses are described and their long-term impact is studied with the help of a survey among former participants.","The positive effect of Masterclasses in shifting the attitude towards science is shown.","We also discuss the modification of Masterclasses for a pandemic situation.","Finally, we present CASCADE projects as a way to foster the interest in a field which has been strongly enhanced by previous experience, e.g., at Masterclasses."],"url":"http://arxiv.org/abs/2401.17103v1","category":"physics.ed-ph"}
{"created":"2024-01-30 15:33:15","title":"Curvature Operators on K\u00e4hler Manifolds","abstract":"We prove that there exist K\\\"{a}hler manifolds that are not homotopy equivalent to a quotient of complex hyperbolic space but which admit a Riemannian metric with nonpositive curvature operator. This shows that K\\\"{a}hler manifolds do not satisfy the same type of rigidity with respect to the curvature operator as quaternionic hyperbolic and Cayley hyperbolic manifolds and are thus more similar to real hyperbolic manifolds in this setting. Along the way we also calculate explicit values for the eigenvalues of the curvature operator with respect to the standard complex hyperbolic metric.","sentences":["We prove that there exist K\\\"{a}hler manifolds that are not homotopy equivalent to a quotient of complex hyperbolic space but which admit a Riemannian metric with nonpositive curvature operator.","This shows that K\\\"{a}hler manifolds do not satisfy the same type of rigidity with respect to the curvature operator as quaternionic hyperbolic and Cayley hyperbolic manifolds and are thus more similar to real hyperbolic manifolds in this setting.","Along the way we also calculate explicit values for the eigenvalues of the curvature operator with respect to the standard complex hyperbolic metric."],"url":"http://arxiv.org/abs/2401.17101v1","category":"math.DG"}
{"created":"2024-01-30 15:32:56","title":"The Influence of Presentation and Performance on User Satisfaction","abstract":"The effectiveness of an IR system is gauged not just by its ability to retrieve relevant results but also by how it presents these results to users; an engaging presentation often correlates with increased user satisfaction. While existing research has delved into the link between user satisfaction, IR performance metrics, and presentation, these aspects have typically been investigated in isolation. Our research aims to bridge this gap by examining the relationship between query performance, presentation and user satisfaction. For our analysis, we conducted a between-subjects experiment comparing the effectiveness of various result card layouts for an ad-hoc news search interface. Drawing data from the TREC WaPo 2018 collection, we centered our study on four specific topics. Within each of these topics, we assessed six distinct queries with varying nDCG values. Our study involved 164 participants who were exposed to one of five distinct layouts containing result cards, such as \"title'', \"title+image'', or \"title+image+summary''. Our findings indicate that while nDCG is a strong predictor of user satisfaction at the query level, there exists no linear relationship between the performance of the query, presentation of results and user satisfaction. However, when considering the total gain on the initial result page, we observed that presentation does play a significant role in user satisfaction (at the query level) for certain layouts with result cards such as, title+image or title+image+summary. Our results also suggest that the layout differences have complex and multifaceted impacts on satisfaction. We demonstrate the capacity to equalize user satisfaction levels between queries of varying performance by changing how results are presented. This emphasizes the necessity to harmonize both performance and presentation in IR systems, considering users' diverse preferences.","sentences":["The effectiveness of an IR system is gauged not just by its ability to retrieve relevant results but also by how it presents these results to users; an engaging presentation often correlates with increased user satisfaction.","While existing research has delved into the link between user satisfaction, IR performance metrics, and presentation, these aspects have typically been investigated in isolation.","Our research aims to bridge this gap by examining the relationship between query performance, presentation and user satisfaction.","For our analysis, we conducted a between-subjects experiment comparing the effectiveness of various result card layouts for an ad-hoc news search interface.","Drawing data from the TREC WaPo 2018 collection, we centered our study on four specific topics.","Within each of these topics, we assessed six distinct queries with varying nDCG values.","Our study involved 164 participants who were exposed to one of five distinct layouts containing result cards, such as \"title'', \"title+image'', or \"title+image+summary''.","Our findings indicate that while nDCG is a strong predictor of user satisfaction at the query level, there exists no linear relationship between the performance of the query, presentation of results and user satisfaction.","However, when considering the total gain on the initial result page, we observed that presentation does play a significant role in user satisfaction (at the query level) for certain layouts with result cards such as, title+image or title+image+summary.","Our results also suggest that the layout differences have complex and multifaceted impacts on satisfaction.","We demonstrate the capacity to equalize user satisfaction levels between queries of varying performance by changing how results are presented.","This emphasizes the necessity to harmonize both performance and presentation in IR systems, considering users' diverse preferences."],"url":"http://arxiv.org/abs/2401.17100v1","category":"cs.HC"}
{"created":"2024-01-30 15:30:03","title":"MT-Ranker: Reference-free machine translation evaluation by inter-system ranking","abstract":"Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score. This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent. In practice, we often care about whether a new MT system is better or worse than some competitors. In addition, reference-free MT evaluation is increasingly practical and necessary. Unfortunately, these two practical considerations have yet to be jointly explored. In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem. Given the source sentence and a pair of translations, our system predicts which translation is better. In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior correlation with human judgments by merely using indirect supervision from natural language inference and weak supervision from our synthetic data. In the context of reference-free evaluation, MT-Ranker, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark, ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors, MT-Ranker marks state-of-the-art against reference-free as well as reference-based baselines.","sentences":["Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score.","This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring methods are based on (reference, translation) pairs, limiting their applicability in real-world scenarios where references are absent.","In practice, we often care about whether a new MT system is better or worse than some competitors.","In addition, reference-free MT evaluation is increasingly practical and necessary.","Unfortunately, these two practical considerations have yet to be jointly explored.","In this work, we formulate the reference-free MT evaluation into a pairwise ranking problem.","Given the source sentence and a pair of translations, our system predicts which translation is better.","In addition to proposing this new formulation, we further show that this new paradigm can demonstrate superior correlation with human judgments by merely using indirect supervision from natural language inference and weak supervision from our synthetic data.","In the context of reference-free evaluation, MT-Ranker, trained without any human annotations, achieves state-of-the-art results on the WMT Shared Metrics Task benchmarks DARR20, MQM20, and MQM21.","On a more challenging benchmark, ACES, which contains fine-grained evaluation criteria such as addition, omission, and mistranslation errors, MT-Ranker marks state-of-the-art against reference-free as well as reference-based baselines."],"url":"http://arxiv.org/abs/2401.17099v1","category":"cs.CL"}
{"created":"2024-01-30 15:29:32","title":"CharNet: Generalized Approach for High-Complexity Character Classification","abstract":"Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a straightforward, generalizable, and highly effective approach (CharNet) for detailed character image classification and compares its performance to that of existing approaches.","sentences":["Handwritten character recognition (HCR) is a challenging problem for machine learning researchers.","Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias.","With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem.","The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features.","With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges.","Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results.","Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity.","This paper proposes a straightforward, generalizable, and highly effective approach (CharNet) for detailed character image classification and compares its performance to that of existing approaches."],"url":"http://arxiv.org/abs/2401.17098v1","category":"cs.CV"}
{"created":"2024-01-30 15:21:50","title":"Traffic estimation in unobserved network locations using data-driven macroscopic models","abstract":"This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable. This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts. The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities. Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable. The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time. Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect to the model parameters. This property facilitates the application of computational graphs to learn parameters from vast amounts of spatiotemporal data. We also integrate neural networks and polynomial kernel functions to capture link flow interactions and enrich the mapping of traffic flows into travel times. MaTE also adds a destination choice model and a trip generation model that uses historical data on the number of trips generated by location. Experiments on synthetic data show that the model can accurately estimate travel time and traffic flow in out-of-sample links. Results obtained using real-world multi-source data from a large-scale transportation network suggest that MaTE outperforms data-driven benchmarks, especially in travel time estimation. The estimated parameters of MaTE are also informative about the hourly change in travel demand and supply characteristics of the transportation network.","sentences":["This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable.","This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts.","The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities.","Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable.","The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time.","Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect to the model parameters.","This property facilitates the application of computational graphs to learn parameters from vast amounts of spatiotemporal data.","We also integrate neural networks and polynomial kernel functions to capture link flow interactions and enrich the mapping of traffic flows into travel times.","MaTE also adds a destination choice model and a trip generation model that uses historical data on the number of trips generated by location.","Experiments on synthetic data show that the model can accurately estimate travel time and traffic flow in out-of-sample links.","Results obtained using real-world multi-source data from a large-scale transportation network suggest that MaTE outperforms data-driven benchmarks, especially in travel time estimation.","The estimated parameters of MaTE are also informative about the hourly change in travel demand and supply characteristics of the transportation network."],"url":"http://arxiv.org/abs/2401.17095v1","category":"cs.LG"}
{"created":"2024-01-30 15:20:26","title":"StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis","abstract":"To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes. This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information. Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation ''stroke tokens'' on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up to a 94x speedup in inference over the speed of prior methods with an exceptional SVG code compression ratio of 6.9%.","sentences":["To leverage LLMs for visual synthesis, traditional methods convert raster image information into discrete grid tokens through specialized visual modules, while disrupting the model's ability to capture the true semantic representation of visual scenes.","This paper posits that an alternative representation of images, vector graphics, can effectively surmount this limitation by enabling a more natural and semantically coherent segmentation of the image information.","Thus, we introduce StrokeNUWA, a pioneering work exploring a better visual representation ''stroke tokens'' on vector graphics, which is inherently visual semantics rich, naturally compatible with LLMs, and highly compressed.","Equipped with stroke tokens, StrokeNUWA can significantly surpass traditional LLM-based and optimization-based methods across various metrics in the vector graphic generation task.","Besides, StrokeNUWA achieves up to a 94x speedup in inference over the speed of prior methods with an exceptional SVG code compression ratio of 6.9%."],"url":"http://arxiv.org/abs/2401.17093v1","category":"cs.CV"}
{"created":"2024-01-30 15:18:29","title":"NNOSE: Nearest Neighbor Occupational Skill Extraction","abstract":"The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks -- combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, \\textbf{N}earest \\textbf{N}eighbor \\textbf{O}ccupational \\textbf{S}kill \\textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction \\emph{without} additional fine-tuning. Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30\\% span-F1 in cross-dataset settings.","sentences":["The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text.","With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well.","We tackle the complexity in occupational skill datasets tasks -- combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets.","In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner.","Our proposed method, \\textbf{N}earest \\textbf{N}eighbor \\textbf{O}ccupational \\textbf{S}kill \\textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore.","This improves skill extraction \\emph{without} additional fine-tuning.","Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30\\% span-F1 in cross-dataset settings."],"url":"http://arxiv.org/abs/2401.17092v1","category":"cs.CL"}
{"created":"2024-01-30 15:18:12","title":"Coexistence of low and high spin states in La$_{18}$Co$_{28}$Pb$_{3}$","abstract":"The electronic structure and magnetic properties of a newly predicted stable ternary compound La$_{18}$Co$_{28}$Pb$_{3}$ are studied using electronic structure analysis. The ground state of this compound is ferromagnetic, with three positions of nonequivalent magnetic Co atoms. A strong dependence of magnetic properties on volume shows that this system is situated near the point of magnetic instability. A coexistence of high- and low-spin ferromagnetic states as a function of volume near equilibrium was discovered. A corresponding spin tunneling splitting was estimated. The stability of the theoretically predicted magnetic ground state was tested by varying the Hubbard parameter. The thermal spin fluctuations were added to estimate the paramagnetic moment and a Curie temperature. The necessity of experimental verification of the obtained results is emphasized.","sentences":["The electronic structure and magnetic properties of a newly predicted stable ternary compound La$_{18}$Co$_{28}$Pb$_{3}$ are studied using electronic structure analysis.","The ground state of this compound is ferromagnetic, with three positions of nonequivalent magnetic Co atoms.","A strong dependence of magnetic properties on volume shows that this system is situated near the point of magnetic instability.","A coexistence of high- and low-spin ferromagnetic states as a function of volume near equilibrium was discovered.","A corresponding spin tunneling splitting was estimated.","The stability of the theoretically predicted magnetic ground state was tested by varying the Hubbard parameter.","The thermal spin fluctuations were added to estimate the paramagnetic moment and a Curie temperature.","The necessity of experimental verification of the obtained results is emphasized."],"url":"http://arxiv.org/abs/2401.17091v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-01-30 15:17:37","title":"Copula-based Estimation of Continuous Sources for a Class of Constrained Rate-Distortion-Functions","abstract":"We present a new method to estimate the rate-distortion-perception function in the perfect realism regime (PR-RDPF), for multivariate continuous sources subject to a single-letter average distortion constraint. The proposed approach is not only able to solve the specific problem but also two related problems: the entropic optimal transport (EOT) and the output-constrained rate-distortion function (OC-RDF), of which the PR-RDPF represents a special case. Using copula distributions, we show that the OC-RDF can be cast as an I-projection problem on a convex set, based on which we develop a parametric solution of the optimal projection proving that its parameters can be estimated, up to an arbitrary precision, via the solution of a convex program. Subsequently, we propose an iterative scheme via gradient methods to estimate the convex program. Lastly, we characterize a Shannon lower bound (SLB) for the PR-RDPF under a mean squared error (MSE) distortion constraint. We support our theoretical findings with numerical examples by assessing the estimation performance of our iterative scheme using the PR-RDPF with the obtained SLB for various sources.","sentences":["We present a new method to estimate the rate-distortion-perception function in the perfect realism regime (PR-RDPF), for multivariate continuous sources subject to a single-letter average distortion constraint.","The proposed approach is not only able to solve the specific problem but also two related problems: the entropic optimal transport (EOT) and the output-constrained rate-distortion function (OC-RDF), of which the PR-RDPF represents a special case.","Using copula distributions, we show that the OC-RDF can be cast as an I-projection problem on a convex set, based on which we develop a parametric solution of the optimal projection proving that its parameters can be estimated, up to an arbitrary precision, via the solution of a convex program.","Subsequently, we propose an iterative scheme via gradient methods to estimate the convex program.","Lastly, we characterize a Shannon lower bound (SLB) for the PR-RDPF under a mean squared error (MSE) distortion constraint.","We support our theoretical findings with numerical examples by assessing the estimation performance of our iterative scheme using the PR-RDPF with the obtained SLB for various sources."],"url":"http://arxiv.org/abs/2401.17089v1","category":"cs.IT"}
{"created":"2024-01-30 15:17:29","title":"Hanbury Brown and Twiss interference of electrons in free space from independent needle tip sources","abstract":"We investigate two-electron interference in free space using two laser-triggered needle tips as independent electron sources, a fermionic realisation of the landmark Hanbury Brown and Twiss interferometer. We calculate the two-electron interference pattern in a quantum path formalism taking into account the fermionic nature and the spin configuration of the electrons. We also estimate the Coulomb repulsion in the setup in a semiclassical approach. We find that antibunching resulting from Pauli's exclusion principle and repulsion stemming from the Coulomb interaction can be clearly distinguished.","sentences":["We investigate two-electron interference in free space using two laser-triggered needle tips as independent electron sources, a fermionic realisation of the landmark Hanbury Brown and Twiss interferometer.","We calculate the two-electron interference pattern in a quantum path formalism taking into account the fermionic nature and the spin configuration of the electrons.","We also estimate the Coulomb repulsion in the setup in a semiclassical approach.","We find that antibunching resulting from Pauli's exclusion principle and repulsion stemming from the Coulomb interaction can be clearly distinguished."],"url":"http://arxiv.org/abs/2401.17088v1","category":"quant-ph"}
{"created":"2024-01-30 15:09:37","title":"Active Generation Network of Human Skeleton for Action Recognition","abstract":"Data generation is a data augmentation technique for enhancing the generalization ability for skeleton-based human action recognition. Most existing data generation methods face challenges to ensure the temporal consistency of the dynamic information for action. In addition, the data generated by these methods lack diversity when only a few training samples are available. To solve those problems, We propose a novel active generative network (AGN), which can adaptively learn various action categories by motion style transfer to generate new actions when the data for a particular action is only a single sample or few samples. The AGN consists of an action generation network and an uncertainty metric network. The former, with ST-GCN as the Backbone, can implicitly learn the morphological features of the target action while preserving the category features of the source action. The latter guides generating actions. Specifically, an action recognition model generates prediction vectors for each action, which is then scored using an uncertainty metric. Finally, UMN provides the uncertainty sampling basis for the generated actions.","sentences":["Data generation is a data augmentation technique for enhancing the generalization ability for skeleton-based human action recognition.","Most existing data generation methods face challenges to ensure the temporal consistency of the dynamic information for action.","In addition, the data generated by these methods lack diversity when only a few training samples are available.","To solve those problems, We propose a novel active generative network (AGN), which can adaptively learn various action categories by motion style transfer to generate new actions when the data for a particular action is only a single sample or few samples.","The AGN consists of an action generation network and an uncertainty metric network.","The former, with ST-GCN as the Backbone, can implicitly learn the morphological features of the target action while preserving the category features of the source action.","The latter guides generating actions.","Specifically, an action recognition model generates prediction vectors for each action, which is then scored using an uncertainty metric.","Finally, UMN provides the uncertainty sampling basis for the generated actions."],"url":"http://arxiv.org/abs/2401.17086v1","category":"cs.CV"}
{"created":"2024-01-30 15:06:33","title":"On $2 \\times 2$ MIMO Gaussian Channels with a Small Discrete-Time Peak-Power Constraint","abstract":"A multi-input multi-output (MIMO) Gaussian channel with two transmit antennas and two receive antennas is studied that is subject to an input peak-power constraint. The capacity and the capacity-achieving input distribution are unknown in general. The problem is shown to be equivalent to a channel with an identity matrix but where the input lies inside and on an ellipse with principal axis length $r_p$ and minor axis length $r_m$. If $r_p \\le \\sqrt{2}$, then the capacity-achieving input has support on the ellipse. A sufficient condition is derived under which a two-point distribution is optimal. Finally, if $r_m < r_p \\le \\sqrt{2}$, then the capacity-achieving distribution is discrete.","sentences":["A multi-input multi-output (MIMO) Gaussian channel with two transmit antennas and two receive antennas is studied that is subject to an input peak-power constraint.","The capacity and the capacity-achieving input distribution are unknown in general.","The problem is shown to be equivalent to a channel with an identity matrix but where the input lies inside and on an ellipse with principal axis length $r_p$ and minor axis length $r_m$. If $r_p \\le \\sqrt{2}$, then the capacity-achieving input has support on the ellipse.","A sufficient condition is derived under which a two-point distribution is optimal.","Finally, if $r_m < r_p \\le \\sqrt{2}$, then the capacity-achieving distribution is discrete."],"url":"http://arxiv.org/abs/2401.17084v1","category":"cs.IT"}
{"created":"2024-01-30 15:05:22","title":"Online Robot Navigation and and Manipulation with Distilled Vision-Language Models","abstract":"Autonomous robot navigation within the dynamic unknown environment is of crucial significance for mobile robotic applications including robot navigation in last-mile delivery and robot-enabled automated supplies in industrial and hospital delivery applications. Current solutions still suffer from limitations, such as the robot cannot recognize unknown objects in real time and cannot navigate freely in a dynamic, narrow, and complex environment. We propose a complete software framework for autonomous robot perception and navigation within very dense obstacles and dense human crowds. First, we propose a framework that accurately detects and segments open-world object categories in a zero-shot manner, which overcomes the over-segmentation limitation of the current SAM model. Second, we proposed the distillation strategy to distill the knowledge to segment the free space of the walkway for robot navigation without the label. In the meantime, we design the trimming strategy that works collaboratively with distillation to enable lightweight inference to deploy the neural network on edge devices such as NVIDIA-TX2 or Xavier NX during autonomous navigation. Integrated into the robot navigation system, extensive experiments demonstrate that our proposed framework has achieved superior performance in terms of both accuracy and efficiency in robot scene perception and autonomous robot navigation.","sentences":["Autonomous robot navigation within the dynamic unknown environment is of crucial significance for mobile robotic applications including robot navigation in last-mile delivery and robot-enabled automated supplies in industrial and hospital delivery applications.","Current solutions still suffer from limitations, such as the robot cannot recognize unknown objects in real time and cannot navigate freely in a dynamic, narrow, and complex environment.","We propose a complete software framework for autonomous robot perception and navigation within very dense obstacles and dense human crowds.","First, we propose a framework that accurately detects and segments open-world object categories in a zero-shot manner, which overcomes the over-segmentation limitation of the current SAM model.","Second, we proposed the distillation strategy to distill the knowledge to segment the free space of the walkway for robot navigation without the label.","In the meantime, we design the trimming strategy that works collaboratively with distillation to enable lightweight inference to deploy the neural network on edge devices such as NVIDIA-TX2 or Xavier NX during autonomous navigation.","Integrated into the robot navigation system, extensive experiments demonstrate that our proposed framework has achieved superior performance in terms of both accuracy and efficiency in robot scene perception and autonomous robot navigation."],"url":"http://arxiv.org/abs/2401.17083v1","category":"cs.RO"}
{"created":"2024-01-30 15:04:04","title":"Casting manipulation of unknown string by robot arm","abstract":"Casting manipulation has been studied to expand the robot's movable range. In this manipulation, the robot throws and reaches the end effector to a distant target. Usually, a special casting manipulator, which consists of rigid arm links and specific flexible linear objects, is constructed for an effective casting manipulation. However, the special manipulator cannot perform normal manipulations, such as picking and placing, grasping, and operating objects. We propose that the normal robot arm, which can perform normal tasks, picks up an unknown string in the surrounding environment and realizes casting manipulation with it. As the properties of the string are not provided in advance, it is crucial how to reflect it in casting manipulation. This is realized by the motion generation of the robot arm with the simulation of string movement, actual string manipulation by the robot arm, and string parameter estimation from the actual string movement. After repeating these three steps, the simulated string movement approximates the actual to realize casting manipulation with the unknown string. We confirmed the effectiveness of the proposed method through experiments. The try of this study will lead to enhancement of the performance of home service robot, exploration robot, rescue robot and entertainment robot.","sentences":["Casting manipulation has been studied to expand the robot's movable range.","In this manipulation, the robot throws and reaches the end effector to a distant target.","Usually, a special casting manipulator, which consists of rigid arm links and specific flexible linear objects, is constructed for an effective casting manipulation.","However, the special manipulator cannot perform normal manipulations, such as picking and placing, grasping, and operating objects.","We propose that the normal robot arm, which can perform normal tasks, picks up an unknown string in the surrounding environment and realizes casting manipulation with it.","As the properties of the string are not provided in advance, it is crucial how to reflect it in casting manipulation.","This is realized by the motion generation of the robot arm with the simulation of string movement, actual string manipulation by the robot arm, and string parameter estimation from the actual string movement.","After repeating these three steps, the simulated string movement approximates the actual to realize casting manipulation with the unknown string.","We confirmed the effectiveness of the proposed method through experiments.","The try of this study will lead to enhancement of the performance of home service robot, exploration robot, rescue robot and entertainment robot."],"url":"http://arxiv.org/abs/2401.17082v1","category":"cs.RO"}
{"created":"2024-01-30 15:02:05","title":"Shortcuts to adiabatic Thouless pumping","abstract":"Thouless pumping, the quantized transport of particles in a cyclic adiabatic evolution, faces a challenge: slow driving may exceed the coherent time, while fast driving may compromise quantization. To address this dilemma, we propose expediting Thouless pumping using shortcuts to adiabaticity. By using counterdiabatic theory, we analytically derive the controlled Hamiltonian for implementing Thouless pumping beyond the adiabatic regime. Remarkably, our fast topological pumping approach allows for a significant reduction in pumping time to orders of magnitude on the order of 10$^{-11}$ when compared to traditional Thouless pumping. Furthermore, we demonstrate the resilience of our protocols against moderate noise levels. Our proposed approach offers a practical and efficient method for achieving fast topological pumping beyond the adiabatic regime.","sentences":["Thouless pumping, the quantized transport of particles in a cyclic adiabatic evolution, faces a challenge: slow driving may exceed the coherent time, while fast driving may compromise quantization.","To address this dilemma, we propose expediting Thouless pumping using shortcuts to adiabaticity.","By using counterdiabatic theory, we analytically derive the controlled Hamiltonian for implementing Thouless pumping beyond the adiabatic regime.","Remarkably, our fast topological pumping approach allows for a significant reduction in pumping time to orders of magnitude on the order of 10$^{-11}$ when compared to traditional Thouless pumping.","Furthermore, we demonstrate the resilience of our protocols against moderate noise levels.","Our proposed approach offers a practical and efficient method for achieving fast topological pumping beyond the adiabatic regime."],"url":"http://arxiv.org/abs/2401.17081v1","category":"quant-ph"}
{"created":"2024-01-30 14:57:32","title":"Dynamical Survival Analysis with Controlled Latent States","abstract":"We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.","sentences":["We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series.","We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation.","We first design a neural estimator by building on neural controlled differential equations.","In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig.","We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management."],"url":"http://arxiv.org/abs/2401.17077v1","category":"stat.ML"}
{"created":"2024-01-30 14:56:59","title":"Non-central panorama indoor dataset","abstract":"Omnidirectional images are one of the main sources of information for learning based scene understanding algorithms. However, annotated datasets of omnidirectional images cannot keep the pace of these learning based algorithms development. Among the different panoramas and in contrast to standard central ones, non-central panoramas provide geometrical information in the distortion of the image from which we can retrieve 3D information of the environment [2]. However, due to the lack of commercial non-central devices, up until now there was no dataset of these kinds of panoramas. In this data paper, we present the first dataset of non-central panoramas for indoor scene understanding. The dataset is composed by {\\bf 2574} RGB non-central panoramas taken in around 650 different rooms. Each panorama has associated a depth map and annotations to obtain the layout of the room from the image as a structural edge map, list of corners in the image, the 3D corners of the room and the camera pose. The images are taken from photorealistic virtual environments and pixel-wise automatically annotated.","sentences":["Omnidirectional images are one of the main sources of information for learning based scene understanding algorithms.","However, annotated datasets of omnidirectional images cannot keep the pace of these learning based algorithms development.","Among the different panoramas and in contrast to standard central ones, non-central panoramas provide geometrical information in the distortion of the image from which we can retrieve 3D information of the environment [2].","However, due to the lack of commercial non-central devices, up until now there was no dataset of these kinds of panoramas.","In this data paper, we present the first dataset of non-central panoramas for indoor scene understanding.","The dataset is composed by {\\bf 2574} RGB non-central panoramas taken in around 650 different rooms.","Each panorama has associated a depth map and annotations to obtain the layout of the room from the image as a structural edge map, list of corners in the image, the 3D corners of the room and the camera pose.","The images are taken from photorealistic virtual environments and pixel-wise automatically annotated."],"url":"http://arxiv.org/abs/2401.17075v1","category":"cs.DB"}
{"created":"2024-01-30 14:52:50","title":"SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity","abstract":"Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for the evaluation of instruction-tuned LLMs.","sentences":["Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions.","However, many current works rely on manual evaluation to judge the quality of generated responses.","Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants.","In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS).","We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation.","We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation.","These findings indicate the utility of our proposed metric for the evaluation of instruction-tuned LLMs."],"url":"http://arxiv.org/abs/2401.17072v1","category":"cs.CL"}
{"created":"2024-01-30 14:52:40","title":"Ultra-low power sensor devices for monitoring physical activity and respiratory frequency in farmed fish","abstract":"Integration of technological solutions aims to improve accuracy, precision and repeatability in farming operations, and biosensor devices are increasingly used for understanding basic biology during livestock production. The aim of this study was to design and validate a miniaturized tri-axial accelerometer for non-invasive monitoring of farmed fish with re-programmable schedule protocols.The device was attached to the operculum of gilthead sea bream and European sea bass juveniles for monitoring their physical activity by measurements of movement accelerations in x and y-axes, while records of operculum beats served as a measurement of respiratory frequency. Data post-processing of exercised fish in swimming test chambers revealed an exponential increase of fish accelerations with the increase of fish speed from 1 body-length to 4 body-lengths per second, while a close relationship between oxygen consumption and opercular frequency was consistently found.The usefulness of low computational load for data pre-processing with on-board algorithms was verified from low to submaximal exercise, increasing this procedure the autonomy of the system up to 6 h of data recording with different programmable schedules. Visual observations regarding tissue damage, feeding behavior and circulating levels of stress markers did not reveal at short term a negative impact of device tagging. Reduced plasma levels of triglycerides revealed a transient inhibition of feed intake in small fish, but this disturbance was not detected in larger fish. All this considered together is the proof of concept that miniaturized devices are suitable for non-invasive and reliable metabolic phenotyping of farmed fish to improve their overall performance and welfare. Further work is underway for improving the attachment procedure and the full device packaging.","sentences":["Integration of technological solutions aims to improve accuracy, precision and repeatability in farming operations, and biosensor devices are increasingly used for understanding basic biology during livestock production.","The aim of this study was to design and validate a miniaturized tri-axial accelerometer for non-invasive monitoring of farmed fish with re-programmable schedule protocols.","The device was attached to the operculum of gilthead sea bream and European sea bass juveniles for monitoring their physical activity by measurements of movement accelerations in x and y-axes, while records of operculum beats served as a measurement of respiratory frequency.","Data post-processing of exercised fish in swimming test chambers revealed an exponential increase of fish accelerations with the increase of fish speed from 1 body-length to 4 body-lengths per second, while a close relationship between oxygen consumption and opercular frequency was consistently found.","The usefulness of low computational load for data pre-processing with on-board algorithms was verified from low to submaximal exercise, increasing this procedure the autonomy of the system up to 6 h of data recording with different programmable schedules.","Visual observations regarding tissue damage, feeding behavior and circulating levels of stress markers did not reveal at short term a negative impact of device tagging.","Reduced plasma levels of triglycerides revealed a transient inhibition of feed intake in small fish, but this disturbance was not detected in larger fish.","All this considered together is the proof of concept that miniaturized devices are suitable for non-invasive and reliable metabolic phenotyping of farmed fish to improve their overall performance and welfare.","Further work is underway for improving the attachment procedure and the full device packaging."],"url":"http://arxiv.org/abs/2401.17070v1","category":"cs.CE"}
{"created":"2024-01-30 14:49:15","title":"Markovian to non-Markovian phase transition in the operator dynamics of a mobile impurity","abstract":"We study a random unitary circuit model of an impurity moving through a chaotic medium. By varying the velocity of the impurity, $v_d$, relative to the speed of information propagation within the medium, $v_B$, we control the exchange of information between the medium and impurity. Above supersonic velocities, $v_d> v_B$, information cannot flow back to the impurity after it has moved into the medium, and the resulting dynamics are Markovian. Below supersonic velocities, $v_d< v_B$, the dynamics of the impurity and medium are non-Markovian, and information is able to flow back onto the impurity. We show the two regimes are separated by a continuous phase transition with exponents directly related to the diffusive spreading of operators in the medium. This is demonstrated by monitoring an out-of-time-order correlator (OTOC) in a scenario where the impurity is substituted at an intermediate time. During the Markovian phase, information from the medium cannot transfer onto the replaced impurity, manifesting in no significant operator development. Conversely, in the non-Markovian phase, we observe that operators acquire support on the newly introduced impurity. We also characterize the dynamics using the coherent information and provide two decoders which can efficiently probe the transition between Markovian and non-Markovian information flow. Our work demonstrates that Markovian and non-Markovian dynamics can be separated by a phase transition, and we propose an efficient protocol for observing this transition.","sentences":["We study a random unitary circuit model of an impurity moving through a chaotic medium.","By varying the velocity of the impurity, $v_d$, relative to the speed of information propagation within the medium, $v_B$, we control the exchange of information between the medium and impurity.","Above supersonic velocities, $v_d> v_B$, information cannot flow back to the impurity after it has moved into the medium, and the resulting dynamics are Markovian.","Below supersonic velocities, $v_d< v_B$, the dynamics of the impurity and medium are non-Markovian, and information is able to flow back onto the impurity.","We show the two regimes are separated by a continuous phase transition with exponents directly related to the diffusive spreading of operators in the medium.","This is demonstrated by monitoring an out-of-time-order correlator (OTOC) in a scenario where the impurity is substituted at an intermediate time.","During the Markovian phase, information from the medium cannot transfer onto the replaced impurity, manifesting in no significant operator development.","Conversely, in the non-Markovian phase, we observe that operators acquire support on the newly introduced impurity.","We also characterize the dynamics using the coherent information and provide two decoders which can efficiently probe the transition between Markovian and non-Markovian information flow.","Our work demonstrates that Markovian and non-Markovian dynamics can be separated by a phase transition, and we propose an efficient protocol for observing this transition."],"url":"http://arxiv.org/abs/2401.17066v1","category":"quant-ph"}
{"created":"2024-01-30 14:42:35","title":"Efficient Gesture Recognition on Spiking Convolutional Networks Through Sensor Fusion of Event-Based and Depth Data","abstract":"As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed. Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient. Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used. This work proposes a Spiking Convolutional Neural Network, processing event- and depth data for gesture recognition. The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system. For the evaluation three open source data sets are used. Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded. The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities.","sentences":["As intelligent systems become increasingly important in our daily lives, new ways of interaction are needed.","Classical user interfaces pose issues for the physically impaired and are partially not practical or convenient.","Gesture recognition is an alternative, but often not reactive enough when conventional cameras are used.","This work proposes a Spiking Convolutional Neural Network, processing event-","and depth data for gesture recognition.","The network is simulated using the open-source neuromorphic computing framework LAVA for offline training and evaluation on an embedded system.","For the evaluation three open source data sets are used.","Since these do not represent the applied bi-modality, a new data set with synchronized event- and depth data was recorded.","The results show the viability of temporal encoding on depth information and modality fusion, even on differently encoded data, to be beneficial to network performance and generalization capabilities."],"url":"http://arxiv.org/abs/2401.17064v1","category":"cs.RO"}
{"created":"2024-01-30 14:41:40","title":"SPViz: A DSL-Driven Approach for Software Project Visualization Tooling","abstract":"For most service architectures, such as OSGi and Spring, architecture-specific tools allow software developers and architects to visualize otherwise obscure configurations hidden in the project files. Such visualization tools are often used for documentation purposes and help to better understand programs than with source code alone. However, such tools often do not address project-specific peculiarities or do not exist at all for less common architectures, requiring developers to use different visualization and analysis tools within the same architecture. Furthermore, many generic modeling tools and architecture visualization tools require their users to create and maintain models manually.   We here propose a DSL-driven approach that allows software architects to define and adapt their own project visualization tool. The approach, which we refer to as Software Project Visualization (SPViz), uses two DSLs, one to describe architectural elements and their relationships, and one to describe how these should be visualized. We demonstrate how SPViz can then automatically synthesize a customized, project-specific visualization tool that can adapt to changes in the underlying project automatically.   We implemented our approach in an open-source library, also termed SPViz and discuss and analyze four different tools that follow this concept, including open-source projects and projects from an industrial partner in the railway domain.","sentences":["For most service architectures, such as OSGi and Spring, architecture-specific tools allow software developers and architects to visualize otherwise obscure configurations hidden in the project files.","Such visualization tools are often used for documentation purposes and help to better understand programs than with source code alone.","However, such tools often do not address project-specific peculiarities or do not exist at all for less common architectures, requiring developers to use different visualization and analysis tools within the same architecture.","Furthermore, many generic modeling tools and architecture visualization tools require their users to create and maintain models manually.   ","We here propose a DSL-driven approach that allows software architects to define and adapt their own project visualization tool.","The approach, which we refer to as Software Project Visualization (SPViz), uses two DSLs, one to describe architectural elements and their relationships, and one to describe how these should be visualized.","We demonstrate how SPViz can then automatically synthesize a customized, project-specific visualization tool that can adapt to changes in the underlying project automatically.   ","We implemented our approach in an open-source library, also termed SPViz and discuss and analyze four different tools that follow this concept, including open-source projects and projects from an industrial partner in the railway domain."],"url":"http://arxiv.org/abs/2401.17063v1","category":"cs.SE"}
{"created":"2024-01-30 14:41:28","title":"Outline of an Independent Systematic Blackbox Test for ML-based Systems","abstract":"This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process. In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data. The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems.","sentences":["This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process.","In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data.","The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems."],"url":"http://arxiv.org/abs/2401.17062v1","category":"cs.LG"}
{"created":"2024-01-30 14:40:19","title":"OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision","abstract":"Omnidirectional and 360{\\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models. Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature. Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas.","sentences":["Omnidirectional and 360{\\deg} images are becoming widespread in industry and in consumer society, causing omnidirectional computer vision to gain attention.","Their wide field of view allows the gathering of a great amount of information about the environment from only an image.","However, the distortion of these images requires the development of specific algorithms for their treatment and interpretation.","Moreover, a high number of images is essential for the correct training of computer vision algorithms based on learning.","In this paper, we present a tool for generating datasets of omnidirectional images with semantic and depth information.","These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin.","We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas, different fish-eye lenses, catadioptric systems, and empiric models.","Furthermore, we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems.","As far as we know, this is the first reported tool for generating photorealistic non-central images in the literature.","Moreover, since the omnidirectional images are made virtually, we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras.","This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches.","To validate the proposed tool, different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images, 3D Layout recovery and SLAM using equirectangular panoramas, and 3D reconstruction from non-central panoramas."],"url":"http://arxiv.org/abs/2401.17061v1","category":"cs.CV"}
{"created":"2024-01-30 14:40:03","title":"Learning Approximation Sets for Exploratory Queries","abstract":"In data exploration, executing complex non-aggregate queries over large databases can be time-consuming. Our paper introduces a novel approach to address this challenge, focusing on finding an optimized subset of data, referred to as the approximation set, for query execution. The goal is to maximize query result quality while minimizing execution time. We formalize this problem as Approximate Non-Aggregates Query Processing (ANAQP) and establish its NP-completeness. To tackle this, we propose an approximate solution using advanced Reinforcement Learning architecture, termed ASQP-RL. This approach overcomes challenges related to the large action space and the need for generalization beyond a known query workload. Experimental results on two benchmarks demonstrate the superior performance of ASQP-RL, outperforming baselines by 30% in accuracy and achieving efficiency gains of 10-35X. Our research sheds light on the potential of reinforcement learning techniques for advancing data management tasks. Experimental results on two benchmarks show that ASQP-RL significantly outperforms the baselines both in terms of accuracy (30% better) and efficiency (10-35X). This research provides valuable insights into the potential of RL techniques for future advancements in data management tasks.","sentences":["In data exploration, executing complex non-aggregate queries over large databases can be time-consuming.","Our paper introduces a novel approach to address this challenge, focusing on finding an optimized subset of data, referred to as the approximation set, for query execution.","The goal is to maximize query result quality while minimizing execution time.","We formalize this problem as Approximate Non-Aggregates Query Processing (ANAQP) and establish its NP-completeness.","To tackle this, we propose an approximate solution using advanced Reinforcement Learning architecture, termed ASQP-RL.","This approach overcomes challenges related to the large action space and the need for generalization beyond a known query workload.","Experimental results on two benchmarks demonstrate the superior performance of ASQP-RL, outperforming baselines by 30% in accuracy and achieving efficiency gains of 10-35X. Our research sheds light on the potential of reinforcement learning techniques for advancing data management tasks.","Experimental results on two benchmarks show that ASQP-RL significantly outperforms the baselines both in terms of accuracy (30% better) and efficiency (10-35X).","This research provides valuable insights into the potential of RL techniques for future advancements in data management tasks."],"url":"http://arxiv.org/abs/2401.17059v1","category":"cs.DB"}
{"created":"2024-01-30 14:39:38","title":"Atlanta Scaled layouts from non-central panoramas","abstract":"In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without geometric nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover an scaled 3D layout. The results of our experiments show that we improve state-of-the-art methods for layout reconstruction and line extraction in non-central projection systems. We completely solve the problem in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authors knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.","sentences":["In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system.","From a non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without geometric nor scale assumptions.","However, their sensitivity to noise and complex geometric modeling has led these panoramas being little investigated.","Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover an scaled 3D layout.","The results of our experiments show that we improve state-of-the-art methods for layout reconstruction and line extraction in non-central projection systems.","We completely solve the problem in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements.","As far as the authors knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas."],"url":"http://arxiv.org/abs/2401.17058v1","category":"cs.CV"}
{"created":"2024-01-30 14:38:45","title":"What can Information Guess? Guessing Advantage vs. R\u00e9nyi Entropy for Small Leakages","abstract":"We leverage the Gibbs inequality and its natural generalization to R\\'enyi entropies to derive closed-form parametric expressions of the optimal lower bounds of $\\rho$th-order guessing entropy (guessing moment) of a secret taking values on a finite set, in terms of the R\\'enyi-Arimoto $\\alpha$-entropy. This is carried out in an non-asymptotic regime when side information may be available. The resulting bounds yield a theoretical solution to a fundamental problem in side-channel analysis: Ensure that an adversary will not gain much guessing advantage when the leakage information is sufficiently weakened by proper countermeasures in a given cryptographic implementation. Practical evaluation for classical leakage models show that the proposed bounds greatly improve previous ones for analyzing the capability of an adversary to perform side-channel attacks.","sentences":["We leverage the Gibbs inequality and its natural generalization to R\\'enyi entropies to derive closed-form parametric expressions of the optimal lower bounds of $\\rho$th-order guessing entropy (guessing moment) of a secret taking values on a finite set, in terms of the R\\'enyi-Arimoto $\\alpha$-entropy.","This is carried out in an non-asymptotic regime when side information may be available.","The resulting bounds yield a theoretical solution to a fundamental problem in side-channel analysis:","Ensure that an adversary will not gain much guessing advantage when the leakage information is sufficiently weakened by proper countermeasures in a given cryptographic implementation.","Practical evaluation for classical leakage models show that the proposed bounds greatly improve previous ones for analyzing the capability of an adversary to perform side-channel attacks."],"url":"http://arxiv.org/abs/2401.17057v1","category":"cs.IT"}
{"created":"2024-01-30 14:38:43","title":"Floor extraction and door detection for visually impaired guidance","abstract":"Finding obstacle-free paths in unknown environments is a big navigation issue for visually impaired people and autonomous robots. Previous works focus on obstacle avoidance, however they do not have a general view of the environment they are moving in. New devices based on computer vision systems can help impaired people to overcome the difficulties of navigating in unknown environments in safe conditions. In this work it is proposed a combination of sensors and algorithms that can lead to the building of a navigation system for visually impaired people. Based on traditional systems that use RGB-D cameras for obstacle avoidance, it is included and combined the information of a fish-eye camera, which will give a better understanding of the user's surroundings. The combination gives robustness and reliability to the system as well as a wide field of view that allows to obtain many information from the environment. This combination of sensors is inspired by human vision where the center of the retina (fovea) provides more accurate information than the periphery, where humans have a wider field of view. The proposed system is mounted on a wearable device that provides the obstacle-free zones of the scene, allowing the planning of trajectories for people guidance.","sentences":["Finding obstacle-free paths in unknown environments is a big navigation issue for visually impaired people and autonomous robots.","Previous works focus on obstacle avoidance, however they do not have a general view of the environment they are moving in.","New devices based on computer vision systems can help impaired people to overcome the difficulties of navigating in unknown environments in safe conditions.","In this work it is proposed a combination of sensors and algorithms that can lead to the building of a navigation system for visually impaired people.","Based on traditional systems that use RGB-D cameras for obstacle avoidance, it is included and combined the information of a fish-eye camera, which will give a better understanding of the user's surroundings.","The combination gives robustness and reliability to the system as well as a wide field of view that allows to obtain many information from the environment.","This combination of sensors is inspired by human vision where the center of the retina (fovea) provides more accurate information than the periphery, where humans have a wider field of view.","The proposed system is mounted on a wearable device that provides the obstacle-free zones of the scene, allowing the planning of trajectories for people guidance."],"url":"http://arxiv.org/abs/2401.17056v1","category":"cs.RO"}
{"created":"2024-01-30 14:34:19","title":"BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation","abstract":"We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.","sentences":["We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene.","BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes.","Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values.","A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed.","Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation.","To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks.","The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations.","Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene.","A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements.","Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios."],"url":"http://arxiv.org/abs/2401.17053v1","category":"cs.CV"}
{"created":"2024-01-30 14:33:18","title":"Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again","abstract":"Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging. While these models excel with unstructured data, their efficacy with structured data has been limited. Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression. In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data. We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \\textit{normal} samples. We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample. Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modules may significantly boost performance.","sentences":["Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging.","While these models excel with unstructured data, their efficacy with structured data has been limited.","Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression.","In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data.","We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \\textit{normal} samples.","We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample.","Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modules may significantly boost performance."],"url":"http://arxiv.org/abs/2401.17052v1","category":"cs.LG"}
{"created":"2024-01-30 14:32:25","title":"ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained Visual Categorization","abstract":"As computer vision continues to advance and finds widespread applications across various domains, the need for interpretability in deep learning models becomes paramount. Existing methods often resort to post-hoc techniques or prototypes to explain the decision-making process, which can be indirect and lack intrinsic illustration. In this research, we introduce ViTree, a novel approach for fine-grained visual categorization that combines the popular vision transformer as a feature extraction backbone with neural decision trees. By traversing the tree paths, ViTree effectively selects patches from transformer-processed features to highlight informative local regions, thereby refining representations in a step-wise manner. Unlike previous tree-based models that rely on soft distributions or ensembles of paths, ViTree selects a single tree path, offering a clearer and simpler decision-making process. This patch and path selectivity enhances model interpretability of ViTree, enabling better insights into the model's inner workings. Remarkably, extensive experimentation validates that this streamlined approach surpasses various strong competitors and achieves state-of-the-art performance while maintaining exceptional interpretability which is proved by multi-perspective methods. Code can be found at https://github.com/SJTU-DeepVisionLab/ViTree.","sentences":["As computer vision continues to advance and finds widespread applications across various domains, the need for interpretability in deep learning models becomes paramount.","Existing methods often resort to post-hoc techniques or prototypes to explain the decision-making process, which can be indirect and lack intrinsic illustration.","In this research, we introduce ViTree, a novel approach for fine-grained visual categorization that combines the popular vision transformer as a feature extraction backbone with neural decision trees.","By traversing the tree paths, ViTree effectively selects patches from transformer-processed features to highlight informative local regions, thereby refining representations in a step-wise manner.","Unlike previous tree-based models that rely on soft distributions or ensembles of paths, ViTree selects a single tree path, offering a clearer and simpler decision-making process.","This patch and path selectivity enhances model interpretability of ViTree, enabling better insights into the model's inner workings.","Remarkably, extensive experimentation validates that this streamlined approach surpasses various strong competitors and achieves state-of-the-art performance while maintaining exceptional interpretability which is proved by multi-perspective methods.","Code can be found at https://github.com/SJTU-DeepVisionLab/ViTree."],"url":"http://arxiv.org/abs/2401.17050v1","category":"cs.CV"}
{"created":"2024-01-30 14:32:09","title":"Movable Antenna-Enabled Full-Duplex Wireless","abstract":"Movable antenna (MA) provides an innovative way to arrange antennas that can contribute to improved signal quality and more effective interference management. This method is especially beneficial for full-duplex (FD) wireless, which struggles with self-interference (SI) that usually overpowers the desired incoming signals. By dynamically repositioning transmit/receive antennas, we can mitigate the SI and enhance the reception of incoming signals. Thus, this paper proposes a novel MA-enabled point-to-point FD wireless system and formulates the minimum achievable rate of two FD terminals. To maximize the minimum achievable rate and determine the near-optimal positions of the MAs, we introduce a solution based on projected particle swarm optimization (PPSO), which can circumvent common suboptimal positioning issues. Moreover, numerical results reveal that the PPSO method leads to a better performance compared to the conventional alternating position optimization (APO). The results also demonstrate that an MA-enabled FD system outperforms the one using fixed-position antennas (FPAs).","sentences":["Movable antenna (MA) provides an innovative way to arrange antennas that can contribute to improved signal quality and more effective interference management.","This method is especially beneficial for full-duplex (FD) wireless, which struggles with self-interference (SI) that usually overpowers the desired incoming signals.","By dynamically repositioning transmit/receive antennas, we can mitigate the SI and enhance the reception of incoming signals.","Thus, this paper proposes a novel MA-enabled point-to-point FD wireless system and formulates the minimum achievable rate of two FD terminals.","To maximize the minimum achievable rate and determine the near-optimal positions of the MAs, we introduce a solution based on projected particle swarm optimization (PPSO), which can circumvent common suboptimal positioning issues.","Moreover, numerical results reveal that the PPSO method leads to a better performance compared to the conventional alternating position optimization (APO).","The results also demonstrate that an MA-enabled FD system outperforms the one using fixed-position antennas (FPAs)."],"url":"http://arxiv.org/abs/2401.17049v1","category":"cs.IT"}
{"created":"2024-01-30 14:29:03","title":"Exploring Spatial Segregation Induced by Competition Avoidance as Driving Mechanism for Emergent Coexistence in Microbial Communities","abstract":"This study investigates the role of spatial segregation, prompted by competition avoidance, as a key mechanism for emergent coexistence within microbial communities. Recognizing these communities as complex adaptive systems, we challenge the sufficiency of pairwise interaction models and consider the impact of spatial dynamics. We developed an individual-based spatial simulation depicting bacterial movement through a pattern of random walks influenced by competition avoidance, leading to the formation of spatially segregated clusters. This model was integrated with a Lotka-Volterra metapopulation framework focused on competitive interactions. Our findings reveal that spatial segregation alone can lead to emergent coexistence in microbial communities, offering a new perspective on the formation of stable, coexisting microbe clusters that differ significantly from their behavior in isolated pairwise interactions. This study underscores the importance of considering spatial factors in understanding the dynamics of microbial ecosystems.","sentences":["This study investigates the role of spatial segregation, prompted by competition avoidance, as a key mechanism for emergent coexistence within microbial communities.","Recognizing these communities as complex adaptive systems, we challenge the sufficiency of pairwise interaction models and consider the impact of spatial dynamics.","We developed an individual-based spatial simulation depicting bacterial movement through a pattern of random walks influenced by competition avoidance, leading to the formation of spatially segregated clusters.","This model was integrated with a Lotka-Volterra metapopulation framework focused on competitive interactions.","Our findings reveal that spatial segregation alone can lead to emergent coexistence in microbial communities, offering a new perspective on the formation of stable, coexisting microbe clusters that differ significantly from their behavior in isolated pairwise interactions.","This study underscores the importance of considering spatial factors in understanding the dynamics of microbial ecosystems."],"url":"http://arxiv.org/abs/2401.17048v1","category":"physics.soc-ph"}
{"created":"2024-01-30 14:27:37","title":"Explaining Explanations in Probabilistic Logic Programming","abstract":"The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being. In some approaches, the system is not transparent (often referred to as a \"black box\"), making it difficult to generate appropriate explanations. In this work, though, we consider probabilistic logic programming, a combination of logic programming (for knowledge representation) and probability (to model uncertainty). In this setting, one can say that models are interpretable, which eases its understanding. However, given a particular query, the usual notion of \"explanation\" is associated with a set of choices, one for each random variable of the model. Unfortunately, this set does not have a causal structure and, in fact, some of the choices are actually irrelevant to the considered query. In order to overcome these shortcomings, we present an approach to explaining explanations which is based on the definition of a query-driven inference mechanism for probabilistic logic programs.","sentences":["The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being.","In some approaches, the system is not transparent (often referred to as a \"black box\"), making it difficult to generate appropriate explanations.","In this work, though, we consider probabilistic logic programming, a combination of logic programming (for knowledge representation) and probability (to model uncertainty).","In this setting, one can say that models are interpretable, which eases its understanding.","However, given a particular query, the usual notion of \"explanation\" is associated with a set of choices, one for each random variable of the model.","Unfortunately, this set does not have a causal structure and, in fact, some of the choices are actually irrelevant to the considered query.","In order to overcome these shortcomings, we present an approach to explaining explanations which is based on the definition of a query-driven inference mechanism for probabilistic logic programs."],"url":"http://arxiv.org/abs/2401.17045v1","category":"cs.AI"}
{"created":"2024-01-30 14:26:04","title":"Scalable Mechanism Design for Multi-Agent Path Finding","abstract":"Multi-Agent Path Finding (MAPF) involves determining paths for multiple agents to travel simultaneously through a shared area toward particular goal locations. This problem is computationally complex, especially when dealing with large numbers of agents, as is common in realistic applications like autonomous vehicle coordination. Finding an optimal solution is often computationally infeasible, making the use of approximate algorithms essential. Adding to the complexity, agents might act in a self-interested and strategic way, possibly misrepresenting their goals to the MAPF algorithm if it benefits them. Although the field of mechanism design offers tools to align incentives, using these tools without careful consideration can fail when only having access to approximately optimal outcomes. Since approximations are crucial for scalable MAPF algorithms, this poses a significant challenge. In this work, we introduce the problem of scalable mechanism design for MAPF and propose three strategyproof mechanisms, two of which even use approximate MAPF algorithms. We test our mechanisms on realistic MAPF domains with problem sizes ranging from dozens to hundreds of agents. Our findings indicate that they improve welfare beyond a simple baseline.","sentences":["Multi-Agent Path Finding (MAPF) involves determining paths for multiple agents to travel simultaneously through a shared area toward particular goal locations.","This problem is computationally complex, especially when dealing with large numbers of agents, as is common in realistic applications like autonomous vehicle coordination.","Finding an optimal solution is often computationally infeasible, making the use of approximate algorithms essential.","Adding to the complexity, agents might act in a self-interested and strategic way, possibly misrepresenting their goals to the MAPF algorithm if it benefits them.","Although the field of mechanism design offers tools to align incentives, using these tools without careful consideration can fail when only having access to approximately optimal outcomes.","Since approximations are crucial for scalable MAPF algorithms, this poses a significant challenge.","In this work, we introduce the problem of scalable mechanism design for MAPF and propose three strategyproof mechanisms, two of which even use approximate MAPF algorithms.","We test our mechanisms on realistic MAPF domains with problem sizes ranging from dozens to hundreds of agents.","Our findings indicate that they improve welfare beyond a simple baseline."],"url":"http://arxiv.org/abs/2401.17044v1","category":"cs.AI"}
{"created":"2024-01-30 14:25:32","title":"CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models","abstract":"Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \"hallucinated\" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the range of RAG applications into four distinct types-Create, Read, Update, and Delete (CRUD), each representing a unique use case. \"Create\" refers to scenarios requiring the generation of original, varied content. \"Read\" involves responding to intricate questions in knowledge-intensive situations. \"Update\" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \"Delete\" pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed comprehensive datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, the context length, the knowledge base construction, and the LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios.","sentences":["Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources.","This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \"hallucinated\" content.","However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity.","Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous.","Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database.","To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios.","Specifically, we have categorized the range of RAG applications into four distinct types-Create, Read, Update, and Delete (CRUD), each representing a unique use case.","\"Create\" refers to scenarios requiring the generation of original, varied content.","\"Read\" involves responding to intricate questions in knowledge-intensive situations.","\"Update\" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts.","\"Delete\" pertains to the task of summarizing extensive texts into more concise forms.","For each of these CRUD categories, we have developed comprehensive datasets to evaluate the performance of RAG systems.","We also analyze the effects of various components of the RAG system, such as the retriever, the context length, the knowledge base construction, and the LLM.","Finally, we provide useful insights for optimizing the RAG technology for different scenarios."],"url":"http://arxiv.org/abs/2401.17043v1","category":"cs.CL"}
{"created":"2024-01-30 14:23:01","title":"Forecasting VIX using Bayesian Deep Learning","abstract":"Recently, deep learning techniques are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks. In this paper, we leverage probabilistic deep learning for inferring the volatility index VIX. We employ the probabilistic counterpart of WaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that TCN outperforms all models with an RMSE around 0.189. In addition, it has been well known that modern neural networks provide inaccurate uncertainty estimates. For solving this problem, we use the standard deviation scaling to calibrate the networks. Furthermore, we found out that MNF with Gaussian prior outperforms Reparameterization Trick and Flipout models in terms of precision and uncertainty predictions. Finally, we claim that MNF with Cauchy and LogUniform prior distributions yield well calibrated TCN and WaveNet networks being the former that best infer the VIX values.","sentences":["Recently, deep learning techniques are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks.","In this paper, we leverage probabilistic deep learning for inferring the volatility index VIX.","We employ the probabilistic counterpart of WaveNet, Temporal Convolutional Network (TCN), and Transformers.","We show that TCN outperforms all models with an RMSE around 0.189.","In addition, it has been well known that modern neural networks provide inaccurate uncertainty estimates.","For solving this problem, we use the standard deviation scaling to calibrate the networks.","Furthermore, we found out that MNF with Gaussian prior outperforms Reparameterization Trick and Flipout models in terms of precision and uncertainty predictions.","Finally, we claim that MNF with Cauchy and LogUniform prior distributions yield well calibrated TCN and WaveNet networks being the former that best infer the VIX values."],"url":"http://arxiv.org/abs/2401.17042v1","category":"cs.LG"}
{"created":"2024-01-30 14:21:23","title":"Condensates Breaking Up Under Rotation","abstract":"The ground state of a rotating Bose-Einstein condensate trapped in a two-dimensional anharmonic--anisotropic potential is analyzed numerically at the limit of an infinite number of particles. We find that the density breaks up along the $x$ direction in position space and along the $p_y$ direction in momentum space together with the acquisition of angular momentum. Side by side, the anisotropies of the many-particle position variances along the $x$ and $y$ directions and of the many-particle momentum variances along the $p_y$ and $p_x$ directions become opposite when computed at the many-body and mean-field levels of theory. All in all, the rotating bosons are found to possess unique correlations at the limit of an infinite number of particles, both in position and momentum spaces, although their many-body and mean-field energies per particle and densities per particle coincide and the condensate fraction is 100\\%. Implications are briefly discussed.","sentences":["The ground state of a rotating Bose-Einstein condensate trapped in a two-dimensional anharmonic--anisotropic potential is analyzed numerically at the limit of an infinite number of particles.","We find that the density breaks up along the $x$ direction in position space and along the $p_y$ direction in momentum space together with the acquisition of angular momentum.","Side by side, the anisotropies of the many-particle position variances along the $x$ and $y$ directions and of the many-particle momentum variances along the $p_y$ and $p_x$ directions become opposite when computed at the many-body and mean-field levels of theory.","All in all, the rotating bosons are found to possess unique correlations at the limit of an infinite number of particles, both in position and momentum spaces, although their many-body and mean-field energies per particle and densities per particle coincide and the condensate fraction is 100\\%.","Implications are briefly discussed."],"url":"http://arxiv.org/abs/2401.17040v1","category":"cond-mat.quant-gas"}
{"created":"2024-01-30 14:18:31","title":"Taking Action Towards Graceful Interaction: The Effects of Performing Actions on Modelling Policies for Instruction Clarification Requests","abstract":"Clarification requests are a mechanism to help solve communication problems, e.g. due to ambiguity or underspecification, in instruction-following interactions. Despite their importance, even skilful models struggle with producing or interpreting such repair acts. In this work, we test three hypotheses concerning the effects of action taking as an auxiliary task in modelling iCR policies. Contrary to initial expectations, we conclude that its contribution to learning an iCR policy is limited, but some information can still be extracted from prediction uncertainty. We present further evidence that even well-motivated, Transformer-based models fail to learn good policies for when to ask Instruction CRs (iCRs), while the task of determining what to ask about can be more successfully modelled. Considering the implications of these findings, we further discuss the shortcomings of the data-driven paradigm for learning meta-communication acts.","sentences":["Clarification requests are a mechanism to help solve communication problems, e.g. due to ambiguity or underspecification, in instruction-following interactions.","Despite their importance, even skilful models struggle with producing or interpreting such repair acts.","In this work, we test three hypotheses concerning the effects of action taking as an auxiliary task in modelling iCR policies.","Contrary to initial expectations, we conclude that its contribution to learning an iCR policy is limited, but some information can still be extracted from prediction uncertainty.","We present further evidence that even well-motivated, Transformer-based models fail to learn good policies for when to ask Instruction CRs (iCRs), while the task of determining what to ask about can be more successfully modelled.","Considering the implications of these findings, we further discuss the shortcomings of the data-driven paradigm for learning meta-communication acts."],"url":"http://arxiv.org/abs/2401.17039v1","category":"cs.CL"}
{"created":"2024-01-30 14:16:24","title":"Towards Assessing the Synthetic-to-Measured Adversarial Vulnerability of SAR ATR","abstract":"Recently, there has been increasing concern about the vulnerability of deep neural network (DNN)-based synthetic aperture radar (SAR) automatic target recognition (ATR) to adversarial attacks, where a DNN could be easily deceived by clean input with imperceptible but aggressive perturbations. This paper studies the synthetic-to-measured (S2M) transfer setting, where an attacker generates adversarial perturbation based solely on synthetic data and transfers it against victim models trained with measured data. Compared with the current measured-to-measured (M2M) transfer setting, our approach does not need direct access to the victim model or the measured SAR data. We also propose the transferability estimation attack (TEA) to uncover the adversarial risks in this more challenging and practical scenario. The TEA makes full use of the limited similarity between the synthetic and measured data pairs for blind estimation and optimization of S2M transferability, leading to feasible surrogate model enhancement without mastering the victim model and data. Comprehensive evaluations based on the publicly available synthetic and measured paired labeled experiment (SAMPLE) dataset demonstrate that the TEA outperforms state-of-the-art methods and can significantly enhance various attack algorithms in computer vision and remote sensing applications. Codes and data are available at https://github.com/scenarri/S2M-TEA.","sentences":["Recently, there has been increasing concern about the vulnerability of deep neural network (DNN)-based synthetic aperture radar (SAR) automatic target recognition (ATR) to adversarial attacks, where a DNN could be easily deceived by clean input with imperceptible but aggressive perturbations.","This paper studies the synthetic-to-measured (S2M) transfer setting, where an attacker generates adversarial perturbation based solely on synthetic data and transfers it against victim models trained with measured data.","Compared with the current measured-to-measured (M2M) transfer setting, our approach does not need direct access to the victim model or the measured SAR data.","We also propose the transferability estimation attack (TEA) to uncover the adversarial risks in this more challenging and practical scenario.","The TEA makes full use of the limited similarity between the synthetic and measured data pairs for blind estimation and optimization of S2M transferability, leading to feasible surrogate model enhancement without mastering the victim model and data.","Comprehensive evaluations based on the publicly available synthetic and measured paired labeled experiment (SAMPLE) dataset demonstrate that the TEA outperforms state-of-the-art methods and can significantly enhance various attack algorithms in computer vision and remote sensing applications.","Codes and data are available at https://github.com/scenarri/S2M-TEA."],"url":"http://arxiv.org/abs/2401.17038v1","category":"cs.CV"}
{"created":"2024-01-30 14:16:06","title":"Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration","abstract":"This paper studies Bayesian optimization with noise-free observations. We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem. Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples.","sentences":["This paper studies Bayesian optimization with noise-free observations.","We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate.","Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem.","Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples."],"url":"http://arxiv.org/abs/2401.17037v1","category":"cs.LG"}
{"created":"2024-01-30 14:16:02","title":"Intrinsic Data Constraints and Upper Bounds in Binary Classification Performance","abstract":"The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks. Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data. Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions. Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained. This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation. Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately linked to the dataset's characteristics, independent of the classifier in use. Additionally, our subsequent analysis uncovers a detailed relationship between the upper limit of performance and the level of class overlap within the binary classification data. This relationship is instrumental for pinpointing the most effective feature subsets for use in feature engineering.","sentences":["The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks.","Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data.","Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions.","Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained.","This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation.","Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately linked to the dataset's characteristics, independent of the classifier in use.","Additionally, our subsequent analysis uncovers a detailed relationship between the upper limit of performance and the level of class overlap within the binary classification data.","This relationship is instrumental for pinpointing the most effective feature subsets for use in feature engineering."],"url":"http://arxiv.org/abs/2401.17036v1","category":"cs.LG"}
{"created":"2024-01-30 14:12:39","title":"Robust Kernel Sparse Subspace Clustering","abstract":"Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC). That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space. Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick. However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem. That, however, implies normal distribution of the error. That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm. Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions. The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption. We validated proposed approach on two well-known datasets with linear robust SSC algorithm as a baseline model. According to Wilcoxon test, clustering performance obtained by the RKSSC algorithm is statistically significantly better than corresponding performance obtained by the robust SSC algorithm. MATLAB code of proposed RKSSC algorithm is posted on https://github.com/ikopriva/RKSSC.","sentences":["Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC).","That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space.","Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick.","However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem.","That, however, implies normal distribution of the error.","That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm.","Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions.","The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption.","We validated proposed approach on two well-known datasets with linear robust SSC algorithm as a baseline model.","According to Wilcoxon test, clustering performance obtained by the RKSSC algorithm is statistically significantly better than corresponding performance obtained by the robust SSC algorithm.","MATLAB code of proposed RKSSC algorithm is posted on https://github.com/ikopriva/RKSSC."],"url":"http://arxiv.org/abs/2401.17035v1","category":"cs.LG"}
{"created":"2024-01-30 14:09:41","title":"Multilayer Graph Approach to Deep Subspace Clustering","abstract":"Deep subspace clustering (DSC) networks based on self-expressive model learn representation matrix, often implemented in terms of fully connected network, in the embedded space. After the learning is finished, representation matrix is used by spectral clustering module to assign labels to clusters. However, such approach ignores complementary information that exist in other layers of the encoder (including the input data themselves). Herein, we apply selected linear subspace clustering algorithm to learn representation matrices from representations learned by all layers of encoder network including the input data. Afterward, we learn a multilayer graph that in a multi-view like manner integrates information from graph Laplacians of all used layers. That improves further performance of selected DSC network. Furthermore, we also provide formulation of our approach to cluster out-of-sample/test data points. We validate proposed approach on four well-known datasets with two DSC networks as baseline models. In almost all the cases, proposed approach achieved statistically significant improvement in three performance metrics. MATLAB code of proposed algorithm is posted on https://github.com/lovro-sinda/MLG-DSC.","sentences":["Deep subspace clustering (DSC) networks based on self-expressive model learn representation matrix, often implemented in terms of fully connected network, in the embedded space.","After the learning is finished, representation matrix is used by spectral clustering module to assign labels to clusters.","However, such approach ignores complementary information that exist in other layers of the encoder (including the input data themselves).","Herein, we apply selected linear subspace clustering algorithm to learn representation matrices from representations learned by all layers of encoder network including the input data.","Afterward, we learn a multilayer graph that in a multi-view like manner integrates information from graph Laplacians of all used layers.","That improves further performance of selected DSC network.","Furthermore, we also provide formulation of our approach to cluster out-of-sample/test data points.","We validate proposed approach on four well-known datasets with two DSC networks as baseline models.","In almost all the cases, proposed approach achieved statistically significant improvement in three performance metrics.","MATLAB code of proposed algorithm is posted on https://github.com/lovro-sinda/MLG-DSC."],"url":"http://arxiv.org/abs/2401.17033v1","category":"cs.CV"}
{"created":"2024-01-30 14:09:35","title":"M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation","abstract":"One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator and we show that it significantly enhances the learning efficiency in different manipulation tasks. This is evidenced by faster convergence rates and higher cumulative rewards per episode, compared to standard RL algorithms without our representation learning approach.","sentences":["One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities.","Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms.","However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives.","To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL).","Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms.","Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm.","We evaluate M2CURL on the Tactile Gym 2 simulator and we show that it significantly enhances the learning efficiency in different manipulation tasks.","This is evidenced by faster convergence rates and higher cumulative rewards per episode, compared to standard RL algorithms without our representation learning approach."],"url":"http://arxiv.org/abs/2401.17032v1","category":"cs.RO"}
{"created":"2024-01-30 14:08:15","title":"Partial tidal disruptions of spinning eccentric white dwarfs by spinning intermediate mass black holes","abstract":"Intermediate mass black holes (IMBHs, $\\sim 10^2-10^5M_{\\odot}$) are often dubbed as the missing link between stellar mass ($\\lesssim 10^2M_{\\odot}$) and super-massive ($\\gtrsim 10^{5-6} M_{\\odot}$) black holes. Observational signatures of these can result from tidal disruption of white dwarfs (WDs), which would otherwise be captured as a whole by super-massive black holes. Recent observations indicate that IMBHs might be rapidly spinning, while it is also known that isolated white dwarfs might have large spins, with spin periods of the order of minutes. Here, we aim to understand the effects of ``coupling'' between black hole and stellar spin, focussing on the tidal disruption of spinning WDs in the background of spinning IMBHs. Using smoothed particle hydrodynamics, we perform a suite of numerical simulations of partial tidal disruptions, where spinning WDs are in eccentric orbits about spinning IMBHs. We take a hybrid approach, where we integrate the Kerr geodesic equations while being in a regime where we can treat the internal stellar fluid dynamics in the Newtonian limit. We find substantial effects of the ``coupling'' between the black hole spin and the spin of the white dwarf, although the pericenter distance of the white dwarf is taken to be large enough so that the Newtonian limit of its fluid dynamics is a robust approximation. In particular, the core mass, the bound tail mass, and the mass difference between the two tidal tails strongly depend on such ``coupled'' spin effects. However, the late time fallback rate of the debris behaves similar to the non-spinning cases. We also compute gravitational wave amplitudes and find that while the black hole spin influences the same, there is no evidence of influence of stellar spin on such amplitudes in our regime of interest.","sentences":["Intermediate mass black holes (IMBHs, $\\sim 10^2-10^5M_{\\odot}$) are often dubbed as the missing link between stellar mass ($\\lesssim 10^2M_{\\odot}$) and super-massive ($\\gtrsim 10^{5-6} M_{\\odot}$) black holes.","Observational signatures of these can result from tidal disruption of white dwarfs (WDs), which would otherwise be captured as a whole by super-massive black holes.","Recent observations indicate that IMBHs might be rapidly spinning, while it is also known that isolated white dwarfs might have large spins, with spin periods of the order of minutes.","Here, we aim to understand the effects of ``coupling'' between black hole and stellar spin, focussing on the tidal disruption of spinning WDs in the background of spinning IMBHs.","Using smoothed particle hydrodynamics, we perform a suite of numerical simulations of partial tidal disruptions, where spinning WDs are in eccentric orbits about spinning IMBHs.","We take a hybrid approach, where we integrate the Kerr geodesic equations while being in a regime where we can treat the internal stellar fluid dynamics in the Newtonian limit.","We find substantial effects of the ``coupling'' between the black hole spin and the spin of the white dwarf, although the pericenter distance of the white dwarf is taken to be large enough so that the Newtonian limit of its fluid dynamics is a robust approximation.","In particular, the core mass, the bound tail mass, and the mass difference between the two tidal tails strongly depend on such ``coupled'' spin effects.","However, the late time fallback rate of the debris behaves similar to the non-spinning cases.","We also compute gravitational wave amplitudes and find that while the black hole spin influences the same, there is no evidence of influence of stellar spin on such amplitudes in our regime of interest."],"url":"http://arxiv.org/abs/2401.17031v1","category":"astro-ph.HE"}
{"created":"2024-01-30 14:06:09","title":"LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning Approaches and Exploring its Applications","abstract":"We investigate the prospect of reconstructing the ``cosmic distance ladder'' of the Universe using a novel deep learning framework called LADDER - Learning Algorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on the apparent magnitude data from the Pantheon Type Ia supernovae compilation, incorporating the full covariance information among data points, to produce predictions along with corresponding errors. After employing several validation tests with a number of deep learning models, we pick LADDER as the best performing one. We then demonstrate applications of our method in the cosmological context, that include serving as a model-independent tool for consistency checks for other datasets like baryon acoustic oscillations, calibration of high-redshift datasets such as gamma ray bursts, use as a model-independent mock catalog generator for future probes, etc. Our analysis advocates for interesting yet cautious consideration of machine learning applications in these contexts.","sentences":["We investigate the prospect of reconstructing the ``cosmic distance ladder'' of the Universe using a novel deep learning framework called LADDER - Learning Algorithm for Deep Distance Estimation and Reconstruction.","LADDER is trained on the apparent magnitude data from the Pantheon Type Ia supernovae compilation, incorporating the full covariance information among data points, to produce predictions along with corresponding errors.","After employing several validation tests with a number of deep learning models, we pick LADDER as the best performing one.","We then demonstrate applications of our method in the cosmological context, that include serving as a model-independent tool for consistency checks for other datasets like baryon acoustic oscillations, calibration of high-redshift datasets such as gamma ray bursts, use as a model-independent mock catalog generator for future probes, etc.","Our analysis advocates for interesting yet cautious consideration of machine learning applications in these contexts."],"url":"http://arxiv.org/abs/2401.17029v1","category":"astro-ph.CO"}
{"created":"2024-01-30 14:02:49","title":"Heterogeneous treatment effect estimation with subpopulation identification for personalized medicine in opioid use disorder","abstract":"Deep learning models have demonstrated promising results in estimating treatment effects (TEE). However, most of them overlook the variations in treatment outcomes among subgroups with distinct characteristics. This limitation hinders their ability to provide accurate estimations and treatment recommendations for specific subgroups. In this study, we introduce a novel neural network-based framework, named SubgroupTE, which incorporates subgroup identification and treatment effect estimation. SubgroupTE identifies diverse subgroups and simultaneously estimates treatment effects for each subgroup, improving the treatment effect estimation by considering the heterogeneity of treatment responses. Comparative experiments on synthetic data show that SubgroupTE outperforms existing models in treatment effect estimation. Furthermore, experiments on a real-world dataset related to opioid use disorder (OUD) demonstrate the potential of our approach to enhance personalized treatment recommendations for OUD patients.","sentences":["Deep learning models have demonstrated promising results in estimating treatment effects (TEE).","However, most of them overlook the variations in treatment outcomes among subgroups with distinct characteristics.","This limitation hinders their ability to provide accurate estimations and treatment recommendations for specific subgroups.","In this study, we introduce a novel neural network-based framework, named SubgroupTE, which incorporates subgroup identification and treatment effect estimation.","SubgroupTE","identifies diverse subgroups and simultaneously estimates treatment effects for each subgroup, improving the treatment effect estimation by considering the heterogeneity of treatment responses.","Comparative experiments on synthetic data show that SubgroupTE outperforms existing models in treatment effect estimation.","Furthermore, experiments on a real-world dataset related to opioid use disorder (OUD) demonstrate the potential of our approach to enhance personalized treatment recommendations for OUD patients."],"url":"http://arxiv.org/abs/2401.17027v1","category":"cs.LG"}
{"created":"2024-01-30 14:01:30","title":"Static and Dynamic Synthesis of Bengali and Devanagari Signatures","abstract":"Developing an automatic signature verification system is challenging and demands a large number of training samples. This is why synthetic handwriting generation is an emerging topic in document image analysis. Some handwriting synthesizers use the motor equivalence model, the well-established hypothesis from neuroscience, which analyses how a human being accomplishes movement. Specifically, a motor equivalence model divides human actions into two steps: 1) the effector independent step at cognitive level and 2) the effector dependent step at motor level. In fact, recent work reports the successful application to Western scripts of a handwriting synthesizer, based on this theory. This paper aims to adapt this scheme for the generation of synthetic signatures in two Indic scripts, Bengali (Bangla), and Devanagari (Hindi). For this purpose, we use two different online and offline databases for both Bengali and Devanagari signatures. This paper reports an effective synthesizer for static and dynamic signatures written in Devanagari or Bengali scripts. We obtain promising results with artificially generated signatures in terms of appearance and performance when we compare the results with those for real signatures.","sentences":["Developing an automatic signature verification system is challenging and demands a large number of training samples.","This is why synthetic handwriting generation is an emerging topic in document image analysis.","Some handwriting synthesizers use the motor equivalence model, the well-established hypothesis from neuroscience, which analyses how a human being accomplishes movement.","Specifically, a motor equivalence model divides human actions into two steps: 1) the effector independent step at cognitive level and 2) the effector dependent step at motor level.","In fact, recent work reports the successful application to Western scripts of a handwriting synthesizer, based on this theory.","This paper aims to adapt this scheme for the generation of synthetic signatures in two Indic scripts, Bengali (Bangla), and Devanagari (Hindi).","For this purpose, we use two different online and offline databases for both Bengali and Devanagari signatures.","This paper reports an effective synthesizer for static and dynamic signatures written in Devanagari or Bengali scripts.","We obtain promising results with artificially generated signatures in terms of appearance and performance when we compare the results with those for real signatures."],"url":"http://arxiv.org/abs/2401.17026v1","category":"cs.CV"}
{"created":"2024-01-30 14:01:05","title":"Bayesian $\\mathcal{F}$-statistic-based parameter estimation of continuous gravitational waves from known pulsars","abstract":"We present a new method and implementation to obtain Bayesian posteriors on the amplitude parameters $\\{h_0, \\cos \\iota, \\psi, \\phi_0\\}$ of continuous-gravitational waves emitted by known pulsars. This approach leverages the well-established $\\mathcal{F}$-statistic framework and software. We further explore the benefits of employing a likelihood function that is analytically marginalized over $\\phi_0$, which avoids signal degeneracy problems in the $\\psi$-$\\phi_0$ subspace. The method is tested on simulated signals, hardware injections in Advanced-LIGO detector data, and by performing percentile-percentile (PP) self-consistency tests of the posteriors via Monte-Carlo simulations. We apply our methodology to PSR J1526-2744, a recently discovered millisecond pulsar. We find no evidence for a signal and obtain a Bayesian upper limit $h_0^{95\\%}$ on the gravitational-wave amplitude of approximately $7 \\times 10^{-27}$, consistent with a previous frequentist upper limit.","sentences":["We present a new method and implementation to obtain Bayesian posteriors on the amplitude parameters $\\{h_0, \\cos \\iota, \\psi, \\phi_0\\}$ of continuous-gravitational waves emitted by known pulsars.","This approach leverages the well-established $\\mathcal{F}$-statistic framework and software.","We further explore the benefits of employing a likelihood function that is analytically marginalized over $\\phi_0$, which avoids signal degeneracy problems in the $\\psi$-$\\phi_0$ subspace.","The method is tested on simulated signals, hardware injections in Advanced-LIGO detector data, and by performing percentile-percentile (PP) self-consistency tests of the posteriors via Monte-Carlo simulations.","We apply our methodology to PSR J1526-2744, a recently discovered millisecond pulsar.","We find no evidence for a signal and obtain a Bayesian upper limit $h_0^{95\\%}$ on the gravitational-wave amplitude of approximately $7 \\times 10^{-27}$, consistent with a previous frequentist upper limit."],"url":"http://arxiv.org/abs/2401.17025v1","category":"gr-qc"}
{"created":"2024-01-30 13:55:56","title":"MF-MOS: A Motion-Focused Model for Moving Object Segmentation","abstract":"Moving object segmentation (MOS) provides a reliable solution for detecting traffic participants and thus is of great interest in the autonomous driving field. Dynamic capture is always critical in the MOS problem. Previous methods capture motion features from the range images directly. Differently, we argue that the residual maps provide greater potential for motion information, while range images contain rich semantic guidance. Based on this intuition, we propose MF-MOS, a novel motion-focused model with a dual-branch structure for LiDAR moving object segmentation. Novelly, we decouple the spatial-temporal information by capturing the motion from residual maps and generating semantic features from range images, which are used as movable object guidance for the motion branch. Our straightforward yet distinctive solution can make the most use of both range images and residual maps, thus greatly improving the performance of the LiDAR-based MOS task. Remarkably, our MF-MOS achieved a leading IoU of 76.7% on the MOS leaderboard of the SemanticKITTI dataset upon submission, demonstrating the current state-of-the-art performance. The implementation of our MF-MOS has been released at https://github.com/SCNU-RISLAB/MF-MOS.","sentences":["Moving object segmentation (MOS) provides a reliable solution for detecting traffic participants and thus is of great interest in the autonomous driving field.","Dynamic capture is always critical in the MOS problem.","Previous methods capture motion features from the range images directly.","Differently, we argue that the residual maps provide greater potential for motion information, while range images contain rich semantic guidance.","Based on this intuition, we propose MF-MOS, a novel motion-focused model with a dual-branch structure for LiDAR moving object segmentation.","Novelly, we decouple the spatial-temporal information by capturing the motion from residual maps and generating semantic features from range images, which are used as movable object guidance for the motion branch.","Our straightforward yet distinctive solution can make the most use of both range images and residual maps, thus greatly improving the performance of the LiDAR-based MOS task.","Remarkably, our MF-MOS achieved a leading IoU of 76.7% on the MOS leaderboard of the SemanticKITTI dataset upon submission, demonstrating the current state-of-the-art performance.","The implementation of our MF-MOS has been released at https://github.com/SCNU-RISLAB/MF-MOS."],"url":"http://arxiv.org/abs/2401.17023v1","category":"cs.CV"}
{"created":"2024-01-30 13:55:41","title":"Realization of fractional quantum Hall state with interacting photons","abstract":"Fractional quantum Hall (FQH) states, known for their robust topological order and the emergence of non-Abelian anyons, have captured significant interest due to the appealing applications in fault-tolerant quantum computing. Bottom-up approach on an engineered quantum platform will provide opportunities to operate FQH states without external magnetic field and enhance local and coherent manipulation of these exotic states. Here we demonstrate a lattice version of photon FQH state using a programmable on-chip platform based on photon blockade and engineering gauge fields on a novel two-dimensional circuit quantum electrodynamics (QED) system. We first observe the effective photon Lorentz force and butterfly spectrum in the artificial gauge field, a prerequisite for FQH states. After adiabatic assembly of Laughlin FQH wavefunction of 1/2 filling factor from localized photons, we observe strong density correlation and chiral topological flow among the FQH photons. We then verify the unique features of FQH states in response to external fields, including the incompressibility of generating quasiparticles and the smoking-gun signature of fractional quantum Hall conductivity. Our work represents a significant advance in the bottom-up creation and manipulation of novel strongly correlated topological quantum matter composed of photons and opens up possibilities for fault-tolerant quantum information devices.","sentences":["Fractional quantum Hall (FQH) states, known for their robust topological order and the emergence of non-Abelian anyons, have captured significant interest due to the appealing applications in fault-tolerant quantum computing.","Bottom-up approach on an engineered quantum platform will provide opportunities to operate FQH states without external magnetic field and enhance local and coherent manipulation of these exotic states.","Here we demonstrate a lattice version of photon FQH state using a programmable on-chip platform based on photon blockade and engineering gauge fields on a novel two-dimensional circuit quantum electrodynamics (QED) system.","We first observe the effective photon Lorentz force and butterfly spectrum in the artificial gauge field, a prerequisite for FQH states.","After adiabatic assembly of Laughlin FQH wavefunction of 1/2 filling factor from localized photons, we observe strong density correlation and chiral topological flow among the FQH photons.","We then verify the unique features of FQH states in response to external fields, including the incompressibility of generating quasiparticles and the smoking-gun signature of fractional quantum Hall conductivity.","Our work represents a significant advance in the bottom-up creation and manipulation of novel strongly correlated topological quantum matter composed of photons and opens up possibilities for fault-tolerant quantum information devices."],"url":"http://arxiv.org/abs/2401.17022v1","category":"quant-ph"}
{"created":"2024-01-30 13:52:47","title":"Towards Generating Executable Metamorphic Relations Using Large Language Models","abstract":"Metamorphic testing (MT) has proven to be a successful solution to automating testing and addressing the oracle problem. However, it entails manually deriving metamorphic relations (MRs) and converting them into an executable form; these steps are time-consuming and may prevent the adoption of MT. In this paper, we propose an approach for automatically deriving executable MRs (EMRs) from requirements using large language models (LLMs). Instead of merely asking the LLM to produce EMRs, our approach relies on a few-shot prompting strategy to instruct the LLM to perform activities in the MT process, by providing requirements and API specifications, as one would do with software engineers. To assess the feasibility of our approach, we conducted a questionnaire-based survey in collaboration with Siemens Industry Software, focusing on four of their software applications. Additionally, we evaluated the accuracy of the generated EMRs for a web application. The outcomes of our study are highly promising, as they demonstrate the capability of our approach to generate MRs and EMRs that are both comprehensible and pertinent for testing purposes.","sentences":["Metamorphic testing (MT) has proven to be a successful solution to automating testing and addressing the oracle problem.","However, it entails manually deriving metamorphic relations (MRs) and converting them into an executable form; these steps are time-consuming and may prevent the adoption of MT.","In this paper, we propose an approach for automatically deriving executable MRs (EMRs) from requirements using large language models (LLMs).","Instead of merely asking the LLM to produce EMRs, our approach relies on a few-shot prompting strategy to instruct the LLM to perform activities in the MT process, by providing requirements and API specifications, as one would do with software engineers.","To assess the feasibility of our approach, we conducted a questionnaire-based survey in collaboration with Siemens Industry Software, focusing on four of their software applications.","Additionally, we evaluated the accuracy of the generated EMRs for a web application.","The outcomes of our study are highly promising, as they demonstrate the capability of our approach to generate MRs and EMRs that are both comprehensible and pertinent for testing purposes."],"url":"http://arxiv.org/abs/2401.17019v1","category":"cs.SE"}
{"created":"2024-01-30 13:52:40","title":"GPU-Accelerated Batch-Dynamic Subgraph Matching","abstract":"Subgraph matching has garnered increasing attention for its diverse real-world applications. Given the dynamic nature of real-world graphs, addressing evolving scenarios without incurring prohibitive overheads has been a focus of research. However, existing approaches for dynamic subgraph matching often proceed serially, retrieving incremental matches for each updated edge individually. This approach falls short when handling batch data updates, leading to a decrease in system throughput. Leveraging the parallel processing power of GPUs, which can execute a massive number of cores simultaneously, has been widely recognized for performance acceleration in various domains. Surprisingly, systematic exploration of subgraph matching in the context of batch-dynamic graphs, particularly on a GPU platform, remains untouched. In this paper, we bridge this gap by introducing an efficient framework, GAMMA (GPU-Accelerated Batch-Dynamic Subgraph Matching). Our approach features a DFS-based warp-centric batch-dynamic subgraph matching algorithm. To ensure load balance in the DFS-based search, we propose warp-level work stealing via shared memory. Additionally, we introduce coalesced search to reduce redundant computations. Comprehensive experiments demonstrate the superior performance of GAMMA. Compared to state-of-the-art algorithms, GAMMA showcases a performance improvement up to hundreds of times.","sentences":["Subgraph matching has garnered increasing attention for its diverse real-world applications.","Given the dynamic nature of real-world graphs, addressing evolving scenarios without incurring prohibitive overheads has been a focus of research.","However, existing approaches for dynamic subgraph matching often proceed serially, retrieving incremental matches for each updated edge individually.","This approach falls short when handling batch data updates, leading to a decrease in system throughput.","Leveraging the parallel processing power of GPUs, which can execute a massive number of cores simultaneously, has been widely recognized for performance acceleration in various domains.","Surprisingly, systematic exploration of subgraph matching in the context of batch-dynamic graphs, particularly on a GPU platform, remains untouched.","In this paper, we bridge this gap by introducing an efficient framework, GAMMA (GPU-Accelerated Batch-Dynamic Subgraph Matching).","Our approach features a DFS-based warp-centric batch-dynamic subgraph matching algorithm.","To ensure load balance in the DFS-based search, we propose warp-level work stealing via shared memory.","Additionally, we introduce coalesced search to reduce redundant computations.","Comprehensive experiments demonstrate the superior performance of GAMMA.","Compared to state-of-the-art algorithms, GAMMA showcases a performance improvement up to hundreds of times."],"url":"http://arxiv.org/abs/2401.17018v1","category":"cs.DC"}
{"created":"2024-01-30 13:52:16","title":"Bonnet-Myers rigidity theorem for globally hyperbolic Lorentzian length spaces","abstract":"We prove a synthetic Bonnet-Myers rigidity theorem for globally hyperbolic Lorentzian length spaces with global curvature bounded below by $K<0$ and an open distance realizer of length $L=\\frac{\\pi}{\\sqrt{|K|}}$. In the course of the proof, we show that the space necessarily is a warped product with warping function $\\cos:(-\\frac{\\pi}{2},\\frac{\\pi}{2})\\to\\mathbb{R}_+$.","sentences":["We prove a synthetic Bonnet-Myers rigidity theorem for globally hyperbolic Lorentzian length spaces with global curvature bounded below by $K<0$ and an open distance realizer of length $L=\\frac{\\pi}{\\sqrt{|K|}}$. In the course of the proof, we show that the space necessarily is a warped product with warping function $\\cos:(-\\frac{\\pi}{2},\\frac{\\pi}{2})\\to\\mathbb{R}_+$."],"url":"http://arxiv.org/abs/2401.17017v1","category":"math.DG"}
{"created":"2024-01-30 13:51:28","title":"DeepH-2: Enhancing deep-learning electronic structure via an equivariant local-coordinate transformer","abstract":"Deep-learning electronic structure calculations show great potential for revolutionizing the landscape of computational materials research. However, current neural-network architectures are not deemed suitable for widespread general-purpose application. Here we introduce a framework of equivariant local-coordinate transformer, designed to enhance the deep-learning density functional theory Hamiltonian referred to as DeepH-2. Unlike previous models such as DeepH and DeepH-E3, DeepH-2 seamlessly integrates the simplicity of local-coordinate transformations and the mathematical elegance of equivariant neural networks, effectively overcoming their respective disadvantages. Based on our comprehensive experiments, DeepH-2 demonstrates superiority over its predecessors in both efficiency and accuracy, showcasing state-of-the-art performance. This advancement opens up opportunities for exploring universal neural network models or even large materials models.","sentences":["Deep-learning electronic structure calculations show great potential for revolutionizing the landscape of computational materials research.","However, current neural-network architectures are not deemed suitable for widespread general-purpose application.","Here we introduce a framework of equivariant local-coordinate transformer, designed to enhance the deep-learning density functional theory Hamiltonian referred to as DeepH-2.","Unlike previous models such as DeepH and DeepH-E3, DeepH-2 seamlessly integrates the simplicity of local-coordinate transformations and the mathematical elegance of equivariant neural networks, effectively overcoming their respective disadvantages.","Based on our comprehensive experiments, DeepH-2 demonstrates superiority over its predecessors in both efficiency and accuracy, showcasing state-of-the-art performance.","This advancement opens up opportunities for exploring universal neural network models or even large materials models."],"url":"http://arxiv.org/abs/2401.17015v1","category":"physics.comp-ph"}
{"created":"2024-01-30 13:50:09","title":"Near-Field Fading Channel Modeling for ELAAs: From Communication to ISAC","abstract":"Extremely large aperture array (ELAA) is anticipated to serve as a pivotal feature of future multiple-input multiple-output (MIMO) systems in 6G. Near-field (NF) fading channel models are essential for reliable link-level simulation and ELAA system design. In this article, we propose a framework designed to generate NF fading channels for both communication and integrated sensing and communication (ISAC) applications. The framework allows a mixed of line of sight (LoS) and non-LoS (NLoS) links. It also considers spherical wave model and spatially non-stationary shadow fading. Based on this framework, we propose a three-dimensional (3D) fading channel model for ELAA systems deployed with a uniform rectangular array (URA). It can capture the impact of sensing object for ISAC applications. Moreover, all parameters involved in the framework are based on specifications or measurements from the 3rd Generation Partnership Project (3GPP) documents. Therefore, the proposed framework and channel model have the potential to contribute to the standard in various aspects, including ISAC, extra-large (XL-) MIMO, and reconfigurable intelligent surface (RIS) aided MIMO systems. Finally, future directions for ELAA are presented, including not only NF channel modeling but also the design of next-generation transceivers.","sentences":["Extremely large aperture array (ELAA) is anticipated to serve as a pivotal feature of future multiple-input multiple-output (MIMO) systems in 6G. Near-field (NF) fading channel models are essential for reliable link-level simulation and ELAA system design.","In this article, we propose a framework designed to generate NF fading channels for both communication and integrated sensing and communication (ISAC) applications.","The framework allows a mixed of line of sight (LoS) and non-LoS (NLoS) links.","It also considers spherical wave model and spatially non-stationary shadow fading.","Based on this framework, we propose a three-dimensional (3D) fading channel model for ELAA systems deployed with a uniform rectangular array (URA).","It can capture the impact of sensing object for ISAC applications.","Moreover, all parameters involved in the framework are based on specifications or measurements from the 3rd Generation Partnership Project (3GPP) documents.","Therefore, the proposed framework and channel model have the potential to contribute to the standard in various aspects, including ISAC, extra-large (XL-) MIMO, and reconfigurable intelligent surface (RIS) aided MIMO systems.","Finally, future directions for ELAA are presented, including not only NF channel modeling but also the design of next-generation transceivers."],"url":"http://arxiv.org/abs/2401.17014v1","category":"cs.IT"}
{"created":"2024-01-30 13:49:03","title":"Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets","abstract":"Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.   This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.","sentences":["Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications.","Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.   ","This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score.","The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets.","The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception."],"url":"http://arxiv.org/abs/2401.17013v1","category":"cs.LG"}
{"created":"2024-01-30 13:47:46","title":"On the Algorithmic Verification of Nonlinear Superposition for Systems of First Order Ordinary Differential Equations","abstract":"This paper belongs to a group of work in the intersection of symbolic computation and group analysis aiming for the symbolic analysis of differential equations. The goal is to extract important properties without finding the explicit general solution. In this contribution, we introduce the algorithmic verification of nonlinear superposition properties and its implementation. More exactly, for a system of nonlinear ordinary differential equations of first order with a polynomial right-hand side, we check if the differential system admits a general solution by means of a superposition rule and a certain number of particular solutions. It is based on the theory of Newton polytopes and associated symbolic computation. The developed method provides the basis for the identification of nonlinear superpositions within a given system and for the construction of numerical methods which preserve important algebraic properties at the numerical level.","sentences":["This paper belongs to a group of work in the intersection of symbolic computation and group analysis aiming for the symbolic analysis of differential equations.","The goal is to extract important properties without finding the explicit general solution.","In this contribution, we introduce the algorithmic verification of nonlinear superposition properties and its implementation.","More exactly, for a system of nonlinear ordinary differential equations of first order with a polynomial right-hand side, we check if the differential system admits a general solution by means of a superposition rule and a certain number of particular solutions.","It is based on the theory of Newton polytopes and associated symbolic computation.","The developed method provides the basis for the identification of nonlinear superpositions within a given system and for the construction of numerical methods which preserve important algebraic properties at the numerical level."],"url":"http://arxiv.org/abs/2401.17012v1","category":"cs.SC"}
{"created":"2024-01-30 13:47:15","title":"Age of Actuated Information and Age of Actuation in a Data-Caching Energy Harvesting Actuator","abstract":"In this paper, we introduce two metrics, namely, age of actuation (AoA) and age of actuated information (AoAI), within a discrete-time system model that integrates data caching and energy harvesting (EH). AoA evaluates the timeliness of actions irrespective of the age of the information, while AoAI considers the freshness of the utilized data packet. We use Markov Chain analysis to model the system's evolution. Furthermore, we employ three-dimensional Markov Chain analysis to characterize the stationary distributions for AoA and AoAI and calculate their average values. Our findings from the analysis, validated by simulations, show that while AoAI consistently decreases with increased data and energy packet arrival rates, AoA presents a more complex behavior, with potential increases under conditions of limited data or energy resources. These metrics go towards the semantics of information and goal-oriented communications since they consider the timeliness of utilizing the information to perform an action.","sentences":["In this paper, we introduce two metrics, namely, age of actuation (AoA) and age of actuated information (AoAI), within a discrete-time system model that integrates data caching and energy harvesting (EH).","AoA evaluates the timeliness of actions irrespective of the age of the information, while AoAI considers the freshness of the utilized data packet.","We use Markov Chain analysis to model the system's evolution.","Furthermore, we employ three-dimensional Markov Chain analysis to characterize the stationary distributions for AoA and AoAI and calculate their average values.","Our findings from the analysis, validated by simulations, show that while AoAI consistently decreases with increased data and energy packet arrival rates, AoA presents a more complex behavior, with potential increases under conditions of limited data or energy resources.","These metrics go towards the semantics of information and goal-oriented communications since they consider the timeliness of utilizing the information to perform an action."],"url":"http://arxiv.org/abs/2401.17011v1","category":"cs.IT"}
{"created":"2024-01-30 13:46:49","title":"Finetuning Large Language Models for Vulnerability Detection","abstract":"This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, handling class imbalance, and improving performance on difficult vulnerability detection datasets. This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks.","sentences":["This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code.","We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning.","To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes.","For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance.","The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code.","The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, handling class imbalance, and improving performance on difficult vulnerability detection datasets.","This demonstrates the potential for transfer learning by finetuning large pretrained language models for specialized source code analysis tasks."],"url":"http://arxiv.org/abs/2401.17010v1","category":"cs.CR"}
{"created":"2024-01-30 13:45:39","title":"Quantum Transfer Learning with Adversarial Robustness for Classification of High-Resolution Image Datasets","abstract":"The application of quantum machine learning to large-scale high-resolution image datasets is not yet possible due to the limited number of qubits and relatively high level of noise in the current generation of quantum devices. In this work, we address this challenge by proposing a quantum transfer learning (QTL) architecture that integrates quantum variational circuits with a classical machine learning network pre-trained on ImageNet dataset. Through a systematic set of simulations over a variety of image datasets such as Ants & Bees, CIFAR-10, and Road Sign Detection, we demonstrate the superior performance of our QTL approach over classical and quantum machine learning without involving transfer learning. Furthermore, we evaluate the adversarial robustness of QTL architecture with and without adversarial training, confirming that our QTL method is adversarially robust against data manipulation attacks and outperforms classical methods.","sentences":["The application of quantum machine learning to large-scale high-resolution image datasets is not yet possible due to the limited number of qubits and relatively high level of noise in the current generation of quantum devices.","In this work, we address this challenge by proposing a quantum transfer learning (QTL) architecture that integrates quantum variational circuits with a classical machine learning network pre-trained on ImageNet dataset.","Through a systematic set of simulations over a variety of image datasets such as Ants & Bees, CIFAR-10, and Road Sign Detection, we demonstrate the superior performance of our QTL approach over classical and quantum machine learning without involving transfer learning.","Furthermore, we evaluate the adversarial robustness of QTL architecture with and without adversarial training, confirming that our QTL method is adversarially robust against data manipulation attacks and outperforms classical methods."],"url":"http://arxiv.org/abs/2401.17009v1","category":"quant-ph"}
