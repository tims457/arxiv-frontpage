{"created":"2024-05-24 17:59:26","title":"FastDrag: Manipulate Anything in One Step","abstract":"Drag-based image editing using generative models provides precise control over image contents, enabling users to manipulate anything in an image with a few clicks. However, prevailing methods typically adopt $n$-step iterations for latent semantic optimization to achieve drag-based image editing, which is time-consuming and limits practical applications. In this paper, we introduce a novel one-step drag-based image editing method, i.e., FastDrag, to accelerate the editing process. Central to our approach is a latent warpage function (LWF), which simulates the behavior of a stretched material to adjust the location of individual pixels within the latent space. This innovation achieves one-step latent semantic optimization and hence significantly promotes editing speeds. Meanwhile, null regions emerging after applying LWF are addressed by our proposed bilateral nearest neighbor interpolation (BNNI) strategy. This strategy interpolates these regions using similar features from neighboring areas, thus enhancing semantic integrity. Additionally, a consistency-preserving strategy is introduced to maintain the consistency between the edited and original images by adopting semantic information from the original image, saved as key and value pairs in self-attention module during diffusion inversion, to guide the diffusion sampling. Our FastDrag is validated on the DragBench dataset, demonstrating substantial improvements in processing time over existing methods, while achieving enhanced editing performance.","sentences":["Drag-based image editing using generative models provides precise control over image contents, enabling users to manipulate anything in an image with a few clicks.","However, prevailing methods typically adopt $n$-step iterations for latent semantic optimization to achieve drag-based image editing, which is time-consuming and limits practical applications.","In this paper, we introduce a novel one-step drag-based image editing method, i.e., FastDrag, to accelerate the editing process.","Central to our approach is a latent warpage function (LWF), which simulates the behavior of a stretched material to adjust the location of individual pixels within the latent space.","This innovation achieves one-step latent semantic optimization and hence significantly promotes editing speeds.","Meanwhile, null regions emerging after applying LWF are addressed by our proposed bilateral nearest neighbor interpolation (BNNI) strategy.","This strategy interpolates these regions using similar features from neighboring areas, thus enhancing semantic integrity.","Additionally, a consistency-preserving strategy is introduced to maintain the consistency between the edited and original images by adopting semantic information from the original image, saved as key and value pairs in self-attention module during diffusion inversion, to guide the diffusion sampling.","Our FastDrag is validated on the DragBench dataset, demonstrating substantial improvements in processing time over existing methods, while achieving enhanced editing performance."],"url":"http://arxiv.org/abs/2405.15769v1","category":"cs.CV"}
{"created":"2024-05-24 17:59:21","title":"Canonical Variates in Wasserstein Metric Space","abstract":"In this paper, we address the classification of instances each characterized not by a singular point, but by a distribution on a vector space. We employ the Wasserstein metric to measure distances between distributions, which are then used by distance-based classification algorithms such as k-nearest neighbors, k-means, and pseudo-mixture modeling. Central to our investigation is dimension reduction within the Wasserstein metric space to enhance classification accuracy. We introduce a novel approach grounded in the principle of maximizing Fisher's ratio, defined as the quotient of between-class variation to within-class variation. The directions in which this ratio is maximized are termed discriminant coordinates or canonical variates axes. In practice, we define both between-class and within-class variations as the average squared distances between pairs of instances, with the pairs either belonging to the same class or to different classes. This ratio optimization is achieved through an iterative algorithm, which alternates between optimal transport and maximization steps within the vector space. We conduct empirical studies to assess the algorithm's convergence and, through experimental validation, demonstrate that our dimension reduction technique substantially enhances classification performance. Moreover, our method outperforms well-established algorithms that operate on vector representations derived from distributional data. It also exhibits robustness against variations in the distributional representations of data clouds.","sentences":["In this paper, we address the classification of instances each characterized not by a singular point, but by a distribution on a vector space.","We employ the Wasserstein metric to measure distances between distributions, which are then used by distance-based classification algorithms such as k-nearest neighbors, k-means, and pseudo-mixture modeling.","Central to our investigation is dimension reduction within the Wasserstein metric space to enhance classification accuracy.","We introduce a novel approach grounded in the principle of maximizing Fisher's ratio, defined as the quotient of between-class variation to within-class variation.","The directions in which this ratio is maximized are termed discriminant coordinates or canonical variates axes.","In practice, we define both between-class and within-class variations as the average squared distances between pairs of instances, with the pairs either belonging to the same class or to different classes.","This ratio optimization is achieved through an iterative algorithm, which alternates between optimal transport and maximization steps within the vector space.","We conduct empirical studies to assess the algorithm's convergence and, through experimental validation, demonstrate that our dimension reduction technique substantially enhances classification performance.","Moreover, our method outperforms well-established algorithms that operate on vector representations derived from distributional data.","It also exhibits robustness against variations in the distributional representations of data clouds."],"url":"http://arxiv.org/abs/2405.15768v1","category":"stat.ML"}
{"created":"2024-05-24 17:58:42","title":"Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development","abstract":"The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making. Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information. With the exponential increase in data sources like social media content, biomedical literature, and Electronic Medical Records (EMR), extracting relevant ADE-related information from these unstructured texts is imperative. Previous ADE mining studies have focused on text-based methodologies, overlooking visual cues, limiting contextual comprehension, and hindering accurate interpretation. To address this gap, we present a MultiModal Adverse Drug Event (MMADE) detection dataset, merging ADE-related textual information with visual aids. Additionally, we introduce a framework that leverages the capabilities of LLMs and VLMs for ADE detection by generating detailed descriptions of medical images depicting ADEs, aiding healthcare professionals in visually identifying adverse events. Using our MMADE dataset, we showcase the significance of integrating visual cues from images to enhance overall performance. This approach holds promise for patient safety, ADE awareness, and healthcare accessibility, paving the way for further exploration in personalized healthcare.","sentences":["The mining of adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making.","Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information.","With the exponential increase in data sources like social media content, biomedical literature, and Electronic Medical Records (EMR), extracting relevant ADE-related information from these unstructured texts is imperative.","Previous ADE mining studies have focused on text-based methodologies, overlooking visual cues, limiting contextual comprehension, and hindering accurate interpretation.","To address this gap, we present a MultiModal Adverse Drug Event (MMADE) detection dataset, merging ADE-related textual information with visual aids.","Additionally, we introduce a framework that leverages the capabilities of LLMs and VLMs for ADE detection by generating detailed descriptions of medical images depicting ADEs, aiding healthcare professionals in visually identifying adverse events.","Using our MMADE dataset, we showcase the significance of integrating visual cues from images to enhance overall performance.","This approach holds promise for patient safety, ADE awareness, and healthcare accessibility, paving the way for further exploration in personalized healthcare."],"url":"http://arxiv.org/abs/2405.15766v1","category":"cs.AI"}
{"created":"2024-05-24 17:58:38","title":"Scaling Laws for Discriminative Classification in Large Language Models","abstract":"Modern large language models (LLMs) represent a paradigm shift in what can plausibly be expected of machine learning models. The fact that LLMs can effectively generate sensible answers to a diverse range of queries suggests that they would be useful in customer support applications. While powerful, LLMs have been observed to be prone to hallucination which unfortunately makes their near term use in customer support applications challenging. To address this issue we present a system that allows us to use an LLM to augment our customer support advocates by re-framing the language modeling task as a discriminative classification task. In this framing, we seek to present the top-K best template responses for a customer support advocate to use when responding to a customer. We present the result of both offline and online experiments where we observed offline gains and statistically significant online lifts for our experimental system. Along the way, we present observed scaling curves for validation loss and top-K accuracy, resulted from model parameter ablation studies. We close by discussing the space of trade-offs with respect to model size, latency, and accuracy as well as and suggesting future applications to explore.","sentences":["Modern large language models (LLMs) represent a paradigm shift in what can plausibly be expected of machine learning models.","The fact that LLMs can effectively generate sensible answers to a diverse range of queries suggests that they would be useful in customer support applications.","While powerful, LLMs have been observed to be prone to hallucination which unfortunately makes their near term use in customer support applications challenging.","To address this issue we present a system that allows us to use an LLM to augment our customer support advocates by re-framing the language modeling task as a discriminative classification task.","In this framing, we seek to present the top-K best template responses for a customer support advocate to use when responding to a customer.","We present the result of both offline and online experiments where we observed offline gains and statistically significant online lifts for our experimental system.","Along the way, we present observed scaling curves for validation loss and top-K accuracy, resulted from model parameter ablation studies.","We close by discussing the space of trade-offs with respect to model size, latency, and accuracy as well as and suggesting future applications to explore."],"url":"http://arxiv.org/abs/2405.15765v1","category":"cs.CL"}
{"created":"2024-05-24 17:58:19","title":"Self-sensing with hollow cylindrical transducers for histotripsy enhanced aspiration mechanical thrombectomy applications","abstract":"To address existing challenges with intravascular mechanical thrombectomy devices, a novel ultrasound-enhanced aspiration approach is being developed to mechanically degrade clots using cavitation. This method employs standing waves within a mm-scale hollow cylindrical transducer to generate high pressures sufficient to perform histotripsy on clots situated within the transducer lumen and generate substantial lesions. The objective of this study is to assess the feasibility of self-sensing cavitation detection by analyzing voltage signals across the transducer during treatment pulses. External ultrasound imaging of the transducer lumen validated cavitation detection. Impedance was also altered by the presence of clot material within the lumen. Experiments varying the driving voltage in water-filled lumens demonstrated changes in the relative amplitudes of the envelopes of the pulse body and ringdown portions of the voltage signals above the cavitation threshold, as well as changes in the spectral domain. In particular both broadband and ultraharmonic signals showed an increase in amplitude above the cavitation threshold. Similar temporal and spectral voltage signal changes in the presence of cavitation were also observed when treating clots within the lumen. This work demonstrates a highly sensitive method for detecting cavitation within the lumen, enabling monitoring with readily acquired signals without additional sensors in the catheter configuration. These findings hold significant potential for improving the efficacy of ultrasound-enhanced aspiration thrombectomy procedures.","sentences":["To address existing challenges with intravascular mechanical thrombectomy devices, a novel ultrasound-enhanced aspiration approach is being developed to mechanically degrade clots using cavitation.","This method employs standing waves within a mm-scale hollow cylindrical transducer to generate high pressures sufficient to perform histotripsy on clots situated within the transducer lumen and generate substantial lesions.","The objective of this study is to assess the feasibility of self-sensing cavitation detection by analyzing voltage signals across the transducer during treatment pulses.","External ultrasound imaging of the transducer lumen validated cavitation detection.","Impedance was also altered by the presence of clot material within the lumen.","Experiments varying the driving voltage in water-filled lumens demonstrated changes in the relative amplitudes of the envelopes of the pulse body and ringdown portions of the voltage signals above the cavitation threshold, as well as changes in the spectral domain.","In particular both broadband and ultraharmonic signals showed an increase in amplitude above the cavitation threshold.","Similar temporal and spectral voltage signal changes in the presence of cavitation were also observed when treating clots within the lumen.","This work demonstrates a highly sensitive method for detecting cavitation within the lumen, enabling monitoring with readily acquired signals without additional sensors in the catheter configuration.","These findings hold significant potential for improving the efficacy of ultrasound-enhanced aspiration thrombectomy procedures."],"url":"http://arxiv.org/abs/2405.15764v1","category":"physics.med-ph"}
{"created":"2024-05-24 17:57:57","title":"FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis","abstract":"Text-to-motion synthesis is a crucial task in computer vision. Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals. To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution. Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis. Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion. Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously.","sentences":["Text-to-motion synthesis is a crucial task in computer vision.","Existing methods are limited in their universality, as they are tailored for single-person or two-person scenarios and can not be applied to generate motions for more individuals.","To achieve the number-free motion synthesis, this paper reconsiders motion generation and proposes to unify the single and multi-person motion by the conditional motion distribution.","Furthermore, a generation module and an interaction module are designed for our FreeMotion framework to decouple the process of conditional motion generation and finally support the number-free motion synthesis.","Besides, based on our framework, the current single-person motion spatial control method could be seamlessly integrated, achieving precise control of multi-person motion.","Extensive experiments demonstrate the superior performance of our method and our capability to infer single and multi-human motions simultaneously."],"url":"http://arxiv.org/abs/2405.15763v1","category":"cs.CV"}
{"created":"2024-05-24 17:53:54","title":"InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation","abstract":"Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars. Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time. Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness. Our project page is https://wangyuchi369.github.io/InstructAvatar/.","sentences":["Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable.","In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video.","Our framework, named InstructAvatar, leverages a natural language interface to control the emotion as well as the facial motion of avatars.","Technically, we design an automatic annotation pipeline to construct an instruction-video paired training dataset, equipped with a novel two-branch diffusion-based generator to predict avatars with audio and text instructions at the same time.","Experimental results demonstrate that InstructAvatar produces results that align well with both conditions, and outperforms existing methods in fine-grained emotion control, lip-sync quality, and naturalness.","Our project page is https://wangyuchi369.github.io/InstructAvatar/."],"url":"http://arxiv.org/abs/2405.15758v1","category":"cs.CV"}
{"created":"2024-05-24 17:51:39","title":"Sparse Expansion and Neuronal Disentanglement","abstract":"We show how to improve the inference efficiency of an LLM by expanding it into a mixture of sparse experts, where each expert is a copy of the original weights, one-shot pruned for a specific cluster of input values. We call this approach $\\textit{Sparse Expansion}$. We show that, for models such as Llama 2 70B, as we increase the number of sparse experts, Sparse Expansion outperforms all other one-shot sparsification approaches for the same inference FLOP budget per token, and that this gap grows as sparsity increases, leading to inference speedups.   But why? To answer this, we provide strong evidence that the mixture of sparse experts is effectively $\\textit{disentangling}$ the input-output relationship of every individual neuron across clusters of inputs. Specifically, sparse experts approximate the dense neuron output distribution with fewer weights by decomposing the distribution into a collection of simpler ones, each with a separate sparse dot product covering it. Interestingly, we show that the Wasserstein distance between a neuron's output distribution and a Gaussian distribution is an indicator of its entanglement level and contribution to the accuracy of the model. Every layer of an LLM has a fraction of highly entangled Wasserstein neurons, and model performance suffers more when these are sparsified as opposed to others.","sentences":["We show how to improve the inference efficiency of an LLM by expanding it into a mixture of sparse experts, where each expert is a copy of the original weights, one-shot pruned for a specific cluster of input values.","We call this approach $\\textit{Sparse Expansion}$. We show that, for models such as Llama 2 70B, as we increase the number of sparse experts, Sparse Expansion outperforms all other one-shot sparsification approaches for the same inference FLOP budget per token, and that this gap grows as sparsity increases, leading to inference speedups.   ","But why?","To answer this, we provide strong evidence that the mixture of sparse experts is effectively $\\textit{disentangling}$ the input-output relationship of every individual neuron across clusters of inputs.","Specifically, sparse experts approximate the dense neuron output distribution with fewer weights by decomposing the distribution into a collection of simpler ones, each with a separate sparse dot product covering it.","Interestingly, we show that the Wasserstein distance between a neuron's output distribution and a Gaussian distribution is an indicator of its entanglement level and contribution to the accuracy of the model.","Every layer of an LLM has a fraction of highly entangled Wasserstein neurons, and model performance suffers more when these are sparsified as opposed to others."],"url":"http://arxiv.org/abs/2405.15756v1","category":"cs.LG"}
{"created":"2024-05-24 17:50:17","title":"Score-based generative models are provably robust: an uncertainty quantification perspective","abstract":"Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation. Our primary tool is the Wasserstein uncertainty propagation (WUP) theorem, a model-form UQ bound that describes how the $L^2$ error from learning the score function propagates to a Wasserstein-1 ($\\mathbf{d}_1$) ball around the true data distribution under the evolution of the Fokker-Planck equation. We show how errors due to (a) finite sample approximation, (b) early stopping, (c) score-matching objective choice, (d) score function parametrization expressiveness, and (e) reference distribution choice, impact the quality of the generative model in terms of a $\\mathbf{d}_1$ bound of computable quantities. The WUP theorem relies on Bernstein estimates for Hamilton-Jacobi-Bellman partial differential equations (PDE) and the regularizing properties of diffusion processes. Specifically, PDE regularity theory shows that stochasticity is the key mechanism ensuring SGM algorithms are provably robust. The WUP theorem applies to integral probability metrics beyond $\\mathbf{d}_1$, such as the total variation distance and the maximum mean discrepancy. Sample complexity and generalization bounds in $\\mathbf{d}_1$ follow directly from the WUP theorem. Our approach requires minimal assumptions, is agnostic to the manifold hypothesis and avoids absolute continuity assumptions for the target distribution. Additionally, our results clarify the trade-offs among multiple error sources in SGMs.","sentences":["Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation.","Our primary tool is the Wasserstein uncertainty propagation (WUP) theorem, a model-form UQ bound that describes how the $L^2$ error from learning the score function propagates to a Wasserstein-1 ($\\mathbf{d}_1$) ball around the true data distribution under the evolution of the Fokker-Planck equation.","We show how errors due to (a) finite sample approximation, (b) early stopping, (c) score-matching objective choice, (d) score function parametrization expressiveness, and (e) reference distribution choice, impact the quality of the generative model in terms of a $\\mathbf{d}_1$ bound of computable quantities.","The WUP theorem relies on Bernstein estimates for Hamilton-Jacobi-Bellman partial differential equations (PDE) and the regularizing properties of diffusion processes.","Specifically, PDE regularity theory shows that stochasticity is the key mechanism ensuring SGM algorithms are provably robust.","The WUP theorem applies to integral probability metrics beyond $\\mathbf{d}_1$, such as the total variation distance and the maximum mean discrepancy.","Sample complexity and generalization bounds in $\\mathbf{d}_1$ follow directly from the WUP theorem.","Our approach requires minimal assumptions, is agnostic to the manifold hypothesis and avoids absolute continuity assumptions for the target distribution.","Additionally, our results clarify the trade-offs among multiple error sources in SGMs."],"url":"http://arxiv.org/abs/2405.15754v1","category":"stat.ML"}
{"created":"2024-05-24 17:48:49","title":"Analysis of Marketed versus Not-marketed Mobile App Releases","abstract":"Market and user characteristics of mobile apps make their release management different from proprietary software products and web services. Despite the wealth of information regarding users' feedback on an app, an in-depth analysis of app releases is difficult due to the inconsistency and uncertainty of the information. To better understand and potentially improve app release processes, we analyze major, minor, and patch releases for releases following semantic versioning. In particular, we were interested in finding out the difference between marketed and not-marketed releases. Our results show that, in general, major, minor, and patch releases have significant differences in the release cycle duration, nature, and change velocity. We also observed that there is a significant difference between marketed and non-marketed mobile app releases in terms of cycle duration, nature and the extent of changes, and the number of opened and closed issues.","sentences":["Market and user characteristics of mobile apps make their release management different from proprietary software products and web services.","Despite the wealth of information regarding users' feedback on an app, an in-depth analysis of app releases is difficult due to the inconsistency and uncertainty of the information.","To better understand and potentially improve app release processes, we analyze major, minor, and patch releases for releases following semantic versioning.","In particular, we were interested in finding out the difference between marketed and not-marketed releases.","Our results show that, in general, major, minor, and patch releases have significant differences in the release cycle duration, nature, and change velocity.","We also observed that there is a significant difference between marketed and non-marketed mobile app releases in terms of cycle duration, nature and the extent of changes, and the number of opened and closed issues."],"url":"http://arxiv.org/abs/2405.15752v1","category":"cs.SE"}
{"created":"2024-05-24 17:48:48","title":"A review of active matter reviews","abstract":"In the past years, the amount of research on active matter has grown extremely rapidly, a fact that is reflected in particular by the existence of more than 600 review articles on this topic. Moreover, the field has become very diverse, ranging from theoretical studies of the statistical mechanics of active particles to applied work on medical applications of microrobots and from biological systems to artificial swimmers. This makes it very difficult to get an overview over the field as a whole. Here, we provide such an overview in the form of a metareview article that surveys the existing review articles on active matter. Thereby, this article provides an introduction to the various subdisciplines of active matter science and constitutes a useful starting point for finding literature about a specific topic.","sentences":["In the past years, the amount of research on active matter has grown extremely rapidly, a fact that is reflected in particular by the existence of more than 600 review articles on this topic.","Moreover, the field has become very diverse, ranging from theoretical studies of the statistical mechanics of active particles to applied work on medical applications of microrobots and from biological systems to artificial swimmers.","This makes it very difficult to get an overview over the field as a whole.","Here, we provide such an overview in the form of a metareview article that surveys the existing review articles on active matter.","Thereby, this article provides an introduction to the various subdisciplines of active matter science and constitutes a useful starting point for finding literature about a specific topic."],"url":"http://arxiv.org/abs/2405.15751v1","category":"cond-mat.soft"}
{"created":"2024-05-24 17:47:20","title":"Filtered Corpus Training (FiCT) Shows that Language Models can Generalize from Indirect Evidence","abstract":"This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence. We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena. Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence.","sentences":["This paper introduces Filtered Corpus Training, a method that trains language models (LMs) on corpora with certain linguistic constructions filtered out from the training data, and uses it to measure the ability of LMs to perform linguistic generalization on the basis of indirect evidence.","We apply the method to both LSTM and Transformer LMs (of roughly comparable size), developing filtered corpora that target a wide range of linguistic phenomena.","Our results show that while transformers are better qua LMs (as measured by perplexity), both models perform equally and surprisingly well on linguistic generalization measures, suggesting that they are capable of generalizing from indirect evidence."],"url":"http://arxiv.org/abs/2405.15750v1","category":"cs.CL"}
{"created":"2024-05-24 17:46:53","title":"Collaborative Access Control for IoT -- A Blockchain Approach","abstract":"The Internet of Things (IoT) necessitates robust access control mechanisms to secure a vast array of interconnected devices. Most of the existing IoT systems in practice use centralized solutions. We identify the problems in such solutions and adopt the blockchain based decentralized access control approach. Though there are works in the literature that use blockchain for access control, there are some gaps in these works. We develop a blockchain embedded access control (BEAC) framework to bridge the gaps. First, blockchain based solutions for access control require an enabling P2P network while existing P2P overlays do not support some required features. We develop a novel P2P infrastructure to seamlessly support our BEAC framework. Second, most of the works consider blockchain based access control for a single access control model, and we develop a generic blockchain mechanism and show that it can support the embedding of various access control models. Finally, existing works adopt existing blockchain mechanisms which may incur a high communication overhead. We develop a shortcut approach to improve the number of message rounds in the access protocol. Our experiments demonstrate the efficacy of our system, showing that the shortcut mechanism can reduces access time by approximately 43%.","sentences":["The Internet of Things (IoT) necessitates robust access control mechanisms to secure a vast array of interconnected devices.","Most of the existing IoT systems in practice use centralized solutions.","We identify the problems in such solutions and adopt the blockchain based decentralized access control approach.","Though there are works in the literature that use blockchain for access control, there are some gaps in these works.","We develop a blockchain embedded access control (BEAC) framework to bridge the gaps.","First, blockchain based solutions for access control require an enabling P2P network while existing P2P overlays do not support some required features.","We develop a novel P2P infrastructure to seamlessly support our BEAC framework.","Second, most of the works consider blockchain based access control for a single access control model, and we develop a generic blockchain mechanism and show that it can support the embedding of various access control models.","Finally, existing works adopt existing blockchain mechanisms which may incur a high communication overhead.","We develop a shortcut approach to improve the number of message rounds in the access protocol.","Our experiments demonstrate the efficacy of our system, showing that the shortcut mechanism can reduces access time by approximately 43%."],"url":"http://arxiv.org/abs/2405.15749v1","category":"cs.DC"}
{"created":"2024-05-24 17:41:50","title":"Characterizing Discourse Group Roles in Inquiry-based University Science Labs","abstract":"Prior research has characterized students' group roles in introductory physics labs with a focus on what students are handling (e.g., equipment) and documented gender inequities in student division of labor. However, student discourse is rarely investigated in university science labs. We aim to bridge the gap in the literature by characterizing student discourse group roles in inquiry-based science labs. The instructional context for this study was a summer program hosted at a private research university in the eastern United States. The program was designed as a bridge program for matriculating students who were first generation and/or deaf or hard-of-hearing (DHH). Accommodations such as interpreters and technology were provided for DHH students. We analyzed 15 students' discourse moves in five lab activities from the video recordings, resulting in a total of 40 student-lab units. We developed codes to describe student discourse moves: asking a question, proposing an idea, participating in discussion, chatting off-task, and talking to instructor. We conducted a cluster analysis using those 40 student-lab units on our quantified discourse moves to characterize students' discourse styles (i.e., clusters). We identified four discourse styles, high on-task high social, high on-task low social, low on-task low social, and low on-task high social. The results show that individual students tend to demonstrate varying discourse styles in different lab activities; students' discourse styles within the same groups tend to be aligned with their group members. Moreover, no difference was observed in discourse styles between genders, but DHH students were observed to participate significantly less in group discourse. We propose that group-level interventions that specifically target discourse should be used to promote productive and equitable small-group discourse in university science labs.","sentences":["Prior research has characterized students' group roles in introductory physics labs with a focus on what students are handling (e.g., equipment) and documented gender inequities in student division of labor.","However, student discourse is rarely investigated in university science labs.","We aim to bridge the gap in the literature by characterizing student discourse group roles in inquiry-based science labs.","The instructional context for this study was a summer program hosted at a private research university in the eastern United States.","The program was designed as a bridge program for matriculating students who were first generation and/or deaf or hard-of-hearing (DHH).","Accommodations such as interpreters and technology were provided for DHH students.","We analyzed 15 students' discourse moves in five lab activities from the video recordings, resulting in a total of 40 student-lab units.","We developed codes to describe student discourse moves: asking a question, proposing an idea, participating in discussion, chatting off-task, and talking to instructor.","We conducted a cluster analysis using those 40 student-lab units on our quantified discourse moves to characterize students' discourse styles (i.e., clusters).","We identified four discourse styles, high on-task high social, high on-task low social, low on-task low social, and low on-task high social.","The results show that individual students tend to demonstrate varying discourse styles in different lab activities; students' discourse styles within the same groups tend to be aligned with their group members.","Moreover, no difference was observed in discourse styles between genders, but DHH students were observed to participate significantly less in group discourse.","We propose that group-level interventions that specifically target discourse should be used to promote productive and equitable small-group discourse in university science labs."],"url":"http://arxiv.org/abs/2405.15746v1","category":"physics.ed-ph"}
{"created":"2024-05-24 17:34:32","title":"Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias","abstract":"Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases. The emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic to these practices. Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored. Here, we analyze these characteristics in an experiment using a dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date, encompassing 3,066 references in total. In our experiment, GPT-4 was tasked with suggesting scholarly references for the anonymized in-text citations within these papers. Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias in GPT-4, which persists even after controlling for publication year, title length, number of authors, and venue. Additionally, we observe a large consistency between the characteristics of GPT-4's existing and non-existent generated references, indicating the model's internalization of citation patterns. By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, they may also amplify existing biases and introduce new ones, potentially skewing scientific knowledge dissemination. Our results underscore the need for identifying the model's biases and for developing balanced methods to interact with LLMs in general.","sentences":["Citation practices are crucial in shaping the structure of scientific knowledge, yet they are often influenced by contemporary norms and biases.","The emergence of Large Language Models (LLMs) like GPT-4 introduces a new dynamic to these practices.","Interestingly, the characteristics and potential biases of references recommended by LLMs that entirely rely on their parametric knowledge, and not on search or retrieval-augmented generation, remain unexplored.","Here, we analyze these characteristics in an experiment using a dataset of 166 papers from AAAI, NeurIPS, ICML, and ICLR, published after GPT-4's knowledge cut-off date, encompassing 3,066 references in total.","In our experiment, GPT-4 was tasked with suggesting scholarly references for the anonymized in-text citations within these papers.","Our findings reveal a remarkable similarity between human and LLM citation patterns, but with a more pronounced high citation bias in GPT-4, which persists even after controlling for publication year, title length, number of authors, and venue.","Additionally, we observe a large consistency between the characteristics of GPT-4's existing and non-existent generated references, indicating the model's internalization of citation patterns.","By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks.","While LLMs can aid in citation generation, they may also amplify existing biases and introduce new ones, potentially skewing scientific knowledge dissemination.","Our results underscore the need for identifying the model's biases and for developing balanced methods to interact with LLMs in general."],"url":"http://arxiv.org/abs/2405.15739v1","category":"cs.DL"}
{"created":"2024-05-24 17:34:15","title":"ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models","abstract":"High-resolution Large Multimodal Models (LMMs) encounter the challenges of excessive visual tokens and quadratic visual complexity. Current high-resolution LMMs address the quadratic complexity while still generating excessive visual tokens. However, the redundancy in visual tokens is the key problem as it leads to more substantial compute. To mitigate this issue, we propose ConvLLaVA, which employs ConvNeXt, a hierarchical backbone, as the visual encoder of LMM to replace Vision Transformer (ViT). ConvLLaVA compresses high-resolution images into information-rich visual features, effectively preventing the generation of excessive visual tokens. To enhance the capabilities of ConvLLaVA, we propose two critical optimizations. Since the low-resolution pretrained ConvNeXt underperforms when directly applied on high resolution, we update it to bridge the gap. Moreover, since ConvNeXt's original compression ratio is inadequate for much higher resolution inputs, we train a successive stage to further compress the visual tokens, thereby reducing redundancy. These optimizations enable ConvLLaVA to support inputs of 1536x1536 resolution generating only 576 visual tokens, capable of handling images of arbitrary aspect ratios. Experimental results demonstrate that our method achieves competitive performance with state-of-the-art models on mainstream benchmarks. The ConvLLaVA model series are publicly available at https://github.com/alibaba/conv-llava.","sentences":["High-resolution Large Multimodal Models (LMMs) encounter the challenges of excessive visual tokens and quadratic visual complexity.","Current high-resolution LMMs address the quadratic complexity while still generating excessive visual tokens.","However, the redundancy in visual tokens is the key problem as it leads to more substantial compute.","To mitigate this issue, we propose ConvLLaVA, which employs ConvNeXt, a hierarchical backbone, as the visual encoder of LMM to replace Vision Transformer (ViT).","ConvLLaVA compresses high-resolution images into information-rich visual features, effectively preventing the generation of excessive visual tokens.","To enhance the capabilities of ConvLLaVA, we propose two critical optimizations.","Since the low-resolution pretrained ConvNeXt underperforms when directly applied on high resolution, we update it to bridge the gap.","Moreover, since ConvNeXt's original compression ratio is inadequate for much higher resolution inputs, we train a successive stage to further compress the visual tokens, thereby reducing redundancy.","These optimizations enable ConvLLaVA to support inputs of 1536x1536 resolution generating only 576 visual tokens, capable of handling images of arbitrary aspect ratios.","Experimental results demonstrate that our method achieves competitive performance with state-of-the-art models on mainstream benchmarks.","The ConvLLaVA model series are publicly available at https://github.com/alibaba/conv-llava."],"url":"http://arxiv.org/abs/2405.15738v1","category":"cs.CV"}
{"created":"2024-05-24 17:26:46","title":"A Higher Order Local Mesh Method for Approximating Laplacians on Unknown Manifolds","abstract":"In this work, we introduce a numerical method for approximating arbitrary differential operators on vector fields in the weak form given point cloud data sampled randomly from a $d$ dimensional manifold embedded in $\\mathbb{R}^n$. This method generalizes the local linear mesh method to the local curved mesh method, thus, allowing for the estimation of differential operators with nontrivial Christoffer symbols, such as the Bochner or Hodge Laplacians. In particular, we leverage the potentially small intrinsic dimension of the manifold $(d \\ll n)$ to construct local parameterizations that incorporate both local meshes and higher-order curvature information. The former is constructed using low dimensional meshes obtained from local data projected to the tangent spaces, while the latter is obtained by fitting local polynomials with the generalized moving least squares. Theoretically, we prove the weak and spectral convergence rates for the proposed method for the estimation of the Bochner Laplacian. We provide numerical results supporting the theoretical convergence rates for the Bochner and Hodge Laplacians on simple manifolds.","sentences":["In this work, we introduce a numerical method for approximating arbitrary differential operators on vector fields in the weak form given point cloud data sampled randomly from a $d$ dimensional manifold embedded in $\\mathbb{R}^n$. This method generalizes the local linear mesh method to the local curved mesh method, thus, allowing for the estimation of differential operators with nontrivial Christoffer symbols, such as the Bochner or Hodge Laplacians.","In particular, we leverage the potentially small intrinsic dimension of the manifold $(d \\ll n)$ to construct local parameterizations that incorporate both local meshes and higher-order curvature information.","The former is constructed using low dimensional meshes obtained from local data projected to the tangent spaces, while the latter is obtained by fitting local polynomials with the generalized moving least squares.","Theoretically, we prove the weak and spectral convergence rates for the proposed method for the estimation of the Bochner Laplacian.","We provide numerical results supporting the theoretical convergence rates for the Bochner and Hodge Laplacians on simple manifolds."],"url":"http://arxiv.org/abs/2405.15735v1","category":"math.NA"}
{"created":"2024-05-24 17:19:57","title":"Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks","abstract":"Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.","sentences":["Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings.","To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives.","While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability.","In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation.","Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.","For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent.","We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated.","Additionally, we substantiate these new insights with empirical validations and mathematical arguments.","This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models."],"url":"http://arxiv.org/abs/2405.15731v1","category":"cs.LG"}
{"created":"2024-05-24 17:19:03","title":"Optimizing Large Language Models for OpenAPI Code Completion","abstract":"Recent advancements in Large Language Models (LLMs) and their utilization in code generation tasks have significantly reshaped the field of software development. Despite the remarkable efficacy of code completion solutions in mainstream programming languages, their performance lags when applied to less ubiquitous formats such as OpenAPI definitions. This study evaluates the OpenAPI completion performance of GitHub Copilot, a prevalent commercial code completion tool, and proposes a set of task-specific optimizations leveraging Meta's open-source model Code Llama. A semantics-aware OpenAPI completion benchmark proposed in this research is used to perform a series of experiments through which the impact of various prompt-engineering and fine-tuning techniques on the Code Llama model's performance is analyzed. The fine-tuned Code Llama model reaches a peak correctness improvement of 55.2% over GitHub Copilot despite utilizing 25 times fewer parameters than the commercial solution's underlying Codex model. Additionally, this research proposes an enhancement to a widely used code infilling training technique, addressing the issue of underperformance when the model is prompted with context sizes smaller than those used during training.","sentences":["Recent advancements in Large Language Models (LLMs) and their utilization in code generation tasks have significantly reshaped the field of software development.","Despite the remarkable efficacy of code completion solutions in mainstream programming languages, their performance lags when applied to less ubiquitous formats such as OpenAPI definitions.","This study evaluates the OpenAPI completion performance of GitHub Copilot, a prevalent commercial code completion tool, and proposes a set of task-specific optimizations leveraging Meta's open-source model Code Llama.","A semantics-aware OpenAPI completion benchmark proposed in this research is used to perform a series of experiments through which the impact of various prompt-engineering and fine-tuning techniques on the Code Llama model's performance is analyzed.","The fine-tuned Code Llama model reaches a peak correctness improvement of 55.2% over GitHub Copilot despite utilizing 25 times fewer parameters than the commercial solution's underlying Codex model.","Additionally, this research proposes an enhancement to a widely used code infilling training technique, addressing the issue of underperformance when the model is prompted with context sizes smaller than those used during training."],"url":"http://arxiv.org/abs/2405.15729v1","category":"cs.SE"}
{"created":"2024-05-24 17:18:02","title":"Disease-informed Adaptation of Vision-Language Models","abstract":"In medical image analysis, the expertise scarcity and the high cost of data annotation limits the development of large artificial intelligence models. This paper investigates the potential of transfer learning with pre-trained vision-language models (VLMs) in this domain. Currently, VLMs still struggle to transfer to the underrepresented diseases with minimal presence and new diseases entirely absent from the pretraining dataset. We argue that effective adaptation of VLMs hinges on the nuanced representation learning of disease concepts. By capitalizing on the joint visual-linguistic capabilities of VLMs, we introduce disease-informed contextual prompting in a novel disease prototype learning framework. This approach enables VLMs to grasp the concepts of new disease effectively and efficiently, even with limited data. Extensive experiments across multiple image modalities showcase notable enhancements in performance compared to existing techniques.","sentences":["In medical image analysis, the expertise scarcity and the high cost of data annotation limits the development of large artificial intelligence models.","This paper investigates the potential of transfer learning with pre-trained vision-language models (VLMs) in this domain.","Currently, VLMs still struggle to transfer to the underrepresented diseases with minimal presence and new diseases entirely absent from the pretraining dataset.","We argue that effective adaptation of VLMs hinges on the nuanced representation learning of disease concepts.","By capitalizing on the joint visual-linguistic capabilities of VLMs, we introduce disease-informed contextual prompting in a novel disease prototype learning framework.","This approach enables VLMs to grasp the concepts of new disease effectively and efficiently, even with limited data.","Extensive experiments across multiple image modalities showcase notable enhancements in performance compared to existing techniques."],"url":"http://arxiv.org/abs/2405.15728v1","category":"cs.CV"}
{"created":"2024-05-24 17:17:28","title":"Interaction in the dark sector: a phenomenological approach","abstract":"The non-gravitational interaction between the dark components of the Universe could lead to the variation of dark matter energy density standard evolution law. When we assume this scenario, the dark matter energy density follows $\\rho_{{dm}}\\sim(1+z)^{3 + \\epsilon(z)}$ (where $\\epsilon(z)=0$ the standard law is recovered). In this paper, we perform a Bayesian analysis to test three parameterizations for $\\epsilon(z)$, namely: $\\epsilon(z)=\\epsilon_0$, $\\epsilon(z)=\\epsilon_0 + \\epsilon_1\\frac{z}{1+z}$ and $\\epsilon(z)=\\epsilon_0 + \\epsilon_1\\frac{z(1+z)}{1+z^2}$, where the first one is motivated through the fundamental grounds and the others are on the phenomenological ones. Through the Gaussian process regression, our method uses galaxy cluster gas mass fraction measurements, SNe Ia observations, Cosmic Chronometers, and BAO data. No specific cosmological model is considered. In all possibilities analyzed, the standard evolution law ($\\epsilon(z)=0$) is within $2\\sigma$ c.l. The investigated cases generally indicated scenarios of inconclusive or weak evidence toward the simplest model from the Bayesian standpoint.","sentences":["The non-gravitational interaction between the dark components of the Universe could lead to the variation of dark matter energy density standard evolution law.","When we assume this scenario, the dark matter energy density follows $\\rho_{{dm}}\\sim(1+z)^{3 + \\epsilon(z)}$ (where $\\epsilon(z)=0$ the standard law is recovered).","In this paper, we perform a Bayesian analysis to test three parameterizations for $\\epsilon(z)$, namely: $\\epsilon(z)=\\epsilon_0$, $\\epsilon(z)=\\epsilon_0 + \\epsilon_1\\frac{z}{1+z}$ and $\\epsilon(z)=\\epsilon_0 + \\epsilon_1\\frac{z(1+z)}{1+z^2}$, where the first one is motivated through the fundamental grounds and the others are on the phenomenological ones.","Through the Gaussian process regression, our method uses galaxy cluster gas mass fraction measurements, SNe Ia observations, Cosmic Chronometers, and BAO data.","No specific cosmological model is considered.","In all possibilities analyzed, the standard evolution law ($\\epsilon(z)=0$) is within $2\\sigma$ c.l.","The investigated cases generally indicated scenarios of inconclusive or weak evidence toward the simplest model from the Bayesian standpoint."],"url":"http://arxiv.org/abs/2405.15726v1","category":"astro-ph.CO"}
{"created":"2024-05-24 17:10:08","title":"Models That Prove Their Own Correctness","abstract":"How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measured \\emph{on average} over a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to train *Self-Proving models* that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. Self-Proving models satisfy that, with high probability over a random input, the model generates a correct output \\emph{and} successfully proves its correctness to $V\\!$. The *soundness* property of $V$ guarantees that, for *every* input, no model can convince $V$ of the correctness of an incorrect output. Thus, a Self-Proving model proves correctness of most of its outputs, while *all* incorrect outputs (of any model) are detected by $V$. We devise a generic method for learning Self-Proving models, and we prove convergence bounds under certain assumptions. The theoretical framework and results are complemented by experiments on an arithmetic capability: computing the greatest common divisor (GCD) of two integers. Our learning method is used to train a Self-Proving transformer that computes the GCD *and* proves the correctness of its answer.","sentences":["How can we trust the correctness of a learned model on a particular input of interest?","Model accuracy is typically measured \\emph{on average} over a distribution of inputs, giving no guarantee for any fixed input.","This paper proposes a theoretically-founded solution to this problem: to train *Self-Proving models* that prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof.","Self-Proving models satisfy that, with high probability over a random input, the model generates a correct output \\emph{and} successfully proves its correctness to $V\\!$. The *soundness* property of $V$ guarantees that, for *every* input, no model can convince $V$ of the correctness of an incorrect output.","Thus, a Self-Proving model proves correctness of most of its outputs, while *all* incorrect outputs (of any model) are detected by $V$. We devise a generic method for learning Self-Proving models, and we prove convergence bounds under certain assumptions.","The theoretical framework and results are complemented by experiments on an arithmetic capability: computing the greatest common divisor (GCD) of two integers.","Our learning method is used to train a Self-Proving transformer that computes the GCD *and* proves the correctness of its answer."],"url":"http://arxiv.org/abs/2405.15722v1","category":"cs.LG"}
{"created":"2024-05-24 17:06:18","title":"Integrated Design for Wave Energy Converter Farms: Assessing Plant, Control, Layout, and Site Selection Coupling in the Presence of Irregular Waves","abstract":"A promising direction towards reducing the levelized cost of energy for wave energy converter (WEC) farms is to improve their performance. WEC design studies generally focus on a single design domain (e.g., geometry, control, or layout) to improve the farm's performance under simplifying assumptions, such as regular waves. This strategy, however, has resulted in design recommendations that are impractical or limited in scope because WEC farms are complex systems that exhibit strong coupling among geometry, control, and layout domains. In addition, the location of the candidate site, which has a large impact on the performance of the farm, is often overlooked. Motivated by some of the limitations observed in WEC literature, this study uses an integrated design framework, based on simultaneous control co-design (CCD) principles, to discuss the impact of site selection and wave type on WEC farm design. Interactions among plant, control, and layout are also investigated and discussed using a wide range of simulations and optimization studies. All of the studies were conducted using frequency-domain heaving cylinder WEC devices within a farm with a linear reactive controller in the presence of irregular probabilistic waves. The results provide high-level guidelines to help the WEC design community move toward an integrated design perspective.","sentences":["A promising direction towards reducing the levelized cost of energy for wave energy converter (WEC) farms is to improve their performance.","WEC design studies generally focus on a single design domain (e.g., geometry, control, or layout) to improve the farm's performance under simplifying assumptions, such as regular waves.","This strategy, however, has resulted in design recommendations that are impractical or limited in scope because WEC farms are complex systems that exhibit strong coupling among geometry, control, and layout domains.","In addition, the location of the candidate site, which has a large impact on the performance of the farm, is often overlooked.","Motivated by some of the limitations observed in WEC literature, this study uses an integrated design framework, based on simultaneous control co-design (CCD) principles, to discuss the impact of site selection and wave type on WEC farm design.","Interactions among plant, control, and layout are also investigated and discussed using a wide range of simulations and optimization studies.","All of the studies were conducted using frequency-domain heaving cylinder WEC devices within a farm with a linear reactive controller in the presence of irregular probabilistic waves.","The results provide high-level guidelines to help the WEC design community move toward an integrated design perspective."],"url":"http://arxiv.org/abs/2405.15717v1","category":"eess.SY"}
{"created":"2024-05-24 17:01:47","title":"Mean Field Limit for Congestion Dynamics in One Dimension","abstract":"This paper addresses congested transport, which can be described, at macroscopic scales, by a continuity equation with a pressure variable generated from the hard-congestion constraint (maximum value of the density). The main goal of the paper is to show that, in one spatial dimension, this continuum PDE can be derived as the mean-field limit of a system of ordinary differential equations that describes the motion of a large number of particles constrained to stay at some finite distance from each others. To show that these two models describe the same dynamics at different scale, we will rely on both the Eulerian and Lagrangian points of view and use two different approximations for the density and pressure variables in the continuum limit.","sentences":["This paper addresses congested transport, which can be described, at macroscopic scales, by a continuity equation with a pressure variable generated from the hard-congestion constraint (maximum value of the density).","The main goal of the paper is to show that, in one spatial dimension, this continuum PDE can be derived as the mean-field limit of a system of ordinary differential equations that describes the motion of a large number of particles constrained to stay at some finite distance from each others.","To show that these two models describe the same dynamics at different scale, we will rely on both the Eulerian and Lagrangian points of view and use two different approximations for the density and pressure variables in the continuum limit."],"url":"http://arxiv.org/abs/2405.15714v1","category":"math.AP"}
{"created":"2024-05-24 16:59:29","title":"Information-theoretic Generalization Analysis for Expected Calibration Error","abstract":"While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited. In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, uniform mass and uniform width binning. Our analysis establishes upper bounds on the bias, achieving an improved convergence rate. Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias. We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data. Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.","sentences":["While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited.","In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, uniform mass and uniform width binning.","Our analysis establishes upper bounds on the bias, achieving an improved convergence rate.","Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias.","We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data.","Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach."],"url":"http://arxiv.org/abs/2405.15709v1","category":"cs.LG"}
{"created":"2024-05-24 16:52:09","title":"The Impact of Geometric Complexity on Neural Collapse in Transfer Learning","abstract":"Many of the recent remarkable advances in computer vision and language models can be attributed to the success of transfer learning via the pre-training of large foundation models. However, a theoretical framework which explains this empirical success is incomplete and remains an active area of research. Flatness of the loss surface and neural collapse have recently emerged as useful pre-training metrics which shed light on the implicit biases underlying pre-training. In this paper, we explore the geometric complexity of a model's learned representations as a fundamental mechanism that relates these two concepts. We show through experiments and theory that mechanisms which affect the geometric complexity of the pre-trained network also influence the neural collapse. Furthermore, we show how this effect of the geometric complexity generalizes to the neural collapse of new classes as well, thus encouraging better performance on downstream tasks, particularly in the few-shot setting.","sentences":["Many of the recent remarkable advances in computer vision and language models can be attributed to the success of transfer learning via the pre-training of large foundation models.","However, a theoretical framework which explains this empirical success is incomplete and remains an active area of research.","Flatness of the loss surface and neural collapse have recently emerged as useful pre-training metrics which shed light on the implicit biases underlying pre-training.","In this paper, we explore the geometric complexity of a model's learned representations as a fundamental mechanism that relates these two concepts.","We show through experiments and theory that mechanisms which affect the geometric complexity of the pre-trained network also influence the neural collapse.","Furthermore, we show how this effect of the geometric complexity generalizes to the neural collapse of new classes as well, thus encouraging better performance on downstream tasks, particularly in the few-shot setting."],"url":"http://arxiv.org/abs/2405.15706v1","category":"cs.LG"}
{"created":"2024-05-24 16:50:37","title":"Quantum nondemolition measurement operator with spontaneous emission","abstract":"We present a theory for quantum nondemolition (QND) measurements of an atomic ensemble in the presence of spontaneous emission. We derive the master equation that governs the evolution of the ground state of the atoms and the quantum state of light. Solving the master equation exactly without invoking the Holstein-Primakoff approximation and projecting out the quantum state of light, we derive a positive operator-valued measure that describes the QND measurement. We show that at high spontaneous emission conditions, the QND measurement has a unique dominant state to which the measurement collapses. We additionally investigate the behavior of the QND measurement in the limiting case of strong atom-light interactions, where we show that the positive operator valued measure becomes a projection operator. We further analyze the effect of spontaneous emission noise on atomic state preparation. We find that it limits the width of the eigenvalue spectrum available to a quantum state in a linear superposition. This effect leads to state collapse on the dominant state. We generate various non-classical states of the atom by tuning the atom-light interaction strength. We find that non-classical states such as the Schr\\\"odinger-cat state, whose coherence spans the entire eigenvalue spectrum of the total spin operator $J_z$ for a given spin eigenvalue $J$, lose their coherence because spontaneous emission limits the accessibility of states farther away from the dominant state.","sentences":["We present a theory for quantum nondemolition (QND) measurements of an atomic ensemble in the presence of spontaneous emission.","We derive the master equation that governs the evolution of the ground state of the atoms and the quantum state of light.","Solving the master equation exactly without invoking the Holstein-Primakoff approximation and projecting out the quantum state of light, we derive a positive operator-valued measure that describes the QND measurement.","We show that at high spontaneous emission conditions, the QND measurement has a unique dominant state to which the measurement collapses.","We additionally investigate the behavior of the QND measurement in the limiting case of strong atom-light interactions, where we show that the positive operator valued measure becomes a projection operator.","We further analyze the effect of spontaneous emission noise on atomic state preparation.","We find that it limits the width of the eigenvalue spectrum available to a quantum state in a linear superposition.","This effect leads to state collapse on the dominant state.","We generate various non-classical states of the atom by tuning the atom-light interaction strength.","We find that non-classical states such as the Schr\\\"odinger-cat state, whose coherence spans the entire eigenvalue spectrum of the total spin operator $J_z$ for a given spin eigenvalue $J$, lose their coherence because spontaneous emission limits the accessibility of states farther away from the dominant state."],"url":"http://arxiv.org/abs/2405.15704v1","category":"quant-ph"}
{"created":"2024-05-24 16:46:55","title":"Metrological usefulness of entanglement and nonlinear Hamiltonians","abstract":"A central task in quantum metrology is to exploit quantum correlations to outperform classical sensitivity limits. Metrologically useful entanglement is identified when the quantum Fisher information (QFI) exceeds a separability bound for a given parameter-encoding Hamiltonian. However, so far, only results for linear Hamiltonians are well-established. Here, we characterize metrologically useful entanglement for nonlinear Hamiltonians, presenting separability bounds for collective angular momenta. Also, we provide a general expression for entangled states maximizing the QFI, showing that these are not always GHZ-like states. Finally, we compare the metrological usefulness of linear and nonlinear cases, in terms of entanglement detection and random symmetric states.","sentences":["A central task in quantum metrology is to exploit quantum correlations to outperform classical sensitivity limits.","Metrologically useful entanglement is identified when the quantum Fisher information (QFI) exceeds a separability bound for a given parameter-encoding Hamiltonian.","However, so far, only results for linear Hamiltonians are well-established.","Here, we characterize metrologically useful entanglement for nonlinear Hamiltonians, presenting separability bounds for collective angular momenta.","Also, we provide a general expression for entangled states maximizing the QFI, showing that these are not always GHZ-like states.","Finally, we compare the metrological usefulness of linear and nonlinear cases, in terms of entanglement detection and random symmetric states."],"url":"http://arxiv.org/abs/2405.15703v1","category":"quant-ph"}
{"created":"2024-05-24 16:44:22","title":"Trackastra: Transformer-based cell tracking for live-cell microscopy","abstract":"Cell tracking is an omnipresent image analysis task in live-cell microscopy. It is similar to multiple object tracking (MOT), however, each frame contains hundreds of similar-looking objects that can divide, making it a challenging problem. Current state-of-the-art approaches follow the tracking-by-detection paradigm, i.e. first all cells are detected per frame and successively linked in a second step to form biologically consistent cell tracks. Linking is commonly solved via discrete optimization methods, which require manual tuning of hyperparameters for each dataset and are therefore cumbersome to use in practice. Here we propose Trackastra, a general purpose cell tracking approach that uses a simple transformer architecture to directly learn pairwise associations of cells within a temporal window from annotated data. Importantly, unlike existing transformer-based MOT pipelines, our learning architecture also accounts for dividing objects such as cells and allows for accurate tracking even with simple greedy linking, thus making strides towards removing the requirement for a complex linking step. The proposed architecture operates on the full spatio-temporal context of detections within a time window by avoiding the computational burden of processing dense images. We show that our tracking approach performs on par with or better than highly tuned state-of-the-art cell tracking algorithms for various biological datasets, such as bacteria, cell cultures and fluorescent particles. We provide code at https://github.com/weigertlab/trackastra.","sentences":["Cell tracking is an omnipresent image analysis task in live-cell microscopy.","It is similar to multiple object tracking (MOT), however, each frame contains hundreds of similar-looking objects that can divide, making it a challenging problem.","Current state-of-the-art approaches follow the tracking-by-detection paradigm, i.e. first all cells are detected per frame and successively linked in a second step to form biologically consistent cell tracks.","Linking is commonly solved via discrete optimization methods, which require manual tuning of hyperparameters for each dataset and are therefore cumbersome to use in practice.","Here we propose Trackastra, a general purpose cell tracking approach that uses a simple transformer architecture to directly learn pairwise associations of cells within a temporal window from annotated data.","Importantly, unlike existing transformer-based MOT pipelines, our learning architecture also accounts for dividing objects such as cells and allows for accurate tracking even with simple greedy linking, thus making strides towards removing the requirement for a complex linking step.","The proposed architecture operates on the full spatio-temporal context of detections within a time window by avoiding the computational burden of processing dense images.","We show that our tracking approach performs on par with or better than highly tuned state-of-the-art cell tracking algorithms for various biological datasets, such as bacteria, cell cultures and fluorescent particles.","We provide code at https://github.com/weigertlab/trackastra."],"url":"http://arxiv.org/abs/2405.15700v1","category":"cs.CV"}
{"created":"2024-05-24 16:43:26","title":"Dimension-free deterministic equivalents for random feature regression","abstract":"In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension -- allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate.","sentences":["In this work we investigate the generalization performance of random feature ridge regression (RFRR).","Our main contribution is a general deterministic equivalent for the test error of RFRR.","Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues.","Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension -- allowing for infinite-dimensional features.","We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets.","As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay.","In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate."],"url":"http://arxiv.org/abs/2405.15699v1","category":"stat.ML"}
{"created":"2024-05-24 16:37:43","title":"Synthetic high angular momentum spin dynamics in a microwave oscillator","abstract":"Spins and oscillators are foundational to much of physics and applied sciences. For quantum information, a spin 1/2 exemplifies the most basic unit, a qubit. High angular momentum spins and harmonic oscillators provide multi-level manifolds (e.g., qudits) which have the potential for hardware-efficient protected encodings of quantum information and simulation of many-body quantum systems. In this work, we demonstrate a new quantum control protocol that conceptually merges these disparate hardware platforms. Namely, we show how to modify a harmonic oscillator on-demand to implement a continuous range of generators associated to resonant driving of a harmonic qudit, and then specifically design a harmonic multi-level spin degree of freedom. The synthetic spin is verified by demonstration of spin coherent (SU(2)) rotations and comparison to other manifolds like simply-truncated oscillators. Our scheme allows universal control of the qudit, and, for the first time, we use linear, harmonic operations to accomplish four logical gates on a harmonic qudit encoding. Our results show how motion on a closed Hilbert space can be useful for quantum information processing and opens the door to superconducting circuit simulations of higher angular momentum quantum magnetism.","sentences":["Spins and oscillators are foundational to much of physics and applied sciences.","For quantum information, a spin 1/2 exemplifies the most basic unit, a qubit.","High angular momentum spins and harmonic oscillators provide multi-level manifolds (e.g., qudits) which have the potential for hardware-efficient protected encodings of quantum information and simulation of many-body quantum systems.","In this work, we demonstrate a new quantum control protocol that conceptually merges these disparate hardware platforms.","Namely, we show how to modify a harmonic oscillator on-demand to implement a continuous range of generators associated to resonant driving of a harmonic qudit, and then specifically design a harmonic multi-level spin degree of freedom.","The synthetic spin is verified by demonstration of spin coherent (SU(2)) rotations and comparison to other manifolds like simply-truncated oscillators.","Our scheme allows universal control of the qudit, and, for the first time, we use linear, harmonic operations to accomplish four logical gates on a harmonic qudit encoding.","Our results show how motion on a closed Hilbert space can be useful for quantum information processing and opens the door to superconducting circuit simulations of higher angular momentum quantum magnetism."],"url":"http://arxiv.org/abs/2405.15695v1","category":"quant-ph"}
{"created":"2024-05-24 16:29:48","title":"A Case Study of LLM for Automated Vulnerability Repair: Assessing Impact of Reasoning and Patch Validation Feedback","abstract":"Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis. The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored. In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security. To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback. VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.) on previously-generated patches. To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature. Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively. We show, through an ablation study, that reasoning and patch validation feedback are critical. We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair","sentences":["Recent work in automated program repair (APR) proposes the use of reasoning and patch validation feedback to reduce the semantic gap between the LLMs and the code under analysis.","The idea has been shown to perform well for general APR, but its effectiveness in other particular contexts remains underexplored.","In this work, we assess the impact of reasoning and patch validation feedback to LLMs in the context of vulnerability repair, an important and challenging task in security.","To support the evaluation, we present VRpilot, an LLM-based vulnerability repair technique based on reasoning and patch validation feedback.","VRpilot (1) uses a chain-of-thought prompt to reason about a vulnerability prior to generating patch candidates and (2) iteratively refines prompts according to the output of external tools (e.g., compiler, code sanitizers, test suite, etc.)","on previously-generated patches.","To evaluate performance, we compare VRpilot against the state-of-the-art vulnerability repair techniques for C and Java using public datasets from the literature.","Our results show that VRpilot generates, on average, 14% and 7.6% more correct patches than the baseline techniques on C and Java, respectively.","We show, through an ablation study, that reasoning and patch validation feedback are critical.","We report several lessons from this study and potential directions for advancing LLM-empowered vulnerability repair"],"url":"http://arxiv.org/abs/2405.15690v1","category":"cs.SE"}
{"created":"2024-05-24 16:28:08","title":"Probing Berry curvature in magnetic topological insulators through resonant infrared magnetic circular dichroism","abstract":"Probing the quantum geometry and topology in condensed matter systems has relied heavily on static electronic transport experiments in magnetic fields. Yet, contact-free optical measurements have rarely been explored. Magnetic dichroism (MCD), the nonreciprocal absorption of circular polarized light, was theoretically linked to the quantized anomalous Hall effect in magnetic insulators and can identify the bands and momenta responsible for the underlying Berry Curvature (BC). Detecting BC through MCD faces two challenges: First, the relevant inter-band transitions usually generate MCD in the infrared (IR) range, requiring large samples with high quality. Second, while most magnetic materials are metallic, the relation between MCD and BC in metals remains unclear. Here, we report the observation of MCD in the IR range along with the anomalous Hall effect in thin film MnBi2Te4. Both phenomena emerge with a field-driven phase transition from an antiferromagnet to a canted ferromagnet. By theoretically relating the MCD to the anomalous Hall effect via BC in a metal, we show that this transition accompanies an abrupt onset of BC, signaling a topological phase transition from a topological insulator to a doped Chern insulator. Our density functional theory calculation suggests the MCD signal mainly originates from an optical transition at the Brillouin zone edge, hinting at a potential new source of BC away from the commonly considered {\\Gamma} point. Our findings demonstrate a novel experimental approach for detecting BC and identifying the responsible bands and momenta, generally applicable to magnetic materials.","sentences":["Probing the quantum geometry and topology in condensed matter systems has relied heavily on static electronic transport experiments in magnetic fields.","Yet, contact-free optical measurements have rarely been explored.","Magnetic dichroism (MCD), the nonreciprocal absorption of circular polarized light, was theoretically linked to the quantized anomalous Hall effect in magnetic insulators and can identify the bands and momenta responsible for the underlying Berry Curvature (BC).","Detecting BC through MCD faces two challenges: First, the relevant inter-band transitions usually generate MCD in the infrared (IR) range, requiring large samples with high quality.","Second, while most magnetic materials are metallic, the relation between MCD and BC in metals remains unclear.","Here, we report the observation of MCD in the IR range along with the anomalous Hall effect in thin film MnBi2Te4.","Both phenomena emerge with a field-driven phase transition from an antiferromagnet to a canted ferromagnet.","By theoretically relating the MCD to the anomalous Hall effect via BC in a metal, we show that this transition accompanies an abrupt onset of BC, signaling a topological phase transition from a topological insulator to a doped Chern insulator.","Our density functional theory calculation suggests the MCD signal mainly originates from an optical transition at the Brillouin zone edge, hinting at a potential new source of BC away from the commonly considered {\\Gamma} point.","Our findings demonstrate a novel experimental approach for detecting BC and identifying the responsible bands and momenta, generally applicable to magnetic materials."],"url":"http://arxiv.org/abs/2405.15689v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-24 16:24:10","title":"Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models","abstract":"To bridge the gap between vision and language modalities, Multimodal Large Language Models (MLLMs) usually learn an adapter that converts visual inputs to understandable tokens for Large Language Models (LLMs). However, most adapters generate consistent visual tokens, regardless of the specific objects of interest mentioned in the prompt. Since these adapters distribute equal attention to every detail in the image and focus on the entire scene, they may increase the cognitive load for LLMs, particularly when processing complex scenes. To alleviate this problem, we propose prompt-aware adapters. These adapters are designed with the capability to dynamically embed visual inputs based on the specific focus of the prompt. Specifically, prompt-aware adapters utilize both global and local textual features to capture the most relevant visual clues from the prompt at both coarse and fine granularity levels. This approach significantly enhances the ability of LLMs to understand and interpret visual content. Experiments on various visual question answering tasks, such as counting and position reasoning, demonstrate the effectiveness of prompt-aware adapters.","sentences":["To bridge the gap between vision and language modalities, Multimodal Large Language Models (MLLMs) usually learn an adapter that converts visual inputs to understandable tokens for Large Language Models (LLMs).","However, most adapters generate consistent visual tokens, regardless of the specific objects of interest mentioned in the prompt.","Since these adapters distribute equal attention to every detail in the image and focus on the entire scene, they may increase the cognitive load for LLMs, particularly when processing complex scenes.","To alleviate this problem, we propose prompt-aware adapters.","These adapters are designed with the capability to dynamically embed visual inputs based on the specific focus of the prompt.","Specifically, prompt-aware adapters utilize both global and local textual features to capture the most relevant visual clues from the prompt at both coarse and fine granularity levels.","This approach significantly enhances the ability of LLMs to understand and interpret visual content.","Experiments on various visual question answering tasks, such as counting and position reasoning, demonstrate the effectiveness of prompt-aware adapters."],"url":"http://arxiv.org/abs/2405.15684v1","category":"cs.CV"}
{"created":"2024-05-24 16:21:59","title":"VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging the Visual Perception Gap","abstract":"Recent interest in Large Vision-Language Models (LVLMs) for practical applications is moderated by the significant challenge of hallucination or the inconsistency between the factual information and the generated text. In this paper, we first perform an in-depth analysis of hallucinations and discover several novel insights about how and when LVLMs hallucinate. From our analysis, we show that: (1) The community's efforts have been primarily targeted towards reducing hallucinations related to visual recognition (VR) prompts (e.g., prompts that only require describing the image), thereby ignoring hallucinations for cognitive prompts (e.g., prompts that require additional skills like reasoning on contents of the image). (2) LVLMs lack visual perception, i.e., they can see but not necessarily understand or perceive the input image. We analyze responses to cognitive prompts and show that LVLMs hallucinate due to a perception gap: although LVLMs accurately recognize visual elements in the input image and possess sufficient cognitive skills, they struggle to respond accurately and hallucinate. To overcome this shortcoming, we propose Visual Description Grounded Decoding (VDGD), a simple, robust, and training-free method for alleviating hallucinations. Specifically, we first describe the image and add it as a prefix to the instruction. Next, during auto-regressive decoding, we sample from the plausible candidates according to their KL-Divergence (KLD) to the description, where lower KLD is given higher preference. Experimental results on several benchmarks and LVLMs show that VDGD improves significantly over other baselines in reducing hallucinations. We also propose VaLLu, a benchmark for the comprehensive evaluation of the cognitive capabilities of LVLMs.","sentences":["Recent interest in Large Vision-Language Models (LVLMs) for practical applications is moderated by the significant challenge of hallucination or the inconsistency between the factual information and the generated text.","In this paper, we first perform an in-depth analysis of hallucinations and discover several novel insights about how and when LVLMs hallucinate.","From our analysis, we show that: (1) The community's efforts have been primarily targeted towards reducing hallucinations related to visual recognition (VR) prompts (e.g., prompts that only require describing the image), thereby ignoring hallucinations for cognitive prompts (e.g., prompts that require additional skills like reasoning on contents of the image).","(2) LVLMs lack visual perception, i.e., they can see but not necessarily understand or perceive the input image.","We analyze responses to cognitive prompts and show that LVLMs hallucinate due to a perception gap: although LVLMs accurately recognize visual elements in the input image and possess sufficient cognitive skills, they struggle to respond accurately and hallucinate.","To overcome this shortcoming, we propose Visual Description Grounded Decoding (VDGD), a simple, robust, and training-free method for alleviating hallucinations.","Specifically, we first describe the image and add it as a prefix to the instruction.","Next, during auto-regressive decoding, we sample from the plausible candidates according to their KL-Divergence (KLD) to the description, where lower KLD is given higher preference.","Experimental results on several benchmarks and LVLMs show that VDGD improves significantly over other baselines in reducing hallucinations.","We also propose VaLLu, a benchmark for the comprehensive evaluation of the cognitive capabilities of LVLMs."],"url":"http://arxiv.org/abs/2405.15683v1","category":"cs.CV"}
{"created":"2024-05-24 16:20:46","title":"The Road Less Scheduled","abstract":"Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available (https://github.com/facebookresearch/schedule_free).","sentences":["Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T.","We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems.","Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum.","Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging.","An open source implementation of our method is available (https://github.com/facebookresearch/schedule_free)."],"url":"http://arxiv.org/abs/2405.15682v1","category":"cs.LG"}
{"created":"2024-05-24 16:19:03","title":"Jet Quenching of the Heavy Quarks in the Quark-Gluon Plasma and the Nonadditive Statistics","abstract":"Using the Plastino-Plastino (PP) equation, we calculate transport coefficients of the heavy-quarks traversing inside the quark-gluon plasma, and generalize their relationship with differential energy loss. The PP equation indicates anomalous diffusion of the probe particles and yields a quasi-exponential stationary distribution obtained also from the nonadditive statistics proposed by C. Tsallis. We estimate energy loss in a nonadditive quark-gluon medium, and calculate the jet-quenching parameter ($\\hat{q}$) for the PP dynamics. With the help of the estimate of $\\hat{q}$, we calculate the nuclear suppression factor ($R_{\\text{AA}}$) of the heavy-quarks passing through a nonadditive quark-gluon plasma using the model proposed by Dokshitzer and Kharzeev. In many a case, the parameters in the analysis are fixed from the experimental results to minimize arbitrariness. There is a good agreement between the theoretical calculation and experimental $R_{\\text{AA}}$ data, indicating that fast heavy-quarks may be subjected to anomalous diffusion inside the QGP.","sentences":["Using the Plastino-Plastino (PP) equation, we calculate transport coefficients of the heavy-quarks traversing inside the quark-gluon plasma, and generalize their relationship with differential energy loss.","The PP equation indicates anomalous diffusion of the probe particles and yields a quasi-exponential stationary distribution obtained also from the nonadditive statistics proposed by C. Tsallis.","We estimate energy loss in a nonadditive quark-gluon medium, and calculate the jet-quenching parameter ($\\hat{q}$) for the PP dynamics.","With the help of the estimate of $\\hat{q}$, we calculate the nuclear suppression factor ($R_{\\text{AA}}$) of the heavy-quarks passing through a nonadditive quark-gluon plasma using the model proposed by Dokshitzer and Kharzeev.","In many a case, the parameters in the analysis are fixed from the experimental results to minimize arbitrariness.","There is a good agreement between the theoretical calculation and experimental $R_{\\text{AA}}$ data, indicating that fast heavy-quarks may be subjected to anomalous diffusion inside the QGP."],"url":"http://arxiv.org/abs/2405.15679v1","category":"hep-ph"}
{"created":"2024-05-24 16:17:35","title":"SMART: Scalable Multi-agent Real-time Simulation via Next-token Prediction","abstract":"Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios. To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens. These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series. This GPT-style method allows the model to learn the motion distribution in real driving scenarios. SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed. Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.71 on the Sim Agents challenge. Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model's scalability. These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications. We have released all the code to promote the exploration of models for motion generation in the autonomous driving field.","sentences":["Data-driven autonomous driving motion generation tasks are frequently impacted by the limitations of dataset size and the domain gap between datasets, which precludes their extensive application in real-world scenarios.","To address this issue, we introduce SMART, a novel autonomous driving motion generation paradigm that models vectorized map and agent trajectory data into discrete sequence tokens.","These tokens are then processed through a decoder-only transformer architecture to train for the next token prediction task across spatial-temporal series.","This GPT-style method allows the model to learn the motion distribution in real driving scenarios.","SMART achieves state-of-the-art performance across most of the metrics on the generative Sim Agents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset (WOMD), demonstrating remarkable inference speed.","Moreover, SMART represents the generative model in the autonomous driving motion domain, exhibiting zero-shot generalization capabilities: Using only the NuPlan dataset for training and WOMD for validation, SMART achieved a competitive score of 0.71 on the Sim Agents challenge.","Lastly, we have collected over 1 billion motion tokens from multiple datasets, validating the model's scalability.","These results suggest that SMART has initially emulated two important properties: scalability and zero-shot generalization, and preliminarily meets the needs of large-scale real-time simulation applications.","We have released all the code to promote the exploration of models for motion generation in the autonomous driving field."],"url":"http://arxiv.org/abs/2405.15677v1","category":"cs.RO"}
{"created":"2024-05-24 16:16:56","title":"General type results for moduli of deformation generalised Kummer varieties","abstract":"In arXiv:1710.01672, we obtained general type results for orthogonal modular varieties associated with moduli spaces of compact hyperk\\\"ahler manifolds of deformation generalised Kummer type (also known as 'deformation generalised Kummer varieties'). The orthogonal modular varieties were defined in terms of an integer 2d, corresponding to the degree of polarisation of the associated hyperk\\\"ahler manifolds. In arXiv:1710.01672, we showed these orthogonal modular varieties are of general type when 2d is square-free and sufficiently large. Here we show that the square-free condition can be removed.","sentences":["In arXiv:1710.01672, we obtained general type results for orthogonal modular varieties associated with moduli spaces of compact hyperk\\\"ahler manifolds of deformation generalised Kummer type (also known as 'deformation generalised Kummer varieties').","The orthogonal modular varieties were defined in terms of an integer 2d, corresponding to the degree of polarisation of the associated hyperk\\\"ahler manifolds.","In arXiv:1710.01672, we showed these orthogonal modular varieties are of general type when 2d is square-free and sufficiently large.","Here we show that the square-free condition can be removed."],"url":"http://arxiv.org/abs/2405.15675v1","category":"math.AG"}
{"created":"2024-05-24 16:12:39","title":"Consistency of Neural Causal Partial Identification","abstract":"Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022]. However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models. In this work, we prove consistency of partial identification via NCMs in a general setting with both continuous and categorical variables. Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase. In particular, we provide a counterexample showing that without Lipschitz regularization the NCM may not be asymptotically consistent. Our results are enabled by new results on the approximability of structural causal models via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds.","sentences":["Recent progress in Neural Causal Models (NCMs) showcased how identification and partial identification of causal effects can be automatically carried out via training of neural generative models that respect the constraints encoded in a given causal graph [Xia et al. 2022, Balazadeh et al. 2022].","However, formal consistency of these methods has only been proven for the case of discrete variables or only for linear causal models.","In this work, we prove consistency of partial identification via NCMs in a general setting with both continuous and categorical variables.","Further, our results highlight the impact of the design of the underlying neural network architecture in terms of depth and connectivity as well as the importance of applying Lipschitz regularization in the training phase.","In particular, we provide a counterexample showing that without Lipschitz regularization the NCM may not be asymptotically consistent.","Our results are enabled by new results on the approximability of structural causal models via neural generative models, together with an analysis of the sample complexity of the resulting architectures and how that translates into an error in the constrained optimization problem that defines the partial identification bounds."],"url":"http://arxiv.org/abs/2405.15673v1","category":"cs.LG"}
{"created":"2024-05-24 16:09:46","title":"Serving economic prosperity: economic impact assessments (EIA) on Earth observation-based services and tools by SERVIR","abstract":"In an era where informed decision-making is paramount for sustainable development and effective resource management, the role of Earth observations (EO) in shaping economic landscapes cannot be overstated. EO, facilitated by satellites, sensors, and data analytics, is a cornerstone for evidence-based policymaking, risk mitigation, and resource allocation. SERVIR is a joint initiative of US Agency for International Development and NASA. This paper presents a comprehensive survey of relevant economic impact assessment (EIA) work, summarizes SERVIRs potential interests in EIA, and identifies how and where EIA could improve how SERVIR quantifies and communicates the impact of its services.","sentences":["In an era where informed decision-making is paramount for sustainable development and effective resource management, the role of Earth observations (EO) in shaping economic landscapes cannot be overstated.","EO, facilitated by satellites, sensors, and data analytics, is a cornerstone for evidence-based policymaking, risk mitigation, and resource allocation.","SERVIR is a joint initiative of US Agency for International Development and NASA.","This paper presents a comprehensive survey of relevant economic impact assessment (EIA) work, summarizes SERVIRs potential interests in EIA, and identifies how and where EIA could improve how SERVIR quantifies and communicates the impact of its services."],"url":"http://arxiv.org/abs/2405.15672v1","category":"econ.GN"}
{"created":"2024-05-24 16:06:19","title":"Post-selection inference for quantifying uncertainty in changes in variance","abstract":"Quantifying uncertainty in detected changepoints is an important problem. However it is challenging as the naive approach would use the data twice, first to detect the changes, and then to test them. This will bias the test, and can lead to anti-conservative p-values. One approach to avoid this is to use ideas from post-selection inference, which conditions on the information in the data used to choose which changes to test. As a result this produces valid p-values; that is, p-values that have a uniform distribution if there is no change. Currently such methods have been developed for detecting changes in mean only. This paper presents two approaches for constructing post-selection p-values for detecting changes in variance. These vary depending on the method use to detect the changes, but are general in terms of being applicable for a range of change-detection methods and a range of hypotheses that we may wish to test.","sentences":["Quantifying uncertainty in detected changepoints is an important problem.","However it is challenging as the naive approach would use the data twice, first to detect the changes, and then to test them.","This will bias the test, and can lead to anti-conservative p-values.","One approach to avoid this is to use ideas from post-selection inference, which conditions on the information in the data used to choose which changes to test.","As a result this produces valid p-values; that is, p-values that have a uniform distribution if there is no change.","Currently such methods have been developed for detecting changes in mean only.","This paper presents two approaches for constructing post-selection p-values for detecting changes in variance.","These vary depending on the method use to detect the changes, but are general in terms of being applicable for a range of change-detection methods and a range of hypotheses that we may wish to test."],"url":"http://arxiv.org/abs/2405.15670v1","category":"stat.ME"}
{"created":"2024-05-24 16:05:15","title":"What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models","abstract":"Large language models (LLMs) has been effectively used for many computer vision tasks, including image classification. In this paper, we present a simple yet effective approach for zero-shot image classification using multimodal LLMs. By employing multimodal LLMs, we generate comprehensive textual representations from input images. These textual representations are then utilized to generate fixed-dimensional features in a cross-modal embedding space. Subsequently, these features are fused together to perform zero-shot classification using a linear classifier. Our method does not require prompt engineering for each dataset; instead, we use a single, straightforward, set of prompts across all datasets. We evaluated our method on several datasets, and our results demonstrate its remarkable effectiveness, surpassing benchmark accuracy on multiple datasets. On average over ten benchmarks, our method achieved an accuracy gain of 4.1 percentage points, with an increase of 6.8 percentage points on the ImageNet dataset, compared to prior methods. Our findings highlight the potential of multimodal LLMs to enhance computer vision tasks such as zero-shot image classification, offering a significant improvement over traditional methods.","sentences":["Large language models (LLMs) has been effectively used for many computer vision tasks, including image classification.","In this paper, we present a simple yet effective approach for zero-shot image classification using multimodal LLMs.","By employing multimodal LLMs, we generate comprehensive textual representations from input images.","These textual representations are then utilized to generate fixed-dimensional features in a cross-modal embedding space.","Subsequently, these features are fused together to perform zero-shot classification using a linear classifier.","Our method does not require prompt engineering for each dataset; instead, we use a single, straightforward, set of prompts across all datasets.","We evaluated our method on several datasets, and our results demonstrate its remarkable effectiveness, surpassing benchmark accuracy on multiple datasets.","On average over ten benchmarks, our method achieved an accuracy gain of 4.1 percentage points, with an increase of 6.8 percentage points on the ImageNet dataset, compared to prior methods.","Our findings highlight the potential of multimodal LLMs to enhance computer vision tasks such as zero-shot image classification, offering a significant improvement over traditional methods."],"url":"http://arxiv.org/abs/2405.15668v1","category":"cs.CV"}
{"created":"2024-05-24 15:58:02","title":"Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables","abstract":"The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key issue: the use of 'shortcuts' by models. Such shortcuts are easy-to-learn patterns from the training data that fail to generalise to new data. Examples include the use of a copyright watermark to recognise horses, snowy background to recognise huskies, or ink markings to detect malignant skin lesions. The explainable AI (XAI) community has suggested using instance-level explanations to detect shortcuts without external data, but this requires the examination of many explanations to confirm the presence of such shortcuts, making it a labour-intensive process. To address these challenges, we introduce Counterfactual Frequency (CoF) tables, a novel approach that aggregates instance-based explanations into global insights, and exposes shortcuts. The aggregation implies the need for some semantic concepts to be used in the explanations, which we solve by labelling the segments of an image. We demonstrate the utility of CoF tables across several datasets, revealing the shortcuts learned from them.","sentences":["The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key issue: the use of 'shortcuts' by models.","Such shortcuts are easy-to-learn patterns from the training data that fail to generalise to new data.","Examples include the use of a copyright watermark to recognise horses, snowy background to recognise huskies, or ink markings to detect malignant skin lesions.","The explainable AI (XAI) community has suggested using instance-level explanations to detect shortcuts without external data, but this requires the examination of many explanations to confirm the presence of such shortcuts, making it a labour-intensive process.","To address these challenges, we introduce Counterfactual Frequency (CoF) tables, a novel approach that aggregates instance-based explanations into global insights, and exposes shortcuts.","The aggregation implies the need for some semantic concepts to be used in the explanations, which we solve by labelling the segments of an image.","We demonstrate the utility of CoF tables across several datasets, revealing the shortcuts learned from them."],"url":"http://arxiv.org/abs/2405.15661v1","category":"cs.CV"}
{"created":"2024-05-24 15:54:22","title":"Spin-down of a pulsar with a yielding crust","abstract":"In light of the discovery of the long-period radio pulsar PSR J0901-4046, it is interesting to revisit a question about how the magnetized neutron star slows down its rotation. In the case of a weak or liquid outer crust, the mechanism of spin-down becomes unclear because the braking stress cannot then be transmitted from the surface to the main bulk of the star. We show that even if the outer crust does not withstand the surface electromagnetic forces creating the braking torque, the stellar spin-down does not stop, and the matter rearranges so that the necessary electromagnetic forces form in more deep and rigid layers capable of withstanding these forces. The spin-down rate remains the same and corresponds to the transformation of the rotational energy of the neutron star into the energy of the generated relativistic electron-positron plasma. The solid iron surface of ultrastrongly magnetized PSR J0901-4046 appears not to be yielding and withstands the braking stress without breaking.","sentences":["In light of the discovery of the long-period radio pulsar PSR J0901-4046, it is interesting to revisit a question about how the magnetized neutron star slows down its rotation.","In the case of a weak or liquid outer crust, the mechanism of spin-down becomes unclear because the braking stress cannot then be transmitted from the surface to the main bulk of the star.","We show that even if the outer crust does not withstand the surface electromagnetic forces creating the braking torque, the stellar spin-down does not stop, and the matter rearranges so that the necessary electromagnetic forces form in more deep and rigid layers capable of withstanding these forces.","The spin-down rate remains the same and corresponds to the transformation of the rotational energy of the neutron star into the energy of the generated relativistic electron-positron plasma.","The solid iron surface of ultrastrongly magnetized PSR J0901-4046 appears not to be yielding and withstands the braking stress without breaking."],"url":"http://arxiv.org/abs/2405.15659v1","category":"astro-ph.HE"}
{"created":"2024-05-24 15:53:59","title":"HDC: Hierarchical Semantic Decoding with Counting Assistance for Generalized Referring Expression Segmentation","abstract":"The newly proposed Generalized Referring Expression Segmentation (GRES) amplifies the formulation of classic RES by involving multiple/non-target scenarios. Recent approaches focus on optimizing the last modality-fused feature which is directly utilized for segmentation and object-existence identification. However, the attempt to integrate all-grained information into a single joint representation is impractical in GRES due to the increased complexity of the spatial relationships among instances and deceptive text descriptions. Furthermore, the subsequent binary target justification across all referent scenarios fails to specify their inherent differences, leading to ambiguity in object understanding. To address the weakness, we propose a $\\textbf{H}$ierarchical Semantic $\\textbf{D}$ecoding with $\\textbf{C}$ounting Assistance framework (HDC). It hierarchically transfers complementary modality information across granularities, and then aggregates each well-aligned semantic correspondence for multi-level decoding. Moreover, with complete semantic context modeling, we endow HDC with explicit counting capability to facilitate comprehensive object perception in multiple/single/non-target settings. Experimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarks demonstrate the effectiveness and rationality of HDC which outperforms the state-of-the-art GRES methods by a remarkable margin. Code will be available $\\href{https://github.com/RobertLuo1/HDC}{here}$.","sentences":["The newly proposed Generalized Referring Expression Segmentation (GRES) amplifies the formulation of classic RES by involving multiple/non-target scenarios.","Recent approaches focus on optimizing the last modality-fused feature which is directly utilized for segmentation and object-existence identification.","However, the attempt to integrate all-grained information into a single joint representation is impractical in GRES due to the increased complexity of the spatial relationships among instances and deceptive text descriptions.","Furthermore, the subsequent binary target justification across all referent scenarios fails to specify their inherent differences, leading to ambiguity in object understanding.","To address the weakness, we propose a $\\textbf{H}$ierarchical Semantic $\\textbf{D}$ecoding with $\\textbf{C}$ounting Assistance framework (HDC).","It hierarchically transfers complementary modality information across granularities, and then aggregates each well-aligned semantic correspondence for multi-level decoding.","Moreover, with complete semantic context modeling, we endow HDC with explicit counting capability to facilitate comprehensive object perception in multiple/single/non-target settings.","Experimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarks demonstrate the effectiveness and rationality of HDC which outperforms the state-of-the-art GRES methods by a remarkable margin.","Code will be available $\\href{https://github.com/RobertLuo1/HDC}{here}$."],"url":"http://arxiv.org/abs/2405.15658v1","category":"cs.CV"}
{"created":"2024-05-24 15:51:32","title":"Balanced truncation with conformal maps","abstract":"We consider the problem of constructing reduced models for large scale systems with poles in general domains in the complex plane (as opposed to, e.g., the open left-half plane or the open unit disk). Our goal is to design a model reduction scheme, building upon theoretically established methodologies, yet encompassing this new class of models. To this aim, we develop a balanced truncation framework through conformal maps to handle poles in general domains. The major difference from classical balanced truncation resides in the formulation of the Gramians. We show that these new Gramians can still be computed by solving modified Lyapunov equations for specific conformal maps. A numerical algorithm to perform balanced truncation with conformal maps is developed and is tested on three numerical examples, namely a heat model, the Schr\\\"odinger equation, and the undamped linear wave equation, the latter two having spectra on the imaginary axis.","sentences":["We consider the problem of constructing reduced models for large scale systems with poles in general domains in the complex plane (as opposed to, e.g., the open left-half plane or the open unit disk).","Our goal is to design a model reduction scheme, building upon theoretically established methodologies, yet encompassing this new class of models.","To this aim, we develop a balanced truncation framework through conformal maps to handle poles in general domains.","The major difference from classical balanced truncation resides in the formulation of the Gramians.","We show that these new Gramians can still be computed by solving modified Lyapunov equations for specific conformal maps.","A numerical algorithm to perform balanced truncation with conformal maps is developed and is tested on three numerical examples, namely a heat model, the Schr\\\"odinger equation, and the undamped linear wave equation, the latter two having spectra on the imaginary axis."],"url":"http://arxiv.org/abs/2405.15656v1","category":"math.NA"}
{"created":"2024-05-24 15:49:00","title":"HiddenSpeaker: Generate Imperceptible Unlearnable Audios for Speaker Verification System","abstract":"In recent years, the remarkable advancements in deep neural networks have brought tremendous convenience. However, the training process of a highly effective model necessitates a substantial quantity of samples, which brings huge potential threats, like unauthorized exploitation with privacy leakage. In response, we propose a framework named HiddenSpeaker, embedding imperceptible perturbations within the training speech samples and rendering them unlearnable for deep-learning-based speaker verification systems that employ large-scale speakers for efficient training. The HiddenSpeaker utilizes a simplified error-minimizing method named Single-Level Error-Minimizing (SLEM) to generate specific and effective perturbations. Additionally, a hybrid objective function is employed for human perceptual optimization, ensuring the perturbation is indistinguishable from human listeners. We conduct extensive experiments on multiple state-of-the-art (SOTA) models in the speaker verification domain to evaluate HiddenSpeaker. Our results demonstrate that HiddenSpeaker not only deceives the model with unlearnable samples but also enhances the imperceptibility of the perturbations, showcasing strong transferability across different models.","sentences":["In recent years, the remarkable advancements in deep neural networks have brought tremendous convenience.","However, the training process of a highly effective model necessitates a substantial quantity of samples, which brings huge potential threats, like unauthorized exploitation with privacy leakage.","In response, we propose a framework named HiddenSpeaker, embedding imperceptible perturbations within the training speech samples and rendering them unlearnable for deep-learning-based speaker verification systems that employ large-scale speakers for efficient training.","The HiddenSpeaker utilizes a simplified error-minimizing method named Single-Level Error-Minimizing (SLEM) to generate specific and effective perturbations.","Additionally, a hybrid objective function is employed for human perceptual optimization, ensuring the perturbation is indistinguishable from human listeners.","We conduct extensive experiments on multiple state-of-the-art (SOTA) models in the speaker verification domain to evaluate HiddenSpeaker.","Our results demonstrate that HiddenSpeaker not only deceives the model with unlearnable samples but also enhances the imperceptibility of the perturbations, showcasing strong transferability across different models."],"url":"http://arxiv.org/abs/2405.15655v1","category":"cs.SD"}
{"created":"2024-05-24 15:48:02","title":"Fourier--Stieltjes category for twisted groupoid actions","abstract":"We extend the theory of Fourier--Stieltjes algebras to the category of twisted actions by \\'etale groupoids on arbitrary C*-bundles, generalizing theories constructed previously by B\\'{e}dos and Conti for twisted group actions on unital C*-algebras, and by Renault and others for groupoid C*-algebras, in each case motivated by the classical theory of Fourier--Stieltjes algebras of discrete groups. To this end we develop a toolbox including, among other things, a theory of multiplier C*-correspondences, multiplier C*-correspondence bundles, Busby--Smith twisted groupoid actions, and the associated crossed products, equivariant representations and Fell's absorption theorems. For a fixed \\'etale groupoid $G$ a Fourier--Stieltjes multiplier is a family of maps acting on fibers, arising from an equivariant representation. It corresponds to a certain fiber-preserving strict completely bounded map between twisted full (or reduced) crossed products. We establish a KSGNS-type dilation result which shows that the correspondence above restricts to a bijection between positive-definite multipliers and a particular class of completely positive maps. Further we introduce a subclass of Fourier multipliers, that enjoys a natural absorption property with respect to Fourier--Stieltjes multipliers and gives rise to `reduced to full' multiplier maps on crossed products. Finally we provide several applications of the theory developed, for example to the approximation properties, such as weak containment or nuclearity, of the crossed products and actions in question, and discuss outstanding open problems.","sentences":["We extend the theory of Fourier--Stieltjes algebras to the category of twisted actions by \\'etale groupoids on arbitrary C*-bundles, generalizing theories constructed previously by B\\'{e}dos and Conti for twisted group actions on unital C*-algebras, and by Renault and others for groupoid C*-algebras, in each case motivated by the classical theory of Fourier--Stieltjes algebras of discrete groups.","To this end we develop a toolbox including, among other things, a theory of multiplier C*-correspondences, multiplier C*-correspondence bundles, Busby--Smith twisted groupoid actions, and the associated crossed products, equivariant representations and Fell's absorption theorems.","For a fixed \\'etale groupoid $G$ a Fourier--Stieltjes multiplier is a family of maps acting on fibers, arising from an equivariant representation.","It corresponds to a certain fiber-preserving strict completely bounded map between twisted full (or reduced) crossed products.","We establish a KSGNS-type dilation result which shows that the correspondence above restricts to a bijection between positive-definite multipliers and a particular class of completely positive maps.","Further we introduce a subclass of Fourier multipliers, that enjoys a natural absorption property with respect to Fourier--Stieltjes multipliers and gives rise to `reduced to full' multiplier maps on crossed products.","Finally we provide several applications of the theory developed, for example to the approximation properties, such as weak containment or nuclearity, of the crossed products and actions in question, and discuss outstanding open problems."],"url":"http://arxiv.org/abs/2405.15653v1","category":"math.OA"}
{"created":"2024-05-24 15:47:35","title":"$$\\mathbf{L^2\\cdot M = C^2}$$ Large Language Models as Covert Channels... a Systematic Analysis","abstract":"Large Language Models (LLMs) have gained significant popularity in the last few years due to their performance in diverse tasks such as translation, prediction, or content generation. At the same time, the research community has shown that LLMs are susceptible to various attacks but can also improve the security of diverse systems. However, besides enabling more secure systems, how well do open source LLMs behave as covertext distributions to, e.g., facilitate censorship resistant communication?   In this paper, we explore the capabilities of open-source LLM-based covert channels. We approach this problem from the experimental side by empirically measuring the security vs. capacity of the open-source LLM model (Llama-7B) to assess how well it performs as a covert channel. Although our results indicate that such channels are not likely to achieve high practical bitrates, which depend on message length and model entropy, we also show that the chance for an adversary to detect covert communication is low. To ensure that our results can be used with the least effort as a general reference, we employ a conceptually simple and concise scheme and only assume public models.","sentences":["Large Language Models (LLMs) have gained significant popularity in the last few years due to their performance in diverse tasks such as translation, prediction, or content generation.","At the same time, the research community has shown that LLMs are susceptible to various attacks but can also improve the security of diverse systems.","However, besides enabling more secure systems, how well do open source LLMs behave as covertext distributions to, e.g., facilitate censorship resistant communication?   ","In this paper, we explore the capabilities of open-source LLM-based covert channels.","We approach this problem from the experimental side by empirically measuring the security vs. capacity of the open-source LLM model (Llama-7B) to assess how well it performs as a covert channel.","Although our results indicate that such channels are not likely to achieve high practical bitrates, which depend on message length and model entropy, we also show that the chance for an adversary to detect covert communication is low.","To ensure that our results can be used with the least effort as a general reference, we employ a conceptually simple and concise scheme and only assume public models."],"url":"http://arxiv.org/abs/2405.15652v1","category":"cs.CR"}
{"created":"2024-05-24 15:47:00","title":"Analytical proxy to families of numerical solutions: the case study of spherical mini-boson stars","abstract":"The Einstein field equations, or generalizations thereof, are difficult to solve analytically. On the other hand, numerical solutions of the same equations have become increasingly common, in particular concerning compact objects. Whereas analytic approximations to each individual solution within a numerical family have been proposed, proxies for whole families are missing, which can facilitate studying properties across the parameter space, data compression and a wider usage of such solutions. In this work we tackle this need, proposing a simple strategy based on a double expansion of the unknown functions in an appropriately chosen basis, to build such proxy. We use as an exploratory case-study spherical, fundamental mini-boson stars, to illustrate the feasibility of such an approach, emphasise its advantage in reducing the data size, and the challenges, say, in covering large parameter spaces.","sentences":["The Einstein field equations, or generalizations thereof, are difficult to solve analytically.","On the other hand, numerical solutions of the same equations have become increasingly common, in particular concerning compact objects.","Whereas analytic approximations to each individual solution within a numerical family have been proposed, proxies for whole families are missing, which can facilitate studying properties across the parameter space, data compression and a wider usage of such solutions.","In this work we tackle this need, proposing a simple strategy based on a double expansion of the unknown functions in an appropriately chosen basis, to build such proxy.","We use as an exploratory case-study spherical, fundamental mini-boson stars, to illustrate the feasibility of such an approach, emphasise its advantage in reducing the data size, and the challenges, say, in covering large parameter spaces."],"url":"http://arxiv.org/abs/2405.15651v1","category":"gr-qc"}
{"created":"2024-05-24 15:35:49","title":"LLM-based Robot Task Planning with Exceptional Handling for General Purpose Service Robots","abstract":"The development of a general purpose service robot for daily life necessitates the robot's ability to deploy a myriad of fundamental behaviors judiciously. Recent advancements in training Large Language Models (LLMs) can be used to generate action sequences directly, given an instruction in natural language with no additional domain information. However, while the outputs of LLMs are semantically correct, the generated task plans may not accurately map to acceptable actions and might encompass various linguistic ambiguities. LLM hallucinations pose another challenge for robot task planning, which results in content that is inconsistent with real-world facts or user inputs. In this paper, we propose a task planning method based on a constrained LLM prompt scheme, which can generate an executable action sequence from a command. An exceptional handling module is further proposed to deal with LLM hallucinations problem. This module can ensure the LLM-generated results are admissible in the current environment. We evaluate our method on the commands generated by the RoboCup@Home Command Generator, observing that the robot demonstrates exceptional performance in both comprehending instructions and executing tasks.","sentences":["The development of a general purpose service robot for daily life necessitates the robot's ability to deploy a myriad of fundamental behaviors judiciously.","Recent advancements in training Large Language Models (LLMs) can be used to generate action sequences directly, given an instruction in natural language with no additional domain information.","However, while the outputs of LLMs are semantically correct, the generated task plans may not accurately map to acceptable actions and might encompass various linguistic ambiguities.","LLM hallucinations pose another challenge for robot task planning, which results in content that is inconsistent with real-world facts or user inputs.","In this paper, we propose a task planning method based on a constrained LLM prompt scheme, which can generate an executable action sequence from a command.","An exceptional handling module is further proposed to deal with LLM hallucinations problem.","This module can ensure the LLM-generated results are admissible in the current environment.","We evaluate our method on the commands generated by the RoboCup@Home Command Generator, observing that the robot demonstrates exceptional performance in both comprehending instructions and executing tasks."],"url":"http://arxiv.org/abs/2405.15646v1","category":"cs.RO"}
{"created":"2024-05-24 15:33:27","title":"Reducing the cost of posterior sampling in linear inverse problems via task-dependent score learning","abstract":"Score-based diffusion models (SDMs) offer a flexible approach to sample from the posterior distribution in a variety of Bayesian inverse problems. In the literature, the prior score is utilized to sample from the posterior by different methods that require multiple evaluations of the forward mapping in order to generate a single posterior sample. These methods are often designed with the objective of enabling the direct use of the unconditional prior score and, therefore, task-independent training. In this paper, we focus on linear inverse problems, when evaluation of the forward mapping is computationally expensive and frequent posterior sampling is required for new measurement data, such as in medical imaging. We demonstrate that the evaluation of the forward mapping can be entirely bypassed during posterior sample generation. Instead, without introducing any error, the computational effort can be shifted to an offline task of training the score of a specific diffusion-like random process. In particular, the training is task-dependent requiring information about the forward mapping but not about the measurement data. It is shown that the conditional score corresponding to the posterior can be obtained from the auxiliary score by suitable affine transformations. We prove that this observation generalizes to the framework of infinite-dimensional diffusion models introduced recently and provide numerical analysis of the method. Moreover, we validate our findings with numerical experiments.","sentences":["Score-based diffusion models (SDMs) offer a flexible approach to sample from the posterior distribution in a variety of Bayesian inverse problems.","In the literature, the prior score is utilized to sample from the posterior by different methods that require multiple evaluations of the forward mapping in order to generate a single posterior sample.","These methods are often designed with the objective of enabling the direct use of the unconditional prior score and, therefore, task-independent training.","In this paper, we focus on linear inverse problems, when evaluation of the forward mapping is computationally expensive and frequent posterior sampling is required for new measurement data, such as in medical imaging.","We demonstrate that the evaluation of the forward mapping can be entirely bypassed during posterior sample generation.","Instead, without introducing any error, the computational effort can be shifted to an offline task of training the score of a specific diffusion-like random process.","In particular, the training is task-dependent requiring information about the forward mapping but not about the measurement data.","It is shown that the conditional score corresponding to the posterior can be obtained from the auxiliary score by suitable affine transformations.","We prove that this observation generalizes to the framework of infinite-dimensional diffusion models introduced recently and provide numerical analysis of the method.","Moreover, we validate our findings with numerical experiments."],"url":"http://arxiv.org/abs/2405.15643v1","category":"stat.ML"}
{"created":"2024-05-24 15:33:08","title":"Effective Confidence Region Prediction Using Probability Forecasters","abstract":"Confidence region prediction is a practically useful extension to the commonly studied pattern recognition problem. Instead of predicting a single label, the constraint is relaxed to allow prediction of a subset of labels given a desired confidence level 1-delta. Ideally, effective region predictions should be (1) well calibrated - predictive regions at confidence level 1-delta should err with relative frequency at most delta and (2) be as narrow (or certain) as possible. We present a simple technique to generate confidence region predictions from conditional probability estimates (probability forecasts). We use this 'conversion' technique to generate confidence region predictions from probability forecasts output by standard machine learning algorithms when tested on 15 multi-class datasets. Our results show that approximately 44% of experiments demonstrate well-calibrated confidence region predictions, with the K-Nearest Neighbour algorithm tending to perform consistently well across all data. Our results illustrate the practical benefits of effective confidence region prediction with respect to medical diagnostics, where guarantees of capturing the true disease label can be given.","sentences":["Confidence region prediction is a practically useful extension to the commonly studied pattern recognition problem.","Instead of predicting a single label, the constraint is relaxed to allow prediction of a subset of labels given a desired confidence level 1-delta.","Ideally, effective region predictions should be (1) well calibrated - predictive regions at confidence level 1-delta should err with relative frequency at most delta and (2) be as narrow (or certain) as possible.","We present a simple technique to generate confidence region predictions from conditional probability estimates (probability forecasts).","We use this 'conversion' technique to generate confidence region predictions from probability forecasts output by standard machine learning algorithms when tested on 15 multi-class datasets.","Our results show that approximately 44% of experiments demonstrate well-calibrated confidence region predictions, with the K-Nearest Neighbour algorithm tending to perform consistently well across all data.","Our results illustrate the practical benefits of effective confidence region prediction with respect to medical diagnostics, where guarantees of capturing the true disease label can be given."],"url":"http://arxiv.org/abs/2405.15642v1","category":"cs.LG"}
{"created":"2024-05-24 15:32:01","title":"Predictive Uncertainty Quantification with Missing Covariates","abstract":"Predictive uncertainty quantification is crucial in decision-making problems. We investigate how to adequately quantify predictive uncertainty with missing covariates. A bottleneck is that missing values induce heteroskedasticity on the response's predictive distribution given the observed covariates. Thus, we focus on building predictive sets for the response that are valid conditionally to the missing values pattern. We show that this goal is impossible to achieve informatively in a distribution-free fashion, and we propose useful restrictions on the distribution class. Motivated by these hardness results, we characterize how missing values and predictive uncertainty intertwine. Particularly, we rigorously formalize the idea that the more missing values, the higher the predictive uncertainty. Then, we introduce a generalized framework, coined CP-MDA-Nested*, outputting predictive sets in both regression and classification. Under independence between the missing value pattern and both the features and the response (an assumption justified by our hardness results), these predictive sets are valid conditionally to any pattern of missing values. Moreover, it provides great flexibility in the trade-off between statistical variability and efficiency. Finally, we experimentally assess the performances of CP-MDA-Nested* beyond its scope of theoretical validity, demonstrating promising outcomes in more challenging configurations than independence.","sentences":["Predictive uncertainty quantification is crucial in decision-making problems.","We investigate how to adequately quantify predictive uncertainty with missing covariates.","A bottleneck is that missing values induce heteroskedasticity on the response's predictive distribution given the observed covariates.","Thus, we focus on building predictive sets for the response that are valid conditionally to the missing values pattern.","We show that this goal is impossible to achieve informatively in a distribution-free fashion, and we propose useful restrictions on the distribution class.","Motivated by these hardness results, we characterize how missing values and predictive uncertainty intertwine.","Particularly, we rigorously formalize the idea that the more missing values, the higher the predictive uncertainty.","Then, we introduce a generalized framework, coined CP-MDA-Nested*, outputting predictive sets in both regression and classification.","Under independence between the missing value pattern and both the features and the response (an assumption justified by our hardness results), these predictive sets are valid conditionally to any pattern of missing values.","Moreover, it provides great flexibility in the trade-off between statistical variability and efficiency.","Finally, we experimentally assess the performances of CP-MDA-Nested* beyond its scope of theoretical validity, demonstrating promising outcomes in more challenging configurations than independence."],"url":"http://arxiv.org/abs/2405.15641v1","category":"stat.ME"}
{"created":"2024-05-24 15:30:41","title":"GECKO: Generative Language Model for English, Code and Korean","abstract":"We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages. GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture. In this report, we share the experiences of several efforts to build a better data pipeline for the corpus and to train our model. GECKO shows great efficiency in token generations for both Korean and English, despite its small size of vocabulary. We measure the performance on the representative benchmarks in terms of Korean, English and Code, and it exhibits great performance on KMMLU (Korean MMLU) and modest performance in English and Code, even with its smaller number of trained tokens compared to English-focused LLMs. GECKO is available to the open-source community under a permissive license. We hope our work offers a research baseline and practical insights for Korean LLM research. The model can be found at: https://huggingface.co/kifai/GECKO-7B","sentences":["We introduce GECKO, a bilingual large language model (LLM) optimized for Korean and English, along with programming languages.","GECKO is pretrained on the balanced, high-quality corpus of Korean and English employing LLaMA architecture.","In this report, we share the experiences of several efforts to build a better data pipeline for the corpus and to train our model.","GECKO shows great efficiency in token generations for both Korean and English, despite its small size of vocabulary.","We measure the performance on the representative benchmarks in terms of Korean, English and Code, and it exhibits great performance on KMMLU (Korean MMLU) and modest performance in English and Code, even with its smaller number of trained tokens compared to English-focused LLMs.","GECKO is available to the open-source community under a permissive license.","We hope our work offers a research baseline and practical insights for Korean LLM research.","The model can be found at: https://huggingface.co/kifai/GECKO-7B"],"url":"http://arxiv.org/abs/2405.15640v1","category":"cs.CL"}
{"created":"2024-05-24 15:25:28","title":"M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models","abstract":"Multilingual multimodal reasoning is a core component in achieving human-level intelligence. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German. Using M4U, we conduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results show that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German. We believe that M4U can serve as a crucial tool for systematically evaluating LMMs based on their multilingual multimodal reasoning capabilities and monitoring their development. The homepage, codes and data are public available.","sentences":["Multilingual multimodal reasoning is a core component in achieving human-level intelligence.","However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores.","This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored.","In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning.","M4U contains 8,931 samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in Chinese, English, and German.","Using M4U, we conduct extensive evaluations of 21 leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools.","The evaluation results show that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences.","Our in-depth analysis indicates that leading LMMs, including GPT-4o, suffer performance degradation when prompted with cross-lingual multimodal questions, such as images with key textual information in Chinese while the question is in German.","We believe that M4U can serve as a crucial tool for systematically evaluating LMMs based on their multilingual multimodal reasoning capabilities and monitoring their development.","The homepage, codes and data are public available."],"url":"http://arxiv.org/abs/2405.15638v1","category":"cs.CV"}
{"created":"2024-05-24 15:22:58","title":"Visualize and Paint GAN Activations","abstract":"We investigate how generated structures of GANs correlate with their activations in hidden layers, with the purpose of better understanding the inner workings of those models and being able to paint structures with unconditionally trained GANs. This gives us more control over the generated images, allowing to generate them from a semantic segmentation map while not requiring such a segmentation in the training data. To this end we introduce the concept of tileable features, allowing us to identify activations that work well for painting.","sentences":["We investigate how generated structures of GANs correlate with their activations in hidden layers, with the purpose of better understanding the inner workings of those models and being able to paint structures with unconditionally trained GANs.","This gives us more control over the generated images, allowing to generate them from a semantic segmentation map while not requiring such a segmentation in the training data.","To this end we introduce the concept of tileable features, allowing us to identify activations that work well for painting."],"url":"http://arxiv.org/abs/2405.15636v1","category":"cs.CV"}
{"created":"2024-05-24 15:21:14","title":"Taut foliations and contact pairs in dimension three","abstract":"We present a new construction of codimension-one foliations from pairs of contact structures in dimension three. This constitutes a converse result to a celebrated theorem of Eliashberg and Thurston on approximations of foliations by contact structures. Under suitable hypotheses on the initial contact pairs, the foliations we construct are taut, allowing us to characterize taut foliations entirely in terms of contact geometry. This viewpoint reveals some surprising flexible phenomena for taut foliations, and provides new insight into the $L$-space conjecture.   The first part of the proof builds upon the work on Colin and Firmo on positive contact pairs. The second part involves a wide generalization of a technical result of Burago and Ivanov on the construction of branching foliations tangent to continuous plane fields, and might be of independent interest.","sentences":["We present a new construction of codimension-one foliations from pairs of contact structures in dimension three.","This constitutes a converse result to a celebrated theorem of Eliashberg and Thurston on approximations of foliations by contact structures.","Under suitable hypotheses on the initial contact pairs, the foliations we construct are taut, allowing us to characterize taut foliations entirely in terms of contact geometry.","This viewpoint reveals some surprising flexible phenomena for taut foliations, and provides new insight into the $L$-space conjecture.   ","The first part of the proof builds upon the work on Colin and Firmo on positive contact pairs.","The second part involves a wide generalization of a technical result of Burago and Ivanov on the construction of branching foliations tangent to continuous plane fields, and might be of independent interest."],"url":"http://arxiv.org/abs/2405.15635v1","category":"math.SG"}
{"created":"2024-05-24 15:18:27","title":"Less is more: Summarizing Patch Tokens for efficient Multi-Label Class-Incremental Learning","abstract":"Prompt tuning has emerged as an effective rehearsal-free technique for class-incremental learning (CIL) that learns a tiny set of task-specific parameters (or prompts) to instruct a pre-trained transformer to learn on a sequence of tasks. Albeit effective, prompt tuning methods do not lend well in the multi-label class incremental learning (MLCIL) scenario (where an image contains multiple foreground classes) due to the ambiguity in selecting the correct prompt(s) corresponding to different foreground objects belonging to multiple tasks. To circumvent this issue we propose to eliminate the prompt selection mechanism by maintaining task-specific pathways, which allow us to learn representations that do not interact with the ones from the other tasks. Since independent pathways in truly incremental scenarios will result in an explosion of computation due to the quadratically complex multi-head self-attention (MSA) operation in prompt tuning, we propose to reduce the original patch token embeddings into summarized tokens. Prompt tuning is then applied to these fewer summarized tokens to compute the final representation. Our proposed method Multi-Label class incremental learning via summarising pAtch tokeN Embeddings (MULTI-LANE) enables learning disentangled task-specific representations in MLCIL while ensuring fast inference. We conduct experiments in common benchmarks and demonstrate that our MULTI-LANE achieves a new state-of-the-art in MLCIL. Additionally, we show that MULTI-LANE is also competitive in the CIL setting. Source code available at https://github.com/tdemin16/multi-lane","sentences":["Prompt tuning has emerged as an effective rehearsal-free technique for class-incremental learning (CIL) that learns a tiny set of task-specific parameters (or prompts) to instruct a pre-trained transformer to learn on a sequence of tasks.","Albeit effective, prompt tuning methods do not lend well in the multi-label class incremental learning (MLCIL) scenario (where an image contains multiple foreground classes) due to the ambiguity in selecting the correct prompt(s) corresponding to different foreground objects belonging to multiple tasks.","To circumvent this issue we propose to eliminate the prompt selection mechanism by maintaining task-specific pathways, which allow us to learn representations that do not interact with the ones from the other tasks.","Since independent pathways in truly incremental scenarios will result in an explosion of computation due to the quadratically complex multi-head self-attention (MSA) operation in prompt tuning, we propose to reduce the original patch token embeddings into summarized tokens.","Prompt tuning is then applied to these fewer summarized tokens to compute the final representation.","Our proposed method Multi-Label class incremental learning via summarising pAtch tokeN Embeddings (MULTI-LANE) enables learning disentangled task-specific representations in MLCIL while ensuring fast inference.","We conduct experiments in common benchmarks and demonstrate that our MULTI-LANE achieves a new state-of-the-art in MLCIL.","Additionally, we show that MULTI-LANE is also competitive in the CIL setting.","Source code available at https://github.com/tdemin16/multi-lane"],"url":"http://arxiv.org/abs/2405.15633v1","category":"cs.CV"}
{"created":"2024-05-24 15:16:50","title":"A Comparative Analysis of Distributed Training Strategies for GPT-2","abstract":"The rapid advancement in Large Language Models has been met with significant challenges in their training processes, primarily due to their considerable computational and memory demands. This research examines parallelization techniques developed to address these challenges, enabling the efficient and scalable training of Large Language Models. A comprehensive analysis of both data and model parallelism strategies, including Fully Sharded Data Parallelism and Distributed Data-Parallel frameworks, is provided to assess methods that facilitate efficient model training. Furthermore, the architectural complexities and training methodologies of the Generative Pre-Trained Transformer-2 model are explored. The application of these strategies is further investigated, which is crucial in managing the substantial computational and memory demands of training sophisticated models. This analysis not only highlights the effectiveness of these parallel training strategies in enhancing training efficiency but also their role in enabling the scalable training of large language models. Drawing on recent research findings, through a comprehensive literature review, this research underscores the critical role of parallelization techniques in addressing the computational challenges of training state-of-the-art Large Language Models, thereby contributing to the advancement of training more sophisticated and capable artificial intelligence systems.","sentences":["The rapid advancement in Large Language Models has been met with significant challenges in their training processes, primarily due to their considerable computational and memory demands.","This research examines parallelization techniques developed to address these challenges, enabling the efficient and scalable training of Large Language Models.","A comprehensive analysis of both data and model parallelism strategies, including Fully Sharded Data Parallelism and Distributed Data-Parallel frameworks, is provided to assess methods that facilitate efficient model training.","Furthermore, the architectural complexities and training methodologies of the Generative Pre-Trained Transformer-2 model are explored.","The application of these strategies is further investigated, which is crucial in managing the substantial computational and memory demands of training sophisticated models.","This analysis not only highlights the effectiveness of these parallel training strategies in enhancing training efficiency but also their role in enabling the scalable training of large language models.","Drawing on recent research findings, through a comprehensive literature review, this research underscores the critical role of parallelization techniques in addressing the computational challenges of training state-of-the-art Large Language Models, thereby contributing to the advancement of training more sophisticated and capable artificial intelligence systems."],"url":"http://arxiv.org/abs/2405.15628v1","category":"cs.DC"}
{"created":"2024-05-24 15:16:29","title":"The Scattering Matrix-Based Characteristic Mode for Structure amidst Arbitrary Background: Theory, Benchmark and Applications","abstract":"This paper presents a novel approach for computing substructure characteristic modes. This method leverages electromagnetic scattering matrices and spherical wave expansion to directly decompose electromagnetic fields. Unlike conventional methods that rely on the impedance matrix generated by the method of moments (MoM), our technique simplifies the problem into a small-scale ordinary eigenvalue problem, improving numerical dynamics and computational efficiency. We have developed analytical substructure characteristic mode solutions for a scenario involving two spheres, which can serve as benchmarks for evaluating other numerical solvers. A key advantage of our method is its independence from specific MoM frameworks, allowing for the use of various numerical methods. This flexibility paves the way for substructure characteristic mode decomposition to become a universal frequency technique.","sentences":["This paper presents a novel approach for computing substructure characteristic modes.","This method leverages electromagnetic scattering matrices and spherical wave expansion to directly decompose electromagnetic fields.","Unlike conventional methods that rely on the impedance matrix generated by the method of moments (MoM), our technique simplifies the problem into a small-scale ordinary eigenvalue problem, improving numerical dynamics and computational efficiency.","We have developed analytical substructure characteristic mode solutions for a scenario involving two spheres, which can serve as benchmarks for evaluating other numerical solvers.","A key advantage of our method is its independence from specific MoM frameworks, allowing for the use of various numerical methods.","This flexibility paves the way for substructure characteristic mode decomposition to become a universal frequency technique."],"url":"http://arxiv.org/abs/2405.15627v1","category":"physics.class-ph"}
{"created":"2024-05-24 15:14:55","title":"A generalized $\u039b$CDM model with parameterized Hubble parameter in particle creation, viscous and $f(R)$ model framework","abstract":"In this study, we construct a theoretical framework based on the generalized Hubble parameter form which may arise within the particle creation, viscous and $f(R)$ gravity theory. The Hubble parameter is scrutinized for its compatibility with the observational data relevant to the late-time universe. By using Bayesian statistical techniques based on $\\chi^{2}$ minimization method, we determine model parameters's best fit values for the cosmic chronometer and supernovae Pantheon datasets. For the best fit values, the cosmographic and physical parameters are analyzed to understand the cosmic dynamics in model.","sentences":["In this study, we construct a theoretical framework based on the generalized Hubble parameter form which may arise within the particle creation, viscous and $f(R)$ gravity theory.","The Hubble parameter is scrutinized for its compatibility with the observational data relevant to the late-time universe.","By using Bayesian statistical techniques based on $\\chi^{2}$ minimization method, we determine model parameters's best fit values for the cosmic chronometer and supernovae Pantheon datasets.","For the best fit values, the cosmographic and physical parameters are analyzed to understand the cosmic dynamics in model."],"url":"http://arxiv.org/abs/2405.15626v1","category":"gr-qc"}
{"created":"2024-05-24 15:14:23","title":"Nonlinear denoising score matching for enhanced learning of structured distributions","abstract":"We present a novel method for training score-based generative models which uses nonlinear noising dynamics to improve learning of structured distributions. Generalizing to a nonlinear drift allows for additional structure to be incorporated into the dynamics, thus making the training better adapted to the data, e.g., in the case of multimodality or (approximate) symmetries. Such structure can be obtained from the data by an inexpensive preprocessing step. The nonlinear dynamics introduces new challenges into training which we address in two ways: 1) we develop a new nonlinear denoising score matching (NDSM) method, 2) we introduce neural control variates in order to reduce the variance of the NDSM training objective. We demonstrate the effectiveness of this method on several examples: a) a collection of low-dimensional examples, motivated by clustering in latent space, b) high-dimensional images, addressing issues with mode collapse, small training sets, and approximate symmetries, the latter being a challenge for methods based on equivariant neural networks, which require exact symmetries.","sentences":["We present a novel method for training score-based generative models which uses nonlinear noising dynamics to improve learning of structured distributions.","Generalizing to a nonlinear drift allows for additional structure to be incorporated into the dynamics, thus making the training better adapted to the data, e.g., in the case of multimodality or (approximate) symmetries.","Such structure can be obtained from the data by an inexpensive preprocessing step.","The nonlinear dynamics introduces new challenges into training which we address in two ways: 1) we develop a new nonlinear denoising score matching (NDSM) method, 2) we introduce neural control variates in order to reduce the variance of the NDSM training objective.","We demonstrate the effectiveness of this method on several examples: a) a collection of low-dimensional examples, motivated by clustering in latent space, b) high-dimensional images, addressing issues with mode collapse, small training sets, and approximate symmetries, the latter being a challenge for methods based on equivariant neural networks, which require exact symmetries."],"url":"http://arxiv.org/abs/2405.15625v1","category":"stat.ML"}
{"created":"2024-05-24 15:13:53","title":"Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment","abstract":"Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility. However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns. In this work, we introduce Alignment from Demonstrations (AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges. We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals. Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD. Analytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior. Practically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD. We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity.","sentences":["Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility.","However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns.","In this work, we introduce Alignment from Demonstrations (AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges.","We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals.","Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD.","Analytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior.","Practically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD.","We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity."],"url":"http://arxiv.org/abs/2405.15624v1","category":"cs.LG"}
{"created":"2024-05-24 15:09:12","title":"LAM3D: Large Image-Point-Cloud Alignment Model for 3D Reconstruction from Single Image","abstract":"Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images. Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data. In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes. Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction. Building upon this, our Image-Point-Cloud Feature Alignment technique processes a single input image, aligning to the latent tri-planes to imbue image features with robust 3D information. This process not only enriches the image features but also facilitates the production of high-fidelity 3D meshes without the need for multi-view input, significantly reducing geometric distortions. Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness.","sentences":["Large Reconstruction Models have made significant strides in the realm of automated 3D content generation from single or multiple input images.","Despite their success, these models often produce 3D meshes with geometric inaccuracies, stemming from the inherent challenges of deducing 3D shapes solely from image data.","In this work, we introduce a novel framework, the Large Image and Point Cloud Alignment Model (LAM3D), which utilizes 3D point cloud data to enhance the fidelity of generated 3D meshes.","Our methodology begins with the development of a point-cloud-based network that effectively generates precise and meaningful latent tri-planes, laying the groundwork for accurate 3D mesh reconstruction.","Building upon this, our Image-Point-Cloud Feature Alignment technique processes a single input image, aligning to the latent tri-planes to imbue image features with robust 3D information.","This process not only enriches the image features but also facilitates the production of high-fidelity 3D meshes without the need for multi-view input, significantly reducing geometric distortions.","Our approach achieves state-of-the-art high-fidelity 3D mesh reconstruction from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness."],"url":"http://arxiv.org/abs/2405.15622v1","category":"cs.CV"}
{"created":"2024-05-24 15:07:49","title":"On the Existence of Generalized Breathers and Transition Fronts in Time-Periodic Nonlinear Lattices","abstract":"We prove the existence of a class of time-localized and space-periodic breathers (called q-gap breathers) in nonlinear lattices with time-periodic coefficients. These q-gap breathers are the counterparts to the classical space-localized and time-periodic breathers found in space-periodic systems. Using normal form transformations, we establish rigorously the existence of such solutions with oscillating tails (in the time domain) that can be made arbitrarily small, but finite. Due to the presence of the oscillating tails, these solutions are coined generalized q-gap breathers. Using a multiple-scale analysis, we also derive a tractable amplitude equation that describes the dynamics of breathers in the limit of small amplitude. In the presence of damping, we demonstrate the existence of transition fronts that connect the trivial state to the time-periodic ones. The analytical results are corroborated by systematic numerical simulations.","sentences":["We prove the existence of a class of time-localized and space-periodic breathers (called q-gap breathers) in nonlinear lattices with time-periodic coefficients.","These q-gap breathers are the counterparts to the classical space-localized and time-periodic breathers found in space-periodic systems.","Using normal form transformations, we establish rigorously the existence of such solutions with oscillating tails (in the time domain) that can be made arbitrarily small, but finite.","Due to the presence of the oscillating tails, these solutions are coined generalized q-gap breathers.","Using a multiple-scale analysis, we also derive a tractable amplitude equation that describes the dynamics of breathers in the limit of small amplitude.","In the presence of damping, we demonstrate the existence of transition fronts that connect the trivial state to the time-periodic ones.","The analytical results are corroborated by systematic numerical simulations."],"url":"http://arxiv.org/abs/2405.15621v1","category":"nlin.PS"}
{"created":"2024-05-24 15:05:04","title":"DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation","abstract":"Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image.","sentences":["Monocular camera calibration is a key precondition for numerous 3D vision applications.","Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data.","Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images.","This success suggests a strong potential of the models to effectively understand varied visual information.","In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation.","Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task.","The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models.","The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference.","Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation.","Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors.","Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image."],"url":"http://arxiv.org/abs/2405.15619v1","category":"cs.CV"}
{"created":"2024-05-24 15:04:26","title":"Inductive detection of inverse spin-orbit torques in magnetic heterostructures","abstract":"The manipulation of magnetization via Magnetic torques is one of the most important phenomena in spintronics. In thin films, conventionally, a charge current flowing in a heavy metal is used to generate transverse spin currents and to exert torques on the magnetization of an adjacent ferromagnetic thin film layer. Here, in contrast to the typically employed heavy metals, we study spin-to-charge conversion in ferromagnetic heterostructures with large spin-orbit interaction that function as the torque-generating layers. In particular, we chose perpendicular magnetic anisotropy (PMA) multilayers [Co/Ni] and [Co/Pt] as the torque-generating layers and drive magnetization dynamics in metallic ferromagnetic thin film $\\mathrm{Co_{20}Fe_{60}B_{20}}$ (CoFeB) layers with in-plane magnetic anisotropy (IMA). We investigate the spin dynamics driven by spin-orbit torque (SOT) and the concomitant charge current generation by the inverse SOT process using an inductive technique based on a vector network analyzer. In our experimental findings, we find that the SOTs generated by our multilayers are of a magnitude comparable to those produced by Pt, consistent with first-principles calculations. Furthermore, we noted a significant correlation between the SOT and the thickness of the CoFeB layer.","sentences":["The manipulation of magnetization via Magnetic torques is one of the most important phenomena in spintronics.","In thin films, conventionally, a charge current flowing in a heavy metal is used to generate transverse spin currents and to exert torques on the magnetization of an adjacent ferromagnetic thin film layer.","Here, in contrast to the typically employed heavy metals, we study spin-to-charge conversion in ferromagnetic heterostructures with large spin-orbit interaction that function as the torque-generating layers.","In particular, we chose perpendicular magnetic anisotropy (PMA) multilayers [Co/Ni] and [Co/Pt] as the torque-generating layers and drive magnetization dynamics in metallic ferromagnetic thin film $\\mathrm{Co_{20}Fe_{60}B_{20}}$ (CoFeB) layers with in-plane magnetic anisotropy (IMA).","We investigate the spin dynamics driven by spin-orbit torque (SOT) and the concomitant charge current generation by the inverse SOT process using an inductive technique based on a vector network analyzer.","In our experimental findings, we find that the SOTs generated by our multilayers are of a magnitude comparable to those produced by Pt, consistent with first-principles calculations.","Furthermore, we noted a significant correlation between the SOT and the thickness of the CoFeB layer."],"url":"http://arxiv.org/abs/2405.15617v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 15:03:56","title":"Neuromorphic dreaming: A pathway to efficient learning in artificial agents","abstract":"Achieving energy efficiency in learning is a key challenge for artificial intelligence (AI) computing platforms. Biological systems demonstrate remarkable abilities to learn complex skills quickly and efficiently. Inspired by this, we present a hardware implementation of model-based reinforcement learning (MBRL) using spiking neural networks (SNNs) on mixed-signal analog/digital neuromorphic hardware. This approach leverages the energy efficiency of mixed-signal neuromorphic chips while achieving high sample efficiency through an alternation of online learning, referred to as the \"awake\" phase, and offline learning, known as the \"dreaming\" phase. The model proposed includes two symbiotic networks: an agent network that learns by combining real and simulated experiences, and a learned world model network that generates the simulated experiences. We validate the model by training the hardware implementation to play the Atari game Pong. We start from a baseline consisting of an agent network learning without a world model and dreaming, which successfully learns to play the game. By incorporating dreaming, the number of required real game experiences are reduced significantly compared to the baseline. The networks are implemented using a mixed-signal neuromorphic processor, with the readout layers trained using a computer in-the-loop, while the other layers remain fixed. These results pave the way toward energy-efficient neuromorphic learning systems capable of rapid learning in real world applications and use-cases.","sentences":["Achieving energy efficiency in learning is a key challenge for artificial intelligence (AI) computing platforms.","Biological systems demonstrate remarkable abilities to learn complex skills quickly and efficiently.","Inspired by this, we present a hardware implementation of model-based reinforcement learning (MBRL) using spiking neural networks (SNNs) on mixed-signal analog/digital neuromorphic hardware.","This approach leverages the energy efficiency of mixed-signal neuromorphic chips while achieving high sample efficiency through an alternation of online learning, referred to as the \"awake\" phase, and offline learning, known as the \"dreaming\" phase.","The model proposed includes two symbiotic networks: an agent network that learns by combining real and simulated experiences, and a learned world model network that generates the simulated experiences.","We validate the model by training the hardware implementation to play the Atari game Pong.","We start from a baseline consisting of an agent network learning without a world model and dreaming, which successfully learns to play the game.","By incorporating dreaming, the number of required real game experiences are reduced significantly compared to the baseline.","The networks are implemented using a mixed-signal neuromorphic processor, with the readout layers trained using a computer in-the-loop, while the other layers remain fixed.","These results pave the way toward energy-efficient neuromorphic learning systems capable of rapid learning in real world applications and use-cases."],"url":"http://arxiv.org/abs/2405.15616v1","category":"cs.AI"}
{"created":"2024-05-24 14:59:19","title":"Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study","abstract":"Despite various approaches being employed to detect vulnerabilities, the number of reported vulnerabilities shows an upward trend over the years. This suggests the problems are not caught before the code is released, which could be caused by many factors, like lack of awareness, limited efficacy of the existing vulnerability detection tools or the tools not being user-friendly. To help combat some issues with traditional vulnerability detection tools, we propose using large language models (LLMs) to assist in finding vulnerabilities in source code. LLMs have shown a remarkable ability to understand and generate code, underlining their potential in code-related tasks. The aim is to test multiple state-of-the-art LLMs and identify the best prompting strategies, allowing extraction of the best value from the LLMs. We provide an overview of the strengths and weaknesses of the LLM-based approach and compare the results to those of traditional static analysis tools. We find that LLMs can pinpoint many more issues than traditional static analysis tools, outperforming traditional tools in terms of recall and F1 scores. The results should benefit software developers and security analysts responsible for ensuring that the code is free of vulnerabilities.","sentences":["Despite various approaches being employed to detect vulnerabilities, the number of reported vulnerabilities shows an upward trend over the years.","This suggests the problems are not caught before the code is released, which could be caused by many factors, like lack of awareness, limited efficacy of the existing vulnerability detection tools or the tools not being user-friendly.","To help combat some issues with traditional vulnerability detection tools, we propose using large language models (LLMs) to assist in finding vulnerabilities in source code.","LLMs have shown a remarkable ability to understand and generate code, underlining their potential in code-related tasks.","The aim is to test multiple state-of-the-art LLMs and identify the best prompting strategies, allowing extraction of the best value from the LLMs.","We provide an overview of the strengths and weaknesses of the LLM-based approach and compare the results to those of traditional static analysis tools.","We find that LLMs can pinpoint many more issues than traditional static analysis tools, outperforming traditional tools in terms of recall and F1 scores.","The results should benefit software developers and security analysts responsible for ensuring that the code is free of vulnerabilities."],"url":"http://arxiv.org/abs/2405.15614v1","category":"cs.CR"}
{"created":"2024-05-24 14:58:51","title":"Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach","abstract":"Self-supervised features are the cornerstone of modern machine learning systems. They are typically pre-trained on data collections whose construction and curation typically require extensive human effort. This manual process has some limitations similar to those encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and time-consuming, preventing scaling the dataset size. In this work, we consider the problem of automatic curation of high-quality datasets for self-supervised pre-training. We posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria. Our method involves successive and hierarchical applications of $k$-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters. Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on our automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data.","sentences":["Self-supervised features are the cornerstone of modern machine learning systems.","They are typically pre-trained on data collections whose construction and curation typically require extensive human effort.","This manual process has some limitations similar to those encountered in supervised learning, e.g., the crowd-sourced selection of data is costly and time-consuming, preventing scaling the dataset size.","In this work, we consider the problem of automatic curation of high-quality datasets for self-supervised pre-training.","We posit that such datasets should be large, diverse and balanced, and propose a clustering-based approach for building ones satisfying all these criteria.","Our method involves successive and hierarchical applications of $k$-means on a large and diverse data repository to obtain clusters that distribute uniformly among data concepts, followed by a hierarchical, balanced sampling step from these clusters.","Extensive experiments on three different data domains including web-based images, satellite images and text show that features trained on our automatically curated datasets outperform those trained on uncurated data while being on par or better than ones trained on manually curated data."],"url":"http://arxiv.org/abs/2405.15613v1","category":"cs.LG"}
{"created":"2024-05-24 14:48:00","title":"A way around the exponential scaling in optimal quantum control","abstract":"We show that combining ideas from the fields of quantum invariants and of optimal control can be used to design quantum control of quantum systems without explicit reference to quantum states. The scaling in numerical effort of the resultant approach is given by commutation relations of system operators, and it can be polynomial in the number of subsystems despite the general quantum mechanical exponential scaling of the Hilbert space. As explicit applications, we discuss state preparation and quantum simulation with Hamiltonians including three-body and many-body interactions with spin chains of up to 50 constituents, and the perspective of use for topologically protected quantum information processing.","sentences":["We show that combining ideas from the fields of quantum invariants and of optimal control can be used to design quantum control of quantum systems without explicit reference to quantum states.","The scaling in numerical effort of the resultant approach is given by commutation relations of system operators, and it can be polynomial in the number of subsystems despite the general quantum mechanical exponential scaling of the Hilbert space.","As explicit applications, we discuss state preparation and quantum simulation with Hamiltonians including three-body and many-body interactions with spin chains of up to 50 constituents, and the perspective of use for topologically protected quantum information processing."],"url":"http://arxiv.org/abs/2405.15609v1","category":"quant-ph"}
{"created":"2024-05-24 14:47:21","title":"Channel Estimation and Reconstruction in Fluid Antenna System: Oversampling is Essential","abstract":"Fluid antenna system (FAS) has recently surfaced as a promising technology for the upcoming sixth generation (6G) wireless networks. Unlike traditional antenna system (TAS) with fixed antenna location, FAS introduces a flexible component where the radiating element can switch its position within a predefined space. This capability allows FAS to achieve additional diversity and multiplexing gains. Nevertheless, to fully reap the benefits of FAS, obtaining channel state information (CSI) over the predefined space is crucial. In this paper, we explore the interaction between a transmitter equipped with a traditional antenna and a receiver with a fluid antenna over an electromagnetic-compliant channel model. We address the challenges of channel estimation and reconstruction using Nyquist sampling and maximum likelihood estimation (MLE) methods. Our analysis reveals a fundamental tradeoff between the accuracy of the reconstructed channel and the number of estimated channels, indicating that half-wavelength sampling is insufficient for perfect reconstruction and that oversampling is essential to enhance accuracy. Despite its advantages, oversampling can introduce practical challenges. Consequently, we propose a suboptimal sampling distance that facilitates efficient channel reconstruction. In addition, we employ the MLE method to bound the channel estimation error by $\\epsilon$, with a specific confidence interval (CI). Our findings enable us to determine the minimum number of estimated channels and the total number of pilot symbols required for efficient channel reconstruction in a given space. Lastly, we investigate the rate performance of FAS and TAS and demonstrate that FAS with imperfect CSI can outperform TAS with perfect CSI.","sentences":["Fluid antenna system (FAS) has recently surfaced as a promising technology for the upcoming sixth generation (6G) wireless networks.","Unlike traditional antenna system (TAS) with fixed antenna location, FAS introduces a flexible component where the radiating element can switch its position within a predefined space.","This capability allows FAS to achieve additional diversity and multiplexing gains.","Nevertheless, to fully reap the benefits of FAS, obtaining channel state information (CSI) over the predefined space is crucial.","In this paper, we explore the interaction between a transmitter equipped with a traditional antenna and a receiver with a fluid antenna over an electromagnetic-compliant channel model.","We address the challenges of channel estimation and reconstruction using Nyquist sampling and maximum likelihood estimation (MLE) methods.","Our analysis reveals a fundamental tradeoff between the accuracy of the reconstructed channel and the number of estimated channels, indicating that half-wavelength sampling is insufficient for perfect reconstruction and that oversampling is essential to enhance accuracy.","Despite its advantages, oversampling can introduce practical challenges.","Consequently, we propose a suboptimal sampling distance that facilitates efficient channel reconstruction.","In addition, we employ the MLE method to bound the channel estimation error by $\\epsilon$, with a specific confidence interval (CI).","Our findings enable us to determine the minimum number of estimated channels and the total number of pilot symbols required for efficient channel reconstruction in a given space.","Lastly, we investigate the rate performance of FAS and TAS and demonstrate that FAS with imperfect CSI can outperform TAS with perfect CSI."],"url":"http://arxiv.org/abs/2405.15607v1","category":"eess.SP"}
{"created":"2024-05-24 14:38:11","title":"Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges","abstract":"Text generation has become more accessible than ever, and the increasing interest in these systems, especially those using large language models, has spurred an increasing number of related publications. We provide a systematic literature review comprising 244 selected papers between 2017 and 2024. This review categorizes works in text generation into five main tasks: open-ended text generation, summarization, translation, paraphrasing, and question answering. For each task, we review their relevant characteristics, sub-tasks, and specific challenges (e.g., missing datasets for multi-document summarization, coherence in story generation, and complex reasoning for question answering). Additionally, we assess current approaches for evaluating text generation systems and ascertain problems with current metrics. Our investigation shows nine prominent challenges common to all tasks and sub-tasks in recent text generation publications: bias, reasoning, hallucinations, misuse, privacy, interpretability, transparency, datasets, and computing. We provide a detailed analysis of these challenges, their potential solutions, and which gaps still require further engagement from the community. This systematic literature review targets two main audiences: early career researchers in natural language processing looking for an overview of the field and promising research directions, as well as experienced researchers seeking a detailed view of tasks, evaluation methodologies, open challenges, and recent mitigation strategies.","sentences":["Text generation has become more accessible than ever, and the increasing interest in these systems, especially those using large language models, has spurred an increasing number of related publications.","We provide a systematic literature review comprising 244 selected papers between 2017 and 2024.","This review categorizes works in text generation into five main tasks: open-ended text generation, summarization, translation, paraphrasing, and question answering.","For each task, we review their relevant characteristics, sub-tasks, and specific challenges (e.g., missing datasets for multi-document summarization, coherence in story generation, and complex reasoning for question answering).","Additionally, we assess current approaches for evaluating text generation systems and ascertain problems with current metrics.","Our investigation shows nine prominent challenges common to all tasks and sub-tasks in recent text generation publications: bias, reasoning, hallucinations, misuse, privacy, interpretability, transparency, datasets, and computing.","We provide a detailed analysis of these challenges, their potential solutions, and which gaps still require further engagement from the community.","This systematic literature review targets two main audiences: early career researchers in natural language processing looking for an overview of the field and promising research directions, as well as experienced researchers seeking a detailed view of tasks, evaluation methodologies, open challenges, and recent mitigation strategies."],"url":"http://arxiv.org/abs/2405.15604v1","category":"cs.CL"}
{"created":"2024-05-24 14:36:02","title":"Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks","abstract":"Physics-informed neural networks (PINNs) are infamous for being hard to train. Recently, second-order methods based on natural gradient and Gauss-Newton methods have shown promising performance, improving the accuracy achieved by first-order methods by several orders of magnitude. While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix. We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks. Our approach goes beyond the established KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization. To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights. This allows us to apply KFAC thanks to a recently-developed general formulation for networks with weight sharing. Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS.","sentences":["Physics-informed neural networks (PINNs) are infamous for being hard to train.","Recently, second-order methods based on natural gradient and Gauss-Newton methods have shown promising performance, improving the accuracy achieved by first-order methods by several orders of magnitude.","While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix.","We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks.","Our approach goes beyond the established KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization.","To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights.","This allows us to apply KFAC thanks to a recently-developed general formulation for networks with weight sharing.","Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS."],"url":"http://arxiv.org/abs/2405.15603v1","category":"cs.LG"}
{"created":"2024-05-24 14:30:00","title":"MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel Data Fusion Network Model Integrating CNN, LSTM, and GRU","abstract":"Accurate demand forecasting is crucial for optimizing supply chain management. Traditional methods often fail to capture complex patterns from seasonal variability and special events. Despite advancements in deep learning, interpretable forecasting models remain a challenge. To address this, we introduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture that integrates Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive performance by extracting spatial and temporal features from time series data. Our rigorous benchmarking demonstrates that MCDFN outperforms seven other deep-learning models, achieving superior metrics: MSE (23.5738%), RMSE (4.8553%), MAE (3.9991%), and MAPE (20.1575%). Additionally, MCDFN's predictions were statistically indistinguishable from actual values, confirmed by a paired t-test with a 5% p-value and a 10-fold cross-validated statistical paired t-test. We apply explainable AI techniques like ShapTime and Permutation Feature Importance to enhance interpretability. This research advances demand forecasting methodologies and offers practical guidelines for integrating MCDFN into supply chain systems, highlighting future research directions for scalability and user-friendly deployment.","sentences":["Accurate demand forecasting is crucial for optimizing supply chain management.","Traditional methods often fail to capture complex patterns from seasonal variability and special events.","Despite advancements in deep learning, interpretable forecasting models remain a challenge.","To address this, we introduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture that integrates Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive performance by extracting spatial and temporal features from time series data.","Our rigorous benchmarking demonstrates that MCDFN outperforms seven other deep-learning models, achieving superior metrics: MSE (23.5738%), RMSE (4.8553%), MAE (3.9991%), and MAPE (20.1575%).","Additionally, MCDFN's predictions were statistically indistinguishable from actual values, confirmed by a paired t-test with a 5% p-value and a 10-fold cross-validated statistical paired t-test.","We apply explainable AI techniques like ShapTime and Permutation Feature Importance to enhance interpretability.","This research advances demand forecasting methodologies and offers practical guidelines for integrating MCDFN into supply chain systems, highlighting future research directions for scalability and user-friendly deployment."],"url":"http://arxiv.org/abs/2405.15598v1","category":"cs.LG"}
{"created":"2024-05-24 14:27:28","title":"Fast adiabatic preparation of multi-squeezed states by jumping along the path","abstract":"Multi-squeezed states, also known as generalized squeezed states, are valuable quantum non-Gaussian resources, because they can feature non-classical properties such as large phase-space Wigner negativities. In this work, we introduce a novel shortcuts to adiabaticity (STA) method for the fast preparation of multi-squeezed states. In contrast to previous STA methods, which rely on the use of counterdiabatic control to suppress unwanted non-adiabatic effects, our method simplifies the process and accelerates state preparation by selecting an appropriate sampling along a quantum evolution path. We demonstrate the high-fidelity and fast preparation of multi-squeezed states, as well as hybrid entangled states between a bosonic mode and a qubit.","sentences":["Multi-squeezed states, also known as generalized squeezed states, are valuable quantum non-Gaussian resources, because they can feature non-classical properties such as large phase-space Wigner negativities.","In this work, we introduce a novel shortcuts to adiabaticity (STA) method for the fast preparation of multi-squeezed states.","In contrast to previous STA methods, which rely on the use of counterdiabatic control to suppress unwanted non-adiabatic effects, our method simplifies the process and accelerates state preparation by selecting an appropriate sampling along a quantum evolution path.","We demonstrate the high-fidelity and fast preparation of multi-squeezed states, as well as hybrid entangled states between a bosonic mode and a qubit."],"url":"http://arxiv.org/abs/2405.15595v1","category":"quant-ph"}
{"created":"2024-05-24 14:26:15","title":"Eikonal amplitudes on the celestial sphere","abstract":"Celestial scattering amplitudes for massless particles are Mellin transforms of momentum-space scattering amplitudes with respect to the energies of the external particles, and behave as conformal correlators on the celestial sphere. However, there are few explicit cases of well-defined celestial amplitudes, particularly for gravitational theories: the mixing between low- and high-energy scales induced by the Mellin transform generically yields divergent integrals. In this paper, we argue that the most natural object to consider is the gravitational amplitude dressed by an oscillating phase arising from semi-classical effects known as eikonal exponentiation. This leads to gravitational celestial amplitudes which are analytic, apart from a set of poles at integer negative conformal dimensions, whose degree and residues we characterize. We also study the large conformal dimension limits, and provide an asymptotic series representation for these celestial eikonal amplitudes. Our investigation covers two different frameworks, related by eikonal exponentiation: $2\\to2$ scattering of scalars in flat spacetime and $1\\to1$ scattering of a probe scalar particle in a curved, stationary spacetime. These provide data which any putative celestial dual for Minkowski, shockwave or black hole spacetimes must reproduce. We also derive dispersion and monodromy relations for these celestial amplitudes and discuss Carrollian eikonal-probe amplitudes in curved spacetimes.","sentences":["Celestial scattering amplitudes for massless particles are Mellin transforms of momentum-space scattering amplitudes with respect to the energies of the external particles, and behave as conformal correlators on the celestial sphere.","However, there are few explicit cases of well-defined celestial amplitudes, particularly for gravitational theories: the mixing between low- and high-energy scales induced by the Mellin transform generically yields divergent integrals.","In this paper, we argue that the most natural object to consider is the gravitational amplitude dressed by an oscillating phase arising from semi-classical effects known as eikonal exponentiation.","This leads to gravitational celestial amplitudes which are analytic, apart from a set of poles at integer negative conformal dimensions, whose degree and residues we characterize.","We also study the large conformal dimension limits, and provide an asymptotic series representation for these celestial eikonal amplitudes.","Our investigation covers two different frameworks, related by eikonal exponentiation: $2\\to2$ scattering of scalars in flat spacetime and $1\\to1$ scattering of a probe scalar particle in a curved, stationary spacetime.","These provide data which any putative celestial dual for Minkowski, shockwave or black hole spacetimes must reproduce.","We also derive dispersion and monodromy relations for these celestial amplitudes and discuss Carrollian eikonal-probe amplitudes in curved spacetimes."],"url":"http://arxiv.org/abs/2405.15594v1","category":"hep-th"}
{"created":"2024-05-24 14:24:49","title":"Density Sensitive Bifiltered Dowker Complexes via Total Weight","abstract":"In this paper, we introduce a density-sensitive bifiltration on Dowker complexes. Previously, Dowker complexes were studied to address directional or bivariate data whereas density-sensitive bifiltrations on \\v{C}ech and Vietoris--Rips complexes were suggested to make them more robust. We combine these two lines of research, noting that the superlevels of the total weight function of a Dowker complex can be identified as an instance of Sheehy's multicover filtration. An application of the multicover nerve theorem then provides a form of Dowker duality that is compatible with this filtration. As a consequence, we find that the subdivision intrinsic \\v{C}ech complex admits a smaller model. Moreover, regarding the total weight function as a counting measure, we generalize it to arbitrary measures and prove a density-sensitive stability theorem for the case of probability measures. Additionally, we provide an algorithm to calculate the appearances of simplices in our bifiltration and present computational examples.","sentences":["In this paper, we introduce a density-sensitive bifiltration on Dowker complexes.","Previously, Dowker complexes were studied to address directional or bivariate data whereas density-sensitive bifiltrations on \\v{C}ech and Vietoris--Rips complexes were suggested to make them more robust.","We combine these two lines of research, noting that the superlevels of the total weight function of a Dowker complex can be identified as an instance of Sheehy's multicover filtration.","An application of the multicover nerve theorem then provides a form of Dowker duality that is compatible with this filtration.","As a consequence, we find that the subdivision intrinsic \\v{C}ech complex admits a smaller model.","Moreover, regarding the total weight function as a counting measure, we generalize it to arbitrary measures and prove a density-sensitive stability theorem for the case of probability measures.","Additionally, we provide an algorithm to calculate the appearances of simplices in our bifiltration and present computational examples."],"url":"http://arxiv.org/abs/2405.15592v1","category":"math.AT"}
{"created":"2024-05-24 14:10:36","title":"Nonlinear studies of modifications to general relativity: Comparing different approaches","abstract":"Studying the dynamical, nonlinear regime of modified theories of gravity remains a theoretical challenge that limits our ability to test general relativity. Here we consider two generally applicable, but approximate methods for treating modifications to full general relativity that have been used to study binary black hole mergers and other phenomena in this regime, and compare solutions obtained by them to those from solving the full equations of motion. The first method evolves corrections to general relativity order by order in a perturbative expansion, while the second method introduces extra dynamical fields in such a way that strong hyperbolicity is recovered. We use shift-symmetric Einstein-scalar-Gauss-Bonnet gravity as a benchmark theory to illustrate the differences between these methods for several spacetimes of physical interest. We study the formation of scalar hair about initially non-spinning black holes, the collision of black holes with scalar charge, and the inspiral and merger of binary black holes. By directly comparing predictions, we assess the extent to which those from the approximate treatments can be meaningfully confronted with gravitational wave observations. We find that the order-by-order approach cannot faithfully track the solutions when the corrections to general relativity are non-negligible. The second approach, however, can provide consistent solutions, provided the ad-hoc timescale over which the dynamical fields are driven to their target values is made short compared to the physical timescales.","sentences":["Studying the dynamical, nonlinear regime of modified theories of gravity remains a theoretical challenge that limits our ability to test general relativity.","Here we consider two generally applicable, but approximate methods for treating modifications to full general relativity that have been used to study binary black hole mergers and other phenomena in this regime, and compare solutions obtained by them to those from solving the full equations of motion.","The first method evolves corrections to general relativity order by order in a perturbative expansion, while the second method introduces extra dynamical fields in such a way that strong hyperbolicity is recovered.","We use shift-symmetric Einstein-scalar-Gauss-Bonnet gravity as a benchmark theory to illustrate the differences between these methods for several spacetimes of physical interest.","We study the formation of scalar hair about initially non-spinning black holes, the collision of black holes with scalar charge, and the inspiral and merger of binary black holes.","By directly comparing predictions, we assess the extent to which those from the approximate treatments can be meaningfully confronted with gravitational wave observations.","We find that the order-by-order approach cannot faithfully track the solutions when the corrections to general relativity are non-negligible.","The second approach, however, can provide consistent solutions, provided the ad-hoc timescale over which the dynamical fields are driven to their target values is made short compared to the physical timescales."],"url":"http://arxiv.org/abs/2405.15581v1","category":"gr-qc"}
{"created":"2024-05-24 14:07:57","title":"Open-Vocabulary SAM3D: Understand Any 3D Scene","abstract":"Open-vocabulary 3D scene understanding presents a significant challenge in the field. Recent advancements have sought to transfer knowledge embedded in vision language models from the 2D domain to 3D domain. However, these approaches often require learning prior knowledge from specific 3D scene datasets, which limits their applicability in open-world scenarios. The Segment Anything Model (SAM) has demonstrated remarkable zero-shot segmentation capabilities, prompting us to investigate its potential for comprehending 3D scenes without the need for training. In this paper, we introduce OV-SAM3D, a universal framework for open-vocabulary 3D scene understanding. This framework is designed to perform understanding tasks for any 3D scene without requiring prior knowledge of the scene. Specifically, our method is composed of two key sub-modules: First, we initiate the process by generating superpoints as the initial 3D prompts and refine these prompts using segment masks derived from SAM. Moreover, we then integrate a specially designed overlapping score table with open tags from the Recognize Anything Model (RAM) to produce final 3D instances with open-world label. Empirical evaluations conducted on the ScanNet200 and nuScenes datasets demonstrate that our approach surpasses existing open-vocabulary methods in unknown open-world environments.","sentences":["Open-vocabulary 3D scene understanding presents a significant challenge in the field.","Recent advancements have sought to transfer knowledge embedded in vision language models from the 2D domain to 3D domain.","However, these approaches often require learning prior knowledge from specific 3D scene datasets, which limits their applicability in open-world scenarios.","The Segment Anything Model (SAM) has demonstrated remarkable zero-shot segmentation capabilities, prompting us to investigate its potential for comprehending 3D scenes without the need for training.","In this paper, we introduce OV-SAM3D, a universal framework for open-vocabulary 3D scene understanding.","This framework is designed to perform understanding tasks for any 3D scene without requiring prior knowledge of the scene.","Specifically, our method is composed of two key sub-modules: First, we initiate the process by generating superpoints as the initial 3D prompts and refine these prompts using segment masks derived from SAM.","Moreover, we then integrate a specially designed overlapping score table with open tags from the Recognize Anything Model (RAM) to produce final 3D instances with open-world label.","Empirical evaluations conducted on the ScanNet200 and nuScenes datasets demonstrate that our approach surpasses existing open-vocabulary methods in unknown open-world environments."],"url":"http://arxiv.org/abs/2405.15580v1","category":"cs.CV"}
{"created":"2024-05-24 14:06:08","title":"Generating density nowcasts for U.S. GDP growth with deep learning: Bayes by Backprop and Monte Carlo dropout","abstract":"Recent results in the literature indicate that artificial neural networks (ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy of GDP nowcasts. Compared to the DFM, the performance advantage of these highly flexible, nonlinear estimators is particularly evident in periods of recessions and structural breaks. From the perspective of policy-makers, however, nowcasts are the most useful when they are conveyed with uncertainty attached to them. While the DFM and other classical time series approaches analytically derive the predictive (conditional) distribution for GDP growth, ANNs can only produce point nowcasts based on their default training procedure (backpropagation). To fill this gap, first in the literature, we adapt two different deep learning algorithms that enable ANNs to generate density nowcasts for U.S. GDP growth: Bayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts, defined as the mean of the empirical predictive distribution, is evaluated relative to a naive constant growth model for GDP and a benchmark DFM specification. Using a 1D CNN as the underlying ANN architecture, both algorithms outperform those benchmarks during the evaluation period (2012:Q1 -- 2022:Q4). Furthermore, both algorithms are able to dynamically adjust the location (mean), scale (variance), and shape (skew) of the empirical predictive distribution. The results indicate that both Bayes by Backprop and Monte Carlo dropout can effectively augment the scope and functionality of ANNs, rendering them a fully compatible and competitive alternative for classical time series approaches.","sentences":["Recent results in the literature indicate that artificial neural networks (ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy of GDP nowcasts.","Compared to the DFM, the performance advantage of these highly flexible, nonlinear estimators is particularly evident in periods of recessions and structural breaks.","From the perspective of policy-makers, however, nowcasts are the most useful when they are conveyed with uncertainty attached to them.","While the DFM and other classical time series approaches analytically derive the predictive (conditional) distribution for GDP growth, ANNs can only produce point nowcasts based on their default training procedure (backpropagation).","To fill this gap, first in the literature, we adapt two different deep learning algorithms that enable ANNs to generate density nowcasts for U.S. GDP growth: Bayes by Backprop and Monte Carlo dropout.","The accuracy of point nowcasts, defined as the mean of the empirical predictive distribution, is evaluated relative to a naive constant growth model for GDP and a benchmark DFM specification.","Using a 1D CNN as the underlying ANN architecture, both algorithms outperform those benchmarks during the evaluation period (2012:Q1 -- 2022:Q4).","Furthermore, both algorithms are able to dynamically adjust the location (mean), scale (variance), and shape (skew) of the empirical predictive distribution.","The results indicate that both Bayes by Backprop and Monte Carlo dropout can effectively augment the scope and functionality of ANNs, rendering them a fully compatible and competitive alternative for classical time series approaches."],"url":"http://arxiv.org/abs/2405.15579v1","category":"econ.EM"}
{"created":"2024-05-24 14:04:38","title":"Manifold Solutions to Navier-Stokes Equations","abstract":"We have developed dynamic manifold solutions for the Navier-Stokes equations using an extension of differential geometry called the calculus for moving surfaces. Specifically, we have shown that the geometric solutions to the Navier-Stokes equations can take the form of fluctuating spheres, constant mean curvature surfaces, generic wave equations for compressible systems, and arbitrarily curved shapes for incompressible systems in various scenarios. These solutions apply to predominantly incompressible and compressible systems for the equations in any dimension, while the remaining cases are yet to be solved. We have demonstrated that for incompressible Navier-Stokes equations, geometric solutions are always bound by the curvature tensor of the closed smooth manifold for every smooth velocity field. As a result, solutions always converge for systems with constant volumes.","sentences":["We have developed dynamic manifold solutions for the Navier-Stokes equations using an extension of differential geometry called the calculus for moving surfaces.","Specifically, we have shown that the geometric solutions to the Navier-Stokes equations can take the form of fluctuating spheres, constant mean curvature surfaces, generic wave equations for compressible systems, and arbitrarily curved shapes for incompressible systems in various scenarios.","These solutions apply to predominantly incompressible and compressible systems for the equations in any dimension, while the remaining cases are yet to be solved.","We have demonstrated that for incompressible Navier-Stokes equations, geometric solutions are always bound by the curvature tensor of the closed smooth manifold for every smooth velocity field.","As a result, solutions always converge for systems with constant volumes."],"url":"http://arxiv.org/abs/2405.15575v1","category":"math.AP"}
{"created":"2024-05-24 14:04:03","title":"Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models","abstract":"The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (e.g., charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, we present a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity. We introduce a new concept of traversal of rationale that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models.","sentences":["The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning.","Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs.","These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (e.g., charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions.","Drawing from the multifaceted information, we present a new efficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages multifaceted rationale to enhance understanding and answering capabilities.","To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity.","We introduce a new concept of traversal of rationale that facilitates efficient embedding of rationale.","Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale.","Through these steps, Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models."],"url":"http://arxiv.org/abs/2405.15574v1","category":"cs.CV"}
{"created":"2024-05-24 14:03:36","title":"Toward a generalization of Lehmer's problem to adelic curves","abstract":"In this short note, we investigate the generalization of Lehmer's problem to finitely generated fields over $\\mathbb{Q}$.","sentences":["In this short note, we investigate the generalization of Lehmer's problem to finitely generated fields over $\\mathbb{Q}$."],"url":"http://arxiv.org/abs/2405.15572v1","category":"math.NT"}
{"created":"2024-05-24 14:01:05","title":"Randomized heuristic repair for large-scale multidimensional knapsack problem","abstract":"The multidimensional knapsack problem (MKP) is an NP-hard combinatorial optimization problem whose solution is determining a subset of maximum total profit items that do not violate capacity constraints. Due to its hardness, large-scale MKP instances are usually a target for metaheuristics, a context in which effective feasibility maintenance strategies are crucial. In 1998, Chu and Beasley proposed an effective heuristic repair that is still relevant for recent metaheuristics. However, due to its deterministic nature, the diversity of solutions such heuristic provides is insufficient for long runs. As a result, the search for new solutions ceases after a while. This paper proposes an efficiency-based randomization strategy for the heuristic repair that increases the variability of the repaired solutions without deteriorating quality and improves the overall results.","sentences":["The multidimensional knapsack problem (MKP) is an NP-hard combinatorial optimization problem whose solution is determining a subset of maximum total profit items that do not violate capacity constraints.","Due to its hardness, large-scale MKP instances are usually a target for metaheuristics, a context in which effective feasibility maintenance strategies are crucial.","In 1998, Chu and Beasley proposed an effective heuristic repair that is still relevant for recent metaheuristics.","However, due to its deterministic nature, the diversity of solutions such heuristic provides is insufficient for long runs.","As a result, the search for new solutions ceases after a while.","This paper proposes an efficiency-based randomization strategy for the heuristic repair that increases the variability of the repaired solutions without deteriorating quality and improves the overall results."],"url":"http://arxiv.org/abs/2405.15569v1","category":"cs.AI"}
{"created":"2024-05-24 13:57:32","title":"OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code","abstract":"Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence. To accomplish this grand vision, learning must occur within a vast array of potential tasks. Existing approaches to automatically generating environments are constrained within manually predefined, often narrow distributions of environment, limiting their ability to create any learning environment. To address this limitation, we introduce a novel framework, OMNI-EPIC, that augments previous work in Open-endedness via Models of human Notions of Interestingness (OMNI) with Environments Programmed in Code (EPIC). OMNI-EPIC leverages foundation models to autonomously generate code specifying the next learnable (i.e., not too easy or difficult for the agent's current skill set) and interesting (e.g., worthwhile and novel) tasks. OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects), enabling it, in principle, to create any simulatable learning task. We showcase the explosive creativity of OMNI-EPIC, which continuously innovates to suggest new, interesting learning challenges. We also highlight how OMNI-EPIC can adapt to reinforcement learning agents' learning progress, generating tasks that are of suitable difficulty. Overall, OMNI-EPIC can endlessly create learnable and interesting environments, further propelling the development of self-improving AI systems and AI-Generating Algorithms. Project website with videos: https://dub.sh/omniepic","sentences":["Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence.","To accomplish this grand vision, learning must occur within a vast array of potential tasks.","Existing approaches to automatically generating environments are constrained within manually predefined, often narrow distributions of environment, limiting their ability to create any learning environment.","To address this limitation, we introduce a novel framework, OMNI-EPIC, that augments previous work in Open-endedness via Models of human Notions of Interestingness (OMNI) with Environments Programmed in Code (EPIC).","OMNI-EPIC leverages foundation models to autonomously generate code specifying the next learnable (i.e., not too easy or difficult for the agent's current skill set) and interesting (e.g., worthwhile and novel) tasks.","OMNI-EPIC generates both environments (e.g., an obstacle course) and reward functions (e.g., progress through the obstacle course quickly without touching red objects), enabling it, in principle, to create any simulatable learning task.","We showcase the explosive creativity of OMNI-EPIC, which continuously innovates to suggest new, interesting learning challenges.","We also highlight how OMNI-EPIC can adapt to reinforcement learning agents' learning progress, generating tasks that are of suitable difficulty.","Overall, OMNI-EPIC can endlessly create learnable and interesting environments, further propelling the development of self-improving AI systems and AI-Generating Algorithms.","Project website with videos: https://dub.sh/omniepic"],"url":"http://arxiv.org/abs/2405.15568v1","category":"cs.AI"}
{"created":"2024-05-24 13:54:11","title":"Error Crafting in Probabilistic Quantum Gate Synthesis","abstract":"At the early stage of fault-tolerant quantum computing, it is envisioned that the gate synthesis of a general unitary gate into universal gate sets yields error whose magnitude is comparable with the noise inherent in the gates themselves. While it is known that the use of probabilistic synthesis already suppresses such coherent errors quadratically, there is no clear understanding on its remnant error, which hinders us from designing a holistic error countermeasure that is effectively combined with error suppression and mitigation. In this work, we propose that, by exploiting the fact that synthesis error can be characterized completely and efficiently, we can craft the remnant error of probabilistic synthesis such that the error profile satisfies desirable properties. We prove for the case of single-qubit unitary synthesis that, there is a guaranteed way to perform probabilistic synthesis such that we can craft the remnant error to be described by Pauli and depolarizing errors, while the conventional twirling cannot be applied in principle. Furthermore, we show a numerical evidence for the synthesis of Pauli rotations based on Clifford+T formalism that, we can craft the remnant error so that it can be eliminated up to {\\it cubic} order by combining logical measurement and feedback operations. As a result, Pauli rotation gates can be implemented with T counts of $\\log_2(1/\\varepsilon)$ on average up to accuracy of $\\varepsilon=10^{-9}$, which can be applied to early fault-tolerant quantum computation beyond classical tractability. Our work opens a novel avenue in quantum circuit design and architecture that orchestrates error countermeasures.","sentences":["At the early stage of fault-tolerant quantum computing, it is envisioned that the gate synthesis of a general unitary gate into universal gate sets yields error whose magnitude is comparable with the noise inherent in the gates themselves.","While it is known that the use of probabilistic synthesis already suppresses such coherent errors quadratically, there is no clear understanding on its remnant error, which hinders us from designing a holistic error countermeasure that is effectively combined with error suppression and mitigation.","In this work, we propose that, by exploiting the fact that synthesis error can be characterized completely and efficiently, we can craft the remnant error of probabilistic synthesis such that the error profile satisfies desirable properties.","We prove for the case of single-qubit unitary synthesis that, there is a guaranteed way to perform probabilistic synthesis such that we can craft the remnant error to be described by Pauli and depolarizing errors, while the conventional twirling cannot be applied in principle.","Furthermore, we show a numerical evidence for the synthesis of Pauli rotations based on Clifford+T formalism that, we can craft the remnant error so that it can be eliminated up to {\\it cubic} order by combining logical measurement and feedback operations.","As a result, Pauli rotation gates can be implemented with T counts of $\\log_2(1/\\varepsilon)$ on average up to accuracy of $\\varepsilon=10^{-9}$, which can be applied to early fault-tolerant quantum computation beyond classical tractability.","Our work opens a novel avenue in quantum circuit design and architecture that orchestrates error countermeasures."],"url":"http://arxiv.org/abs/2405.15565v1","category":"quant-ph"}
{"created":"2024-05-24 13:52:41","title":"Rethinking Independent Cross-Entropy Loss For Graph-Structured Data","abstract":"Graph neural networks (GNNs) have exhibited prominent performance in learning graph-structured data. Considering node classification task, based on the i.i.d assumption among node labels, the traditional supervised learning simply sums up cross-entropy losses of the independent training nodes and applies the average loss to optimize GNNs' weights. But different from other data formats, the nodes are naturally connected. It is found that the independent distribution modeling of node labels restricts GNNs' capability to generalize over the entire graph and defend adversarial attacks. In this work, we propose a new framework, termed joint-cluster supervised learning, to model the joint distribution of each node with its corresponding cluster. We learn the joint distribution of node and cluster labels conditioned on their representations, and train GNNs with the obtained joint loss. In this way, the data-label reference signals extracted from the local cluster explicitly strengthen the discrimination ability on the target node. The extensive experiments demonstrate that our joint-cluster supervised learning can effectively bolster GNNs' node classification accuracy. Furthermore, being benefited from the reference signals which may be free from spiteful interference, our learning paradigm significantly protects the node classification from being affected by the adversarial attack.","sentences":["Graph neural networks (GNNs) have exhibited prominent performance in learning graph-structured data.","Considering node classification task, based on the i.i.d assumption among node labels, the traditional supervised learning simply sums up cross-entropy losses of the independent training nodes and applies the average loss to optimize GNNs' weights.","But different from other data formats, the nodes are naturally connected.","It is found that the independent distribution modeling of node labels restricts GNNs' capability to generalize over the entire graph and defend adversarial attacks.","In this work, we propose a new framework, termed joint-cluster supervised learning, to model the joint distribution of each node with its corresponding cluster.","We learn the joint distribution of node and cluster labels conditioned on their representations, and train GNNs with the obtained joint loss.","In this way, the data-label reference signals extracted from the local cluster explicitly strengthen the discrimination ability on the target node.","The extensive experiments demonstrate that our joint-cluster supervised learning can effectively bolster GNNs' node classification accuracy.","Furthermore, being benefited from the reference signals which may be free from spiteful interference, our learning paradigm significantly protects the node classification from being affected by the adversarial attack."],"url":"http://arxiv.org/abs/2405.15564v1","category":"cs.LG"}
{"created":"2024-05-24 13:49:18","title":"When Generative AI Meets Workplace Learning: Creating A Realistic & Motivating Learning Experience With A Generative PCA","abstract":"Workplace learning is used to train employees systematically, e.g., via e-learning or in 1:1 training. However, this is often deemed ineffective and costly. Whereas pure e-learning lacks the possibility of conversational exercise and personal contact, 1:1 training with human instructors involves a high level of personnel and organizational costs. Hence, pedagogical conversational agents (PCAs), based on generative AI, seem to compensate for the disadvantages of both forms. Following Action Design Research, this paper describes an organizational communication training with a Generative PCA (GenPCA). The evaluation shows promising results: the agent was perceived positively among employees and contributed to an improvement in self-determined learning. However, the integration of such agent comes not without limitations. We conclude with suggestions concerning the didactical methods, which are supported by a GenPCA, and possible improvements of such an agent for workplace learning.","sentences":["Workplace learning is used to train employees systematically, e.g., via e-learning or in 1:1 training.","However, this is often deemed ineffective and costly.","Whereas pure e-learning lacks the possibility of conversational exercise and personal contact, 1:1 training with human instructors involves a high level of personnel and organizational costs.","Hence, pedagogical conversational agents (PCAs), based on generative AI, seem to compensate for the disadvantages of both forms.","Following Action Design Research, this paper describes an organizational communication training with a Generative PCA (GenPCA).","The evaluation shows promising results: the agent was perceived positively among employees and contributed to an improvement in self-determined learning.","However, the integration of such agent comes not without limitations.","We conclude with suggestions concerning the didactical methods, which are supported by a GenPCA, and possible improvements of such an agent for workplace learning."],"url":"http://arxiv.org/abs/2405.15561v1","category":"cs.HC"}
{"created":"2024-05-24 13:44:25","title":"Certifiably Robust RAG against Retrieval Corruption","abstract":"Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM responses from each passage in isolation and then securely aggregate these isolated responses. To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when the attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages. We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets.","sentences":["Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses.","In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks.","The key insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM responses from each passage in isolation and then securely aggregate these isolated responses.","To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses.","Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when the attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages.","We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets."],"url":"http://arxiv.org/abs/2405.15556v1","category":"cs.LG"}
{"created":"2024-05-24 13:36:00","title":"CowScreeningDB: A public benchmark dataset for lameness detection in dairy cows","abstract":"Lameness is one of the costliest pathological problems affecting dairy animals. It is usually assessed by trained veterinary clinicians who observe features such as gait symmetry or gait parameters as step counts in real-time. With the development of artificial intelligence, various modular systems have been proposed to minimize subjectivity in lameness assessment. However, the major limitation in their development is the unavailability of a public dataset which is currently either commercial or privately held. To tackle this limitation, we have introduced CowScreeningDB which was created using sensory data. This dataset was sourced from 43 cows at a dairy located in Gran Canaria, Spain. It consists of a multi-sensor dataset built on data collected using an Apple Watch 6 during the normal daily routine of a dairy cow. Thanks to the collection environment, sampling technique, information regarding the sensors, the applications used for data conversion and storage make the dataset a transparent one. This transparency of data can thus be used for further development of techniques for lameness detection for dairy cows which can be objectively compared. Aside from the public sharing of the dataset, we have also shared a machine-learning technique which classifies the caws in healthy and lame by using the raw sensory data. Hence validating the major objective which is to establish the relationship between sensor data and lameness.","sentences":["Lameness is one of the costliest pathological problems affecting dairy animals.","It is usually assessed by trained veterinary clinicians who observe features such as gait symmetry or gait parameters as step counts in real-time.","With the development of artificial intelligence, various modular systems have been proposed to minimize subjectivity in lameness assessment.","However, the major limitation in their development is the unavailability of a public dataset which is currently either commercial or privately held.","To tackle this limitation, we have introduced CowScreeningDB which was created using sensory data.","This dataset was sourced from 43 cows at a dairy located in Gran Canaria, Spain.","It consists of a multi-sensor dataset built on data collected using an Apple Watch 6 during the normal daily routine of a dairy cow.","Thanks to the collection environment, sampling technique, information regarding the sensors, the applications used for data conversion and storage make the dataset a transparent one.","This transparency of data can thus be used for further development of techniques for lameness detection for dairy cows which can be objectively compared.","Aside from the public sharing of the dataset, we have also shared a machine-learning technique which classifies the caws in healthy and lame by using the raw sensory data.","Hence validating the major objective which is to establish the relationship between sensor data and lameness."],"url":"http://arxiv.org/abs/2405.15550v1","category":"cs.CV"}
{"created":"2024-05-24 13:35:56","title":"SEP: Self-Enhanced Prompt Tuning for Visual-Language Model","abstract":"Prompt tuning based on Context Optimization (CoOp) effectively adapts visual-language models (VLMs) to downstream tasks by inferring additional learnable prompt tokens. However, these tokens are less discriminative as they are independent of the pre-trained tokens and fail to capture input-specific knowledge, such as class-aware textual or instance-aware visual knowledge. Leveraging the discriminative and generalization capabilities inherent in pre-trained tokens, we introduce a novel approach named Self-Enhanced Prompt Tuning (SEP). The core principle of SEP involves adapting the learnable prompt tokens at each encoder layer from the corresponding self-pretrained tokens, thereby explicitly incorporating discriminative prior knowledge to enhance both textual-level and visual-level embeddings. Furthermore, SEP's self-enhanced tokens not only boost discrimination but also mitigate domain shifts in unseen domains, enhancing generalization. In practice, SEP selects several representative tokens from all pre-trained tokens for each input data at every layer of the text/visual encoders. Subsequently, a Token Fusion Module (TFM) is introduced to generate a self-enhanced token by merging these representative tokens with the learnable tokens using a cross-attention mechanism. This self-enhanced token is then concatenated with all pre-trained tokens, serving as input for subsequent encoder layers to produce the relevant embeddings. Comprehensive evaluations across various benchmarks and tasks confirm SEP's efficacy in prompt tuning. Code: \\href{Code}{https://github.com/htyao89/SEP}.","sentences":["Prompt tuning based on Context Optimization (CoOp) effectively adapts visual-language models (VLMs) to downstream tasks by inferring additional learnable prompt tokens.","However, these tokens are less discriminative as they are independent of the pre-trained tokens and fail to capture input-specific knowledge, such as class-aware textual or instance-aware visual knowledge.","Leveraging the discriminative and generalization capabilities inherent in pre-trained tokens, we introduce a novel approach named Self-Enhanced Prompt Tuning (SEP).","The core principle of SEP involves adapting the learnable prompt tokens at each encoder layer from the corresponding self-pretrained tokens, thereby explicitly incorporating discriminative prior knowledge to enhance both textual-level and visual-level embeddings.","Furthermore, SEP's self-enhanced tokens not only boost discrimination but also mitigate domain shifts in unseen domains, enhancing generalization.","In practice, SEP selects several representative tokens from all pre-trained tokens for each input data at every layer of the text/visual encoders.","Subsequently, a Token Fusion Module (TFM) is introduced to generate a self-enhanced token by merging these representative tokens with the learnable tokens using a cross-attention mechanism.","This self-enhanced token is then concatenated with all pre-trained tokens, serving as input for subsequent encoder layers to produce the relevant embeddings.","Comprehensive evaluations across various benchmarks and tasks confirm SEP's efficacy in prompt tuning.","Code: \\href{Code}{https://github.com/htyao89/SEP}."],"url":"http://arxiv.org/abs/2405.15549v1","category":"cs.CV"}
{"created":"2024-05-24 13:34:52","title":"UAV-assisted C-RAN for On-demand Cellular Coverage: Opportunities and Challenges","abstract":"The deployment of beyond fifth-generation (5G) infrastructure over disaster-affected regions, temporary hotspot situations (e.g., massive gatherings, etc.), complex terrains (e.g., sea, hills, marshes, etc.) poses numerous challenges for cellular service providers. Recently, unmanned aerial vehicles (UAVs) have emerged as potential candidates to overcome the aforementioned technical issues based on their multi-role capabilities to serve as aerial base stations, mobile relays, and flying wireless access points. As such, the UAVs can act as portable platforms that can be deployed immediately on demand without requiring massive ground infrastructure to support wireless services. This article introduces the integration of UAVs to cloud radio access networks (C-RAN) for beyond 5G applications. The article mainly focuses on the underlying opportunities and challenges to realize the UAV-assisted C-RAN (UC-RAN) architecture in view of three generic application scenarios, i.e., disaster management, hotspots, and complex terrains. A preliminary performance analysis via simulation is further provided for the proposed UC-RAN under hotspot application scenario based on the relevant metrics.","sentences":["The deployment of beyond fifth-generation (5G) infrastructure over disaster-affected regions, temporary hotspot situations (e.g., massive gatherings, etc.), complex terrains (e.g., sea, hills, marshes, etc.) poses numerous challenges for cellular service providers.","Recently, unmanned aerial vehicles (UAVs) have emerged as potential candidates to overcome the aforementioned technical issues based on their multi-role capabilities to serve as aerial base stations, mobile relays, and flying wireless access points.","As such, the UAVs can act as portable platforms that can be deployed immediately on demand without requiring massive ground infrastructure to support wireless services.","This article introduces the integration of UAVs to cloud radio access networks (C-RAN) for beyond 5G applications.","The article mainly focuses on the underlying opportunities and challenges to realize the UAV-assisted C-RAN (UC-RAN) architecture in view of three generic application scenarios, i.e., disaster management, hotspots, and complex terrains.","A preliminary performance analysis via simulation is further provided for the proposed UC-RAN under hotspot application scenario based on the relevant metrics."],"url":"http://arxiv.org/abs/2405.15548v1","category":"cs.NI"}
{"created":"2024-05-24 13:33:51","title":"Quantum gravity inspired nonlocal quantum dynamics preserving the classical limit","abstract":"Several approaches to quantum gravity lead to nonlocal modifications of fields' dynamics. This, in turn, can give rise to nonlocal modifications of quantum mechanics at non-relativistic energies. Here, we analyze the nonlocal Schr\\\"{o}dinger evolution of a quantum harmonic oscillator in one such scenario, where the problem can be addressed without the use of perturbation theory. We demonstrate that although deviations from standard quantum predictions occur at low occupation numbers, where they could potentially be detected or constrained by high-precision experiments, the classical limits of quantum probability densities and free energy remain unaffected up to energies comparable with the nonlocality scale. These results provide an example of nonlocal quantum dynamics compatible with classical predictions, suggesting massive quantum objects as a promising avenue for testing some phenomenological aspects of quantum gravity.","sentences":["Several approaches to quantum gravity lead to nonlocal modifications of fields' dynamics.","This, in turn, can give rise to nonlocal modifications of quantum mechanics at non-relativistic energies.","Here, we analyze the nonlocal Schr\\\"{o}dinger evolution of a quantum harmonic oscillator in one such scenario, where the problem can be addressed without the use of perturbation theory.","We demonstrate that although deviations from standard quantum predictions occur at low occupation numbers, where they could potentially be detected or constrained by high-precision experiments, the classical limits of quantum probability densities and free energy remain unaffected up to energies comparable with the nonlocality scale.","These results provide an example of nonlocal quantum dynamics compatible with classical predictions, suggesting massive quantum objects as a promising avenue for testing some phenomenological aspects of quantum gravity."],"url":"http://arxiv.org/abs/2405.15546v1","category":"gr-qc"}
{"created":"2024-05-24 13:33:30","title":"Freya PAGE: First Optimal Time Complexity for Large-Scale Nonconvex Finite-Sum Optimization with Heterogeneous Asynchronous Computations","abstract":"In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times. We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations. By being robust to \"stragglers\" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions. The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods. Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit. This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when $\\sqrt{m} \\geq n$, where $n$ is # of workers, and $m$ is # of data samples.","sentences":["In practical distributed systems, workers are typically not homogeneous, and due to differences in hardware configurations and network conditions, can have highly varying processing times.","We consider smooth nonconvex finite-sum (empirical risk minimization) problems in this setup and introduce a new parallel method, Freya PAGE, designed to handle arbitrarily heterogeneous and asynchronous computations.","By being robust to \"stragglers\" and adaptively ignoring slow computations, Freya PAGE offers significantly improved time complexity guarantees compared to all previous methods, including Asynchronous SGD, Rennala SGD, SPIDER, and PAGE, while requiring weaker assumptions.","The algorithm relies on novel generic stochastic gradient collection strategies with theoretical guarantees that can be of interest on their own, and may be used in the design of future optimization methods.","Furthermore, we establish a lower bound for smooth nonconvex finite-sum problems in the asynchronous setup, providing a fundamental time complexity limit.","This lower bound is tight and demonstrates the optimality of Freya PAGE in the large-scale regime, i.e., when $\\sqrt{m} \\geq n$, where $n$ is # of workers, and $m$ is # of data samples."],"url":"http://arxiv.org/abs/2405.15545v1","category":"math.OC"}
{"created":"2024-05-24 13:31:19","title":"Knowledge-enhanced Relation Graph and Task Sampling for Few-shot Molecular Property Prediction","abstract":"Recently, few-shot molecular property prediction (FSMPP) has garnered increasing attention. Despite impressive breakthroughs achieved by existing methods, they often overlook the inherent many-to-many relationships between molecules and properties, which limits their performance. For instance, similar substructures of molecules can inspire the exploration of new compounds. Additionally, the relationships between properties can be quantified, with high-related properties providing more information in exploring the target property than those low-related. To this end, this paper proposes a novel meta-learning FSMPP framework (KRGTS), which comprises the Knowledge-enhanced Relation Graph module and the Task Sampling module. The knowledge-enhanced relation graph module constructs the molecule-property multi-relation graph (MPMRG) to capture the many-to-many relationships between molecules and properties. The task sampling module includes a meta-training task sampler and an auxiliary task sampler, responsible for scheduling the meta-training process and sampling high-related auxiliary tasks, respectively, thereby achieving efficient meta-knowledge learning and reducing noise introduction. Empirically, extensive experiments on five datasets demonstrate the superiority of KRGTS over a variety of state-of-the-art methods. The code is available in https://github.com/Vencent-Won/KRGTS-public.","sentences":["Recently, few-shot molecular property prediction (FSMPP) has garnered increasing attention.","Despite impressive breakthroughs achieved by existing methods, they often overlook the inherent many-to-many relationships between molecules and properties, which limits their performance.","For instance, similar substructures of molecules can inspire the exploration of new compounds.","Additionally, the relationships between properties can be quantified, with high-related properties providing more information in exploring the target property than those low-related.","To this end, this paper proposes a novel meta-learning FSMPP framework (KRGTS), which comprises the Knowledge-enhanced Relation Graph module and the Task Sampling module.","The knowledge-enhanced relation graph module constructs the molecule-property multi-relation graph (MPMRG) to capture the many-to-many relationships between molecules and properties.","The task sampling module includes a meta-training task sampler and an auxiliary task sampler, responsible for scheduling the meta-training process and sampling high-related auxiliary tasks, respectively, thereby achieving efficient meta-knowledge learning and reducing noise introduction.","Empirically, extensive experiments on five datasets demonstrate the superiority of KRGTS over a variety of state-of-the-art methods.","The code is available in https://github.com/Vencent-Won/KRGTS-public."],"url":"http://arxiv.org/abs/2405.15544v1","category":"q-bio.QM"}
{"created":"2024-05-24 13:29:12","title":"Learning Generalizable Human Motion Generator with Reinforcement Learning","abstract":"Text-driven human motion generation, as one of the vital tasks in computer-aided content creation, has recently attracted increasing attention. While pioneering research has largely focused on improving numerical performance metrics on given datasets, practical applications reveal a common challenge: existing methods often overfit specific motion expressions in the training data, hindering their ability to generalize to novel descriptions like unseen combinations of motions. This limitation restricts their broader applicability. We argue that the aforementioned problem primarily arises from the scarcity of available motion-text pairs, given the many-to-many nature of text-driven motion generation. To tackle this problem, we formulate text-to-motion generation as a Markov decision process and present \\textbf{InstructMotion}, which incorporate the trail and error paradigm in reinforcement learning for generalizable human motion generation. Leveraging contrastive pre-trained text and motion encoders, we delve into optimizing reward design to enable InstructMotion to operate effectively on both paired data, enhancing global semantic level text-motion alignment, and synthetic text-only data, facilitating better generalization to novel prompts without the need for ground-truth motion supervision. Extensive experiments on prevalent benchmarks and also our synthesized unpaired dataset demonstrate that the proposed InstructMotion achieves outstanding performance both quantitatively and qualitatively.","sentences":["Text-driven human motion generation, as one of the vital tasks in computer-aided content creation, has recently attracted increasing attention.","While pioneering research has largely focused on improving numerical performance metrics on given datasets, practical applications reveal a common challenge: existing methods often overfit specific motion expressions in the training data, hindering their ability to generalize to novel descriptions like unseen combinations of motions.","This limitation restricts their broader applicability.","We argue that the aforementioned problem primarily arises from the scarcity of available motion-text pairs, given the many-to-many nature of text-driven motion generation.","To tackle this problem, we formulate text-to-motion generation as a Markov decision process and present \\textbf{InstructMotion}, which incorporate the trail and error paradigm in reinforcement learning for generalizable human motion generation.","Leveraging contrastive pre-trained text and motion encoders, we delve into optimizing reward design to enable InstructMotion to operate effectively on both paired data, enhancing global semantic level text-motion alignment, and synthetic text-only data, facilitating better generalization to novel prompts without the need for ground-truth motion supervision.","Extensive experiments on prevalent benchmarks and also our synthesized unpaired dataset demonstrate that the proposed InstructMotion achieves outstanding performance both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2405.15541v1","category":"cs.CV"}
{"created":"2024-05-24 13:27:23","title":"A generalized neural tangent kernel for surrogate gradient learning","abstract":"State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.","sentences":["State-of-the-art neural network training methods depend on the gradient of the network function.","Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks.","To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL).","This method works well in practice but lacks theoretical foundation.","The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent.","Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL.","First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit.","To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL.","We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor.","Further, we illustrate our findings with numerical experiments.","Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL."],"url":"http://arxiv.org/abs/2405.15539v1","category":"stat.ML"}
{"created":"2024-05-24 13:27:22","title":"$\u03c0$ phase ambiguity of cosmic birefringence","abstract":"We point out that the rotation angle $\\beta$ of cosmic birefringence, which is a recently reported parity-violating signal in the cosmic microwave background (CMB), has a phase ambiguity of $n\\pi \\,(n\\in\\mathbb{Z})$. This ambiguity has a significant impact on the interpretation of the origin of cosmic birefringence. Assuming an axion-like particle as the origin of cosmic birefringence, this ambiguity can be partly broken by the anisotropic cosmic birefringence and the shape of the CMB angular power spectra. We also discuss constraints on $\\beta$ from existing experimental results.","sentences":["We point out that the rotation angle $\\beta$ of cosmic birefringence, which is a recently reported parity-violating signal in the cosmic microwave background (CMB), has a phase ambiguity of $n\\pi \\,(n\\in\\mathbb{Z})$. This ambiguity has a significant impact on the interpretation of the origin of cosmic birefringence.","Assuming an axion-like particle as the origin of cosmic birefringence, this ambiguity can be partly broken by the anisotropic cosmic birefringence and the shape of the CMB angular power spectra.","We also discuss constraints on $\\beta$ from existing experimental results."],"url":"http://arxiv.org/abs/2405.15538v1","category":"astro-ph.CO"}
{"created":"2024-05-24 13:26:39","title":"Do Not Trust Power Management: Challenges and Hints for Securing Future Trusted Execution Environments","abstract":"Over the past few years, several research groups have introduced innovative hardware designs for Trusted Execution Environments (TEEs), aiming to secure applications against potentially compromised privileged software, including the kernel. Since 2017, Tang et al. introduced a new class of software-enabled hardware attacks, which leverages energy management mechanisms. These attacks aim at bypassing TEE security guarantees and exposing sensitive information like cryptographic keys. They have increased in prevalence over the past few years. Despite that, current RISC-V TEE architectures have yet to incorporate them into their threat models. Proprietary implementations, such as Arm TrustZone and Intel SGX, embed countermeasures. However, these countermeasures are not viable in the long term and hinder the capabilities of energy management mechanisms. This article presents the first comprehensive knowledge survey of these attacks, along with an evaluation of literature countermeasures. Our analysis highlights a substantial security gap between assumed threat models and the actual ones, presenting considerable threats in modern systems-on-chip that can undermine even the security guarantees provided by TEEs. We advocate for the enhancement of the next generation of RISC-V TEEs to address these attacks within their threat models, and we believe this study will spur further community efforts in this direction.","sentences":["Over the past few years, several research groups have introduced innovative hardware designs for Trusted Execution Environments (TEEs), aiming to secure applications against potentially compromised privileged software, including the kernel.","Since 2017, Tang et al. introduced a new class of software-enabled hardware attacks, which leverages energy management mechanisms.","These attacks aim at bypassing TEE security guarantees and exposing sensitive information like cryptographic keys.","They have increased in prevalence over the past few years.","Despite that, current RISC-V TEE architectures have yet to incorporate them into their threat models.","Proprietary implementations, such as Arm TrustZone and Intel SGX, embed countermeasures.","However, these countermeasures are not viable in the long term and hinder the capabilities of energy management mechanisms.","This article presents the first comprehensive knowledge survey of these attacks, along with an evaluation of literature countermeasures.","Our analysis highlights a substantial security gap between assumed threat models and the actual ones, presenting considerable threats in modern systems-on-chip that can undermine even the security guarantees provided by TEEs.","We advocate for the enhancement of the next generation of RISC-V TEEs to address these attacks within their threat models, and we believe this study will spur further community efforts in this direction."],"url":"http://arxiv.org/abs/2405.15537v1","category":"cs.CR"}
{"created":"2024-05-24 13:25:28","title":"A graph-space optimal transport FWI approach based on \u03ba-generalized Gaussian distribution","abstract":"The statistical basis for conventional full-waveform inversion (FWI) approaches is commonly associated with Gaussian statistics. However, errors are rarely Gaussian in non-linear problems like FWI. In this work, we investigate the portability of a new objective function for FWI applications based on the graph-space optimal transport and $\\kappa$-generalized Gaussian probability distribution. In particular, we demonstrate that the proposed objective function is robust in mitigating two critical problems in FWI, which are associated with cycle skipping issues and non-Gaussian errors. The results reveal that our proposal can mitigate the negative influence of cycle-skipping ambiguity and non-Gaussian noises and reduce the computational runtime for computing the transport plan associated with the optimal transport theory.","sentences":["The statistical basis for conventional full-waveform inversion (FWI) approaches is commonly associated with Gaussian statistics.","However, errors are rarely Gaussian in non-linear problems like FWI.","In this work, we investigate the portability of a new objective function for FWI applications based on the graph-space optimal transport and $\\kappa$-generalized Gaussian probability distribution.","In particular, we demonstrate that the proposed objective function is robust in mitigating two critical problems in FWI, which are associated with cycle skipping issues and non-Gaussian errors.","The results reveal that our proposal can mitigate the negative influence of cycle-skipping ambiguity and non-Gaussian noises and reduce the computational runtime for computing the transport plan associated with the optimal transport theory."],"url":"http://arxiv.org/abs/2405.15536v1","category":"physics.geo-ph"}
{"created":"2024-05-24 13:21:37","title":"Electron localization function for non-collinear spins","abstract":"We extend the electron localization function -- a popular descriptor of molecular bonds and atomic shells -- to general non-collinearly magnetized states, open-shell solutions of the Pauli-Schr{\\\"o}dinger equation. The description is also improved in paradigmatic, collinear states.","sentences":["We extend the electron localization function -- a popular descriptor of molecular bonds and atomic shells -- to general non-collinearly magnetized states, open-shell solutions of the Pauli-Schr{\\\"o}dinger equation.","The description is also improved in paradigmatic, collinear states."],"url":"http://arxiv.org/abs/2405.15530v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 13:09:52","title":"Polyp Segmentation Generalisability of Pretrained Backbones","abstract":"It has recently been demonstrated that pretraining backbones in a self-supervised manner generally provides better fine-tuned polyp segmentation performance, and that models with ViT-B backbones typically perform better than models with ResNet50 backbones. In this paper, we extend this recent work to consider generalisability. I.e., we assess the performance of models on a different dataset to that used for fine-tuning, accounting for variation in network architecture and pretraining pipeline (algorithm and dataset). This reveals how well models with different pretrained backbones generalise to data of a somewhat different distribution to the training data, which will likely arise in deployment due to different cameras and demographics of patients, amongst other factors. We observe that the previous findings, regarding pretraining pipelines for polyp segmentation, hold true when considering generalisability. However, our results imply that models with ResNet50 backbones typically generalise better, despite being outperformed by models with ViT-B backbones in evaluation on the test set from the same dataset used for fine-tuning.","sentences":["It has recently been demonstrated that pretraining backbones in a self-supervised manner generally provides better fine-tuned polyp segmentation performance, and that models with ViT-B backbones typically perform better than models with ResNet50 backbones.","In this paper, we extend this recent work to consider generalisability.","I.e., we assess the performance of models on a different dataset to that used for fine-tuning, accounting for variation in network architecture and pretraining pipeline (algorithm and dataset).","This reveals how well models with different pretrained backbones generalise to data of a somewhat different distribution to the training data, which will likely arise in deployment due to different cameras and demographics of patients, amongst other factors.","We observe that the previous findings, regarding pretraining pipelines for polyp segmentation, hold true when considering generalisability.","However, our results imply that models with ResNet50 backbones typically generalise better, despite being outperformed by models with ViT-B backbones in evaluation on the test set from the same dataset used for fine-tuning."],"url":"http://arxiv.org/abs/2405.15524v1","category":"cs.CV"}
{"created":"2024-05-24 13:05:05","title":"Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models","abstract":"The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent. Copyright traps have been proposed to be injected into the original content, improving content detectability in newly released LLMs. Traps, however, rely on the exact duplication of a unique text sequence, leaving them vulnerable to commonly deployed data deduplication techniques. We here propose the generation of fuzzy copyright traps, featuring slight modifications across duplication. When injected in the fine-tuning data of a 1.3B LLM, we show fuzzy trap sequences to be memorized nearly as well as exact duplicates. Specifically, the Membership Inference Attack (MIA) ROC AUC only drops from 0.90 to 0.87 when 4 tokens are replaced across the fuzzy duplicates. We also find that selecting replacement positions to minimize the exact overlap between fuzzy duplicates leads to similar memorization, while making fuzzy duplicates highly unlikely to be removed by any deduplication process. Lastly, we argue that the fact that LLMs memorize across fuzzy duplicates challenges the study of LLM memorization relying on naturally occurring duplicates. Indeed, we find that the commonly used training dataset, The Pile, contains significant amounts of fuzzy duplicates. This introduces a previously unexplored confounding factor in post-hoc studies of LLM memorization, and questions the effectiveness of (exact) data deduplication as a privacy protection technique.","sentences":["The immense datasets used to develop Large Language Models (LLMs) often include copyright-protected content, typically without the content creator's consent.","Copyright traps have been proposed to be injected into the original content, improving content detectability in newly released LLMs.","Traps, however, rely on the exact duplication of a unique text sequence, leaving them vulnerable to commonly deployed data deduplication techniques.","We here propose the generation of fuzzy copyright traps, featuring slight modifications across duplication.","When injected in the fine-tuning data of a 1.3B LLM, we show fuzzy trap sequences to be memorized nearly as well as exact duplicates.","Specifically, the Membership Inference Attack (MIA) ROC AUC only drops from 0.90 to 0.87 when 4 tokens are replaced across the fuzzy duplicates.","We also find that selecting replacement positions to minimize the exact overlap between fuzzy duplicates leads to similar memorization, while making fuzzy duplicates highly unlikely to be removed by any deduplication process.","Lastly, we argue that the fact that LLMs memorize across fuzzy duplicates challenges the study of LLM memorization relying on naturally occurring duplicates.","Indeed, we find that the commonly used training dataset, The Pile, contains significant amounts of fuzzy duplicates.","This introduces a previously unexplored confounding factor in post-hoc studies of LLM memorization, and questions the effectiveness of (exact) data deduplication as a privacy protection technique."],"url":"http://arxiv.org/abs/2405.15523v1","category":"cs.CL"}
{"created":"2024-05-24 13:04:47","title":"A derivation of the first generation particle masses from internal spacetime","abstract":"Internal spacetime geometry was recently introduced to model certain quantum phenomena using spacetime metrics that are degenerate. We use the Ricci tensors of these metrics to derive a ratio of the bare up and down quark masses, obtaining $m_u/m_d = 9604/19683 \\approx .4879$. This value is within the lattice QCD value at $2 \\operatorname{GeV}$ in the $\\overline{\\operatorname{MS}}$-scheme, $.473 \\pm .023$. Moreover, using the Levi-Cevita Poisson equation, we derive ratios of the dressed electron mass and bare quark masses. For a dressed electron mass of $.511 \\operatorname{MeV}$, these ratios yield the bare quark masses $m_u \\approx 2.2440 \\operatorname{MeV}$ and $m_d \\approx 4.599 \\operatorname{MeV}$, which are within/near the lattice QCD values $m^{\\overline{\\operatorname{MS}}}_u = (2.20\\pm .10) \\operatorname{MeV}$ and $m^{\\overline{\\operatorname{MS}}}_d = (4.69 \\pm .07) \\operatorname{MeV}$. Finally, using $4$-accelerations, we derive the ratio $\\tilde{m}_u/\\tilde{m}_d = 48/49 \\approx .98$ of the constituent up and down quark masses. This value is within the $.97 \\sim 1$ range of constituent quark models. All of the ratios we obtain are from first principles alone, with no free or ad hoc parameters. Furthermore, and rather curiously, our derivations do not use quantum field theory, but only tools from general relativity.","sentences":["Internal spacetime geometry was recently introduced to model certain quantum phenomena using spacetime metrics that are degenerate.","We use the Ricci tensors of these metrics to derive a ratio of the bare up and down quark masses, obtaining $m_u/m_d","= 9604/19683 \\approx .4879$.","This value is within the lattice QCD value at $2 \\operatorname{GeV}$ in the $\\overline{\\operatorname{MS}}$-scheme, $.473 \\pm .023$.","Moreover, using the Levi-Cevita Poisson equation, we derive ratios of the dressed electron mass and bare quark masses.","For a dressed electron mass of $.511 \\operatorname{MeV}$, these ratios yield the bare quark masses $m_u","\\approx 2.2440 \\operatorname{MeV}$ and $m_d \\approx 4.599 \\operatorname{MeV}$, which are within/near the lattice QCD values $m^{\\overline{\\operatorname{MS}}}_u = (2.20\\pm .10) \\operatorname{MeV}$ and $m^{\\overline{\\operatorname{MS}}}_d = (4.69 \\pm .07) \\operatorname{MeV}$.","Finally, using $4$-accelerations, we derive the ratio $\\tilde{m}_u/\\tilde{m}_d = 48/49 \\approx .98$ of the constituent up and down quark masses.","This value is within the $.97 \\sim 1$ range of constituent quark models.","All of the ratios we obtain are from first principles alone, with no free or ad hoc parameters.","Furthermore, and rather curiously, our derivations do not use quantum field theory, but only tools from general relativity."],"url":"http://arxiv.org/abs/2405.15522v1","category":"physics.gen-ph"}
{"created":"2024-05-24 13:03:34","title":"A Preference-oriented Diversity Model Based on Mutual-information in Re-ranking for E-commerce Search","abstract":"Re-ranking is a process of rearranging ranking list to more effectively meet user demands by accounting for the interrelationships between items. Existing methods predominantly enhance the precision of search results, often at the expense of diversity, leading to outcomes that may not fulfill the varied needs of users. Conversely, methods designed to promote diversity might compromise the precision of the results, failing to satisfy the users' requirements for accuracy. To alleviate the above problems, this paper proposes a Preference-oriented Diversity Model Based on Mutual-information (PODM-MI), which consider both accuracy and diversity in the re-ranking process. Specifically, PODM-MI adopts Multidimensional Gaussian distributions based on variational inference to capture users' diversity preferences with uncertainty. Then we maximize the mutual information between the diversity preferences of the users and the candidate items using the maximum variational inference lower bound to enhance their correlations. Subsequently, we derive a utility matrix based on the correlations, enabling the adaptive ranking of items in line with user preferences and establishing a balance between the aforementioned objectives. Experimental results on real-world online e-commerce systems demonstrate the significant improvements of PODM-MI, and we have successfully deployed PODM-MI on an e-commerce search platform.","sentences":["Re-ranking is a process of rearranging ranking list to more effectively meet user demands by accounting for the interrelationships between items.","Existing methods predominantly enhance the precision of search results, often at the expense of diversity, leading to outcomes that may not fulfill the varied needs of users.","Conversely, methods designed to promote diversity might compromise the precision of the results, failing to satisfy the users' requirements for accuracy.","To alleviate the above problems, this paper proposes a Preference-oriented Diversity Model Based on Mutual-information (PODM-MI), which consider both accuracy and diversity in the re-ranking process.","Specifically, PODM-MI adopts Multidimensional Gaussian distributions based on variational inference to capture users' diversity preferences with uncertainty.","Then we maximize the mutual information between the diversity preferences of the users and the candidate items using the maximum variational inference lower bound to enhance their correlations.","Subsequently, we derive a utility matrix based on the correlations, enabling the adaptive ranking of items in line with user preferences and establishing a balance between the aforementioned objectives.","Experimental results on real-world online e-commerce systems demonstrate the significant improvements of PODM-MI, and we have successfully deployed PODM-MI on an e-commerce search platform."],"url":"http://arxiv.org/abs/2405.15521v1","category":"cs.IR"}
{"created":"2024-05-24 13:03:20","title":"From Data Complexity to User Simplicity: A Framework for Linked Open Data Reconciliation and Serendipitous Discovery","abstract":"This article introduces a novel software solution to create a Web portal to align Linked Open Data sources and provide user-friendly interfaces for serendipitous discovery. We present the Polifonia Web portal as a motivating scenario and case study to address research problems such as data reconciliation and serving generous interfaces in the music heritage domain.","sentences":["This article introduces a novel software solution to create a Web portal to align Linked Open Data sources and provide user-friendly interfaces for serendipitous discovery.","We present the Polifonia Web portal as a motivating scenario and case study to address research problems such as data reconciliation and serving generous interfaces in the music heritage domain."],"url":"http://arxiv.org/abs/2405.15520v1","category":"cs.IR"}
{"created":"2024-05-24 13:02:29","title":"Feature Splatting for Better Novel View Synthesis with Low Overlap","abstract":"3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives. However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views. In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat). To synthesize a novel view, Gaussians are first \"splatted\" into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values. To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information. Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views. Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels. We will release the code upon acceptance.   Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting","sentences":["3D Gaussian Splatting has emerged as a very promising scene representation, achieving state-of-the-art quality in novel view synthesis significantly faster than competing alternatives.","However, its use of spherical harmonics to represent scene colors limits the expressivity of 3D Gaussians and, as a consequence, the capability of the representation to generalize as we move away from the training views.","In this paper, we propose to encode the color information of 3D Gaussians into per-Gaussian feature vectors, which we denote as Feature Splatting (FeatSplat).","To synthesize a novel view, Gaussians are first \"splatted\" into the image plane, then the corresponding feature vectors are alpha-blended, and finally the blended vector is decoded by a small MLP to render the RGB pixel values.","To further inform the model, we concatenate a camera embedding to the blended feature vector, to condition the decoding also on the viewpoint information.","Our experiments show that these novel model for encoding the radiance considerably improves novel view synthesis for low overlap views that are distant from the training views.","Finally, we also show the capacity and convenience of our feature vector representation, demonstrating its capability not only to generate RGB values for novel views, but also their per-pixel semantic labels.","We will release the code upon acceptance.   ","Keywords: Gaussian Splatting, Novel View Synthesis, Feature Splatting"],"url":"http://arxiv.org/abs/2405.15518v1","category":"cs.CV"}
{"created":"2024-05-24 12:57:40","title":"On the Convexity and Reliability of the Bethe Free Energy Approximation","abstract":"The Bethe free energy approximation provides an effective way for relaxing NP-hard problems of probabilistic inference. However, its accuracy depends on the model parameters and particularly degrades if a phase transition in the model occurs. In this work, we analyze when the Bethe approximation is reliable and how this can be verified. We argue and show by experiment that it is mostly accurate if it is convex on a submanifold of its domain, the 'Bethe box'. For verifying its convexity, we derive two sufficient conditions that are based on the definiteness properties of the Bethe Hessian matrix: the first uses the concept of diagonal dominance, and the second decomposes the Bethe Hessian matrix into a sum of sparse matrices and characterizes the definiteness properties of the individual matrices in that sum. These theoretical results provide a simple way to estimate the critical phase transition temperature of a model. As a practical contribution we propose $\\texttt{BETHE-MIN}$, a projected quasi-Newton method to efficiently find a minimum of the Bethe free energy.","sentences":["The Bethe free energy approximation provides an effective way for relaxing NP-hard problems of probabilistic inference.","However, its accuracy depends on the model parameters and particularly degrades if a phase transition in the model occurs.","In this work, we analyze when the Bethe approximation is reliable and how this can be verified.","We argue and show by experiment that it is mostly accurate if it is convex on a submanifold of its domain, the 'Bethe box'.","For verifying its convexity, we derive two sufficient conditions that are based on the definiteness properties of the Bethe Hessian matrix: the first uses the concept of diagonal dominance, and the second decomposes the Bethe Hessian matrix into a sum of sparse matrices and characterizes the definiteness properties of the individual matrices in that sum.","These theoretical results provide a simple way to estimate the critical phase transition temperature of a model.","As a practical contribution we propose $\\texttt{BETHE-MIN}$, a projected quasi-Newton method to efficiently find a minimum of the Bethe free energy."],"url":"http://arxiv.org/abs/2405.15514v1","category":"stat.ML"}
{"created":"2024-05-24 12:57:29","title":"Seismic fragility curves fitting revisited: ordinal regression models and their generalization","abstract":"This research conducts a thorough reevaluation of seismic fragility curves by utilizing ordinal regression models, moving away from the commonly used log-normal distribution function known for its simplicity. It explores the nuanced differences and interrelations among various ordinal regression approaches, including Cumulative, Sequential, and Adjacent Category models, alongside their enhanced versions that incorporate category-specific effects and variance heterogeneity. The study applies these methodologies to empirical bridge damage data from the 2008 Wenchuan earthquake, using both frequentist and Bayesian inference methods, and conducts model diagnostics using surrogate residuals. The analysis covers eleven models, from basic to those with heteroscedastic extensions and category-specific effects. Through rigorous leave-one-out cross-validation, the Sequential model with category-specific effects emerges as the most effective. The findings underscore a notable divergence in damage probability predictions between this model and conventional Cumulative probit models, advocating for a substantial transition towards more adaptable fragility curve modeling techniques that enhance the precision of seismic risk assessments. In conclusion, this research not only readdresses the challenge of fitting seismic fragility curves but also advances methodological standards and expands the scope of seismic fragility analysis. It advocates for ongoing innovation and critical reevaluation of conventional methods to advance the predictive accuracy and applicability of seismic fragility models within the performance-based earthquake engineering domain.","sentences":["This research conducts a thorough reevaluation of seismic fragility curves by utilizing ordinal regression models, moving away from the commonly used log-normal distribution function known for its simplicity.","It explores the nuanced differences and interrelations among various ordinal regression approaches, including Cumulative, Sequential, and Adjacent Category models, alongside their enhanced versions that incorporate category-specific effects and variance heterogeneity.","The study applies these methodologies to empirical bridge damage data from the 2008 Wenchuan earthquake, using both frequentist and Bayesian inference methods, and conducts model diagnostics using surrogate residuals.","The analysis covers eleven models, from basic to those with heteroscedastic extensions and category-specific effects.","Through rigorous leave-one-out cross-validation, the Sequential model with category-specific effects emerges as the most effective.","The findings underscore a notable divergence in damage probability predictions between this model and conventional Cumulative probit models, advocating for a substantial transition towards more adaptable fragility curve modeling techniques that enhance the precision of seismic risk assessments.","In conclusion, this research not only readdresses the challenge of fitting seismic fragility curves but also advances methodological standards and expands the scope of seismic fragility analysis.","It advocates for ongoing innovation and critical reevaluation of conventional methods to advance the predictive accuracy and applicability of seismic fragility models within the performance-based earthquake engineering domain."],"url":"http://arxiv.org/abs/2405.15513v1","category":"stat.AP"}
{"created":"2024-05-24 12:56:18","title":"ChatGPT Code Detection: Techniques for Uncovering the Source of Code","abstract":"In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI). As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education. This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM. We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of 98%. For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated. Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach. Both approaches work well but provide at most 85-88% accuracy. We also show that untrained humans solve the same task not better than random guessing. This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming.","sentences":["In recent times, large language models (LLMs) have made significant strides in generating computer code, blurring the lines between code created by humans and code produced by artificial intelligence (AI).","As these technologies evolve rapidly, it is crucial to explore how they influence code generation, especially given the risk of misuse in areas like higher education.","This paper explores this issue by using advanced classification techniques to differentiate between code written by humans and that generated by ChatGPT, a type of LLM.","We employ a new approach that combines powerful embedding features (black-box) with supervised learning algorithms - including Deep Neural Networks, Random Forests, and Extreme Gradient Boosting - to achieve this differentiation with an impressive accuracy of 98%.","For the successful combinations, we also examine their model calibration, showing that some of the models are extremely well calibrated.","Additionally, we present white-box features and an interpretable Bayes classifier to elucidate critical differences between the code sources, enhancing the explainability and transparency of our approach.","Both approaches work well but provide at most 85-88% accuracy.","We also show that untrained humans solve the same task not better than random guessing.","This study is crucial in understanding and mitigating the potential risks associated with using AI in code generation, particularly in the context of higher education, software development, and competitive programming."],"url":"http://arxiv.org/abs/2405.15512v1","category":"cs.LG"}
{"created":"2024-05-24 12:55:34","title":"On Colimits and Model Structures in Various Categories of Manifolds","abstract":"After explaining the importance of model categories in abstract homotopy theory, we provide concrete examples demonstrating that various categories of manifolds do not have all finite colimits, and hence cannot be model categories. We then consider various enlargements of our categories of manifolds, culminating in categories of presheaves. We explain how to produce model structures on these enlarged categories, culminating with answering an open problem involving Poincar\\'{e} spaces.","sentences":["After explaining the importance of model categories in abstract homotopy theory, we provide concrete examples demonstrating that various categories of manifolds do not have all finite colimits, and hence cannot be model categories.","We then consider various enlargements of our categories of manifolds, culminating in categories of presheaves.","We explain how to produce model structures on these enlarged categories, culminating with answering an open problem involving Poincar\\'{e} spaces."],"url":"http://arxiv.org/abs/2405.15511v1","category":"math.AT"}
{"created":"2024-05-24 12:53:10","title":"Double EPW-cubes: automorphisms and degenerations","abstract":"We describe the group of naturally polarized automorphisms of a double EPW-cube. As an application, we exhibit examples of manifolds of K3^[3]-type with a symplectic action of a large group and determine their transcendental lattices. We study degenerations of smooth double EPW-cubes and give a resolution of mildly singular double EPW-cubes, this gives a family of hyperkaehler manifolds of K3^[3]-type whose general element is not birational to a moduli space of sheaves on a K3 surface.","sentences":["We describe the group of naturally polarized automorphisms of a double EPW-cube.","As an application, we exhibit examples of manifolds of K3^[3]-type with a symplectic action of a large group and determine their transcendental lattices.","We study degenerations of smooth double EPW-cubes and give a resolution of mildly singular double EPW-cubes, this gives a family of hyperkaehler manifolds of K3^[3]-type whose general element is not birational to a moduli space of sheaves on a K3 surface."],"url":"http://arxiv.org/abs/2405.15510v1","category":"math.AG"}
{"created":"2024-05-24 12:53:07","title":"Randomized algorithms and PAC bounds for inverse reinforcement learning in continuous spaces","abstract":"This work studies discrete-time discounted Markov decision processes with continuous state and action spaces and addresses the inverse problem of inferring a cost function from observed optimal behavior. We first consider the case in which we have access to the entire expert policy and characterize the set of solutions to the inverse problem by using occupation measures, linear duality, and complementary slackness conditions. To avoid trivial solutions and ill-posedness, we introduce a natural linear normalization constraint. This results in an infinite-dimensional linear feasibility problem, prompting a thorough analysis of its properties. Next, we use linear function approximators and adopt a randomized approach, namely the scenario approach and related probabilistic feasibility guarantees, to derive epsilon-optimal solutions for the inverse problem. We further discuss the sample complexity for a desired approximation accuracy. Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples.","sentences":["This work studies discrete-time discounted Markov decision processes with continuous state and action spaces and addresses the inverse problem of inferring a cost function from observed optimal behavior.","We first consider the case in which we have access to the entire expert policy and characterize the set of solutions to the inverse problem by using occupation measures, linear duality, and complementary slackness conditions.","To avoid trivial solutions and ill-posedness, we introduce a natural linear normalization constraint.","This results in an infinite-dimensional linear feasibility problem, prompting a thorough analysis of its properties.","Next, we use linear function approximators and adopt a randomized approach, namely the scenario approach and related probabilistic feasibility guarantees, to derive epsilon-optimal solutions for the inverse problem.","We further discuss the sample complexity for a desired approximation accuracy.","Finally, we deal with the more realistic case where we only have access to a finite set of expert demonstrations and a generative model and provide bounds on the error made when working with samples."],"url":"http://arxiv.org/abs/2405.15509v1","category":"math.OC"}
{"created":"2024-05-24 12:51:23","title":"Learning to Discretize Denoising Diffusion ODEs","abstract":"Diffusion Probabilistic Models (DPMs) are powerful generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. However, sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, a crucial problem is to reduce NFE while preserving generation quality. To this end, we propose LD3, a lightweight framework for learning time discretization while sampling from the diffusion ODE encapsulated by DPMs. LD3 can be combined with various diffusion ODE solvers and consistently improves performance without retraining resource-intensive neural networks. We demonstrate analytically and empirically that LD3 enhances sampling efficiency compared to distillation-based methods, without the extensive computational overhead. We evaluate our method with extensive experiments on 5 datasets, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. For example, in about 5 minutes of training on a single GPU, our method reduces the FID score from 6.63 to 2.68 on CIFAR10 (7 NFE), and in around 20 minutes, decreases the FID from 8.51 to 5.03 on class-conditional ImageNet-256 (5 NFE). LD3 complements distillation methods, offering a more efficient approach to sampling from pre-trained diffusion models.","sentences":["Diffusion Probabilistic Models (DPMs) are powerful generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation.","However, sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs.","Therefore, a crucial problem is to reduce NFE while preserving generation quality.","To this end, we propose LD3, a lightweight framework for learning time discretization while sampling from the diffusion ODE encapsulated by DPMs.","LD3 can be combined with various diffusion ODE solvers and consistently improves performance without retraining resource-intensive neural networks.","We demonstrate analytically and empirically that LD3 enhances sampling efficiency compared to distillation-based methods, without the extensive computational overhead.","We evaluate our method with extensive experiments on 5 datasets, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs.","For example, in about 5 minutes of training on a single GPU, our method reduces the FID score from 6.63 to 2.68 on CIFAR10 (7 NFE), and in around 20 minutes, decreases the FID from 8.51 to 5.03 on class-conditional ImageNet-256 (5 NFE).","LD3 complements distillation methods, offering a more efficient approach to sampling from pre-trained diffusion models."],"url":"http://arxiv.org/abs/2405.15506v1","category":"cs.LG"}
{"created":"2024-05-24 12:48:24","title":"Revisiting Counterfactual Regression through the Lens of Gromov-Wasserstein Information Bottleneck","abstract":"As a promising individualized treatment effect (ITE) estimation method, counterfactual regression (CFR) maps individuals' covariates to a latent space and predicts their counterfactual outcomes. However, the selection bias between control and treatment groups often imbalances the two groups' latent distributions and negatively impacts this method's performance. In this study, we revisit counterfactual regression through the lens of information bottleneck and propose a novel learning paradigm called Gromov-Wasserstein information bottleneck (GWIB). In this paradigm, we learn CFR by maximizing the mutual information between covariates' latent representations and outcomes while penalizing the kernelized mutual information between the latent representations and the covariates. We demonstrate that the upper bound of the penalty term can be implemented as a new regularizer consisting of $i)$ the fused Gromov-Wasserstein distance between the latent representations of different groups and $ii)$ the gap between the transport cost generated by the model and the cross-group Gromov-Wasserstein distance between the latent representations and the covariates. GWIB effectively learns the CFR model through alternating optimization, suppressing selection bias while avoiding trivial latent distributions. Experiments on ITE estimation tasks show that GWIB consistently outperforms state-of-the-art CFR methods. To promote the research community, we release our project at https://github.com/peteryang1031/Causal-GWIB.","sentences":["As a promising individualized treatment effect (ITE) estimation method, counterfactual regression (CFR) maps individuals' covariates to a latent space and predicts their counterfactual outcomes.","However, the selection bias between control and treatment groups often imbalances the two groups' latent distributions and negatively impacts this method's performance.","In this study, we revisit counterfactual regression through the lens of information bottleneck and propose a novel learning paradigm called Gromov-Wasserstein information bottleneck (GWIB).","In this paradigm, we learn CFR by maximizing the mutual information between covariates' latent representations and outcomes while penalizing the kernelized mutual information between the latent representations and the covariates.","We demonstrate that the upper bound of the penalty term can be implemented as a new regularizer consisting of $i)$ the fused Gromov-Wasserstein distance between the latent representations of different groups and $ii)$ the gap between the transport cost generated by the model and the cross-group Gromov-Wasserstein distance between the latent representations and the covariates.","GWIB effectively learns the CFR model through alternating optimization, suppressing selection bias while avoiding trivial latent distributions.","Experiments on ITE estimation tasks show that GWIB consistently outperforms state-of-the-art CFR methods.","To promote the research community, we release our project at https://github.com/peteryang1031/Causal-GWIB."],"url":"http://arxiv.org/abs/2405.15505v1","category":"cs.LG"}
{"created":"2024-05-24 12:41:02","title":"Unsupervised Deep Neural Network Approach To Solve Fermionic Systems","abstract":"Solving the Schr\\\"{o}dinger equation for interacting many-body quantum systems faces computational challenges due to exponential scaling with system size. This complexity limits the study of important phenomena in materials science and physics. We develop an Artificial Neural Network (ANN)-driven algorithm to simulate fermionic systems on lattices. Our method uses Pauli matrices to represent quantum states, incorporates Markov Chain Monte Carlo sampling, and leverages an adaptive momentum optimizer. We demonstrate the algorithm's accuracy by simulating the Heisenberg Hamiltonian on a one-dimensional lattice, achieving results with an error in the order of $10^{-4}$ compared to exact diagonalization. Furthermore, we successfully model a magnetic phase transition in a two-dimensional lattice under an applied magnetic field. Importantly, our approach avoids the sign problem common to traditional Fermionic Monte Carlo methods, enabling the investigation of frustrated systems. This work demonstrates the potential of ANN-based algorithms for efficient simulation of complex quantum systems, opening avenues for discoveries in condensed matter physics and materials science.","sentences":["Solving the Schr\\\"{o}dinger equation for interacting many-body quantum systems faces computational challenges due to exponential scaling with system size.","This complexity limits the study of important phenomena in materials science and physics.","We develop an Artificial Neural Network (ANN)-driven algorithm to simulate fermionic systems on lattices.","Our method uses Pauli matrices to represent quantum states, incorporates Markov Chain Monte Carlo sampling, and leverages an adaptive momentum optimizer.","We demonstrate the algorithm's accuracy by simulating the Heisenberg Hamiltonian on a one-dimensional lattice, achieving results with an error in the order of $10^{-4}$ compared to exact diagonalization.","Furthermore, we successfully model a magnetic phase transition in a two-dimensional lattice under an applied magnetic field.","Importantly, our approach avoids the sign problem common to traditional Fermionic Monte Carlo methods, enabling the investigation of frustrated systems.","This work demonstrates the potential of ANN-based algorithms for efficient simulation of complex quantum systems, opening avenues for discoveries in condensed matter physics and materials science."],"url":"http://arxiv.org/abs/2405.15502v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 12:38:49","title":"Energy of Gravitational Radiation and the Background Energy of the Space-Time","abstract":"We address the issue of gravitational radiation in the context of the Bondi-Sachs space-time, and consider the expression for the gravitational energy of the radiation obtained in the realm of the teleparallel equivalent of general relativity (TEGR). This expression is independent of the radial distance (i.e., of powers of $1/r$) and depends exclusively on the functions $c(u,\\theta,\\phi)$ and $d(u,\\theta,\\phi)$, which yield the news functions ($u$ is the retarded time, $u=t-r$). We investigate the mathematical and physical features of this energy expression in the simpler framework of axial symmetry. Once a burst of gravitational radiation takes place in a self gravitating system, that leads to a loss of the Bondi mass, gravitational radiation is emitted throughout the whole space-time. The existence and presence of this radiation in the background structure of the space-time is consistent with the analysis developed by Papapetrou, and Hallidy and Janis, who found no proof that a gravitational system that emits a burst of gravitational radiation is preceded and followed by two stationary gravitational field configurations, namely, it seems that it is impossible for a gravitational system, which is initially stationary, to return to a stationary state after emitting a burst of axially symmetric gravitational radiation, in which case the space-time is not even asymptotically stationary. Therefore, it is plausible that the gravitational energy of radiation is present in the background structure of the space-time, and this is the energy predicted in the TEGR. This analysis lead us to conjecture that the noise detected in the large terrestrial gravitational wave observatories is intrinsically related to the background gravitational radiation.","sentences":["We address the issue of gravitational radiation in the context of the Bondi-Sachs space-time, and consider the expression for the gravitational energy of the radiation obtained in the realm of the teleparallel equivalent of general relativity (TEGR).","This expression is independent of the radial distance (i.e., of powers of $1/r$) and depends exclusively on the functions $c(u,\\theta,\\phi)$ and $d(u,\\theta,\\phi)$, which yield the news functions ($u$ is the retarded time, $u=t-r$).","We investigate the mathematical and physical features of this energy expression in the simpler framework of axial symmetry.","Once a burst of gravitational radiation takes place in a self gravitating system, that leads to a loss of the Bondi mass, gravitational radiation is emitted throughout the whole space-time.","The existence and presence of this radiation in the background structure of the space-time is consistent with the analysis developed by Papapetrou, and Hallidy and Janis, who found no proof that a gravitational system that emits a burst of gravitational radiation is preceded and followed by two stationary gravitational field configurations, namely, it seems that it is impossible for a gravitational system, which is initially stationary, to return to a stationary state after emitting a burst of axially symmetric gravitational radiation, in which case the space-time is not even asymptotically stationary.","Therefore, it is plausible that the gravitational energy of radiation is present in the background structure of the space-time, and this is the energy predicted in the TEGR.","This analysis lead us to conjecture that the noise detected in the large terrestrial gravitational wave observatories is intrinsically related to the background gravitational radiation."],"url":"http://arxiv.org/abs/2405.15499v1","category":"gr-qc"}
{"created":"2024-05-24 12:34:32","title":"Finite-time convergence to an $\u03b5$-efficient Nash equilibrium in potential games","abstract":"This paper investigates the convergence time of log-linear learning to an $\\epsilon$-efficient Nash equilibrium (NE) in potential games. In such games, an efficient NE is defined as the maximizer of the potential function. Existing results are limited to potential games with stringent structural assumptions and entail exponential convergence times in $1/\\epsilon$. Unaddressed so far, we tackle general potential games and prove the first finite-time convergence to an $\\epsilon$-efficient NE. In particular, by using a problem-dependent analysis, our bound depends polynomially on $1/\\epsilon$. Furthermore, we provide two extensions of our convergence result: first, we show that a variant of log-linear learning that requires a factor $A$ less feedback on the utility per round enjoys a similar convergence time; second, we demonstrate the robustness of our convergence guarantee if log-linear learning is subject to small perturbations such as alterations in the learning rule or noise-corrupted utilities.","sentences":["This paper investigates the convergence time of log-linear learning to an $\\epsilon$-efficient Nash equilibrium (NE) in potential games.","In such games, an efficient NE is defined as the maximizer of the potential function.","Existing results are limited to potential games with stringent structural assumptions and entail exponential convergence times in $1/\\epsilon$. Unaddressed so far, we tackle general potential games and prove the first finite-time convergence to an $\\epsilon$-efficient NE.","In particular, by using a problem-dependent analysis, our bound depends polynomially on $1/\\epsilon$. Furthermore, we provide two extensions of our convergence result: first, we show that a variant of log-linear learning that requires a factor $A$ less feedback on the utility per round enjoys a similar convergence time; second, we demonstrate the robustness of our convergence guarantee if log-linear learning is subject to small perturbations such as alterations in the learning rule or noise-corrupted utilities."],"url":"http://arxiv.org/abs/2405.15497v1","category":"cs.MA"}
{"created":"2024-05-24 12:16:47","title":"Finding Induced Subgraphs from Graphs with Small Mim-Width","abstract":"In the last decade, algorithmic frameworks based on a structural graph parameter called mim-width have been developed to solve generally NP-hard problems. However, it is known that the frameworks cannot be applied to the Clique problem, and the complexity status of many problems of finding dense induced subgraphs remains open when parameterized by mim-width. In this paper, we investigate the complexity of the problem of finding a maximum induced subgraph that satisfies prescribed properties from a given graph with small mim-width. We first give a meta-theorem implying that various induced subgraph problems are NP-hard for bounded mim-width graphs. Moreover, we show that some problems, including Clique and Induced Cluster Subgraph, remain NP-hard even for graphs with (linear) mim-width at most 2. In contrast to the intractability, we provide an algorithm that, given a graph and its branch decomposition with mim-width at most 1, solves Induced Cluster Subgraph in polynomial time. We emphasize that our algorithmic technique is applicable to other problems such as Induced Polar Subgraph and Induced Split Subgraph. Since a branch decomposition with mim-width at most 1 can be constructed in polynomial time for block graphs, interval graphs, permutation graphs, cographs, distance-hereditary graphs, convex graphs, and their complement graphs, our positive results reveal the polynomial-time solvability of various problems for these graph classes.","sentences":["In the last decade, algorithmic frameworks based on a structural graph parameter called mim-width have been developed to solve generally NP-hard problems.","However, it is known that the frameworks cannot be applied to the Clique problem, and the complexity status of many problems of finding dense induced subgraphs remains open when parameterized by mim-width.","In this paper, we investigate the complexity of the problem of finding a maximum induced subgraph that satisfies prescribed properties from a given graph with small mim-width.","We first give a meta-theorem implying that various induced subgraph problems are NP-hard for bounded mim-width graphs.","Moreover, we show that some problems, including Clique and Induced Cluster Subgraph, remain NP-hard even for graphs with (linear) mim-width at most 2.","In contrast to the intractability, we provide an algorithm that, given a graph and its branch decomposition with mim-width at most 1, solves Induced Cluster Subgraph in polynomial time.","We emphasize that our algorithmic technique is applicable to other problems such as Induced Polar Subgraph and Induced Split Subgraph.","Since a branch decomposition with mim-width at most 1 can be constructed in polynomial time for block graphs, interval graphs, permutation graphs, cographs, distance-hereditary graphs, convex graphs, and their complement graphs, our positive results reveal the polynomial-time solvability of various problems for these graph classes."],"url":"http://arxiv.org/abs/2405.15492v1","category":"cs.DS"}
{"created":"2024-05-24 12:11:41","title":"Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2","abstract":"Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design. Genie 2 inference and training code, as well as model weights, are freely available at: https://github.com/aqlaboratory/genie2.","sentences":["Protein diffusion models have emerged as a promising approach for protein design.","One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter.","In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation.","Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations.","This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions.","On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty.","Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions.","Taken together, these advances set a new standard for structure-based protein design.","Genie 2 inference and training code, as well as model weights, are freely available at: https://github.com/aqlaboratory/genie2."],"url":"http://arxiv.org/abs/2405.15489v1","category":"q-bio.BM"}
{"created":"2024-05-24 12:05:59","title":"Digital finance, Bargaining Power and Gender Wage Gap","abstract":"The proliferation of internet technology has catalyzed the rapid development of digital finance, significantly impacting the optimization of resource allocation in China and exerting a substantial and enduring influence on the structure of employment and income distribution. This research utilizes data sourced from the Chinese General Social Survey and the Digital Financial Inclusion Index to scrutinize the influence of digital finance on the gender wage disparity in China. The findings reveal that digital finance reduces the gender wage gap, and this conclusion remains robust after addressing endogeneity problem using instrumental variable methods. Further analysis of the underlying mechanisms indicates that digital finance facilitates female entrepreneurship by lowering financing barriers, thereby promoting employment opportunities for women and also empowering them to negotiate higher wages. Specially, digital finance enhances women's bargaining power within domestic settings, therefore exerts a positive influence on the wages of women. Sub-sample regressions demonstrate that women from economically disadvantaged backgrounds, with lower human capital, benefit more from digital finance, underscoring its inclusive nature. This study provides policy evidence for empowering vulnerable groups to increase their wages and addressing the persistent issue of gender income disparity in the labor market.","sentences":["The proliferation of internet technology has catalyzed the rapid development of digital finance, significantly impacting the optimization of resource allocation in China and exerting a substantial and enduring influence on the structure of employment and income distribution.","This research utilizes data sourced from the Chinese General Social Survey and the Digital Financial Inclusion Index to scrutinize the influence of digital finance on the gender wage disparity in China.","The findings reveal that digital finance reduces the gender wage gap, and this conclusion remains robust after addressing endogeneity problem using instrumental variable methods.","Further analysis of the underlying mechanisms indicates that digital finance facilitates female entrepreneurship by lowering financing barriers, thereby promoting employment opportunities for women and also empowering them to negotiate higher wages.","Specially, digital finance enhances women's bargaining power within domestic settings, therefore exerts a positive influence on the wages of women.","Sub-sample regressions demonstrate that women from economically disadvantaged backgrounds, with lower human capital, benefit more from digital finance, underscoring its inclusive nature.","This study provides policy evidence for empowering vulnerable groups to increase their wages and addressing the persistent issue of gender income disparity in the labor market."],"url":"http://arxiv.org/abs/2405.15486v1","category":"econ.GN"}
{"created":"2024-05-24 12:04:54","title":"Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs","abstract":"We are beginning to see progress in language model assisted scientific discovery. Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical skills required to solve problems. In particular, we look at not just what the pre-trained model already knows, but how it learned to learn from information during in-context learning or instruction-tuning through exploiting the complex knowledge structure within mathematics. Motivated by the Neural Tangent Kernel (NTK), we propose \\textit{NTKEval} to assess changes in LLM's probability distribution via training on different kinds of math data. Our systematic analysis finds evidence of domain understanding during in-context learning. By contrast, certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding across different skills.","sentences":["We are beginning to see progress in language model assisted scientific discovery.","Motivated by the use of LLMs as a general scientific assistant, this paper assesses the domain knowledge of LLMs through its understanding of different mathematical skills required to solve problems.","In particular, we look at not just what the pre-trained model already knows, but how it learned to learn from information during in-context learning or instruction-tuning through exploiting the complex knowledge structure within mathematics.","Motivated by the Neural Tangent Kernel (NTK), we propose \\textit{NTKEval} to assess changes in LLM's probability distribution via training on different kinds of math data.","Our systematic analysis finds evidence of domain understanding during in-context learning.","By contrast, certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding across different skills."],"url":"http://arxiv.org/abs/2405.15485v1","category":"cs.AI"}
{"created":"2024-05-24 12:04:01","title":"The spin, inclination, and magnetic field evolution of magnetar population in Vacuum and Plasma-filled Magnetospheres","abstract":"Magnetars are potential energy sources or central engines for numerous transient phenomena in the universe. How newborn magnetars evolve in different environments remains an open question. Based on both observed and candidate magnetars, it is found that the periods of all magnetars or candidates appear as a bimodal distribution, and are defined as the ``long-P'' and ``short-P'' magnetar sub-classes, respectively. We find that for the ``short-P'' sub-class of magnetars, the $\\dot{P}$ values also appear as a bimodal distribution, and therefore can be classified as ``high-$\\dot{P}$ short-P'' and ``low-$\\dot{P}$ short-P'' magnetar sub-classes. In this paper, we use Monte Carlo simulations to generate synthetic magnetar populations and investigate the evolution of the ``high-$\\dot{P}$ short-P'' and ``low-$\\dot{P}$ short-P'' magnetar sub-classes by considering both the magnetar spin and inclination, as well as the decay of their magnetic field within their evolution in both vacuum and plasma-filled magnetospheres. We find that the magnetar evolution is dependent on both spin and magnetic field, but seems to be insensitive to inclination evolution and magnetospheric environment for the ``high-$\\dot{P}$ short-P'' sub-class. In comparison for the case of ``high-$\\dot{P}$ short-P'', the magnetar evolution is dependent on spin, magnetic field, and inclination evolution, as well as the magnetospheric environment. The best evolution model should be the case of inclination evolution in vacuum with a small value of $\\overline{\\mathrm{FOM}}$. The differences in the best-fit parameters also suggest that the ``high-$\\dot{P}$ short-P'' and ``low-$\\dot{P}$ short-P'' magnetar sub-classes may be tracking with different evolution channels.","sentences":["Magnetars are potential energy sources or central engines for numerous transient phenomena in the universe.","How newborn magnetars evolve in different environments remains an open question.","Based on both observed and candidate magnetars, it is found that the periods of all magnetars or candidates appear as a bimodal distribution, and are defined as the ``long-P'' and ``short-P'' magnetar sub-classes, respectively.","We find that for the ``short-P'' sub-class of magnetars, the $\\dot{P}$ values also appear as a bimodal distribution, and therefore can be classified as ``high-$\\dot{P}$ short-P'' and ``low-$\\dot{P}$ short-P'' magnetar sub-classes.","In this paper, we use Monte Carlo simulations to generate synthetic magnetar populations and investigate the evolution of the ``high-$\\dot{P}$ short-P'' and ``low-$\\dot{P}$ short-P'' magnetar sub-classes by considering both the magnetar spin and inclination, as well as the decay of their magnetic field within their evolution in both vacuum and plasma-filled magnetospheres.","We find that the magnetar evolution is dependent on both spin and magnetic field, but seems to be insensitive to inclination evolution and magnetospheric environment for the ``high-$\\dot{P}$ short-P'' sub-class.","In comparison for the case of ``high-$\\dot{P}$ short-P'', the magnetar evolution is dependent on spin, magnetic field, and inclination evolution, as well as the magnetospheric environment.","The best evolution model should be the case of inclination evolution in vacuum with a small value of $\\overline{\\mathrm{FOM}}$. The differences in the best-fit parameters also suggest that the ``high-$\\dot{P}$ short-P'' and ``low-$\\dot{P}$ short-P'' magnetar sub-classes may be tracking with different evolution channels."],"url":"http://arxiv.org/abs/2405.15484v1","category":"astro-ph.HE"}
{"created":"2024-05-24 12:00:29","title":"An input-output continuous-time version of Willems' lemma","abstract":"We illustrate a novel version of Willems' lemma for data-based representation of continuous-time systems. The main novelties compared to previous works are two. First, the proposed framework relies only on measured input-output trajectories from the system and no internal (state) information is required. Second, our system representation makes use of exact system trajectories, without resorting to orthogonal bases representations and consequent approximations. We first establish sufficient and necessary conditions for data-based generation of system trajectories in terms of suitable latent variables. Subsequently, we reformulate these conditions using measured input-output data and show how to span the full behavior of the system. Furthermore, we show how to use the developed framework to solve the data-based continuous-time simulation problem.","sentences":["We illustrate a novel version of Willems' lemma for data-based representation of continuous-time systems.","The main novelties compared to previous works are two.","First, the proposed framework relies only on measured input-output trajectories from the system and no internal (state) information is required.","Second, our system representation makes use of exact system trajectories, without resorting to orthogonal bases representations and consequent approximations.","We first establish sufficient and necessary conditions for data-based generation of system trajectories in terms of suitable latent variables.","Subsequently, we reformulate these conditions using measured input-output data and show how to span the full behavior of the system.","Furthermore, we show how to use the developed framework to solve the data-based continuous-time simulation problem."],"url":"http://arxiv.org/abs/2405.15482v1","category":"eess.SY"}
{"created":"2024-05-24 11:59:41","title":"Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks","abstract":"The growing computational demands posed by increasingly number of neural network's parameters necessitate low-memory-consumption training approaches. Previous memory reduction techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, suffer from the limitation of low rank and saddle point issues, particularly during intensive tasks like pre-training. In this paper, we propose Sparse Spectral Training (SST), an advanced training methodology that updates all singular values and selectively updates singular vectors of network weights, thereby optimizing resource usage while closely approximating full-rank training. SST refines the training process by employing a targeted updating strategy for singular vectors, which is determined by a multinomial sampling method weighted by the significance of the singular values, ensuring both high performance and memory reduction. Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, including natural language generation, machine translation, node classification and link prediction, SST demonstrates its capability to outperform existing memory reduction training methods and is comparable with full-rank training in some cases. On OPT-125M, with rank equating to 8.3% of embedding dimension, SST reduces the perplexity gap to full-rank training by 67.6%, demonstrating a significant reduction of the performance loss with prevalent low-rank methods. This approach offers a strong alternative to traditional training techniques, paving the way for more efficient and scalable neural network training solutions.","sentences":["The growing computational demands posed by increasingly number of neural network's parameters necessitate low-memory-consumption training approaches.","Previous memory reduction techniques, such as Low-Rank Adaptation (LoRA) and ReLoRA, suffer from the limitation of low rank and saddle point issues, particularly during intensive tasks like pre-training.","In this paper, we propose Sparse Spectral Training (SST), an advanced training methodology that updates all singular values and selectively updates singular vectors of network weights, thereby optimizing resource usage while closely approximating full-rank training.","SST refines the training process by employing a targeted updating strategy for singular vectors, which is determined by a multinomial sampling method weighted by the significance of the singular values, ensuring both high performance and memory reduction.","Through comprehensive testing on both Euclidean and hyperbolic neural networks across various tasks, including natural language generation, machine translation, node classification and link prediction, SST demonstrates its capability to outperform existing memory reduction training methods and is comparable with full-rank training in some cases.","On OPT-125M, with rank equating to 8.3% of embedding dimension, SST reduces the perplexity gap to full-rank training by 67.6%, demonstrating a significant reduction of the performance loss with prevalent low-rank methods.","This approach offers a strong alternative to traditional training techniques, paving the way for more efficient and scalable neural network training solutions."],"url":"http://arxiv.org/abs/2405.15481v1","category":"cs.LG"}
{"created":"2024-05-24 11:58:05","title":"Stability Analysis of a Diffusive SVIR Epidemic Model with Distributed Delay, Imperfect Vaccine and General Incidence Rate","abstract":"In this chapter, we consider a reaction-diffusion SVIR infection model with dis-tributed delay and nonlinear incidence rate. The wellposedness of the proposed model is proved. By means of Lyapunov functionals, we show that the disease-free equilibrium state is globally asymptotically stable when the basic reproduction number is less or equal than one, and that the disease endemic equilibrium is globally asymptotically stable when the basic reproduction number is greater than one. Numerical simulations are provided to illustrate the obtained theoretical results.","sentences":["In this chapter, we consider a reaction-diffusion SVIR infection model with dis-tributed delay and nonlinear incidence rate.","The wellposedness of the proposed model is proved.","By means of Lyapunov functionals, we show that the disease-free equilibrium state is globally asymptotically stable when the basic reproduction number is less or equal than one, and that the disease endemic equilibrium is globally asymptotically stable when the basic reproduction number is greater than one.","Numerical simulations are provided to illustrate the obtained theoretical results."],"url":"http://arxiv.org/abs/2405.15478v1","category":"math.DS"}
{"created":"2024-05-24 11:55:46","title":"Editable Concept Bottleneck Models","abstract":"Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on cases where the data, including concepts, are clean. In many scenarios, we always need to remove/insert some training data or new concepts from trained CBMs due to different reasons, such as privacy concerns, data mislabelling, spurious concepts, and concept annotation errors. Thus, the challenge of deriving efficient editable CBMs without retraining from scratch persists, particularly in large-scale applications. To address these challenges, we propose Editable Concept Bottleneck Models (ECBMs). Specifically, ECBMs support three different levels of data removal: concept-label-level, concept-level, and data-level. ECBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for re-training. Experimental results demonstrate the efficiency and effectiveness of our ECBMs, affirming their adaptability within the realm of CBMs.","sentences":["Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer.","However, most previous studies focused on cases where the data, including concepts, are clean.","In many scenarios, we always need to remove/insert some training data or new concepts from trained CBMs due to different reasons, such as privacy concerns, data mislabelling, spurious concepts, and concept annotation errors.","Thus, the challenge of deriving efficient editable CBMs without retraining from scratch persists, particularly in large-scale applications.","To address these challenges, we propose Editable Concept Bottleneck Models (ECBMs).","Specifically, ECBMs support three different levels of data removal: concept-label-level, concept-level, and data-level.","ECBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for re-training.","Experimental results demonstrate the efficiency and effectiveness of our ECBMs, affirming their adaptability within the realm of CBMs."],"url":"http://arxiv.org/abs/2405.15476v1","category":"cs.LG"}
{"created":"2024-05-24 11:53:27","title":"Efficient Degradation-aware Any Image Restoration","abstract":"Reconstructing missing details from degraded low-quality inputs poses a significant challenge. Recent progress in image restoration has demonstrated the efficacy of learning large models capable of addressing various degradations simultaneously. Nonetheless, these approaches introduce considerable computational overhead and complex learning paradigms, limiting their practical utility. In response, we propose \\textit{DaAIR}, an efficient All-in-One image restorer employing a Degradation-aware Learner (DaLe) in the low-rank regime to collaboratively mine shared aspects and subtle nuances across diverse degradations, generating a degradation-aware embedding. By dynamically allocating model capacity to input degradations, we realize an efficient restorer integrating holistic and specific learning within a unified model. Furthermore, DaAIR introduces a cost-efficient parameter update mechanism that enhances degradation awareness while maintaining computational efficiency. Extensive comparisons across five image degradations demonstrate that our DaAIR outperforms both state-of-the-art All-in-One models and degradation-specific counterparts, affirming our efficacy and practicality. The source will be publicly made available at \\url{https://eduardzamfir.github.io/daair/}","sentences":["Reconstructing missing details from degraded low-quality inputs poses a significant challenge.","Recent progress in image restoration has demonstrated the efficacy of learning large models capable of addressing various degradations simultaneously.","Nonetheless, these approaches introduce considerable computational overhead and complex learning paradigms, limiting their practical utility.","In response, we propose \\textit{DaAIR}, an efficient All-in-One image restorer employing a Degradation-aware Learner (DaLe) in the low-rank regime to collaboratively mine shared aspects and subtle nuances across diverse degradations, generating a degradation-aware embedding.","By dynamically allocating model capacity to input degradations, we realize an efficient restorer integrating holistic and specific learning within a unified model.","Furthermore, DaAIR introduces a cost-efficient parameter update mechanism that enhances degradation awareness while maintaining computational efficiency.","Extensive comparisons across five image degradations demonstrate that our DaAIR outperforms both state-of-the-art All-in-One models and degradation-specific counterparts, affirming our efficacy and practicality.","The source will be publicly made available at \\url{https://eduardzamfir.github.io/daair/}"],"url":"http://arxiv.org/abs/2405.15475v1","category":"cs.CV"}
{"created":"2024-05-24 11:51:08","title":"Encoder Embedding for General Graph and Node Classification","abstract":"Graph encoder embedding, a recent technique for graph data, offers speed and scalability in producing vertex-level representations from binary graphs. In this paper, we extend the applicability of this method to a general graph model, which includes weighted graphs, distance matrices, and kernel matrices. We prove that the encoder embedding satisfies the law of large numbers and the central limit theorem on a per-observation basis. Under certain condition, it achieves asymptotic normality on a per-class basis, enabling optimal classification through discriminant analysis. These theoretical findings are validated through a series of experiments involving weighted graphs, as well as text and image data transformed into general graph representations using appropriate distance metrics.","sentences":["Graph encoder embedding, a recent technique for graph data, offers speed and scalability in producing vertex-level representations from binary graphs.","In this paper, we extend the applicability of this method to a general graph model, which includes weighted graphs, distance matrices, and kernel matrices.","We prove that the encoder embedding satisfies the law of large numbers and the central limit theorem on a per-observation basis.","Under certain condition, it achieves asymptotic normality on a per-class basis, enabling optimal classification through discriminant analysis.","These theoretical findings are validated through a series of experiments involving weighted graphs, as well as text and image data transformed into general graph representations using appropriate distance metrics."],"url":"http://arxiv.org/abs/2405.15473v1","category":"stat.ML"}
{"created":"2024-05-24 11:44:22","title":"Semantic Aware Diffusion Inverse Tone Mapping","abstract":"The range of real-world scene luminance is larger than the capture capability of many digital camera sensors which leads to details being lost in captured images, most typically in bright regions. Inverse tone mapping attempts to boost these captured Standard Dynamic Range (SDR) images back to High Dynamic Range (HDR) by creating a mapping that linearizes the well exposed values from the SDR image, and provides a luminance boost to the clipped content. However, in most cases, the details in the clipped regions cannot be recovered or estimated. In this paper, we present a novel inverse tone mapping approach for mapping SDR images to HDR that generates lost details in clipped regions through a semantic-aware diffusion based inpainting approach. Our method proposes two major contributions - first, we propose to use a semantic graph to guide SDR diffusion based inpainting in masked regions in a saturated image. Second, drawing inspiration from traditional HDR imaging and bracketing methods, we propose a principled formulation to lift the SDR inpainted regions to HDR that is compatible with generative inpainting methods. Results show that our method demonstrates superior performance across different datasets on objective metrics, and subjective experiments show that the proposed method matches (and in most cases outperforms) state-of-art inverse tone mapping operators in terms of objective metrics and outperforms them for visual fidelity.","sentences":["The range of real-world scene luminance is larger than the capture capability of many digital camera sensors which leads to details being lost in captured images, most typically in bright regions.","Inverse tone mapping attempts to boost these captured Standard Dynamic Range (SDR) images back to High Dynamic Range (HDR) by creating a mapping that linearizes the well exposed values from the SDR image, and provides a luminance boost to the clipped content.","However, in most cases, the details in the clipped regions cannot be recovered or estimated.","In this paper, we present a novel inverse tone mapping approach for mapping SDR images to HDR that generates lost details in clipped regions through a semantic-aware diffusion based inpainting approach.","Our method proposes two major contributions - first, we propose to use a semantic graph to guide SDR diffusion based inpainting in masked regions in a saturated image.","Second, drawing inspiration from traditional HDR imaging and bracketing methods, we propose a principled formulation to lift the SDR inpainted regions to HDR that is compatible with generative inpainting methods.","Results show that our method demonstrates superior performance across different datasets on objective metrics, and subjective experiments show that the proposed method matches (and in most cases outperforms) state-of-art inverse tone mapping operators in terms of objective metrics and outperforms them for visual fidelity."],"url":"http://arxiv.org/abs/2405.15468v1","category":"cs.CV"}
{"created":"2024-05-24 11:43:20","title":"The comparison of alternative spacetimes using the spherical accretion around the black hole","abstract":"In the region where the gravitational field is strong, we have examined the influence of different gravities on the accretion disk formed due to spherical accretion. To achieve this, we obtain numerical solutions of the GRH equations, utilizing Schwarzschild, Kerr, Einstein-Gauss-Bonnet, and Hartle-Thorne spacetime metrics. We investigate the impact of the rotation parameter of a black hole (a/M), the EGB coupling constant ($\\alpha$), and the quadrupole moment of the rotating black hole (q) on the accretion disk formed in a strong field. The formation of the disk for the slowly and rapidly rotating black hole models is separately examined, and comparisons are made. Our numerical simulations reveal that, under the specific conditions, the solution derived from Hartle-Thorne gravity converges towards solutions obtained from Kerr and other gravitational models. In the context of the slowly rotating black hole with $a/M=0.28$, we observe a favorable agreement between the Hartle-Thorne result and the Kerr result within the range of 0 < q < 0.5. Conversely, in the scenario of the rapidly rotating black hole, a more pronounced alignment with the value of q=1 is evident within the range of 0.5 < q < 1. Nevertheless, for q > 1, it becomes apparent that the Hartle-Thorne solution diverges from solutions provided by all gravitational models. Our motivation here is to utilize the Hartle-Thorne spacetime metric for the first time in the numerical solutions of the GRH equations for the black holes, compare the results with those obtained using other gravities, and identify under which conditions the Hartle-Thorne solution is compatible with known black hole spacetime metric solutions. This may allow us to provide an alternative perspective in explaining observed X-ray data.","sentences":["In the region where the gravitational field is strong, we have examined the influence of different gravities on the accretion disk formed due to spherical accretion.","To achieve this, we obtain numerical solutions of the GRH equations, utilizing Schwarzschild, Kerr, Einstein-Gauss-Bonnet, and Hartle-Thorne spacetime metrics.","We investigate the impact of the rotation parameter of a black hole (a/M), the EGB coupling constant ($\\alpha$), and the quadrupole moment of the rotating black hole (q) on the accretion disk formed in a strong field.","The formation of the disk for the slowly and rapidly rotating black hole models is separately examined, and comparisons are made.","Our numerical simulations reveal that, under the specific conditions, the solution derived from Hartle-Thorne gravity converges towards solutions obtained from Kerr and other gravitational models.","In the context of the slowly rotating black hole with $a/M=0.28$, we observe a favorable agreement between the Hartle-Thorne result and the Kerr result within the range of 0 <","q < 0.5.","Conversely, in the scenario of the rapidly rotating black hole, a more pronounced alignment with the value of q=1 is evident within the range of 0.5 < q < 1.","Nevertheless, for q > 1, it becomes apparent that the Hartle-Thorne solution diverges from solutions provided by all gravitational models.","Our motivation here is to utilize the Hartle-Thorne spacetime metric for the first time in the numerical solutions of the GRH equations for the black holes, compare the results with those obtained using other gravities, and identify under which conditions the Hartle-Thorne solution is compatible with known black hole spacetime metric solutions.","This may allow us to provide an alternative perspective in explaining observed X-ray data."],"url":"http://arxiv.org/abs/2405.15467v1","category":"gr-qc"}
{"created":"2024-05-24 11:36:26","title":"PoinTramba: A Hybrid Transformer-Mamba Framework for Point Cloud Analysis","abstract":"Point cloud analysis has seen substantial advancements due to deep learning, although previous Transformer-based methods excel at modeling long-range dependencies on this task, their computational demands are substantial. Conversely, the Mamba offers greater efficiency but shows limited potential compared with Transformer-based methods. In this study, we introduce PoinTramba, a pioneering hybrid framework that synergies the analytical power of Transformer with the remarkable computational efficiency of Mamba for enhanced point cloud analysis. Specifically, our approach first segments point clouds into groups, where the Transformer meticulously captures intricate intra-group dependencies and produces group embeddings, whose inter-group relationships will be simultaneously and adeptly captured by efficient Mamba architecture, ensuring comprehensive analysis. Unlike previous Mamba approaches, we introduce a bi-directional importance-aware ordering (BIO) strategy to tackle the challenges of random ordering effects. This innovative strategy intelligently reorders group embeddings based on their calculated importance scores, significantly enhancing Mamba's performance and optimizing the overall analytical process. Our framework achieves a superior balance between computational efficiency and analytical performance by seamlessly integrating these advanced techniques, marking a substantial leap forward in point cloud analysis. Extensive experiments on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our approach, establishing a new state-of-the-art analysis benchmark on point cloud recognition. For the first time, this paradigm leverages the combined strengths of both Transformer and Mamba architectures, facilitating a new standard in the field. The code is available at https://github.com/xiaoyao3302/PoinTramba.","sentences":["Point cloud analysis has seen substantial advancements due to deep learning, although previous Transformer-based methods excel at modeling long-range dependencies on this task, their computational demands are substantial.","Conversely, the Mamba offers greater efficiency but shows limited potential compared with Transformer-based methods.","In this study, we introduce PoinTramba, a pioneering hybrid framework that synergies the analytical power of Transformer with the remarkable computational efficiency of Mamba for enhanced point cloud analysis.","Specifically, our approach first segments point clouds into groups, where the Transformer meticulously captures intricate intra-group dependencies and produces group embeddings, whose inter-group relationships will be simultaneously and adeptly captured by efficient Mamba architecture, ensuring comprehensive analysis.","Unlike previous Mamba approaches, we introduce a bi-directional importance-aware ordering (BIO) strategy to tackle the challenges of random ordering effects.","This innovative strategy intelligently reorders group embeddings based on their calculated importance scores, significantly enhancing Mamba's performance and optimizing the overall analytical process.","Our framework achieves a superior balance between computational efficiency and analytical performance by seamlessly integrating these advanced techniques, marking a substantial leap forward in point cloud analysis.","Extensive experiments on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our approach, establishing a new state-of-the-art analysis benchmark on point cloud recognition.","For the first time, this paradigm leverages the combined strengths of both Transformer and Mamba architectures, facilitating a new standard in the field.","The code is available at https://github.com/xiaoyao3302/PoinTramba."],"url":"http://arxiv.org/abs/2405.15463v1","category":"cs.CV"}
{"created":"2024-05-24 11:34:31","title":"Repetita Iuvant: Data Repetition Allows SGD to Learn High-Dimensional Multi-Index Functions","abstract":"Neural networks can identify low-dimensional relevant structures within high-dimensional noisy data, yet our mathematical understanding of how they do so remains scarce. Here, we investigate the training dynamics of two-layer shallow neural networks trained with gradient-based algorithms, and discuss how they learn pertinent features in multi-index models, that is target functions with low-dimensional relevant directions. In the high-dimensional regime, where the input dimension $d$ diverges, we show that a simple modification of the idealized single-pass gradient descent training scenario, where data can now be repeated or iterated upon twice, drastically improves its computational efficiency. In particular, it surpasses the limitations previously believed to be dictated by the Information and Leap exponents associated with the target function to be learned. Our results highlight the ability of networks to learn relevant structures from data alone without any pre-processing. More precisely, we show that (almost) all directions are learned with at most $O(d \\log d)$ steps. Among the exceptions is a set of hard functions that includes sparse parities. In the presence of coupling between directions, however, these can be learned sequentially through a hierarchical mechanism that generalizes the notion of staircase functions. Our results are proven by a rigorous study of the evolution of the relevant statistics for high-dimensional dynamics.","sentences":["Neural networks can identify low-dimensional relevant structures within high-dimensional noisy data, yet our mathematical understanding of how they do so remains scarce.","Here, we investigate the training dynamics of two-layer shallow neural networks trained with gradient-based algorithms, and discuss how they learn pertinent features in multi-index models, that is target functions with low-dimensional relevant directions.","In the high-dimensional regime, where the input dimension $d$ diverges, we show that a simple modification of the idealized single-pass gradient descent training scenario, where data can now be repeated or iterated upon twice, drastically improves its computational efficiency.","In particular, it surpasses the limitations previously believed to be dictated by the Information and Leap exponents associated with the target function to be learned.","Our results highlight the ability of networks to learn relevant structures from data alone without any pre-processing.","More precisely, we show that (almost) all directions are learned with at most $O(d \\log d)$ steps.","Among the exceptions is a set of hard functions that includes sparse parities.","In the presence of coupling between directions, however, these can be learned sequentially through a hierarchical mechanism that generalizes the notion of staircase functions.","Our results are proven by a rigorous study of the evolution of the relevant statistics for high-dimensional dynamics."],"url":"http://arxiv.org/abs/2405.15459v1","category":"stat.ML"}
{"created":"2024-05-24 11:33:58","title":"FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler","abstract":"Federated learning (FL) enables collaborative machine learning across distributed data owners, but data heterogeneity poses a challenge for model calibration. While prior work focused on improving accuracy for non-iid data, calibration remains under-explored. This study reveals existing FL aggregation approaches lead to sub-optimal calibration, and theoretical analysis shows despite constraining variance in clients' label distributions, global calibration error is still asymptotically lower bounded. To address this, we propose a novel Federated Calibration (FedCal) approach, emphasizing both local and global calibration. It leverages client-specific scalers for local calibration to effectively correct output misalignment without sacrificing prediction accuracy. These scalers are then aggregated via weight averaging to generate a global scaler, minimizing the global calibration error. Extensive experiments demonstrate FedCal significantly outperforms the best-performing baseline, reducing global calibration error by 47.66% on average.","sentences":["Federated learning (FL) enables collaborative machine learning across distributed data owners, but data heterogeneity poses a challenge for model calibration.","While prior work focused on improving accuracy for non-iid data, calibration remains under-explored.","This study reveals existing FL aggregation approaches lead to sub-optimal calibration, and theoretical analysis shows despite constraining variance in clients' label distributions, global calibration error is still asymptotically lower bounded.","To address this, we propose a novel Federated Calibration (FedCal) approach, emphasizing both local and global calibration.","It leverages client-specific scalers for local calibration to effectively correct output misalignment without sacrificing prediction accuracy.","These scalers are then aggregated via weight averaging to generate a global scaler, minimizing the global calibration error.","Extensive experiments demonstrate FedCal significantly outperforms the best-performing baseline, reducing global calibration error by 47.66% on average."],"url":"http://arxiv.org/abs/2405.15458v1","category":"cs.LG"}
{"created":"2024-05-24 11:33:48","title":"Study of a class of triangular starvation driven cross-diffusion systems","abstract":"We study the existence, regularity and uniqueness for a general class of triangular reaction-cross-diffusion systems coming from the study of starvation driven behavior for two species in competition. This study involves an equivalent system in non-divergence form, for which existence can be obtained thanks to Schauder's fixed point theorem.","sentences":["We study the existence, regularity and uniqueness for a general class of triangular reaction-cross-diffusion systems coming from the study of starvation driven behavior for two species in competition.","This study involves an equivalent system in non-divergence form, for which existence can be obtained thanks to Schauder's fixed point theorem."],"url":"http://arxiv.org/abs/2405.15457v1","category":"math.AP"}
{"created":"2024-05-24 11:31:27","title":"Towards Relational Quantum Field Theory","abstract":"This paper presents a research program aimed at establishing relational foundations for relativistic quantum physics. Although the formalism is still under development, we believe it has matured enough to be shared with the broader scientific community. Our approach seeks to integrate Quantum Field Theory on curved backgrounds and scenarios with indefinite causality. Building on concepts from the operational approach to Quantum Reference Frames, we extend these ideas significantly. Specifically, we initiate the development of a general integration theory for operator-valued functions (quantum fields) with respect to positive operator-valued measures (quantum frames). This allows us to define quantum frames within the context of arbitrary principal bundles, replacing group structures. By considering Lorentz principal bundles, we enable a relational treatment of quantum fields on arbitrarily curved spacetimes. A~form of~indefinite spatiotemporality arises from quantum states in the context of frame bundles. This offers novel perspectives on the problem of reconciling principles of generally relativistic and quantum physics and on modelling gravitational fields sourced by quantum systems.","sentences":["This paper presents a research program aimed at establishing relational foundations for relativistic quantum physics.","Although the formalism is still under development, we believe it has matured enough to be shared with the broader scientific community.","Our approach seeks to integrate Quantum Field Theory on curved backgrounds and scenarios with indefinite causality.","Building on concepts from the operational approach to Quantum Reference Frames, we extend these ideas significantly.","Specifically, we initiate the development of a general integration theory for operator-valued functions (quantum fields) with respect to positive operator-valued measures (quantum frames).","This allows us to define quantum frames within the context of arbitrary principal bundles, replacing group structures.","By considering Lorentz principal bundles, we enable a relational treatment of quantum fields on arbitrarily curved spacetimes.","A~form of~indefinite spatiotemporality arises from quantum states in the context of frame bundles.","This offers novel perspectives on the problem of reconciling principles of generally relativistic and quantum physics and on modelling gravitational fields sourced by quantum systems."],"url":"http://arxiv.org/abs/2405.15455v1","category":"quant-ph"}
{"created":"2024-05-24 11:30:44","title":"Linearly Controlled Language Generation with Performative Guarantees","abstract":"The increasing prevalence of Large Language Models (LMs) in critical applications highlights the need for controlled language generation strategies that are not only computationally efficient but that also enjoy performance guarantees. To achieve this, we use a common model of concept semantics as linearly represented in an LM's latent space. In particular, we take the view that natural language generation traces a trajectory in this continuous semantic space, realized by the language model's hidden activations. This view permits a control-theoretic treatment of text generation in latent space, in which we propose a lightweight, gradient-free intervention that dynamically steers trajectories away from regions corresponding to undesired meanings. Crucially, we show that this intervention, which we compute in closed form, is guaranteed (in probability) to steer the output into the allowed region. Finally, we demonstrate on a toxicity avoidance objective that the intervention steers language away from undesired content while maintaining text quality.","sentences":["The increasing prevalence of Large Language Models (LMs) in critical applications highlights the need for controlled language generation strategies that are not only computationally efficient but that also enjoy performance guarantees.","To achieve this, we use a common model of concept semantics as linearly represented in an LM's latent space.","In particular, we take the view that natural language generation traces a trajectory in this continuous semantic space, realized by the language model's hidden activations.","This view permits a control-theoretic treatment of text generation in latent space, in which we propose a lightweight, gradient-free intervention that dynamically steers trajectories away from regions corresponding to undesired meanings.","Crucially, we show that this intervention, which we compute in closed form, is guaranteed (in probability) to steer the output into the allowed region.","Finally, we demonstrate on a toxicity avoidance objective that the intervention steers language away from undesired content while maintaining text quality."],"url":"http://arxiv.org/abs/2405.15454v1","category":"cs.CL"}
{"created":"2024-05-24 11:30:37","title":"Benchmarking Pre-trained Large Language Models' Potential Across Urdu NLP tasks","abstract":"Large Language Models (LLMs) pre-trained on multilingual data have revolutionized natural language processing research, by transitioning from languages and task specific model pipelines to a single model adapted on a variety of tasks. However majority of existing multilingual NLP benchmarks for LLMs provide evaluation data in only few languages with little linguistic diversity. In addition these benchmarks lack quality assessment against the respective state-of the art models. This study presents an in-depth examination of prominent LLMs; GPT-3.5-turbo, Llama2-7B-Chat, Bloomz 7B1 and Bloomz 3B, across 14 tasks using 15 Urdu datasets, in a zero-shot setting, and their performance against state-of-the-art (SOTA) models, has been compared and analysed. Our experiments show that SOTA models surpass all the encoder-decoder pre-trained language models in all Urdu NLP tasks with zero-shot learning. Our results further show that LLMs with fewer parameters, but more language specific data in the base model perform better than larger computational models, but low language data.","sentences":["Large Language Models (LLMs) pre-trained on multilingual data have revolutionized natural language processing research, by transitioning from languages and task specific model pipelines to a single model adapted on a variety of tasks.","However majority of existing multilingual NLP benchmarks for LLMs provide evaluation data in only few languages with little linguistic diversity.","In addition these benchmarks lack quality assessment against the respective state-of the art models.","This study presents an in-depth examination of prominent LLMs; GPT-3.5-turbo, Llama2-7B-Chat, Bloomz 7B1 and Bloomz 3B, across 14 tasks using 15 Urdu datasets, in a zero-shot setting, and their performance against state-of-the-art (SOTA) models, has been compared and analysed.","Our experiments show that SOTA models surpass all the encoder-decoder pre-trained language models in all Urdu NLP tasks with zero-shot learning.","Our results further show that LLMs with fewer parameters, but more language specific data in the base model perform better than larger computational models, but low language data."],"url":"http://arxiv.org/abs/2405.15453v1","category":"cs.CL"}
{"created":"2024-05-24 11:30:00","title":"Leveraging Logical Rules in Knowledge Editing: A Cherry on the Top","abstract":"Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key challenge in Large Language Models (LLMs). While best-performing solutions in this domain use a plan and solve paradigm to split a question into sub-questions followed by response generation, we claim that this approach is sub-optimal as it fails for hard to decompose questions, and it does not explicitly cater to correlated knowledge updates resulting as a consequence of knowledge edits. This has a detrimental impact on the overall consistency of the updated knowledge. To address these issues, in this paper, we propose a novel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a cherry on the top for augmenting the performance of all existing MQA methods under KE. Specifically, RULE-KE leverages rule discovery to discover a set of logical rules. Then, it uses these discovered rules to update knowledge about facts highly correlated with the edit. Experimental evaluation using existing and newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment both performances of parameter-based and memory-based solutions up to 92% and 112.9%, respectively.","sentences":["Multi-hop Question Answering (MQA) under knowledge editing (KE) is a key challenge in Large Language Models (LLMs).","While best-performing solutions in this domain use a plan and solve paradigm to split a question into sub-questions followed by response generation, we claim that this approach is sub-optimal as it fails for hard to decompose questions, and it does not explicitly cater to correlated knowledge updates resulting as a consequence of knowledge edits.","This has a detrimental impact on the overall consistency of the updated knowledge.","To address these issues, in this paper, we propose a novel framework named RULE-KE, i.e., RULE based Knowledge Editing, which is a cherry on the top for augmenting the performance of all existing MQA methods under KE.","Specifically, RULE-KE leverages rule discovery to discover a set of logical rules.","Then, it uses these discovered rules to update knowledge about facts highly correlated with the edit.","Experimental evaluation using existing and newly curated datasets (i.e., RKE-EVAL) shows that RULE-KE helps augment both performances of parameter-based and memory-based solutions up to 92% and 112.9%, respectively."],"url":"http://arxiv.org/abs/2405.15452v1","category":"cs.CL"}
{"created":"2024-05-24 11:22:19","title":"Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making","abstract":"Investigating fairness and equity of automated systems has become a critical field of inquiry. Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline. For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice). In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness. We introduce the notion of a margin complement, which measures how much a prediction score $S$ changes due to a thresholding operation. We then demonstrate that the marginal difference in the optimal 0/1 predictor $\\widehat Y$ between groups, written $P(\\hat y \\mid x_1) - P(\\hat y \\mid x_0)$, can be causally decomposed into the influences of $X$ on the $L_2$-optimal prediction score $S$ and the influences of $X$ on the margin complement $M$, along different causal pathways (direct, indirect, spurious). We then show that under suitable causal assumptions, the influences of $X$ on the prediction score $S$ are equal to the influences of $X$ on the true outcome $Y$. This yields a new decomposition of the disparity in the predictor $\\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself. This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied.","sentences":["Investigating fairness and equity of automated systems has become a critical field of inquiry.","Most of the literature in fair machine learning focuses on defining and achieving fairness criteria in the context of prediction, while not explicitly focusing on how these predictions may be used later on in the pipeline.","For instance, if commonly used criteria, such as independence or sufficiency, are satisfied for a prediction score $S$ used for binary classification, they need not be satisfied after an application of a simple thresholding operation on $S$ (as commonly used in practice).","In this paper, we take an important step to address this issue in numerous statistical and causal notions of fairness.","We introduce the notion of a margin complement, which measures how much a prediction score $S$ changes due to a thresholding operation.","We then demonstrate that the marginal difference in the optimal 0/1 predictor $\\widehat Y$ between groups, written $P(\\hat y \\mid x_1) -","P(\\hat y \\mid x_0)$, can be causally decomposed into the influences of $X$ on the $L_2$-optimal prediction score $S$ and the influences of $X$ on the margin complement $M$, along different causal pathways (direct, indirect, spurious).","We then show that under suitable causal assumptions, the influences of $X$ on the prediction score $S$ are equal to the influences of $X$ on the true outcome $Y$. This yields a new decomposition of the disparity in the predictor $\\widehat Y$ that allows us to disentangle causal differences inherited from the true outcome $Y$ that exists in the real world vs. those coming from the optimization procedure itself.","This observation highlights the need for more regulatory oversight due to the potential for bias amplification, and to address this issue we introduce new notions of weak and strong business necessity, together with an algorithm for assessing whether these notions are satisfied."],"url":"http://arxiv.org/abs/2405.15446v1","category":"cs.LG"}
{"created":"2024-05-24 11:21:51","title":"Cracking of submerged beds","abstract":"We investigate the phenomena of crater formation and gas release caused by projectile impact on underwater beds, which occurs in many natural, geophysical, and industrial applications. The bed in our experiment is constructed of hydrophobic particles, which trap a substantial amount of air in its pores. In contrast to dry beds, the air-water interface in a submerged bed generates a granular skin that provides rigidity to the medium by producing skin over the bulk. The projectile's energy is used to reorganise the grains, which causes the skin to crack, allowing the trapped air to escape. The morphology of the craters as a function of impact energy in submerged beds exhibits different scaling laws than what is known for dry beds. This phenomenon is attributed to the contact line motion on the hydrophobic fractal-like surface of submerged grains. The volume of the gas released is a function of multiple factors, chiefly the velocity of the projectile, depth of the bed and depth of the water column.","sentences":["We investigate the phenomena of crater formation and gas release caused by projectile impact on underwater beds, which occurs in many natural, geophysical, and industrial applications.","The bed in our experiment is constructed of hydrophobic particles, which trap a substantial amount of air in its pores.","In contrast to dry beds, the air-water interface in a submerged bed generates a granular skin that provides rigidity to the medium by producing skin over the bulk.","The projectile's energy is used to reorganise the grains, which causes the skin to crack, allowing the trapped air to escape.","The morphology of the craters as a function of impact energy in submerged beds exhibits different scaling laws than what is known for dry beds.","This phenomenon is attributed to the contact line motion on the hydrophobic fractal-like surface of submerged grains.","The volume of the gas released is a function of multiple factors, chiefly the velocity of the projectile, depth of the bed and depth of the water column."],"url":"http://arxiv.org/abs/2405.15445v1","category":"cond-mat.soft"}
{"created":"2024-05-24 11:20:41","title":"HyperInterval: Hypernetwork approach to training weight interval regions in continual learning","abstract":"Recently, a new Continual Learning (CL) paradigm was presented to control catastrophic forgetting, called Interval Continual Learning (InterContiNet), which relies on enforcing interval constraints on the neural network parameter space. Unfortunately, InterContiNet training is challenging due to the high dimensionality of the weight space, making intervals difficult to manage. To address this issue, we introduce HyperInterval, a technique that employs interval arithmetic within the embedding space and utilizes a hypernetwork to map these intervals to the target network parameter space. We train interval embeddings for consecutive tasks and train a hypernetwork to transform these embeddings into weights of the target network. An embedding for a given task is trained along with the hypernetwork, preserving the response of the target network for the previous task embeddings. Interval arithmetic works with a more manageable, lower-dimensional embedding space rather than directly preparing intervals in a high-dimensional weight space. Our model allows faster and more efficient training. Furthermore, HyperInterval maintains the guarantee of not forgetting. At the end of training, we can choose one universal embedding to produce a single network dedicated to all tasks. In such a framework, hypernetwork is used only for training and can be seen as a meta-trainer. HyperInterval obtains significantly better results than InterContiNet and gives SOTA results on several benchmarks.","sentences":["Recently, a new Continual Learning (CL) paradigm was presented to control catastrophic forgetting, called Interval Continual Learning (InterContiNet), which relies on enforcing interval constraints on the neural network parameter space.","Unfortunately, InterContiNet training is challenging due to the high dimensionality of the weight space, making intervals difficult to manage.","To address this issue, we introduce HyperInterval, a technique that employs interval arithmetic within the embedding space and utilizes a hypernetwork to map these intervals to the target network parameter space.","We train interval embeddings for consecutive tasks and train a hypernetwork to transform these embeddings into weights of the target network.","An embedding for a given task is trained along with the hypernetwork, preserving the response of the target network for the previous task embeddings.","Interval arithmetic works with a more manageable, lower-dimensional embedding space rather than directly preparing intervals in a high-dimensional weight space.","Our model allows faster and more efficient training.","Furthermore, HyperInterval maintains the guarantee of not forgetting.","At the end of training, we can choose one universal embedding to produce a single network dedicated to all tasks.","In such a framework, hypernetwork is used only for training and can be seen as a meta-trainer.","HyperInterval obtains significantly better results than InterContiNet and gives SOTA results on several benchmarks."],"url":"http://arxiv.org/abs/2405.15444v1","category":"cs.LG"}
{"created":"2024-05-24 11:19:52","title":"Fairness-Accuracy Trade-Offs: A Causal Perspective","abstract":"Systems based on machine learning may exhibit discriminatory behavior based on sensitive characteristics such as gender, sex, religion, or race. In light of this, various notions of fairness and methods to quantify discrimination were proposed, leading to the development of numerous approaches for constructing fair predictors. At the same time, imposing fairness constraints may decrease the utility of the decision-maker, highlighting a tension between fairness and utility. This tension is also recognized in legal frameworks, for instance in the disparate impact doctrine of Title VII of the Civil Rights Act of 1964 -- in which specific attention is given to considerations of business necessity -- possibly allowing the usage of proxy variables associated with the sensitive attribute in case a high-enough utility cannot be achieved without them. In this work, we analyze the tension between fairness and accuracy from a causal lens for the first time. We introduce the notion of a path-specific excess loss (PSEL) that captures how much the predictor's loss increases when a causal fairness constraint is enforced. We then show that the total excess loss (TEL), defined as the difference between the loss of predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of more local PSELs. At the same time, enforcing a causal constraint often reduces the disparity between demographic groups. Thus, we introduce a quantity that summarizes the fairness-utility trade-off, called the causal fairness/utility ratio, defined as the ratio of the reduction in discrimination vs. the excess loss from constraining a causal pathway. This quantity is suitable for comparing the fairness-utility trade-off across causal pathways. Finally, as our approach requires causally-constrained fair predictors, we introduce a new neural approach for causally-constrained fair learning.","sentences":["Systems based on machine learning may exhibit discriminatory behavior based on sensitive characteristics such as gender, sex, religion, or race.","In light of this, various notions of fairness and methods to quantify discrimination were proposed, leading to the development of numerous approaches for constructing fair predictors.","At the same time, imposing fairness constraints may decrease the utility of the decision-maker, highlighting a tension between fairness and utility.","This tension is also recognized in legal frameworks, for instance in the disparate impact doctrine of Title VII of the Civil Rights Act of 1964 -- in which specific attention is given to considerations of business necessity -- possibly allowing the usage of proxy variables associated with the sensitive attribute in case a high-enough utility cannot be achieved without them.","In this work, we analyze the tension between fairness and accuracy from a causal lens for the first time.","We introduce the notion of a path-specific excess loss (PSEL) that captures how much the predictor's loss increases when a causal fairness constraint is enforced.","We then show that the total excess loss (TEL), defined as the difference between the loss of predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of more local PSELs.","At the same time, enforcing a causal constraint often reduces the disparity between demographic groups.","Thus, we introduce a quantity that summarizes the fairness-utility trade-off, called the causal fairness/utility ratio, defined as the ratio of the reduction in discrimination vs. the excess loss from constraining a causal pathway.","This quantity is suitable for comparing the fairness-utility trade-off across causal pathways.","Finally, as our approach requires causally-constrained fair predictors, we introduce a new neural approach for causally-constrained fair learning."],"url":"http://arxiv.org/abs/2405.15443v1","category":"cs.LG"}
{"created":"2024-05-24 11:14:56","title":"Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances","abstract":"Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality. Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures. The kernel max-sliced (KMS) Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimensions before computing the Wasserstein distance. However, its theoretical properties have not yet been fully developed. In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\\in[1,\\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the SDP solution. We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing.","sentences":["Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality.","Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures.","The kernel max-sliced (KMS)","Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimensions before computing the Wasserstein distance.","However, its theoretical properties have not yet been fully developed.","In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\\in[1,\\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the SDP solution.","We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing."],"url":"http://arxiv.org/abs/2405.15441v1","category":"stat.ML"}
{"created":"2024-05-24 11:12:37","title":"Text-guided 3D Human Motion Generation with Keyframe-based Parallel Skip Transformer","abstract":"Text-driven human motion generation is an emerging task in animation and humanoid robot design. Existing algorithms directly generate the full sequence which is computationally expensive and prone to errors as it does not pay special attention to key poses, a process that has been the cornerstone of animation for decades. We propose KeyMotion, that generates plausible human motion sequences corresponding to input text by first generating keyframes followed by in-filling. We use a Variational Autoencoder (VAE) with Kullback-Leibler regularization to project the keyframes into a latent space to reduce dimensionality and further accelerate the subsequent diffusion process. For the reverse diffusion, we propose a novel Parallel Skip Transformer that performs cross-modal attention between the keyframe latents and text condition. To complete the motion sequence, we propose a text-guided Transformer designed to perform motion-in-filling, ensuring the preservation of both fidelity and adherence to the physical constraints of human motion. Experiments show that our method achieves state-of-theart results on the HumanML3D dataset outperforming others on all R-precision metrics and MultiModal Distance. KeyMotion also achieves competitive performance on the KIT dataset, achieving the best results on Top3 R-precision, FID, and Diversity metrics.","sentences":["Text-driven human motion generation is an emerging task in animation and humanoid robot design.","Existing algorithms directly generate the full sequence which is computationally expensive and prone to errors as it does not pay special attention to key poses, a process that has been the cornerstone of animation for decades.","We propose KeyMotion, that generates plausible human motion sequences corresponding to input text by first generating keyframes followed by in-filling.","We use a Variational Autoencoder (VAE) with Kullback-Leibler regularization to project the keyframes into a latent space to reduce dimensionality and further accelerate the subsequent diffusion process.","For the reverse diffusion, we propose a novel Parallel Skip Transformer that performs cross-modal attention between the keyframe latents and text condition.","To complete the motion sequence, we propose a text-guided Transformer designed to perform motion-in-filling, ensuring the preservation of both fidelity and adherence to the physical constraints of human motion.","Experiments show that our method achieves state-of-theart results on the HumanML3D dataset outperforming others on all R-precision metrics and MultiModal Distance.","KeyMotion also achieves competitive performance on the KIT dataset, achieving the best results on Top3 R-precision, FID, and Diversity metrics."],"url":"http://arxiv.org/abs/2405.15439v1","category":"cs.CV"}
{"created":"2024-05-24 11:10:58","title":"Comparing remote sensing-based forest biomass mapping approaches using new forest inventory plots in contrasting forests in northeastern and southwestern China","abstract":"Large-scale high spatial resolution aboveground biomass (AGB) maps play a crucial role in determining forest carbon stocks and how they are changing, which is instrumental in understanding the global carbon cycle, and implementing policy to mitigate climate change. The advent of the new space-borne LiDAR sensor, NASA's GEDI instrument, provides unparalleled possibilities for the accurate and unbiased estimation of forest AGB at high resolution, particularly in dense and tall forests, where Synthetic Aperture Radar (SAR) and passive optical data exhibit saturation. However, GEDI is a sampling instrument, collecting dispersed footprints, and its data must be combined with that from other continuous cover satellites to create high-resolution maps, using local machine learning methods. In this study, we developed local models to estimate forest AGB from GEDI L2A data, as the models used to create GEDI L4 AGB data incorporated minimal field data from China. We then applied LightGBM and random forest regression to generate wall-to-wall AGB maps at 25 m resolution, using extensive GEDI footprints as well as Sentinel-1 data, ALOS-2 PALSAR-2 and Sentinel-2 optical data. Through a 5-fold cross-validation, LightGBM demonstrated a slightly better performance than Random Forest across two contrasting regions. However, in both regions, the computation speed of LightGBM is substantially faster than that of the random forest model, requiring roughly one-third of the time to compute on the same hardware. Through the validation against field data, the 25 m resolution AGB maps generated using the local models developed in this study exhibited higher accuracy compared to the GEDI L4B AGB data. We found in both regions an increase in error as slope increased. The trained models were tested on nearby but different regions and exhibited good performance.","sentences":["Large-scale high spatial resolution aboveground biomass (AGB) maps play a crucial role in determining forest carbon stocks and how they are changing, which is instrumental in understanding the global carbon cycle, and implementing policy to mitigate climate change.","The advent of the new space-borne LiDAR sensor, NASA's GEDI instrument, provides unparalleled possibilities for the accurate and unbiased estimation of forest AGB at high resolution, particularly in dense and tall forests, where Synthetic Aperture Radar (SAR) and passive optical data exhibit saturation.","However, GEDI is a sampling instrument, collecting dispersed footprints, and its data must be combined with that from other continuous cover satellites to create high-resolution maps, using local machine learning methods.","In this study, we developed local models to estimate forest AGB from GEDI L2A data, as the models used to create GEDI L4 AGB data incorporated minimal field data from China.","We then applied LightGBM and random forest regression to generate wall-to-wall AGB maps at 25 m resolution, using extensive GEDI footprints as well as Sentinel-1 data, ALOS-2 PALSAR-2 and Sentinel-2 optical data.","Through a 5-fold cross-validation, LightGBM demonstrated a slightly better performance than Random Forest across two contrasting regions.","However, in both regions, the computation speed of LightGBM is substantially faster than that of the random forest model, requiring roughly one-third of the time to compute on the same hardware.","Through the validation against field data, the 25 m resolution AGB maps generated using the local models developed in this study exhibited higher accuracy compared to the GEDI L4B AGB data.","We found in both regions an increase in error as slope increased.","The trained models were tested on nearby but different regions and exhibited good performance."],"url":"http://arxiv.org/abs/2405.15438v1","category":"cs.CV"}
{"created":"2024-05-24 11:07:48","title":"Learning about Data, Algorithms, and Algorithmic Justice on TikTok in Personally Meaningful Ways","abstract":"TikTok, a popular short video sharing application, emerged as the dominant social media platform for young people, with a pronounced influence on how young women and people of color interact online. The application has become a global space for youth to connect with each other, offering not only entertainment but also opportunities to engage with artificial intelligence/machine learning (AI/ML)-driven recommendations and create content using AI/M-powered tools, such as generative AI filters. This provides opportunities for youth to explore and question the inner workings of these systems, their implications, and even use them to advocate for causes they are passionate about. We present different perspectives on how youth may learn in personally meaningful ways when engaging with TikTok. We discuss how youth investigate how TikTok works (considering data and algorithms), take into account issues of ethics and algorithmic justice and use their understanding of the platform to advocate for change.","sentences":["TikTok, a popular short video sharing application, emerged as the dominant social media platform for young people, with a pronounced influence on how young women and people of color interact online.","The application has become a global space for youth to connect with each other, offering not only entertainment but also opportunities to engage with artificial intelligence/machine learning (AI/ML)-driven recommendations and create content using AI/M-powered tools, such as generative AI filters.","This provides opportunities for youth to explore and question the inner workings of these systems, their implications, and even use them to advocate for causes they are passionate about.","We present different perspectives on how youth may learn in personally meaningful ways when engaging with TikTok.","We discuss how youth investigate how TikTok works (considering data and algorithms), take into account issues of ethics and algorithmic justice and use their understanding of the platform to advocate for change."],"url":"http://arxiv.org/abs/2405.15437v1","category":"cs.CY"}
{"created":"2024-05-24 11:05:45","title":"Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance","abstract":"In higher education, accreditation is a quality assurance process, where an institution demonstrates a commitment to delivering high quality programs and services to their students. For business schools nationally and internationally the Association to Advance Collegiate Schools of Business (AACSB) accreditation is the gold standard. For a business school to receive and subsequently maintain accreditation, the school must undertake a rigorous, time consuming reporting and peer review process, to demonstrate alignment with the AACSB Standards. For this project we create a hybrid context retrieval augmented generation pipeline that can assist in the documentation alignment and reporting process necessary for accreditation. We implement both a vector database and knowledge graph, as knowledge stores containing both institutional data and AACSB Standard data. The output of the pipeline can be used by institution stakeholders to build their accreditation report, dually grounded by the context from the knowledge stores. To develop our knowledge graphs we utilized both a manual construction process as well as an LLM Augmented Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework and observed optimal performance on answer relevancy and answer correctness metrics.","sentences":["In higher education, accreditation is a quality assurance process, where an institution demonstrates a commitment to delivering high quality programs and services to their students.","For business schools nationally and internationally the Association to Advance Collegiate Schools of Business (AACSB) accreditation is the gold standard.","For a business school to receive and subsequently maintain accreditation, the school must undertake a rigorous, time consuming reporting and peer review process, to demonstrate alignment with the AACSB Standards.","For this project we create a hybrid context retrieval augmented generation pipeline that can assist in the documentation alignment and reporting process necessary for accreditation.","We implement both a vector database and knowledge graph, as knowledge stores containing both institutional data and AACSB Standard data.","The output of the pipeline can be used by institution stakeholders to build their accreditation report, dually grounded by the context from the knowledge stores.","To develop our knowledge graphs we utilized both a manual construction process as well as an LLM Augmented Knowledge Graph approach.","We evaluated the pipeline using the RAGAs framework and observed optimal performance on answer relevancy and answer correctness metrics."],"url":"http://arxiv.org/abs/2405.15436v1","category":"cs.IR"}
{"created":"2024-05-24 10:58:20","title":"Throughput Requirements for RAN Functional Splits in 3D-Networks","abstract":"The rapid growth of non-terrestrial communication necessitates its integration with existing terrestrial networks, as highlighted in 3GPP Releases 16 and 17. This paper analyses the concept of functional splits in 3D-Networks. To manage this complex structure effectively, the adoption of a Radio Access Network (RAN) architecture with Functional Split (FS) offers advantages in flexibility, scalability, and cost-efficiency. RAN achieves this by disaggregating functionalities into three separate units. Analogous to the terrestrial network approach, 3GPP is extending this concept to non-terrestrial platforms as well. This work presents a general analysis of the requested Fronthaul (FH) data rate on feeder link between a non-terrestrial platform and the ground-station. Each split option is a trade-of between FH data rate and the respected complexity. Since flying nodes face more limitations regarding power consumption and complexity on board in comparison to terrestrial ones, we are investigating the split options between lower and higher physical layer.","sentences":["The rapid growth of non-terrestrial communication necessitates its integration with existing terrestrial networks, as highlighted in 3GPP Releases 16 and 17.","This paper analyses the concept of functional splits in 3D-Networks.","To manage this complex structure effectively, the adoption of a Radio Access Network (RAN) architecture with Functional Split (FS) offers advantages in flexibility, scalability, and cost-efficiency.","RAN achieves this by disaggregating functionalities into three separate units.","Analogous to the terrestrial network approach, 3GPP is extending this concept to non-terrestrial platforms as well.","This work presents a general analysis of the requested Fronthaul (FH) data rate on feeder link between a non-terrestrial platform and the ground-station.","Each split option is a trade-of between FH data rate and the respected complexity.","Since flying nodes face more limitations regarding power consumption and complexity on board in comparison to terrestrial ones, we are investigating the split options between lower and higher physical layer."],"url":"http://arxiv.org/abs/2405.15432v1","category":"eess.SP"}
{"created":"2024-05-24 10:44:22","title":"AuthNet: Neural Network with Integrated Authentication Logic","abstract":"Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats. Proprietary models may be protected by access controls and encryption. However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider. Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations. To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures. Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer. Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet. It exhibits two intuitive advantages. It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key. Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model. We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples. AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks.","sentences":["Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats.","Proprietary models may be protected by access controls and encryption.","However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider.","Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations.","To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures.","Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer.","Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet.","It exhibits two intuitive advantages.","It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key.","Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model.","We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples.","AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks."],"url":"http://arxiv.org/abs/2405.15426v1","category":"cs.CR"}
{"created":"2024-05-24 10:42:05","title":"Volumetric Primitives for Modeling and Rendering Scattering and Emissive Media","abstract":"We propose a volumetric representation based on primitives to model scattering and emissive media. Accurate scene representations enabling efficient rendering are essential for many computer graphics applications. General and unified representations that can handle surface and volume-based representations simultaneously, allowing for physically accurate modeling, remain a research challenge. Inspired by recent methods for scene reconstruction that leverage mixtures of 3D Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives. We introduce closed-form solutions for transmittance and free-flight distance sampling for 3D Gaussian kernels, and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer by leveraging ray tracing for efficiently querying the medium. We demonstrate our method as an alternative to other forms of volume modeling (e.g. voxel grid-based representations) for forward and inverse rendering of scattering media. Furthermore, we adapt our method to the problem of radiance field optimization and rendering, and demonstrate comparable performance to the state of the art, while providing additional flexibility in terms of performance and usability.","sentences":["We propose a volumetric representation based on primitives to model scattering and emissive media.","Accurate scene representations enabling efficient rendering are essential for many computer graphics applications.","General and unified representations that can handle surface and volume-based representations simultaneously, allowing for physically accurate modeling, remain a research challenge.","Inspired by recent methods for scene reconstruction that leverage mixtures of 3D Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives.","We introduce closed-form solutions for transmittance and free-flight distance sampling for 3D Gaussian kernels, and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer by leveraging ray tracing for efficiently querying the medium.","We demonstrate our method as an alternative to other forms of volume modeling (e.g. voxel grid-based representations) for forward and inverse rendering of scattering media.","Furthermore, we adapt our method to the problem of radiance field optimization and rendering, and demonstrate comparable performance to the state of the art, while providing additional flexibility in terms of performance and usability."],"url":"http://arxiv.org/abs/2405.15425v1","category":"cs.GR"}
{"created":"2024-05-24 10:34:00","title":"Visible-frequency hyperbolic plasmon polaritons in a natural van der Waals crystal","abstract":"Controlling light at subwavelength scales is one of the main challenges of nanophotonics. Leveraging hyperbolic polaritons supporting arbitrarily large wavevectors can lead to extreme light confinement, effectively overcoming the diffraction limit. Hyperbolicity was initially realized in artificial metamaterials, but their performances are limited by high losses in the metallic components and the constraint of long wavelengths. While recently discovered natural low-loss hyperbolic phonon polaritons initiated a revival in the interest for hyperbolic materials, they are confined to the mid-infrared frequency range, limiting their use for several applications. Some hyperbolic materials at visible frequencies have been studied, but they are either very lossy or only feature out-of-plane hyperbolicity. Here, we demonstrate the presence of low-loss, in-plane hyperbolic plasmon polaritons in the visible and near-infrared in thin films of MoOCl2, a natural van der Waals crystal. The polariton dispersion is predicted based on the framework of light propagation in biaxial media, and experimentally confirmed by real space nano imaging on exfoliated flakes. MoOCl2 constitutes a novel material platform for visible-range applications leveraging the hyperbolic dispersion, such as hyperlensing, Purcell factor enhancement and super-resolution imaging, without the drawbacks of metamaterials.","sentences":["Controlling light at subwavelength scales is one of the main challenges of nanophotonics.","Leveraging hyperbolic polaritons supporting arbitrarily large wavevectors can lead to extreme light confinement, effectively overcoming the diffraction limit.","Hyperbolicity was initially realized in artificial metamaterials, but their performances are limited by high losses in the metallic components and the constraint of long wavelengths.","While recently discovered natural low-loss hyperbolic phonon polaritons initiated a revival in the interest for hyperbolic materials, they are confined to the mid-infrared frequency range, limiting their use for several applications.","Some hyperbolic materials at visible frequencies have been studied, but they are either very lossy or only feature out-of-plane hyperbolicity.","Here, we demonstrate the presence of low-loss, in-plane hyperbolic plasmon polaritons in the visible and near-infrared in thin films of MoOCl2, a natural van der Waals crystal.","The polariton dispersion is predicted based on the framework of light propagation in biaxial media, and experimentally confirmed by real space nano imaging on exfoliated flakes.","MoOCl2 constitutes a novel material platform for visible-range applications leveraging the hyperbolic dispersion, such as hyperlensing, Purcell factor enhancement and super-resolution imaging, without the drawbacks of metamaterials."],"url":"http://arxiv.org/abs/2405.15420v1","category":"physics.optics"}
{"created":"2024-05-24 10:25:59","title":"Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification","abstract":"Building open agents has always been the ultimate goal in AI research, and creative agents are the more enticing. Existing LLM agents excel at long-horizon tasks with well-defined goals (e.g., `mine diamonds' in Minecraft). However, they encounter difficulties on creative tasks with open goals and abstract criteria due to the inability to bridge the gap between them, thus lacking feedback for self-improvement in solving the task. In this work, we introduce autonomous embodied verification techniques for agents to fill the gap, laying the groundwork for creative tasks. Specifically, we propose the Luban agent target creative building tasks in Minecraft, which equips with two-level autonomous embodied verification inspired by human design practices: (1) visual verification of 3D structural speculates, which comes from agent synthesized CAD modeling programs; (2) pragmatic verification of the creation by generating and verifying environment-relevant functionality programs based on the abstract criteria. Extensive multi-dimensional human studies and Elo ratings show that the Luban completes diverse creative building tasks in our proposed benchmark and outperforms other baselines ($33\\%$ to $100\\%$) in both visualization and pragmatism. Additional demos on the real-world robotic arm show the creation potential of the Luban in the physical world.","sentences":["Building open agents has always been the ultimate goal in AI research, and creative agents are the more enticing.","Existing LLM agents excel at long-horizon tasks with well-defined goals (e.g., `mine diamonds' in Minecraft).","However, they encounter difficulties on creative tasks with open goals and abstract criteria due to the inability to bridge the gap between them, thus lacking feedback for self-improvement in solving the task.","In this work, we introduce autonomous embodied verification techniques for agents to fill the gap, laying the groundwork for creative tasks.","Specifically, we propose the Luban agent target creative building tasks in Minecraft, which equips with two-level autonomous embodied verification inspired by human design practices: (1) visual verification of 3D structural speculates, which comes from agent synthesized CAD modeling programs; (2) pragmatic verification of the creation by generating and verifying environment-relevant functionality programs based on the abstract criteria.","Extensive multi-dimensional human studies and Elo ratings show that the Luban completes diverse creative building tasks in our proposed benchmark and outperforms other baselines ($33\\%$ to $100\\%$) in both visualization and pragmatism.","Additional demos on the real-world robotic arm show the creation potential of the Luban in the physical world."],"url":"http://arxiv.org/abs/2405.15414v1","category":"cs.AI"}
{"created":"2024-05-24 10:23:17","title":"ORCA: A Global Ocean Emulator for Multi-year to Decadal Predictions","abstract":"Ocean dynamics plays a crucial role in driving global weather and climate patterns. Accurate and efficient modeling of ocean dynamics is essential for improved understanding of complex ocean circulation and processes, for predicting climate variations and their associated teleconnections, and for addressing the challenges of climate change. While great efforts have been made to improve numerical Ocean General Circulation Models (OGCMs), accurate forecasting of global oceanic variations for multi-year remains to be a long-standing challenge. Here, we introduce ORCA (Oceanic Reliable foreCAst), the first data-driven model predicting global ocean circulation from multi-year to decadal time scales. ORCA accurately simulates the three-dimensional circulations and dynamics of the global ocean with high physical consistency. Hindcasts of key oceanic variables demonstrate ORCA's remarkable prediction skills in predicting ocean variations compared with state-of-the-art numerical OGCMs and abilities in capturing occurrences of extreme events at the subsurface ocean and ENSO vertical patterns. These results demonstrate the potential of data-driven ocean models for providing cheap, efficient, and accurate global ocean modeling and prediction. Moreover, ORCA stably and faithfully emulates ocean dynamics at decadal timescales, demonstrating its potential even for climate projections. The model will be available at https://github.com/OpenEarthLab/ORCA.","sentences":["Ocean dynamics plays a crucial role in driving global weather and climate patterns.","Accurate and efficient modeling of ocean dynamics is essential for improved understanding of complex ocean circulation and processes, for predicting climate variations and their associated teleconnections, and for addressing the challenges of climate change.","While great efforts have been made to improve numerical Ocean General Circulation Models (OGCMs), accurate forecasting of global oceanic variations for multi-year remains to be a long-standing challenge.","Here, we introduce ORCA (Oceanic Reliable foreCAst), the first data-driven model predicting global ocean circulation from multi-year to decadal time scales.","ORCA accurately simulates the three-dimensional circulations and dynamics of the global ocean with high physical consistency.","Hindcasts of key oceanic variables demonstrate ORCA's remarkable prediction skills in predicting ocean variations compared with state-of-the-art numerical OGCMs and abilities in capturing occurrences of extreme events at the subsurface ocean and ENSO vertical patterns.","These results demonstrate the potential of data-driven ocean models for providing cheap, efficient, and accurate global ocean modeling and prediction.","Moreover, ORCA stably and faithfully emulates ocean dynamics at decadal timescales, demonstrating its potential even for climate projections.","The model will be available at https://github.com/OpenEarthLab/ORCA."],"url":"http://arxiv.org/abs/2405.15412v1","category":"physics.ao-ph"}
{"created":"2024-05-24 10:22:41","title":"Short range order and topology of Te-rich amorphous Ge-Sb-Te alloys","abstract":"The structure of evaporated amorphous Ge$_x$Sb$_x$Te$_{100-2x}$ ($x =$ 6, 9, 13) alloys was investigated by neutron diffraction, X-ray diffraction and extended X-ray absorption spectroscopy (EXAFS) at the Ge, Sb and Te K-edges. Large scale structural models were generated by fitting the experimental datasets (5 for each composition) simultaneously in the framework of the reverse Monte Carlo simulation technique. It was found that the alloys are chemically ordered (Ge and Sb have predominantly Te neighbors) and within the experimental uncertainty each component satisfies the $8-N$ rule. A comparison with the pair correlation functions of melt quenched Ge$_{20}$Te$_{80}$ revealed that the first minimum of $g_{\\mathrm{TeTe}}(r)$ is shallower in the ternary alloys than in Ge$_{20}$Te$_{80}$. On the other hand, the separation of the first and second coordination environments of Ge atoms is stronger in the Ge-Sb-Te alloys investigated.","sentences":["The structure of evaporated amorphous Ge$_x$Sb$_x$Te$_{100-2x}$ ($x =$ 6, 9, 13) alloys was investigated by neutron diffraction, X-ray diffraction and extended X-ray absorption spectroscopy (EXAFS) at the Ge, Sb and Te K-edges.","Large scale structural models were generated by fitting the experimental datasets (5 for each composition) simultaneously in the framework of the reverse Monte Carlo simulation technique.","It was found that the alloys are chemically ordered (Ge and Sb have predominantly Te neighbors) and within the experimental uncertainty each component satisfies the $8-N$ rule.","A comparison with the pair correlation functions of melt quenched Ge$_{20}$Te$_{80}$ revealed that the first minimum of $g_{\\mathrm{TeTe}}(r)$ is shallower in the ternary alloys than in Ge$_{20}$Te$_{80}$.","On the other hand, the separation of the first and second coordination environments of Ge atoms is stronger in the Ge-Sb-Te alloys investigated."],"url":"http://arxiv.org/abs/2405.15411v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 10:22:15","title":"Electric Hall Effect and Quantum Electric Hall Effect","abstract":"Exploring new Hall effect is always a fascinating research topic. The ordinary Hall effect and the quantum Hall effect, initially discovered in two-dimensional (2D) non-magnetic systems, are the phenomena that a transverse current is generated when a system carrying an electron current is placed in a magnetic field perpendicular to the currents. In this work, we propose the electric counterparts of these two Hall effects, termed as electric Hall effect (EHE) and quantum electric Hall effect (QEHE). The EHE and QEHE emerge in 2D magnetic systems, where the transverse current is generated by applying an electric gate-field instead of a magnetic field. We present a symmetry requirement for intrinsic EHE and QEHE. With a weak gate-field, we establish an analytical expression of the intrinsic EHE coefficient. We show that it is determined by intrinsic band geometric quantities: Berry curvature and its polarizability which consists of both intraband and interband layer polarization. Via first-principles calculations, we investigate the EHE in the monolayer Ca(FeN)$_2$, where significant EHE coefficient is observed around band crossings. Furthermore, we demonstrate that the QEHE can appear in the semiconductor monolayer $\\rm BaMn_2S_3$, of which the Hall conductivity exhibits steps that take on the quantized values $0$ and $\\pm1$ in the unit of $e^2/h$ by varying the gate-field within the experimentally achievable range. Due to the great tunability of the electric gate-field, the EHE and QEHE proposed here can be easily controlled and should have more potential applications.","sentences":["Exploring new Hall effect is always a fascinating research topic.","The ordinary Hall effect and the quantum Hall effect, initially discovered in two-dimensional (2D) non-magnetic systems, are the phenomena that a transverse current is generated when a system carrying an electron current is placed in a magnetic field perpendicular to the currents.","In this work, we propose the electric counterparts of these two Hall effects, termed as electric Hall effect (EHE) and quantum electric Hall effect (QEHE).","The EHE and QEHE emerge in 2D magnetic systems, where the transverse current is generated by applying an electric gate-field instead of a magnetic field.","We present a symmetry requirement for intrinsic EHE and QEHE.","With a weak gate-field, we establish an analytical expression of the intrinsic EHE coefficient.","We show that it is determined by intrinsic band geometric quantities: Berry curvature and its polarizability which consists of both intraband and interband layer polarization.","Via first-principles calculations, we investigate the EHE in the monolayer Ca(FeN)$_2$, where significant EHE coefficient is observed around band crossings.","Furthermore, we demonstrate that the QEHE can appear in the semiconductor monolayer $\\rm BaMn_2S_3$, of which the Hall conductivity exhibits steps that take on the quantized values $0$ and $\\pm1$ in the unit of $e^2/h$ by varying the gate-field within the experimentally achievable range.","Due to the great tunability of the electric gate-field, the EHE and QEHE proposed here can be easily controlled and should have more potential applications."],"url":"http://arxiv.org/abs/2405.15410v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-24 10:21:22","title":"SU(2) structures in four dimensions and Plebanski formalism for GR","abstract":"An SU(2) structure in four dimensions can be described as a triple of 2-forms Sigma^i in Lambda^2(M), i=1,2,3 satisfying Sigma^i wedge Sigma^j ~ delta^{ij}. Such a triple defines a Riemannian signature metric on M. An SU(2) structure is said to be integrable if the holonomy of this Riemannian metric is contained in SU(2). It is well-known that this is the case if and only if the 2-forms are closed dSigma^i=0. The main purpose of the paper is to analyse the second order in derivatives diffeomorphism invariant action functionals that can be constructed for an SU(2) structure. The main result is that there is a unique such action functional if one imposes an additional requirement that the action is also SU(2) invariant, with SU(2) acting on the triple Sigma^i as in its vector representation. This action functional has a very simple expression in terms of the intrinsic torsion of the SU(2) structure. We show that its critical points are SU(2) structures whose associated metric is Einstein. The action we describe has also a first order in derivatives version, and we show how this is related to what in the physics literature is known as Plebanski formalism for GR.","sentences":["An SU(2) structure in four dimensions can be described as a triple of 2-forms Sigma^i in Lambda^2(M), i=1,2,3 satisfying Sigma^i wedge Sigma^j ~ delta^{ij}.","Such a triple defines a Riemannian signature metric on M. An SU(2) structure is said to be integrable if the holonomy of this Riemannian metric is contained in SU(2).","It is well-known that this is the case if and only if the 2-forms are closed dSigma^i=0.","The main purpose of the paper is to analyse the second order in derivatives diffeomorphism invariant action functionals that can be constructed for an SU(2) structure.","The main result is that there is a unique such action functional if one imposes an additional requirement that the action is also SU(2) invariant, with SU(2) acting on the triple Sigma^i as in its vector representation.","This action functional has a very simple expression in terms of the intrinsic torsion of the SU(2) structure.","We show that its critical points are SU(2) structures whose associated metric is Einstein.","The action we describe has also a first order in derivatives version, and we show how this is related to what in the physics literature is known as Plebanski formalism for GR."],"url":"http://arxiv.org/abs/2405.15408v1","category":"math.DG"}
{"created":"2024-05-24 10:17:15","title":"A Misleading Gallery of Fluid Motion by Generative Artificial Intelligence","abstract":"In this technical report, we extensively investigate the accuracy of outputs from well-known generative artificial intelligence (AI) applications in response to prompts describing common fluid motion phenomena familiar to the fluid mechanics community. We examine a range of applications, including Midjourney, Dall-E, Runway ML, Microsoft Designer, Gemini, Meta AI, and Leonardo AI, introduced by prominent companies such as Google, OpenAI, Meta, and Microsoft. Our text prompts for generating images or videos include examples such as \"Von Karman vortex street\", \"flow past an airfoil\", \"Kelvin-Helmholtz instability\", \"shock waves on a sharp-nosed supersonic body\", etc. We compare the images generated by these applications with real images from laboratory experiments and numerical software. Our findings indicate that these generative AI models are not adequately trained in fluid dynamics imagery, leading to potentially misleading outputs. Beyond text-to-image/video generation, we further explore the transition from image/video to text generation using these AI tools, aiming to investigate the accuracy of their descriptions of fluid motion phenomena. This report serves as a cautionary note for educators in academic institutions, highlighting the potential for these tools to mislead students. It also aims to inform researchers at these renowned companies, encouraging them to address this issue. We conjecture that a primary reason for this shortcoming is the limited access to copyright-protected fluid motion images from scientific journals.","sentences":["In this technical report, we extensively investigate the accuracy of outputs from well-known generative artificial intelligence (AI) applications in response to prompts describing common fluid motion phenomena familiar to the fluid mechanics community.","We examine a range of applications, including Midjourney, Dall-E, Runway ML, Microsoft Designer, Gemini, Meta AI, and Leonardo AI, introduced by prominent companies such as Google, OpenAI, Meta, and Microsoft.","Our text prompts for generating images or videos include examples such as \"Von Karman vortex street\", \"flow past an airfoil\", \"Kelvin-Helmholtz instability\", \"shock waves on a sharp-nosed supersonic body\", etc.","We compare the images generated by these applications with real images from laboratory experiments and numerical software.","Our findings indicate that these generative AI models are not adequately trained in fluid dynamics imagery, leading to potentially misleading outputs.","Beyond text-to-image/video generation, we further explore the transition from image/video to text generation using these AI tools, aiming to investigate the accuracy of their descriptions of fluid motion phenomena.","This report serves as a cautionary note for educators in academic institutions, highlighting the potential for these tools to mislead students.","It also aims to inform researchers at these renowned companies, encouraging them to address this issue.","We conjecture that a primary reason for this shortcoming is the limited access to copyright-protected fluid motion images from scientific journals."],"url":"http://arxiv.org/abs/2405.15406v1","category":"physics.flu-dyn"}
{"created":"2024-05-24 10:13:49","title":"Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification","abstract":"Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.","sentences":["Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients.","However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence.","In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS).","The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels.","The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities.","On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC.","The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer."],"url":"http://arxiv.org/abs/2405.15405v1","category":"cs.CV"}
{"created":"2024-05-24 10:13:03","title":"Relations between nonsmooth vector variational inequalities and nonsmooth vector optimization problems on Hadamard manifold in terms of bifunction","abstract":"In this paper, we discuss the concepts of bifunction and geodesic convexity for vector valued functions on Hadamard manifold. The Hadamard manifold is a particular type of Riemannian manifold with non-positive sectional curvature. Using bifunction, we introduce a definition of generalized geodesic convexity in the context of the Hadamard manifold. To support the definition, we construct a non-trivial example that demonstrates the property of geodesic convexity on Hadamard manifold. Additionally, we define the geodesic $h$-convexity, geodesic $h$-pseudoconvexity and geodesic $h$-quasiconvexity for vector valued function using bifunction and study their several properties. Furthermore, we demonstrate the uniqueness of the solution for nonsmooth vector variational inequality problem (NVVIP) and prove the characterization property for the solution of NVVIP and the Minty type NVVIP (MNVVIP) on Hadamard manifold in terms of bifunction. Afterward, we consider a nonsmooth vector optimization problem (NVOP) and investigate the relationships among the solutions of NVOP, NVVIP, and MNVVIP.","sentences":["In this paper, we discuss the concepts of bifunction and geodesic convexity for vector valued functions on Hadamard manifold.","The Hadamard manifold is a particular type of Riemannian manifold with non-positive sectional curvature.","Using bifunction, we introduce a definition of generalized geodesic convexity in the context of the Hadamard manifold.","To support the definition, we construct a non-trivial example that demonstrates the property of geodesic convexity on Hadamard manifold.","Additionally, we define the geodesic $h$-convexity, geodesic $h$-pseudoconvexity and geodesic $h$-quasiconvexity for vector valued function using bifunction and study their several properties.","Furthermore, we demonstrate the uniqueness of the solution for nonsmooth vector variational inequality problem (NVVIP) and prove the characterization property for the solution of NVVIP and the Minty type NVVIP (MNVVIP) on Hadamard manifold in terms of bifunction.","Afterward, we consider a nonsmooth vector optimization problem (NVOP) and investigate the relationships among the solutions of NVOP, NVVIP, and MNVVIP."],"url":"http://arxiv.org/abs/2405.15404v1","category":"math.OC"}
{"created":"2024-05-24 10:07:09","title":"Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random","abstract":"In most practical applications such as recommendation systems, display advertising, and so forth, the collected data often contains missing values and those missing values are generally missing-not-at-random, which deteriorates the prediction performance of models. Some existing estimators and regularizers attempt to achieve unbiased estimation to improve the predictive performance. However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness. In this paper, we first theoretically reveal that limitations of regularization techniques. Besides, we further illustrate that, for more general estimators, unbiasedness will inevitably lead to unbounded variance. These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off. Instead, it involves a quantitative joint optimization of bias and variance. Then, we develop a systematic fine-grained dynamic learning framework to jointly optimize bias and variance, which adaptively selects an appropriate estimator for each user-item pair according to the predefined objective function. With this operation, the generalization bounds and variances of models are reduced and bounded with theoretical guarantees. Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework.","sentences":["In most practical applications such as recommendation systems, display advertising, and so forth, the collected data often contains missing values and those missing values are generally missing-not-at-random, which deteriorates the prediction performance of models.","Some existing estimators and regularizers attempt to achieve unbiased estimation to improve the predictive performance.","However, variances and generalization bound of these methods are generally unbounded when the propensity scores tend to zero, compromising their stability and robustness.","In this paper, we first theoretically reveal that limitations of regularization techniques.","Besides, we further illustrate that, for more general estimators, unbiasedness will inevitably lead to unbounded variance.","These general laws inspire us that the estimator designs is not merely about eliminating bias, reducing variance, or simply achieve a bias-variance trade-off.","Instead, it involves a quantitative joint optimization of bias and variance.","Then, we develop a systematic fine-grained dynamic learning framework to jointly optimize bias and variance, which adaptively selects an appropriate estimator for each user-item pair according to the predefined objective function.","With this operation, the generalization bounds and variances of models are reduced and bounded with theoretical guarantees.","Extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed dynamic learning framework."],"url":"http://arxiv.org/abs/2405.15403v1","category":"cs.LG"}
{"created":"2024-05-24 09:53:24","title":"Stochastic SR for Gaussian microtextures","abstract":"Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator. This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images. While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input. In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures. Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures. The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines.","sentences":["Super-Resolution (SR) is the problem that consists in reconstructing images that have been degraded by a zoom-out operator.","This is an ill-posed problem that does not have a unique solution, and numerical approaches rely on a prior on high-resolution images.","While optimization-based methods are generally deterministic, with the rise of image generative models more and more interest has been given to stochastic SR, that is, sampling among all possible SR images associated with a given low-resolution input.","In this paper, we construct an efficient, stable and provably exact sampler for the stochastic SR of Gaussian microtextures.","Even though our approach is limited regarding the scope of images it encompasses, our algorithm is competitive with deep learning state-of-the-art methods both in terms of perceptual metric and execution time when applied to microtextures.","The framework of Gaussian microtextures also allows us to rigorously discuss the limitations of various reconstruction metrics to evaluate the efficiency of SR routines."],"url":"http://arxiv.org/abs/2405.15399v1","category":"eess.IV"}
{"created":"2024-05-24 09:52:00","title":"PriCE: Privacy-Preserving and Cost-Effective Scheduling for Parallelizing the Large Medical Image Processing Workflow over Hybrid Clouds","abstract":"Running deep neural networks for large medical images is a resource-hungry and time-consuming task with centralized computing. Outsourcing such medical image processing tasks to hybrid clouds has benefits, such as a significant reduction of execution time and monetary cost. However, due to privacy concerns, it is still challenging to process sensitive medical images over clouds, which would hinder their deployment in many real-world applications. To overcome this, we first formulate the overall optimization objectives of the privacy-preserving distributed system model, i.e., minimizing the amount of information about the private data learned by the adversaries throughout the process, reducing the maximum execution time and cost under the user budget constraint. We propose a novel privacy-preserving and cost-effective method called PriCE to solve this multi-objective optimization problem. We performed extensive simulation experiments for artifact detection tasks on medical images using an ensemble of five deep convolutional neural network inferences as the workflow task. Experimental results show that PriCE successfully splits a wide range of input gigapixel medical images with graph-coloring-based strategies, yielding desired output utility and lowering the privacy risk, makespan, and monetary cost under user's budget.","sentences":["Running deep neural networks for large medical images is a resource-hungry and time-consuming task with centralized computing.","Outsourcing such medical image processing tasks to hybrid clouds has benefits, such as a significant reduction of execution time and monetary cost.","However, due to privacy concerns, it is still challenging to process sensitive medical images over clouds, which would hinder their deployment in many real-world applications.","To overcome this, we first formulate the overall optimization objectives of the privacy-preserving distributed system model, i.e., minimizing the amount of information about the private data learned by the adversaries throughout the process, reducing the maximum execution time and cost under the user budget constraint.","We propose a novel privacy-preserving and cost-effective method called PriCE to solve this multi-objective optimization problem.","We performed extensive simulation experiments for artifact detection tasks on medical images using an ensemble of five deep convolutional neural network inferences as the workflow task.","Experimental results show that PriCE successfully splits a wide range of input gigapixel medical images with graph-coloring-based strategies, yielding desired output utility and lowering the privacy risk, makespan, and monetary cost under user's budget."],"url":"http://arxiv.org/abs/2405.15398v1","category":"cs.CE"}
{"created":"2024-05-24 09:49:52","title":"Fieldscale: Locality-Aware Field-based Adaptive Rescaling for Thermal Infrared Image","abstract":"Thermal infrared (TIR) cameras are emerging as promising sensors in safety-related fields due to their robustness against external illumination. However, RAW TIR image has 14 bits of pixel depth and needs to be rescaled into 8 bits for general applications. Previous works utilize a global 1D look-up table to compute pixel-wise gain solely based on its intensity, which degrades image quality by failing to consider the local nature of the heat. We propose Fieldscale, a rescaling based on locality-aware 2D fields where both the intensity value and spatial context of each pixel within an image are embedded. It can adaptively determine the pixel gain for each region and produce spatially consistent 8-bit rescaled images with minimal information loss and high visibility. Consistent performance improvement on image quality assessment and two other downstream tasks support the effectiveness and usability of Fieldscale. All the codes are publicly opened to facilitate research advancements in this field. https://github.com/hyeonjaegil/fieldscale","sentences":["Thermal infrared (TIR) cameras are emerging as promising sensors in safety-related fields due to their robustness against external illumination.","However, RAW TIR image has 14 bits of pixel depth and needs to be rescaled into 8 bits for general applications.","Previous works utilize a global 1D look-up table to compute pixel-wise gain solely based on its intensity, which degrades image quality by failing to consider the local nature of the heat.","We propose Fieldscale, a rescaling based on locality-aware 2D fields where both the intensity value and spatial context of each pixel within an image are embedded.","It can adaptively determine the pixel gain for each region and produce spatially consistent 8-bit rescaled images with minimal information loss and high visibility.","Consistent performance improvement on image quality assessment and two other downstream tasks support the effectiveness and usability of Fieldscale.","All the codes are publicly opened to facilitate research advancements in this field.","https://github.com/hyeonjaegil/fieldscale"],"url":"http://arxiv.org/abs/2405.15395v1","category":"cs.CV"}
{"created":"2024-05-24 09:48:18","title":"Reshuffling Resampling Splits Can Improve Generalization of Hyperparameter Optimization","abstract":"Hyperparameter optimization is crucial for obtaining peak performance of machine learning models. The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide optimization and select a final hyperparameter configuration. Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross-validation scheme, are often recommended. We show that, surprisingly, reshuffling the splits for every configuration often improves the final model's generalization performance on unseen data. Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime. This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem. We confirm our theoretical results in a controlled simulation study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment. While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper.","sentences":["Hyperparameter optimization is crucial for obtaining peak performance of machine learning models.","The standard protocol evaluates various hyperparameter configurations using a resampling estimate of the generalization error to guide optimization and select a final hyperparameter configuration.","Without much evidence, paired resampling splits, i.e., either a fixed train-validation split or a fixed cross-validation scheme, are often recommended.","We show that, surprisingly, reshuffling the splits for every configuration often improves the final model's generalization performance on unseen data.","Our theoretical analysis explains how reshuffling affects the asymptotic behavior of the validation loss surface and provides a bound on the expected regret in the limiting regime.","This bound connects the potential benefits of reshuffling to the signal and noise characteristics of the underlying optimization problem.","We confirm our theoretical results in a controlled simulation study and demonstrate the practical usefulness of reshuffling in a large-scale, realistic hyperparameter optimization experiment.","While reshuffling leads to test performances that are competitive with using fixed splits, it drastically improves results for a single train-validation holdout protocol and can often make holdout become competitive with standard CV while being computationally cheaper."],"url":"http://arxiv.org/abs/2405.15393v1","category":"stat.ML"}
{"created":"2024-05-24 09:43:49","title":"Representation theory of the group of automorphisms of a finite rooted tree","abstract":"We show that the ordinary irreducible representations of the group of automorphisms of a finite, rooted tree may be obtained following one of the classical method used for the irreducible representations of the symmetric group, namely the approach in Chapter 2 of the classical book by James and Kerber. To reach this goal, we introduce and study the combinatorics of tree compositions, a natural generalization of set compositions but with new features and more complexity due to the tree structure. This leads to a natural parametrization of the conjugacy classes and of the irreducible representations by means of the same combinatorial objects. Our trees are not necessarily spherically homogeneous and our approach is coordinate free.","sentences":["We show that the ordinary irreducible representations of the group of automorphisms of a finite, rooted tree may be obtained following one of the classical method used for the irreducible representations of the symmetric group, namely the approach in Chapter 2 of the classical book by James and Kerber.","To reach this goal, we introduce and study the combinatorics of tree compositions, a natural generalization of set compositions but with new features and more complexity due to the tree structure.","This leads to a natural parametrization of the conjugacy classes and of the irreducible representations by means of the same combinatorial objects.","Our trees are not necessarily spherically homogeneous and our approach is coordinate free."],"url":"http://arxiv.org/abs/2405.15391v1","category":"math.RT"}
{"created":"2024-05-24 09:38:36","title":"Language-Driven Interactive Traffic Trajectory Generation","abstract":"Realistic trajectory generation with natural language control is pivotal for advancing autonomous vehicle technology. However, previous methods focus on individual traffic participant trajectory generation, thus failing to account for the complexity of interactive traffic dynamics. In this work, we propose InteractTraj, the first language-driven traffic trajectory generator that can generate interactive traffic trajectories. InteractTraj interprets abstract trajectory descriptions into concrete formatted interaction-aware numerical codes and learns a mapping between these formatted codes and the final interactive trajectories. To interpret language descriptions, we propose a language-to-code encoder with a novel interaction-aware encoding strategy. To produce interactive traffic trajectories, we propose a code-to-trajectory decoder with interaction-aware feature aggregation that synergizes vehicle interactions with the environmental map and the vehicle moves. Extensive experiments show our method demonstrates superior performance over previous SoTA methods, offering a more realistic generation of interactive traffic trajectories with high controllability via diverse natural language commands. Our code is available at https://github.com/X1a-jk/InteractTraj.git","sentences":["Realistic trajectory generation with natural language control is pivotal for advancing autonomous vehicle technology.","However, previous methods focus on individual traffic participant trajectory generation, thus failing to account for the complexity of interactive traffic dynamics.","In this work, we propose InteractTraj, the first language-driven traffic trajectory generator that can generate interactive traffic trajectories.","InteractTraj interprets abstract trajectory descriptions into concrete formatted interaction-aware numerical codes and learns a mapping between these formatted codes and the final interactive trajectories.","To interpret language descriptions, we propose a language-to-code encoder with a novel interaction-aware encoding strategy.","To produce interactive traffic trajectories, we propose a code-to-trajectory decoder with interaction-aware feature aggregation that synergizes vehicle interactions with the environmental map and the vehicle moves.","Extensive experiments show our method demonstrates superior performance over previous SoTA methods, offering a more realistic generation of interactive traffic trajectories with high controllability via diverse natural language commands.","Our code is available at https://github.com/X1a-jk/InteractTraj.git"],"url":"http://arxiv.org/abs/2405.15388v1","category":"cs.AI"}
{"created":"2024-05-24 09:36:03","title":"Exploring Baryon Resonances with Transition Generalized Parton Distributions: Status and Perspectives","abstract":"QCD gives rise to a rich spectrum of excited baryon states. Understanding their internal structure is important for many areas of nuclear physics, such as nuclear forces, dense matter, and neutrino-nucleus interactions. Generalized parton distributions (GPDs) are an established tool for characterizing the QCD structure of the ground-state nucleon. They are used to create 3D tomographic images of the quark/gluon structure and quantify the mechanical properties such as the distribution of mass, angular momentum and forces in the system. Transition GPDs extend these concepts to $N \\rightarrow N^\\ast$ transitions and can be used to characterize the 3D structure and mechanical properties of baryon resonances. They can be probed in high-momentum-transfer exclusive electroproduction processes with resonance transitions $e + N \\rightarrow e' + M + N^\\ast$, such as deeply-virtual Compton scattering ($M = \\gamma$) or meson production ($M = \\pi, K$, $etc.$), and in related photon/hadron-induced processes. This White Paper describes a research program aiming to explore baryon resonance structure with transition GPDs. This includes the properties and interpretation of the transition GPDs, theoretical methods for structures and processes, first experimental results from JLab 12 GeV, future measurements with existing and planned facilities (JLab detector and energy upgrades, COMPASS/AMBER, EIC, EicC, J-PARC, LHC ultraperihperal collisions), and the theoretical and experimental developments needed to realize this program.","sentences":["QCD gives rise to a rich spectrum of excited baryon states.","Understanding their internal structure is important for many areas of nuclear physics, such as nuclear forces, dense matter, and neutrino-nucleus interactions.","Generalized parton distributions (GPDs) are an established tool for characterizing the QCD structure of the ground-state nucleon.","They are used to create 3D tomographic images of the quark/gluon structure and quantify the mechanical properties such as the distribution of mass, angular momentum and forces in the system.","Transition GPDs extend these concepts to $N \\rightarrow N^\\ast$ transitions and can be used to characterize the 3D structure and mechanical properties of baryon resonances.","They can be probed in high-momentum-transfer exclusive electroproduction processes with resonance transitions $e + N \\rightarrow e' + M + N^\\ast$, such as deeply-virtual Compton scattering ($M = \\gamma$) or meson production ($M = \\pi, K$, $etc.$), and in related photon/hadron-induced processes.","This White Paper describes a research program aiming to explore baryon resonance structure with transition GPDs.","This includes the properties and interpretation of the transition GPDs, theoretical methods for structures and processes, first experimental results from JLab 12 GeV, future measurements with existing and planned facilities (JLab detector and energy upgrades, COMPASS/AMBER, EIC, EicC, J-PARC, LHC ultraperihperal collisions), and the theoretical and experimental developments needed to realize this program."],"url":"http://arxiv.org/abs/2405.15386v1","category":"hep-ph"}
{"created":"2024-05-24 09:35:42","title":"CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D Medical Image Interpolation","abstract":"Motion information from 4D medical imaging offers critical insights into dynamic changes in patient anatomy for clinical assessments and radiotherapy planning and, thereby, enhances the capabilities of 3D image analysis. However, inherent physical and technical constraints of imaging hardware often necessitate a compromise between temporal resolution and image quality. Frame interpolation emerges as a pivotal solution to this challenge. Previous methods often suffer from discretion when they estimate the intermediate motion and execute the forward warping. In this study, we draw inspiration from fluid mechanics to propose a novel approach for continuously modeling patient anatomic motion using implicit neural representation. It ensures both spatial and temporal continuity, effectively bridging Eulerian and Lagrangian specifications together to naturally facilitate continuous frame interpolation. Our experiments across multiple datasets underscore the method's superior accuracy and speed. Furthermore, as a case-specific optimization (training-free) approach, it circumvents the need for extensive datasets and addresses model generalization issues.","sentences":["Motion information from 4D medical imaging offers critical insights into dynamic changes in patient anatomy for clinical assessments and radiotherapy planning and, thereby, enhances the capabilities of 3D image analysis.","However, inherent physical and technical constraints of imaging hardware often necessitate a compromise between temporal resolution and image quality.","Frame interpolation emerges as a pivotal solution to this challenge.","Previous methods often suffer from discretion when they estimate the intermediate motion and execute the forward warping.","In this study, we draw inspiration from fluid mechanics to propose a novel approach for continuously modeling patient anatomic motion using implicit neural representation.","It ensures both spatial and temporal continuity, effectively bridging Eulerian and Lagrangian specifications together to naturally facilitate continuous frame interpolation.","Our experiments across multiple datasets underscore the method's superior accuracy and speed.","Furthermore, as a case-specific optimization (training-free) approach, it circumvents the need for extensive datasets and addresses model generalization issues."],"url":"http://arxiv.org/abs/2405.15385v1","category":"cs.CV"}
{"created":"2024-05-24 09:31:26","title":"Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search","abstract":"In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.","sentences":["In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL).","Calling code instead of LLMs for planning has the advantages of being precise, reliable, interpretable, and extremely efficient.","However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories.","To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs.","To test our approach, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories.","GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed."],"url":"http://arxiv.org/abs/2405.15383v1","category":"cs.AI"}
{"created":"2024-05-24 09:24:46","title":"Full-stack evaluation of Machine Learning inference workloads for RISC-V systems","abstract":"Architectural simulators hold a vital role in RISC-V research, providing a crucial platform for workload evaluation without the need for costly physical prototypes. They serve as a dynamic environment for exploring innovative architectural concepts, enabling swift iteration and thorough analysis of performance metrics. As deep learning algorithms become increasingly pervasive, it is essential to benchmark new architectures with machine learning workloads. The diverse computational kernels used in deep learning algorithms highlight the necessity for a comprehensive compilation toolchain to map to target hardware platforms. This study evaluates the performance of a wide array of machine learning workloads on RISC-V architectures using gem5, an open-source architectural simulator. Leveraging an open-source compilation toolchain based on Multi-Level Intermediate Representation (MLIR), the research presents benchmarking results specifically focused on deep learning inference workloads. Additionally, the study sheds light on current limitations of gem5 when simulating RISC-V architectures, offering insights for future development and refinement.","sentences":["Architectural simulators hold a vital role in RISC-V research, providing a crucial platform for workload evaluation without the need for costly physical prototypes.","They serve as a dynamic environment for exploring innovative architectural concepts, enabling swift iteration and thorough analysis of performance metrics.","As deep learning algorithms become increasingly pervasive, it is essential to benchmark new architectures with machine learning workloads.","The diverse computational kernels used in deep learning algorithms highlight the necessity for a comprehensive compilation toolchain to map to target hardware platforms.","This study evaluates the performance of a wide array of machine learning workloads on RISC-V architectures using gem5, an open-source architectural simulator.","Leveraging an open-source compilation toolchain based on Multi-Level Intermediate Representation (MLIR), the research presents benchmarking results specifically focused on deep learning inference workloads.","Additionally, the study sheds light on current limitations of gem5 when simulating RISC-V architectures, offering insights for future development and refinement."],"url":"http://arxiv.org/abs/2405.15380v1","category":"cs.AR"}
{"created":"2024-05-24 09:24:21","title":"Log-Concave Sampling on Compact Supports: A Versatile Proximal Framework","abstract":"In this paper, we explore sampling from strongly log-concave distributions defined on convex and compact supports. We propose a general proximal framework that involves projecting onto the constrained set, which is highly flexible and supports various projection options. Specifically, we consider the cases of Euclidean and Gauge projections, with the latter having the advantage of being performed efficiently using a membership oracle. This framework can be seamlessly integrated with multiple sampling methods. Our analysis focuses on Langevin-type sampling algorithms within the context of constrained sampling. We provide nonasymptotic upper bounds on the W1 and W2 errors, offering a detailed comparison of the performance of these methods in constrained sampling.","sentences":["In this paper, we explore sampling from strongly log-concave distributions defined on convex and compact supports.","We propose a general proximal framework that involves projecting onto the constrained set, which is highly flexible and supports various projection options.","Specifically, we consider the cases of Euclidean and Gauge projections, with the latter having the advantage of being performed efficiently using a membership oracle.","This framework can be seamlessly integrated with multiple sampling methods.","Our analysis focuses on Langevin-type sampling algorithms within the context of constrained sampling.","We provide nonasymptotic upper bounds on the W1 and W2 errors, offering a detailed comparison of the performance of these methods in constrained sampling."],"url":"http://arxiv.org/abs/2405.15379v1","category":"stat.ML"}
{"created":"2024-05-24 09:23:57","title":"Dominating surface-group representations via Fock-Goncharov coordinates","abstract":"Let $S$ be a punctured surface of negative Euler characteristic. We show that given a generic representation $\\rho:\\pi_1(S) \\rightarrow \\mathrm{PSL}_n(\\mathbb{C})$, there exists a positive representation $\\rho_0:\\pi_1(S) \\rightarrow \\mathrm{PSL}_n(\\mathbb{R})$ that dominates $\\rho$ in the Hilbert length spectrum as well as in the translation length spectrum, for the translation length in the symmetric space $\\mathbb{X}_n= \\mathrm{PSL}_n(\\mathbb{C})/\\mathrm{PSU}(n)$. Moreover, the $\\rho_0$-lengths of peripheral curves remain unchanged. The dominating representation $\\rho_0$ is explicitly described via Fock-Goncharov coordinates. Our methods are linear-algebraic, and involve weight matrices of weighted planar networks.","sentences":["Let $S$ be a punctured surface of negative Euler characteristic.","We show that given a generic representation $\\rho:\\pi_1(S) \\rightarrow \\mathrm{PSL}_n(\\mathbb{C})$, there exists a positive representation $\\rho_0:\\pi_1(S) \\rightarrow \\mathrm{PSL}_n(\\mathbb{R})$ that dominates $\\rho$ in the Hilbert length spectrum as well as in the translation length spectrum, for the translation length in the symmetric space $\\mathbb{X}_n= \\mathrm{PSL}_n(\\mathbb{C})/\\mathrm{PSU}(n)$.","Moreover, the $\\rho_0$-lengths of peripheral curves remain unchanged.","The dominating representation $\\rho_0$ is explicitly described via Fock-Goncharov coordinates.","Our methods are linear-algebraic, and involve weight matrices of weighted planar networks."],"url":"http://arxiv.org/abs/2405.15378v1","category":"math.GT"}
{"created":"2024-05-24 09:23:51","title":"Dynamic Planning for Sequential Whole-body Mobile Manipulation","abstract":"The dynamic Sequential Mobile Manipulation Planning (SMMP) framework is essential for the safe and robust operation of mobile manipulators in dynamic environments. Previous research has primarily focused on either motion-level or task-level dynamic planning, with limitations in handling state changes that have long-term effects or in generating responsive motions for diverse tasks, respectively. This paper presents a holistic dynamic planning framework that extends the Virtual Kinematic Chain (VKC)-based SMMP method, automating dynamic long-term task planning and reactive whole-body motion generation for SMMP problems. The framework consists of an online task planning module designed to respond to environment changes with long-term effects, a VKC-based whole-body motion planning module for manipulating both rigid and articulated objects, alongside a reactive Model Predictive Control (MPC) module for obstacle avoidance during execution. Simulations and real-world experiments validate the framework, demonstrating its efficacy and validity across sequential mobile manipulation tasks, even in scenarios involving human interference.","sentences":["The dynamic Sequential Mobile Manipulation Planning (SMMP) framework is essential for the safe and robust operation of mobile manipulators in dynamic environments.","Previous research has primarily focused on either motion-level or task-level dynamic planning, with limitations in handling state changes that have long-term effects or in generating responsive motions for diverse tasks, respectively.","This paper presents a holistic dynamic planning framework that extends the Virtual Kinematic Chain (VKC)-based SMMP method, automating dynamic long-term task planning and reactive whole-body motion generation for SMMP problems.","The framework consists of an online task planning module designed to respond to environment changes with long-term effects, a VKC-based whole-body motion planning module for manipulating both rigid and articulated objects, alongside a reactive Model Predictive Control (MPC) module for obstacle avoidance during execution.","Simulations and real-world experiments validate the framework, demonstrating its efficacy and validity across sequential mobile manipulation tasks, even in scenarios involving human interference."],"url":"http://arxiv.org/abs/2405.15377v1","category":"cs.RO"}
{"created":"2024-05-24 09:22:20","title":"A Planet Scale Spatial-Temporal Knowledge Graph Based On OpenStreetMap And H3 Grid","abstract":"Geospatial data plays a central role in modeling our world, for which OpenStreetMap (OSM) provides a rich source of such data. While often spatial data is represented in a tabular format, a graph based representation provides the possibility to interconnect entities which would have been separated in a tabular representation. We propose in our paper a framework which supports a planet scale transformation of OpenStreetMap data into a Spatial Temporal Knowledge Graph. In addition to OpenStreetMap data, we align the different OpenStreetMap geometries on individual h3 grid cells. We compare our constructed spatial knowledge graph to other spatial knowledge graphs and outline our contribution in this paper. As a basis for our computation, we use Apache Sedona as a computational framework for our Spatial Temporal Knowledge Graph construction","sentences":["Geospatial data plays a central role in modeling our world, for which OpenStreetMap (OSM) provides a rich source of such data.","While often spatial data is represented in a tabular format, a graph based representation provides the possibility to interconnect entities which would have been separated in a tabular representation.","We propose in our paper a framework which supports a planet scale transformation of OpenStreetMap data into a Spatial Temporal Knowledge Graph.","In addition to OpenStreetMap data, we align the different OpenStreetMap geometries on individual h3 grid cells.","We compare our constructed spatial knowledge graph to other spatial knowledge graphs and outline our contribution in this paper.","As a basis for our computation, we use Apache Sedona as a computational framework for our Spatial Temporal Knowledge Graph construction"],"url":"http://arxiv.org/abs/2405.15375v1","category":"cs.AI"}
{"created":"2024-05-24 09:19:45","title":"Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph","abstract":"The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science (CS) researchers at the Australian National University (ANU). The system integrates Large Language Models (LLMs) with the ANU Scholarly Knowledge Graph (ASKG), a structured repository of all research-related artifacts produced at ANU in the CS field. Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph (KG).   To address the limitations of traditional scholarly KG construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model (DDM) for comprehensive document representation and the KG-enhanced Query Processing (KGQP) for optimized complex query handling. DDM enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while KGQP leverages the KG structure to improve query accuracy and efficiency with LLMs.   By combining the ASKG with LLMs, our approach enhances knowledge utilization and natural language understanding capabilities. The proposed system employs an automatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from the ASKG. Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency.   We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery. This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with LLMs.","sentences":["The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science (CS) researchers at the Australian National University (ANU).","The system integrates Large Language Models (LLMs) with the ANU Scholarly Knowledge Graph (ASKG), a structured repository of all research-related artifacts produced at ANU in the CS field.","Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph (KG).   ","To address the limitations of traditional scholarly KG construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model (DDM) for comprehensive document representation and the KG-enhanced Query Processing (KGQP) for optimized complex query handling.","DDM enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while KGQP leverages the KG structure to improve query accuracy and efficiency with LLMs.   ","By combining the ASKG with LLMs, our approach enhances knowledge utilization and natural language understanding capabilities.","The proposed system employs an automatic LLM-SPARQL fusion to retrieve relevant facts and textual nodes from the ASKG.","Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency.   ","We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery.","This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with LLMs."],"url":"http://arxiv.org/abs/2405.15374v1","category":"cs.IR"}
{"created":"2024-05-24 09:11:29","title":"Autonomous Quilt Spreading for Caregiving Robots","abstract":"In this work, we propose a novel strategy to ensure infants, who inadvertently displace their quilts during sleep, are promptly and accurately re-covered. Our approach is formulated into two subsequent steps: interference resolution and quilt spreading. By leveraging the DWPose human skeletal detection and the Segment Anything instance segmentation models, the proposed method can accurately recognize the states of the infant and the quilt over her, which involves addressing the interferences resulted from an infant's limbs laid on part of the quilt. Building upon prior research, the EM*D deep learning model is employed to forecast quilt state transitions before and after quilt spreading actions. To improve the sensitivity of the network in distinguishing state variation of the handled quilt, we introduce an enhanced loss function that translates the voxelized quilt state into a more representative one. Both simulation and real-world experiments validate the efficacy of our method, in spreading and recover a quilt over an infant.","sentences":["In this work, we propose a novel strategy to ensure infants, who inadvertently displace their quilts during sleep, are promptly and accurately re-covered.","Our approach is formulated into two subsequent steps: interference resolution and quilt spreading.","By leveraging the DWPose human skeletal detection and the Segment Anything instance segmentation models, the proposed method can accurately recognize the states of the infant and the quilt over her, which involves addressing the interferences resulted from an infant's limbs laid on part of the quilt.","Building upon prior research, the EM*D deep learning model is employed to forecast quilt state transitions before and after quilt spreading actions.","To improve the sensitivity of the network in distinguishing state variation of the handled quilt, we introduce an enhanced loss function that translates the voxelized quilt state into a more representative one.","Both simulation and real-world experiments validate the efficacy of our method, in spreading and recover a quilt over an infant."],"url":"http://arxiv.org/abs/2405.15373v1","category":"cs.RO"}
{"created":"2024-05-24 09:07:10","title":"When far is better: The Chamberlin-Courant approach to obnoxious committee selection","abstract":"Classical work on metric space based committee selection problem interprets distance as ``near is better''. In this work, motivated by real-life situations, we interpret distance as ``far is better''. Formally stated, we initiate the study of ``obnoxious'' committee scoring rules when the voters' preferences are expressed via a metric space. To this end, we propose a model where large distances imply high satisfaction and study the egalitarian avatar of the well-known Chamberlin-Courant voting rule and some of its generalizations. For a given integer value $1 \\le \\lambda \\le k$, the committee size k, a voter derives satisfaction from only the $\\lambda$-th favorite committee member; the goal is to maximize the satisfaction of the least satisfied voter. For the special case of $\\lambda = 1$, this yields the egalitarian Chamberlin-Courant rule. In this paper, we consider general metric space and the special case of a $d$-dimensional Euclidean space.   We show that when $\\lambda$ is $1$ and $k$, the problem is polynomial-time solvable in $\\mathbb{R}^2$ and general metric space, respectively. However, for $\\lambda = k-1$, it is NP-hard even in $\\mathbb{R}^2$. Thus, we have ``double-dichotomy'' in $\\mathbb{R}^2$ with respect to the value of {\\lambda}, where the extreme cases are solvable in polynomial time but an intermediate case is NP-hard. Furthermore, this phenomenon appears to be ``tight'' for $\\mathbb{R}^2$ because the problem is NP-hard for general metric space, even for $\\lambda=1$. Consequently, we are motivated to explore the problem in the realm of (parameterized) approximation algorithms and obtain positive results. Interestingly, we note that this generalization of Chamberlin-Courant rules encodes practical constraints that are relevant to solutions for certain facility locations.","sentences":["Classical work on metric space based committee selection problem interprets distance as ``near is better''.","In this work, motivated by real-life situations, we interpret distance as ``far is better''.","Formally stated, we initiate the study of ``obnoxious'' committee scoring rules when the voters' preferences are expressed via a metric space.","To this end, we propose a model where large distances imply high satisfaction and study the egalitarian avatar of the well-known Chamberlin-Courant voting rule and some of its generalizations.","For a given integer value $1 \\le \\lambda \\le k$, the committee size k, a voter derives satisfaction from only the $\\lambda$-th favorite committee member; the goal is to maximize the satisfaction of the least satisfied voter.","For the special case of $\\lambda = 1$, this yields the egalitarian Chamberlin-Courant rule.","In this paper, we consider general metric space and the special case of a $d$-dimensional Euclidean space.   ","We show that when $\\lambda$ is $1$ and $k$, the problem is polynomial-time solvable in $\\mathbb{R}^2$ and general metric space, respectively.","However, for $\\lambda = k-1$, it is NP-hard even in $\\mathbb{R}^2$. Thus, we have ``double-dichotomy'' in $\\mathbb{R}^2$ with respect to the value of {\\lambda}, where the extreme cases are solvable in polynomial time but an intermediate case is NP-hard.","Furthermore, this phenomenon appears to be ``tight'' for $\\mathbb{R}^2$ because the problem is NP-hard for general metric space, even for $\\lambda=1$. Consequently, we are motivated to explore the problem in the realm of (parameterized) approximation algorithms and obtain positive results.","Interestingly, we note that this generalization of Chamberlin-Courant rules encodes practical constraints that are relevant to solutions for certain facility locations."],"url":"http://arxiv.org/abs/2405.15372v1","category":"cs.DS"}
{"created":"2024-05-24 09:06:12","title":"Cross-Domain Policy Adaptation by Capturing Representation Mismatch","abstract":"It is vital to learn effective policies that can be transferred to different domains with dynamics discrepancies in reinforcement learning (RL). In this paper, we consider dynamics adaptation settings where there exists dynamics mismatch between the source domain and the target domain, and one can get access to sufficient source domain data, while can only have limited interactions with the target domain. Existing methods address this problem by learning domain classifiers, performing data filtering from a value discrepancy perspective, etc. Instead, we tackle this challenge from a decoupled representation learning perspective. We perform representation learning only in the target domain and measure the representation deviations on the transitions from the source domain, which we show can be a signal of dynamics mismatch. We also show that representation deviation upper bounds performance difference of a given policy in the source domain and target domain, which motivates us to adopt representation deviation as a reward penalty. The produced representations are not involved in either policy or value function, but only serve as a reward penalizer. We conduct extensive experiments on environments with kinematic and morphology mismatch, and the results show that our method exhibits strong performance on many tasks. Our code is publicly available at https://github.com/dmksjfl/PAR.","sentences":["It is vital to learn effective policies that can be transferred to different domains with dynamics discrepancies in reinforcement learning (RL).","In this paper, we consider dynamics adaptation settings where there exists dynamics mismatch between the source domain and the target domain, and one can get access to sufficient source domain data, while can only have limited interactions with the target domain.","Existing methods address this problem by learning domain classifiers, performing data filtering from a value discrepancy perspective, etc.","Instead, we tackle this challenge from a decoupled representation learning perspective.","We perform representation learning only in the target domain and measure the representation deviations on the transitions from the source domain, which we show can be a signal of dynamics mismatch.","We also show that representation deviation upper bounds performance difference of a given policy in the source domain and target domain, which motivates us to adopt representation deviation as a reward penalty.","The produced representations are not involved in either policy or value function, but only serve as a reward penalizer.","We conduct extensive experiments on environments with kinematic and morphology mismatch, and the results show that our method exhibits strong performance on many tasks.","Our code is publicly available at https://github.com/dmksjfl/PAR."],"url":"http://arxiv.org/abs/2405.15369v1","category":"cs.LG"}
{"created":"2024-05-24 09:05:49","title":"Complexity of Robust Orbit Problems for Torus Actions and the abc-conjecture","abstract":"When a group acts on a set, it naturally partitions it into orbits, giving rise to orbit problems. These are natural algorithmic problems, as symmetries are central in numerous questions and structures in physics, mathematics, computer science, optimization, and more. Accordingly, it is of high interest to understand their computational complexity. Recently, B\\\"urgisser et al. gave the first polynomial-time algorithms for orbit problems of torus actions, that is, actions of commutative continuous groups on Euclidean space. In this work, motivated by theoretical and practical applications, we study the computational complexity of robust generalizations of these orbit problems, which amount to approximating the distance of orbits in $\\mathbb{C}^n$ up to a factor $\\gamma>1$. In particular, this allows deciding whether two inputs are approximately in the same orbit or far from being so. On the one hand, we prove the NP-hardness of this problem for $\\gamma = n^{\\Omega(1/\\log\\log n)}$ by reducing the closest vector problem for lattices to it. On the other hand, we describe algorithms for solving this problem for an approximation factor $\\gamma = \\exp(\\mathrm{poly}(n))$. Our algorithms combine tools from invariant theory and algorithmic lattice theory, and they also provide group elements witnessing the proximity of the given orbits (in contrast to the algebraic algorithms of prior work). We prove that they run in polynomial time if and only if a version of the famous number-theoretic $abc$-conjecture holds -- establishing a new and surprising connection between computational complexity and number theory.","sentences":["When a group acts on a set, it naturally partitions it into orbits, giving rise to orbit problems.","These are natural algorithmic problems, as symmetries are central in numerous questions and structures in physics, mathematics, computer science, optimization, and more.","Accordingly, it is of high interest to understand their computational complexity.","Recently, B\\\"urgisser et al. gave the first polynomial-time algorithms for orbit problems of torus actions, that is, actions of commutative continuous groups on Euclidean space.","In this work, motivated by theoretical and practical applications, we study the computational complexity of robust generalizations of these orbit problems, which amount to approximating the distance of orbits in $\\mathbb{C}^n$ up to a factor $\\gamma>1$. In particular, this allows deciding whether two inputs are approximately in the same orbit or far from being so.","On the one hand, we prove the NP-hardness of this problem for $\\gamma = n^{\\Omega(1/\\log\\log n)}$ by reducing the closest vector problem for lattices to it.","On the other hand, we describe algorithms for solving this problem for an approximation factor $\\gamma = \\exp(\\mathrm{poly}(n))$. Our algorithms combine tools from invariant theory and algorithmic lattice theory, and they also provide group elements witnessing the proximity of the given orbits (in contrast to the algebraic algorithms of prior work).","We prove that they run in polynomial time if and only if a version of the famous number-theoretic $abc$-conjecture holds -- establishing a new and surprising connection between computational complexity and number theory."],"url":"http://arxiv.org/abs/2405.15368v1","category":"cs.CC"}
{"created":"2024-05-24 08:58:51","title":"Measurement of $\u03b3$-rays generated by neutron interaction with ${}^{16}$O at 30 MeV and 250 MeV","abstract":"Deep understanding of $\\gamma$-ray production from the fast neutron reaction in water is crucial for various physics studies at large-scale water Cherenkov detectors. We performed test experiments using quasi-mono energetic neutron beams ($E_n = 30$ and 250~MeV) at Osaka University's Research Center for Nuclear Physics to measure $\\gamma$-rays originating from the neutron-oxygen reaction with a high-purity germanium detector. Multiple $\\gamma$-ray peaks which are expected to be from excited nuclei after the neutron-oxygen reaction were successfully observed. We measured the neutron beam flux by using a liquid scintillator for the cross section measurement. With a spectral fitting analysis based on the tailored $\\gamma$-ray signal and background templates, we measured cross sections for each observed $\\gamma$-ray component. The results will be useful to validate neutron models employed in the on-going and future water Cherenkov experiments.","sentences":["Deep understanding of $\\gamma$-ray production from the fast neutron reaction in water is crucial for various physics studies at large-scale water Cherenkov detectors.","We performed test experiments using quasi-mono energetic neutron beams ($E_n = 30$ and 250~MeV) at Osaka University's Research Center for Nuclear Physics to measure $\\gamma$-rays originating from the neutron-oxygen reaction with a high-purity germanium detector.","Multiple $\\gamma$-ray peaks which are expected to be from excited nuclei after the neutron-oxygen reaction were successfully observed.","We measured the neutron beam flux by using a liquid scintillator for the cross section measurement.","With a spectral fitting analysis based on the tailored $\\gamma$-ray signal and background templates, we measured cross sections for each observed $\\gamma$-ray component.","The results will be useful to validate neutron models employed in the on-going and future water Cherenkov experiments."],"url":"http://arxiv.org/abs/2405.15366v1","category":"nucl-ex"}
{"created":"2024-05-24 08:58:48","title":"U3M: Unbiased Multiscale Modal Fusion Model for Multimodal Semantic Segmentation","abstract":"Multimodal semantic segmentation is a pivotal component of computer vision and typically surpasses unimodal methods by utilizing rich information set from various sources.Current models frequently adopt modality-specific frameworks that inherently biases toward certain modalities. Although these biases might be advantageous in specific situations, they generally limit the adaptability of the models across different multimodal contexts, thereby potentially impairing performance. To address this issue, we leverage the inherent capabilities of the model itself to discover the optimal equilibrium in multimodal fusion and introduce U3M: An Unbiased Multiscale Modal Fusion Model for Multimodal Semantic Segmentation. Specifically, this method involves an unbiased integration of multimodal visual data. Additionally, we employ feature fusion at multiple scales to ensure the effective extraction and integration of both global and local features. Experimental results demonstrate that our approach achieves superior performance across multiple datasets, verifing its efficacy in enhancing the robustness and versatility of semantic segmentation in diverse settings. Our code is available at U3M-multimodal-semantic-segmentation.","sentences":["Multimodal semantic segmentation is a pivotal component of computer vision and typically surpasses unimodal methods by utilizing rich information set from various sources.","Current models frequently adopt modality-specific frameworks that inherently biases toward certain modalities.","Although these biases might be advantageous in specific situations, they generally limit the adaptability of the models across different multimodal contexts, thereby potentially impairing performance.","To address this issue, we leverage the inherent capabilities of the model itself to discover the optimal equilibrium in multimodal fusion and introduce U3M: An Unbiased Multiscale Modal Fusion Model for Multimodal Semantic Segmentation.","Specifically, this method involves an unbiased integration of multimodal visual data.","Additionally, we employ feature fusion at multiple scales to ensure the effective extraction and integration of both global and local features.","Experimental results demonstrate that our approach achieves superior performance across multiple datasets, verifing its efficacy in enhancing the robustness and versatility of semantic segmentation in diverse settings.","Our code is available at U3M-multimodal-semantic-segmentation."],"url":"http://arxiv.org/abs/2405.15365v1","category":"cs.CV"}
{"created":"2024-05-24 08:56:19","title":"NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer","abstract":"By harnessing the potent generative capabilities of pre-trained large video diffusion models, we propose NVS-Solver, a new novel view synthesis (NVS) paradigm that operates \\textit{without} the need for training. NVS-Solver adaptively modulates the diffusion sampling process with the given views to enable the creation of remarkable visual experiences from single or multiple views of static scenes or monocular videos of dynamic scenes. Specifically, built upon our theoretical modeling, we iteratively modulate the score function with the given scene priors represented with warped input views to control the video diffusion process. Moreover, by theoretically exploring the boundary of the estimation error, we achieve the modulation in an adaptive fashion according to the view pose and the number of diffusion steps. Extensive evaluations on both static and dynamic scenes substantiate the significant superiority of our NVS-Solver over state-of-the-art methods both quantitatively and qualitatively. \\textit{ Source code in } \\href{https://github.com/ZHU-Zhiyu/NVS_Solver}{https://github.com/ZHU-Zhiyu/NVS$\\_$Solver}.","sentences":["By harnessing the potent generative capabilities of pre-trained large video diffusion models, we propose NVS-Solver, a new novel view synthesis (NVS) paradigm that operates \\textit{without} the need for training.","NVS-Solver adaptively modulates the diffusion sampling process with the given views to enable the creation of remarkable visual experiences from single or multiple views of static scenes or monocular videos of dynamic scenes.","Specifically, built upon our theoretical modeling, we iteratively modulate the score function with the given scene priors represented with warped input views to control the video diffusion process.","Moreover, by theoretically exploring the boundary of the estimation error, we achieve the modulation in an adaptive fashion according to the view pose and the number of diffusion steps.","Extensive evaluations on both static and dynamic scenes substantiate the significant superiority of our NVS-Solver over state-of-the-art methods both quantitatively and qualitatively.","\\textit{ Source code in } \\href{https://github.com/ZHU-Zhiyu/NVS_Solver}{https://github.com/ZHU-Zhiyu/NVS$\\_$Solver}."],"url":"http://arxiv.org/abs/2405.15364v1","category":"cs.CV"}
{"created":"2024-05-24 08:52:22","title":"Multi-qubit quantum state preparation enabled by topology optimization","abstract":"Using topology optimization, we inverse-design nanophotonic cavities enabling the preparation of pure states of pairs and triples of quantum emitters. Our devices involve moderate values of the dielectric constant, operate under continuous laser driving, and yield fidelities to the target (Bell and W) states approaching unity for distant qubits (several natural wavelengths apart). In the fidelity optimization procedure, our algorithm generates entanglement by maximizing the dissipative coupling between the emitters, which allows the formation of multipartite pure steady states in the driven-dissipative dynamics of the system. Our findings open the way towards the efficient and fast preparation of multiqubit quantum states with engineered features, with potential applications for nonclassical light generation, quantum simulation, and quantum sensing.","sentences":["Using topology optimization, we inverse-design nanophotonic cavities enabling the preparation of pure states of pairs and triples of quantum emitters.","Our devices involve moderate values of the dielectric constant, operate under continuous laser driving, and yield fidelities to the target (Bell and W) states approaching unity for distant qubits (several natural wavelengths apart).","In the fidelity optimization procedure, our algorithm generates entanglement by maximizing the dissipative coupling between the emitters, which allows the formation of multipartite pure steady states in the driven-dissipative dynamics of the system.","Our findings open the way towards the efficient and fast preparation of multiqubit quantum states with engineered features, with potential applications for nonclassical light generation, quantum simulation, and quantum sensing."],"url":"http://arxiv.org/abs/2405.15361v1","category":"quant-ph"}
{"created":"2024-05-24 08:51:22","title":"Adaptive probabilistic forecasting of French electricity spot prices","abstract":"Electricity price forecasting (EPF) plays a major role for electricity companies as a fundamental entry for trading decisions or energy management operations. As electricity can not be stored, electricity prices are highly volatile which make EPF a particularly difficult task. This is all the more true when dramatic fortuitous events disrupt the markets. Trading and more generally energy management decisions require risk management tools which are based on probabilistic EPF (PEPF). In this challenging context, we argue in favor of the deployment of highly adaptive black-boxes strategies allowing to turn any forecasts into a robust adaptive predictive interval, such as conformal prediction and online aggregation, as a fundamental last layer of any operational pipeline.   We propose to investigate a novel data set containing the French electricity spot prices during the turbulent 2020-2021 years, and build a new explanatory feature revealing high predictive power, namely the nuclear availability. Benchmarking state-of-the-art PEPF on this data set highlights the difficulty of choosing a given model, as they all behave very differently in practice, and none of them is reliable. However, we propose an adequate conformalisation, OSSCP-horizon, that improves the performances of PEPF methods, even in the most hazardous period of late 2021. Finally, we emphasize that combining it with online aggregation significantly outperforms any other approaches, and should be the preferred pipeline, as it provides trustworthy probabilistic forecasts.","sentences":["Electricity price forecasting (EPF) plays a major role for electricity companies as a fundamental entry for trading decisions or energy management operations.","As electricity can not be stored, electricity prices are highly volatile which make EPF a particularly difficult task.","This is all the more true when dramatic fortuitous events disrupt the markets.","Trading and more generally energy management decisions require risk management tools which are based on probabilistic EPF (PEPF).","In this challenging context, we argue in favor of the deployment of highly adaptive black-boxes strategies allowing to turn any forecasts into a robust adaptive predictive interval, such as conformal prediction and online aggregation, as a fundamental last layer of any operational pipeline.   ","We propose to investigate a novel data set containing the French electricity spot prices during the turbulent 2020-2021 years, and build a new explanatory feature revealing high predictive power, namely the nuclear availability.","Benchmarking state-of-the-art PEPF on this data set highlights the difficulty of choosing a given model, as they all behave very differently in practice, and none of them is reliable.","However, we propose an adequate conformalisation, OSSCP-horizon, that improves the performances of PEPF methods, even in the most hazardous period of late 2021.","Finally, we emphasize that combining it with online aggregation significantly outperforms any other approaches, and should be the preferred pipeline, as it provides trustworthy probabilistic forecasts."],"url":"http://arxiv.org/abs/2405.15359v1","category":"stat.AP"}
{"created":"2024-05-24 08:49:43","title":"Coordinated Multi-Neighborhood Learning on a Directed Acyclic Graph","abstract":"Learning the structure of causal directed acyclic graphs (DAGs) is useful in many areas of machine learning and artificial intelligence, with wide applications. However, in the high-dimensional setting, it is challenging to obtain good empirical and theoretical results without strong and often restrictive assumptions. Additionally, it is questionable whether all of the variables purported to be included in the network are observable. It is of interest then to restrict consideration to a subset of the variables for relevant and reliable inferences. In fact, researchers in various disciplines can usually select a set of target nodes in the network for causal discovery. This paper develops a new constraint-based method for estimating the local structure around multiple user-specified target nodes, enabling coordination in structure learning between neighborhoods. Our method facilitates causal discovery without learning the entire DAG structure. We establish consistency results for our algorithm with respect to the local neighborhood structure of the target nodes in the true graph. Experimental results on synthetic and real-world data show that our algorithm is more accurate in learning the neighborhood structures with much less computational cost than standard methods that estimate the entire DAG. An R package implementing our methods may be accessed at https://github.com/stephenvsmith/CML.","sentences":["Learning the structure of causal directed acyclic graphs (DAGs) is useful in many areas of machine learning and artificial intelligence, with wide applications.","However, in the high-dimensional setting, it is challenging to obtain good empirical and theoretical results without strong and often restrictive assumptions.","Additionally, it is questionable whether all of the variables purported to be included in the network are observable.","It is of interest then to restrict consideration to a subset of the variables for relevant and reliable inferences.","In fact, researchers in various disciplines can usually select a set of target nodes in the network for causal discovery.","This paper develops a new constraint-based method for estimating the local structure around multiple user-specified target nodes, enabling coordination in structure learning between neighborhoods.","Our method facilitates causal discovery without learning the entire DAG structure.","We establish consistency results for our algorithm with respect to the local neighborhood structure of the target nodes in the true graph.","Experimental results on synthetic and real-world data show that our algorithm is more accurate in learning the neighborhood structures with much less computational cost than standard methods that estimate the entire DAG.","An R package implementing our methods may be accessed at https://github.com/stephenvsmith/CML."],"url":"http://arxiv.org/abs/2405.15358v1","category":"stat.ML"}
{"created":"2024-05-24 08:46:31","title":"Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization","abstract":"Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.","sentences":["Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images.","Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones.","However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations.","To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding.","Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO).","This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs.","Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks."],"url":"http://arxiv.org/abs/2405.15356v1","category":"cs.CV"}
{"created":"2024-05-24 08:42:46","title":"Coloring invariants for links in $\u03a3_g\\times S^1$","abstract":"Let $\\Sigma_g$ be a closed oriented surface of genus $g$, in this paper we discuss how to define coloring invariants and its generalizations for links in $\\Sigma_g\\times S^1$.","sentences":["Let $\\Sigma_g$ be a closed oriented surface of genus $g$, in this paper we discuss how to define coloring invariants and its generalizations for links in $\\Sigma_g\\times S^1$."],"url":"http://arxiv.org/abs/2405.15350v1","category":"math.GT"}
{"created":"2024-05-24 08:42:40","title":"UnKE: Unstructured Knowledge Editing in Large Language Models","abstract":"Recent knowledge editing methods have primarily focused on modifying structured knowledge in large language models, heavily relying on the assumption that structured knowledge is stored as key-value pairs locally in MLP layers or specific neurons. However, this task setting overlooks the fact that a significant portion of real-world knowledge is stored in an unstructured format, characterized by long-form content, noise, and a complex yet comprehensive nature. The \"knowledge locating\" and \"term-driven optimization\" techniques conducted from the assumption used in previous methods (e.g., MEMIT) are ill-suited for unstructured knowledge. To address these challenges, we propose a novel unstructured knowledge editing method, namely UnKE, which extends previous assumptions in the layer dimension and token dimension. Firstly, in the layer dimension, we discard the \"knowledge locating\" step and treat first few layers as the key, which expand knowledge storage through layers to break the \"knowledge stored locally\" assumption. Next, we replace \"term-driven optimization\" with \"cause-driven optimization\" across all inputted tokens in the token dimension, directly optimizing the last layer of the key generator to perform editing to generate the required key vectors. By utilizing key-value pairs at the layer level, UnKE effectively represents and edits complex and comprehensive unstructured knowledge, leveraging the potential of both the MLP and attention layers. Results on newly proposed unstructure knowledge editing dataset (UnKEBench) and traditional structured datasets demonstrate that UnKE achieves remarkable performance, surpassing strong baselines.","sentences":["Recent knowledge editing methods have primarily focused on modifying structured knowledge in large language models, heavily relying on the assumption that structured knowledge is stored as key-value pairs locally in MLP layers or specific neurons.","However, this task setting overlooks the fact that a significant portion of real-world knowledge is stored in an unstructured format, characterized by long-form content, noise, and a complex yet comprehensive nature.","The \"knowledge locating\" and \"term-driven optimization\" techniques conducted from the assumption used in previous methods (e.g., MEMIT) are ill-suited for unstructured knowledge.","To address these challenges, we propose a novel unstructured knowledge editing method, namely UnKE, which extends previous assumptions in the layer dimension and token dimension.","Firstly, in the layer dimension, we discard the \"knowledge locating\" step and treat first few layers as the key, which expand knowledge storage through layers to break the \"knowledge stored locally\" assumption.","Next, we replace \"term-driven optimization\" with \"cause-driven optimization\" across all inputted tokens in the token dimension, directly optimizing the last layer of the key generator to perform editing to generate the required key vectors.","By utilizing key-value pairs at the layer level, UnKE effectively represents and edits complex and comprehensive unstructured knowledge, leveraging the potential of both the MLP and attention layers.","Results on newly proposed unstructure knowledge editing dataset (UnKEBench) and traditional structured datasets demonstrate that UnKE achieves remarkable performance, surpassing strong baselines."],"url":"http://arxiv.org/abs/2405.15349v1","category":"cs.CL"}
{"created":"2024-05-24 08:39:27","title":"BiSup: Bidirectional Quantization Error Suppression for Large Language Models","abstract":"As the size and context length of Large Language Models (LLMs) grow, weight-activation quantization has emerged as a crucial technique for efficient deployment of LLMs. Compared to weight-only quantization, weight-activation quantization presents greater challenges due to the presence of outliers in activations. Existing methods have made significant progress by exploring mixed-precision quantization and outlier suppression. However, these methods primarily focus on optimizing the results of single matrix multiplication, neglecting the bidirectional propagation of quantization errors in LLMs. Specifically, errors accumulate vertically within the same token through layers, and diffuse horizontally across different tokens due to self-attention mechanisms. To address this issue, we introduce BiSup, a Bidirectional quantization error Suppression method. By constructing appropriate optimizable parameter spaces, BiSup utilizes a small amount of data for quantization-aware parameter-efficient fine-tuning to suppress the error vertical accumulation. Besides, BiSup employs prompt mixed-precision quantization strategy, which preserves high precision for the key-value cache of system prompts, to mitigate the error horizontal diffusion. Extensive experiments on Llama and Qwen families demonstrate that BiSup can improve performance over two state-of-the-art methods (the average WikiText2 perplexity decreases from 13.26 to 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128 configuration), further facilitating the practical applications of low-bit weight-activation quantization.","sentences":["As the size and context length of Large Language Models (LLMs) grow, weight-activation quantization has emerged as a crucial technique for efficient deployment of LLMs.","Compared to weight-only quantization, weight-activation quantization presents greater challenges due to the presence of outliers in activations.","Existing methods have made significant progress by exploring mixed-precision quantization and outlier suppression.","However, these methods primarily focus on optimizing the results of single matrix multiplication, neglecting the bidirectional propagation of quantization errors in LLMs.","Specifically, errors accumulate vertically within the same token through layers, and diffuse horizontally across different tokens due to self-attention mechanisms.","To address this issue, we introduce BiSup, a Bidirectional quantization error Suppression method.","By constructing appropriate optimizable parameter spaces, BiSup utilizes a small amount of data for quantization-aware parameter-efficient fine-tuning to suppress the error vertical accumulation.","Besides, BiSup employs prompt mixed-precision quantization strategy, which preserves high precision for the key-value cache of system prompts, to mitigate the error horizontal diffusion.","Extensive experiments on Llama and Qwen families demonstrate that BiSup can improve performance over two state-of-the-art methods (the average WikiText2 perplexity decreases from 13.26 to 9.41 for Atom and from 14.33 to 7.85 for QuaRot under the W3A3-g128 configuration), further facilitating the practical applications of low-bit weight-activation quantization."],"url":"http://arxiv.org/abs/2405.15346v1","category":"cs.CL"}
{"created":"2024-05-24 08:34:40","title":"Hybrid-Field Channel Estimation for XL-MIMO Systems with Stochastic Gradient Pursuit Algorithm","abstract":"Extremely large-scale multiple-input multiple-output (XL-MIMO) is crucial for satisfying the high data rate requirements of the sixth-generation (6G) wireless networks. In this context, ensuring accurate acquisition of channel state information (CSI) with low complexity becomes imperative. Moreover, deploying an extremely large antenna array at the base station (BS) might result in some scatterers being located in near-field, while others are situated in far-field, leading to a hybrid-field communication scenario. To address these challenges, this paper introduces two stochastic gradient pursuit (SGP)-based schemes for the hybrid-field channel estimation in two scenarios. For the first scenario in which the prior knowledge of the specific proportion of the number of near-field and far-field channel paths is known, the scheme can effectively leverage the angular-domain sparsity of the far-field channels and the polar-domain sparsity of the near-field channels such that the channel estimation in these two fields can be performed separately. For the second scenario which the proportion is not available, we propose an off-grid SGP-based channel estimation scheme, which iterates through the values of the proportion parameter based on a criterion before performing the hybrid-field channel estimation. We demonstrate numerically that both of the proposed channel estimation schemes achieve superior performance in terms of both estimation accuracy and achievable rates while enjoying lower computational complexity compared with existing schemes. Additionally, we reveal that as the number of antennas at the UE increases, the normalized mean square error (NMSE) performances of the proposed schemes remain basically unchanged, while the NMSE performances of existing ones improve. Remarkably, even in this scenario, the proposed schemes continue to outperform the existing ones.","sentences":["Extremely large-scale multiple-input multiple-output (XL-MIMO) is crucial for satisfying the high data rate requirements of the sixth-generation (6G) wireless networks.","In this context, ensuring accurate acquisition of channel state information (CSI) with low complexity becomes imperative.","Moreover, deploying an extremely large antenna array at the base station (BS) might result in some scatterers being located in near-field, while others are situated in far-field, leading to a hybrid-field communication scenario.","To address these challenges, this paper introduces two stochastic gradient pursuit (SGP)-based schemes for the hybrid-field channel estimation in two scenarios.","For the first scenario in which the prior knowledge of the specific proportion of the number of near-field and far-field channel paths is known, the scheme can effectively leverage the angular-domain sparsity of the far-field channels and the polar-domain sparsity of the near-field channels such that the channel estimation in these two fields can be performed separately.","For the second scenario which the proportion is not available, we propose an off-grid SGP-based channel estimation scheme, which iterates through the values of the proportion parameter based on a criterion before performing the hybrid-field channel estimation.","We demonstrate numerically that both of the proposed channel estimation schemes achieve superior performance in terms of both estimation accuracy and achievable rates while enjoying lower computational complexity compared with existing schemes.","Additionally, we reveal that as the number of antennas at the UE increases, the normalized mean square error (NMSE) performances of the proposed schemes remain basically unchanged, while the NMSE performances of existing ones improve.","Remarkably, even in this scenario, the proposed schemes continue to outperform the existing ones."],"url":"http://arxiv.org/abs/2405.15345v1","category":"eess.SP"}
{"created":"2024-05-24 08:26:04","title":"Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features","abstract":"The development of AI-Generated Content (AIGC) has empowered the creation of remarkably realistic AI-generated videos, such as those involving Sora. However, the widespread adoption of these models raises concerns regarding potential misuse, including face video scams and copyright disputes. Addressing these concerns requires the development of robust tools capable of accurately determining video authenticity. The main challenges lie in the dataset and neural classifier for training. Current datasets lack a varied and comprehensive repository of real and generated content for effective discrimination. In this paper, we first introduce an extensive video dataset designed specifically for AI-Generated Video Detection (GenVidDet). It includes over 2.66 M instances of both real and generated videos, varying in categories, frames per second, resolutions, and lengths. The comprehensiveness of GenVidDet enables the training of a generalizable video detector. We also present the Dual-Branch 3D Transformer (DuB3D), an innovative and effective method for distinguishing between real and generated videos, enhanced by incorporating motion information alongside visual appearance. DuB3D utilizes a dual-branch architecture that adaptively leverages and fuses raw spatio-temporal data and optical flow. We systematically explore the critical factors affecting detection performance, achieving the optimal configuration for DuB3D. Trained on GenVidDet, DuB3D can distinguish between real and generated video content with 96.77% accuracy, and strong generalization capability even for unseen types.","sentences":["The development of AI-Generated Content (AIGC) has empowered the creation of remarkably realistic AI-generated videos, such as those involving Sora.","However, the widespread adoption of these models raises concerns regarding potential misuse, including face video scams and copyright disputes.","Addressing these concerns requires the development of robust tools capable of accurately determining video authenticity.","The main challenges lie in the dataset and neural classifier for training.","Current datasets lack a varied and comprehensive repository of real and generated content for effective discrimination.","In this paper, we first introduce an extensive video dataset designed specifically for AI-Generated Video Detection (GenVidDet).","It includes over 2.66 M instances of both real and generated videos, varying in categories, frames per second, resolutions, and lengths.","The comprehensiveness of GenVidDet enables the training of a generalizable video detector.","We also present the Dual-Branch 3D Transformer (DuB3D), an innovative and effective method for distinguishing between real and generated videos, enhanced by incorporating motion information alongside visual appearance.","DuB3D utilizes a dual-branch architecture that adaptively leverages and fuses raw spatio-temporal data and optical flow.","We systematically explore the critical factors affecting detection performance, achieving the optimal configuration for DuB3D. Trained on GenVidDet, DuB3D can distinguish between real and generated video content with 96.77% accuracy, and strong generalization capability even for unseen types."],"url":"http://arxiv.org/abs/2405.15343v1","category":"cs.CV"}
{"created":"2024-05-24 08:21:45","title":"V-Zen: Efficient GUI Understanding and Precise Grounding With A Novel Multimodal LLM","abstract":"In the rapidly evolving landscape of AI research and application, Multimodal Large Language Models (MLLMs) have emerged as a transformative force, adept at interpreting and integrating information from diverse modalities such as text, images, and Graphical User Interfaces (GUIs). Despite these advancements, the nuanced interaction and understanding of GUIs pose a significant challenge, limiting the potential of existing models to enhance automation levels. To bridge this gap, this paper presents V-Zen, an innovative Multimodal Large Language Model (MLLM) meticulously crafted to revolutionise the domain of GUI understanding and grounding. Equipped with dual-resolution image encoders, V-Zen establishes new benchmarks in efficient grounding and next-action prediction, thereby laying the groundwork for self-operating computer systems. Complementing V-Zen is the GUIDE dataset, an extensive collection of real-world GUI elements and task-based sequences, serving as a catalyst for specialised fine-tuning. The successful integration of V-Zen and GUIDE marks the dawn of a new era in multimodal AI research, opening the door to intelligent, autonomous computing experiences. This paper extends an invitation to the research community to join this exciting journey, shaping the future of GUI automation. In the spirit of open science, our code, data, and model will be made publicly available, paving the way for multimodal dialogue scenarios with intricate and precise interactions.","sentences":["In the rapidly evolving landscape of AI research and application, Multimodal Large Language Models (MLLMs) have emerged as a transformative force, adept at interpreting and integrating information from diverse modalities such as text, images, and Graphical User Interfaces (GUIs).","Despite these advancements, the nuanced interaction and understanding of GUIs pose a significant challenge, limiting the potential of existing models to enhance automation levels.","To bridge this gap, this paper presents V-Zen, an innovative Multimodal Large Language Model (MLLM) meticulously crafted to revolutionise the domain of GUI understanding and grounding.","Equipped with dual-resolution image encoders, V-Zen establishes new benchmarks in efficient grounding and next-action prediction, thereby laying the groundwork for self-operating computer systems.","Complementing V-Zen is the GUIDE dataset, an extensive collection of real-world GUI elements and task-based sequences, serving as a catalyst for specialised fine-tuning.","The successful integration of V-Zen and GUIDE marks the dawn of a new era in multimodal AI research, opening the door to intelligent, autonomous computing experiences.","This paper extends an invitation to the research community to join this exciting journey, shaping the future of GUI automation.","In the spirit of open science, our code, data, and model will be made publicly available, paving the way for multimodal dialogue scenarios with intricate and precise interactions."],"url":"http://arxiv.org/abs/2405.15341v1","category":"cs.AI"}
{"created":"2024-05-24 17:20:18","title":"Neural Persistence Dynamics","abstract":"We consider the problem of learning the dynamics in the topology of time-evolving point clouds, the prevalent spatiotemporal model for systems exhibiting collective behavior, such as swarms of insects and birds or particles in physics. In such systems, patterns emerge from (local) interactions among self-propelled entities. While several well-understood governing equations for motion and interaction exist, they are difficult to fit to data due to the often large number of entities and missing correspondences between the observation times, which may also not be equidistant. To evade such confounding factors, we investigate collective behavior from a \\textit{topological perspective}, but instead of summarizing entire observation sequences (as in prior work), we propose learning a latent dynamical model from topological features \\textit{per time point}. The latter is then used to formulate a downstream regression task to predict the parametrization of some a priori specified governing equation. We implement this idea based on a latent ODE learned from vectorized (static) persistence diagrams and show that this modeling choice is justified by a combination of recent stability results for persistent homology. Various (ablation) experiments not only demonstrate the relevance of each individual model component, but provide compelling empirical evidence that our proposed model -- \\textit{neural persistence dynamics} -- substantially outperforms the state-of-the-art across a diverse set of parameter regression tasks.","sentences":["We consider the problem of learning the dynamics in the topology of time-evolving point clouds, the prevalent spatiotemporal model for systems exhibiting collective behavior, such as swarms of insects and birds or particles in physics.","In such systems, patterns emerge from (local) interactions among self-propelled entities.","While several well-understood governing equations for motion and interaction exist, they are difficult to fit to data due to the often large number of entities and missing correspondences between the observation times, which may also not be equidistant.","To evade such confounding factors, we investigate collective behavior from a \\textit{topological perspective}, but instead of summarizing entire observation sequences (as in prior work), we propose learning a latent dynamical model from topological features \\textit{per time point}.","The latter is then used to formulate a downstream regression task to predict the parametrization of some a priori specified governing equation.","We implement this idea based on a latent ODE learned from vectorized (static) persistence diagrams and show that this modeling choice is justified by a combination of recent stability results for persistent homology.","Various (ablation) experiments not only demonstrate the relevance of each individual model component, but provide compelling empirical evidence that our proposed model -- \\textit{neural persistence dynamics} -- substantially outperforms the state-of-the-art across a diverse set of parameter regression tasks."],"url":"http://arxiv.org/abs/2405.15732v1","category":"cs.LG"}
{"created":"2024-05-24 16:57:18","title":"EmpathicStories++: A Multimodal Dataset for Empathy towards Personal Experiences","abstract":"Modeling empathy is a complex endeavor that is rooted in interpersonal and experiential dimensions of human interaction, and remains an open problem within AI. Existing empathy datasets fall short in capturing the richness of empathy responses, often being confined to in-lab or acted scenarios, lacking longitudinal data, and missing self-reported labels. We introduce a new multimodal dataset for empathy during personal experience sharing: the EmpathicStories++ dataset (https://mitmedialab.github.io/empathic-stories-multimodal/) containing 53 hours of video, audio, and text data of 41 participants sharing vulnerable experiences and reading empathically resonant stories with an AI agent. EmpathicStories++ is the first longitudinal dataset on empathy, collected over a month-long deployment of social robots in participants' homes, as participants engage in natural, empathic storytelling interactions with AI agents. We then introduce a novel task of predicting individuals' empathy toward others' stories based on their personal experiences, evaluated in two contexts: participants' own personal shared story context and their reflections on stories they read. We benchmark this task using state-of-the-art models to pave the way for future improvements in contextualized and longitudinal empathy modeling. Our work provides a valuable resource for further research in developing empathetic AI systems and understanding the intricacies of human empathy within genuine, real-world settings.","sentences":["Modeling empathy is a complex endeavor that is rooted in interpersonal and experiential dimensions of human interaction, and remains an open problem within AI.","Existing empathy datasets fall short in capturing the richness of empathy responses, often being confined to in-lab or acted scenarios, lacking longitudinal data, and missing self-reported labels.","We introduce a new multimodal dataset for empathy during personal experience sharing: the EmpathicStories++ dataset (https://mitmedialab.github.io/empathic-stories-multimodal/) containing 53 hours of video, audio, and text data of 41 participants sharing vulnerable experiences and reading empathically resonant stories with an AI agent.","EmpathicStories++ is the first longitudinal dataset on empathy, collected over a month-long deployment of social robots in participants' homes, as participants engage in natural, empathic storytelling interactions with AI agents.","We then introduce a novel task of predicting individuals' empathy toward others' stories based on their personal experiences, evaluated in two contexts: participants' own personal shared story context and their reflections on stories they read.","We benchmark this task using state-of-the-art models to pave the way for future improvements in contextualized and longitudinal empathy modeling.","Our work provides a valuable resource for further research in developing empathetic AI systems and understanding the intricacies of human empathy within genuine, real-world settings."],"url":"http://arxiv.org/abs/2405.15708v1","category":"cs.CL"}
{"created":"2024-05-24 14:03:27","title":"RCInvestigator: Towards Better Investigation of Anomaly Root Causes in Cloud Computing Systems","abstract":"Finding the root causes of anomalies in cloud computing systems quickly is crucial to ensure availability and efficiency since accurate root causes can guide engineers to take appropriate actions to address the anomalies and maintain customer satisfaction. However, it is difficult to investigate and identify the root causes based on large-scale and high-dimension monitoring data collected from complex cloud computing environments. Due to the inherently dynamic characteristics of cloud computing systems, the existing approaches in practice largely rely on manual analyses for flexibility and reliability, but massive unpredictable factors and high data complexity make the process time-consuming. Despite recent advances in automated detection and investigation approaches, the speed and quality of root cause analyses remain limited by the lack of expert involvement in these approaches. The limitations found in the current solutions motivate us to propose a visual analytics approach that facilitates the interactive investigation of the anomaly root causes in cloud computing systems. We identified three challenges, namely, a) modeling databases for the root cause investigation, b) inferring root causes from large-scale time series, and c) building comprehensible investigation results. In collaboration with domain experts, we addressed these challenges with RCInvestigator, a novel visual analytics system that establishes a tight collaboration between human and machine and assists experts in investigating the root causes of cloud computing system anomalies. We evaluated the effectiveness of RCInvestigator through two use cases based on real-world data and received positive feedback from experts.","sentences":["Finding the root causes of anomalies in cloud computing systems quickly is crucial to ensure availability and efficiency since accurate root causes can guide engineers to take appropriate actions to address the anomalies and maintain customer satisfaction.","However, it is difficult to investigate and identify the root causes based on large-scale and high-dimension monitoring data collected from complex cloud computing environments.","Due to the inherently dynamic characteristics of cloud computing systems, the existing approaches in practice largely rely on manual analyses for flexibility and reliability, but massive unpredictable factors and high data complexity make the process time-consuming.","Despite recent advances in automated detection and investigation approaches, the speed and quality of root cause analyses remain limited by the lack of expert involvement in these approaches.","The limitations found in the current solutions motivate us to propose a visual analytics approach that facilitates the interactive investigation of the anomaly root causes in cloud computing systems.","We identified three challenges, namely, a) modeling databases for the root cause investigation, b) inferring root causes from large-scale time series, and c) building comprehensible investigation results.","In collaboration with domain experts, we addressed these challenges with RCInvestigator, a novel visual analytics system that establishes a tight collaboration between human and machine and assists experts in investigating the root causes of cloud computing system anomalies.","We evaluated the effectiveness of RCInvestigator through two use cases based on real-world data and received positive feedback from experts."],"url":"http://arxiv.org/abs/2405.15571v1","category":"cs.HC"}
{"created":"2024-05-24 13:55:42","title":"PyCellMech: A shape-based feature extraction pipeline for use in medical and biological studies","abstract":"Summary: Medical researchers obtain knowledge about the prevention and treatment of disability and disease using physical measurements and image data. To assist in this endeavor, feature extraction packages are available that are designed to collect data from the image structure. In this study, we aim to augment current works by adding to the current mix of shape-based features. The significance of shape-based features has been explored extensively in research for several decades, but there is no single package available in which all shape-related features can be extracted easily by the researcher. PyCellMech has been crafted to address this gap. The PyCellMech package extracts three classes of shape features, which are classified as one-dimensional, geometric, and polygonal. Future iterations will be expanded to include other feature classes, such as scale-space.   Availability and implementation: PyCellMech is freely available at https://github.com/icm-dac/pycellmech.","sentences":["Summary: Medical researchers obtain knowledge about the prevention and treatment of disability and disease using physical measurements and image data.","To assist in this endeavor, feature extraction packages are available that are designed to collect data from the image structure.","In this study, we aim to augment current works by adding to the current mix of shape-based features.","The significance of shape-based features has been explored extensively in research for several decades, but there is no single package available in which all shape-related features can be extracted easily by the researcher.","PyCellMech has been crafted to address this gap.","The PyCellMech package extracts three classes of shape features, which are classified as one-dimensional, geometric, and polygonal.","Future iterations will be expanded to include other feature classes, such as scale-space.   ","Availability and implementation: PyCellMech is freely available at https://github.com/icm-dac/pycellmech."],"url":"http://arxiv.org/abs/2405.15567v1","category":"cs.CV"}
{"created":"2024-05-24 13:52:14","title":"Heterogeneous virus classification using a functional deep learning model based on transmission electron microscopy images (Preprint)","abstract":"Viruses are submicroscopic agents that can infect all kinds of lifeforms and use their hosts' living cells to replicate themselves. Despite having some of the simplest genetic structures among all living beings, viruses are highly adaptable, resilient, and given the right conditions, are capable of causing unforeseen complications in their hosts' bodies. Due to their multiple transmission pathways, high contagion rate, and lethality, viruses are the biggest biological threat faced by animal and plant species. It is often challenging to promptly detect the presence of a virus in a possible host's body and accurately determine its type using manual examination techniques; however, it can be done using computer-based automatic diagnosis methods. Most notably, the analysis of Transmission Electron Microscopy (TEM) images has been proven to be quite successful in instant virus identification. Using TEM images collected from a recently published dataset, this article proposes a deep learning-based classification model to identify the type of virus within those images correctly. The methodology of this study includes two coherent image processing techniques to reduce the noise present in the raw microscopy images. Experimental results show that it can differentiate among the 14 types of viruses present in the dataset with a maximum of 97.44% classification accuracy and F1-score, which asserts the effectiveness and reliability of the proposed method. Implementing this scheme will impart a fast and dependable way of virus identification subsidiary to the thorough diagnostic procedures.","sentences":["Viruses are submicroscopic agents that can infect all kinds of lifeforms and use their hosts' living cells to replicate themselves.","Despite having some of the simplest genetic structures among all living beings, viruses are highly adaptable, resilient, and given the right conditions, are capable of causing unforeseen complications in their hosts' bodies.","Due to their multiple transmission pathways, high contagion rate, and lethality, viruses are the biggest biological threat faced by animal and plant species.","It is often challenging to promptly detect the presence of a virus in a possible host's body and accurately determine its type using manual examination techniques; however, it can be done using computer-based automatic diagnosis methods.","Most notably, the analysis of Transmission Electron Microscopy (TEM) images has been proven to be quite successful in instant virus identification.","Using TEM images collected from a recently published dataset, this article proposes a deep learning-based classification model to identify the type of virus within those images correctly.","The methodology of this study includes two coherent image processing techniques to reduce the noise present in the raw microscopy images.","Experimental results show that it can differentiate among the 14 types of viruses present in the dataset with a maximum of 97.44% classification accuracy and F1-score, which asserts the effectiveness and reliability of the proposed method.","Implementing this scheme will impart a fast and dependable way of virus identification subsidiary to the thorough diagnostic procedures."],"url":"http://arxiv.org/abs/2405.15563v1","category":"cs.CV"}
{"created":"2024-05-24 13:00:28","title":"Source Code Archiving to the Rescue of Reproducible Deployment","abstract":"The ability to verify research results and to experiment with methodologies are core tenets of science. As research results are increasingly the outcome of computational processes, software plays a central role. GNU Guix is a software deployment tool that supports reproducible software deployment, making it a foundation for computational research workflows. To achieve reproducibility, we must first ensure the source code of software packages Guix deploys remains available.We describe our work connecting Guix with Software Heritage, the universal source code archive, making Guix the first free software distribution and tool backed by a stable archive. Our contribution is twofold: we explain the rationale and present the design and implementation we came up with; second, we report on the archival coverage for package source code with data collected over five years and discuss remaining challenges.","sentences":["The ability to verify research results and to experiment with methodologies are core tenets of science.","As research results are increasingly the outcome of computational processes, software plays a central role.","GNU Guix is a software deployment tool that supports reproducible software deployment, making it a foundation for computational research workflows.","To achieve reproducibility, we must first ensure the source code of software packages Guix deploys remains available.","We describe our work connecting Guix with Software Heritage, the universal source code archive, making Guix the first free software distribution and tool backed by a stable archive.","Our contribution is twofold: we explain the rationale and present the design and implementation we came up with; second, we report on the archival coverage for package source code with data collected over five years and discuss remaining challenges."],"url":"http://arxiv.org/abs/2405.15516v1","category":"cs.SE"}
{"created":"2024-05-24 12:52:46","title":"Human-in-the-loop Reinforcement Learning for Data Quality Monitoring in Particle Physics Experiments","abstract":"Data Quality Monitoring (DQM) is a crucial task in large particle physics experiments, since detector malfunctioning can compromise the data. DQM is currently performed by human shifters, which is costly and results in limited accuracy. In this work, we provide a proof-of-concept for applying human-in-the-loop Reinforcement Learning (RL) to automate the DQM process while adapting to operating conditions that change over time. We implement a prototype based on the Proximal Policy Optimization (PPO) algorithm and validate it on a simplified synthetic dataset. We demonstrate how a multi-agent system can be trained for continuous automated monitoring during data collection, with human intervention actively requested only when relevant. We show that random, unbiased noise in human classification can be reduced, leading to an improved accuracy over the baseline. Additionally, we propose data augmentation techniques to deal with scarce data and to accelerate the learning process. Finally, we discuss further steps needed to implement the approach in the real world, including protocols for periodic control of the algorithm's outputs.","sentences":["Data Quality Monitoring (DQM) is a crucial task in large particle physics experiments, since detector malfunctioning can compromise the data.","DQM is currently performed by human shifters, which is costly and results in limited accuracy.","In this work, we provide a proof-of-concept for applying human-in-the-loop Reinforcement Learning (RL) to automate the DQM process while adapting to operating conditions that change over time.","We implement a prototype based on the Proximal Policy Optimization (PPO) algorithm and validate it on a simplified synthetic dataset.","We demonstrate how a multi-agent system can be trained for continuous automated monitoring during data collection, with human intervention actively requested only when relevant.","We show that random, unbiased noise in human classification can be reduced, leading to an improved accuracy over the baseline.","Additionally, we propose data augmentation techniques to deal with scarce data and to accelerate the learning process.","Finally, we discuss further steps needed to implement the approach in the real world, including protocols for periodic control of the algorithm's outputs."],"url":"http://arxiv.org/abs/2405.15508v1","category":"hep-ex"}
{"created":"2024-05-24 11:45:05","title":"Unsteady aerodynamic prediction using limited samples based on transfer learning","abstract":"In this study, a method for predicting unsteady aerodynamic forces under different initial conditions using a limited number of samples based on transfer learning is proposed, aiming to avoid the need for large-scale high-fidelity aerodynamic simulations. First, a large number of training samples are acquired through high-fidelity simulation under the initial condition for the baseline, followed by the establishment of a pre-trained network as the source model using a long short-term memory (LSTM) network. When unsteady aerodynamic forces are predicted under the new initial conditions, a limited number of training samples are collected by high-fidelity simulations. Then, the parameters of the source model are transferred to the new prediction model, which is further fine-tuned and trained with limited samples. The new prediction model can be used to predict the unsteady aerodynamic forces of the entire process under the new initial conditions. The proposed method is validated by predicting the aerodynamic forces of free flight of a high-spinning projectile with a large extension of initial angular velocity and pitch angle. The results indicatethat the proposed method can predict unsteady aerodynamic forces under different initial conditions using 1/3 of the sample size of the source model. Compared with direct modeling using the LSTM networks, the proposed method shows improved accuracy and efficiency.","sentences":["In this study, a method for predicting unsteady aerodynamic forces under different initial conditions using a limited number of samples based on transfer learning is proposed, aiming to avoid the need for large-scale high-fidelity aerodynamic simulations.","First, a large number of training samples are acquired through high-fidelity simulation under the initial condition for the baseline, followed by the establishment of a pre-trained network as the source model using a long short-term memory (LSTM) network.","When unsteady aerodynamic forces are predicted under the new initial conditions, a limited number of training samples are collected by high-fidelity simulations.","Then, the parameters of the source model are transferred to the new prediction model, which is further fine-tuned and trained with limited samples.","The new prediction model can be used to predict the unsteady aerodynamic forces of the entire process under the new initial conditions.","The proposed method is validated by predicting the aerodynamic forces of free flight of a high-spinning projectile with a large extension of initial angular velocity and pitch angle.","The results indicatethat the proposed method can predict unsteady aerodynamic forces under different initial conditions using 1/3 of the sample size of the source model.","Compared with direct modeling using the LSTM networks, the proposed method shows improved accuracy and efficiency."],"url":"http://arxiv.org/abs/2405.15470v1","category":"physics.flu-dyn"}
{"created":"2024-05-24 11:39:09","title":"A note about a transition of Ratliff and Rosenthal's order picking algorithm for rectangular warehouses","abstract":"In the order picking problem, a picker has to collect a number of products in a warehouse with a minimum length tour. Ratliff and Rosenthal gave a linear algorithm solving the order picking problem in the case where the warehouse has two cross aisles. Their algorithm allow the tour to double cross an entire aisle. We prove that, in rectangular warehouses, there always exists a minimum length tour which doesn't double cross an aisle.","sentences":["In the order picking problem, a picker has to collect a number of products in a warehouse with a minimum length tour.","Ratliff and Rosenthal gave a linear algorithm solving the order picking problem in the case where the warehouse has two cross aisles.","Their algorithm allow the tour to double cross an entire aisle.","We prove that, in rectangular warehouses, there always exists a minimum length tour which doesn't double cross an aisle."],"url":"http://arxiv.org/abs/2405.15464v1","category":"math.OC"}
{"created":"2024-05-24 11:02:55","title":"Biometrics and Behavioral Modelling for Detecting Distractions in Online Learning","abstract":"In this article, we explore computer vision approaches to detect abnormal head pose during e-learning sessions and we introduce a study on the effects of mobile phone usage during these sessions. We utilize behavioral data collected from 120 learners monitored while participating in a MOOC learning sessions. Our study focuses on the influence of phone-usage events on behavior and physiological responses, specifically attention, heart rate, and meditation, before, during, and after phone usage. Additionally, we propose an approach for estimating head pose events using images taken by the webcam during the MOOC learning sessions to detect phone-usage events. Our hypothesis suggests that head posture undergoes significant changes when learners interact with a mobile phone, contrasting with the typical behavior seen when learners face a computer during e-learning sessions. We propose an approach designed to detect deviations in head posture from the average observed during a learner's session, operating as a semi-supervised method. This system flags events indicating alterations in head posture for subsequent human review and selection of mobile phone usage occurrences with a sensitivity over 90%.","sentences":["In this article, we explore computer vision approaches to detect abnormal head pose during e-learning sessions and we introduce a study on the effects of mobile phone usage during these sessions.","We utilize behavioral data collected from 120 learners monitored while participating in a MOOC learning sessions.","Our study focuses on the influence of phone-usage events on behavior and physiological responses, specifically attention, heart rate, and meditation, before, during, and after phone usage.","Additionally, we propose an approach for estimating head pose events using images taken by the webcam during the MOOC learning sessions to detect phone-usage events.","Our hypothesis suggests that head posture undergoes significant changes when learners interact with a mobile phone, contrasting with the typical behavior seen when learners face a computer during e-learning sessions.","We propose an approach designed to detect deviations in head posture from the average observed during a learner's session, operating as a semi-supervised method.","This system flags events indicating alterations in head posture for subsequent human review and selection of mobile phone usage occurrences with a sensitivity over 90%."],"url":"http://arxiv.org/abs/2405.15434v1","category":"cs.CV"}
{"created":"2024-05-24 10:27:11","title":"A Systematic Review on Custom Data Gloves","abstract":"Hands are a fundamental tool humans use to interact with the environment and objects. Through hand motions, we can obtain information about the shape and materials of the surfaces we touch, modify our surroundings by interacting with objects, manipulate objects and tools, or communicate with other people by leveraging the power of gestures. For these reasons, sensorized gloves, which can collect information about hand motions and interactions, have been of interest since the 1980s in various fields, such as Human-Machine Interaction (HMI) and the analysis and control of human motions.   Over the last 40 years, research in this field explored different technological approaches and contributed to the popularity of wearable custom and commercial products targeting hand sensorization. Despite a positive research trend, these instruments are not widespread yet outside research environments and devices aimed at research are often ad hoc solutions with a low chance of being reused. This paper aims to provide a systematic literature review for custom gloves to analyze their main characteristics and critical issues, from the type and number of sensors to the limitations due to device encumbrance. The collection of this information lays the foundation for a standardization process necessary for future breakthroughs in this research field.","sentences":["Hands are a fundamental tool humans use to interact with the environment and objects.","Through hand motions, we can obtain information about the shape and materials of the surfaces we touch, modify our surroundings by interacting with objects, manipulate objects and tools, or communicate with other people by leveraging the power of gestures.","For these reasons, sensorized gloves, which can collect information about hand motions and interactions, have been of interest since the 1980s in various fields, such as Human-Machine Interaction (HMI) and the analysis and control of human motions.   ","Over the last 40 years, research in this field explored different technological approaches and contributed to the popularity of wearable custom and commercial products targeting hand sensorization.","Despite a positive research trend, these instruments are not widespread yet outside research environments and devices aimed at research are often ad hoc solutions with a low chance of being reused.","This paper aims to provide a systematic literature review for custom gloves to analyze their main characteristics and critical issues, from the type and number of sensors to the limitations due to device encumbrance.","The collection of this information lays the foundation for a standardization process necessary for future breakthroughs in this research field."],"url":"http://arxiv.org/abs/2405.15417v1","category":"cs.HC"}
{"created":"2024-05-24 09:50:49","title":"Metastability of multi-population Kuramoto-Sakaguchi oscillators","abstract":"An Ott-Antonsen reduced $M$-population of Kuramoto-Sakaguchi oscillators is investigated, focusing on the influence of the phase-lag parameter $\\alpha$ on the collective dynamics. For oscillator populations coupled on a ring, we obtained a wide variety of spatiotemporal patterns, including coherent states, traveling waves, partially synchronized states, modulated states, and incoherent states. Back-and-forth transitions between these states are found, which suggest metastability. Linear stability analysis reveals the stable regions of coherent states with different winding numbers $q$. Within certain $\\alpha$ ranges, the system settles into stable traveling wave solutions despite the coherent states also being linearly stable. For around $\\alpha \\approx 0.46\\pi$, the system displays the most frequent metastable transitions between coherent states and partially synchronized states, while for $\\alpha$ closer to $\\pi/2$, metastable transitions arise between partially synchronized states and modulated states. This model captures metastable dynamics akin to brain activity, offering insights into the synchronization of brain networks.","sentences":["An Ott-Antonsen reduced $M$-population of Kuramoto-Sakaguchi oscillators is investigated, focusing on the influence of the phase-lag parameter $\\alpha$ on the collective dynamics.","For oscillator populations coupled on a ring, we obtained a wide variety of spatiotemporal patterns, including coherent states, traveling waves, partially synchronized states, modulated states, and incoherent states.","Back-and-forth transitions between these states are found, which suggest metastability.","Linear stability analysis reveals the stable regions of coherent states with different winding numbers $q$. Within certain $\\alpha$ ranges, the system settles into stable traveling wave solutions despite the coherent states also being linearly stable.","For around $\\alpha \\approx 0.46\\pi$, the system displays the most frequent metastable transitions between coherent states and partially synchronized states, while for $\\alpha$ closer to $\\pi/2$, metastable transitions arise between partially synchronized states and modulated states.","This model captures metastable dynamics akin to brain activity, offering insights into the synchronization of brain networks."],"url":"http://arxiv.org/abs/2405.15396v1","category":"nlin.AO"}
{"created":"2024-05-24 08:44:40","title":"Measurements of $\\boldsymbol{B\\rightarrow K\u03c0}$ and $\\boldsymbol{B\\rightarrow \u03c0\u03c0}$ Branching Fractions and $\\boldsymbol{\\mathcal{A}_{CP}}$ Asymmetries at Belle II","abstract":"Analyses of $B$ meson decays to charmless hadronic final states are an important part of the Belle II program. They are sensitive to effects from non-standard model physics and provide experimentally precise constraints on the weak interactions of quarks. We present recent Belle II results on branching fractions and direct $CP$-violating asymmetries of the decays $B^0 \\rightarrow K^+\\pi^-$, $B^+ \\rightarrow K^+\\pi^0$, $B^+ \\rightarrow K^0\\pi^+$, and $B^0 \\rightarrow K^0\\pi^0$, and use these to test the standard model through an isospin-based sum rule. In addition, we measure the branching fraction and direct $CP$ asymmetry of the decay $B^+ \\rightarrow \\pi^+\\pi^0$ and the branching fraction of the decay $B^0 \\rightarrow \\pi^+\\pi^-$, which contribute towards the determination of the CKM angle $\\phi_2$. The data are collected with the Belle II detector from the SuperKEKB asymmetric-energy $e^+e^-$ collider, consisting of $387 \\times 10^6$ $\\Upsilon(4S)\\rightarrow B\\bar{B}$ events. We obtain $-0.03 \\pm 0.13 \\pm 0.04$ for the sum rule, in agreement with the standard model expectation of zero and with a precision comparable to the best existing determinations.","sentences":["Analyses of $B$ meson decays to charmless hadronic final states are an important part of the Belle II program.","They are sensitive to effects from non-standard model physics and provide experimentally precise constraints on the weak interactions of quarks.","We present recent Belle II results on branching fractions and direct $CP$-violating asymmetries of the decays $B^0 \\rightarrow K^+\\pi^-$, $B^+ \\rightarrow K^+\\pi^0$, $B^+ \\rightarrow K^0\\pi^+$, and $B^0 \\rightarrow K^0\\pi^0$, and use these to test the standard model through an isospin-based sum rule.","In addition, we measure the branching fraction and direct $CP$ asymmetry of the decay $B^+ \\rightarrow \\pi^+\\pi^0$ and the branching fraction of the decay $B^0 \\rightarrow \\pi^+\\pi^-$, which contribute towards the determination of the CKM angle $\\phi_2$. The data are collected with the Belle II detector from the SuperKEKB asymmetric-energy $e^+e^-$ collider, consisting of $387 \\times 10^6$ $\\Upsilon(4S)\\rightarrow B\\bar{B}$ events.","We obtain $-0.03 \\pm 0.13 \\pm 0.04$ for the sum rule, in agreement with the standard model expectation of zero and with a precision comparable to the best existing determinations."],"url":"http://arxiv.org/abs/2405.15352v1","category":"hep-ex"}
{"created":"2024-05-24 17:59:06","title":"Improved Particle Approximation Error for Mean Field Neural Networks","abstract":"Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions. MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks. Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting. Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases. In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors, which can exponentially deteriorate with the regularization coefficient. Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity.","sentences":["Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions.","MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks.","Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting.","Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases.","In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors, which can exponentially deteriorate with the regularization coefficient.","Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization.","As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity."],"url":"http://arxiv.org/abs/2405.15767v1","category":"cs.LG"}
{"created":"2024-05-24 17:57:52","title":"Sliding-Mode Nash Equilibrium Seeking for a Quadratic Duopoly Game","abstract":"This paper introduces a new method to achieve stable convergence to Nash equilibrium in duopoly noncooperative games. Inspired by the recent fixed-time Nash Equilibrium seeking (NES) as well as prescribed-time extremum seeking (ES) and source seeking schemes, our approach employs a distributed sliding mode control (SMC) scheme, integrating extremum seeking with sinusoidal perturbation signals to estimate the pseudogradients of quadratic payoff functions. Notably, this is the first attempt to address noncooperative games without relying on models, combining classical extremum seeking with relay components instead of proportional control laws. We prove finite-time convergence of the closed-loop average system to Nash equilibrium using stability analysis techniques such as time-scaling, Lyapunov's direct method, and averaging theory for discontinuous systems. Additionally, we quantify the size of residual sets around the Nash equilibrium and validate our theoretical results through simulations.","sentences":["This paper introduces a new method to achieve stable convergence to Nash equilibrium in duopoly noncooperative games.","Inspired by the recent fixed-time Nash Equilibrium seeking (NES) as well as prescribed-time extremum seeking (ES) and source seeking schemes, our approach employs a distributed sliding mode control (SMC) scheme, integrating extremum seeking with sinusoidal perturbation signals to estimate the pseudogradients of quadratic payoff functions.","Notably, this is the first attempt to address noncooperative games without relying on models, combining classical extremum seeking with relay components instead of proportional control laws.","We prove finite-time convergence of the closed-loop average system to Nash equilibrium using stability analysis techniques such as time-scaling, Lyapunov's direct method, and averaging theory for discontinuous systems.","Additionally, we quantify the size of residual sets around the Nash equilibrium and validate our theoretical results through simulations."],"url":"http://arxiv.org/abs/2405.15762v1","category":"math.OC"}
{"created":"2024-05-24 17:54:40","title":"A skew Specht perspective of RoCK blocks and cuspidal systems for KLR algebras in affine type A","abstract":"Cuspidal systems parameterize KLR algebra representations via root partitions $\\pi$, where simple modules $L(\\pi)$ arise as heads of proper standard modules. Working in affine type A with an arbitrary convex preorder, we construct explicit skew diagrams $\\zeta(\\pi)$ such that the skew Specht module $S^{\\zeta(\\pi)}$ has simple head $L(\\pi)$ and a filtration by proper standard modules. This result arises from an in-depth study of the combinatorial interplay between cuspidal systems and RoCK cyclotomic KLR algebras.","sentences":["Cuspidal systems parameterize KLR algebra representations via root partitions $\\pi$, where simple modules $L(\\pi)$ arise as heads of proper standard modules.","Working in affine type A with an arbitrary convex preorder, we construct explicit skew diagrams $\\zeta(\\pi)$ such that the skew Specht module $S^{\\zeta(\\pi)}$ has simple head $L(\\pi)$ and a filtration by proper standard modules.","This result arises from an in-depth study of the combinatorial interplay between cuspidal systems and RoCK cyclotomic KLR algebras."],"url":"http://arxiv.org/abs/2405.15759v1","category":"math.RT"}
{"created":"2024-05-24 17:51:33","title":"ETTrack: Enhanced Temporal Motion Predictor for Multi-Object Tracking","abstract":"Many Multi-Object Tracking (MOT) approaches exploit motion information to associate all the detected objects across frames. However, many methods that rely on filtering-based algorithms, such as the Kalman Filter, often work well in linear motion scenarios but struggle to accurately predict the locations of objects undergoing complex and non-linear movements. To tackle these scenarios, we propose a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack. Specifically, the motion predictor integrates a transformer model and a Temporal Convolutional Network (TCN) to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information. Additionally, we propose a novel Momentum Correction Loss function that provides additional information regarding the motion direction of objects during training. This allows the motion predictor rapidly adapt to motion variations and more accurately predict future motion. Our experimental results demonstrate that ETTrack achieves a competitive performance compared with state-of-the-art trackers on DanceTrack and SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively.","sentences":["Many Multi-Object Tracking (MOT) approaches exploit motion information to associate all the detected objects across frames.","However, many methods that rely on filtering-based algorithms, such as the Kalman Filter, often work well in linear motion scenarios but struggle to accurately predict the locations of objects undergoing complex and non-linear movements.","To tackle these scenarios, we propose a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack.","Specifically, the motion predictor integrates a transformer model and a Temporal Convolutional Network (TCN) to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information.","Additionally, we propose a novel Momentum Correction Loss function that provides additional information regarding the motion direction of objects during training.","This allows the motion predictor rapidly adapt to motion variations and more accurately predict future motion.","Our experimental results demonstrate that ETTrack achieves a competitive performance compared with state-of-the-art trackers on DanceTrack and SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively."],"url":"http://arxiv.org/abs/2405.15755v1","category":"cs.CV"}
{"created":"2024-05-24 17:49:34","title":"Data Reconstruction: When You See It and When You Don't","abstract":"We revisit the fundamental question of formally defining what constitutes a reconstruction attack. While often clear from the context, our exploration reveals that a precise definition is much more nuanced than it appears, to the extent that a single all-encompassing definition may not exist. Thus, we employ a different strategy and aim to \"sandwich\" the concept of reconstruction attacks by addressing two complementing questions: (i) What conditions guarantee that a given system is protected against such attacks? (ii) Under what circumstances does a given attack clearly indicate that a system is not protected? More specifically,   * We introduce a new definitional paradigm -- Narcissus Resiliency -- to formulate a security definition for protection against reconstruction attacks. This paradigm has a self-referential nature that enables it to circumvent shortcomings of previously studied notions of security. Furthermore, as a side-effect, we demonstrate that Narcissus resiliency captures as special cases multiple well-studied concepts including differential privacy and other security notions of one-way functions and encryption schemes.   * We formulate a link between reconstruction attacks and Kolmogorov complexity. This allows us to put forward a criterion for evaluating when such attacks are convincingly successful.","sentences":["We revisit the fundamental question of formally defining what constitutes a reconstruction attack.","While often clear from the context, our exploration reveals that a precise definition is much more nuanced than it appears, to the extent that a single all-encompassing definition may not exist.","Thus, we employ a different strategy and aim to \"sandwich\" the concept of reconstruction attacks by addressing two complementing questions: (i) What conditions guarantee that a given system is protected against such attacks?","(ii) Under what circumstances does a given attack clearly indicate that a system is not protected?","More specifically,   *","We introduce a new definitional paradigm -- Narcissus Resiliency -- to formulate a security definition for protection against reconstruction attacks.","This paradigm has a self-referential nature that enables it to circumvent shortcomings of previously studied notions of security.","Furthermore, as a side-effect, we demonstrate that Narcissus resiliency captures as special cases multiple well-studied concepts including differential privacy and other security notions of one-way functions and encryption schemes.   ","* We formulate a link between reconstruction attacks and Kolmogorov complexity.","This allows us to put forward a criterion for evaluating when such attacks are convincingly successful."],"url":"http://arxiv.org/abs/2405.15753v1","category":"cs.CR"}
{"created":"2024-05-24 17:36:34","title":"Near-threshold states in coupled $DD^{\\ast}-D^{\\ast}D^{\\ast}$ scattering from lattice QCD","abstract":"The first determination of doubly-charmed isospin-0 coupled-channel $DD^\\ast-D^\\ast D^\\ast$ scattering amplitudes from lattice QCD is presented. The finite-volume spectrum is computed for three lattice volumes with a light-quark mass corresponding to $m_\\pi\\approx 391$ MeV and is used to extract the scattering amplitudes in $J^P = 1^+$ via the L\\\"{u}scher quantization condition. By analytically continuing the scattering amplitudes to complex energies, a $T_{cc}$ pole corresponding to a virtual bound state is found below $DD^\\ast$ threshold. We also find a second pole, $T_{cc}^\\prime$, corresponding to a resonance pole below the kinematically closed $D^\\ast D^\\ast$ channel, to which it has a strong coupling. A non-zero coupling is robustly found between the $S$-wave $D D^\\ast$ and $D^\\ast D^\\ast$ channels producing a clear cusp in the $D D^\\ast$ amplitude at the $D^\\ast D^\\ast$ threshold energy. This suggests that the experimental $T_{cc}^\\prime$ should be observable in $D D^\\ast$ and $D^\\ast D^\\ast$ final states at ongoing experiments.","sentences":["The first determination of doubly-charmed isospin-0 coupled-channel $DD^\\ast-D^\\ast D^\\ast$ scattering amplitudes from lattice QCD is presented.","The finite-volume spectrum is computed for three lattice volumes with a light-quark mass corresponding to $m_\\pi\\approx 391$ MeV and is used to extract the scattering amplitudes in $J^P = 1^+$ via the L\\\"{u}scher quantization condition.","By analytically continuing the scattering amplitudes to complex energies, a $T_{cc}$ pole corresponding to a virtual bound state is found below $DD^\\ast$ threshold.","We also find a second pole, $T_{cc}^\\prime$, corresponding to a resonance pole below the kinematically closed $D^\\ast D^\\ast$ channel, to which it has a strong coupling.","A non-zero coupling is robustly found between the $S$-wave $D D^\\ast$ and $D^\\ast D^\\ast$ channels producing a clear cusp in the $D D^\\ast$ amplitude at the $D^\\ast D^\\ast$ threshold energy.","This suggests that the experimental $T_{cc}^\\prime$ should be observable in $D D^\\ast$ and $D^\\ast D^\\ast$ final states at ongoing experiments."],"url":"http://arxiv.org/abs/2405.15741v1","category":"hep-lat"}
{"created":"2024-05-24 17:34:06","title":"More Insight from Being More Focused: Analysis of Clustered Market Apps","abstract":"The increasing attraction of mobile apps has inspired researchers to analyze apps from different perspectives. As with any software product, apps have different attributes such as size, content maturity, rating, category, or number of downloads. Current research studies mostly consider sampling across all apps. This often results in comparisons of apps being quite different in nature and category (games compared with weather and calendar apps), also being different in size and complexity. Similar to proprietary software and web-based services, more specific results can be expected from looking at more homogeneous samples as they can be received as a result of applying clustering. In this paper, we target homogeneous samples of apps to increase the degree of insight gained from analytics. As a proof-of-concept, we applied the clustering technique DBSCAN and subsequent correlation analysis between app attributes for a set of 940 open-source mobile apps from F-Droid. We showed that (i) clusters of apps with similar characteristics provided more insight compared to applying the same to the whole data and (ii) defining the similarity of apps based on the similarity of topics as created from the topic modeling technique Latent Dirichlet Allocation does not significantly improve clustering results.","sentences":["The increasing attraction of mobile apps has inspired researchers to analyze apps from different perspectives.","As with any software product, apps have different attributes such as size, content maturity, rating, category, or number of downloads.","Current research studies mostly consider sampling across all apps.","This often results in comparisons of apps being quite different in nature and category (games compared with weather and calendar apps), also being different in size and complexity.","Similar to proprietary software and web-based services, more specific results can be expected from looking at more homogeneous samples as they can be received as a result of applying clustering.","In this paper, we target homogeneous samples of apps to increase the degree of insight gained from analytics.","As a proof-of-concept, we applied the clustering technique DBSCAN and subsequent correlation analysis between app attributes for a set of 940 open-source mobile apps from F-Droid.","We showed that (i) clusters of apps with similar characteristics provided more insight compared to applying the same to the whole data and (ii) defining the similarity of apps based on the similarity of topics as created from the topic modeling technique Latent Dirichlet Allocation does not significantly improve clustering results."],"url":"http://arxiv.org/abs/2405.15737v1","category":"cs.SE"}
{"created":"2024-05-24 17:17:34","title":"Anomalous Change Point Detection Using Probabilistic Predictive Coding","abstract":"Change point detection (CPD) and anomaly detection (AD) are essential techniques in various fields to identify abrupt changes or abnormal data instances. However, existing methods are often constrained to univariate data, face scalability challenges with large datasets due to computational demands, and experience reduced performance with high-dimensional or intricate data, as well as hidden anomalies. Furthermore, they often lack interpretability and adaptability to domain-specific knowledge, which limits their versatility across different fields. In this work, we propose a deep learning-based CPD/AD method called Probabilistic Predictive Coding (PPC) that jointly learns to encode sequential data to low dimensional latent space representations and to predict the subsequent data representations as well as the corresponding prediction uncertainties. The model parameters are optimized with maximum likelihood estimation by comparing these predictions with the true encodings. At the time of application, the true and predicted encodings are used to determine the probability of conformity, an interpretable and meaningful anomaly score. Furthermore, our approach has linear time complexity, scalability issues are prevented, and the method can easily be adjusted to a wide range of data types and intricate applications. We demonstrate the effectiveness and adaptability of our proposed method across synthetic time series experiments, image data, and real-world magnetic resonance spectroscopic imaging data.","sentences":["Change point detection (CPD) and anomaly detection (AD) are essential techniques in various fields to identify abrupt changes or abnormal data instances.","However, existing methods are often constrained to univariate data, face scalability challenges with large datasets due to computational demands, and experience reduced performance with high-dimensional or intricate data, as well as hidden anomalies.","Furthermore, they often lack interpretability and adaptability to domain-specific knowledge, which limits their versatility across different fields.","In this work, we propose a deep learning-based CPD/AD method called Probabilistic Predictive Coding (PPC) that jointly learns to encode sequential data to low dimensional latent space representations and to predict the subsequent data representations as well as the corresponding prediction uncertainties.","The model parameters are optimized with maximum likelihood estimation by comparing these predictions with the true encodings.","At the time of application, the true and predicted encodings are used to determine the probability of conformity, an interpretable and meaningful anomaly score.","Furthermore, our approach has linear time complexity, scalability issues are prevented, and the method can easily be adjusted to a wide range of data types and intricate applications.","We demonstrate the effectiveness and adaptability of our proposed method across synthetic time series experiments, image data, and real-world magnetic resonance spectroscopic imaging data."],"url":"http://arxiv.org/abs/2405.15727v1","category":"stat.ML"}
{"created":"2024-05-24 17:11:27","title":"Bisimulation Learning","abstract":"We introduce a data-driven approach to computing finite bisimulations for state transition systems with very large, possibly infinite state space. Our novel technique computes stutter-insensitive bisimulations of deterministic systems, which we characterize as the problem of learning a state classifier together with a ranking function for each class. Our procedure learns a candidate state classifier and candidate ranking functions from a finite dataset of sample states; then, it checks whether these generalise to the entire state space using satisfiability modulo theory solving. Upon the affirmative answer, the procedure concludes that the classifier constitutes a valid stutter-insensitive bisimulation of the system. Upon a negative answer, the solver produces a counterexample state for which the classifier violates the claim, adds it to the dataset, and repeats learning and checking in a counterexample-guided inductive synthesis loop until a valid bisimulation is found. We demonstrate on a range of benchmarks from reactive verification and software model checking that our method yields faster verification results than alternative state-of-the-art tools in practice. Our method produces succinct abstractions that enable an effective verification of linear temporal logic without next operator, and are interpretable for system diagnostics.","sentences":["We introduce a data-driven approach to computing finite bisimulations for state transition systems with very large, possibly infinite state space.","Our novel technique computes stutter-insensitive bisimulations of deterministic systems, which we characterize as the problem of learning a state classifier together with a ranking function for each class.","Our procedure learns a candidate state classifier and candidate ranking functions from a finite dataset of sample states; then, it checks whether these generalise to the entire state space using satisfiability modulo theory solving.","Upon the affirmative answer, the procedure concludes that the classifier constitutes a valid stutter-insensitive bisimulation of the system.","Upon a negative answer, the solver produces a counterexample state for which the classifier violates the claim, adds it to the dataset, and repeats learning and checking in a counterexample-guided inductive synthesis loop until a valid bisimulation is found.","We demonstrate on a range of benchmarks from reactive verification and software model checking that our method yields faster verification results than alternative state-of-the-art tools in practice.","Our method produces succinct abstractions that enable an effective verification of linear temporal logic without next operator, and are interpretable for system diagnostics."],"url":"http://arxiv.org/abs/2405.15723v1","category":"cs.LO"}
{"created":"2024-05-24 17:06:43","title":"The electromagnetic symmetry sphere: a framework for energy, momentum, spin and other electromagnetic quantities","abstract":"Electromagnetic quantities such as energy density, momentum, spin, and helicity bring meaning and intuition to electromagnetism and possess intricate interrelations, particularly prominent in complex non-paraxial near-fields. These quantities are conventionally expressed using electric and magnetic field vectors, yet the electric-magnetic basis is one among other often overlooked alternatives, including parallel-antiparallel and right-left-handed helicity bases, related to the parity and duality symmetries of electromagnetism. Projecting time-harmonic electromagnetic fields into a variety of bases allows re-interpreting established quantities and reveals underlying mathematical structures: a Bloch sphere which describes asymmetries in electromagnetic energy, a systematic path to unify and uncover relations between electromagnetic quantities, and the unlocking of symmetry-driven equations in light-matter interaction.","sentences":["Electromagnetic quantities such as energy density, momentum, spin, and helicity bring meaning and intuition to electromagnetism and possess intricate interrelations, particularly prominent in complex non-paraxial near-fields.","These quantities are conventionally expressed using electric and magnetic field vectors, yet the electric-magnetic basis is one among other often overlooked alternatives, including parallel-antiparallel and right-left-handed helicity bases, related to the parity and duality symmetries of electromagnetism.","Projecting time-harmonic electromagnetic fields into a variety of bases allows re-interpreting established quantities and reveals underlying mathematical structures: a Bloch sphere which describes asymmetries in electromagnetic energy, a systematic path to unify and uncover relations between electromagnetic quantities, and the unlocking of symmetry-driven equations in light-matter interaction."],"url":"http://arxiv.org/abs/2405.15718v1","category":"physics.optics"}
{"created":"2024-05-24 17:01:38","title":"Understanding the Star Formation Efficiency in Dense Gas: Initial Results from the CAFFEINE Survey with ArT\u00e9MiS","abstract":"Despite recent progress, the question of what regulates the star formation efficiency in galaxies remains one of the most debated problems in astrophysics. According to the dominant picture, star formation (SF) is regulated by turbulence and feedback, and the SFE is 1-2% per local free-fall time. In an alternate scenario, the SF rate in galactic disks is linearly proportional to the mass of dense gas above a critical density threshold. We aim to discriminate between these two pictures thanks to high-resolution observations tracing dense gas and young stellar objects (YSOs) for a comprehensive sample of 49 nearby massive SF complexes out to d < 3 kpc in the Galactic disk. We use data from CAFFEINE, a 350/450 $\\mu$m survey with APEX/ArT\\'eMiS of the densest portions of all southern molecular clouds, in combination with Herschel data to produce column density maps at 8\" resolution. Our maps are free of saturation and resolve the structure of dense gas and the typical 0.1 pc width of molecular filaments at 3 kpc, which is impossible with Herschel data alone. Coupled with SFR estimates derived from Spitzer observations of the YSO content of the same clouds, this allows us to study the dependence of the SFE with density in the CAFFEINE clouds. We also combine our findings with existing SFE measurements in nearby clouds to extend our analysis down to lower column densities. Our results suggest that the SFE does not increase with density above the critical threshold and support a scenario in which the SFE in dense gas is approximately constant. However, the SFE measurements traced by Class I YSOs in nearby clouds are more inconclusive, since they are consistent with both the presence of a density threshold and a dependence on density above the threshold. Overall, we suggest that the SFE in dense gas is primarily governed by the physics of filament fragmentation into protostellar cores.","sentences":["Despite recent progress, the question of what regulates the star formation efficiency in galaxies remains one of the most debated problems in astrophysics.","According to the dominant picture, star formation (SF) is regulated by turbulence and feedback, and the SFE is 1-2% per local free-fall time.","In an alternate scenario, the SF rate in galactic disks is linearly proportional to the mass of dense gas above a critical density threshold.","We aim to discriminate between these two pictures thanks to high-resolution observations tracing dense gas and young stellar objects (YSOs) for a comprehensive sample of 49 nearby massive SF complexes out to d < 3 kpc in the Galactic disk.","We use data from CAFFEINE, a 350/450 $\\mu$m survey with APEX/ArT\\'eMiS of the densest portions of all southern molecular clouds, in combination with Herschel data to produce column density maps at 8\" resolution.","Our maps are free of saturation and resolve the structure of dense gas and the typical 0.1 pc width of molecular filaments at 3 kpc, which is impossible with Herschel data alone.","Coupled with SFR estimates derived from Spitzer observations of the YSO content of the same clouds, this allows us to study the dependence of the SFE with density in the CAFFEINE clouds.","We also combine our findings with existing SFE measurements in nearby clouds to extend our analysis down to lower column densities.","Our results suggest that the SFE does not increase with density above the critical threshold and support a scenario in which the SFE in dense gas is approximately constant.","However, the SFE measurements traced by Class I YSOs in nearby clouds are more inconclusive, since they are consistent with both the presence of a density threshold and a dependence on density above the threshold.","Overall, we suggest that the SFE in dense gas is primarily governed by the physics of filament fragmentation into protostellar cores."],"url":"http://arxiv.org/abs/2405.15713v1","category":"astro-ph.GA"}
{"created":"2024-05-24 17:01:37","title":"Infinite Limits of Multi-head Transformer Dynamics","abstract":"In this work, we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime. We identify the set of parameterizations that admit well-defined infinite width and depth limits, allowing the attention layers to update throughout training--a relevant notion of feature learning in these models. We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite key/query dimension, infinite heads, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled. We provide numerical evidence of convergence to the limits and discuss how the parameterization qualitatively influences learned features.","sentences":["In this work, we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime.","We identify the set of parameterizations that admit well-defined infinite width and depth limits, allowing the attention layers to update throughout training--a relevant notion of feature learning in these models.","We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite key/query dimension, infinite heads, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled.","We provide numerical evidence of convergence to the limits and discuss how the parameterization qualitatively influences learned features."],"url":"http://arxiv.org/abs/2405.15712v1","category":"stat.ML"}
{"created":"2024-05-24 17:01:12","title":"An Adaptive Framework for Manipulator Skill Reproduction in Dynamic Environments","abstract":"Robot skill learning and execution in uncertain and dynamic environments is a challenging task. This paper proposes an adaptive framework that combines Learning from Demonstration (LfD), environment state prediction, and high-level decision making. Proactive adaptation prevents the need for reactive adaptation, which lags behind changes in the environment rather than anticipating them. We propose a novel LfD representation, Elastic-Laplacian Trajectory Editing (ELTE), which continuously adapts the trajectory shape to predictions of future states. Then, a high-level reactive system using an Unscented Kalman Filter (UKF) and Hidden Markov Model (HMM) prevents unsafe execution in the current state of the dynamic environment based on a discrete set of decisions. We first validate our LfD representation in simulation, then experimentally assess the entire framework using a legged mobile manipulator in 36 real-world scenarios. We show the effectiveness of the proposed framework under different dynamic changes in the environment. Our results show that the proposed framework produces robust and stable adaptive behaviors.","sentences":["Robot skill learning and execution in uncertain and dynamic environments is a challenging task.","This paper proposes an adaptive framework that combines Learning from Demonstration (LfD), environment state prediction, and high-level decision making.","Proactive adaptation prevents the need for reactive adaptation, which lags behind changes in the environment rather than anticipating them.","We propose a novel LfD representation, Elastic-Laplacian Trajectory Editing (ELTE), which continuously adapts the trajectory shape to predictions of future states.","Then, a high-level reactive system using an Unscented Kalman Filter (UKF) and Hidden Markov Model (HMM) prevents unsafe execution in the current state of the dynamic environment based on a discrete set of decisions.","We first validate our LfD representation in simulation, then experimentally assess the entire framework using a legged mobile manipulator in 36 real-world scenarios.","We show the effectiveness of the proposed framework under different dynamic changes in the environment.","Our results show that the proposed framework produces robust and stable adaptive behaviors."],"url":"http://arxiv.org/abs/2405.15711v1","category":"cs.RO"}
{"created":"2024-05-24 16:52:04","title":"Sums: Sniffing Unknown Multiband Signals under Low Sampling Rates","abstract":"Due to sophisticated deployments of all kinds of wireless networks (e.g., 5G, Wi-Fi, Bluetooth, LEO satellite, etc.), multiband signals distribute in a large bandwidth (e.g., from 70 MHz to 8 GHz). Consequently, for network monitoring and spectrum sharing applications, a sniffer for extracting physical layer information, such as structure of packet, with low sampling rate (especially, sub-Nyquist sampling) can significantly improve their cost- and energy-efficiency. However, to achieve a multiband signals sniffer is really a challenge. To this end, we propose Sums, a system that can sniff and analyze multiband signals in a blind manner. Our Sums takes advantage of hardware and algorithm co-design, multi-coset sub-Nyquist sampling hardware, and a multi-task deep learning framework. The hardware component breaks the Nyquist rule to sample GHz bandwidth, but only pays for a 50 MSPS sampling rate. Our multi-task learning framework directly tackles the sampling data to perform spectrum sensing, physical layer protocol recognition, and demodulation for deep inspection from multiband signals. Extensive experiments demonstrate that Sums achieves higher accuracy than the state-of-theart baselines in spectrum sensing, modulation classification, and demodulation. As a result, our Sums can help researchers and end-users to diagnose or troubleshoot their problems of wireless infrastructures deployments in practice.","sentences":["Due to sophisticated deployments of all kinds of wireless networks (e.g., 5G, Wi-Fi, Bluetooth, LEO satellite, etc.), multiband signals distribute in a large bandwidth (e.g., from 70 MHz to 8 GHz).","Consequently, for network monitoring and spectrum sharing applications, a sniffer for extracting physical layer information, such as structure of packet, with low sampling rate (especially, sub-Nyquist sampling) can significantly improve their cost- and energy-efficiency.","However, to achieve a multiband signals sniffer is really a challenge.","To this end, we propose Sums, a system that can sniff and analyze multiband signals in a blind manner.","Our Sums takes advantage of hardware and algorithm co-design, multi-coset sub-Nyquist sampling hardware, and a multi-task deep learning framework.","The hardware component breaks the Nyquist rule to sample GHz bandwidth, but only pays for a 50 MSPS sampling rate.","Our multi-task learning framework directly tackles the sampling data to perform spectrum sensing, physical layer protocol recognition, and demodulation for deep inspection from multiband signals.","Extensive experiments demonstrate that Sums achieves higher accuracy than the state-of-theart baselines in spectrum sensing, modulation classification, and demodulation.","As a result, our Sums can help researchers and end-users to diagnose or troubleshoot their problems of wireless infrastructures deployments in practice."],"url":"http://arxiv.org/abs/2405.15705v1","category":"cs.AR"}
{"created":"2024-05-24 16:32:33","title":"An operational distinction between quantum entanglement and classical non-separability","abstract":"Quantum entanglement describes superposition states in multi-dimensional systems, at least two partite, which cannot be factorized and are thus non-separable. Non-separable states exist also in classical theories involving vector spaces. In both cases, it is possible to violate a Bell-like inequality. This has led to controversial discussions, which we resolve by identifying an operational distinction between the classical and quantum cases.","sentences":["Quantum entanglement describes superposition states in multi-dimensional systems, at least two partite, which cannot be factorized and are thus non-separable.","Non-separable states exist also in classical theories involving vector spaces.","In both cases, it is possible to violate a Bell-like inequality.","This has led to controversial discussions, which we resolve by identifying an operational distinction between the classical and quantum cases."],"url":"http://arxiv.org/abs/2405.15692v1","category":"quant-ph"}
{"created":"2024-05-24 16:26:21","title":"Stochastic thermodynamics of Fisher information","abstract":"In this manuscript, we investigate the stochastic thermodynamics of Fisher information (FI), meaning we characterize both the \\textit{fluctuations} of FI, introducing a parastatistics of that quantity, and thermodynamic quantities. We introduce two initial conditions: an equilibrium initial condition and a minimum entropy initial condition, both under a protocol that drives the system to equilibrium. Its results indicate a dependence of the average FI on both the initial condition and path taken. Furthermore, the results point the chosen parameter directly affects the FI of thermodynamic quantities such as irreversible work and entropy, along with fluctuations of a stochastic FI. Last, we assess the further role of FI of the distribution of thermal quantities within the context of thermostatistical inequalities.","sentences":["In this manuscript, we investigate the stochastic thermodynamics of Fisher information (FI), meaning we characterize both the \\textit{fluctuations} of FI, introducing a parastatistics of that quantity, and thermodynamic quantities.","We introduce two initial conditions: an equilibrium initial condition and a minimum entropy initial condition, both under a protocol that drives the system to equilibrium.","Its results indicate a dependence of the average FI on both the initial condition and path taken.","Furthermore, the results point the chosen parameter directly affects the FI of thermodynamic quantities such as irreversible work and entropy, along with fluctuations of a stochastic FI.","Last, we assess the further role of FI of the distribution of thermal quantities within the context of thermostatistical inequalities."],"url":"http://arxiv.org/abs/2405.15685v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-24 16:17:01","title":"Taming Score-Based Diffusion Priors for Infinite-Dimensional Nonlinear Inverse Problems","abstract":"This work introduces a sampling method capable of solving Bayesian inverse problems in function space. It does not assume the log-concavity of the likelihood, meaning that it is compatible with nonlinear inverse problems. The method leverages the recently defined infinite-dimensional score-based diffusion models as a learning-based prior, while enabling provable posterior sampling through a Langevin-type MCMC algorithm defined on function spaces. A novel convergence analysis is conducted, inspired by the fixed-point methods established for traditional regularization-by-denoising algorithms and compatible with weighted annealing. The obtained convergence bound explicitly depends on the approximation error of the score; a well-approximated score is essential to obtain a well-approximated posterior. Stylized and PDE-based examples are provided, demonstrating the validity of our convergence analysis. We conclude by presenting a discussion of the method's challenges related to learning the score and computational complexity.","sentences":["This work introduces a sampling method capable of solving Bayesian inverse problems in function space.","It does not assume the log-concavity of the likelihood, meaning that it is compatible with nonlinear inverse problems.","The method leverages the recently defined infinite-dimensional score-based diffusion models as a learning-based prior, while enabling provable posterior sampling through a Langevin-type MCMC algorithm defined on function spaces.","A novel convergence analysis is conducted, inspired by the fixed-point methods established for traditional regularization-by-denoising algorithms and compatible with weighted annealing.","The obtained convergence bound explicitly depends on the approximation error of the score; a well-approximated score is essential to obtain a well-approximated posterior.","Stylized and PDE-based examples are provided, demonstrating the validity of our convergence analysis.","We conclude by presenting a discussion of the method's challenges related to learning the score and computational complexity."],"url":"http://arxiv.org/abs/2405.15676v1","category":"stat.ML"}
{"created":"2024-05-24 16:06:01","title":"Enhancing Reentry Support Programs Through Digital Literacy Integration","abstract":"Challenges faced by formerly incarcerated individuals in the United States raise questions about our society's ability to truly provide second chances. This paper presents the outcomes of our ongoing collaboration with a non-profit organization dedicated to reentry support. We highlight the multifaceted challenges individuals face during their reentry journey, including support programs that prioritize supervision over service, unresponsive support systems, limited access to resources, financial struggles exacerbated by restricted employment opportunities, and technological barriers. In the face of such complex social challenges, our work aims to facilitate our partner organization's ongoing efforts to promote digital literacy through a web application that is integrated into their existing processes. We share initial feedback from the stakeholders, draw out four implications: supporting continuity of care, promoting reflection through slow technology, building in flexibility, and reconfiguring toward existing infrastructure, and conclude with a reflection on our role as partners on the side.","sentences":["Challenges faced by formerly incarcerated individuals in the United States raise questions about our society's ability to truly provide second chances.","This paper presents the outcomes of our ongoing collaboration with a non-profit organization dedicated to reentry support.","We highlight the multifaceted challenges individuals face during their reentry journey, including support programs that prioritize supervision over service, unresponsive support systems, limited access to resources, financial struggles exacerbated by restricted employment opportunities, and technological barriers.","In the face of such complex social challenges, our work aims to facilitate our partner organization's ongoing efforts to promote digital literacy through a web application that is integrated into their existing processes.","We share initial feedback from the stakeholders, draw out four implications: supporting continuity of care, promoting reflection through slow technology, building in flexibility, and reconfiguring toward existing infrastructure, and conclude with a reflection on our role as partners on the side."],"url":"http://arxiv.org/abs/2405.15669v1","category":"cs.HC"}
{"created":"2024-05-24 16:04:36","title":"Grand orbit relations in wandering domains","abstract":"One of the fundamental distinctions in McMullen and Sullivan's description of the Teichm\\\"uller space of a complex dynamical system is between discrete and indiscrete grand orbit relations. We investigate these on the Fatou set of transcendental entire maps and provide criteria to distinguish between the two types. Furthermore, we show that discrete and indiscrete grand orbit relations may coexist non-trivially in a wandering domain, a phenomenon which does not occur for any other type of Fatou component. One of the tools used is a novel quasiconformal surgery technique of independent interest.","sentences":["One of the fundamental distinctions in McMullen and Sullivan's description of the Teichm\\\"uller space of a complex dynamical system is between discrete and indiscrete grand orbit relations.","We investigate these on the Fatou set of transcendental entire maps and provide criteria to distinguish between the two types.","Furthermore, we show that discrete and indiscrete grand orbit relations may coexist non-trivially in a wandering domain, a phenomenon which does not occur for any other type of Fatou component.","One of the tools used is a novel quasiconformal surgery technique of independent interest."],"url":"http://arxiv.org/abs/2405.15667v1","category":"math.DS"}
{"created":"2024-05-24 16:02:44","title":"GroundGrid:LiDAR Point Cloud Ground Segmentation and Terrain Estimation","abstract":"The precise point cloud ground segmentation is a crucial prerequisite of virtually all perception tasks for LiDAR sensors in autonomous vehicles. Especially the clustering and extraction of objects from a point cloud usually relies on an accurate removal of ground points. The correct estimation of the surrounding terrain is important for aspects of the drivability of a surface, path planning, and obstacle prediction. In this article, we propose our system GroundGrid which relies on 2D elevation maps to solve the terrain estimation and point cloud ground segmentation problems. We evaluate the ground segmentation and terrain estimation performance of GroundGrid and compare it to other state-of-the-art methods using the SemanticKITTI dataset and a novel evaluation method relying on airborne LiDAR scanning. The results show that GroundGrid is capable of outperforming other state-of-the-art systems with an average IoU of 94.78% while maintaining a high run-time performance of 171Hz. The source code is available at https://github.com/dcmlr/groundgrid","sentences":["The precise point cloud ground segmentation is a crucial prerequisite of virtually all perception tasks for LiDAR sensors in autonomous vehicles.","Especially the clustering and extraction of objects from a point cloud usually relies on an accurate removal of ground points.","The correct estimation of the surrounding terrain is important for aspects of the drivability of a surface, path planning, and obstacle prediction.","In this article, we propose our system GroundGrid which relies on 2D elevation maps to solve the terrain estimation and point cloud ground segmentation problems.","We evaluate the ground segmentation and terrain estimation performance of GroundGrid and compare it to other state-of-the-art methods using the SemanticKITTI dataset and a novel evaluation method relying on airborne LiDAR scanning.","The results show that GroundGrid is capable of outperforming other state-of-the-art systems with an average IoU of 94.78% while maintaining a high run-time performance of 171Hz.","The source code is available at https://github.com/dcmlr/groundgrid"],"url":"http://arxiv.org/abs/2405.15664v1","category":"cs.RO"}
{"created":"2024-05-24 15:59:17","title":"Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning","abstract":"In current AI era, users may request AI companies to delete their data from the training dataset due to the privacy concerns. As a model owner, retraining a model will consume significant computational resources. Therefore, machine unlearning is a new emerged technology to allow model owner to delete requested training data or a class with little affecting on the model performance. However, for large-scaling complex data, such as image or text data, unlearning a class from a model leads to a inferior performance due to the difficulty to identify the link between classes and model. An inaccurate class deleting may lead to over or under unlearning. In this paper, to accurately defining the unlearning class of complex data, we apply the definition of Concept, rather than an image feature or a token of text data, to represent the semantic information of unlearning class. This new representation can cut the link between the model and the class, leading to a complete erasing of the impact of a class. To analyze the impact of the concept of complex data, we adopt a Post-hoc Concept Bottleneck Model, and Integrated Gradients to precisely identify concepts across different classes. Next, we take advantage of data poisoning with random and targeted labels to propose unlearning methods. We test our methods on both image classification models and large language models (LLMs). The results consistently show that the proposed methods can accurately erase targeted information from models and can largely maintain the performance of the models.","sentences":["In current AI era, users may request AI companies to delete their data from the training dataset due to the privacy concerns.","As a model owner, retraining a model will consume significant computational resources.","Therefore, machine unlearning is a new emerged technology to allow model owner to delete requested training data or a class with little affecting on the model performance.","However, for large-scaling complex data, such as image or text data, unlearning a class from a model leads to a inferior performance due to the difficulty to identify the link between classes and model.","An inaccurate class deleting may lead to over or under unlearning.","In this paper, to accurately defining the unlearning class of complex data, we apply the definition of Concept, rather than an image feature or a token of text data, to represent the semantic information of unlearning class.","This new representation can cut the link between the model and the class, leading to a complete erasing of the impact of a class.","To analyze the impact of the concept of complex data, we adopt a Post-hoc Concept Bottleneck Model, and Integrated Gradients to precisely identify concepts across different classes.","Next, we take advantage of data poisoning with random and targeted labels to propose unlearning methods.","We test our methods on both image classification models and large language models (LLMs).","The results consistently show that the proposed methods can accurately erase targeted information from models and can largely maintain the performance of the models."],"url":"http://arxiv.org/abs/2405.15662v1","category":"cs.LG"}
{"created":"2024-05-24 15:48:06","title":"Interfacially enhanced superconductivity in Fe(Te,Se)/Bi4Te3 heterostructures","abstract":"Realizing topological superconductivity by integrating high-transition-temperature ($T_C$) superconductors with topological insulators can open new paths for quantum computing applications. Here, we report a new approach for increasing the superconducting transition temperature ($T_{C}^{onset}$) by interfacing the unconventional superconductor Fe(Te,Se) with the topological insulator Bi-Te system in the low-Se doping regime, near where superconductivity vanishes in the bulk. The critical finding is that the $T_{C}^{onset}$ of Fe(Te,Se) increases from nominally non-superconducting to as high as 12.5 K when $Bi_2Te_3$ is replaced with the topological phase $Bi_4Te_3$. Interfacing Fe(Te,Se) with $Bi_4Te_3$ is also found to be critical for stabilizing superconductivity in monolayer films where $T_{C}^{onset}$ can be as high as 6 K. Measurements of the electronic and crystalline structure of the $Bi_4Te_3$ layer reveal that a large electron transfer, epitaxial strain, and novel chemical reduction processes are critical factors for the enhancement of superconductivity. This novel route for enhancing $T_C$ in an important epitaxial system provides new insight on the nature of interfacial superconductivity and a platform to identify and utilize new electronic phases.","sentences":["Realizing topological superconductivity by integrating high-transition-temperature ($T_C$) superconductors with topological insulators can open new paths for quantum computing applications.","Here, we report a new approach for increasing the superconducting transition temperature ($T_{C}^{onset}$) by interfacing the unconventional superconductor Fe(Te,Se) with the topological insulator Bi-Te system in the low-Se doping regime, near where superconductivity vanishes in the bulk.","The critical finding is that the $T_{C}^{onset}$ of Fe(Te,Se) increases from nominally non-superconducting to as high as 12.5 K when $Bi_2Te_3$ is replaced with the topological phase $Bi_4Te_3$. Interfacing Fe(Te,Se) with $Bi_4Te_3$ is also found to be critical for stabilizing superconductivity in monolayer films where $T_{C}^{onset}$ can be as high as 6 K. Measurements of the electronic and crystalline structure of the $Bi_4Te_3$ layer reveal that a large electron transfer, epitaxial strain, and novel chemical reduction processes are critical factors for the enhancement of superconductivity.","This novel route for enhancing $T_C$ in an important epitaxial system provides new insight on the nature of interfacial superconductivity and a platform to identify and utilize new electronic phases."],"url":"http://arxiv.org/abs/2405.15654v1","category":"cond-mat.supr-con"}
{"created":"2024-05-24 15:40:01","title":"A journey on self-$G$-ality","abstract":"We explore topological manipulations in one spatial dimension, which are defined for a system with a global symmetry and map the system to another one with a dual symmetry. In particular, we discuss fusion category symmetries enhanced by the invariance of the actions of topological manipulations, i.e., self-$G$-alities for topological manipulations. Based on the self-$G$-ality conditions, we provide LSM-type constraints on the ground states of many-body Hamiltonians. We clarify the relationship between different enhanced symmetries and introduce the notion of $\\textit{codimension-two transitions}$. We explore concrete lattice models for such self-$G$-alities and find how the self-$G$-ality structures match the IR critical theories.","sentences":["We explore topological manipulations in one spatial dimension, which are defined for a system with a global symmetry and map the system to another one with a dual symmetry.","In particular, we discuss fusion category symmetries enhanced by the invariance of the actions of topological manipulations, i.e., self-$G$-alities for topological manipulations.","Based on the self-$G$-ality conditions, we provide LSM-type constraints on the ground states of many-body Hamiltonians.","We clarify the relationship between different enhanced symmetries and introduce the notion of $\\textit{codimension-two transitions}$. We explore concrete lattice models for such self-$G$-alities and find how the self-$G$-ality structures match the IR critical theories."],"url":"http://arxiv.org/abs/2405.15648v1","category":"cond-mat.str-el"}
{"created":"2024-05-24 15:37:55","title":"A Logic of Knowledge and Justifications, with an Application to Computational Trust","abstract":"We present a logical framework that enables us to define a formal theory of computational trust in which this notion is analysed in terms of epistemic attitudes towards the possible objects of trust and in relation to existing evidence in favour of the trustworthiness of these objects. The framework is based on a quantified epistemic and justification logic featuring a non-standard handling of identities. Thus, the theory is able to account for the hyperintensional nature of computational trust. We present a proof system and a frame semantics for the logic, we prove soundness and completeness results and we introduce the syntactical machinery required to define a theory of trust.","sentences":["We present a logical framework that enables us to define a formal theory of computational trust in which this notion is analysed in terms of epistemic attitudes towards the possible objects of trust and in relation to existing evidence in favour of the trustworthiness of these objects.","The framework is based on a quantified epistemic and justification logic featuring a non-standard handling of identities.","Thus, the theory is able to account for the hyperintensional nature of computational trust.","We present a proof system and a frame semantics for the logic, we prove soundness and completeness results and we introduce the syntactical machinery required to define a theory of trust."],"url":"http://arxiv.org/abs/2405.15647v1","category":"cs.LO"}
{"created":"2024-05-24 15:34:25","title":"An Online Probabilistic Distributed Tracing System","abstract":"Distributed tracing has become a fundamental tool for diagnosing performance issues in the cloud by recording causally ordered, end-to-end workflows of request executions. However, tracing in production workloads can introduce significant overheads due to the extensive instrumentation needed for identifying performance variations. This paper addresses the trade-off between the cost of tracing and the utility of the \"spans\" within that trace through Astraea, an online probabilistic distributed tracing system. Astraea is based on our technique that combines online Bayesian learning and multi-armed bandit frameworks. This formulation enables Astraea to effectively steer tracing towards the useful instrumentation needed for accurate performance diagnosis. Astraea localizes performance variations using only 10-28% of available instrumentation, markedly reducing tracing overhead, storage, compute costs, and trace analysis time.","sentences":["Distributed tracing has become a fundamental tool for diagnosing performance issues in the cloud by recording causally ordered, end-to-end workflows of request executions.","However, tracing in production workloads can introduce significant overheads due to the extensive instrumentation needed for identifying performance variations.","This paper addresses the trade-off between the cost of tracing and the utility of the \"spans\" within that trace through Astraea, an online probabilistic distributed tracing system.","Astraea is based on our technique that combines online Bayesian learning and multi-armed bandit frameworks.","This formulation enables Astraea to effectively steer tracing towards the useful instrumentation needed for accurate performance diagnosis.","Astraea localizes performance variations using only 10-28% of available instrumentation, markedly reducing tracing overhead, storage, compute costs, and trace analysis time."],"url":"http://arxiv.org/abs/2405.15645v1","category":"cs.PF"}
{"created":"2024-05-24 15:28:00","title":"Well Posed Origin Anywhere Consistent Systems in Celestial Mechanics","abstract":"Certain measurements in celestial mechanics necessitate having the origin O of a Cartesian coordinate system (CCS) coincide with a point mass. For the two and three body problems we show mathematical inadequacies in Newton's celestial mechanics equations (NCME) when the origin of a coordinate system coincides with a point mass. A certain system of equations of relative differences implied by NCME is free of these inadequacies and is invariant with respect to any CCS translation. A new constant of motion is derived for the relative system. It shows that the universe of relative differences of the $N$-body problem is ``restless''.","sentences":["Certain measurements in celestial mechanics necessitate having the origin O of a Cartesian coordinate system (CCS) coincide with a point mass.","For the two and three body problems we show mathematical inadequacies in Newton's celestial mechanics equations (NCME) when the origin of a coordinate system coincides with a point mass.","A certain system of equations of relative differences implied by NCME is free of these inadequacies and is invariant with respect to any CCS translation.","A new constant of motion is derived for the relative system.","It shows that the universe of relative differences of the $N$-body problem is ``restless''."],"url":"http://arxiv.org/abs/2405.15639v1","category":"math-ph"}
{"created":"2024-05-24 15:24:24","title":"Clearing the Path for Software Sustainability","abstract":"The advancement of software sustainability encounters notable challenges, underscoring the necessity for understanding these challenges to facilitate significant progress and pave the way for effective solutions to advance software sustainability. This paper outlines key challenges identified in literature based on findings from a tertiary study. Challenges identified include: confusion regarding the definition of software sustainability, uncertainty about when to consider sustainability in software development, lack of assessment metrics and tools, narrow perspectives on sustainability in software systems, insufficient awareness and education, and a lack of serious considerations in practice. The paper aims at clarifying the confusion surrounding software sustainability to motivate effective solutions. The provided recommendations aim to give a more organized approach towards advancing sustainable software development, emphasizing comprehensive strategies, the integration of sustainability as a fundamental aspect of software development, actionable research directions, and the cultivation of a common understanding of sustainable software.","sentences":["The advancement of software sustainability encounters notable challenges, underscoring the necessity for understanding these challenges to facilitate significant progress and pave the way for effective solutions to advance software sustainability.","This paper outlines key challenges identified in literature based on findings from a tertiary study.","Challenges identified include: confusion regarding the definition of software sustainability, uncertainty about when to consider sustainability in software development, lack of assessment metrics and tools, narrow perspectives on sustainability in software systems, insufficient awareness and education, and a lack of serious considerations in practice.","The paper aims at clarifying the confusion surrounding software sustainability to motivate effective solutions.","The provided recommendations aim to give a more organized approach towards advancing sustainable software development, emphasizing comprehensive strategies, the integration of sustainability as a fundamental aspect of software development, actionable research directions, and the cultivation of a common understanding of sustainable software."],"url":"http://arxiv.org/abs/2405.15637v1","category":"cs.SE"}
{"created":"2024-05-24 15:17:51","title":"An analysis of Wardrop equilibrium and social optimum in congested transit networks","abstract":"The effective design and management of public transport systems are essential to ensure the best service for users. The performance of a transport system will depend heavily on user behaviour. In the common-lines problem approach, users choose which lines to use based on the best strategy for them. While Wardrop equilibrium has been studied for the common-lines problem, no contributions have been made towards achieving the social optimum. In this work, we propose two optimisation problems to obtain this optimum, using strategy flow and line flow formulations. We prove that both optimisation problems are equivalent, and we obtain a characterisation of the social optimum flows. The social optimum makes it possible to compute the price of anarchy (PoA), which quantifies the system's efficiency. The study of the PoA enables the effective design and management of public transport systems, guaranteeing the best service to users.","sentences":["The effective design and management of public transport systems are essential to ensure the best service for users.","The performance of a transport system will depend heavily on user behaviour.","In the common-lines problem approach, users choose which lines to use based on the best strategy for them.","While Wardrop equilibrium has been studied for the common-lines problem, no contributions have been made towards achieving the social optimum.","In this work, we propose two optimisation problems to obtain this optimum, using strategy flow and line flow formulations.","We prove that both optimisation problems are equivalent, and we obtain a characterisation of the social optimum flows.","The social optimum makes it possible to compute the price of anarchy (PoA), which quantifies the system's efficiency.","The study of the PoA enables the effective design and management of public transport systems, guaranteeing the best service to users."],"url":"http://arxiv.org/abs/2405.15631v1","category":"math.OC"}
{"created":"2024-05-24 15:17:51","title":"Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning","abstract":"Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms.","sentences":["Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks.","However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature.","To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space).","Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours.","Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms."],"url":"http://arxiv.org/abs/2405.15632v1","category":"cs.LG"}
{"created":"2024-05-24 15:10:31","title":"Pseudo-easy-axis anisotropy in antiferromagnetic $S=1$ diamond-lattice systems","abstract":"We investigate the magnetic properties of $S=1$ antiferromagnetic diamond lattice, Ni$X_{2}$(pyrimidine)$_{2}$ ($X$ = Cl, Br), hosting a single-ion anisotropy (SIA) orientation which alternates between neighbouring sites. Through neutron diffraction measurements of the $X$ = Cl compound, the ordered state spins are found to align collinearly along a pseudo-easy-axis, a unique direction created by the intersection of two easy planes. Similarities in the magnetization, exhibiting spin-flop transitions, and the magnetic susceptibility in the two compounds imply that the same magnetic structure and a pseudo-easy-axis is also present for $X$ = Br. We estimate the Hamiltonian parameters by combining analytical calculations and Monte-Carlo (MC) simulations of the spin-flop and saturation field. The MC simulations also reveal that the spin-flop transition occurs when the applied field is parallel to the pseudo-easy-axis. Contrary to conventional easy-axis systems, there exist field directions perpendicular to the pseudo-easy-axis for which the magnetic saturation is approached asymptotically and no symmetry-breaking phase transition is observed at finite fields.","sentences":["We investigate the magnetic properties of $S=1$ antiferromagnetic diamond lattice, Ni$X_{2}$(pyrimidine)$_{2}$ ($X$ = Cl, Br), hosting a single-ion anisotropy (SIA) orientation which alternates between neighbouring sites.","Through neutron diffraction measurements of the $X$ = Cl compound, the ordered state spins are found to align collinearly along a pseudo-easy-axis, a unique direction created by the intersection of two easy planes.","Similarities in the magnetization, exhibiting spin-flop transitions, and the magnetic susceptibility in the two compounds imply that the same magnetic structure and a pseudo-easy-axis is also present for $X$ = Br.","We estimate the Hamiltonian parameters by combining analytical calculations and Monte-Carlo (MC) simulations of the spin-flop and saturation field.","The MC simulations also reveal that the spin-flop transition occurs when the applied field is parallel to the pseudo-easy-axis.","Contrary to conventional easy-axis systems, there exist field directions perpendicular to the pseudo-easy-axis for which the magnetic saturation is approached asymptotically and no symmetry-breaking phase transition is observed at finite fields."],"url":"http://arxiv.org/abs/2405.15623v1","category":"cond-mat.str-el"}
{"created":"2024-05-24 14:57:31","title":"Dual opposing quadrature-PT symmetry","abstract":"Our recent research on type-I quadrature parity-time (PT) symmetry, utilizing an open twin-beam system, not only enables observing genuine quantum photonic PT symmetry amid phase-sensitive amplification (PSA) and loss in the presence of Langevin noise but also reveals additional classical-to-quantum (C2Q) transitions in quadrature and relative-intensity noise fluctuations. In contrast to the previous setup, our exploration of an alternative system assuming no loss involves a type-II PSA-only scheme. This scheme facilitates dual opposing quadrature PT symmetry, offering a comprehensive and complementary comprehension of C2Q transitions and anti-Hermiticity-enhanced quantum sensing. Furthermore, our investigation into the correlation with the Einstein-Podolsky-Rosen criteria uncovers previously unexplored connections between PT symmetry and nonclassicality, as well as quantum entanglement within the continuous-variable framework.","sentences":["Our recent research on type-I quadrature parity-time (PT) symmetry, utilizing an open twin-beam system, not only enables observing genuine quantum photonic PT symmetry amid phase-sensitive amplification (PSA) and loss in the presence of Langevin noise but also reveals additional classical-to-quantum (C2Q) transitions in quadrature and relative-intensity noise fluctuations.","In contrast to the previous setup, our exploration of an alternative system assuming no loss involves a type-II PSA-only scheme.","This scheme facilitates dual opposing quadrature PT symmetry, offering a comprehensive and complementary comprehension of C2Q transitions and anti-Hermiticity-enhanced quantum sensing.","Furthermore, our investigation into the correlation with the Einstein-Podolsky-Rosen criteria uncovers previously unexplored connections between PT symmetry and nonclassicality, as well as quantum entanglement within the continuous-variable framework."],"url":"http://arxiv.org/abs/2405.15612v1","category":"quant-ph"}
{"created":"2024-05-24 14:43:37","title":"Fast-PGM: Fast Probabilistic Graphical Model Learning and Inference","abstract":"Probabilistic graphical models (PGMs) serve as a powerful framework for modeling complex systems with uncertainty and extracting valuable insights from data. However, users face challenges when applying PGMs to their problems in terms of efficiency and usability. This paper presents Fast-PGM, an efficient and open-source library for PGM learning and inference. Fast-PGM supports comprehensive tasks on PGMs, including structure and parameter learning, as well as exact and approximate inference, and enhances efficiency of the tasks through computational and memory optimizations and parallelization techniques. Concurrently, Fast-PGM furnishes developers with flexible building blocks, furnishes learners with detailed documentation, and affords non-experts user-friendly interfaces, thereby ameliorating the usability of PGMs to users across a spectrum of expertise levels. The source code of Fast-PGM is available at https://github.com/jjiantong/FastPGM.","sentences":["Probabilistic graphical models (PGMs) serve as a powerful framework for modeling complex systems with uncertainty and extracting valuable insights from data.","However, users face challenges when applying PGMs to their problems in terms of efficiency and usability.","This paper presents Fast-PGM, an efficient and open-source library for PGM learning and inference.","Fast-PGM supports comprehensive tasks on PGMs, including structure and parameter learning, as well as exact and approximate inference, and enhances efficiency of the tasks through computational and memory optimizations and parallelization techniques.","Concurrently, Fast-PGM furnishes developers with flexible building blocks, furnishes learners with detailed documentation, and affords non-experts user-friendly interfaces, thereby ameliorating the usability of PGMs to users across a spectrum of expertise levels.","The source code of Fast-PGM is available at https://github.com/jjiantong/FastPGM."],"url":"http://arxiv.org/abs/2405.15605v1","category":"cs.LG"}
{"created":"2024-05-24 14:35:12","title":"Far-from-equilibrium travelling pulses in sloped semi-arid environments driven by autotoxicity effects","abstract":"In this work, an extension of the 1D Klausmeier model that accounts for the toxicity compounds is considered and the occurrence of travelling stripes is investigated. Numerical simulations are firstly conducted to capture the qualitative behaviours of the pulse-type solutions and, then, geometric singular perturbation theory is used to prove the existence of such travelling pulses by constructing the corresponding homoclinic orbits in the associated 4-dimensional system. A scaling analysis on the investigated model is performed to identify the asymptotic scaling regime in which travelling pulses can be constructed. Biological observations are extracted from the analytical results and the role of autotoxicity in travelling patterns is emphasized. Finally, the analytically constructed solutions are compared with the numerical ones, leading to a good agreement that confirms the validity of the conducted analysis. Numerical investigations are also carried out in order to gain additional information on vegetation dynamics.","sentences":["In this work, an extension of the 1D Klausmeier model that accounts for the toxicity compounds is considered and the occurrence of travelling stripes is investigated.","Numerical simulations are firstly conducted to capture the qualitative behaviours of the pulse-type solutions and, then, geometric singular perturbation theory is used to prove the existence of such travelling pulses by constructing the corresponding homoclinic orbits in the associated 4-dimensional system.","A scaling analysis on the investigated model is performed to identify the asymptotic scaling regime in which travelling pulses can be constructed.","Biological observations are extracted from the analytical results and the role of autotoxicity in travelling patterns is emphasized.","Finally, the analytically constructed solutions are compared with the numerical ones, leading to a good agreement that confirms the validity of the conducted analysis.","Numerical investigations are also carried out in order to gain additional information on vegetation dynamics."],"url":"http://arxiv.org/abs/2405.15602v1","category":"math.DS"}
{"created":"2024-05-24 14:30:40","title":"On the Computational Landscape of Replicable Learning","abstract":"We study computational aspects of algorithmic replicability, a notion of stability introduced by Impagliazzo, Lei, Pitassi, and Sorrell [2022]. Motivated by a recent line of work that established strong statistical connections between replicability and other notions of learnability such as online learning, private learning, and SQ learning, we aim to understand better the computational connections between replicability and these learning paradigms. Our first result shows that there is a concept class that is efficiently replicably PAC learnable, but, under standard cryptographic assumptions, no efficient online learner exists for this class. Subsequently, we design an efficient replicable learner for PAC learning parities when the marginal distribution is far from uniform, making progress on a question posed by Impagliazzo et al. [2022]. To obtain this result, we design a replicable lifting framework inspired by Blanc, Lange, Malik, and Tan [2023] that transforms in a black-box manner efficient replicable PAC learners under the uniform marginal distribution over the Boolean hypercube to replicable PAC learners under any marginal distribution, with sample and time complexity that depends on a certain measure of the complexity of the distribution. Finally, we show that any pure DP learner can be transformed to a replicable one in time polynomial in the accuracy, confidence parameters and exponential in the representation dimension of the underlying hypothesis class.","sentences":["We study computational aspects of algorithmic replicability, a notion of stability introduced by Impagliazzo, Lei, Pitassi, and Sorrell","[2022].","Motivated by a recent line of work that established strong statistical connections between replicability and other notions of learnability such as online learning, private learning, and SQ learning, we aim to understand better the computational connections between replicability and these learning paradigms.","Our first result shows that there is a concept class that is efficiently replicably PAC learnable, but, under standard cryptographic assumptions, no efficient online learner exists for this class.","Subsequently, we design an efficient replicable learner for PAC learning parities when the marginal distribution is far from uniform, making progress on a question posed by Impagliazzo et al.","[2022].","To obtain this result, we design a replicable lifting framework inspired by Blanc, Lange, Malik, and Tan [2023] that transforms in a black-box manner efficient replicable PAC learners under the uniform marginal distribution over the Boolean hypercube to replicable PAC learners under any marginal distribution, with sample and time complexity that depends on a certain measure of the complexity of the distribution.","Finally, we show that any pure DP learner can be transformed to a replicable one in time polynomial in the accuracy, confidence parameters and exponential in the representation dimension of the underlying hypothesis class."],"url":"http://arxiv.org/abs/2405.15599v1","category":"cs.LG"}
{"created":"2024-05-24 14:13:54","title":"Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems","abstract":"Large language models (LLM) based end-to-end task-oriented dialog (TOD) systems built using few-shot (in-context) learning perform better than supervised models only when the train data is limited. This is due to the inherent ability of LLMs to learn any task with just a few demonstrations. As the number of train dialogs increases, supervised SoTA models surpass in-context learning LLMs as they learn to better align with the style of the system responses in the training data, which LLMs struggle to mimic. In response, we propose SyncTOD, which synergizes LLMs with useful hints about the task for improved alignment. At a high level, SyncTOD trains auxiliary models to provide these hints and select exemplars for the in-context prompts. With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings","sentences":["Large language models (LLM) based end-to-end task-oriented dialog (TOD) systems built using few-shot (in-context) learning perform better than supervised models only when the train data is limited.","This is due to the inherent ability of LLMs to learn any task with just a few demonstrations.","As the number of train dialogs increases, supervised SoTA models surpass in-context learning LLMs as they learn to better align with the style of the system responses in the training data, which LLMs struggle to mimic.","In response, we propose SyncTOD, which synergizes LLMs with useful hints about the task for improved alignment.","At a high level, SyncTOD trains auxiliary models to provide these hints and select exemplars for the in-context prompts.","With ChatGPT, SyncTOD achieves superior performance compared to LLM-based baselines and SoTA models in low-data settings, while retaining competitive performance in full-data settings"],"url":"http://arxiv.org/abs/2405.15585v1","category":"cs.CL"}
{"created":"2024-05-24 14:06:05","title":"Distributed Locking as a Data Type","abstract":"Mixed-consistency programming models assist programmers in designing applications that provide high availability while still ensuring application-specific safety invariants. However, existing models often make specific system assumptions, such as building on a particular database system or having baked-in coordination strategies. This makes it difficult to apply these strategies in diverse settings, ranging from client/server to ad-hoc peer-to-peer networks.   This work proposes a new strategy for building programmable coordination mechanisms based on the algebraic replicated data types (ARDTs) approach. ARDTs allow for simple and composable implementations of various protocols, while making minimal assumptions about the network environment. As a case study, two different locking protocols are presented, both implemented as ARDTs. In addition, we elaborate on our ongoing efforts to integrate the approach into the LoRe mixed-consistency programming language.","sentences":["Mixed-consistency programming models assist programmers in designing applications that provide high availability while still ensuring application-specific safety invariants.","However, existing models often make specific system assumptions, such as building on a particular database system or having baked-in coordination strategies.","This makes it difficult to apply these strategies in diverse settings, ranging from client/server to ad-hoc peer-to-peer networks.   ","This work proposes a new strategy for building programmable coordination mechanisms based on the algebraic replicated data types (ARDTs) approach.","ARDTs allow for simple and composable implementations of various protocols, while making minimal assumptions about the network environment.","As a case study, two different locking protocols are presented, both implemented as ARDTs.","In addition, we elaborate on our ongoing efforts to integrate the approach into the LoRe mixed-consistency programming language."],"url":"http://arxiv.org/abs/2405.15578v1","category":"cs.PL"}
{"created":"2024-05-24 14:05:50","title":"Online Changepoint Detection via Dynamic Mode Decomposition","abstract":"Detecting changes in data streams is a vital task in many applications. There is increasing interest in changepoint detection in the online setting, to enable real-time monitoring and support prompt responses and informed decision-making. Many approaches assume stationary sequences before encountering an abrupt change in the mean or variance. Notably less attention has focused on the challenging case where the monitored sequences exhibit trend, periodicity and seasonality. Dynamic mode decomposition is a data-driven dimensionality reduction technique that extracts the essential components of a dynamical system. We propose a changepoint detection method that leverages this technique to sequentially model the dynamics of a moving window of data and produce a low-rank reconstruction. A change is identified when there is a significant difference between this reconstruction and the observed data, and we provide theoretical justification for this approach. Extensive simulations demonstrate that our approach has superior detection performance compared to other methods for detecting small changes in mean, variance, periodicity, and second-order structure, among others, in data that exhibits seasonality. Results on real-world datasets also show excellent performance compared to contemporary approaches.","sentences":["Detecting changes in data streams is a vital task in many applications.","There is increasing interest in changepoint detection in the online setting, to enable real-time monitoring and support prompt responses and informed decision-making.","Many approaches assume stationary sequences before encountering an abrupt change in the mean or variance.","Notably less attention has focused on the challenging case where the monitored sequences exhibit trend, periodicity and seasonality.","Dynamic mode decomposition is a data-driven dimensionality reduction technique that extracts the essential components of a dynamical system.","We propose a changepoint detection method that leverages this technique to sequentially model the dynamics of a moving window of data and produce a low-rank reconstruction.","A change is identified when there is a significant difference between this reconstruction and the observed data, and we provide theoretical justification for this approach.","Extensive simulations demonstrate that our approach has superior detection performance compared to other methods for detecting small changes in mean, variance, periodicity, and second-order structure, among others, in data that exhibits seasonality.","Results on real-world datasets also show excellent performance compared to contemporary approaches."],"url":"http://arxiv.org/abs/2405.15576v1","category":"stat.ME"}
{"created":"2024-05-24 14:03:42","title":"Uniform $\\mathcal{H}$-matrix Compression with Applications to Boundary Integral Equations","abstract":"Boundary integral equation formulations of elliptic partial differential equations lead to dense system matrices when discretized, yet they are data-sparse. Using the $\\mathcal{H}$-matrix format, this sparsity is exploited to achieve $\\mathcal{O}(N\\log N)$ complexity for storage and multiplication by a vector. This is achieved purely algebraically, based on low-rank approximations of subblocks, and hence the format is also applicable to a wider range of problems. The $\\mathcal{H}^2$-matrix format improves the complexity to $\\mathcal{O}(N)$ by introducing a recursive structure onto subblocks on multiple levels. However, in practice this comes with a large proportionality constant, making the $\\mathcal{H}^2$-matrix format advantageous mostly for large problems. In this paper we investigate the usefulness of a matrix format that lies in between these two: Uniform $\\mathcal{H}$-matrices. An algebraic compression algorithm is introduced to transform a regular $\\mathcal{H}$-matrix into a uniform $\\mathcal{H}$-matrix, which maintains the asymptotic complexity.","sentences":["Boundary integral equation formulations of elliptic partial differential equations lead to dense system matrices when discretized, yet they are data-sparse.","Using the $\\mathcal{H}$-matrix format, this sparsity is exploited to achieve $\\mathcal{O}(N\\log N)$ complexity for storage and multiplication by a vector.","This is achieved purely algebraically, based on low-rank approximations of subblocks, and hence the format is also applicable to a wider range of problems.","The $\\mathcal{H}^2$-matrix format improves the complexity to $\\mathcal{O}(N)$ by introducing a recursive structure onto subblocks on multiple levels.","However, in practice this comes with a large proportionality constant, making the $\\mathcal{H}^2$-matrix format advantageous mostly for large problems.","In this paper we investigate the usefulness of a matrix format that lies in between these two: Uniform $\\mathcal{H}$-matrices.","An algebraic compression algorithm is introduced to transform a regular $\\mathcal{H}$-matrix into a uniform $\\mathcal{H}$-matrix, which maintains the asymptotic complexity."],"url":"http://arxiv.org/abs/2405.15573v1","category":"math.NA"}
{"created":"2024-05-24 14:03:16","title":"Multi-Gigabit Interactive Extended Reality over Millimeter-Wave: An End-to-End System Approach","abstract":"Achieving high-quality wireless interactive Extended Reality (XR) will require multi-gigabit throughput at extremely low latency. The Millimeter-Wave (mmWave) frequency bands, between 24 and 300GHz, can achieve such extreme performance. However, maintaining a consistently high Quality of Experience with highly mobile users is challenging, as mmWave communications are inherently directional. In this work, we present and evaluate an end-to-end approach to such a mmWave-based mobile XR system. We perform a highly realistic simulation of the system, incorporating accurate XR data traffic, detailed mmWave propagation models and actual user motion. We evaluate the impact of the beamforming strategy and frequency on the overall performance. In addition, we provide the first system-level evaluation of the CoVRage algorithm, a proactive and spatially aware user-side beamforming approach designed specifically for highly mobile XR environments.","sentences":["Achieving high-quality wireless interactive Extended Reality (XR) will require multi-gigabit throughput at extremely low latency.","The Millimeter-Wave (mmWave) frequency bands, between 24 and 300GHz, can achieve such extreme performance.","However, maintaining a consistently high Quality of Experience with highly mobile users is challenging, as mmWave communications are inherently directional.","In this work, we present and evaluate an end-to-end approach to such a mmWave-based mobile XR system.","We perform a highly realistic simulation of the system, incorporating accurate XR data traffic, detailed mmWave propagation models and actual user motion.","We evaluate the impact of the beamforming strategy and frequency on the overall performance.","In addition, we provide the first system-level evaluation of the CoVRage algorithm, a proactive and spatially aware user-side beamforming approach designed specifically for highly mobile XR environments."],"url":"http://arxiv.org/abs/2405.15570v1","category":"cs.NI"}
{"created":"2024-05-24 13:49:31","title":"Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration","abstract":"This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.","sentences":["This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD).","The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector.","By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data.","The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs).","The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations."],"url":"http://arxiv.org/abs/2405.15562v1","category":"cs.RO"}
{"created":"2024-05-24 13:47:04","title":"Super-diffusive transport in two-dimensional Fermionic wires","abstract":"We consider a two-dimensional model of a Fermionic wire in contact with reservoirs along its two opposite edges. With the reservoirs biased around a Fermi level, $E$, we study the scaling of the conductance of the wire with its length, $L$ as the width of the wire $W\\rightarrow\\infty$. The wire is disordered along the direction of the transport so the conductance is expected to exponentially decay with the length of the wire. However, we show that our model shows a super-diffusive scaling ($1/L^{1/2}$) of the conductance within $|E|<E_c$. This behavior is attributed to the presence of eigenstates of diverging localization length as $W\\rightarrow\\infty$. At $|E|=E_c$, the conductance behavior is sensitive to the disorder and scales sub-diffusively as $1/L^{3/2}$, and $1/L^{5/2}$ for zero and nonzero expectation value of the disorder. Furthermore, at this Fermi level and at certain points in the parameter space of the wire, the behavior of the conductance is also sensitive to the sign of the expectation value of the disorder. At these points we find $1/L^{7/4}$ for zero expectation value of the disorder and $1/L$, $1/L^{3}$ for different signs of the expectation value of the disorder.","sentences":["We consider a two-dimensional model of a Fermionic wire in contact with reservoirs along its two opposite edges.","With the reservoirs biased around a Fermi level, $E$, we study the scaling of the conductance of the wire with its length, $L$ as the width of the wire $W\\rightarrow\\infty$. The wire is disordered along the direction of the transport so the conductance is expected to exponentially decay with the length of the wire.","However, we show that our model shows a super-diffusive scaling ($1/L^{1/2}$) of the conductance within $|E|<E_c$. This behavior is attributed to the presence of eigenstates of diverging localization length as $W\\rightarrow\\infty$. At $|E|=E_c$, the conductance behavior is sensitive to the disorder and scales sub-diffusively as $1/L^{3/2}$, and $1/L^{5/2}$ for zero and nonzero expectation value of the disorder.","Furthermore, at this Fermi level and at certain points in the parameter space of the wire, the behavior of the conductance is also sensitive to the sign of the expectation value of the disorder.","At these points we find $1/L^{7/4}$ for zero expectation value of the disorder and $1/L$, $1/L^{3}$ for different signs of the expectation value of the disorder."],"url":"http://arxiv.org/abs/2405.15560v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-24 13:44:30","title":"Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers","abstract":"Large linear systems are ubiquitous in modern computational science. The main recipe for solving them is iterative solvers with well-designed preconditioners. Deep learning models may be used to precondition residuals during iteration of such linear solvers as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners of the predefined sparsity pattern. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN. Numerical experiments demonstrate that our approach outperforms both classical methods and neural network-based preconditioning. We also provide a heuristic justification for the loss function used and validate our approach on complex datasets.","sentences":["Large linear systems are ubiquitous in modern computational science.","The main recipe for solving them is iterative solvers with well-designed preconditioners.","Deep learning models may be used to precondition residuals during iteration of such linear solvers as the conjugate gradient (CG) method.","Neural network models require an enormous number of parameters to approximate well in this setup.","Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners of the predefined sparsity pattern.","In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN.","Numerical experiments demonstrate that our approach outperforms both classical methods and neural network-based preconditioning.","We also provide a heuristic justification for the loss function used and validate our approach on complex datasets."],"url":"http://arxiv.org/abs/2405.15557v1","category":"cs.LG"}
{"created":"2024-05-24 13:43:13","title":"ZTF J185259.31+124955.2: A new evolved disc-eclipsing binary system","abstract":"Discs in long-period eclipsing binary systems are rare and can lead to extraordinary eclipsing events. ZTF J185259.31+124955.2 was identified as a candidate disc-eclipsing system through a continuing search programme of ZTF variables with a near-IR excess in the WISE data. Examination of the combined ZTF and ATLAS photometry shows seven eclipses since 2017 with depths of 0\\fm34 in all bands on a period of $289.57\\pm0.09$\\,d. The eclipse width is $\\sim 40$\\,d but this and the profile evolve over time. Comparison with library spectra shows that the spectral energy distribution from the available photometry is consistent with an early K-type giant, and fitting black-body profiles suggests $T_{eff} \\sim 4000$\\,K for the stellar component, with a cool component having $T_{eff} < 500$\\,K. The reddening and distance, and hence the luminosity place the star within the giant branch. The most likely scenario is that the system is in a state of rapid evolution following Case B/C mass transfer into an extended disc around an unseen companion.","sentences":["Discs in long-period eclipsing binary systems are rare and can lead to extraordinary eclipsing events.","ZTF J185259.31+124955.2 was identified as a candidate disc-eclipsing system through a continuing search programme of ZTF variables with a near-IR excess in the WISE data.","Examination of the combined ZTF and ATLAS photometry shows seven eclipses since 2017 with depths of 0\\fm34 in all bands on a period of $289.57\\pm0.09$\\,d.","The eclipse width is $\\sim 40$\\,d but this and the profile evolve over time.","Comparison with library spectra shows that the spectral energy distribution from the available photometry is consistent with an early K-type giant, and fitting black-body profiles suggests $T_{eff} \\sim 4000$\\,K for the stellar component, with a cool component having $T_{eff} < 500$\\,K. The reddening and distance, and hence the luminosity place the star within the giant branch.","The most likely scenario is that the system is in a state of rapid evolution following Case B/C mass transfer into an extended disc around an unseen companion."],"url":"http://arxiv.org/abs/2405.15555v1","category":"astro-ph.SR"}
{"created":"2024-05-24 13:42:43","title":"Massive MIMO-ISAC System With 1-Bit ADCs/DACs","abstract":"This paper investigates a hardware-efficient massive multiple-input multiple-output integrated sensing and communication (MIMO-ISAC) system with 1-bit analog-to-digital converters (ADCs)/digital-to-analog converters (DACs). The proposed system, referred to as 1BitISAC, employs 1-bit DACs at the ISAC transmitter and 1-bit ADCs at the sensing receiver, achieving significant reductions in power consumption and hardware costs. For such kind of systems, two 1BitISAC joint transceiver designs, i.e., i) quality of service constrained 1BitISAC design and ii) quality of detection constrained design, are considered and the corresponding problems are formulated. In order to address these problems, we thoroughly analyze the radar detection performance after 1-bit ADCs quantization and the communication bit error rate. This analysis yields new design insights and leads to unique radar and communication metrics, which enables us to simplify the original problems and employ majorization-minimization and integer linear programming methods to solve the problems. Numerical results are provided to validate the performance analysis of the proposed 1BitISAC and to compare with other ISAC configurations. The superiority of the proposed 1BitISAC system in terms of balancing ISAC performance and energy efficiency is also demonstrated.","sentences":["This paper investigates a hardware-efficient massive multiple-input multiple-output integrated sensing and communication (MIMO-ISAC) system with 1-bit analog-to-digital converters (ADCs)/digital-to-analog converters (DACs).","The proposed system, referred to as 1BitISAC, employs 1-bit DACs at the ISAC transmitter and 1-bit ADCs at the sensing receiver, achieving significant reductions in power consumption and hardware costs.","For such kind of systems, two 1BitISAC joint transceiver designs, i.e., i) quality of service constrained 1BitISAC design and ii) quality of detection constrained design, are considered and the corresponding problems are formulated.","In order to address these problems, we thoroughly analyze the radar detection performance after 1-bit ADCs quantization and the communication bit error rate.","This analysis yields new design insights and leads to unique radar and communication metrics, which enables us to simplify the original problems and employ majorization-minimization and integer linear programming methods to solve the problems.","Numerical results are provided to validate the performance analysis of the proposed 1BitISAC and to compare with other ISAC configurations.","The superiority of the proposed 1BitISAC system in terms of balancing ISAC performance and energy efficiency is also demonstrated."],"url":"http://arxiv.org/abs/2405.15553v1","category":"eess.SP"}
{"created":"2024-05-24 13:37:54","title":"Stability and Performance Analysis of Model Predictive Control of Uncertain Linear Systems","abstract":"Model mismatch often poses challenges in model-based controller design. This paper investigates model predictive control (MPC) of uncertain linear systems with input constraints, focusing on stability and closed-loop infinite-horizon performance. The uncertainty arises from a parametric mismatch between the true and the estimated system under the matrix Frobenius norm. We examine a simple MPC controller that exclusively uses the estimated system model and establishes sufficient conditions under which the MPC controller can stabilize the true system. Moreover, we derive a theoretical performance bound based on relaxed dynamic programming, elucidating the impact of prediction horizon and modeling errors on the suboptimality gap between the MPC controller and the Oracle infinite-horizon optimal controller with knowledge of the true system. Simulations of a numerical example validate the theoretical results. Our theoretical analysis offers guidelines for obtaining the desired modeling accuracy and choosing a proper prediction horizon to develop certainty-equivalent MPC controllers for uncertain linear systems.","sentences":["Model mismatch often poses challenges in model-based controller design.","This paper investigates model predictive control (MPC) of uncertain linear systems with input constraints, focusing on stability and closed-loop infinite-horizon performance.","The uncertainty arises from a parametric mismatch between the true and the estimated system under the matrix Frobenius norm.","We examine a simple MPC controller that exclusively uses the estimated system model and establishes sufficient conditions under which the MPC controller can stabilize the true system.","Moreover, we derive a theoretical performance bound based on relaxed dynamic programming, elucidating the impact of prediction horizon and modeling errors on the suboptimality gap between the MPC controller and the Oracle infinite-horizon optimal controller with knowledge of the true system.","Simulations of a numerical example validate the theoretical results.","Our theoretical analysis offers guidelines for obtaining the desired modeling accuracy and choosing a proper prediction horizon to develop certainty-equivalent MPC controllers for uncertain linear systems."],"url":"http://arxiv.org/abs/2405.15552v1","category":"math.OC"}
{"created":"2024-05-24 13:24:36","title":"Formulating human risk response in epidemic models: exogenous vs endogenous approaches","abstract":"The recent pandemic emphasized the need to consider the role of human behavior in shaping epidemic dynamics. In particular, it is necessary to extend beyond the classical epidemiological structures to fully capture the interplay between the spread of disease and how people respond. Here, we focus on the challenge of incorporating change in human behavior in the form of \"risk response\" into compartmental epidemiological models, where humans adapt their actions in response to their perceived risk of becoming infected. The review examines over 40 compartmental models, categorizing them into two fundamentally distinct classes: exogenous and endogenous approaches to modeling risk response. While in exogenous approaches, human behavior is often included using different fixed parameter values for certain time periods, endogenous approaches seek for a mechanism internal to the model to explain changes in human behavior as a function of the state of disease. We further discuss two different formulations within endogenous models as implicit versus explicit representation of information diffusion. This comprehensive analysis provides modelers with guidance in selecting an appropriate framework for epidemic modeling.","sentences":["The recent pandemic emphasized the need to consider the role of human behavior in shaping epidemic dynamics.","In particular, it is necessary to extend beyond the classical epidemiological structures to fully capture the interplay between the spread of disease and how people respond.","Here, we focus on the challenge of incorporating change in human behavior in the form of \"risk response\" into compartmental epidemiological models, where humans adapt their actions in response to their perceived risk of becoming infected.","The review examines over 40 compartmental models, categorizing them into two fundamentally distinct classes: exogenous and endogenous approaches to modeling risk response.","While in exogenous approaches, human behavior is often included using different fixed parameter values for certain time periods, endogenous approaches seek for a mechanism internal to the model to explain changes in human behavior as a function of the state of disease.","We further discuss two different formulations within endogenous models as implicit versus explicit representation of information diffusion.","This comprehensive analysis provides modelers with guidance in selecting an appropriate framework for epidemic modeling."],"url":"http://arxiv.org/abs/2405.15535v1","category":"math.DS"}
{"created":"2024-05-24 13:23:59","title":"$K^-d\\rightarrow\u03c0\u039bN$ reaction with in-flight kaons for studying the $\u039bN$ interaction","abstract":"The $\\Lambda N$ invariant mass spectra for the reactions $K^-d\\rightarrow\\pi^-\\Lambda p$ and $K^-d\\rightarrow\\pi^0\\Lambda n$ are calculated for experimental study of isospin symmetry breaking in the $\\Lambda N$ scattering at low energies, the difference in the scattering lengths and effective ranges of $\\Lambda p$ and $\\Lambda n$ systems. The calculations are performed for in-flight kaons with a momentum of 1000 MeV/c, employing partial wave analysis up to the p-wave for meson-baryon amplitudes and the spin-flip term for baryon-baryon amplitudes. Kinematic selection is utilized to suppress the background processes, involving the angular selection of pions to the forward direction and selecting nucleons with higher momentum. It is worth noting that isospin symmetry breaking in the $\\Lambda N$ system can be extracted from the observed deviations in that breaking in the $\\Lambda N$ invariant mass spectra for the $K^-d\\rightarrow\\pi\\Lambda N$ reaction.","sentences":["The $\\Lambda N$ invariant mass spectra for the reactions $K^-d\\rightarrow\\pi^-\\Lambda p$ and $K^-d\\rightarrow\\pi^0\\Lambda n$ are calculated for experimental study of isospin symmetry breaking in the $\\Lambda N$ scattering at low energies, the difference in the scattering lengths and effective ranges of $\\Lambda p$ and $\\Lambda n$ systems.","The calculations are performed for in-flight kaons with a momentum of 1000 MeV/c, employing partial wave analysis up to the p-wave for meson-baryon amplitudes and the spin-flip term for baryon-baryon amplitudes.","Kinematic selection is utilized to suppress the background processes, involving the angular selection of pions to the forward direction and selecting nucleons with higher momentum.","It is worth noting that isospin symmetry breaking in the $\\Lambda N$ system can be extracted from the observed deviations in that breaking in the $\\Lambda N$ invariant mass spectra for the $K^-d\\rightarrow\\pi\\Lambda N$ reaction."],"url":"http://arxiv.org/abs/2405.15534v1","category":"nucl-th"}
{"created":"2024-05-24 13:23:47","title":"Dynamical Analysis of a Cocaine-Heroin Epidemiological Model with Spatial Distributions","abstract":"This article conducts an in-depth investigation of a new spatio-temporal model for the cocaine-heroin epidemiological model with vital dynamics, incorporating the Laplacian operator. The study rigorously establishes the existence, uniqueness, non-negativity, and boundedness of solutions for the proposed model. In addition, the local stability of both a drug-free equilibrium and a drug-addiction equilibrium are analyzed by studying the corresponding characteristic equations. The research provides conclusive evidence that when the basic reproductive number $\\mathcal{R}_0$ exceeds 1, the drug-addiction equilibrium is globally asymptotically stable. Conversely, using comparative arguments, it is shown that if $\\mathcal{R}_0$ is less than 1, the drug-free equilibrium is globally asymptotically stable. Furthermore, the article includes a series of numerical simulations to visually convey and support the analytical results.","sentences":["This article conducts an in-depth investigation of a new spatio-temporal model for the cocaine-heroin epidemiological model with vital dynamics, incorporating the Laplacian operator.","The study rigorously establishes the existence, uniqueness, non-negativity, and boundedness of solutions for the proposed model.","In addition, the local stability of both a drug-free equilibrium and a drug-addiction equilibrium are analyzed by studying the corresponding characteristic equations.","The research provides conclusive evidence that when the basic reproductive number $\\mathcal{R}_0$ exceeds 1, the drug-addiction equilibrium is globally asymptotically stable.","Conversely, using comparative arguments, it is shown that if $\\mathcal{R}_0$ is less than 1, the drug-free equilibrium is globally asymptotically stable.","Furthermore, the article includes a series of numerical simulations to visually convey and support the analytical results."],"url":"http://arxiv.org/abs/2405.15532v1","category":"math.DS"}
{"created":"2024-05-24 13:17:25","title":"Polygons and non-starlikeness of Teichmuller spaces","abstract":"The problem of starlikeness of Teichmuller spaces in Bers' embedding was raised in 1974 and is solved (negatively) for Teichmuller spaces of sufficiently large dimensions.   The original proof given by the author relies on the existence of conformally rigid domains established by Thurston. Later the author found another proof of non-starlikeness of universal Teichmuller space based on geometric features of rectilinear polygons.   This paper provides a complete solution of the problem for Teichmuller spaces $T(g,0)$ of closed Riemann surfaces of genus $g \\ge 2$.","sentences":["The problem of starlikeness of Teichmuller spaces in Bers' embedding was raised in 1974 and is solved (negatively) for Teichmuller spaces of sufficiently large dimensions.   ","The original proof given by the author relies on the existence of conformally rigid domains established by Thurston.","Later the author found another proof of non-starlikeness of universal Teichmuller space based on geometric features of rectilinear polygons.   ","This paper provides a complete solution of the problem for Teichmuller spaces $T(g,0)$ of closed Riemann surfaces of genus $g \\ge 2$."],"url":"http://arxiv.org/abs/2405.15527v1","category":"math.CV"}
{"created":"2024-05-24 13:01:35","title":"Erase to Enhance: Data-Efficient Machine Unlearning in MRI Reconstruction","abstract":"Machine unlearning is a promising paradigm for removing unwanted data samples from a trained model, towards ensuring compliance with privacy regulations and limiting harmful biases. Although unlearning has been shown in, e.g., classification and recommendation systems, its potential in medical image-to-image translation, specifically in image recon-struction, has not been thoroughly investigated. This paper shows that machine unlearning is possible in MRI tasks and has the potential to benefit for bias removal. We set up a protocol to study how much shared knowledge exists between datasets of different organs, allowing us to effectively quantify the effect of unlearning. Our study reveals that combining training data can lead to hallucinations and reduced image quality in the reconstructed data. We use unlearning to remove hallucinations as a proxy exemplar of undesired data removal. Indeed, we show that machine unlearning is possible without full retraining. Furthermore, our observations indicate that maintaining high performance is feasible even when using only a subset of retain data. We have made our code publicly accessible.","sentences":["Machine unlearning is a promising paradigm for removing unwanted data samples from a trained model, towards ensuring compliance with privacy regulations and limiting harmful biases.","Although unlearning has been shown in, e.g., classification and recommendation systems, its potential in medical image-to-image translation, specifically in image recon-struction, has not been thoroughly investigated.","This paper shows that machine unlearning is possible in MRI tasks and has the potential to benefit for bias removal.","We set up a protocol to study how much shared knowledge exists between datasets of different organs, allowing us to effectively quantify the effect of unlearning.","Our study reveals that combining training data can lead to hallucinations and reduced image quality in the reconstructed data.","We use unlearning to remove hallucinations as a proxy exemplar of undesired data removal.","Indeed, we show that machine unlearning is possible without full retraining.","Furthermore, our observations indicate that maintaining high performance is feasible even when using only a subset of retain data.","We have made our code publicly accessible."],"url":"http://arxiv.org/abs/2405.15517v1","category":"eess.IV"}
{"created":"2024-05-24 13:00:00","title":"The handlebody group is a virtual duality group","abstract":"We show that the mapping class group of a handlebody is a virtual duality group, in the sense of Bieri and Eckmann. In positive genus we give a description of the dualising module of any torsion-free, finite-index subgroup of the handlebody mapping class group as the homology of the complex of non-simple disc systems.","sentences":["We show that the mapping class group of a handlebody is a virtual duality group, in the sense of Bieri and Eckmann.","In positive genus we give a description of the dualising module of any torsion-free, finite-index subgroup of the handlebody mapping class group as the homology of the complex of non-simple disc systems."],"url":"http://arxiv.org/abs/2405.15515v1","category":"math.GT"}
{"created":"2024-05-24 12:37:05","title":"Node Accessibility Characterization of Radially-Grown Structures","abstract":"Complex systems have motivated continuing interest from the scientific community, leading to new concepts and methods. Growing systems represent a case of particular interest, as their topological, geometrical, and also dynamical properties change along time, as new elements are incorporated into the existing structure. In the present work, an approach is the case in which systems grown radially around some straight axis of reference, such as particle deposition on electrodes, or urban expansion along avenues, roads, coastline, or rivers, among several other possibilities. More specifically, we aim at characterizing the topological properties of simulated growing structures, which are represented as graphs, in terms of a measurement corresponding to the accessibility of each involved node. The incorporation of new elements (nodes and links) is performed preferentially to the angular orientation respectively to the reference axis. Several interesting results are reported, including the tendency of structures grown preferentially to the orientation normal to the axis to have smaller accessibility.","sentences":["Complex systems have motivated continuing interest from the scientific community, leading to new concepts and methods.","Growing systems represent a case of particular interest, as their topological, geometrical, and also dynamical properties change along time, as new elements are incorporated into the existing structure.","In the present work, an approach is the case in which systems grown radially around some straight axis of reference, such as particle deposition on electrodes, or urban expansion along avenues, roads, coastline, or rivers, among several other possibilities.","More specifically, we aim at characterizing the topological properties of simulated growing structures, which are represented as graphs, in terms of a measurement corresponding to the accessibility of each involved node.","The incorporation of new elements (nodes and links) is performed preferentially to the angular orientation respectively to the reference axis.","Several interesting results are reported, including the tendency of structures grown preferentially to the orientation normal to the axis to have smaller accessibility."],"url":"http://arxiv.org/abs/2405.15498v1","category":"cs.SI"}
{"created":"2024-05-24 12:22:16","title":"Creation and manipulation of Schr\u00f6dinger cat states of a nuclear spin qudit in silicon","abstract":"High-dimensional quantum systems are a valuable resource for quantum information processing. They can be used to encode error-correctable logical qubits, for instance in continuous-variable states of oscillators such as microwave cavities or the motional modes of trapped ions. Powerful encodings include 'Schr\\\"odinger cat' states, superpositions of widely displaced coherent states, which also embody the challenge of quantum effects at the large scale. Alternatively, recent proposals suggest encoding logical qubits in high-spin atomic nuclei, which can host hardware-efficient versions of continuous-variable codes on a finite-dimensional system. Here we demonstrate the creation and manipulation of Schr\\\"odinger cat states using the spin-7/2 nucleus of a single antimony ($^{123}$Sb) atom, embedded and operated within a silicon nanoelectronic device. We use a coherent multi-frequency control scheme to produce spin rotations that preserve the SU(2) symmetry of the qudit, and constitute logical Pauli operations for logical qubits encoded in the Schr\\\"odinger cat states. The Wigner function of the cat states exhibits parity oscillations with a contrast up to 0.982(5), and state fidelities up to 0.913(2). These results demonstrate high-fidelity preparation of nonclassical resource states and logical control in a single atomic-scale object, opening up applications in quantum information processing and quantum error correction within a scalable, manufacturable semiconductor platform.","sentences":["High-dimensional quantum systems are a valuable resource for quantum information processing.","They can be used to encode error-correctable logical qubits, for instance in continuous-variable states of oscillators such as microwave cavities or the motional modes of trapped ions.","Powerful encodings include 'Schr\\\"odinger cat' states, superpositions of widely displaced coherent states, which also embody the challenge of quantum effects at the large scale.","Alternatively, recent proposals suggest encoding logical qubits in high-spin atomic nuclei, which can host hardware-efficient versions of continuous-variable codes on a finite-dimensional system.","Here we demonstrate the creation and manipulation of Schr\\\"odinger cat states using the spin-7/2 nucleus of a single antimony ($^{123}$Sb) atom, embedded and operated within a silicon nanoelectronic device.","We use a coherent multi-frequency control scheme to produce spin rotations that preserve the SU(2) symmetry of the qudit, and constitute logical Pauli operations for logical qubits encoded in the Schr\\\"odinger cat states.","The Wigner function of the cat states exhibits parity oscillations with a contrast up to 0.982(5), and state fidelities up to 0.913(2).","These results demonstrate high-fidelity preparation of nonclassical resource states and logical control in a single atomic-scale object, opening up applications in quantum information processing and quantum error correction within a scalable, manufacturable semiconductor platform."],"url":"http://arxiv.org/abs/2405.15494v1","category":"quant-ph"}
{"created":"2024-05-24 12:21:57","title":"Design and Implementation of DC-DC Buck Converter based on Deep Neural Network Sliding Mode Control","abstract":"In order to address the challenge of traditional sliding mode controllers struggling to balance between suppressing system jitter and accelerating convergence speed, a deep neural network (DNN)-based sliding mode control strategy is proposed in this paper. The strategy achieves dynamic adjustment of parameters by modelling and learning the system through deep neural networks, which suppresses the system jitter while ensuring the convergence speed of the system. To demonstrate the stability of the system, a Lyapunov function is designed to prove the stability of the mathematical model of the DNN-based sliding mode control strategy for DC-DC buck switching power supply. We adopt a double closed-loop control mode to combine the sliding mode control of the voltage inner loop with the PI control of the current outer loop. Simultaneously, The DNN performance is evaluated through simulation and hardware experiments and compared with conventional control methods. The results demonstrate that the sliding mode controller based on the DNN exhibits faster system convergence speed, enhanced jitter suppression capability, and greater robustness.","sentences":["In order to address the challenge of traditional sliding mode controllers struggling to balance between suppressing system jitter and accelerating convergence speed, a deep neural network (DNN)-based sliding mode control strategy is proposed in this paper.","The strategy achieves dynamic adjustment of parameters by modelling and learning the system through deep neural networks, which suppresses the system jitter while ensuring the convergence speed of the system.","To demonstrate the stability of the system, a Lyapunov function is designed to prove the stability of the mathematical model of the DNN-based sliding mode control strategy for DC-DC buck switching power supply.","We adopt a double closed-loop control mode to combine the sliding mode control of the voltage inner loop with the PI control of the current outer loop.","Simultaneously, The DNN performance is evaluated through simulation and hardware experiments and compared with conventional control methods.","The results demonstrate that the sliding mode controller based on the DNN exhibits faster system convergence speed, enhanced jitter suppression capability, and greater robustness."],"url":"http://arxiv.org/abs/2405.15493v1","category":"eess.SY"}
{"created":"2024-05-24 12:15:58","title":"Leveraging Large Language Models and Social Media for Automation in Scanning Probe Microscopy","abstract":"We present the development of an automated scanning probe microscopy (SPM) measurement system using an advanced large-scale language model (LLM). This SPM system can receive instructions via social networking services (SNS), and the integration of SNS and LLMs enables real-time, language-agnostic control of SPM operations, thereby improving accessibility and efficiency. The integration of LLMs with AI systems with specialized functions brings the realization of self-driving labs closer.","sentences":["We present the development of an automated scanning probe microscopy (SPM) measurement system using an advanced large-scale language model (LLM).","This SPM system can receive instructions via social networking services (SNS), and the integration of SNS and LLMs enables real-time, language-agnostic control of SPM operations, thereby improving accessibility and efficiency.","The integration of LLMs with AI systems with specialized functions brings the realization of self-driving labs closer."],"url":"http://arxiv.org/abs/2405.15490v1","category":"physics.app-ph"}
{"created":"2024-05-24 12:09:20","title":"Unsupervised Deep Neural Network Approach To Solve Bosonic Systems","abstract":"The simulation of quantum many-body systems poses a significant challenge in physics due to the exponential scaling of Hilbert space with the number of particles. Traditional methods often struggle with large system sizes and frustrated lattices. In this research article, we present a novel algorithm that leverages the power of deep neural networks combined with Markov Chain Monte Carlo simulation to address these limitations. Our method introduces a neural network architecture specifically designed to represent bosonic quantum states on a 1D lattice chain. We successfully achieve the ground state of the Bose-Hubbard model, demonstrating the superiority of the adaptive momentum optimizer for convergence speed and stability. Notably, our approach offers flexibility in simulating various lattice geometries and potentially larger system sizes, making it a valuable tool for exploring complex quantum phenomena. This work represents a substantial advancement in the field of quantum simulation, opening new possibilities for investigating previously challenging systems.","sentences":["The simulation of quantum many-body systems poses a significant challenge in physics due to the exponential scaling of Hilbert space with the number of particles.","Traditional methods often struggle with large system sizes and frustrated lattices.","In this research article, we present a novel algorithm that leverages the power of deep neural networks combined with Markov Chain Monte Carlo simulation to address these limitations.","Our method introduces a neural network architecture specifically designed to represent bosonic quantum states on a 1D lattice chain.","We successfully achieve the ground state of the Bose-Hubbard model, demonstrating the superiority of the adaptive momentum optimizer for convergence speed and stability.","Notably, our approach offers flexibility in simulating various lattice geometries and potentially larger system sizes, making it a valuable tool for exploring complex quantum phenomena.","This work represents a substantial advancement in the field of quantum simulation, opening new possibilities for investigating previously challenging systems."],"url":"http://arxiv.org/abs/2405.15488v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 11:59:02","title":"Fundamental limits of weak learnability in high-dimensional multi-index models","abstract":"Multi-index models -- functions which only depend on the covariates through a non-linear transformation of their projection on a subspace -- are a useful benchmark for investigating feature learning with neural networks. This paper examines the theoretical boundaries of learnability in this hypothesis class, focusing particularly on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples is $n=\\alpha d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) first, we identify under which conditions a \\textit{trivial subspace} can be learned with a single step of a first-order algorithm for any $\\alpha\\!>\\!0$; (ii) second, in the case where the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an {\\it easy subspace} consisting of directions that can be learned only above a certain sample complexity $\\alpha\\!>\\!\\alpha_c$. The critical threshold $\\alpha_{c}$ marks the presence of a computational phase transition, in the sense that no efficient iterative algorithm can succeed for $\\alpha\\!<\\!\\alpha_c$. In a limited but interesting set of really hard directions -- akin to the parity problem -- $\\alpha_c$ is found to diverge. Finally, (iii) we demonstrate that interactions between different directions can result in an intricate hierarchical learning phenomenon, where some directions can be learned sequentially when coupled to easier ones. Our analytical approach is built on the optimality of approximate message-passing algorithms among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent.","sentences":["Multi-index models -- functions which only depend on the covariates through a non-linear transformation of their projection on a subspace -- are a useful benchmark for investigating feature learning with neural networks.","This paper examines the theoretical boundaries of learnability in this hypothesis class, focusing particularly on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples is $n=\\alpha d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) first, we identify under which conditions a \\textit{trivial subspace} can be learned with a single step of a first-order algorithm for any $\\alpha\\!>\\!0$; (ii) second, in the case where the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an {\\it easy subspace} consisting of directions that can be learned only above a certain sample complexity $\\alpha\\!>\\!\\alpha_c$. The critical threshold $\\alpha_{c}$ marks the presence of a computational phase transition, in the sense that no efficient iterative algorithm can succeed for $\\alpha\\!<\\!\\alpha_c$. In a limited but interesting set of really hard directions -- akin to the parity problem -- $\\alpha_c$ is found to diverge.","Finally, (iii) we demonstrate that interactions between different directions can result in an intricate hierarchical learning phenomenon, where some directions can be learned sequentially when coupled to easier ones.","Our analytical approach is built on the optimality of approximate message-passing algorithms among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent."],"url":"http://arxiv.org/abs/2405.15480v1","category":"cs.LG"}
{"created":"2024-05-24 11:58:02","title":"MagicBathyNet: A Multimodal Remote Sensing Dataset for Bathymetry Prediction and Pixel-based Classification in Shallow Waters","abstract":"Accurate, detailed, and high-frequent bathymetry, coupled with complex semantic content, is crucial for the undermapped shallow seabed areas facing intense climatological and anthropogenic pressures. Current methods exploiting remote sensing images to derive bathymetry or seabed classes mainly exploit non-open data. This lack of openly accessible benchmark archives prevents the wider use of deep learning methods in such applications. To address this issue, in this paper we present the MagicBathyNet, which is a benchmark dataset made up of image patches of Sentinel2, SPOT-6 and aerial imagery, bathymetry in raster format and annotations of seabed classes. MagicBathyNet is then exploited to benchmark state-of-the-art methods in learning-based bathymetry and pixel-based classification. Dataset, pre-trained weights, and code are publicly available at www.magicbathy.eu/magicbathynet.html.","sentences":["Accurate, detailed, and high-frequent bathymetry, coupled with complex semantic content, is crucial for the undermapped shallow seabed areas facing intense climatological and anthropogenic pressures.","Current methods exploiting remote sensing images to derive bathymetry or seabed classes mainly exploit non-open data.","This lack of openly accessible benchmark archives prevents the wider use of deep learning methods in such applications.","To address this issue, in this paper we present the MagicBathyNet, which is a benchmark dataset made up of image patches of Sentinel2, SPOT-6 and aerial imagery, bathymetry in raster format and annotations of seabed classes.","MagicBathyNet is then exploited to benchmark state-of-the-art methods in learning-based bathymetry and pixel-based classification.","Dataset, pre-trained weights, and code are publicly available at www.magicbathy.eu/magicbathynet.html."],"url":"http://arxiv.org/abs/2405.15477v1","category":"cs.CV"}
{"created":"2024-05-24 11:50:04","title":"Stability Analysis of Biochemical Reaction Networks Linearly Conjugated to complex balanced Systems with Time Delays Added","abstract":"Linear conjugacy offers a new perspective to broaden the scope of stable biochemical reaction networks to the systems linearly conjugated to the well-established complex balanced mass action systems ($\\ell$cCBMASs). This paper addresses the challenge posed by time delay, which can disrupt the linear conjugacy relationship and complicate stability analysis for delayed versions of $\\ell$cCBMASs (D$\\ell$cCBMAS). Firstly, we develop Lyapunov functionals tailored to some D$\\ell$cCBMASs by using the persisted parameter relationships under time delays. Subsequently, we redivide the phase space as several invariant sets of trajectories and further investigate the existence and uniqueness of equilibriums in each newly defined invariant set. This enables us to determine the local asymptotic stability of some D$\\ell$cCBMASs within an updated framework. Furthermore, illustrative examples are provided to demonstrate the practical implications of our approach.","sentences":["Linear conjugacy offers a new perspective to broaden the scope of stable biochemical reaction networks to the systems linearly conjugated to the well-established complex balanced mass action systems ($\\ell$cCBMASs).","This paper addresses the challenge posed by time delay, which can disrupt the linear conjugacy relationship and complicate stability analysis for delayed versions of $\\ell$cCBMASs (D$\\ell$cCBMAS).","Firstly, we develop Lyapunov functionals tailored to some D$\\ell$cCBMASs by using the persisted parameter relationships under time delays.","Subsequently, we redivide the phase space as several invariant sets of trajectories and further investigate the existence and uniqueness of equilibriums in each newly defined invariant set.","This enables us to determine the local asymptotic stability of some D$\\ell$cCBMASs within an updated framework.","Furthermore, illustrative examples are provided to demonstrate the practical implications of our approach."],"url":"http://arxiv.org/abs/2405.15472v1","category":"math.DS"}
{"created":"2024-05-24 11:36:02","title":"Sub-20 kHz low frequency noise near ultraviolet miniature external cavity laser diode","abstract":"We present a compact InGaN fiber Bragg grating (FBG) semiconductor laser diode operating below 400 nm in the single-mode emission regime. This compact coherent laser source exhibits an intrinsic linewidth of 14 kHz in the near-UV range and a side-mode suppression ratio reaching up to 40 dB accompanied by a mW-level output power. Furthermore, the properties of the FBG, including its central wavelength, bandwidth, and reflectivity, can be readily customized to fulfill specific requirements. As a result, the small footprint design of this laser is compatible with integration into a standard butterfly package to ease the lab-to-market technology transfer. The combination of low frequency noise and fibered output signal positions these FBG laser systems as strong candidates for hybridization with integrated photonic platforms tailored for quantum information processing and metrology.","sentences":["We present a compact InGaN fiber Bragg grating (FBG) semiconductor laser diode operating below 400 nm in the single-mode emission regime.","This compact coherent laser source exhibits an intrinsic linewidth of 14 kHz in the near-UV range and a side-mode suppression ratio reaching up to 40 dB accompanied by a mW-level output power.","Furthermore, the properties of the FBG, including its central wavelength, bandwidth, and reflectivity, can be readily customized to fulfill specific requirements.","As a result, the small footprint design of this laser is compatible with integration into a standard butterfly package to ease the lab-to-market technology transfer.","The combination of low frequency noise and fibered output signal positions these FBG laser systems as strong candidates for hybridization with integrated photonic platforms tailored for quantum information processing and metrology."],"url":"http://arxiv.org/abs/2405.15462v1","category":"physics.optics"}
{"created":"2024-05-24 11:34:45","title":"TD3 Based Collision Free Motion Planning for Robot Navigation","abstract":"This paper addresses the challenge of collision-free motion planning in automated navigation within complex environments. Utilizing advancements in Deep Reinforcement Learning (DRL) and sensor technologies like LiDAR, we propose the TD3-DWA algorithm, an innovative fusion of the traditional Dynamic Window Approach (DWA) with the Twin Delayed Deep Deterministic Policy Gradient (TD3). This hybrid algorithm enhances the efficiency of robotic path planning by optimizing the sampling interval parameters of DWA to effectively navigate around both static and dynamic obstacles. The performance of the TD3-DWA algorithm is validated through various simulation experiments, demonstrating its potential to significantly improve the reliability and safety of autonomous navigation systems.","sentences":["This paper addresses the challenge of collision-free motion planning in automated navigation within complex environments.","Utilizing advancements in Deep Reinforcement Learning (DRL) and sensor technologies like LiDAR, we propose the TD3-DWA algorithm, an innovative fusion of the traditional Dynamic Window Approach (DWA) with the Twin Delayed Deep Deterministic Policy Gradient (TD3).","This hybrid algorithm enhances the efficiency of robotic path planning by optimizing the sampling interval parameters of DWA to effectively navigate around both static and dynamic obstacles.","The performance of the TD3-DWA algorithm is validated through various simulation experiments, demonstrating its potential to significantly improve the reliability and safety of autonomous navigation systems."],"url":"http://arxiv.org/abs/2405.15460v1","category":"cs.RO"}
{"created":"2024-05-24 11:33:40","title":"Deterministic interconversion of GHZ state and KLM state via Lie-transform-based pulse design in Rydberg atoms","abstract":"Conversion between different types of entangled states is an interesting problem in quantum mechanics. But research on the conversion between Greenberger-Horne-Zeilinger (GHZ) state and Knill-Laflamme-Milburn (KLM) state in atomic system is absent. In this paper, we propose a scheme to realize the interconversion (one-step) between GHZ state and KLM state with Rydberg atoms. By utilizing Rydberg-mediated interactions, we simplify the system. By combining Lie-transform-based pulse design, the evolution path is built up to realize interconversion of GHZ state and KLM state. The numerical simulation result shows that the present scheme is robust against decoherence and operational imperfection, the analysis shows that the scheme is feasible with current experimental technology.","sentences":["Conversion between different types of entangled states is an interesting problem in quantum mechanics.","But research on the conversion between Greenberger-Horne-Zeilinger (GHZ) state and Knill-Laflamme-Milburn (KLM) state in atomic system is absent.","In this paper, we propose a scheme to realize the interconversion (one-step) between GHZ state and KLM state with Rydberg atoms.","By utilizing Rydberg-mediated interactions, we simplify the system.","By combining Lie-transform-based pulse design, the evolution path is built up to realize interconversion of GHZ state and KLM state.","The numerical simulation result shows that the present scheme is robust against decoherence and operational imperfection, the analysis shows that the scheme is feasible with current experimental technology."],"url":"http://arxiv.org/abs/2405.15456v1","category":"quant-ph"}
{"created":"2024-05-24 11:25:36","title":"Dynamical behavior of Predator-Prey with Allee Effect on Both Populations and Disease in Predator","abstract":"In the current study, we took into account a model of nonlinear ``predator-prey'' interactions including the ``Allee effect'' on both populations and disease in the predator population. The population as a whole is split into three: the prey population, susceptible predator, and diseased predator. The ``Takagi-Sugeno (T-S) impulsive control model'' and the Fuzzy impulsive control model have been used to test the stability of the three-dimensional ``Lotka-Volterra predator-prey system'' model. Following the model's formulation, the global-stability and the fuzzy solution are examined using numerical simulations and graphical displays, together with the necessary consultation, to help comprehend the effectiveness of our suggested model.","sentences":["In the current study, we took into account a model of nonlinear ``predator-prey'' interactions including the ``Allee effect'' on both populations and disease in the predator population.","The population as a whole is split into three: the prey population, susceptible predator, and diseased predator.","The ``Takagi-Sugeno (T-S) impulsive control model'' and the Fuzzy impulsive control model have been used to test the stability of the three-dimensional ``Lotka-Volterra predator-prey system'' model.","Following the model's formulation, the global-stability and the fuzzy solution are examined using numerical simulations and graphical displays, together with the necessary consultation, to help comprehend the effectiveness of our suggested model."],"url":"http://arxiv.org/abs/2405.15448v1","category":"q-bio.PE"}
{"created":"2024-05-24 11:18:13","title":"Towards Precision Healthcare: Robust Fusion of Time Series and Image Data","abstract":"With the increasing availability of diverse data types, particularly images and time series data from medical experiments, there is a growing demand for techniques designed to combine various modalities of data effectively. Our motivation comes from the important areas of predicting mortality and phenotyping where using different modalities of data could significantly improve our ability to predict. To tackle this challenge, we introduce a new method that uses two separate encoders, one for each type of data, allowing the model to understand complex patterns in both visual and time-based information. Apart from the technical challenges, our goal is to make the predictive model more robust in noisy conditions and perform better than current methods. We also deal with imbalanced datasets and use an uncertainty loss function, yielding improved results while simultaneously providing a principled means of modeling uncertainty. Additionally, we include attention mechanisms to fuse different modalities, allowing the model to focus on what's important for each task. We tested our approach using the comprehensive multimodal MIMIC dataset, combining MIMIC-IV and MIMIC-CXR datasets. Our experiments show that our method is effective in improving multimodal deep learning for clinical applications. The code will be made available online.","sentences":["With the increasing availability of diverse data types, particularly images and time series data from medical experiments, there is a growing demand for techniques designed to combine various modalities of data effectively.","Our motivation comes from the important areas of predicting mortality and phenotyping where using different modalities of data could significantly improve our ability to predict.","To tackle this challenge, we introduce a new method that uses two separate encoders, one for each type of data, allowing the model to understand complex patterns in both visual and time-based information.","Apart from the technical challenges, our goal is to make the predictive model more robust in noisy conditions and perform better than current methods.","We also deal with imbalanced datasets and use an uncertainty loss function, yielding improved results while simultaneously providing a principled means of modeling uncertainty.","Additionally, we include attention mechanisms to fuse different modalities, allowing the model to focus on what's important for each task.","We tested our approach using the comprehensive multimodal MIMIC dataset, combining MIMIC-IV and MIMIC-CXR datasets.","Our experiments show that our method is effective in improving multimodal deep learning for clinical applications.","The code will be made available online."],"url":"http://arxiv.org/abs/2405.15442v1","category":"eess.IV"}
{"created":"2024-05-24 10:59:38","title":"Dipolar Droplets at 3D-1D Crossover","abstract":"We investigate beyond-mean-field corrections to the energy of a one-dimensional Bose gas confined by a box potential with dipoles aligned along the unconfined direction. When the dipolar interaction reaches its critical strength, the system becomes unstable at the mean-field level. Then, similarly to the free space case, the beyond-mean-field contribution significantly alters the ground state of the system, leading to the formation of a self-bound atomic cloud known as a quantum droplet. Our analysis demonstrates that the beyond-mean-field correction can be understood as an effective three-body repulsion stabilizing the gas, preventing its collapse and leading to a finite-density solution. The quasi-1D dipolar droplets bring into play yet another subtle quantum effect: the effective three-body interactions.","sentences":["We investigate beyond-mean-field corrections to the energy of a one-dimensional Bose gas confined by a box potential with dipoles aligned along the unconfined direction.","When the dipolar interaction reaches its critical strength, the system becomes unstable at the mean-field level.","Then, similarly to the free space case, the beyond-mean-field contribution significantly alters the ground state of the system, leading to the formation of a self-bound atomic cloud known as a quantum droplet.","Our analysis demonstrates that the beyond-mean-field correction can be understood as an effective three-body repulsion stabilizing the gas, preventing its collapse and leading to a finite-density solution.","The quasi-1D dipolar droplets bring into play yet another subtle quantum effect: the effective three-body interactions."],"url":"http://arxiv.org/abs/2405.15433v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-24 10:58:00","title":"Counterfactual Explanations for Linear Optimization","abstract":"The concept of counterfactual explanations (CE) has emerged as one of the important concepts to understand the inner workings of complex AI systems. In this paper, we translate the idea of CEs to linear optimization and propose, motivate, and analyze three different types of CEs: strong, weak, and relative. While deriving strong and weak CEs appears to be computationally intractable, we show that calculating relative CEs can be done efficiently. By detecting and exploiting the hidden convex structure of the optimization problem that arises in the latter case, we show that obtaining relative CEs can be done in the same magnitude of time as solving the original linear optimization problem. This is confirmed by an extensive numerical experiment study on the NETLIB library.","sentences":["The concept of counterfactual explanations (CE) has emerged as one of the important concepts to understand the inner workings of complex AI systems.","In this paper, we translate the idea of CEs to linear optimization and propose, motivate, and analyze three different types of CEs: strong, weak, and relative.","While deriving strong and weak CEs appears to be computationally intractable, we show that calculating relative CEs can be done efficiently.","By detecting and exploiting the hidden convex structure of the optimization problem that arises in the latter case, we show that obtaining relative CEs can be done in the same magnitude of time as solving the original linear optimization problem.","This is confirmed by an extensive numerical experiment study on the NETLIB library."],"url":"http://arxiv.org/abs/2405.15431v1","category":"math.OC"}
{"created":"2024-05-24 10:56:51","title":"Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics","abstract":"Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.","sentences":["Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints.","To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour.","We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics.","The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation."],"url":"http://arxiv.org/abs/2405.15430v1","category":"cs.LG"}
{"created":"2024-05-24 10:55:38","title":"E(n) Equivariant Topological Neural Networks","abstract":"Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features. Topological deep learning (TDL) has emerged recently as a promising tool for addressing this issue. TDL enables the principled modeling of arbitrary multi-way, hierarchical higher-order interactions by operating on combinatorial topological spaces, such as simplicial or cell complexes, instead of graphs. However, little is known about how to leverage geometric features such as positions and velocities for TDL. This paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing networks operating on combinatorial complexes, formal objects unifying graphs, hypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric node features while respecting rotation and translation equivariance. Moreover, ETNNs are natively ready for settings with heterogeneous interactions. We provide a theoretical analysis to show the improved expressiveness of ETNNs over architectures for geometric graphs. We also show how several E(n) equivariant variants of TDL models can be directly derived from our framework. The broad applicability of ETNNs is demonstrated through two tasks of vastly different nature: i) molecular property prediction on the QM9 benchmark and ii) land-use regression for hyper-local estimation of air pollution with multi-resolution irregular geospatial data. The experiment results indicate that ETNNs are an effective tool for learning from diverse types of richly structured data, highlighting the benefits of principled geometric inductive bias.","sentences":["Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features.","Topological deep learning (TDL) has emerged recently as a promising tool for addressing this issue.","TDL enables the principled modeling of arbitrary multi-way, hierarchical higher-order interactions by operating on combinatorial topological spaces, such as simplicial or cell complexes, instead of graphs.","However, little is known about how to leverage geometric features such as positions and velocities for TDL.","This paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are E(n)-equivariant message-passing networks operating on combinatorial complexes, formal objects unifying graphs, hypergraphs, simplicial, path, and cell complexes.","ETNNs incorporate geometric node features while respecting rotation and translation equivariance.","Moreover, ETNNs are natively ready for settings with heterogeneous interactions.","We provide a theoretical analysis to show the improved expressiveness of ETNNs over architectures for geometric graphs.","We also show how several E(n) equivariant variants of TDL models can be directly derived from our framework.","The broad applicability of ETNNs is demonstrated through two tasks of vastly different nature: i) molecular property prediction on the QM9 benchmark and ii) land-use regression for hyper-local estimation of air pollution with multi-resolution irregular geospatial data.","The experiment results indicate that ETNNs are an effective tool for learning from diverse types of richly structured data, highlighting the benefits of principled geometric inductive bias."],"url":"http://arxiv.org/abs/2405.15429v1","category":"cs.LG"}
{"created":"2024-05-24 10:37:31","title":"Quantum-centric strong and dynamical electron correlation: A resource-efficient second-order $N$-electron valence perturbation theory formulation for near-term quantum devices","abstract":"We present a measurement-cost efficient implementation of Strongly-Contracted $N$-Electron Valence Perturbation Theory (SC-NEVPT2) for use on near-term quantum devices. At the heart of our algorithm we exploit the properties of adaptive Informationally Complete positive operator valued measures (IC-POVMs) to recycle the measurement outcomes from a ground state energy estimation on a quantum device to reconstruct the matrix elements of the three- and four-body reduced density matrices for use in a subsequent CPU-driven NEVPT2 calculation. The proposed scheme is capable of producing results in good agreement with corresponding conventional NEVPT2 simulations, while significantly reducing the cost of quantum measurements and allowing for embarrassingly parallel estimations of higher-order RDMs in classical post-processing. Our scheme shows favourable scaling of the total number of shots with respect to system size. This paves the way for routine inclusion of dynamic electron correlation effects in hybrid quantum-classical computing pipelines.","sentences":["We present a measurement-cost efficient implementation of Strongly-Contracted $N$-Electron Valence Perturbation Theory (SC-NEVPT2) for use on near-term quantum devices.","At the heart of our algorithm we exploit the properties of adaptive Informationally Complete positive operator valued measures (IC-POVMs) to recycle the measurement outcomes from a ground state energy estimation on a quantum device to reconstruct the matrix elements of the three- and four-body reduced density matrices for use in a subsequent CPU-driven NEVPT2 calculation.","The proposed scheme is capable of producing results in good agreement with corresponding conventional NEVPT2 simulations, while significantly reducing the cost of quantum measurements and allowing for embarrassingly parallel estimations of higher-order RDMs in classical post-processing.","Our scheme shows favourable scaling of the total number of shots with respect to system size.","This paves the way for routine inclusion of dynamic electron correlation effects in hybrid quantum-classical computing pipelines."],"url":"http://arxiv.org/abs/2405.15422v1","category":"quant-ph"}
{"created":"2024-05-24 10:27:28","title":"Plasmon-driven substitution of 4 mercaptophenylboronic acid to 4-nitrothiophenol monitored by surface-enhanced Raman spectroscopy","abstract":"Plasmon-driven reactions on plasmonic nanoparticles (NPs) occur under significantly different conditions from those of classical organic synthesis and provide a promising pathway for enhancing the efficiency of various chemical processes. However, these reactions can also have undesirable effects, such as 4-mercaptophenylboronic acid (MPBA) deboronation. MPBA chemisorbs well to Ag NPs through its thiol group and can subsequently bind to diols, enabling the detection of various biological structures by surface-enhanced Raman scattering (SERS), but not upon its deboronation. To avoid this reaction, we investigated the experimental conditions of MPBA deboronation on Ag NPs by SERS. Our results showed that the level of deboronation strongly depends on both the morphology of the system and the excitation laser wavelength and power. In addition, we detected not only the expected products, namely thiophenol and biphenyl-4,4-dithiol, but also 4-nitrothiophenol (NTP). The crucial reagent for NTP formation was an oxidation product of hydroxylamine hydrochloride, the reduction agent used in Ag NP synthesis. Ultimately, this reaction was replicated by adding NaNO2 to the system, and its progress was monitored as a function of the laser power, thereby identifying a new reaction of plasmon-driven -B(OH)2 substitution for -NO2.","sentences":["Plasmon-driven reactions on plasmonic nanoparticles (NPs) occur under significantly different conditions from those of classical organic synthesis and provide a promising pathway for enhancing the efficiency of various chemical processes.","However, these reactions can also have undesirable effects, such as 4-mercaptophenylboronic acid (MPBA) deboronation.","MPBA chemisorbs well to Ag NPs through its thiol group and can subsequently bind to diols, enabling the detection of various biological structures by surface-enhanced Raman scattering (SERS), but not upon its deboronation.","To avoid this reaction, we investigated the experimental conditions of MPBA deboronation on Ag NPs by SERS.","Our results showed that the level of deboronation strongly depends on both the morphology of the system and the excitation laser wavelength and power.","In addition, we detected not only the expected products, namely thiophenol and biphenyl-4,4-dithiol, but also 4-nitrothiophenol (NTP).","The crucial reagent for NTP formation was an oxidation product of hydroxylamine hydrochloride, the reduction agent used in Ag NP synthesis.","Ultimately, this reaction was replicated by adding NaNO2 to the system, and its progress was monitored as a function of the laser power, thereby identifying a new reaction of plasmon-driven -B(OH)2 substitution for -NO2."],"url":"http://arxiv.org/abs/2405.15418v1","category":"physics.chem-ph"}
{"created":"2024-05-24 10:26:56","title":"Semi-Supervised Learning via Cross-Prediction-Powered Inference for Wireless Systems","abstract":"In many wireless application scenarios, acquiring labeled data can be prohibitively costly, requiring complex optimization processes or measurement campaigns. Semi-supervised learning leverages unlabeled samples to augment the available dataset by assigning synthetic labels obtained via machine learning (ML)-based predictions. However, treating the synthetic labels as true labels may yield worse-performing models as compared to models trained using only labeled data. Inspired by the recently developed prediction-powered inference (PPI) framework, this work investigates how to leverage the synthetic labels produced by an ML model, while accounting for the inherent bias with respect to true labels. To this end, we first review PPI and its recent extensions, namely tuned PPI and cross-prediction-powered inference (CPPI). Then, we introduce a novel variant of PPI, referred to as tuned CPPI, that provides CPPI with an additional degree of freedom in adapting to the quality of the ML-based labels. Finally, we showcase two applications of PPI-based techniques in wireless systems, namely beam alignment based on channel knowledge maps in millimeter-wave systems and received signal strength information-based indoor localization. Simulation results show the advantages of PPI-based techniques over conventional approaches that rely solely on labeled data or that apply standard pseudo-labeling strategies from semi-supervised learning. Furthermore, the proposed tuned CPPI method is observed to guarantee the best performance among all benchmark schemes, especially in the regime of limited labeled data.","sentences":["In many wireless application scenarios, acquiring labeled data can be prohibitively costly, requiring complex optimization processes or measurement campaigns.","Semi-supervised learning leverages unlabeled samples to augment the available dataset by assigning synthetic labels obtained via machine learning (ML)-based predictions.","However, treating the synthetic labels as true labels may yield worse-performing models as compared to models trained using only labeled data.","Inspired by the recently developed prediction-powered inference (PPI) framework, this work investigates how to leverage the synthetic labels produced by an ML model, while accounting for the inherent bias with respect to true labels.","To this end, we first review PPI and its recent extensions, namely tuned PPI and cross-prediction-powered inference (CPPI).","Then, we introduce a novel variant of PPI, referred to as tuned CPPI, that provides CPPI with an additional degree of freedom in adapting to the quality of the ML-based labels.","Finally, we showcase two applications of PPI-based techniques in wireless systems, namely beam alignment based on channel knowledge maps in millimeter-wave systems and received signal strength information-based indoor localization.","Simulation results show the advantages of PPI-based techniques over conventional approaches that rely solely on labeled data or that apply standard pseudo-labeling strategies from semi-supervised learning.","Furthermore, the proposed tuned CPPI method is observed to guarantee the best performance among all benchmark schemes, especially in the regime of limited labeled data."],"url":"http://arxiv.org/abs/2405.15415v1","category":"cs.IT"}
{"created":"2024-05-24 10:17:49","title":"Towards Client Driven Federated Learning","abstract":"Conventional federated learning (FL) frameworks follow a server-driven model where the server determines session initiation and client participation, which faces challenges in accommodating clients' asynchronous needs for model updates. We introduce Client-Driven Federated Learning (CDFL), a novel FL framework that puts clients at the driving role. In CDFL, each client independently and asynchronously updates its model by uploading the locally trained model to the server and receiving a customized model tailored to its local task. The server maintains a repository of cluster models, iteratively refining them using received client models. Our framework accommodates complex dynamics in clients' data distributions, characterized by time-varying mixtures of cluster distributions, enabling rapid adaptation to new tasks with superior performance. In contrast to traditional clustered FL protocols that send multiple cluster models to a client to perform distribution estimation, we propose a paradigm that offloads the estimation task to the server and only sends a single model to a client, and novel strategies to improve estimation accuracy. We provide a theoretical analysis of CDFL's convergence. Extensive experiments across various datasets and system settings highlight CDFL's substantial advantages in model performance and computation efficiency over baselines.","sentences":["Conventional federated learning (FL) frameworks follow a server-driven model where the server determines session initiation and client participation, which faces challenges in accommodating clients' asynchronous needs for model updates.","We introduce Client-Driven Federated Learning (CDFL), a novel FL framework that puts clients at the driving role.","In CDFL, each client independently and asynchronously updates its model by uploading the locally trained model to the server and receiving a customized model tailored to its local task.","The server maintains a repository of cluster models, iteratively refining them using received client models.","Our framework accommodates complex dynamics in clients' data distributions, characterized by time-varying mixtures of cluster distributions, enabling rapid adaptation to new tasks with superior performance.","In contrast to traditional clustered FL protocols that send multiple cluster models to a client to perform distribution estimation, we propose a paradigm that offloads the estimation task to the server and only sends a single model to a client, and novel strategies to improve estimation accuracy.","We provide a theoretical analysis of CDFL's convergence.","Extensive experiments across various datasets and system settings highlight CDFL's substantial advantages in model performance and computation efficiency over baselines."],"url":"http://arxiv.org/abs/2405.15407v1","category":"cs.LG"}
{"created":"2024-05-24 09:57:55","title":"Symmetries for spherical functions of type $\u03c7$ for quantum symmetric pairs","abstract":"Let $\\mathfrak{g}$ be a complex semisimple Lie algebra and let $\\mathbf{U}_q(\\mathfrak{g})$ denote the associated Drinfel'd Jimbo quantized enveloping algebra. In this paper we study spherical functions of $\\mathbf{U}_q(\\mathfrak{g})$ related to characters. We show invariance under the Wang-Zhang braid group operators and show relative Weyl group invariance, when restricted to the quantum torus.","sentences":["Let $\\mathfrak{g}$ be a complex semisimple Lie algebra and let $\\mathbf{U}_q(\\mathfrak{g})$ denote the associated Drinfel'd","Jimbo quantized enveloping algebra.","In this paper we study spherical functions of $\\mathbf{U}_q(\\mathfrak{g})$ related to characters.","We show invariance under the Wang-Zhang braid group operators and show relative Weyl group invariance, when restricted to the quantum torus."],"url":"http://arxiv.org/abs/2405.15401v1","category":"math.RT"}
{"created":"2024-05-24 09:51:13","title":"Comparative Analysis of Four Prominent Ant Colony Optimization Variants: Ant System, Rank-Based Ant System, Max-Min Ant System, and Ant Colony System","abstract":"This research conducts a comparative analysis of four Ant Colony Optimization (ACO) variants -- Ant System (AS), Rank-Based Ant System (ASRank), Max-Min Ant System (MMAS), and Ant Colony System (ACS) -- for solving the Traveling Salesman Problem (TSP). Our findings demonstrate that algorithm performance is significantly influenced by problem scale and instance type. ACS excels in smaller TSP instances due to its rapid convergence, while PACS proves more adaptable for medium-sized problems. MMAS consistently achieves competitive results across all scales, particularly for larger instances, due to its ability to avoid local optima. ASRank, however, struggles to match the performance of the other algorithms. This research provides insights into the strengths and weaknesses of these ACO variants, guiding the selection of the most suitable algorithm for specific TSP applications.","sentences":["This research conducts a comparative analysis of four Ant Colony Optimization (ACO) variants -- Ant System (AS), Rank-Based Ant System (ASRank), Max-Min Ant System (MMAS), and Ant Colony System (ACS) -- for solving the Traveling Salesman Problem (TSP).","Our findings demonstrate that algorithm performance is significantly influenced by problem scale and instance type.","ACS excels in smaller TSP instances due to its rapid convergence, while PACS proves more adaptable for medium-sized problems.","MMAS consistently achieves competitive results across all scales, particularly for larger instances, due to its ability to avoid local optima.","ASRank, however, struggles to match the performance of the other algorithms.","This research provides insights into the strengths and weaknesses of these ACO variants, guiding the selection of the most suitable algorithm for specific TSP applications."],"url":"http://arxiv.org/abs/2405.15397v1","category":"cs.NE"}
{"created":"2024-05-24 09:23:43","title":"Fast, accurate training and sampling of Restricted Boltzmann Machines","abstract":"Thanks to their simple architecture, Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting interpretable insights from data. However, training RBMs, as other energy-based models, on highly structured data poses a major challenge, as effective training relies on mixing the Markov chain Monte Carlo simulations used to estimate the gradient. This process is often hindered by multiple second-order phase transitions and the associated critical slowdown. In this paper, we present an innovative method in which the principal directions of the dataset are integrated into a low-rank RBM through a convex optimization procedure. This approach enables efficient sampling of the equilibrium measure via a static Monte Carlo process. By starting the standard training process with a model that already accurately represents the main modes of the data, we bypass the initial phase transitions. Our results show that this strategy successfully trains RBMs to capture the full diversity of data in datasets where previous methods fail. Furthermore, we use the training trajectories to propose a new sampling method, {\\em parallel trajectory tempering}, which allows us to sample the equilibrium measure of the trained model much faster than previous optimized MCMC approaches and a better estimation of the log-likelihood. We illustrate the success of the training method on several highly structured datasets.","sentences":["Thanks to their simple architecture, Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting interpretable insights from data.","However, training RBMs, as other energy-based models, on highly structured data poses a major challenge, as effective training relies on mixing the Markov chain Monte Carlo simulations used to estimate the gradient.","This process is often hindered by multiple second-order phase transitions and the associated critical slowdown.","In this paper, we present an innovative method in which the principal directions of the dataset are integrated into a low-rank RBM through a convex optimization procedure.","This approach enables efficient sampling of the equilibrium measure via a static Monte Carlo process.","By starting the standard training process with a model that already accurately represents the main modes of the data, we bypass the initial phase transitions.","Our results show that this strategy successfully trains RBMs to capture the full diversity of data in datasets where previous methods fail.","Furthermore, we use the training trajectories to propose a new sampling method, {\\em parallel trajectory tempering}, which allows us to sample the equilibrium measure of the trained model much faster than previous optimized MCMC approaches and a better estimation of the log-likelihood.","We illustrate the success of the training method on several highly structured datasets."],"url":"http://arxiv.org/abs/2405.15376v1","category":"cs.LG"}
{"created":"2024-05-24 09:07:02","title":"Large Language Models can Deliver Accurate and Interpretable Time Series Anomaly Detection","abstract":"Time series anomaly detection (TSAD) plays a crucial role in various industries by identifying atypical patterns that deviate from standard trends, thereby maintaining system integrity and enabling prompt response measures. Traditional TSAD models, which often rely on deep learning, require extensive training data and operate as black boxes, lacking interpretability for detected anomalies. To address these challenges, we propose LLMAD, a novel TSAD method that employs Large Language Models (LLMs) to deliver accurate and interpretable TSAD results. LLMAD innovatively applies LLMs for in-context anomaly detection by retrieving both positive and negative similar time series segments, significantly enhancing LLMs' effectiveness. Furthermore, LLMAD employs the Anomaly Detection Chain-of-Thought (AnoCoT) approach to mimic expert logic for its decision-making process. This method further enhances its performance and enables LLMAD to provide explanations for their detections through versatile perspectives, which are particularly important for user decision-making. Experiments on three datasets indicate that our LLMAD achieves detection performance comparable to state-of-the-art deep learning methods while offering remarkable interpretability for detections. To the best of our knowledge, this is the first work that directly employs LLMs for TSAD.","sentences":["Time series anomaly detection (TSAD) plays a crucial role in various industries by identifying atypical patterns that deviate from standard trends, thereby maintaining system integrity and enabling prompt response measures.","Traditional TSAD models, which often rely on deep learning, require extensive training data and operate as black boxes, lacking interpretability for detected anomalies.","To address these challenges, we propose LLMAD, a novel TSAD method that employs Large Language Models (LLMs) to deliver accurate and interpretable TSAD results.","LLMAD innovatively applies LLMs for in-context anomaly detection by retrieving both positive and negative similar time series segments, significantly enhancing LLMs' effectiveness.","Furthermore, LLMAD employs the Anomaly Detection Chain-of-Thought (AnoCoT) approach to mimic expert logic for its decision-making process.","This method further enhances its performance and enables LLMAD to provide explanations for their detections through versatile perspectives, which are particularly important for user decision-making.","Experiments on three datasets indicate that our LLMAD achieves detection performance comparable to state-of-the-art deep learning methods while offering remarkable interpretability for detections.","To the best of our knowledge, this is the first work that directly employs LLMs for TSAD."],"url":"http://arxiv.org/abs/2405.15370v1","category":"cs.CL"}
{"created":"2024-05-24 09:01:27","title":"X-ray Coulomb explosion imaging reveals role of molecular structure in internal conversion","abstract":"Molecular photoabsorption results in an electronic excitation/ionization which couples to the rearrangement of the nuclei. The resulting intertwined change of nuclear and electronic degrees of freedom determines the conversion of photoenergy into other molecular energy forms. Nucleobases are excellent candidates for studying such dynamics, and great effort has been taken in the past to observe the electronic changes induced by the initial excitation in a time-resolved manner using ultrafast electron spectroscopy. The linked geometrical changes during nucleobase photorelaxation have so far not been observed directly in time-resolved experiments. Here, we present a study on a thionucleobase, where we extract comprehensive information on the molecular rearrangement using Coulomb explosion imaging. Our measurement links the extracted deplanarization of the molecular geometry to the previously studied temporal evolution of the electronic properties of the system. In particular, the protons of the exploded molecule are well-suited messengers carrying rich information on the molecule's geometry at distinct times after the initial electronic excitation. The combination of ultrashort laser pulses to trigger molecular dynamics, intense X-ray free-electron laser pulses for the explosion of the molecule, and multi-particle coincidence detection opens new avenues for time-resolved studies of complex molecules in the gas phase.","sentences":["Molecular photoabsorption results in an electronic excitation/ionization which couples to the rearrangement of the nuclei.","The resulting intertwined change of nuclear and electronic degrees of freedom determines the conversion of photoenergy into other molecular energy forms.","Nucleobases are excellent candidates for studying such dynamics, and great effort has been taken in the past to observe the electronic changes induced by the initial excitation in a time-resolved manner using ultrafast electron spectroscopy.","The linked geometrical changes during nucleobase photorelaxation have so far not been observed directly in time-resolved experiments.","Here, we present a study on a thionucleobase, where we extract comprehensive information on the molecular rearrangement using Coulomb explosion imaging.","Our measurement links the extracted deplanarization of the molecular geometry to the previously studied temporal evolution of the electronic properties of the system.","In particular, the protons of the exploded molecule are well-suited messengers carrying rich information on the molecule's geometry at distinct times after the initial electronic excitation.","The combination of ultrashort laser pulses to trigger molecular dynamics, intense X-ray free-electron laser pulses for the explosion of the molecule, and multi-particle coincidence detection opens new avenues for time-resolved studies of complex molecules in the gas phase."],"url":"http://arxiv.org/abs/2405.15367v1","category":"physics.chem-ph"}
{"created":"2024-05-24 08:41:33","title":"Normalized ground states for the mass supercritical Schr\u00f6dinger-Bopp-Podolsky system: existence, limit behavior, strong instability","abstract":"This paper concerns the normalized ground states for the nonlinear Schr\\\"{o}dinger equation in the Bopp-Podolsky electrodynamics. This equation has a nonlocal nonlinearity and a mass supercritical power nonlinearity, both of which have deep impact on the geometry of the corresponding functional, and thus on the existence, limit behavior and stability of the normalized ground states. In the present study, the existence of critical points is obtained by a mountain-pass argument developed on the $L^2$-spheres. To be specific, we show that normalized ground states exist, provided that spherical radius of the $L^2$-spheres is sufficiently small. Then, by discussing the convergence relation of the minimizer for the minimization problem between the Schr\\\"{o}dinger-Bopp-Podolsky system and the minimization problem of the classical Schrodinger equation, we show a precise description of the asymptotic behavior of the normalized ground states as the mass vanishes or tends to infinity. Finally, the strong instability of standing waves at the mountain-pass energy level is studied by constructing an equivalent minimizing problem. Also, as a byproduct, we prove that the mountain-pass energy level gives a threshold for global existence based on this equivalent minimizing problem.","sentences":["This paper concerns the normalized ground states for the nonlinear Schr\\\"{o}dinger equation in the Bopp-Podolsky electrodynamics.","This equation has a nonlocal nonlinearity and a mass supercritical power nonlinearity, both of which have deep impact on the geometry of the corresponding functional, and thus on the existence, limit behavior and stability of the normalized ground states.","In the present study, the existence of critical points is obtained by a mountain-pass argument developed on the $L^2$-spheres.","To be specific, we show that normalized ground states exist, provided that spherical radius of the $L^2$-spheres is sufficiently small.","Then, by discussing the convergence relation of the minimizer for the minimization problem between the Schr\\\"{o}dinger-Bopp-Podolsky system and the minimization problem of the classical Schrodinger equation, we show a precise description of the asymptotic behavior of the normalized ground states as the mass vanishes or tends to infinity.","Finally, the strong instability of standing waves at the mountain-pass energy level is studied by constructing an equivalent minimizing problem.","Also, as a byproduct, we prove that the mountain-pass energy level gives a threshold for global existence based on this equivalent minimizing problem."],"url":"http://arxiv.org/abs/2405.15347v1","category":"math.AP"}
{"created":"2024-05-24 17:53:06","title":"Looking Backward: Streaming Video-to-Video Translation with Feature Banks","abstract":"This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames. At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. This is realized by maintaining a feature bank, which archives information from past frames. For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output. The feature bank is continually updated by merging stored and new features, making it compact but informative. StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative metrics and user studies confirm StreamV2V's exceptional ability to maintain temporal consistency.","sentences":["This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts.","Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames.","At the heart of StreamV2V lies a backward-looking principle that relates the present to the past.","This is realized by maintaining a feature bank, which archives information from past frames.","For incoming frames, StreamV2V extends self-attention to include banked keys and values and directly fuses similar past features into the output.","The feature bank is continually updated by merging stored and new features, making it compact but informative.","StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning.","It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively.","Quantitative metrics and user studies confirm StreamV2V's exceptional ability to maintain temporal consistency."],"url":"http://arxiv.org/abs/2405.15757v1","category":"cs.CV"}
{"created":"2024-05-24 17:42:15","title":"Over-the-Air Runtime Wi-Fi MAC Address Re-randomization","abstract":"Medium Access Control (MAC) address randomization is a key component for privacy protection in Wi-Fi networks. Current proposals periodically change the mobile device MAC addresses when it disconnects from the Access Point (AP). This way frames cannot be linked across changes, but the mobile device presence is exposed as long as it remains connected: all its communication is trivially linkable by observing the randomized yet same MAC address throughout the connection. Our runtime MAC re-randomization scheme addresses this issue, reducing or eliminating Wi-Fi frames linkability without awaiting for or requiring a disconnection. Our MAC re-randomization is practically 'over-the-air': MAC addresses are re-randomized just before transmission, while the protocol stacks (at the mobile and the AP) maintain locally the original connection MAC addresses - making our MAC layer scheme transparent to upper layers. With an implementation and a set of small-scale experiments with off-the-shelf devices, we show the feasibility of our scheme and the potential towards future deployment.","sentences":["Medium Access Control (MAC) address randomization is a key component for privacy protection in Wi-Fi networks.","Current proposals periodically change the mobile device MAC addresses when it disconnects from the Access Point (AP).","This way frames cannot be linked across changes, but the mobile device presence is exposed as long as it remains connected: all its communication is trivially linkable by observing the randomized yet same MAC address throughout the connection.","Our runtime MAC re-randomization scheme addresses this issue, reducing or eliminating Wi-Fi frames linkability without awaiting for or requiring a disconnection.","Our MAC re-randomization is practically 'over-the-air': MAC addresses are re-randomized just before transmission, while the protocol stacks (at the mobile and the AP) maintain locally the original connection MAC addresses - making our MAC layer scheme transparent to upper layers.","With an implementation and a set of small-scale experiments with off-the-shelf devices, we show the feasibility of our scheme and the potential towards future deployment."],"url":"http://arxiv.org/abs/2405.15747v1","category":"cs.NI"}
{"created":"2024-05-24 17:39:26","title":"Sparse maximal update parameterization: A holistic approach to sparse training dynamics","abstract":"Several challenges make it difficult for sparse neural networks to compete with dense models. First, setting a large fraction of weights to zero impairs forward and gradient signal propagation. Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs. Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models. Unfortunately, we show sparse and dense networks do not share the same optimal HPs. Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware. A holistic approach is needed to tackle these challenges and we propose S$\\mu$Par as one such approach. S$\\mu$Par ensures activations, gradients, and weight updates all scale independently of sparsity level. Further, by reparameterizing the HPs, S$\\mu$Par enables the same HP values to be optimal as we vary both sparsity level and model width. HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs. On large-scale language modeling, S$\\mu$Par training improves loss by up to 8.2% over the common approach of using the dense model standard parameterization.","sentences":["Several challenges make it difficult for sparse neural networks to compete with dense models.","First, setting a large fraction of weights to zero impairs forward and gradient signal propagation.","Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs.","Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models.","Unfortunately, we show sparse and dense networks do not share the same optimal HPs.","Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware.","A holistic approach is needed to tackle these challenges and we propose S$\\mu$Par as one such approach.","S$\\mu$Par ensures activations, gradients, and weight updates all scale independently of sparsity level.","Further, by reparameterizing the HPs, S$\\mu$Par enables the same HP values to be optimal as we vary both sparsity level and model width.","HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs.","On large-scale language modeling, S$\\mu$Par training improves loss by up to 8.2% over the common approach of using the dense model standard parameterization."],"url":"http://arxiv.org/abs/2405.15743v1","category":"cs.LG"}
{"created":"2024-05-24 17:36:22","title":"On Flexible Inverse Probability of Treatment and Intensity Weighting: Informative Censoring, Variable Inclusion, and Weight Trimming","abstract":"Many observational studies feature irregular longitudinal data, where the observation times are not common across individuals in the study. Further, the observation times may be related to the longitudinal outcome. In this setting, failing to account for the informative observation process may result in biased causal estimates. This can be coupled with other sources of bias, including non-randomized treatment assignments and informative censoring. This paper provides an overview of a flexible weighting method used to adjust for informative observation processes and non-randomized treatment assignments. We investigate the sensitivity of the flexible weighting method to violations of the noninformative censoring assumption, examine variable selection for the observation process weighting model, known as inverse intensity weighting, and look at the impacts of weight trimming for the flexible weighting model. We show that the flexible weighting method is sensitive to violations of the noninformative censoring assumption and show that a previously proposed extension fails under such violations. We also show that variables confounding the observation and outcome processes should always be included in the observation intensity model. Finally, we show that weight trimming should be applied in the flexible weighting model when the treatment assignment process is highly informative and driving the extreme weights. We conclude with an application of the methodology to a real data set to examine the impacts of household water sources on malaria diagnoses.","sentences":["Many observational studies feature irregular longitudinal data, where the observation times are not common across individuals in the study.","Further, the observation times may be related to the longitudinal outcome.","In this setting, failing to account for the informative observation process may result in biased causal estimates.","This can be coupled with other sources of bias, including non-randomized treatment assignments and informative censoring.","This paper provides an overview of a flexible weighting method used to adjust for informative observation processes and non-randomized treatment assignments.","We investigate the sensitivity of the flexible weighting method to violations of the noninformative censoring assumption, examine variable selection for the observation process weighting model, known as inverse intensity weighting, and look at the impacts of weight trimming for the flexible weighting model.","We show that the flexible weighting method is sensitive to violations of the noninformative censoring assumption and show that a previously proposed extension fails under such violations.","We also show that variables confounding the observation and outcome processes should always be included in the observation intensity model.","Finally, we show that weight trimming should be applied in the flexible weighting model when the treatment assignment process is highly informative and driving the extreme weights.","We conclude with an application of the methodology to a real data set to examine the impacts of household water sources on malaria diagnoses."],"url":"http://arxiv.org/abs/2405.15740v1","category":"stat.ME"}
{"created":"2024-05-24 17:33:10","title":"Single-Round Proofs of Quantumness from Knowledge Assumptions","abstract":"A proof of quantumness is an efficiently verifiable interactive test that an efficient quantum computer can pass, but all efficient classical computers cannot (under some cryptographic assumption). Such protocols play a crucial role in the certification of quantum devices. Existing single-round protocols (like asking the quantum computer to factor a large number) require large quantum circuits, whereas multi-round ones use smaller circuits but require experimentally challenging mid-circuit measurements. As such, current proofs of quantumness are out of reach for near-term devices.   In this work, we construct efficient single-round proofs of quantumness based on existing knowledge assumptions. While knowledge assumptions have not been previously considered in this context, we show that they provide a natural basis for separating classical and quantum computation. Specifically, we show that multi-round protocols based on Decisional Diffie-Hellman (DDH) or Learning With Errors (LWE) can be \"compiled\" into single-round protocols using a knowledge-of-exponent assumption or knowledge-of-lattice-point assumption, respectively. We also prove an adaptive hardcore-bit statement for a family of claw-free functions based on DDH, which might be of independent interest.   Previous approaches to constructing single-round protocols relied on the random oracle model and thus incurred the overhead associated with instantiating the oracle with a cryptographic hash function. In contrast, our protocols have the same resource requirements as their multi-round counterparts without necessitating mid-circuit measurements, making them, arguably, the most efficient single-round proofs of quantumness to date. Our work also helps in understanding the interplay between black-box/white-box reductions and cryptographic assumptions in the design of proofs of quantumness.","sentences":["A proof of quantumness is an efficiently verifiable interactive test that an efficient quantum computer can pass, but all efficient classical computers cannot (under some cryptographic assumption).","Such protocols play a crucial role in the certification of quantum devices.","Existing single-round protocols (like asking the quantum computer to factor a large number) require large quantum circuits, whereas multi-round ones use smaller circuits but require experimentally challenging mid-circuit measurements.","As such, current proofs of quantumness are out of reach for near-term devices.   ","In this work, we construct efficient single-round proofs of quantumness based on existing knowledge assumptions.","While knowledge assumptions have not been previously considered in this context, we show that they provide a natural basis for separating classical and quantum computation.","Specifically, we show that multi-round protocols based on Decisional Diffie-Hellman (DDH) or Learning With Errors (LWE) can be \"compiled\" into single-round protocols using a knowledge-of-exponent assumption or knowledge-of-lattice-point assumption, respectively.","We also prove an adaptive hardcore-bit statement for a family of claw-free functions based on DDH, which might be of independent interest.   ","Previous approaches to constructing single-round protocols relied on the random oracle model and thus incurred the overhead associated with instantiating the oracle with a cryptographic hash function.","In contrast, our protocols have the same resource requirements as their multi-round counterparts without necessitating mid-circuit measurements, making them, arguably, the most efficient single-round proofs of quantumness to date.","Our work also helps in understanding the interplay between black-box/white-box reductions and cryptographic assumptions in the design of proofs of quantumness."],"url":"http://arxiv.org/abs/2405.15736v1","category":"quant-ph"}
{"created":"2024-05-24 17:06:51","title":"Hierarchical Uncertainty Exploration via Feedforward Posterior Trees","abstract":"When solving ill-posed inverse problems, one often desires to explore the space of potential solutions rather than be presented with a single plausible reconstruction. Valuable insights into these feasible solutions and their associated probabilities are embedded in the posterior distribution. However, when confronted with data of high dimensionality (such as images), visualizing this distribution becomes a formidable challenge, necessitating the application of effective summarization techniques before user examination. In this work, we introduce a new approach for visualizing posteriors across multiple levels of granularity using tree-valued predictions. Our method predicts a tree-valued hierarchical summarization of the posterior distribution for any input measurement, in a single forward pass of a neural network. We showcase the efficacy of our approach across diverse datasets and image restoration challenges, highlighting its prowess in uncertainty quantification and visualization. Our findings reveal that our method performs comparably to a baseline that hierarchically clusters samples from a diffusion-based posterior sampler, yet achieves this with orders of magnitude greater speed.","sentences":["When solving ill-posed inverse problems, one often desires to explore the space of potential solutions rather than be presented with a single plausible reconstruction.","Valuable insights into these feasible solutions and their associated probabilities are embedded in the posterior distribution.","However, when confronted with data of high dimensionality (such as images), visualizing this distribution becomes a formidable challenge, necessitating the application of effective summarization techniques before user examination.","In this work, we introduce a new approach for visualizing posteriors across multiple levels of granularity using tree-valued predictions.","Our method predicts a tree-valued hierarchical summarization of the posterior distribution for any input measurement, in a single forward pass of a neural network.","We showcase the efficacy of our approach across diverse datasets and image restoration challenges, highlighting its prowess in uncertainty quantification and visualization.","Our findings reveal that our method performs comparably to a baseline that hierarchically clusters samples from a diffusion-based posterior sampler, yet achieves this with orders of magnitude greater speed."],"url":"http://arxiv.org/abs/2405.15719v1","category":"cs.CV"}
{"created":"2024-05-24 16:53:30","title":"Digitized Counterdiabatic Quantum Algorithms for Logistics Scheduling","abstract":"We study a job shop scheduling problem for an automatized robot in a high-throughput laboratory and a travelling salesperson problem with recently proposed digitized counterdiabatic quantum optimization (DCQO) algorithms. In DCQO, we find the solution of an optimization problem via an adiabatic quantum dynamics, which is accelerated with counterdiabatic protocols. Thereafter, we digitize the global unitary to encode it in a digital quantum computer. For the job-shop scheduling problem, we aim at finding the optimal schedule for a robot executing a number of tasks under specific constraints, such that the total execution time of the process is minimized. For the traveling salesperson problem, the goal is to find the path that covers all cities and is associated with the shortest traveling distance. We consider both hybrid and pure versions of DCQO algorithms and benchmark the performance against digitized quantum annealing and the quantum approximate optimization algorithm (QAOA). In comparison to QAOA, the DCQO solution is improved by several orders of magnitude in success probability using the same number of two-qubit gates. Moreover, we experimentally implement our algorithms on superconducting and trapped-ion quantum processors. Our results demonstrate that circuit compression using counterdiabatic protocols is amenable to current NISQ hardware and can solve logistics scheduling problems, where other digital quantum algorithms show insufficient performance.","sentences":["We study a job shop scheduling problem for an automatized robot in a high-throughput laboratory and a travelling salesperson problem with recently proposed digitized counterdiabatic quantum optimization (DCQO) algorithms.","In DCQO, we find the solution of an optimization problem via an adiabatic quantum dynamics, which is accelerated with counterdiabatic protocols.","Thereafter, we digitize the global unitary to encode it in a digital quantum computer.","For the job-shop scheduling problem, we aim at finding the optimal schedule for a robot executing a number of tasks under specific constraints, such that the total execution time of the process is minimized.","For the traveling salesperson problem, the goal is to find the path that covers all cities and is associated with the shortest traveling distance.","We consider both hybrid and pure versions of DCQO algorithms and benchmark the performance against digitized quantum annealing and the quantum approximate optimization algorithm (QAOA).","In comparison to QAOA, the DCQO solution is improved by several orders of magnitude in success probability using the same number of two-qubit gates.","Moreover, we experimentally implement our algorithms on superconducting and trapped-ion quantum processors.","Our results demonstrate that circuit compression using counterdiabatic protocols is amenable to current NISQ hardware and can solve logistics scheduling problems, where other digital quantum algorithms show insufficient performance."],"url":"http://arxiv.org/abs/2405.15707v1","category":"quant-ph"}
{"created":"2024-05-24 16:38:13","title":"Interplay between hyperfine and anisotropic exchange interactions in exciton luminescence of quantum dots","abstract":"The optical orientation and alignment of excitons in semiconductor indirect gap quantum dots have been studied theoretically. A special regime is analyzed in which the energy of the hyperfine interaction of an electron with lattice nuclei is small compared to the exchange splitting between bright and dark excitonic levels, but is comparable to the anisotropic exchange splitting of the radiative doublet. The dependencies of degrees of circular and linear polarization on the external magnetic field under resonant excitation of excitons by polarized light are calculated.","sentences":["The optical orientation and alignment of excitons in semiconductor indirect gap quantum dots have been studied theoretically.","A special regime is analyzed in which the energy of the hyperfine interaction of an electron with lattice nuclei is small compared to the exchange splitting between bright and dark excitonic levels, but is comparable to the anisotropic exchange splitting of the radiative doublet.","The dependencies of degrees of circular and linear polarization on the external magnetic field under resonant excitation of excitons by polarized light are calculated."],"url":"http://arxiv.org/abs/2405.15696v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-24 16:37:01","title":"A unified approach to some rigidity and stability problems in algebra","abstract":"In this note, we use give some algebraic applications of a previous result by the author which compares the deformations parameterized by the Maurer-Cartan elements of a differential graded Lie algebra, and a differential graded Lie subalgebra: It gives a criterion for the map on the space of Maurer-Cartan elements up to gauge equivalence, induced by the inclusion of the subalgebra, to be locally surjective. By making appropriate choices for the differential graded Lie algebra and a differential graded Lie subalgebra, we recover some classical results in the deformation theory of finite-dimensional Lie and associative algebras. We consider rigidity of Lie and associative algebras, Lie algebra morphisms, and give a quick proof of the fact that the deformation theories of a unital associative algebra as a unital associative algebra and as an associative algebra are equivalent. We then turn to stability of Lie subalgebras and their morphisms under deformations of the ambient Lie algebra structure, and study when the compatibility with a geometric structure of a representation of a Lie algebra on a vector space is stable under deformations of the representation.","sentences":["In this note, we use give some algebraic applications of a previous result by the author which compares the deformations parameterized by the Maurer-Cartan elements of a differential graded Lie algebra, and a differential graded Lie subalgebra: It gives a criterion for the map on the space of Maurer-Cartan elements up to gauge equivalence, induced by the inclusion of the subalgebra, to be locally surjective.","By making appropriate choices for the differential graded Lie algebra and a differential graded Lie subalgebra, we recover some classical results in the deformation theory of finite-dimensional Lie and associative algebras.","We consider rigidity of Lie and associative algebras, Lie algebra morphisms, and give a quick proof of the fact that the deformation theories of a unital associative algebra as a unital associative algebra and as an associative algebra are equivalent.","We then turn to stability of Lie subalgebras and their morphisms under deformations of the ambient Lie algebra structure, and study when the compatibility with a geometric structure of a representation of a Lie algebra on a vector space is stable under deformations of the representation."],"url":"http://arxiv.org/abs/2405.15694v1","category":"math.RT"}
{"created":"2024-05-24 16:36:45","title":"Non-invertible symmetries in $S_N$ orbifold CFTs and holography","abstract":"We study non-invertible defects in two-dimensional $S_N$ orbifold CFTs. We construct universal defects which do not depend on the details of the seed CFT and hence exist in any orbifold CFT. Additionally, we investigate non-universal defects arising from the topological defects of the seed CFT. We argue that there exist universal defects that are non-trivial in the large-$N$ limit, making them relevant for the AdS$_3$/CFT$_2$ correspondence. We then focus on AdS$_3\\times$S$^3\\times \\mathcal M_4$ with one unit of NS-NS flux and propose an explicit realization of these defects on the worldsheet.","sentences":["We study non-invertible defects in two-dimensional $S_N$ orbifold CFTs.","We construct universal defects which do not depend on the details of the seed CFT and hence exist in any orbifold CFT.","Additionally, we investigate non-universal defects arising from the topological defects of the seed CFT.","We argue that there exist universal defects that are non-trivial in the large-$N$ limit, making them relevant for the AdS$_3$/CFT$_2$ correspondence.","We then focus on AdS$_3\\times$S$^3\\times \\mathcal M_4$ with one unit of NS-NS flux and propose an explicit realization of these defects on the worldsheet."],"url":"http://arxiv.org/abs/2405.15693v1","category":"hep-th"}
{"created":"2024-05-24 16:30:05","title":"Simulation-based inference of radio millisecond pulsars in globular clusters","abstract":"Millisecond pulsars (MSPs) are abundant in globular clusters (GCs), which offer favorable environments for their creation. While the advent of recent, powerful facilities led to a rapid increase in MSP discoveries in GCs through pulsation searches, detection biases persist. In this work, we investigate the ability of current and future detections in GCs to constrain the parameters of the MSP population in GCs through a careful study of their luminosity function. Parameters of interest are the number of MSPs hosted by a GC, as well as the mean and the width of their luminosity function, which are typically affected by large uncertainties. While, as we show, likelihood-based studies can lead to ill-behaved posterior on the size of the MSP population, we introduce a novel, likelihood-free analysis, based on Marginal Neural Ratio Estimation, which consistently produces well-behaved posteriors. We focus on the GC Terzan 5, which currently counts 48 detected MSPs. We find that about 158 MSPs should be hosted in this GC, but the uncertainty on this number remains large. We explore the performance of our new method on simulated Terzan 5-like datasets mimicking possible future observational outcomes. We find that significant improvement on the posteriors can be obtained by adding a reliable measurement of the diffuse radio emission of the GC to the analysis or by improving the detection threshold of current radio pulsation surveys by at least a factor two.","sentences":["Millisecond pulsars (MSPs) are abundant in globular clusters (GCs), which offer favorable environments for their creation.","While the advent of recent, powerful facilities led to a rapid increase in MSP discoveries in GCs through pulsation searches, detection biases persist.","In this work, we investigate the ability of current and future detections in GCs to constrain the parameters of the MSP population in GCs through a careful study of their luminosity function.","Parameters of interest are the number of MSPs hosted by a GC, as well as the mean and the width of their luminosity function, which are typically affected by large uncertainties.","While, as we show, likelihood-based studies can lead to ill-behaved posterior on the size of the MSP population, we introduce a novel, likelihood-free analysis, based on Marginal Neural Ratio Estimation, which consistently produces well-behaved posteriors.","We focus on the GC Terzan 5, which currently counts 48 detected MSPs.","We find that about 158 MSPs should be hosted in this GC, but the uncertainty on this number remains large.","We explore the performance of our new method on simulated Terzan 5-like datasets mimicking possible future observational outcomes.","We find that significant improvement on the posteriors can be obtained by adding a reliable measurement of the diffuse radio emission of the GC to the analysis or by improving the detection threshold of current radio pulsation surveys by at least a factor two."],"url":"http://arxiv.org/abs/2405.15691v1","category":"astro-ph.HE"}
{"created":"2024-05-24 16:27:05","title":"UNION: Unsupervised 3D Object Detection using Object Appearance-based Pseudo-Classes","abstract":"Unsupervised 3D object detection methods have emerged to leverage vast amounts of data efficiently without requiring manual labels for training. Recent approaches rely on dynamic objects for learning to detect objects but penalize the detections of static instances during training. Multiple rounds of (self) training are used in which detected static instances are added to the set of training targets; this procedure to improve performance is computationally expensive. To address this, we propose the method UNION. We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR. Subsequently, object proposals' visual appearances are encoded to distinguish static objects in the foreground and background by selecting static instances that are visually similar to dynamic objects. As a result, static and dynamic foreground objects are obtained together, and existing detectors can be trained with a single training. In addition, we extend 3D object discovery to detection by using object appearance-based cluster labels as pseudo-class labels for training object classification. We conduct extensive experiments on the nuScenes dataset and increase the state-of-the-art performance for unsupervised object discovery, i.e. UNION more than doubles the average precision to 33.9. The code will be made publicly available.","sentences":["Unsupervised 3D object detection methods have emerged to leverage vast amounts of data efficiently without requiring manual labels for training.","Recent approaches rely on dynamic objects for learning to detect objects but penalize the detections of static instances during training.","Multiple rounds of (self) training are used in which detected static instances are added to the set of training targets; this procedure to improve performance is computationally expensive.","To address this, we propose the method UNION.","We use spatial clustering and self-supervised scene flow to obtain a set of static and dynamic object proposals from LiDAR.","Subsequently, object proposals' visual appearances are encoded to distinguish static objects in the foreground and background by selecting static instances that are visually similar to dynamic objects.","As a result, static and dynamic foreground objects are obtained together, and existing detectors can be trained with a single training.","In addition, we extend 3D object discovery to detection by using object appearance-based cluster labels as pseudo-class labels for training object classification.","We conduct extensive experiments on the nuScenes dataset and increase the state-of-the-art performance for unsupervised object discovery, i.e. UNION more than doubles the average precision to 33.9.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2405.15688v1","category":"cs.CV"}
{"created":"2024-05-24 16:26:56","title":"Chain-of-Thought Prompting for Demographic Inference with Large Multimodal Models","abstract":"Conventional demographic inference methods have predominantly operated under the supervision of accurately labeled data, yet struggle to adapt to shifting social landscapes and diverse cultural contexts, leading to narrow specialization and limited accuracy in applications. Recently, the emergence of large multimodal models (LMMs) has shown transformative potential across various research tasks, such as visual comprehension and description. In this study, we explore the application of LMMs to demographic inference and introduce a benchmark for both quantitative and qualitative evaluation. Our findings indicate that LMMs possess advantages in zero-shot learning, interpretability, and handling uncurated 'in-the-wild' inputs, albeit with a propensity for off-target predictions. To enhance LMM performance and achieve comparability with supervised learning baselines, we propose a Chain-of-Thought augmented prompting approach, which effectively mitigates the off-target prediction issue.","sentences":["Conventional demographic inference methods have predominantly operated under the supervision of accurately labeled data, yet struggle to adapt to shifting social landscapes and diverse cultural contexts, leading to narrow specialization and limited accuracy in applications.","Recently, the emergence of large multimodal models (LMMs) has shown transformative potential across various research tasks, such as visual comprehension and description.","In this study, we explore the application of LMMs to demographic inference and introduce a benchmark for both quantitative and qualitative evaluation.","Our findings indicate that LMMs possess advantages in zero-shot learning, interpretability, and handling uncurated 'in-the-wild' inputs, albeit with a propensity for off-target predictions.","To enhance LMM performance and achieve comparability with supervised learning baselines, we propose a Chain-of-Thought augmented prompting approach, which effectively mitigates the off-target prediction issue."],"url":"http://arxiv.org/abs/2405.15687v1","category":"cs.CV"}
{"created":"2024-05-24 16:07:25","title":"The Undecidability of Quantified Announcements","abstract":"This paper demonstrates the undecidability of a number of logics with quantification over public announcements: arbitrary public announcement logic (APAL), group announcement logic (GAL), and coalition announcement logic (CAL). In APAL we consider the informative consequences of any announcement, in GAL we consider the informative consequences of a group of agents (this group may be a proper subset of the set of all agents) all of which are simultaneously (and publicly) making known announcements. So this is more restrictive than APAL. Finally, CAL is as GAL except that we now quantify over anything the agents not in that group may announce simultaneously as well. The logic CAL therefore has some features of game logic and of ATL. We show that when there are multiple agents in the language, the satisfiability problem is undecidable for APAL, GAL, and CAL. In the single agent case, the satisfiability problem is decidable for all three logics. This paper corrects an error to the submitted version of Undecidability of Quantified Announcements, identified by Yuta Assami . The nature of the error was in the definition of the formula cga (X) (see Subsection 5.2) which is corrected in this version.","sentences":["This paper demonstrates the undecidability of a number of logics with quantification over public announcements: arbitrary public announcement logic (APAL), group announcement logic (GAL), and coalition announcement logic (CAL).","In APAL we consider the informative consequences of any announcement, in GAL we consider the informative consequences of a group of agents (this group may be a proper subset of the set of all agents) all of which are simultaneously (and publicly) making known announcements.","So this is more restrictive than APAL.","Finally, CAL is as GAL except that we now quantify over anything the agents not in that group may announce simultaneously as well.","The logic CAL therefore has some features of game logic and of ATL.","We show that when there are multiple agents in the language, the satisfiability problem is undecidable for APAL, GAL, and CAL.","In the single agent case, the satisfiability problem is decidable for all three logics.","This paper corrects an error to the submitted version of Undecidability of Quantified Announcements, identified by Yuta Assami .","The nature of the error was in the definition of the formula cga (X) (see Subsection 5.2) which is corrected in this version."],"url":"http://arxiv.org/abs/2405.15671v1","category":"cs.LO"}
{"created":"2024-05-24 16:00:38","title":"Soft happy colourings and community structure of networks","abstract":"For $0<\\rho\\leq 1$, a $\\rho$-happy vertex $v$ in a coloured graph $G$ has at least $\\rho\\cdot \\mathrm{deg}(v)$ same-colour neighbours, and a $\\rho$-happy colouring (aka soft happy colouring) of $G$ is a vertex colouring that makes all the vertices $\\rho$-happy. A community is a subgraph whose vertices are more adjacent to themselves than the rest of the vertices. Graphs with community structures can be modelled by random graph models such as the stochastic block model (SBM). In this paper, we present several theorems showing that both of these notions are related, with numerous real-world applications. We show that, with high probability, communities of graphs in the stochastic block model induce $\\rho$-happy colouring on all vertices if certain conditions on the model parameters are satisfied. Moreover, a probabilistic threshold on $\\rho$ is derived so that communities of a graph in the SBM induce a $\\rho$-happy colouring. Furthermore, the asymptotic behaviour of $\\rho$-happy colouring induced by the graph's communities is discussed when $\\rho$ is less than a threshold. We develop heuristic polynomial-time algorithms for soft happy colouring that often correlate with the graphs' community structure. Finally, we present an experimental evaluation to compare the performance of the proposed algorithms thereby demonstrating the validity of the theoretical results.","sentences":["For $0<\\rho\\leq 1$, a $\\rho$-happy vertex $v$ in a coloured graph $G$ has at least $\\rho\\cdot \\mathrm{deg}(v)$ same-colour neighbours, and a $\\rho$-happy colouring (aka soft happy colouring) of $G$ is a vertex colouring that makes all the vertices $\\rho$-happy.","A community is a subgraph whose vertices are more adjacent to themselves than the rest of the vertices.","Graphs with community structures can be modelled by random graph models such as the stochastic block model (SBM).","In this paper, we present several theorems showing that both of these notions are related, with numerous real-world applications.","We show that, with high probability, communities of graphs in the stochastic block model induce $\\rho$-happy colouring on all vertices if certain conditions on the model parameters are satisfied.","Moreover, a probabilistic threshold on $\\rho$ is derived so that communities of a graph in the SBM induce a $\\rho$-happy colouring.","Furthermore, the asymptotic behaviour of $\\rho$-happy colouring induced by the graph's communities is discussed when $\\rho$ is less than a threshold.","We develop heuristic polynomial-time algorithms for soft happy colouring that often correlate with the graphs' community structure.","Finally, we present an experimental evaluation to compare the performance of the proposed algorithms thereby demonstrating the validity of the theoretical results."],"url":"http://arxiv.org/abs/2405.15663v1","category":"cs.DM"}
{"created":"2024-05-24 15:44:31","title":"Asymptotic Theory for Estimation of the Husler-Reiss Distribution via Block Maxima Method","abstract":"The H\\\"usler-Reiss distribution describes the limit of the pointwise maxima of a bivariate normal distribution. This distribution is defined by a single parameter, $\\lambda$. We provide asymptotic theory for maximum likelihood estimation of $\\lambda$ under a block maxima approach. Our work assumes independent and identically distributed bivariate normal random variables, grouped into blocks where the block size and number of blocks increase simultaneously. With these assumptions our results provide conditions for the asymptotic normality of the Maximum Likelihood Estimator (MLE). We characterize the bias of the MLE, provide conditions under which this bias is asymptotically negligible, and discuss how to choose the block size to minimize a bias-variance trade-off. The proofs are an extension of previous results for choosing the block size in the estimation of univariate extreme value distributions (Dombry and Ferreria 2019), providing a potential basis for extensions to multivariate cases where both the marginal and dependence parameters are unknown. The proofs rely on the Argmax Theorem applied to a localized loglikelihood function, combined with a Lindeberg-Feller Central Limit Theorem argument to establish asymptotic normality. Possible applications of the method include composite likelihood estimation in Brown-Resnick processes, where it is known that the bivariate distributions are of H\\\"usler-Reiss form.","sentences":["The H\\\"usler-Reiss distribution describes the limit of the pointwise maxima of a bivariate normal distribution.","This distribution is defined by a single parameter, $\\lambda$. We provide asymptotic theory for maximum likelihood estimation of $\\lambda$ under a block maxima approach.","Our work assumes independent and identically distributed bivariate normal random variables, grouped into blocks where the block size and number of blocks increase simultaneously.","With these assumptions our results provide conditions for the asymptotic normality of the Maximum Likelihood Estimator (MLE).","We characterize the bias of the MLE, provide conditions under which this bias is asymptotically negligible, and discuss how to choose the block size to minimize a bias-variance trade-off.","The proofs are an extension of previous results for choosing the block size in the estimation of univariate extreme value distributions (Dombry and Ferreria 2019), providing a potential basis for extensions to multivariate cases where both the marginal and dependence parameters are unknown.","The proofs rely on the Argmax Theorem applied to a localized loglikelihood function, combined with a Lindeberg-Feller Central Limit Theorem argument to establish asymptotic normality.","Possible applications of the method include composite likelihood estimation in Brown-Resnick processes, where it is known that the bivariate distributions are of H\\\"usler-Reiss form."],"url":"http://arxiv.org/abs/2405.15649v1","category":"math.ST"}
{"created":"2024-05-24 14:53:33","title":"Increasing Efficiency and Result Reliability of Continuous Benchmarking for FaaS Applications","abstract":"In a continuous deployment setting, Function-as-a-Service (FaaS) applications frequently receive updated releases, each of which can cause a performance regression. While continuous benchmarking, i.e., comparing benchmark results of the updated and the previous version, can detect such regressions, performance variability of FaaS platforms necessitates thousands of function calls, thus, making continuous benchmarking time-intensive and expensive.   In this paper, we propose DuetFaaS, an approach which adapts duet benchmarking to FaaS applications. With DuetFaaS, we deploy two versions of FaaS function in a single cloud function instance and execute them in parallel to reduce the impact of platform variability. We evaluate our approach against state-of-the-art approaches, running on AWS Lambda. Overall, DuetFaaS requires fewer invocations to accurately detect performance regressions than other state-of-the-art approaches. In 99.65% of evaluated cases, our approach provides smaller confidence interval sizes than the comparing approaches, and can reduce the size by up to 98.23%.","sentences":["In a continuous deployment setting, Function-as-a-Service (FaaS) applications frequently receive updated releases, each of which can cause a performance regression.","While continuous benchmarking, i.e., comparing benchmark results of the updated and the previous version, can detect such regressions, performance variability of FaaS platforms necessitates thousands of function calls, thus, making continuous benchmarking time-intensive and expensive.   ","In this paper, we propose DuetFaaS, an approach which adapts duet benchmarking to FaaS applications.","With DuetFaaS, we deploy two versions of FaaS function in a single cloud function instance and execute them in parallel to reduce the impact of platform variability.","We evaluate our approach against state-of-the-art approaches, running on AWS Lambda.","Overall, DuetFaaS requires fewer invocations to accurately detect performance regressions than other state-of-the-art approaches.","In 99.65% of evaluated cases, our approach provides smaller confidence interval sizes than the comparing approaches, and can reduce the size by up to 98.23%."],"url":"http://arxiv.org/abs/2405.15610v1","category":"cs.DC"}
{"created":"2024-05-24 14:29:19","title":"One-level densities in families of Gr\u00f6ssencharakters associated to CM elliptic curves","abstract":"We study the low-lying zeros of a family of $L$-functions attached to the CM elliptic curves $E_d \\;:\\; y^2 = x^3 - dx$, for each odd and square-free integer $d$. Writing the $L$-function of $E_d$ as $L(s-\\frac12, \\xi_d)$ for the appropriate Gr\\\"ossencharakter $\\xi_d$ of conductor $\\mathfrak{f}_d$, the family $\\mathcal F_d$ is defined as the family of $L$-functions attached to the Gr\\\"ossencharakters $\\xi_{d,k}$, where for each integer $k \\geq 1$, $\\xi_{d, k}$ denotes the primitive character inducing $\\xi_d^k$. We observe that the average root number over the family $\\mathcal F_d$ is $\\frac14$, which makes the symmetry type of the family (unitary, symplectic or orthogonal) somehow mysterious, as none of the symmetry types would lead to this average value. By computing the one-level density, we find that $\\mathcal F_d$ breaks down into two natural subfamilies, namely a symplectic family ($L(s, \\xi_{d,k})$ for $k$ even) and an orthogonal family ($L(s, \\xi_{d,k})$ for $k$ odd). For $k$ odd, $\\mathcal F_d$ is in fact a subfamily of the automorphic forms of fixed level $4 N (\\mathfrak{f}_d )$, and even weight $k+1$, and this larger family also has orthogonal symmetry. The main term of the one-level density gives the symmetry and we also compute explicit lower order terms for each case.","sentences":["We study the low-lying zeros of a family of $L$-functions attached to the CM elliptic curves $E_d \\;:\\; y^2 = x^3 - dx$, for each odd and square-free integer $d$. Writing the $L$-function of $E_d$ as $L(s-\\frac12, \\xi_d)$ for the appropriate Gr\\\"ossencharakter $\\xi_d$ of conductor $\\mathfrak{f}_d$, the family $\\mathcal F_d$ is defined as the family of $L$-functions attached to the Gr\\\"ossencharakters $\\xi_{d,k}$, where for each integer $k \\geq 1$, $\\xi_{d, k}$ denotes the primitive character inducing $\\xi_d^k$. We observe that the average root number over the family $\\mathcal F_d$ is $\\frac14$, which makes the symmetry type of the family (unitary, symplectic or orthogonal) somehow mysterious, as none of the symmetry types would lead to this average value.","By computing the one-level density, we find that $\\mathcal F_d$ breaks down into two natural subfamilies, namely a symplectic family ($L(s, \\xi_{d,k})$ for $k$ even) and an orthogonal family ($L(s, \\xi_{d,k})$ for $k$ odd).","For $k$ odd, $\\mathcal F_d$ is in fact a subfamily of the automorphic forms of fixed level $4 N (\\mathfrak{f}_d )$, and even weight $k+1$, and this larger family also has orthogonal symmetry.","The main term of the one-level density gives the symmetry and we also compute explicit lower order terms for each case."],"url":"http://arxiv.org/abs/2405.15597v1","category":"math.NT"}
{"created":"2024-05-24 14:20:09","title":"Efficient Adversarial Training in LLMs with Continuous Attacks","abstract":"Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs.","sentences":["Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails.","In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks.","Yet, in the context of LLMs, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration.","We address this problem by instead calculating adversarial attacks in the continuous embedding space of the LLM, which is orders of magnitudes more efficient.","We propose a fast adversarial training algorithm (C-AdvUL) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data.","Moreover, we introduce C-AdvIPO, an adversarial variant of IPO that does not require utility data for adversarially robust alignment.","Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance LLM robustness against discrete attacks (GCG, AutoDAN, PAIR), while maintaining utility.","Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models.","Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning LLMs."],"url":"http://arxiv.org/abs/2405.15589v1","category":"cs.LG"}
{"created":"2024-05-24 14:20:00","title":"Modified 3D Massive Abelian 2-From Theory with a Single Pseudo-Scalar Field: BRST Approach","abstract":"We obtain the off-shell nilpotent Becchi-Rouet-Stora-Tyutin (BRST) and anti-BRST symmetry transformations (corresponding to the infinitesimal classical gauge symmetry transformations) for the St${\\ddot u}$ckelberg-modified massive three $(2 + 1)$-dimensional (3D) Abelian 2-form gauge theory with a single pseudo-scalar field. The latter field (having the negative kinetic term and a well-defined mass) has already been shown (i) to exist in the modified version of the standard 3D St${\\ddot u}$ckelberg formalism (on solid mathematical grounds), (ii) to be a possible candidate for dark matter, and (iii) to correspond to the ``phantom\" field of some of the cosmological models of the Universe. A couple of highlights of our present endeavor are (i) the observation that, even though the pseudo-scalar field does not transform under the gauge and (anti-)BRST symmetry transformations, it appears in the first-class constraints which annihilate the physical states at the quantum level, and (ii) the Noether conserved (anti-)BRST charges are found to be non-nilpotent. In our present investigation, we derive (i) the coupled (but equivalent) BRST and anti-BRST invariant Lagrangian densities, (ii) the conserved and off-shell nilpotent versions of the (anti-)BRST charges and the conserved ghost charge, (iii) the (anti-)BRST invariant Curci-Ferrari (CF) type restrictions, and (iv) the standard BRST algebra amongst the conserved and nilpotent (anti-)BRST and conserved ghost charges of our theory.","sentences":["We obtain the off-shell nilpotent Becchi-Rouet-Stora-Tyutin (BRST) and anti-BRST symmetry transformations (corresponding to the infinitesimal classical gauge symmetry transformations) for the St${\\ddot u}$ckelberg-modified massive three $(2 + 1)$-dimensional (3D)","Abelian 2-form gauge theory with a single pseudo-scalar field.","The latter field (having the negative kinetic term and a well-defined mass) has already been shown (i) to exist in the modified version of the standard 3D St${\\ddot u}$ckelberg formalism (on solid mathematical grounds), (ii) to be a possible candidate for dark matter, and (iii) to correspond to the ``phantom\" field of some of the cosmological models of the Universe.","A couple of highlights of our present endeavor are (i) the observation that, even though the pseudo-scalar field does not transform under the gauge and (anti-)BRST symmetry transformations, it appears in the first-class constraints which annihilate the physical states at the quantum level, and (ii) the Noether conserved (anti-)BRST charges are found to be non-nilpotent.","In our present investigation, we derive (i) the coupled (but equivalent) BRST and anti-BRST invariant Lagrangian densities, (ii) the conserved and off-shell nilpotent versions of the (anti-)BRST charges and the conserved ghost charge, (iii) the (anti-)BRST invariant Curci-Ferrari (CF) type restrictions, and (iv) the standard BRST algebra amongst the conserved and nilpotent (anti-)BRST and conserved ghost charges of our theory."],"url":"http://arxiv.org/abs/2405.15588v1","category":"hep-th"}
{"created":"2024-05-24 14:12:27","title":"Unveiling the True Nature of Plasma Dynamics from the Reference Frame of a Super-penumbral Fibril","abstract":"The magnetic geometry of the solar atmosphere, combined with projection effects, makes it difficult to accurately map the propagation of ubiquitous waves in fibrillar structures. These waves are of interest due to their ability to carry energy into the chromosphere and deposit it through damping and dissipation mechanisms. To this end, the Interferometric Bidimensional Spectrometer (IBIS) at the Dunn Solar Telescope was employed to capture high resolution H$\\alpha$ spectral scans of a sunspot, with the transverse oscillations of a prominent super-penumbral fibril examined in depth. The oscillations are re-projected from the helioprojective-cartesian frame to a new frame of reference oriented along the average fibril axis through non-linear force-free field extrapolations. The fibril was found to be carrying an elliptically polarised, propagating kink oscillation with a period of $430$ s and a phase velocity of $69\\pm4$ km s$^{-1}$. The oscillation is damped as it propagates away from the sunspot with a damping length of approximately $9.2$ Mm, resulting in the energy flux decreasing at a rate on the order of $460$ W m$^{-2}$/Mm. The H$\\alpha$ line width is examined and found to increase with distance from the sunspot; a potential sign of a temperature increase. Different linear and non-linear mechanisms are investigated for the damping of the wave energy flux, but a first-order approximation of their combined effects is insufficient to recreate the observed damping length by a factor of at least $3$. It is anticipated that the re-projection methodology demonstrated in this study will aid with future studies of transverse waves within fibrillar structures.","sentences":["The magnetic geometry of the solar atmosphere, combined with projection effects, makes it difficult to accurately map the propagation of ubiquitous waves in fibrillar structures.","These waves are of interest due to their ability to carry energy into the chromosphere and deposit it through damping and dissipation mechanisms.","To this end, the Interferometric Bidimensional Spectrometer (IBIS) at the Dunn Solar Telescope was employed to capture high resolution H$\\alpha$ spectral scans of a sunspot, with the transverse oscillations of a prominent super-penumbral fibril examined in depth.","The oscillations are re-projected from the helioprojective-cartesian frame to a new frame of reference oriented along the average fibril axis through non-linear force-free field extrapolations.","The fibril was found to be carrying an elliptically polarised, propagating kink oscillation with a period of $430$ s and a phase velocity of $69\\pm4$ km s$^{-1}$.","The oscillation is damped as it propagates away from the sunspot with a damping length of approximately $9.2$ Mm, resulting in the energy flux decreasing at a rate on the order of $460$ W m$^{-2}$/Mm.","The H$\\alpha$ line width is examined and found to increase with distance from the sunspot; a potential sign of a temperature increase.","Different linear and non-linear mechanisms are investigated for the damping of the wave energy flux, but a first-order approximation of their combined effects is insufficient to recreate the observed damping length by a factor of at least $3$. It is anticipated that the re-projection methodology demonstrated in this study will aid with future studies of transverse waves within fibrillar structures."],"url":"http://arxiv.org/abs/2405.15584v1","category":"astro-ph.SR"}
{"created":"2024-05-24 13:37:48","title":"Thinking Forward: Memory-Efficient Federated Finetuning of Language Models","abstract":"Finetuning large language models (LLMs) in federated learning (FL) settings has become important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. This work introduces Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimates of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We theoretically show that the global gradients in Spry are unbiased estimates of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1$\\times$ in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. Spry reduces the convergence time by 1.2-20.3$\\times$ and achieves 5.2-13.5\\% higher accuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory usage of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices. Source code is available at https://github.com/Astuary/Spry.","sentences":["Finetuning large language models (LLMs) in federated learning (FL) settings has become important as it allows resource-constrained devices to finetune a model using private data.","However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices.","While Forward-mode Auto-Differentiation (AD) can reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy.","This work introduces Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using Forward-mode AD that are closer estimates of the true gradients.","Spry achieves a low memory footprint, high accuracy, and fast convergence.","We theoretically show that the global gradients in Spry are unbiased estimates of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates.","We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity.","Empirically, Spry reduces the memory footprint during training by 1.4-7.1$\\times$ in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings.","Spry reduces the convergence time by 1.2-20.3$\\times$ and achieves 5.2-13.5\\% higher accuracy against state-of-the-art zero-order methods.","When finetuning Llama2-7B with LoRA, compared to the peak memory usage of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory.","For OPT13B, the reduction is from 76.5GB to 10.8GB.","Spry makes feasible previously impossible FL deployments on commodity mobile and edge devices.","Source code is available at https://github.com/Astuary/Spry."],"url":"http://arxiv.org/abs/2405.15551v1","category":"cs.LG"}
{"created":"2024-05-24 13:29:57","title":"SATSense: Multi-Satellite Collaborative Framework for Spectrum Sensing","abstract":"Low Earth Orbit satellite Internet has recently been deployed, providing worldwide service with non-terrestrial networks. With the large-scale deployment of both non-terrestrial and terrestrial networks, limited spectrum resources will not be allocated enough. Consequently, dynamic spectrum sharing is crucial for their coexistence in the same spectrum, where accurate spectrum sensing is essential. However, spectrum sensing in space is more challenging than in terrestrial networks due to variable channel conditions, making single-satellite sensing unstable. Therefore, we first attempt to design a collaborative sensing scheme utilizing diverse data from multiple satellites. However, it is non-trivial to achieve this collaboration due to heterogeneous channel quality, considerable raw sampling data, and packet loss. To address the above challenges, we first establish connections between the satellites by modeling their sensing data as a graph and devising a graph neural network-based algorithm to achieve effective spectrum sensing. Meanwhile, we establish a joint sub-Nyquist sampling and autoencoder data compression framework to reduce the amount of transmitted sensing data. Finally, we propose a contrastive learning-based mechanism compensates for missing packets. Extensive experiments demonstrate that our proposed strategy can achieve efficient spectrum sensing performance and outperform the conventional deep learning algorithm in spectrum sensing accuracy.","sentences":["Low Earth Orbit satellite Internet has recently been deployed, providing worldwide service with non-terrestrial networks.","With the large-scale deployment of both non-terrestrial and terrestrial networks, limited spectrum resources will not be allocated enough.","Consequently, dynamic spectrum sharing is crucial for their coexistence in the same spectrum, where accurate spectrum sensing is essential.","However, spectrum sensing in space is more challenging than in terrestrial networks due to variable channel conditions, making single-satellite sensing unstable.","Therefore, we first attempt to design a collaborative sensing scheme utilizing diverse data from multiple satellites.","However, it is non-trivial to achieve this collaboration due to heterogeneous channel quality, considerable raw sampling data, and packet loss.","To address the above challenges, we first establish connections between the satellites by modeling their sensing data as a graph and devising a graph neural network-based algorithm to achieve effective spectrum sensing.","Meanwhile, we establish a joint sub-Nyquist sampling and autoencoder data compression framework to reduce the amount of transmitted sensing data.","Finally, we propose a contrastive learning-based mechanism compensates for missing packets.","Extensive experiments demonstrate that our proposed strategy can achieve efficient spectrum sensing performance and outperform the conventional deep learning algorithm in spectrum sensing accuracy."],"url":"http://arxiv.org/abs/2405.15542v1","category":"cs.NI"}
{"created":"2024-05-24 13:21:54","title":"MMD Two-sample Testing in the Presence of Arbitrarily Missing Data","abstract":"In many real-world applications, it is common that a proportion of the data may be missing or only partially observed. We develop a novel two-sample testing method based on the Maximum Mean Discrepancy (MMD) which accounts for missing data in both samples, without making assumptions about the missingness mechanism. Our approach is based on deriving the mathematically precise bounds of the MMD test statistic after accounting for all possible missing values. To the best of our knowledge, it is the only two-sample testing method that is guaranteed to control the Type I error for both univariate and multivariate data where data may be arbitrarily missing. Simulation results show that our method has good statistical power, typically for cases where 5% to 10% of the data are missing. We highlight the value of our approach when the data are missing not at random, a context in which either ignoring the missing values or using common imputation methods may not control the Type I error.","sentences":["In many real-world applications, it is common that a proportion of the data may be missing or only partially observed.","We develop a novel two-sample testing method based on the Maximum Mean Discrepancy (MMD) which accounts for missing data in both samples, without making assumptions about the missingness mechanism.","Our approach is based on deriving the mathematically precise bounds of the MMD test statistic after accounting for all possible missing values.","To the best of our knowledge, it is the only two-sample testing method that is guaranteed to control the Type I error for both univariate and multivariate data where data may be arbitrarily missing.","Simulation results show that our method has good statistical power, typically for cases where 5% to 10% of the data are missing.","We highlight the value of our approach when the data are missing not at random, a context in which either ignoring the missing values or using common imputation methods may not control the Type I error."],"url":"http://arxiv.org/abs/2405.15531v1","category":"stat.ME"}
{"created":"2024-05-24 13:20:34","title":"pyLOM: A HPC open source reduced order model suite for fluid dynamics applications","abstract":"This paper describes the numerical implementation in a high-performance computing environment of an open-source library for model order reduction in fluid dynamics. This library, called pyLOM, contains the algorithms of proper orthogonal decomposition (POD), dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition (SPOD), as well as, efficient SVD and matrix-matrix multiplication, all of them tailored for supercomputers. The library is profiled in detail under the MareNostrum IV supercomputer. The bottleneck is found to be in the QR factorization, which has been solved by an efficient binary tree communications pattern. Strong and weak scalability benchmarks reveal that the serial part (i.e., the part of the code that cannot be parallelized) of these algorithms is under 10% for the strong scaling and under 0.7% for the weak scaling. Using pyLOM, a POD of a dataset containing 1.14 x 108 gridpoints and 1808 snapshots that takes 6.3Tb of memory can be computed in 81.08 seconds using 10368 CPUs. Additioally, the algorithms are validated using the datasets of a flow around a circular cylinder at ReD = 100 and ReD = 1 x 104, as well as the flow in the Stanford diffuser at Reh = 1 x 104.","sentences":["This paper describes the numerical implementation in a high-performance computing environment of an open-source library for model order reduction in fluid dynamics.","This library, called pyLOM, contains the algorithms of proper orthogonal decomposition (POD), dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition (SPOD), as well as, efficient SVD and matrix-matrix multiplication, all of them tailored for supercomputers.","The library is profiled in detail under the MareNostrum IV supercomputer.","The bottleneck is found to be in the QR factorization, which has been solved by an efficient binary tree communications pattern.","Strong and weak scalability benchmarks reveal that the serial part (i.e., the part of the code that cannot be parallelized) of these algorithms is under 10% for the strong scaling and under 0.7% for the weak scaling.","Using pyLOM, a POD of a dataset containing 1.14 x 108 gridpoints and 1808 snapshots that takes 6.3Tb of memory can be computed in 81.08 seconds using 10368 CPUs.","Additioally, the algorithms are validated using the datasets of a flow around a circular cylinder at ReD = 100 and ReD = 1 x 104, as well as the flow in the Stanford diffuser at Reh = 1 x 104."],"url":"http://arxiv.org/abs/2405.15529v1","category":"physics.flu-dyn"}
{"created":"2024-05-24 13:13:49","title":"Syngas conversion to higher alcohols via wood-framed Cu/Co-carbon catalyst","abstract":"Syngas conversion into higher alcohols represents a promising avenue for transforming coal or biomass into liquid fuels. However, the commercialization of this process has been hindered by the high cost, low activity, and inadequate C$_{2+}$OH selectivity of catalysts. Herein, we have developed Cu/Co carbon wood catalysts, offering a cost-effective and stable alternative with exceptional selectivity for catalytic conversion. The formation of Cu/Co nanoparticles was found, influenced by water-1,2-propylene glycol ratios in the solution, resulting in bidisperse nanoparticles. The catalyst exhibited a remarkable CO conversion rate of 74.8% and a selectivity of 58.7% for C$_{2+}$OH, primarily comprising linear primary alcohols. This catalyst demonstrated enduring stability and selectivity under industrial conditions, maintaining its efficacy for up to 350 h of operation. We also employed density functional theory (DFT) to analyze selectivity, particularly focusing on the binding strength of CO, a crucial precursor for subsequent reactions leading to the formation of CH$_3$OH. DFT identified the pathway of CH$_x$ and CO coupling, ultimately yielding C$_2$H$_5$OH. This computational understanding, coupled with high performance of the Cu/Co-carbon wood catalyst, paves ways for the development of catalytically selective materials tailored for higher alcohols production, thereby ushering in new possibility in this field.","sentences":["Syngas conversion into higher alcohols represents a promising avenue for transforming coal or biomass into liquid fuels.","However, the commercialization of this process has been hindered by the high cost, low activity, and inadequate C$_{2+}$OH selectivity of catalysts.","Herein, we have developed Cu/Co carbon wood catalysts, offering a cost-effective and stable alternative with exceptional selectivity for catalytic conversion.","The formation of Cu/Co nanoparticles was found, influenced by water-1,2-propylene glycol ratios in the solution, resulting in bidisperse nanoparticles.","The catalyst exhibited a remarkable CO conversion rate of 74.8% and a selectivity of 58.7% for C$_{2+}$OH, primarily comprising linear primary alcohols.","This catalyst demonstrated enduring stability and selectivity under industrial conditions, maintaining its efficacy for up to 350 h of operation.","We also employed density functional theory (DFT) to analyze selectivity, particularly focusing on the binding strength of CO, a crucial precursor for subsequent reactions leading to the formation of CH$_3$OH.","DFT identified the pathway of CH$_x$ and CO coupling, ultimately yielding C$_2$H$_5$OH.","This computational understanding, coupled with high performance of the Cu/Co-carbon wood catalyst, paves ways for the development of catalytically selective materials tailored for higher alcohols production, thereby ushering in new possibility in this field."],"url":"http://arxiv.org/abs/2405.15526v1","category":"physics.chem-ph"}
{"created":"2024-05-24 12:40:21","title":"Absence of Long-Range Magnetic Ordering in a Trirutile High-Entropy Oxide (Mn0.2Fe0.2Co0.2Ni0.2Cu0.2)Ta2O6","abstract":"Functionalities of solid-state materials are usually considered to be dependent on their crystal structures. The limited structural types observed in the emerged high-entropy oxides put constraints on exploration of their physical properties and potential applications. Herein, we synthesized the first high-entropy oxide in a trirutile structure, (Mn0.2Fe0.2Co0.2Ni0.2Cu0.2)Ta2O6, and investigated its magnetism. The phase purity and high-entropy nature were confirmed by powder X-ray diffraction and energy-dispersive spectroscopy, respectively. X-ray photoelectron spectroscopy indicated divalent Mn, Co and Cu along with coexistence of divalent and trivalent Fe and Ni. Magnetic properties measurements showed antiferromagnetic coupling and potential short-range magnetic ordering below ~ 4 K. The temperature-dependent heat capacity data measured under zero and high magnetic field confirmed the short-range magnetic ordering and a possible low-temperature phonon excitation. Moreover, the possible reasons for the absence of long-range magnetic ordering are concluded from a molecular orbital perspective. The discovery of the first trirutile high-entropy oxide opens a new way for studying the relationship between the highly disordered atomic arrangement and their magnetic interaction. Furthermore, it provides a new direction for exploring functionalities of high-entropy oxides.","sentences":["Functionalities of solid-state materials are usually considered to be dependent on their crystal structures.","The limited structural types observed in the emerged high-entropy oxides put constraints on exploration of their physical properties and potential applications.","Herein, we synthesized the first high-entropy oxide in a trirutile structure, (Mn0.2Fe0.2Co0.2Ni0.2Cu0.2)Ta2O6, and investigated its magnetism.","The phase purity and high-entropy nature were confirmed by powder X-ray diffraction and energy-dispersive spectroscopy, respectively.","X-ray photoelectron spectroscopy indicated divalent Mn, Co and Cu along with coexistence of divalent and trivalent Fe and Ni.","Magnetic properties measurements showed antiferromagnetic coupling and potential short-range magnetic ordering below ~ 4 K. The temperature-dependent heat capacity data measured under zero and high magnetic field confirmed the short-range magnetic ordering and a possible low-temperature phonon excitation.","Moreover, the possible reasons for the absence of long-range magnetic ordering are concluded from a molecular orbital perspective.","The discovery of the first trirutile high-entropy oxide opens a new way for studying the relationship between the highly disordered atomic arrangement and their magnetic interaction.","Furthermore, it provides a new direction for exploring functionalities of high-entropy oxides."],"url":"http://arxiv.org/abs/2405.15501v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 12:23:38","title":"Towards Natural Machine Unlearning","abstract":"Machine unlearning (MU) aims to eliminate information that has been learned from specific training data, namely forgetting data, from a pre-trained model. Currently, the mainstream of existing MU methods involves modifying the forgetting data with incorrect labels and subsequently fine-tuning the model. While learning such incorrect information can indeed remove knowledge, the process is quite unnatural as the unlearning process undesirably reinforces the incorrect information and leads to over-forgetting. Towards more \\textit{natural} machine unlearning, we inject correct information from the remaining data to the forgetting samples when changing their labels. Through pairing these adjusted samples with their labels, the model will tend to use the injected correct information and naturally suppress the information meant to be forgotten. Albeit straightforward, such a first step towards natural machine unlearning can significantly outperform current state-of-the-art approaches. In particular, our method substantially reduces the over-forgetting and leads to strong robustness to hyperparameters, making it a promising candidate for practical machine unlearning.","sentences":["Machine unlearning (MU) aims to eliminate information that has been learned from specific training data, namely forgetting data, from a pre-trained model.","Currently, the mainstream of existing MU methods involves modifying the forgetting data with incorrect labels and subsequently fine-tuning the model.","While learning such incorrect information can indeed remove knowledge, the process is quite unnatural as the unlearning process undesirably reinforces the incorrect information and leads to over-forgetting.","Towards more \\textit{natural} machine unlearning, we inject correct information from the remaining data to the forgetting samples when changing their labels.","Through pairing these adjusted samples with their labels, the model will tend to use the injected correct information and naturally suppress the information meant to be forgotten.","Albeit straightforward, such a first step towards natural machine unlearning can significantly outperform current state-of-the-art approaches.","In particular, our method substantially reduces the over-forgetting and leads to strong robustness to hyperparameters, making it a promising candidate for practical machine unlearning."],"url":"http://arxiv.org/abs/2405.15495v1","category":"cs.LG"}
{"created":"2024-05-24 12:16:28","title":"GSDeformer: Direct Cage-based Deformation for 3D Gaussian Splatting","abstract":"We present GSDeformer, a method that achieves free-form deformation on 3D Gaussian Splatting(3DGS) without requiring any architectural changes. Our method extends cage-based deformation, a traditional mesh deformation method, to 3DGS. This is done by converting 3DGS into a novel proxy point cloud representation, where its deformation can be used to infer the transformations to apply on the 3D gaussians making up 3DGS. We also propose an automatic cage construction algorithm for 3DGS to minimize manual work. Our method does not modify the underlying architecture of 3DGS. Therefore, any existing trained vanilla 3DGS can be easily edited by our method. We compare the deformation capability of our method against other existing methods, demonstrating the ease of use and comparable quality of our method, despite being more direct and thus easier to integrate with other concurrent developments on 3DGS.","sentences":["We present GSDeformer, a method that achieves free-form deformation on 3D Gaussian Splatting(3DGS) without requiring any architectural changes.","Our method extends cage-based deformation, a traditional mesh deformation method, to 3DGS.","This is done by converting 3DGS into a novel proxy point cloud representation, where its deformation can be used to infer the transformations to apply on the 3D gaussians making up 3DGS.","We also propose an automatic cage construction algorithm for 3DGS to minimize manual work.","Our method does not modify the underlying architecture of 3DGS.","Therefore, any existing trained vanilla 3DGS can be easily edited by our method.","We compare the deformation capability of our method against other existing methods, demonstrating the ease of use and comparable quality of our method, despite being more direct and thus easier to integrate with other concurrent developments on 3DGS."],"url":"http://arxiv.org/abs/2405.15491v1","category":"cs.CV"}
{"created":"2024-05-24 12:03:58","title":"Extended Kohler's scaling, a low temperature anomaly and Isosbestic point in the charge density wave state of 1T-VSe$_2$","abstract":"1T-VSe$_2$ is a narrow band transition metal chalcogenide that shows charge density wave (CDW) state below $T_{CDW}$ = 110 K. Here, we have explored the relevance of Kohler's rule and the thermal transport properties of VSe$_2$ across the CDW state. The magnetoresistance (MR) follows Kohler's rule above $T_{CDW}$, while an extended Kohler's rule is employed below $T_{CDW}$. Interestingly, we observed an anomaly in MR at T = 20 K, below which MR value decreases on lowering temperature. This anomaly is also reflected in the slope ($\\kappa$) of Kohler's plots and the relative change in the thermal excitation induced carrier density ($n_T$) also. The $T_{CDW}$ remains largely unaffected in both electrical resistivity ($\\rho(T)$) and longitudinal Seebeck coefficient ($\\it{S_{xx}}$) even under a strong magnetic field of 14 Tesla. However, the application of magnetic field enhances the peak intensity of $\\it{S_{xx}}$ at T $\\sim$ 60 K. Additionally, $\\it{S_{xx}(T)}$ curves measured at different fields exhibit a crossover at T = 20 K, which suggest the existence of unique feature in the CDW state of VSe$_2$ \\textit{i.e.} a locally exact isosbestic point.","sentences":["1T-VSe$_2$ is a narrow band transition metal chalcogenide that shows charge density wave (CDW) state below $T_{CDW}$ = 110 K. Here, we have explored the relevance of Kohler's rule and the thermal transport properties of VSe$_2$ across the CDW state.","The magnetoresistance (MR) follows Kohler's rule above $T_{CDW}$, while an extended Kohler's rule is employed below $T_{CDW}$. Interestingly, we observed an anomaly in MR at T = 20 K, below which MR value decreases on lowering temperature.","This anomaly is also reflected in the slope ($\\kappa$) of Kohler's plots and the relative change in the thermal excitation induced carrier density ($n_T$) also.","The $T_{CDW}$ remains largely unaffected in both electrical resistivity ($\\rho(T)$) and longitudinal Seebeck coefficient ($\\it{S_{xx}}$) even under a strong magnetic field of 14 Tesla.","However, the application of magnetic field enhances the peak intensity of $\\it{S_{xx}}$ at T $\\sim$ 60 K. Additionally, $\\it{S_{xx}(T)}$ curves measured at different fields exhibit a crossover at T = 20 K, which suggest the existence of unique feature in the CDW state of VSe$_2$ \\textit{i.e.}","a locally exact isosbestic point."],"url":"http://arxiv.org/abs/2405.15483v1","category":"cond-mat.str-el"}
{"created":"2024-05-24 11:40:22","title":"Scale-Invariant Feature Disentanglement via Adversarial Learning for UAV-based Object Detection","abstract":"Detecting objects from Unmanned Aerial Vehicles (UAV) is often hindered by a large number of small objects, resulting in low detection accuracy. To address this issue, mainstream approaches typically utilize multi-stage inferences. Despite their remarkable detecting accuracies, real-time efficiency is sacrificed, making them less practical to handle real applications. To this end, we propose to improve the single-stage inference accuracy through learning scale-invariant features. Specifically, a Scale-Invariant Feature Disentangling module is designed to disentangle scale-related and scale-invariant features. Then an Adversarial Feature Learning scheme is employed to enhance disentanglement. Finally, scale-invariant features are leveraged for robust UAV-based object detection. Furthermore, we construct a multi-modal UAV object detection dataset, State-Air, which incorporates annotated UAV state parameters. We apply our approach to three state-of-the-art lightweight detection frameworks on three benchmark datasets, including State-Air. Extensive experiments demonstrate that our approach can effectively improve model accuracy. Our code and dataset are provided in Supplementary Materials and will be publicly available once the paper is accepted.","sentences":["Detecting objects from Unmanned Aerial Vehicles (UAV) is often hindered by a large number of small objects, resulting in low detection accuracy.","To address this issue, mainstream approaches typically utilize multi-stage inferences.","Despite their remarkable detecting accuracies, real-time efficiency is sacrificed, making them less practical to handle real applications.","To this end, we propose to improve the single-stage inference accuracy through learning scale-invariant features.","Specifically, a Scale-Invariant Feature Disentangling module is designed to disentangle scale-related and scale-invariant features.","Then an Adversarial Feature Learning scheme is employed to enhance disentanglement.","Finally, scale-invariant features are leveraged for robust UAV-based object detection.","Furthermore, we construct a multi-modal UAV object detection dataset, State-Air, which incorporates annotated UAV state parameters.","We apply our approach to three state-of-the-art lightweight detection frameworks on three benchmark datasets, including State-Air.","Extensive experiments demonstrate that our approach can effectively improve model accuracy.","Our code and dataset are provided in Supplementary Materials and will be publicly available once the paper is accepted."],"url":"http://arxiv.org/abs/2405.15465v1","category":"cs.CV"}
{"created":"2024-05-24 11:35:49","title":"Optimal market-neutral currency trading on the cryptocurrency platform","abstract":"This research proposes a novel arbitrage approach with respect to multivariate pair trading called Optimal Trading Technique (OTT). We introduce the method to selectively form a \"bucket\" of fiat currencies anchored to cryptocurrency for simultaneously monitoring and exploiting trading opportunities. To handle the quantitative conflicts that arise when receiving multiple trading signals, a bi-objective convex optimization process is designed to cater to the investor's preference between profitability and risk tolerance. This process includes tunable parameters such as volatility punishment, action thresholds. During our experiments in the cryptocurrency market from 2020 to 2022 when the market was experiencing a vigorous bull-run immediately followed by a bear-run, the OTT realized an annualized profit of 15.49%. We further carried out the experiments in bull, bear, and full-cycle market conditions separately, and found that OTT is capable of achieving stable profit under various market conditions. Apart from the profitability side of the OTT, the arbitrage operation provides a new perspective of trading, which requires no external shorting and never hold intermediate cryptocurrency during the arbitrage period.","sentences":["This research proposes a novel arbitrage approach with respect to multivariate pair trading called Optimal Trading Technique (OTT).","We introduce the method to selectively form a \"bucket\" of fiat currencies anchored to cryptocurrency for simultaneously monitoring and exploiting trading opportunities.","To handle the quantitative conflicts that arise when receiving multiple trading signals, a bi-objective convex optimization process is designed to cater to the investor's preference between profitability and risk tolerance.","This process includes tunable parameters such as volatility punishment, action thresholds.","During our experiments in the cryptocurrency market from 2020 to 2022 when the market was experiencing a vigorous bull-run immediately followed by a bear-run, the OTT realized an annualized profit of 15.49%.","We further carried out the experiments in bull, bear, and full-cycle market conditions separately, and found that OTT is capable of achieving stable profit under various market conditions.","Apart from the profitability side of the OTT, the arbitrage operation provides a new perspective of trading, which requires no external shorting and never hold intermediate cryptocurrency during the arbitrage period."],"url":"http://arxiv.org/abs/2405.15461v1","category":"cs.CE"}
{"created":"2024-05-24 11:28:01","title":"Self-distilled Dynamic Fusion Network for Language-based Fashion Retrieval","abstract":"In the domain of language-based fashion image retrieval, pinpointing the desired fashion item using both a reference image and its accompanying textual description is an intriguing challenge. Existing approaches lean heavily on static fusion techniques, intertwining image and text. Despite their commendable advancements, these approaches are still limited by a deficiency in flexibility. In response, we propose a Self-distilled Dynamic Fusion Network to compose the multi-granularity features dynamically by considering the consistency of routing path and modality-specific information simultaneously. Two new modules are included in our proposed method: (1) Dynamic Fusion Network with Modality Specific Routers. The dynamic network enables a flexible determination of the routing for each reference image and modification text, taking into account their distinct semantics and distributions. (2) Self Path Distillation Loss. A stable path decision for queries benefits the optimization of feature extraction as well as routing, and we approach this by progressively refine the path decision with previous path information. Extensive experiments demonstrate the effectiveness of our proposed model compared to existing methods.","sentences":["In the domain of language-based fashion image retrieval, pinpointing the desired fashion item using both a reference image and its accompanying textual description is an intriguing challenge.","Existing approaches lean heavily on static fusion techniques, intertwining image and text.","Despite their commendable advancements, these approaches are still limited by a deficiency in flexibility.","In response, we propose a Self-distilled Dynamic Fusion Network to compose the multi-granularity features dynamically by considering the consistency of routing path and modality-specific information simultaneously.","Two new modules are included in our proposed method: (1) Dynamic Fusion Network with Modality Specific Routers.","The dynamic network enables a flexible determination of the routing for each reference image and modification text, taking into account their distinct semantics and distributions.","(2) Self Path Distillation Loss.","A stable path decision for queries benefits the optimization of feature extraction as well as routing, and we approach this by progressively refine the path decision with previous path information.","Extensive experiments demonstrate the effectiveness of our proposed model compared to existing methods."],"url":"http://arxiv.org/abs/2405.15451v1","category":"cs.CV"}
{"created":"2024-05-24 10:37:39","title":"Smoothed Online Classification can be Harder than Batch Classification","abstract":"We study online classification under smoothed adversaries. In this setting, at each time point, the adversary draws an example from a distribution that has a bounded density with respect to a fixed base measure, which is known apriori to the learner. For binary classification and scalar-valued regression, previous works \\citep{haghtalab2020smoothed, block2022smoothed} have shown that smoothed online learning is as easy as learning in the iid batch setting under PAC model. However, we show that smoothed online classification can be harder than the iid batch classification when the label space is unbounded. In particular, we construct a hypothesis class that is learnable in the iid batch setting under the PAC model but is not learnable under the smoothed online model. Finally, we identify a condition that ensures that the PAC learnability of a hypothesis class is sufficient for its smoothed online learnability.","sentences":["We study online classification under smoothed adversaries.","In this setting, at each time point, the adversary draws an example from a distribution that has a bounded density with respect to a fixed base measure, which is known apriori to the learner.","For binary classification and scalar-valued regression, previous works \\citep{haghtalab2020smoothed, block2022smoothed} have shown that smoothed online learning is as easy as learning in the iid batch setting under PAC model.","However, we show that smoothed online classification can be harder than the iid batch classification when the label space is unbounded.","In particular, we construct a hypothesis class that is learnable in the iid batch setting under the PAC model but is not learnable under the smoothed online model.","Finally, we identify a condition that ensures that the PAC learnability of a hypothesis class is sufficient for its smoothed online learnability."],"url":"http://arxiv.org/abs/2405.15424v1","category":"cs.LG"}
{"created":"2024-05-24 10:36:23","title":"Model-free reinforcement learning with noisy actions for automated experimental control in optics","abstract":"Experimental control involves a lot of manual effort with non-trivial decisions for precise adjustments. Here, we study the automatic experimental alignment for coupling laser light into an optical fiber using reinforcement learning (RL). We face several real-world challenges, such as time-consuming training, partial observability, and noisy actions due to imprecision in the mirror steering motors. We show that we can overcome these challenges: To save time, we use a virtual testbed to tune our environment for dealing with partial observability and use relatively sample-efficient model-free RL algorithms like Soft Actor-Critic (SAC) or Truncated Quantile Critics (TQC). Furthermore, by fully training on the experiment, the agent learns directly to handle the noise present. In our extensive experimentation, we show that we are able to achieve 90% coupling, showcasing the effectiveness of our proposed approaches. We reach this efficiency, which is comparable to that of a human expert, without additional feedback loops despite the motors' inaccuracies. Our result is an example of the readiness of RL for real-world tasks. We consider RL a promising tool for reducing the workload in labs.","sentences":["Experimental control involves a lot of manual effort with non-trivial decisions for precise adjustments.","Here, we study the automatic experimental alignment for coupling laser light into an optical fiber using reinforcement learning (RL).","We face several real-world challenges, such as time-consuming training, partial observability, and noisy actions due to imprecision in the mirror steering motors.","We show that we can overcome these challenges: To save time, we use a virtual testbed to tune our environment for dealing with partial observability and use relatively sample-efficient model-free RL algorithms like Soft Actor-Critic (SAC) or Truncated Quantile Critics (TQC).","Furthermore, by fully training on the experiment, the agent learns directly to handle the noise present.","In our extensive experimentation, we show that we are able to achieve 90% coupling, showcasing the effectiveness of our proposed approaches.","We reach this efficiency, which is comparable to that of a human expert, without additional feedback loops despite the motors' inaccuracies.","Our result is an example of the readiness of RL for real-world tasks.","We consider RL a promising tool for reducing the workload in labs."],"url":"http://arxiv.org/abs/2405.15421v1","category":"cs.LG"}
{"created":"2024-05-24 10:33:58","title":"On Phase Unwrapping via Digital Wavefront Sensors","abstract":"In this paper, we consider a new class of methods for phase unwrapping of 2D images based on digital wavefront sensors. Mathematically, many wavefront sensors produce the same measurements of incoming wavefronts regardless of whether they are wrapped or not. Since typical reconstructors for these sensors are optimized to compute smooth wavefronts, it is possible to digitally ``propagate'' a wrapped phase through such a sensor, resulting in a smooth unwrapped phase. First, we show how this principle can be applied for phase unwrapping using digital Shack-Hartmann and Fourier-type wavefront sensors. Then, we apply our methods to an unwrapping problem appearing in a real-world adaptive optics project currently under development, and compare the results to those obtained with other state-of-the-art algorithms.","sentences":["In this paper, we consider a new class of methods for phase unwrapping of 2D images based on digital wavefront sensors.","Mathematically, many wavefront sensors produce the same measurements of incoming wavefronts regardless of whether they are wrapped or not.","Since typical reconstructors for these sensors are optimized to compute smooth wavefronts, it is possible to digitally ``propagate'' a wrapped phase through such a sensor, resulting in a smooth unwrapped phase.","First, we show how this principle can be applied for phase unwrapping using digital Shack-Hartmann and Fourier-type wavefront sensors.","Then, we apply our methods to an unwrapping problem appearing in a real-world adaptive optics project currently under development, and compare the results to those obtained with other state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.15419v1","category":"math.NA"}
{"created":"2024-05-24 09:46:17","title":"Decentralized Virtual Research Environment: Empowering Peer-to-Peer Trustworthy Data Sharing and Collaboration","abstract":"Scientific research, increasingly reliant on data and computational analysis, confronts the challenge of integrating collaboration and data sharing across disciplines. Collaborative frameworks that support decentralized decision-making and knowledge-sharing are essential, yet integrating them into computational environments presents technical challenges, such as decentralized identity, user-centered policy-making, flexible asset management, automated provenance, and distributed collaborative workflow management. In this work, we propose a novel framework called Decentralized Virtual Research Environment (D-VRE) that addresses barriers to scientific research collaboration and data sharing, offering a scalable and adaptable decentralized model. It enhances seamless, trusted data sharing and collaboration within research lifecycles. It incorporates custom sharing policies, secure asset management, collaborative workflows, and research activity tracking, all without centralized oversight. Demonstrated through a real-world case study in the CLARIFY project, D-VRE proved effective in enabling advanced data sharing and collaborative scenarios, showcasing its adaptability in scientific research. Integrated into JupyterLab, D-VRE supports custom collaboration agreements and smart contract-based automated execution on the Ethereum blockchain. This ensures secure, verifiable transactions and promotes trust and reliability in shared research findings.","sentences":["Scientific research, increasingly reliant on data and computational analysis, confronts the challenge of integrating collaboration and data sharing across disciplines.","Collaborative frameworks that support decentralized decision-making and knowledge-sharing are essential, yet integrating them into computational environments presents technical challenges, such as decentralized identity, user-centered policy-making, flexible asset management, automated provenance, and distributed collaborative workflow management.","In this work, we propose a novel framework called Decentralized Virtual Research Environment (D-VRE) that addresses barriers to scientific research collaboration and data sharing, offering a scalable and adaptable decentralized model.","It enhances seamless, trusted data sharing and collaboration within research lifecycles.","It incorporates custom sharing policies, secure asset management, collaborative workflows, and research activity tracking, all without centralized oversight.","Demonstrated through a real-world case study in the CLARIFY project, D-VRE proved effective in enabling advanced data sharing and collaborative scenarios, showcasing its adaptability in scientific research.","Integrated into JupyterLab, D-VRE supports custom collaboration agreements and smart contract-based automated execution on the Ethereum blockchain.","This ensures secure, verifiable transactions and promotes trust and reliability in shared research findings."],"url":"http://arxiv.org/abs/2405.15392v1","category":"cs.DC"}
{"created":"2024-05-24 09:41:06","title":"Tensor Frames -- How To Make Any Message Passing Network Equivariant","abstract":"In many applications of geometric deep learning, the choice of global coordinate frame is arbitrary, and predictions should be independent of the reference frame. In other words, the network should be equivariant with respect to rotations and reflections of the input, i.e., the transformations of O(d). We present a novel framework for building equivariant message passing architectures and modifying existing non-equivariant architectures to be equivariant. Our approach is based on local coordinate frames, between which geometric information is communicated consistently by including tensorial objects in the messages. Our framework can be applied to message passing on geometric data in arbitrary dimensional Euclidean space. While many other approaches for equivariant message passing require specialized building blocks, such as non-standard normalization layers or non-linearities, our approach can be adapted straightforwardly to any existing architecture without such modifications. We explicitly demonstrate the benefit of O(3)-equivariance for a popular point cloud architecture and produce state-of-the-art results on normal vector regression on point clouds.","sentences":["In many applications of geometric deep learning, the choice of global coordinate frame is arbitrary, and predictions should be independent of the reference frame.","In other words, the network should be equivariant with respect to rotations and reflections of the input, i.e., the transformations of O(d).","We present a novel framework for building equivariant message passing architectures and modifying existing non-equivariant architectures to be equivariant.","Our approach is based on local coordinate frames, between which geometric information is communicated consistently by including tensorial objects in the messages.","Our framework can be applied to message passing on geometric data in arbitrary dimensional Euclidean space.","While many other approaches for equivariant message passing require specialized building blocks, such as non-standard normalization layers or non-linearities, our approach can be adapted straightforwardly to any existing architecture without such modifications.","We explicitly demonstrate the benefit of O(3)-equivariance for a popular point cloud architecture and produce state-of-the-art results on normal vector regression on point clouds."],"url":"http://arxiv.org/abs/2405.15389v1","category":"cs.LG"}
{"created":"2024-05-24 09:36:58","title":"Study some two loop contribution to moun MDM in the N-B-LSSM","abstract":"It is well known that the muon magnetic dipole moment (MDM) has close relation with the new physics (NP) in the development of the Standard Model (SM). Combined with the Fermilab National Accelerator Laboratory (FNAL) and the Brookhaven National Laboratory (BNL) E821 result, the departure from the SM prediction is about 5.0 $\\sigma$. We study the electroweak corrections from several type two-loop SUSY diagrams and the virtual SUSY particles include chargino, neutralino, scalar lepton and scalar neutrino. Based on the latest experimental constraints, we study the moun MDM under the next to the minimal supersymmetric extension of the SM with local B-L gauge symmetry (N-B-LSSM). The abundant numerical results verify that $\\tan{\\beta},~T_e,~M^2_L,~M^2_e,~M_{BB'}$ and $M^2_{eij}$ play an important role in moun MDM . $M^2_e,~\\tan{\\beta}$ and $T_e$ are sensitive parameters to the process of moun MDM. From the data obtained in the figure, most of $a_{\\mu}^{NBL}$ are in 2$\\sigma$, which can compensate the departure between the experiment data and the SM prediction.","sentences":["It is well known that the muon magnetic dipole moment (MDM) has close relation with the new physics (NP) in the development of the Standard Model (SM).","Combined with the Fermilab National Accelerator Laboratory (FNAL) and the Brookhaven National Laboratory (BNL) E821 result, the departure from the SM prediction is about 5.0 $\\sigma$. We study the electroweak corrections from several type two-loop SUSY diagrams and the virtual SUSY particles include chargino, neutralino, scalar lepton and scalar neutrino.","Based on the latest experimental constraints, we study the moun MDM under the next to the minimal supersymmetric extension of the SM with local B-L gauge symmetry (N-B-LSSM).","The abundant numerical results verify that $\\tan{\\beta},~T_e,~M^2_L,~M^2_e,~M_{BB'}$ and $M^2_{eij}$ play an important role in moun MDM .","$M^2_e,~\\tan{\\beta}$ and $T_e$ are sensitive parameters to the process of moun MDM.","From the data obtained in the figure, most of $a_{\\mu}^{NBL}$ are in 2$\\sigma$, which can compensate the departure between the experiment data and the SM prediction."],"url":"http://arxiv.org/abs/2405.15387v1","category":"hep-ph"}
{"created":"2024-05-24 09:33:47","title":"Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate","abstract":"Real-world decision-making tasks are usually partially observable Markov decision processes (POMDPs), where the state is not fully observable. Recent progress has demonstrated that recurrent reinforcement learning (RL), which consists of a context encoder based on recurrent neural networks (RNNs) for unobservable state prediction and a multilayer perceptron (MLP) policy for decision making, can mitigate partial observability and serve as a robust baseline for POMDP tasks. However, previous recurrent RL methods face training stability issues due to the gradient instability of RNNs. In this paper, we propose Recurrent Off-policy RL with Context-Encoder-Specific Learning Rate (RESeL) to tackle this issue. Specifically, RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter. We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm. We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks. The experiments demonstrate significant improvements in training stability with RESeL. Comparative results show that RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with or even surpasses state-of-the-art methods in MDP tasks. Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder.","sentences":["Real-world decision-making tasks are usually partially observable Markov decision processes (POMDPs), where the state is not fully observable.","Recent progress has demonstrated that recurrent reinforcement learning (RL), which consists of a context encoder based on recurrent neural networks (RNNs) for unobservable state prediction and a multilayer perceptron (MLP) policy for decision making, can mitigate partial observability and serve as a robust baseline for POMDP tasks.","However, previous recurrent RL methods face training stability issues due to the gradient instability of RNNs.","In this paper, we propose Recurrent Off-policy RL with Context-Encoder-Specific Learning Rate (RESeL) to tackle this issue.","Specifically, RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter.","We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm.","We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks.","The experiments demonstrate significant improvements in training stability with RESeL. Comparative results show that RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with or even surpasses state-of-the-art methods in MDP tasks.","Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder."],"url":"http://arxiv.org/abs/2405.15384v1","category":"cs.LG"}
{"created":"2024-05-24 08:48:06","title":"Strong screening rules for group-based SLOPE models","abstract":"Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable for the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.","sentences":["Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters.","Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting.","We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE.","The developed rules are applicable for the wider family of group-based OWL models, including OSCAR.","Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process.","The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics."],"url":"http://arxiv.org/abs/2405.15357v1","category":"stat.ML"}
{"created":"2024-05-24 08:22:22","title":"Implementation of New Security Features in CMSWEB Kubernetes Cluster at CERN","abstract":"The CMSWEB cluster is pivotal to the activities of the Compact Muon Solenoid (CMS) experiment, as it hosts critical services required for the operational needs of the CMS experiment. The security of these services and the corresponding data is crucial to CMS. Any malicious attack can compromise the availability of our services. Therefore, it is important to construct a robust security infrastructure. In this work, we discuss new security features introduced to the CMSWEB Kubernetes (\"k8s\") cluster. The new features include the implementation of network policies, deployment of Open Policy Agent (OPA), enforcement of OPA policies, and the integration of Vault. The network policies act as an inside-the-cluster firewall to limit the network communication between the pods to the minimum necessary, and its dynamic nature allows us to work with microservices. The OPA validates the objects against some custom-defined policies during create, update, and delete operations to further enhance security. Without recompiling or changing the configuration of the Kubernetes API server, it can apply customized policies on Kubernetes objects and their audit functionality enabling us to detect pre-existing conflicts and issues. Although Kubernetes incorporates the concepts of secrets, they are only base64 encoded and are not dynamically configured. This is where Vault comes into play: Vault dynamically secures, stores, and tightly controls access to sensitive data. This way, the secret information is encrypted, secured, and centralized, making it more scalable and easier to manage. Thus, the implementation of these three security features corroborate the enhanced security and reliability of the CMSWEB Kubernetes infrastructure.","sentences":["The CMSWEB cluster is pivotal to the activities of the Compact Muon Solenoid (CMS) experiment, as it hosts critical services required for the operational needs of the CMS experiment.","The security of these services and the corresponding data is crucial to CMS.","Any malicious attack can compromise the availability of our services.","Therefore, it is important to construct a robust security infrastructure.","In this work, we discuss new security features introduced to the CMSWEB Kubernetes (\"k8s\") cluster.","The new features include the implementation of network policies, deployment of Open Policy Agent (OPA), enforcement of OPA policies, and the integration of Vault.","The network policies act as an inside-the-cluster firewall to limit the network communication between the pods to the minimum necessary, and its dynamic nature allows us to work with microservices.","The OPA validates the objects against some custom-defined policies during create, update, and delete operations to further enhance security.","Without recompiling or changing the configuration of the Kubernetes API server, it can apply customized policies on Kubernetes objects and their audit functionality enabling us to detect pre-existing conflicts and issues.","Although Kubernetes incorporates the concepts of secrets, they are only base64 encoded and are not dynamically configured.","This is where Vault comes into play: Vault dynamically secures, stores, and tightly controls access to sensitive data.","This way, the secret information is encrypted, secured, and centralized, making it more scalable and easier to manage.","Thus, the implementation of these three security features corroborate the enhanced security and reliability of the CMSWEB Kubernetes infrastructure."],"url":"http://arxiv.org/abs/2405.15342v1","category":"cs.CR"}
{"created":"2024-05-24 17:03:10","title":"Hyperbolic Shear Metasurfaces","abstract":"Polar dielectrics with low crystal symmetry and sharp phonon resonances can support hyperbolic shear polaritons - highly confined surface modes with frequency-dependent optical axes and asymmetric dissipation features. So far, these modes have been observed only in bulk natural materials at mid-infrared frequencies, with properties limited by available crystal geometries and phonon resonance strength. Here we introduce hyperbolic shear metasurfaces: ultrathin engineered surfaces supporting hyperbolic surface modes with symmetry-tailored axial dispersion and loss redistribution that can maximally enhance light-matter interactions. By engineering effective shear phenomena in these engineered surfaces, we demonstrate geometry-controlled, ultra-confined, low-loss hyperbolic surface waves with broadband Purcell enhancements, applicable across a broad range of the electromagnetic spectrum.","sentences":["Polar dielectrics with low crystal symmetry and sharp phonon resonances can support hyperbolic shear polaritons - highly confined surface modes with frequency-dependent optical axes and asymmetric dissipation features.","So far, these modes have been observed only in bulk natural materials at mid-infrared frequencies, with properties limited by available crystal geometries and phonon resonance strength.","Here we introduce hyperbolic shear metasurfaces: ultrathin engineered surfaces supporting hyperbolic surface modes with symmetry-tailored axial dispersion and loss redistribution that can maximally enhance light-matter interactions.","By engineering effective shear phenomena in these engineered surfaces, we demonstrate geometry-controlled, ultra-confined, low-loss hyperbolic surface waves with broadband Purcell enhancements, applicable across a broad range of the electromagnetic spectrum."],"url":"http://arxiv.org/abs/2405.15715v1","category":"physics.optics"}
{"created":"2024-05-24 14:45:48","title":"Chiral-Gain Photonics","abstract":"Here, we present an exploratory study of the potential applications of electrically biased materials that possess a nonreciprocal and non-Hermitian electromagnetic response analogous to the electronic response of field-effect transistors. The most distinctive feature of such materials is their chiral-gain, meaning that their response can be active or dissipative depending on the handedness of the wave polarization. Here, we show how the chiral-gain can be harnessed to develop novel electromagnetic devices with unique properties such as chiral lasers, polarization-dependent mirrors, and coherent-perfect-absorber lasers. Furthermore, it is demonstrated that materials with chiral-gain can bypass a reciprocity constraint that typically limits the external coupling strength, thus facilitating the excitation of cavities with extremely large quality factors.","sentences":["Here, we present an exploratory study of the potential applications of electrically biased materials that possess a nonreciprocal and non-Hermitian electromagnetic response analogous to the electronic response of field-effect transistors.","The most distinctive feature of such materials is their chiral-gain, meaning that their response can be active or dissipative depending on the handedness of the wave polarization.","Here, we show how the chiral-gain can be harnessed to develop novel electromagnetic devices with unique properties such as chiral lasers, polarization-dependent mirrors, and coherent-perfect-absorber lasers.","Furthermore, it is demonstrated that materials with chiral-gain can bypass a reciprocity constraint that typically limits the external coupling strength, thus facilitating the excitation of cavities with extremely large quality factors."],"url":"http://arxiv.org/abs/2405.15606v1","category":"physics.optics"}
{"created":"2024-05-24 14:25:23","title":"MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence","abstract":"We propose a new variant of the Adam optimizer [Kingma and Ba, 2014] called MICROADAM that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees. We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly. We control the resulting compression error via a novel instance of the classical error feedback mechanism from distributed optimization [Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al., 2019] in which the error correction information is itself compressed to allow for practical memory gains. We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance. Specifically, we show that MICROADAM can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with lower memory usage and similar running time. Our code is available at https://github.com/IST-DASLab/MicroAdam.","sentences":["We propose a new variant of the Adam optimizer [Kingma and Ba, 2014] called MICROADAM that specifically minimizes memory overheads, while maintaining theoretical convergence guarantees.","We achieve this by compressing the gradient information before it is fed into the optimizer state, thereby reducing its memory footprint significantly.","We control the resulting compression error via a novel instance of the classical error feedback mechanism from distributed optimization","[Seide et al., 2014, Alistarh et al., 2018, Karimireddy et al., 2019] in which the error correction information is itself compressed to allow for practical memory gains.","We prove that the resulting approach maintains theoretical convergence guarantees competitive to those of AMSGrad, while providing good practical performance.","Specifically, we show that MICROADAM can be implemented efficiently on GPUs: on both million-scale (BERT) and billion-scale (LLaMA) models, MicroAdam provides practical convergence competitive to that of the uncompressed Adam baseline, with lower memory usage and similar running time.","Our code is available at https://github.com/IST-DASLab/MicroAdam."],"url":"http://arxiv.org/abs/2405.15593v1","category":"cs.LG"}
{"created":"2024-05-24 08:26:39","title":"Adaptive Finite Element Method for a Nonlinear Helmholtz Equation with High Wave Number","abstract":"A nonlinear Helmholtz (NLH) equation with high frequencies and corner singularities is discretized by the linear finite element method (FEM). After deriving some wave-number-explicit stability estimates and the singularity decomposition for the NLH problem, a priori stability and error estimates are established for the FEM on shape regular meshes including the case of locally refined meshes. Then a posteriori upper and lower bounds using a new residual-type error estimator, which is equivalent to the standard one, are derived for the FE solutions to the NLH problem. These a posteriori estimates have confirmed a significant fact that is also valid for the NLH problem, namely the residual-type estimator seriously underestimates the error of the FE solution in the preasymptotic regime, which was first observed by Babu\\v{s}ka et al. [Int J Numer Methods Eng 40 (1997)] for a one-dimensional linear problem. Based on the new a posteriori error estimator, both the convergence and the quasi-optimality of the resulting adaptive finite element algorithm are proved the first time for the NLH problem, when the initial mesh size lying in the preasymptotic regime. Finally, numerical examples are presented to validate the theoretical findings and demonstrate that applying the continuous interior penalty (CIP) technique with appropriate penalty parameters can reduce the pollution errors efficiently. In particular, the nonlinear phenomenon of optical bistability with Gaussian incident waves is successfully simulated by the adaptive CIPFEM.","sentences":["A nonlinear Helmholtz (NLH) equation with high frequencies and corner singularities is discretized by the linear finite element method (FEM).","After deriving some wave-number-explicit stability estimates and the singularity decomposition for the NLH problem, a priori stability and error estimates are established for the FEM on shape regular meshes including the case of locally refined meshes.","Then a posteriori upper and lower bounds using a new residual-type error estimator, which is equivalent to the standard one, are derived for the FE solutions to the NLH problem.","These a posteriori estimates have confirmed a significant fact that is also valid for the NLH problem, namely the residual-type estimator seriously underestimates the error of the FE solution in the preasymptotic regime, which was first observed by Babu\\v{s}ka et al.","[Int J Numer Methods Eng 40 (1997)] for a one-dimensional linear problem.","Based on the new a posteriori error estimator, both the convergence and the quasi-optimality of the resulting adaptive finite element algorithm are proved the first time for the NLH problem, when the initial mesh size lying in the preasymptotic regime.","Finally, numerical examples are presented to validate the theoretical findings and demonstrate that applying the continuous interior penalty (CIP) technique with appropriate penalty parameters can reduce the pollution errors efficiently.","In particular, the nonlinear phenomenon of optical bistability with Gaussian incident waves is successfully simulated by the adaptive CIPFEM."],"url":"http://arxiv.org/abs/2405.15344v1","category":"math.NA"}
{"created":"2024-05-24 17:59:49","title":"Self-consistent evaluation of proximity and inverse proximity effects with pair-breaking in diffusive SN junctions","abstract":"We consider a planar superconducting-normal-metal (SN) junction with both inelastic and spin-flip scattering processes present. In the diffusive limit, we use a one-dimensional formulation of the Usadel equation to compute the self-consistent energy dependence of the single-particle density of states as a function of distance from the interface on both the superconducting and metallic sides for various spatial profiles of a pair-breaking spin-flip term. The pair-breaking processes fill in the superconducting gap at zero energy, which is reflected in the zero-bias tunneling conductance in scanning tunneling microscopy/spectroscopy experiments, in the vicinity of the junction. We also investigate the impact of having a partially transparent interface at the junction. We compare our findings with the observed exponential rise in the zero-bias conductance at the 1H step edge in recent experiments on 4Hb-TaS$_2$ [A. K. Nayak et al., Nat. Phys. 17, 1413 (2021)].","sentences":["We consider a planar superconducting-normal-metal (SN) junction with both inelastic and spin-flip scattering processes present.","In the diffusive limit, we use a one-dimensional formulation of the Usadel equation to compute the self-consistent energy dependence of the single-particle density of states as a function of distance from the interface on both the superconducting and metallic sides for various spatial profiles of a pair-breaking spin-flip term.","The pair-breaking processes fill in the superconducting gap at zero energy, which is reflected in the zero-bias tunneling conductance in scanning tunneling microscopy/spectroscopy experiments, in the vicinity of the junction.","We also investigate the impact of having a partially transparent interface at the junction.","We compare our findings with the observed exponential rise in the zero-bias conductance at the 1H step edge in recent experiments on 4Hb-TaS$_2$","[A. K. Nayak et al., Nat. Phys. 17, 1413 (2021)]."],"url":"http://arxiv.org/abs/2405.15770v1","category":"cond-mat.supr-con"}
{"created":"2024-05-24 17:19:25","title":"Multi-objective control for stochastic parabolic equations with dynamic boundary conditions","abstract":"This paper deals with a hierarchical multi-objective control problem for forward stochastic parabolic equations with dynamic boundary conditions. The controls are divided into two classes: leaders and followers. The goal of the leaders is of null controllability type while the followers are in charge of letting the state close to prescribed targets in fixed observation regions. To solve the problem, Nash and Stackelberg strategies are used. To implement these strategies, we combine some appropriate Carleman estimates and the well-known control duality approach.","sentences":["This paper deals with a hierarchical multi-objective control problem for forward stochastic parabolic equations with dynamic boundary conditions.","The controls are divided into two classes: leaders and followers.","The goal of the leaders is of null controllability type while the followers are in charge of letting the state close to prescribed targets in fixed observation regions.","To solve the problem, Nash and Stackelberg strategies are used.","To implement these strategies, we combine some appropriate Carleman estimates and the well-known control duality approach."],"url":"http://arxiv.org/abs/2405.15730v1","category":"math.OC"}
{"created":"2024-05-24 17:00:23","title":"On the theory of earthquakes: Paradoxical contradiction of Omori's law to the law of energy conservation","abstract":"After the main shock of an earthquake the aftershocks are observed. According to Omori's law, the frequency of aftershocks decreases hyperbolically over time. We noticed that, strictly speaking, Omori's law paradoxically contradicts the law of energy conservation. The contradiction is that the excitation of each aftershock consumes a finite portion of the source's energy, so that the total energy released by the source tends to infinity over time. The paradox is formally theoretical, but its analysis has proved useful. Eliminating the contradiction between Omori's law and the fundamental law of conservation of energy allowed us to further understand the nature of the phenomenological theory of aftershocks. We used the concept of deactivation of a source after the formation of a main rupture in it. We have based the theory on the original aftershock evolution equation, which has the form of a first-order linear differential equation. Two ways to eliminate the paradoxical situation are indicated. Key words: earthquake source, aftershock, evolution equation, deactivation coefficient, inverse problem, Omori epoch, source bifurcation, logistic equation, Hirano-Utsu formula.","sentences":["After the main shock of an earthquake the aftershocks are observed.","According to Omori's law, the frequency of aftershocks decreases hyperbolically over time.","We noticed that, strictly speaking, Omori's law paradoxically contradicts the law of energy conservation.","The contradiction is that the excitation of each aftershock consumes a finite portion of the source's energy, so that the total energy released by the source tends to infinity over time.","The paradox is formally theoretical, but its analysis has proved useful.","Eliminating the contradiction between Omori's law and the fundamental law of conservation of energy allowed us to further understand the nature of the phenomenological theory of aftershocks.","We used the concept of deactivation of a source after the formation of a main rupture in it.","We have based the theory on the original aftershock evolution equation, which has the form of a first-order linear differential equation.","Two ways to eliminate the paradoxical situation are indicated.","Key words: earthquake source, aftershock, evolution equation, deactivation coefficient, inverse problem, Omori epoch, source bifurcation, logistic equation, Hirano-Utsu formula."],"url":"http://arxiv.org/abs/2405.15710v1","category":"physics.geo-ph"}
{"created":"2024-05-24 16:45:48","title":"realSEUDO for real-time calcium imaging analysis","abstract":"Closed-loop neuroscience experimentation, where recorded neural activity is used to modify the experiment on-the-fly, is critical for deducing causal connections and optimizing experimental time. A critical step in creating a closed-loop experiment is real-time inference of neural activity from streaming recordings. One challenging modality for real-time processing is multi-photon calcium imaging (CI). CI enables the recording of activity in large populations of neurons however, often requires batch processing of the video data to extract single-neuron activity from the fluorescence videos. We use the recently proposed robust time-trace estimator-Sparse Emulation of Unused Dictionary Objects (SEUDO) algorithm-as a basis for a new on-line processing algorithm that simultaneously identifies neurons in the fluorescence video and infers their time traces in a way that is robust to as-yet unidentified neurons. To achieve real-time SEUDO (realSEUDO), we optimize the core estimator via both algorithmic improvements and an fast C-based implementation, and create a new cell finding loop to enable realSEUDO to also identify new cells. We demonstrate comparable performance to offline algorithms (e.g., CNMF), and improved performance over the current on-line approach (OnACID) at speeds of 120 Hz on average.","sentences":["Closed-loop neuroscience experimentation, where recorded neural activity is used to modify the experiment on-the-fly, is critical for deducing causal connections and optimizing experimental time.","A critical step in creating a closed-loop experiment is real-time inference of neural activity from streaming recordings.","One challenging modality for real-time processing is multi-photon calcium imaging (CI).","CI enables the recording of activity in large populations of neurons however, often requires batch processing of the video data to extract single-neuron activity from the fluorescence videos.","We use the recently proposed robust time-trace estimator-Sparse Emulation of Unused Dictionary Objects (SEUDO) algorithm-as a basis for a new on-line processing algorithm that simultaneously identifies neurons in the fluorescence video and infers their time traces in a way that is robust to as-yet unidentified neurons.","To achieve real-time SEUDO (realSEUDO), we optimize the core estimator via both algorithmic improvements and an fast C-based implementation, and create a new cell finding loop to enable realSEUDO to also identify new cells.","We demonstrate comparable performance to offline algorithms (e.g., CNMF), and improved performance over the current on-line approach (OnACID) at speeds of 120 Hz on average."],"url":"http://arxiv.org/abs/2405.15701v1","category":"eess.IV"}
{"created":"2024-05-24 16:26:26","title":"Stratified Sampling Algorithms for Machine Learning Methods in Solving Two-scale Partial Differential Equations","abstract":"Partial differential equations (PDEs) with multiple scales or those defined over sufficiently large domains arise in various areas of science and engineering and often present problems when approximating the solutions numerically. Machine learning techniques are a relatively recent method for solving PDEs. Despite the increasing number of machine learning strategies developed to approximate PDEs, many remain focused on relatively small domains. When scaling the equations, a large domain is naturally obtained, especially when the solution exhibits multiscale characteristics. This study examines two-scale equations whose solution structures exhibit distinct characteristics: highly localized in some regions and significantly flat in others. These two regions must be adequately addressed over a large domain to approximate the solution more accurately. We focus on the vanishing gradient problem given by the diminishing gradient zone of the activation function over large domains and propose a stratified sampling algorithm to address this problem. We compare the uniform random classical sampling method over the entire domain and the proposed stratified sampling method. The numerical results confirm that the proposed method yields more accurate and consistent solutions than classical methods.","sentences":["Partial differential equations (PDEs) with multiple scales or those defined over sufficiently large domains arise in various areas of science and engineering and often present problems when approximating the solutions numerically.","Machine learning techniques are a relatively recent method for solving PDEs.","Despite the increasing number of machine learning strategies developed to approximate PDEs, many remain focused on relatively small domains.","When scaling the equations, a large domain is naturally obtained, especially when the solution exhibits multiscale characteristics.","This study examines two-scale equations whose solution structures exhibit distinct characteristics: highly localized in some regions and significantly flat in others.","These two regions must be adequately addressed over a large domain to approximate the solution more accurately.","We focus on the vanishing gradient problem given by the diminishing gradient zone of the activation function over large domains and propose a stratified sampling algorithm to address this problem.","We compare the uniform random classical sampling method over the entire domain and the proposed stratified sampling method.","The numerical results confirm that the proposed method yields more accurate and consistent solutions than classical methods."],"url":"http://arxiv.org/abs/2405.15686v1","category":"math.NA"}
{"created":"2024-05-24 16:04:30","title":"Well-posedness and invariant measures for the stochastically perturbed Landau-Lifshitz-Baryakhtar equation","abstract":"In this paper, we study the initial-boundary value problem for the stochastic Landau-Lifshitz-Baryakhtar (SLLBar) equation with Stratonovich-type noise in bounded domains $\\mathcal{O}\\subset\\mathbb{R}^d$, $d=1,2,3$. Our main results can be briefly described as follows: (1) for $d=1,2,3$ and any $\\mathbf{u}_0\\in\\mathbb{H}^1$, the SLLBar equation admits a unique local-in-time pathwise weak solution; (2) for $d=1$ and small-data $\\mathbf{u}_0\\in\\mathbb{H}^1$, the SLLBar equation has a unique global-in-time pathwise weak solution and at least one invariant measure; (3) for $d=1,2$ and small-data $\\mathbf{u}_0\\in\\mathbb{L}^2$, the SLLBar equation possesses a unique global-in-time pathwise very weak solution and at least one invariant measure, while for $d=3$ only the existence of martingale solution is obtained due to the loss of pathwise uniqueness.","sentences":["In this paper, we study the initial-boundary value problem for the stochastic Landau-Lifshitz-Baryakhtar (SLLBar) equation with Stratonovich-type noise in bounded domains $\\mathcal{O}\\subset\\mathbb{R}^d$, $d=1,2,3$. Our main results can be briefly described as follows: (1) for $d=1,2,3$ and any $\\mathbf{u}_0\\in\\mathbb{H}^1$, the SLLBar equation admits a unique local-in-time pathwise weak solution; (2) for $d=1$ and small-data $\\mathbf{u}_0\\in\\mathbb{H}^1$, the SLLBar equation has a unique global-in-time pathwise weak solution and at least one invariant measure; (3) for $d=1,2$ and small-data $\\mathbf{u}_0\\in\\mathbb{L}^2$, the SLLBar equation possesses a unique global-in-time pathwise very weak solution and at least one invariant measure, while for $d=3$ only the existence of martingale solution is obtained due to the loss of pathwise uniqueness."],"url":"http://arxiv.org/abs/2405.15666v1","category":"math.AP"}
{"created":"2024-05-24 15:20:48","title":"Proyective Cohen-Macaulay monomial curves and their affine projections","abstract":"In this paper, we explore when the Betti numbers of the coordinate rings of a projective monomial curve and one of its affine charts are identical. Given an infinite field $k$ and a sequence of relatively prime integers $a_0 = 0 < a_1 < \\cdots < a_n = d$, we consider the projective monomial curve $\\mathcal{C}\\subset\\mathbb{P}_k^{\\,n}$ of degree $d$ parametrically defined by $x_i = u^{a_i}v^{d-a_i}$ for all $i \\in \\{0,\\ldots,n\\}$ and its coordinate ring $k[\\mathcal{C}]$. The curve $\\mathcal{C}_1 \\subset \\mathbb A_k^n$ with parametric equations $x_i = t^{a_i}$ for $i \\in \\{1,\\ldots,n\\}$ is an affine chart of $\\mathcal{C}$ and we denote by $k[\\mathcal{C}_1]$ its coordinate ring. The main contribution of this paper is the introduction of a novel (Gr\\\"obner-free) combinatorial criterion that provides a sufficient condition for the equality of the Betti numbers of $k[\\mathcal{C}]$ and $k[\\mathcal{C}_1]$. Leveraging this criterion, we identify infinite families of projective curves satisfying this property. Also, we use our results to study the so-called shifted family of monomial curves, i.e., the family of curves associated to the sequences $j+a_1 < \\cdots < j+a_n$ for different values of $j \\in \\mathbb N$. In this context, Vu proved that for large enough values of $j$, one has an equality between the Betti numbers of the corresponding affine and projective curves. Using our results, we improve Vu's upper bound for the least value of $j$ such that this occurs.","sentences":["In this paper, we explore when the Betti numbers of the coordinate rings of a projective monomial curve and one of its affine charts are identical.","Given an infinite field $k$ and a sequence of relatively prime integers $a_0 = 0 <","a_1 <","\\cdots < a_n","= d$, we consider the projective monomial curve $\\mathcal{C}\\subset\\mathbb{P}_k^{\\,n}$ of degree $d$ parametrically defined by $x_i = u^{a_i}v^{d-a_i}$ for all $i \\in \\{0,\\ldots,n\\}$ and its coordinate ring $k[\\mathcal{C}]$.","The curve $\\mathcal{C}_1 \\subset \\mathbb A_k^n$ with parametric equations $x_i = t^{a_i}$ for $i \\in \\{1,\\ldots,n\\}$ is an affine chart of $\\mathcal{C}$ and we denote by $k[\\mathcal{C}_1]$ its coordinate ring.","The main contribution of this paper is the introduction of a novel (Gr\\\"obner-free) combinatorial criterion that provides a sufficient condition for the equality of the Betti numbers of $k[\\mathcal{C}]$ and $k[\\mathcal{C}_1]$. Leveraging this criterion, we identify infinite families of projective curves satisfying this property.","Also, we use our results to study the so-called shifted family of monomial curves, i.e., the family of curves associated to the sequences $j+a_1 <","\\cdots < j+a_n$ for different values of $j \\in \\mathbb N$.","In this context, Vu proved that for large enough values of $j$, one has an equality between the Betti numbers of the corresponding affine and projective curves.","Using our results, we improve Vu's upper bound for the least value of $j$ such that this occurs."],"url":"http://arxiv.org/abs/2405.15634v1","category":"math.AC"}
{"created":"2024-05-24 15:04:36","title":"MLPs Learn In-Context","abstract":"In-context learning (ICL), the remarkable ability to solve a task from only input exemplars, has commonly been assumed to be a unique hallmark of Transformer models. In this study, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context. Moreover, we find that MLPs, and the closely related MLP-Mixer models, learn in-context competitively with Transformers given the same compute budget. We further show that MLPs outperform Transformers on a subset of ICL tasks designed to test relational reasoning. These results suggest that in-context learning is not exclusive to Transformers and highlight the potential of exploring this phenomenon beyond attention-based architectures. In addition, MLPs' surprising success on relational tasks challenges prior assumptions about simple connectionist models. Altogether, our results endorse the broad trend that ``less inductive bias is better\" and contribute to the growing interest in all-MLP alternatives to task-specific architectures.","sentences":["In-context learning (ICL), the remarkable ability to solve a task from only input exemplars, has commonly been assumed to be a unique hallmark of Transformer models.","In this study, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context.","Moreover, we find that MLPs, and the closely related MLP-Mixer models, learn in-context competitively with Transformers given the same compute budget.","We further show that MLPs outperform Transformers on a subset of ICL tasks designed to test relational reasoning.","These results suggest that in-context learning is not exclusive to Transformers and highlight the potential of exploring this phenomenon beyond attention-based architectures.","In addition, MLPs' surprising success on relational tasks challenges prior assumptions about simple connectionist models.","Altogether, our results endorse the broad trend that ``less inductive bias is better\" and contribute to the growing interest in all-MLP alternatives to task-specific architectures."],"url":"http://arxiv.org/abs/2405.15618v1","category":"cs.LG"}
{"created":"2024-05-24 14:54:42","title":"Planet-driven spirals in protoplanetary discs: limitations of the semi-analytical theory for observations","abstract":"Detecting protoplanets during their formation stage is an important but elusive goal of modern astronomy. Kinematic detections via the spiral wakes in the gaseous disc are a promising avenue to achieve this goal. We aim to test the applicability to observations in the low and intermediate planet mass regimes of a commonly used semi-analytical model for planet induced spiral waves. In contrast with previous works which proposed to use the semi-analytical model to interpret observations, in this study we analyse for the first time both the structure of the velocity and density perturbations. We run a set of FARGO3D hydrodynamic simulations and compare them with the output of the semi-analytic model in the code wakeflow, which is obtained by solving Burgers' equation using the simulations as an initial condition. We find that the velocity field derived from the analytic theory is discontinuous at the interface between the linear and nonlinear regions. After 0.2 r$_p$ from the planet, the behaviour of the velocity field closely follows that of the density perturbations. In the low mass limit, the analytical model is in qualitative agreement with the simulations, although it underestimates the azimuthal width and the amplitude of the perturbations, predicting a stronger decay but a slower azimuthal advance of the shock fronts. In the intermediate regime, the discrepancy increases, resulting in a different pitch angle between the spirals of the simulations and the analytic model. The implementation of a fitting procedure based on the minimisation of intensity residuals is bound to fail due to the deviation in pitch angle between the analytic model and the simulations. In order to apply this model to observations, it needs to be revisited accounting also for higher planet masses.","sentences":["Detecting protoplanets during their formation stage is an important but elusive goal of modern astronomy.","Kinematic detections via the spiral wakes in the gaseous disc are a promising avenue to achieve this goal.","We aim to test the applicability to observations in the low and intermediate planet mass regimes of a commonly used semi-analytical model for planet induced spiral waves.","In contrast with previous works which proposed to use the semi-analytical model to interpret observations, in this study we analyse for the first time both the structure of the velocity and density perturbations.","We run a set of FARGO3D hydrodynamic simulations and compare them with the output of the semi-analytic model in the code wakeflow, which is obtained by solving Burgers' equation using the simulations as an initial condition.","We find that the velocity field derived from the analytic theory is discontinuous at the interface between the linear and nonlinear regions.","After 0.2 r$_p$ from the planet, the behaviour of the velocity field closely follows that of the density perturbations.","In the low mass limit, the analytical model is in qualitative agreement with the simulations, although it underestimates the azimuthal width and the amplitude of the perturbations, predicting a stronger decay but a slower azimuthal advance of the shock fronts.","In the intermediate regime, the discrepancy increases, resulting in a different pitch angle between the spirals of the simulations and the analytic model.","The implementation of a fitting procedure based on the minimisation of intensity residuals is bound to fail due to the deviation in pitch angle between the analytic model and the simulations.","In order to apply this model to observations, it needs to be revisited accounting also for higher planet masses."],"url":"http://arxiv.org/abs/2405.15611v1","category":"astro-ph.EP"}
{"created":"2024-05-24 14:20:45","title":"Profiling checkpointing schedules in adjoint ST-AD","abstract":"Checkpointing is a cornerstone of data-flow reversal in adjoint algorithmic differentiation. Checkpointing is a storage/recomputation trade-off that can be applied at different levels, one of which being the call tree. We are looking for good placements of checkpoints onto the call tree of a given application, to reduce run time and memory footprint of its adjoint. There is no known optimal solution to this problem other than a combinatorial search on all placements. We propose a heuristics based on run-time profiling of the adjoint code. We describe implementation of this profiling tool in an existing source-transformation AD tool. We demonstrate the interest of this approach on test cases taken from the MITgcm ocean and atmospheric global circulation model. We discuss the limitations of our approach and propose directions to lift them.","sentences":["Checkpointing is a cornerstone of data-flow reversal in adjoint algorithmic differentiation.","Checkpointing is a storage/recomputation trade-off that can be applied at different levels, one of which being the call tree.","We are looking for good placements of checkpoints onto the call tree of a given application, to reduce run time and memory footprint of its adjoint.","There is no known optimal solution to this problem other than a combinatorial search on all placements.","We propose a heuristics based on run-time profiling of the adjoint code.","We describe implementation of this profiling tool in an existing source-transformation AD tool.","We demonstrate the interest of this approach on test cases taken from the MITgcm ocean and atmospheric global circulation model.","We discuss the limitations of our approach and propose directions to lift them."],"url":"http://arxiv.org/abs/2405.15590v1","category":"cs.CL"}
{"created":"2024-05-24 14:12:23","title":"Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported","abstract":"We pursue transfer learning to improve classifier accuracy on a target task with few labeled examples available for training. Recent work suggests that using a source task to learn a prior distribution over neural net weights, not just an initialization, can boost target task performance. In this study, we carefully compare transfer learning with and without source task informed priors across 5 datasets. We find that standard transfer learning informed by an initialization only performs far better than reported in previous comparisons. The relative gains of methods using informative priors over standard transfer learning vary in magnitude across datasets. For the scenario of 5-300 examples per class, we find negative or negligible gains on 2 datasets, modest gains (between 1.5-3 points of accuracy) on 2 other datasets, and substantial gains (>8 points) on one dataset. Among methods using informative priors, we find that an isotropic covariance appears competitive with learned low-rank covariance matrix while being substantially simpler to understand and tune. Further analysis suggests that the mechanistic justification for informed priors -- hypothesized improved alignment between train and test loss landscapes -- is not consistently supported due to high variability in empirical landscapes. We release code to allow independent reproduction of all experiments.","sentences":["We pursue transfer learning to improve classifier accuracy on a target task with few labeled examples available for training.","Recent work suggests that using a source task to learn a prior distribution over neural net weights, not just an initialization, can boost target task performance.","In this study, we carefully compare transfer learning with and without source task informed priors across 5 datasets.","We find that standard transfer learning informed by an initialization only performs far better than reported in previous comparisons.","The relative gains of methods using informative priors over standard transfer learning vary in magnitude across datasets.","For the scenario of 5-300 examples per class, we find negative or negligible gains on 2 datasets, modest gains (between 1.5-3 points of accuracy) on 2 other datasets, and substantial gains (>8 points) on one dataset.","Among methods using informative priors, we find that an isotropic covariance appears competitive with learned low-rank covariance matrix while being substantially simpler to understand and tune.","Further analysis suggests that the mechanistic justification for informed priors -- hypothesized improved alignment between train and test loss landscapes -- is not consistently supported due to high variability in empirical landscapes.","We release code to allow independent reproduction of all experiments."],"url":"http://arxiv.org/abs/2405.15583v1","category":"cs.LG"}
{"created":"2024-05-24 14:06:01","title":"Closed mean curvature flows with asymptotically conical singularities","abstract":"In this paper, we prove that for any asymptotically conical self-shrinker, there exists a closed hypersurface such that the mean curvature flow starting from it develops a singularity modeled on the given shrinker. The main technique is the Wa\\.zewski box argument, used by Stolarski \\cite{Sto} in the proof of the corresponding theorem in the Ricci flow case.","sentences":["In this paper, we prove that for any asymptotically conical self-shrinker, there exists a closed hypersurface such that the mean curvature flow starting from it develops a singularity modeled on the given shrinker.","The main technique is the Wa\\.zewski box argument, used by Stolarski \\cite{Sto} in the proof of the corresponding theorem in the Ricci flow case."],"url":"http://arxiv.org/abs/2405.15577v1","category":"math.DG"}
{"created":"2024-05-24 13:28:48","title":"Bundle Neural Networks for message diffusion on graphs","abstract":"The dominant paradigm for learning on graph-structured data is message passing. Despite being a strong inductive bias, the local message passing mechanism suffers from pathological issues such as over-smoothing, over-squashing, and limited node-level expressivity. To address these limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that operates via message diffusion over flat vector bundles - structures analogous to connections on Riemannian manifolds that augment the graph by assigning to each node a vector space and an orthogonal map. A BuNN layer evolves the features according to a diffusion-type partial differential equation. When discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a recently proposed MPNN capable of mitigating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate on larger scales of the graph and, therefore, to mitigate over-squashing. Finally, we prove that BuNN can approximate any feature transformation over nodes on any (potentially infinite) family of graphs given injective positional encodings, resulting in universal node-level expressivity. We support our theory via synthetic experiments and showcase the strong empirical performance of BuNNs over a range of real-world tasks, achieving state-of-the-art results on several standard benchmarks in transductive and inductive settings.","sentences":["The dominant paradigm for learning on graph-structured data is message passing.","Despite being a strong inductive bias, the local message passing mechanism suffers from pathological issues such as over-smoothing, over-squashing, and limited node-level expressivity.","To address these limitations we propose Bundle Neural Networks (BuNN), a new type of GNN that operates via message diffusion over flat vector bundles - structures analogous to connections on Riemannian manifolds that augment the graph by assigning to each node a vector space and an orthogonal map.","A BuNN layer evolves the features according to a diffusion-type partial differential equation.","When discretized, BuNNs are a special case of Sheaf Neural Networks (SNNs), a recently proposed MPNN capable of mitigating over-smoothing.","The continuous nature of message diffusion enables BuNNs to operate on larger scales of the graph and, therefore, to mitigate over-squashing.","Finally, we prove that BuNN can approximate any feature transformation over nodes on any (potentially infinite) family of graphs given injective positional encodings, resulting in universal node-level expressivity.","We support our theory via synthetic experiments and showcase the strong empirical performance of BuNNs over a range of real-world tasks, achieving state-of-the-art results on several standard benchmarks in transductive and inductive settings."],"url":"http://arxiv.org/abs/2405.15540v1","category":"cs.LG"}
{"created":"2024-05-24 13:23:49","title":"Using covariance extension equation to solve the Nevanlinna-Pick interpolation with degree constraint","abstract":"Nevanlinna-Pick interpolation problem has been widely studied in recent decades, however, the known algorithm is not simplistic and robust enough. This paper provide a new method to solve the Nevanlinna-Pick interpolation problem with degree constraint. It is based on the covariance extension equation proposed by Byrnes and Lindquist. A reformulation of the Nevanlinna-Pick interpolation problem is achieved and then solved by continuation method. This method need not calculate the initial value and a numerical example illustrates robustness and effciency of the proposed procedure","sentences":["Nevanlinna-Pick interpolation problem has been widely studied in recent decades, however, the known algorithm is not simplistic and robust enough.","This paper provide a new method to solve the Nevanlinna-Pick interpolation problem with degree constraint.","It is based on the covariance extension equation proposed by Byrnes and Lindquist.","A reformulation of the Nevanlinna-Pick interpolation problem is achieved and then solved by continuation method.","This method need not calculate the initial value and a numerical example illustrates robustness and effciency of the proposed procedure"],"url":"http://arxiv.org/abs/2405.15533v1","category":"math.NA"}
{"created":"2024-05-24 09:25:37","title":"Single-Event Upset Analysis of a Systolic Array based Deep Neural Network Accelerator","abstract":"Deep Neural Network (DNN) accelerators are extensively used to improve the computational efficiency of DNNs, but are prone to faults through Single-Event Upsets (SEUs). In this work, we present an in-depth analysis of the impact of SEUs on a Systolic Array (SA) based DNN accelerator. A fault injection campaign is performed through a Register-Transfer Level (RTL) based simulation environment to improve the observability of each hardware block, including the SA itself as well as the post-processing pipeline. From this analysis, we present the sensitivity, independent of a DNN model architecture, for various flip-flop groups both in terms of fault propagation probability and fault magnitude. This allows us to draw detailed conclusions and determine optimal mitigation strategies.","sentences":["Deep Neural Network (DNN) accelerators are extensively used to improve the computational efficiency of DNNs, but are prone to faults through Single-Event Upsets (SEUs).","In this work, we present an in-depth analysis of the impact of SEUs on a Systolic Array (SA) based DNN accelerator.","A fault injection campaign is performed through a Register-Transfer Level (RTL) based simulation environment to improve the observability of each hardware block, including the SA itself as well as the post-processing pipeline.","From this analysis, we present the sensitivity, independent of a DNN model architecture, for various flip-flop groups both in terms of fault propagation probability and fault magnitude.","This allows us to draw detailed conclusions and determine optimal mitigation strategies."],"url":"http://arxiv.org/abs/2405.15381v1","category":"cs.AR"}
{"created":"2024-05-24 17:41:30","title":"CAFe: Cost and Age aware Federated Learning","abstract":"In many federated learning (FL) models, a common strategy employed to ensure the progress in the training process, is to wait for at least $M$ clients out of the total $N$ clients to send back their local gradients based on a reporting deadline $T$, once the parameter server (PS) has broadcasted the global model. If enough clients do not report back within the deadline, the particular round is considered to be a failed round and the training round is restarted from scratch. If enough clients have responded back, the round is deemed successful and the local gradients of all the clients that responded back are used to update the global model. In either case, the clients that failed to report back an update within the deadline would have wasted their computational resources. Having a tighter deadline (small $T$) and waiting for a larger number of participating clients (large $M$) leads to a large number of failed rounds and therefore greater communication cost and computation resource wastage. However, having a larger $T$ leads to longer round durations whereas smaller $M$ may lead to noisy gradients. Therefore, there is a need to optimize the parameters $M$ and $T$ such that communication cost and the resource wastage is minimized while having an acceptable convergence rate. In this regard, we show that the average age of a client at the PS appears explicitly in the theoretical convergence bound, and therefore, can be used as a metric to quantify the convergence of the global model. We provide an analytical scheme to select the parameters $M$ and $T$ in this setting.","sentences":["In many federated learning (FL) models, a common strategy employed to ensure the progress in the training process, is to wait for at least $M$ clients out of the total $N$ clients to send back their local gradients based on a reporting deadline $T$, once the parameter server (PS) has broadcasted the global model.","If enough clients do not report back within the deadline, the particular round is considered to be a failed round and the training round is restarted from scratch.","If enough clients have responded back, the round is deemed successful and the local gradients of all the clients that responded back are used to update the global model.","In either case, the clients that failed to report back an update within the deadline would have wasted their computational resources.","Having a tighter deadline (small $T$) and waiting for a larger number of participating clients (large $M$) leads to a large number of failed rounds and therefore greater communication cost and computation resource wastage.","However, having a larger $T$ leads to longer round durations whereas smaller $M$ may lead to noisy gradients.","Therefore, there is a need to optimize the parameters $M$ and $T$ such that communication cost and the resource wastage is minimized while having an acceptable convergence rate.","In this regard, we show that the average age of a client at the PS appears explicitly in the theoretical convergence bound, and therefore, can be used as a metric to quantify the convergence of the global model.","We provide an analytical scheme to select the parameters $M$ and $T$ in this setting."],"url":"http://arxiv.org/abs/2405.15744v1","category":"cs.LG"}
{"created":"2024-05-24 15:34:09","title":"Harnessing Increased Client Participation with Cohort-Parallel Federated Learning","abstract":"Federated Learning (FL) is a machine learning approach where nodes collaboratively train a global model. As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes. In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts. We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using one-shot Knowledge Distillation (KD) and a cross-domain, unlabeled dataset. The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate. Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute and communication resources. Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9$\\times$ reduction in train time and a 1.3$\\times$ reduction in resource usage, with a minimal drop in test accuracy.","sentences":["Federated Learning (FL) is a machine learning approach where nodes collaboratively train a global model.","As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes.","In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts.","We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using one-shot Knowledge Distillation (KD) and a cross-domain, unlabeled dataset.","The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate.","Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute and communication resources.","Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9$\\times$ reduction in train time and a 1.3$\\times$ reduction in resource usage, with a minimal drop in test accuracy."],"url":"http://arxiv.org/abs/2405.15644v1","category":"cs.LG"}
{"created":"2024-05-24 14:18:31","title":"Composed Image Retrieval for Remote Sensing","abstract":"This work introduces composed image retrieval to remote sensing. It allows to query a large image archive by image examples alternated by a textual description, enriching the descriptive power over unimodal queries, either visual or textual. Various attributes can be modified by the textual part, such as shape, color, or context. A novel method fusing image-to-image and text-to-image similarity is introduced. We demonstrate that a vision-language model possesses sufficient descriptive power and no further learning step or training data are necessary. We present a new evaluation benchmark focused on color, context, density, existence, quantity, and shape modifications. Our work not only sets the state-of-the-art for this task, but also serves as a foundational step in addressing a gap in the field of remote sensing image retrieval. Code at: https://github.com/billpsomas/rscir","sentences":["This work introduces composed image retrieval to remote sensing.","It allows to query a large image archive by image examples alternated by a textual description, enriching the descriptive power over unimodal queries, either visual or textual.","Various attributes can be modified by the textual part, such as shape, color, or context.","A novel method fusing image-to-image and text-to-image similarity is introduced.","We demonstrate that a vision-language model possesses sufficient descriptive power and no further learning step or training data are necessary.","We present a new evaluation benchmark focused on color, context, density, existence, quantity, and shape modifications.","Our work not only sets the state-of-the-art for this task, but also serves as a foundational step in addressing a gap in the field of remote sensing image retrieval.","Code at: https://github.com/billpsomas/rscir"],"url":"http://arxiv.org/abs/2405.15587v1","category":"cs.CV"}
{"created":"2024-05-24 14:14:24","title":"DAGER: Exact Gradient Inversion for Large Language Models","abstract":"Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99).","sentences":["Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data.","However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks.","While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences.","In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly.","DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data.","We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively.","We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 > 0.99)."],"url":"http://arxiv.org/abs/2405.15586v1","category":"cs.LG"}
{"created":"2024-05-24 13:54:55","title":"Exploring galactic properties with machine learning Predicting star formation, stellar mass, and metallicity from photometric data","abstract":"Aims. We explore machine learning techniques to forecast star formation rate, stellar mass, and metallicity across galaxies with redshifts ranging from 0.01 to 0.3.   Methods. Leveraging CatBoost and deep learning architectures, we utilize multiband optical and infrared photometric data from SDSS and AllWISE, trained on the SDSS MPA-JHU DR8 catalogue.   Results. Our study demonstrates the potential of machine learning in accurately predicting galaxy properties solely from photometric data. We achieve minimised root mean square errors, specifically employing the CatBoost model. For star formation rate prediction, we attain a value of RMSESFR = 0.336 dex, while for stellar mass prediction, the error is reduced to RMSESM = 0.206 dex. Additionally, our model yields a metallicity prediction of RMSEmetallicity = 0.097 dex.   Conclusions. These findings underscore the significance of automated methodologies in efficiently estimating critical galaxy properties, amid the exponential growth of multi-wavelength astronomy data. Future research may focus on refining machine learning models and expanding datasets for even more accurate predictions.","sentences":["Aims.","We explore machine learning techniques to forecast star formation rate, stellar mass, and metallicity across galaxies with redshifts ranging from 0.01 to 0.3.   Methods.","Leveraging CatBoost and deep learning architectures, we utilize multiband optical and infrared photometric data from SDSS and AllWISE, trained on the SDSS MPA-JHU DR8 catalogue.   Results.","Our study demonstrates the potential of machine learning in accurately predicting galaxy properties solely from photometric data.","We achieve minimised root mean square errors, specifically employing the CatBoost model.","For star formation rate prediction, we attain a value of RMSESFR = 0.336 dex, while for stellar mass prediction, the error is reduced to RMSESM = 0.206 dex.","Additionally, our model yields a metallicity prediction of RMSEmetallicity = 0.097 dex.   Conclusions.","These findings underscore the significance of automated methodologies in efficiently estimating critical galaxy properties, amid the exponential growth of multi-wavelength astronomy data.","Future research may focus on refining machine learning models and expanding datasets for even more accurate predictions."],"url":"http://arxiv.org/abs/2405.15566v1","category":"astro-ph.GA"}
{"created":"2024-05-24 12:39:21","title":"Hierarchical Loss And Geometric Mask Refinement For Multilabel Ribs Segmentation","abstract":"Automatic ribs segmentation and numeration can increase computed tomography assessment speed and reduce radiologists mistakes. We introduce a model for multilabel ribs segmentation with hierarchical loss function, which enable to improve multilabel segmentation quality. Also we propose postprocessing technique to further increase labeling quality. Our model achieved new state-of-the-art 98.2% label accuracy on public RibSeg v2 dataset, surpassing previous result by 6.7%.","sentences":["Automatic ribs segmentation and numeration can increase computed tomography assessment speed and reduce radiologists mistakes.","We introduce a model for multilabel ribs segmentation with hierarchical loss function, which enable to improve multilabel segmentation quality.","Also we propose postprocessing technique to further increase labeling quality.","Our model achieved new state-of-the-art 98.2% label accuracy on public RibSeg v2 dataset, surpassing previous result by 6.7%."],"url":"http://arxiv.org/abs/2405.15500v1","category":"eess.IV"}
{"created":"2024-05-24 11:53:13","title":"Unlearning during Learning: An Efficient Federated Machine Unlearning Method","abstract":"In recent years, Federated Learning (FL) has garnered significant attention as a distributed machine learning paradigm. To facilitate the implementation of the right to be forgotten, the concept of federated machine unlearning (FMU) has also emerged. However, current FMU approaches often involve additional time-consuming steps and may not offer comprehensive unlearning capabilities, which renders them less practical in real FL scenarios. In this paper, we introduce FedAU, an innovative and efficient FMU framework aimed at overcoming these limitations. Specifically, FedAU incorporates a lightweight auxiliary unlearning module into the learning process and employs a straightforward linear operation to facilitate unlearning. This approach eliminates the requirement for extra time-consuming steps, rendering it well-suited for FL. Furthermore, FedAU exhibits remarkable versatility. It not only enables multiple clients to carry out unlearning tasks concurrently but also supports unlearning at various levels of granularity, including individual data samples, specific classes, and even at the client level. We conducted extensive experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the performance of FedAU. The results demonstrate that FedAU effectively achieves the desired unlearning effect while maintaining model accuracy.","sentences":["In recent years, Federated Learning (FL) has garnered significant attention as a distributed machine learning paradigm.","To facilitate the implementation of the right to be forgotten, the concept of federated machine unlearning (FMU) has also emerged.","However, current FMU approaches often involve additional time-consuming steps and may not offer comprehensive unlearning capabilities, which renders them less practical in real FL scenarios.","In this paper, we introduce FedAU, an innovative and efficient FMU framework aimed at overcoming these limitations.","Specifically, FedAU incorporates a lightweight auxiliary unlearning module into the learning process and employs a straightforward linear operation to facilitate unlearning.","This approach eliminates the requirement for extra time-consuming steps, rendering it well-suited for FL.","Furthermore, FedAU exhibits remarkable versatility.","It not only enables multiple clients to carry out unlearning tasks concurrently but also supports unlearning at various levels of granularity, including individual data samples, specific classes, and even at the client level.","We conducted extensive experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the performance of FedAU.","The results demonstrate that FedAU effectively achieves the desired unlearning effect while maintaining model accuracy."],"url":"http://arxiv.org/abs/2405.15474v1","category":"cs.LG"}
{"created":"2024-05-24 10:45:24","title":"Enhancing Pollinator Conservation towards Agriculture 4.0: Monitoring of Bees through Object Recognition","abstract":"In an era of rapid climate change and its adverse effects on food production, technological intervention to monitor pollinator conservation is of paramount importance for environmental monitoring and conservation for global food security. The survival of the human species depends on the conservation of pollinators. This article explores the use of Computer Vision and Object Recognition to autonomously track and report bee behaviour from images. A novel dataset of 9664 images containing bees is extracted from video streams and annotated with bounding boxes. With training, validation and testing sets (6722, 1915, and 997 images, respectively), the results of the COCO-based YOLO model fine-tuning approaches show that YOLOv5m is the most effective approach in terms of recognition accuracy. However, YOLOv5s was shown to be the most optimal for real-time bee detection with an average processing and inference time of 5.1ms per video frame at the cost of slightly lower ability. The trained model is then packaged within an explainable AI interface, which converts detection events into timestamped reports and charts, with the aim of facilitating use by non-technical users such as expert stakeholders from the apiculture industry towards informing responsible consumption and production.","sentences":["In an era of rapid climate change and its adverse effects on food production, technological intervention to monitor pollinator conservation is of paramount importance for environmental monitoring and conservation for global food security.","The survival of the human species depends on the conservation of pollinators.","This article explores the use of Computer Vision and Object Recognition to autonomously track and report bee behaviour from images.","A novel dataset of 9664 images containing bees is extracted from video streams and annotated with bounding boxes.","With training, validation and testing sets (6722, 1915, and 997 images, respectively), the results of the COCO-based YOLO model fine-tuning approaches show that YOLOv5m is the most effective approach in terms of recognition accuracy.","However, YOLOv5s was shown to be the most optimal for real-time bee detection with an average processing and inference time of 5.1ms per video frame at the cost of slightly lower ability.","The trained model is then packaged within an explainable AI interface, which converts detection events into timestamped reports and charts, with the aim of facilitating use by non-technical users such as expert stakeholders from the apiculture industry towards informing responsible consumption and production."],"url":"http://arxiv.org/abs/2405.15428v1","category":"cs.CV"}
{"created":"2024-05-24 10:37:38","title":"Lost in the Averages: A New Specific Setup to Evaluate Membership Inference Attacks Against Machine Learning Models","abstract":"Membership Inference Attacks (MIAs) are widely used to evaluate the propensity of a machine learning (ML) model to memorize an individual record and the privacy risk releasing the model poses. MIAs are commonly evaluated similarly to ML models: the MIA is performed on a test set of models trained on datasets unseen during training, which are sampled from a larger pool, $D_{eval}$. The MIA is evaluated across all datasets in this test set, and is thus evaluated across the distribution of samples from $D_{eval}$. While this was a natural extension of ML evaluation to MIAs, recent work has shown that a record's risk heavily depends on its specific dataset. For example, outliers are particularly vulnerable, yet an outlier in one dataset may not be one in another. The sources of randomness currently used to evaluate MIAs may thus lead to inaccurate individual privacy risk estimates. We propose a new, specific evaluation setup for MIAs against ML models, using weight initialization as the sole source of randomness. This allows us to accurately evaluate the risk associated with the release of a model trained on a specific dataset. Using SOTA MIAs, we empirically show that the risk estimates given by the current setup lead to many records being misclassified as low risk. We derive theoretical results which, combined with empirical evidence, suggest that the risk calculated in the current setup is an average of the risks specific to each sampled dataset, validating our use of weight initialization as the only source of randomness. Finally, we consider an MIA with a stronger adversary leveraging information about the target dataset to infer membership. Taken together, our results show that current MIA evaluation is averaging the risk across datasets leading to inaccurate risk estimates, and the risk posed by attacks leveraging information about the target dataset to be potentially underestimated.","sentences":["Membership Inference Attacks (MIAs) are widely used to evaluate the propensity of a machine learning (ML) model to memorize an individual record and the privacy risk releasing the model poses.","MIAs are commonly evaluated similarly to ML models: the MIA is performed on a test set of models trained on datasets unseen during training, which are sampled from a larger pool, $D_{eval}$. The MIA is evaluated across all datasets in this test set, and is thus evaluated across the distribution of samples from $D_{eval}$. While this was a natural extension of ML evaluation to MIAs, recent work has shown that a record's risk heavily depends on its specific dataset.","For example, outliers are particularly vulnerable, yet an outlier in one dataset may not be one in another.","The sources of randomness currently used to evaluate MIAs may thus lead to inaccurate individual privacy risk estimates.","We propose a new, specific evaluation setup for MIAs against ML models, using weight initialization as the sole source of randomness.","This allows us to accurately evaluate the risk associated with the release of a model trained on a specific dataset.","Using SOTA MIAs, we empirically show that the risk estimates given by the current setup lead to many records being misclassified as low risk.","We derive theoretical results which, combined with empirical evidence, suggest that the risk calculated in the current setup is an average of the risks specific to each sampled dataset, validating our use of weight initialization as the only source of randomness.","Finally, we consider an MIA with a stronger adversary leveraging information about the target dataset to infer membership.","Taken together, our results show that current MIA evaluation is averaging the risk across datasets leading to inaccurate risk estimates, and the risk posed by attacks leveraging information about the target dataset to be potentially underestimated."],"url":"http://arxiv.org/abs/2405.15423v1","category":"cs.LG"}
{"created":"2024-05-24 10:24:30","title":"MambaVC: Learned Visual Compression with Selective State Spaces","abstract":"Learned visual compression is an important and active task in multimedia. Existing approaches have explored various CNN- and Transformer-based designs to model content distribution and eliminate redundancy, where balancing efficacy (i.e., rate-distortion trade-off) and efficiency remains a challenge. Recently, state-space models (SSMs) have shown promise due to their long-range modeling capacity and efficiency. Inspired by this, we take the first step to explore SSMs for visual compression. We introduce MambaVC, a simple, strong and efficient compression network based on SSM. MambaVC develops a visual state space (VSS) block with a 2D selective scanning (2DSS) module as the nonlinear activation function after each downsampling, which helps to capture informative global contexts and enhances compression. On compression benchmark datasets, MambaVC achieves superior rate-distortion performance with lower computational and memory overheads. Specifically, it outperforms CNN and Transformer variants by 9.3% and 15.6% on Kodak, respectively, while reducing computation by 42% and 24%, and saving 12% and 71% of memory. MambaVC shows even greater improvements with high-resolution images, highlighting its potential and scalability in real-world applications. We also provide a comprehensive comparison of different network designs, underscoring MambaVC's advantages.","sentences":["Learned visual compression is an important and active task in multimedia.","Existing approaches have explored various CNN- and Transformer-based designs to model content distribution and eliminate redundancy, where balancing efficacy (i.e., rate-distortion trade-off) and efficiency remains a challenge.","Recently, state-space models (SSMs) have shown promise due to their long-range modeling capacity and efficiency.","Inspired by this, we take the first step to explore SSMs for visual compression.","We introduce MambaVC, a simple, strong and efficient compression network based on SSM.","MambaVC develops a visual state space (VSS) block with a 2D selective scanning (2DSS) module as the nonlinear activation function after each downsampling, which helps to capture informative global contexts and enhances compression.","On compression benchmark datasets, MambaVC achieves superior rate-distortion performance with lower computational and memory overheads.","Specifically, it outperforms CNN and Transformer variants by 9.3% and 15.6% on Kodak, respectively, while reducing computation by 42% and 24%, and saving 12% and 71% of memory.","MambaVC shows even greater improvements with high-resolution images, highlighting its potential and scalability in real-world applications.","We also provide a comprehensive comparison of different network designs, underscoring MambaVC's advantages."],"url":"http://arxiv.org/abs/2405.15413v1","category":"eess.IV"}
{"created":"2024-05-24 09:48:50","title":"Leveraging knowledge distillation for partial multi-task learning from multiple remote sensing datasets","abstract":"Partial multi-task learning where training examples are annotated for one of the target tasks is a promising idea in remote sensing as it allows combining datasets annotated for different tasks and predicting more tasks with fewer network parameters. The na\\\"ive approach to partial multi-task learning is sub-optimal due to the lack of all-task annotations for learning joint representations. This paper proposes using knowledge distillation to replace the need of ground truths for the alternate task and enhance the performance of such approach. Experiments conducted on the public ISPRS 2D Semantic Labeling Contest dataset show the effectiveness of the proposed idea on partial multi-task learning for semantic tasks including object detection and semantic segmentation in aerial images.","sentences":["Partial multi-task learning where training examples are annotated for one of the target tasks is a promising idea in remote sensing as it allows combining datasets annotated for different tasks and predicting more tasks with fewer network parameters.","The na\\\"ive approach to partial multi-task learning is sub-optimal due to the lack of all-task annotations for learning joint representations.","This paper proposes using knowledge distillation to replace the need of ground truths for the alternate task and enhance the performance of such approach.","Experiments conducted on the public ISPRS 2D Semantic Labeling Contest dataset show the effectiveness of the proposed idea on partial multi-task learning for semantic tasks including object detection and semantic segmentation in aerial images."],"url":"http://arxiv.org/abs/2405.15394v1","category":"cs.CV"}
{"created":"2024-05-24 08:54:36","title":"Pipeline Parallelism with Controllable Memory","abstract":"Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block and we show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our proposed methods demonstrate a 16% throughput improvement over the 1F1B baseline for large language models.","sentences":["Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology.","In this paper, we propose a framework to decompose pipeline schedules as repeating a building block and we show that the lifespan of the building block decides the peak activation memory of the pipeline schedule.","Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient.","To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput.","We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in terms of throughput.","When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our proposed methods demonstrate a 16% throughput improvement over the 1F1B baseline for large language models."],"url":"http://arxiv.org/abs/2405.15362v1","category":"cs.LG"}
{"created":"2024-05-24 16:46:02","title":"A Variable Neighborhood Search approach for solving the Rank Pricing Problem","abstract":"The Rank Pricing Problem (RPP) is a challenging bilevel optimization problem with binary variables whose objective is to determine the optimal pricing strategy for a set of products to maximize the total benefit, given that customer preferences influence the price for each product. Traditional methods for solving RPP are based on exact approaches which may be computationally expensive. In contrast, this paper presents a novel approach utilizing Variable Neighborhood Search (VNS), a popular heuristic known for its effectiveness in solving combinatorial optimization problems. Our proposed VNS algorithm introduces problem-specific neighborhood operators designed to effectively explore the solution space of the RPP. Even though our methodology does not have optimality guarantees, our computational experiments show that it outperforms Mixed Integer Program solvers regarding solution quality and computational burden.","sentences":["The Rank Pricing Problem (RPP) is a challenging bilevel optimization problem with binary variables whose objective is to determine the optimal pricing strategy for a set of products to maximize the total benefit, given that customer preferences influence the price for each product.","Traditional methods for solving RPP are based on exact approaches which may be computationally expensive.","In contrast, this paper presents a novel approach utilizing Variable Neighborhood Search (VNS), a popular heuristic known for its effectiveness in solving combinatorial optimization problems.","Our proposed VNS algorithm introduces problem-specific neighborhood operators designed to effectively explore the solution space of the RPP.","Even though our methodology does not have optimality guarantees, our computational experiments show that it outperforms Mixed Integer Program solvers regarding solution quality and computational burden."],"url":"http://arxiv.org/abs/2405.15702v1","category":"math.OC"}
{"created":"2024-05-24 14:47:59","title":"Autonomous programmable microscopic electronic lablets optimized with digital control","abstract":"Lablets are autonomous microscopic particles with programmable CMOS electronics that can control electrokinetic phenomena and electrochemical reactions in solution via actuator and sensor microelectrodes. In this paper, we describe the design and fabrication of optimized singulated lablets (CMOS3) with dimensions 140x140x50 um carrying an integrated coplanar encapsulated supercapacitor as a rechargeable power supply. The lablets are designed to allow docking to one another or to a smart surface for interchange of energy, electronic information, and chemicals. The paper focusses on the digital and analog design of the lablets to allow significant programmable functionality in a microscopic footprint, including the control of autonomous actuation and sensing up to the level of being able to support a complete lablet self-reproduction life cycle, although experimentally this remains to be proven. The potential of lablets in autonomous sensing and control and for evolutionary experimentation are discussed.","sentences":["Lablets are autonomous microscopic particles with programmable CMOS electronics that can control electrokinetic phenomena and electrochemical reactions in solution via actuator and sensor microelectrodes.","In this paper, we describe the design and fabrication of optimized singulated lablets (CMOS3) with dimensions 140x140x50 um carrying an integrated coplanar encapsulated supercapacitor as a rechargeable power supply.","The lablets are designed to allow docking to one another or to a smart surface for interchange of energy, electronic information, and chemicals.","The paper focusses on the digital and analog design of the lablets to allow significant programmable functionality in a microscopic footprint, including the control of autonomous actuation and sensing up to the level of being able to support a complete lablet self-reproduction life cycle, although experimentally this remains to be proven.","The potential of lablets in autonomous sensing and control and for evolutionary experimentation are discussed."],"url":"http://arxiv.org/abs/2405.15608v1","category":"cs.RO"}
{"created":"2024-05-24 10:01:46","title":"On relationships between vector variational inequalities and optimization problems using convexificators on Hadamard manifold","abstract":"An important concept of convexificators has been extended to Hadamard manifolds in this paper. The mean value theorem for convexificators on the Hadamard manifold has also been derived. Monotonicity of the bounded convexificators has been discussed and an important characterization for the bounded convexificators to be $\\partial_{*}^{*}$-geodesic convexity has been derived. Furthermore, a vector variational inequalities problem using convexificators on Hadamard manifold has been considered. In addition, the necessary and sufficient conditions for vector optimization problems in terms of Stampacchia and Minty type partial vector variational inequality problem ($\\partial_{*}^{*}$-VVIP) have been derived.","sentences":["An important concept of convexificators has been extended to Hadamard manifolds in this paper.","The mean value theorem for convexificators on the Hadamard manifold has also been derived.","Monotonicity of the bounded convexificators has been discussed and an important characterization for the bounded convexificators to be $\\partial_{*}^{*}$-geodesic convexity has been derived.","Furthermore, a vector variational inequalities problem using convexificators on Hadamard manifold has been considered.","In addition, the necessary and sufficient conditions for vector optimization problems in terms of Stampacchia and Minty type partial vector variational inequality problem ($\\partial_{*}^{*}$-VVIP) have been derived."],"url":"http://arxiv.org/abs/2405.15402v1","category":"math.OC"}
{"created":"2024-05-24 09:07:05","title":"Engineering Optimal Parallel Task Scheduling","abstract":"The NP-hard scheduling problem P||C_max encompasses a set of tasks with known execution time which must be mapped to a set of identical machines such that the overall completion time is minimized. In this work, we improve existing techniques for optimal P||C_max scheduling with a combination of new theoretical insights and careful practical engineering. Most importantly, we derive techniques to prune vast portions of the search space of branch-and-bound (BnB) approaches. We also propose improved upper and lower bounding techniques which can be combined with any approach to P||C_max. Moreover, we present new benchmarks for P||C_max, based on diverse application data, which can shed light on aspects which prior synthetic instances fail to capture. In an extensive evaluation, we observe that our pruning techniques reduce the number of explored nodes by 90$\\times$ and running times by 12$\\times$. Compared to a state-of-the-art ILP-based approach, our approach is preferable for short running time limits and for instances with large makespans.","sentences":["The NP-hard scheduling problem P||C_max encompasses a set of tasks with known execution time which must be mapped to a set of identical machines such that the overall completion time is minimized.","In this work, we improve existing techniques for optimal P||C_max scheduling with a combination of new theoretical insights and careful practical engineering.","Most importantly, we derive techniques to prune vast portions of the search space of branch-and-bound (BnB) approaches.","We also propose improved upper and lower bounding techniques which can be combined with any approach to P||C_max.","Moreover, we present new benchmarks for P||C_max, based on diverse application data, which can shed light on aspects which prior synthetic instances fail to capture.","In an extensive evaluation, we observe that our pruning techniques reduce the number of explored nodes by 90$\\times$ and running times by 12$\\times$. Compared to a state-of-the-art ILP-based approach, our approach is preferable for short running time limits and for instances with large makespans."],"url":"http://arxiv.org/abs/2405.15371v1","category":"cs.DS"}
{"created":"2024-05-24 17:41:46","title":"Magnetic effects in the Hadron Resonance Gas","abstract":"We discuss the modeling of the hadronic phase of QCD at finite magnetic field in the framework of hadron resonance gas (HRG). We focus on the statistical description of particle yields that include contribution from resonance decays. We demonstrate that the swift increase in the number of protons with magnetic field predicted in the HRG is due to the ill-defined description of higher-spin states. We discuss fluctuations of conserved charges and show that at present the qualitative comparison of the model predictions with the Lattice QCD data should be treated with care. We also discuss the principle of detailed balance which allows to study the magnetic field dependence of neutral resonances.","sentences":["We discuss the modeling of the hadronic phase of QCD at finite magnetic field in the framework of hadron resonance gas (HRG).","We focus on the statistical description of particle yields that include contribution from resonance decays.","We demonstrate that the swift increase in the number of protons with magnetic field predicted in the HRG is due to the ill-defined description of higher-spin states.","We discuss fluctuations of conserved charges and show that at present the qualitative comparison of the model predictions with the Lattice QCD data should be treated with care.","We also discuss the principle of detailed balance which allows to study the magnetic field dependence of neutral resonances."],"url":"http://arxiv.org/abs/2405.15745v1","category":"hep-ph"}
{"created":"2024-05-24 17:09:28","title":"Dynamic Latent-Factor Model with High-Dimensional Asset Characteristics","abstract":"We develop novel estimation procedures with supporting econometric theory for a dynamic latent-factor model with high-dimensional asset characteristics, that is, the number of characteristics is on the order of the sample size. Utilizing the Double Selection Lasso estimator, our procedure employs regularization to eliminate characteristics with low signal-to-noise ratios yet maintains asymptotically valid inference for asset pricing tests. The crypto asset class is well-suited for applying this model given the limited number of tradable assets and years of data as well as the rich set of available asset characteristics. The empirical results present out-of-sample pricing abilities and risk-adjusted returns for our novel estimator as compared to benchmark methods. We provide an inference procedure for measuring the risk premium of an observable nontradable factor, and employ this to find that the inflation-mimicking portfolio in the crypto asset class has positive risk compensation.","sentences":["We develop novel estimation procedures with supporting econometric theory for a dynamic latent-factor model with high-dimensional asset characteristics, that is, the number of characteristics is on the order of the sample size.","Utilizing the Double Selection Lasso estimator, our procedure employs regularization to eliminate characteristics with low signal-to-noise ratios yet maintains asymptotically valid inference for asset pricing tests.","The crypto asset class is well-suited for applying this model given the limited number of tradable assets and years of data as well as the rich set of available asset characteristics.","The empirical results present out-of-sample pricing abilities and risk-adjusted returns for our novel estimator as compared to benchmark methods.","We provide an inference procedure for measuring the risk premium of an observable nontradable factor, and employ this to find that the inflation-mimicking portfolio in the crypto asset class has positive risk compensation."],"url":"http://arxiv.org/abs/2405.15721v1","category":"econ.EM"}
{"created":"2024-05-24 16:41:27","title":"First-principles studies of fermiology in topological phases of bulk ZrTe$_5$","abstract":"Topological insulators have been studied intensively over the last decades. Among these materials, three-dimensional (3D) zirconium pentatelluride (ZrTe$_5$) stands out as one of the most intriguing for both theoretical and experimental studies because of its diverse range of distinct topological phases. In this work, we employ density functional theory to study the electronic structure and quantum oscillations exhibited by various topological phases of 3D bulk ZrTe$_5$. We have discovered that by analyzing combined patterns in band structures, Fermi surfaces, and Shubnikov-de Haas (SdH) oscillations we can determine the corresponding topological phase without relying on the conventional calculation of topological invariants or boundary state contributions. This approach facilitates the identification of topological phases in ZrTe$_5$ directly from experimental quantum oscillation measurements. Using this method, we have analyzed the entire process of topological phase transition, revealing changes in the topology of the Fermi pockets and validating the shapes deduced from the experimental data for the topological phases.","sentences":["Topological insulators have been studied intensively over the last decades.","Among these materials, three-dimensional (3D) zirconium pentatelluride (ZrTe$_5$) stands out as one of the most intriguing for both theoretical and experimental studies because of its diverse range of distinct topological phases.","In this work, we employ density functional theory to study the electronic structure and quantum oscillations exhibited by various topological phases of 3D bulk ZrTe$_5$.","We have discovered that by analyzing combined patterns in band structures, Fermi surfaces, and Shubnikov-de Haas (SdH) oscillations we can determine the corresponding topological phase without relying on the conventional calculation of topological invariants or boundary state contributions.","This approach facilitates the identification of topological phases in ZrTe$_5$ directly from experimental quantum oscillation measurements.","Using this method, we have analyzed the entire process of topological phase transition, revealing changes in the topology of the Fermi pockets and validating the shapes deduced from the experimental data for the topological phases."],"url":"http://arxiv.org/abs/2405.15698v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-24 16:16:03","title":"Causal features in turbulent channel flow","abstract":"The causal relevance of local flow conditions in wall-bounded turbulence is analysed using ensembles of interventional experiments in which the effect of perturbing the flow within a small cell is monitored at some future time. When this is done using the relative amplification of the perturbation energy, causality depends on the flow conditions within the cell before it is perturbed, and can be used as a probe of the flow dynamics. The key scaling parameter is the ambient shear, which is also the dominant diagnostic variable for wall-attached perturbations. Away from the wall, the relevant variables are the streamwise and wall-normal velocities. Causally significant cells are associated with sweeps that carry the perturbation towards the stronger shear near the wall, whereas irrelevant ones are associated with ejections that carry it towards the weaker shear in the outer layers. Causally significant and irrelevant cells are themselves organised into structures that share many characteristics with classical sweeps and ejections, such as forming spanwise pairs whose dimensions and geometry are similar to those of classical quadrants. At the wall, this is consistent with causally significant configurations in which a high-speed streak overtakes a low-speed one, and causally irrelevant ones in which the two streaks pull apart from each other. It is argued that this is probably associated with streak meandering.","sentences":["The causal relevance of local flow conditions in wall-bounded turbulence is analysed using ensembles of interventional experiments in which the effect of perturbing the flow within a small cell is monitored at some future time.","When this is done using the relative amplification of the perturbation energy, causality depends on the flow conditions within the cell before it is perturbed, and can be used as a probe of the flow dynamics.","The key scaling parameter is the ambient shear, which is also the dominant diagnostic variable for wall-attached perturbations.","Away from the wall, the relevant variables are the streamwise and wall-normal velocities.","Causally significant cells are associated with sweeps that carry the perturbation towards the stronger shear near the wall, whereas irrelevant ones are associated with ejections that carry it towards the weaker shear in the outer layers.","Causally significant and irrelevant cells are themselves organised into structures that share many characteristics with classical sweeps and ejections, such as forming spanwise pairs whose dimensions and geometry are similar to those of classical quadrants.","At the wall, this is consistent with causally significant configurations in which a high-speed streak overtakes a low-speed one, and causally irrelevant ones in which the two streaks pull apart from each other.","It is argued that this is probably associated with streak meandering."],"url":"http://arxiv.org/abs/2405.15674v1","category":"physics.flu-dyn"}
{"created":"2024-05-24 15:52:55","title":"Multiple Emission Regions in Jets of Low Luminosity Active Galactic Nucleus in NGC 4278","abstract":"The Large High Altitude Airshower Array (LHAASO) has detected very high energy gamma rays from the LINER galaxy NGC 4278, which has a low luminosity active galactic nucleus, and symmetric mildly relativistic S-shaped twin jets detected by radio observations. Few low-luminosity active galactic nuclei are detected in gamma rays due to their faintness. Earlier, several radio-emitting components were detected in the jets of NGC 4278. We model their radio emission with synchrotron emission of ultra-relativistic electrons to estimate the strength of the magnetic field inside these components within a time-dependent framework after including the ages of the different components. We show that the synchrotron and synchrotron self-Compton emission by these components cannot explain the Swift X-ray data detected from NGC 4278 and the LHAASO gamma-ray data associated with NGC 4278. We suggest that a separate component in one of the jets is responsible for the high energy emission whose age, size, magnetic field and the spectrum of the ultra-relativistic electrons inside it have been estimated after fitting the multi-wavelength data of NGC 4278 with the sum of the spectral energy distributions from the radio components and the high energy component.","sentences":["The Large High Altitude Airshower Array (LHAASO) has detected very high energy gamma rays from the LINER galaxy NGC 4278, which has a low luminosity active galactic nucleus, and symmetric mildly relativistic S-shaped twin jets detected by radio observations.","Few low-luminosity active galactic nuclei are detected in gamma rays due to their faintness.","Earlier, several radio-emitting components were detected in the jets of NGC 4278.","We model their radio emission with synchrotron emission of ultra-relativistic electrons to estimate the strength of the magnetic field inside these components within a time-dependent framework after including the ages of the different components.","We show that the synchrotron and synchrotron self-Compton emission by these components cannot explain the Swift X-ray data detected from NGC 4278 and the LHAASO gamma-ray data associated with NGC 4278.","We suggest that a separate component in one of the jets is responsible for the high energy emission whose age, size, magnetic field and the spectrum of the ultra-relativistic electrons inside it have been estimated after fitting the multi-wavelength data of NGC 4278 with the sum of the spectral energy distributions from the radio components and the high energy component."],"url":"http://arxiv.org/abs/2405.15657v1","category":"astro-ph.HE"}
{"created":"2024-05-24 15:16:50","title":"HI Galaxy Signatures in the SARAO MeerKAT Galactic Plane Survey -- III. Unveiling the obscured part of the Vela Supercluster","abstract":"We conducted a search for HI emission of the gas-rich galaxies in the Vela region ($260^{\\circ} \\leq \\ell \\leq 290^{\\circ}, -2^{\\circ} \\leq b \\leq 1^{\\circ}$) to explore the Vela Supercluster (VSCL) at $V_\\mathrm{hel} \\sim 18000$ km s$^{-1}$, largely obscured by Galactic dust. Within the mostly RFI-free band ($250 < V_\\mathrm{hel} < 25000$ km s$^{-1}$) of MeerKAT, the analysis focuses on $157$ hexagonally distributed pointings extracted from the SARAO MeerKAT Galactic Plane Survey located in the Vela region (Vela$-$SMGPS). These were combined into 10 contiguous mosaics, covering a ${\\sim}90$ deg$^2$ area. Among the $843$ HI detected sources, 39 were previously discovered in the Parkes HIZOA survey ($V_\\mathrm{hel} < 12000$ km s$^{-1}$; rms $\\sim 6$ mJy beam$^{-1}$). With the improved rms level of the Vela$-$SMGPS, i.e., $0.29 - 0.56$ mJy beam$^{-1}$, our study unveils nearly 12 times more detections (471 candidates) in that same velocity range. We furthermore could identify $187$ galaxy candidates with an HI mass limit reaching $\\log (M_{\\rm HI}/\\rm M_{\\odot}) = 9.44$ in the VSCL velocity range $V_\\mathrm{hel} \\sim 19500 \\pm 3500$ km s$^{-1}$. We find indications of two wall-like overdensities that confirm the original suspicion that these walls intersect at low latitudes around longitudes of $\\ell \\sim 272^{\\circ} - 278^{\\circ}$. We also find a strong signature most likely associated with the Hydra/Antlia extension and evidence of a previously unknown narrow filament at $V_\\mathrm{hel} \\sim 12000$ km s$^{-1}$. This paper demonstrates the efficiency of systematic HI surveys with the SKA precursor MeerKAT, even in the most obscured part of the Zone of Avoidance (ZOA).","sentences":["We conducted a search for HI emission of the gas-rich galaxies in the Vela region ($260^{\\circ} \\leq \\ell \\leq 290^{\\circ}, -2^{\\circ} \\leq b \\leq 1^{\\circ}$) to explore the Vela Supercluster (VSCL) at $V_\\mathrm{hel} \\sim 18000$ km s$^{-1}$, largely obscured by Galactic dust.","Within the mostly RFI-free band ($250 < V_\\mathrm{hel} < 25000$ km s$^{-1}$) of MeerKAT, the analysis focuses on $157$ hexagonally distributed pointings extracted from the SARAO MeerKAT Galactic Plane Survey located in the Vela region (Vela$-$SMGPS).","These were combined into 10 contiguous mosaics, covering a ${\\sim}90$ deg$^2$ area.","Among the $843$ HI detected sources, 39 were previously discovered in the Parkes HIZOA survey ($V_\\mathrm{hel} < 12000$ km s$^{-1}$; rms $\\sim 6$ mJy beam$^{-1}$).","With the improved rms level of the Vela$-$SMGPS, i.e., $0.29 - 0.56$ mJy beam$^{-1}$, our study unveils nearly 12 times more detections (471 candidates) in that same velocity range.","We furthermore could identify $187$ galaxy candidates with an HI mass limit reaching $\\log (M_{\\rm HI}/\\rm M_{\\odot}) = 9.44$ in the VSCL velocity range $V_\\mathrm{hel} \\sim 19500","\\pm 3500$ km s$^{-1}$. We find indications of two wall-like overdensities that confirm the original suspicion that these walls intersect at low latitudes around longitudes of $\\ell \\sim 272^{\\circ} - 278^{\\circ}$.","We also find a strong signature most likely associated with the Hydra/Antlia extension and evidence of a previously unknown narrow filament at $V_\\mathrm{hel} \\sim 12000$ km s$^{-1}$.","This paper demonstrates the efficiency of systematic HI surveys with the SKA precursor MeerKAT, even in the most obscured part of the Zone of Avoidance (ZOA)."],"url":"http://arxiv.org/abs/2405.15629v1","category":"astro-ph.GA"}
{"created":"2024-05-24 15:02:09","title":"Sensitivity of K$\u03b2$ mainline X-ray emission to structural dynamics in iron photosensitizer","abstract":"Photochemistry and photophysics processes involve structures far from equilibrium. In these reactions, there is often strong coupling between nuclear and electronic degrees of freedom. For first-row transition metals, K$\\beta$ X-ray emission spectroscopy (XES) is a sensitive probe of electronic structure due to the direct overlap between the valence orbitals and the 3p hole in the final state. Here the sensitivity of K$\\beta$ mainline (K$\\beta$1,3) XES to structural dynamics is analyzed by simulating spectral changes along the excited state dynamics of an iron photosensitizer [FeII(bmip)2]2+ [bmip = 2,6-bis(3-methyl-imidazole-1-ylidine)-pyridine], using both restricted active space (RAS) multiconfigurational wavefunction theory and a one-electron orbital-energy approach in density-functional theory (1-DFT). Both methods predict a spectral blue-shift with increasing metal-ligand distance, which changes the emission intensity for any given detection energy. These results support the suggestion that the [FeII(bmip)2]2+ femtosecond K$\\beta$ XES signal shows oscillations due to coherent wavepacket dynamics. Based on the RAS results, the sensitivity to structural dynamics is twice as high for K$\\beta$ compared to K$\\alpha$, with the drawback of a lower signal-to-noise ratio. K$\\beta$ sensitivity is favored by a larger spectral blue-shift with increasing metal-ligand distance and larger changes in spectral shape. Comparing the two simulations methods, 1-DFT predicts smaller energy shifts and lower sensitivity, likely due to missing final-state effects. The simulations can be used to design and interpret XES probes of non-equilibrium structures to gain mechanistic insights in photocatalysis.","sentences":["Photochemistry and photophysics processes involve structures far from equilibrium.","In these reactions, there is often strong coupling between nuclear and electronic degrees of freedom.","For first-row transition metals, K$\\beta$ X-ray emission spectroscopy (XES) is a sensitive probe of electronic structure due to the direct overlap between the valence orbitals and the 3p hole in the final state.","Here the sensitivity of K$\\beta$ mainline (K$\\beta$1,3) XES to structural dynamics is analyzed by simulating spectral changes along the excited state dynamics of an iron photosensitizer [FeII(bmip)2]2+ [bmip = 2,6-bis(3-methyl-imidazole-1-ylidine)-pyridine], using both restricted active space (RAS) multiconfigurational wavefunction theory and a one-electron orbital-energy approach in density-functional theory (1-DFT).","Both methods predict a spectral blue-shift with increasing metal-ligand distance, which changes the emission intensity for any given detection energy.","These results support the suggestion that the [FeII(bmip)2]2+ femtosecond K$\\beta$ XES signal shows oscillations due to coherent wavepacket dynamics.","Based on the RAS results, the sensitivity to structural dynamics is twice as high for K$\\beta$ compared to K$\\alpha$, with the drawback of a lower signal-to-noise ratio.","K$\\beta$ sensitivity is favored by a larger spectral blue-shift with increasing metal-ligand distance and larger changes in spectral shape.","Comparing the two simulations methods, 1-DFT predicts smaller energy shifts and lower sensitivity, likely due to missing final-state effects.","The simulations can be used to design and interpret XES probes of non-equilibrium structures to gain mechanistic insights in photocatalysis."],"url":"http://arxiv.org/abs/2405.15615v1","category":"physics.chem-ph"}
{"created":"2024-05-24 14:21:31","title":"Constraints for electron-capture decays mimicking production of axion-like particles in nuclei","abstract":"We give for the first time, theoretical estimates of ground-state-to-ground-state (GS-to-GS) electron-capture (EC) branch decay rates of $^{44}$Ti, $^{57}$Co, and $^{139}$Ce. The nuclear-structure calculations have been done exploiting the nuclear shell model (NSM) with well-established Hamiltonians and an advanced theory of $\\beta$ decay. In the absence of experimental measurements of these GS-to-GS branches, these estimates are of utmost importance for terrestrial searches of axion-like particles (ALPs). Predictions are made for EC-decay rates of 2$^{nd}$-forbidden unique (FU) and 2$^{nd}$-forbidden non-unique (FNU) EC transitions that can potentially mimic nuclear axion production in experiments designed to detect ALPs in nuclear environments.","sentences":["We give for the first time, theoretical estimates of ground-state-to-ground-state (GS-to-GS) electron-capture (EC) branch decay rates of $^{44}$Ti, $^{57}$Co, and $^{139}$Ce.","The nuclear-structure calculations have been done exploiting the nuclear shell model (NSM) with well-established Hamiltonians and an advanced theory of $\\beta$ decay.","In the absence of experimental measurements of these GS-to-GS branches, these estimates are of utmost importance for terrestrial searches of axion-like particles (ALPs).","Predictions are made for EC-decay rates of 2$^{nd}$-forbidden unique (FU) and 2$^{nd}$-forbidden non-unique (FNU) EC transitions that can potentially mimic nuclear axion production in experiments designed to detect ALPs in nuclear environments."],"url":"http://arxiv.org/abs/2405.15591v1","category":"nucl-th"}
{"created":"2024-05-24 14:12:10","title":"The ABCDEFGJK of maximally strongly coupled $\\mathcal{N}=2$ SCFTs","abstract":"We classify all possible charge lattices and 1-form symmetry groups for $\\mathcal{N}=2$ SCFTs with characteristic dimension $\\varkappa \\neq \\{1,2\\}$. For rank-$r$ SCFTs that are not stacks of lower rank theories the order of the 1-form symmetry group can be 1,2,3,4 and $r+1$. As an application of the classification we show that $\\mathcal{N}=2$ $S$-folds and Minahan-Nemeschansky SCFTs have trivial 1-form symmetry. We find two sporadic lattices compatible with the Coulomb branch geometries $\\mathbb{C}^5/G_{33}$ and $\\mathbb{C}^6/G_{34}$ that are not realized by any known SCFT. The former, if realized, would have a $\\mathbb{Z}_2$ 1-form symmetry and a non-invertible topological defect.","sentences":["We classify all possible charge lattices and 1-form symmetry groups for $\\mathcal{N}=2$ SCFTs with characteristic dimension $\\varkappa \\neq \\{1,2\\}$. For rank-$r$ SCFTs that are not stacks of lower rank theories the order of the 1-form symmetry group can be 1,2,3,4 and $r+1$. As an application of the classification we show that $\\mathcal{N}=2$ $S$-folds and Minahan-Nemeschansky SCFTs have trivial 1-form symmetry.","We find two sporadic lattices compatible with the Coulomb branch geometries $\\mathbb{C}^5/G_{33}$ and $\\mathbb{C}^6/G_{34}$ that are not realized by any known SCFT.","The former, if realized, would have a $\\mathbb{Z}_2$ 1-form symmetry and a non-invertible topological defect."],"url":"http://arxiv.org/abs/2405.15582v1","category":"hep-th"}
{"created":"2024-05-24 13:02:31","title":"Confocal structured illumination microscopy","abstract":"Confocal microscopy, a critical advancement in optical imaging, is widely applied because of its excellent anti-noise ability. However, it has low imaging efficiency and can cause phototoxicity. Optical-sectioning structured illumination microscopy (OS-SIM) can overcome the limitations of confocal microscopy but still face challenges in imaging depth and signal-to-noise ratio (SNR). We introduce the concept of confocal imaging into OS-SIM and propose confocal structured illumination microscopy (CSIM) to enhance the imaging performance of OS-SIM. CSIM exploits the principle of dual photography to reconstruct a dual image from each pixel of the camera. The reconstructed dual image is equivalent to the image obtained by using the spatial light modulator (SLM) as a virtual camera, enabling the separation of the conjugate and non-conjugate signals recorded by the camera pixel. We can reject the non-conjugate signals by extracting the conjugate signal from each dual image to reconstruct a confocal image when establishing the conjugate relationship between the camera and the SLM. We have constructed the theoretical framework of CSIM. Optical-sectioning experimental results demonstrate that CSIM can reconstruct images with superior SNR and greater imaging depth compared with existing OS-SIM. CSIM is expected to expand the application scope of OS-SIM.","sentences":["Confocal microscopy, a critical advancement in optical imaging, is widely applied because of its excellent anti-noise ability.","However, it has low imaging efficiency and can cause phototoxicity.","Optical-sectioning structured illumination microscopy (OS-SIM) can overcome the limitations of confocal microscopy but still face challenges in imaging depth and signal-to-noise ratio (SNR).","We introduce the concept of confocal imaging into OS-SIM and propose confocal structured illumination microscopy (CSIM) to enhance the imaging performance of OS-SIM.","CSIM exploits the principle of dual photography to reconstruct a dual image from each pixel of the camera.","The reconstructed dual image is equivalent to the image obtained by using the spatial light modulator (SLM) as a virtual camera, enabling the separation of the conjugate and non-conjugate signals recorded by the camera pixel.","We can reject the non-conjugate signals by extracting the conjugate signal from each dual image to reconstruct a confocal image when establishing the conjugate relationship between the camera and the SLM.","We have constructed the theoretical framework of CSIM.","Optical-sectioning experimental results demonstrate that CSIM can reconstruct images with superior SNR and greater imaging depth compared with existing OS-SIM.","CSIM is expected to expand the application scope of OS-SIM."],"url":"http://arxiv.org/abs/2405.15519v1","category":"physics.optics"}
{"created":"2024-05-24 11:13:53","title":"Investigation of spectral properties of ^{11}Be in breakup reactions","abstract":"We investigate the breakup of the ^{11}Be halo nuclei on a light target ^{12}C within quantum-quasiclassical approach in a wide range of the beam energy (5-67 MeV/nucleon) including bound states and low-lying resonances in different partial and spin states of ^{11}Be. The obtained results are in good agreement with existing experimental data at 67 MeV/nucleon. We also demonstrate that the developed computational scheme can be used for investigation of nuclei spectral properties in low-energy breakup experiments on different targets.","sentences":["We investigate the breakup of the ^{11}Be halo nuclei on a light target ^{12}C within quantum-quasiclassical approach in a wide range of the beam energy (5-67 MeV/nucleon) including bound states and low-lying resonances in different partial and spin states of ^{11}Be.","The obtained results are in good agreement with existing experimental data at 67 MeV/nucleon.","We also demonstrate that the developed computational scheme can be used for investigation of nuclei spectral properties in low-energy breakup experiments on different targets."],"url":"http://arxiv.org/abs/2405.15440v1","category":"nucl-th"}
{"created":"2024-05-24 11:05:05","title":"Searching For Gamma-ray Emission from Stellar Flares","abstract":"Flares from magnetically active dwarf stars should produce relativistic particles capable of creating gamma-rays. So far, the only isolated main sequence star besides the Sun to have been detected in gamma-rays is TVLM 513-46546. Detecting gamma-ray flares from more dwarf stars can improve our understanding of their magnetospheric properties, and could also indicate a diminished likelihood of their planets' habitability. In this work, we stack data from the Fermi Gamma-ray Space Telescope during a large number of events identified from optical and X-ray flare surveys. We report an upper limit of gamma-ray emission from the population of flare stars. Stacking results towards control positions are consistent with a non-detection. We compare these results to observed Solar gamma-ray flares and against a model of emission from neutral pion decay. The upper limit is consistent with Solar flares when scaled to the flare energies and distances of the target stars. As with Solar flares, the neutral pion decay mechanism for gamma-ray production is also consistent with these results.","sentences":["Flares from magnetically active dwarf stars should produce relativistic particles capable of creating gamma-rays.","So far, the only isolated main sequence star besides the Sun to have been detected in gamma-rays is TVLM 513-46546.","Detecting gamma-ray flares from more dwarf stars can improve our understanding of their magnetospheric properties, and could also indicate a diminished likelihood of their planets' habitability.","In this work, we stack data from the Fermi Gamma-ray Space Telescope during a large number of events identified from optical and X-ray flare surveys.","We report an upper limit of gamma-ray emission from the population of flare stars.","Stacking results towards control positions are consistent with a non-detection.","We compare these results to observed Solar gamma-ray flares and against a model of emission from neutral pion decay.","The upper limit is consistent with Solar flares when scaled to the flare energies and distances of the target stars.","As with Solar flares, the neutral pion decay mechanism for gamma-ray production is also consistent with these results."],"url":"http://arxiv.org/abs/2405.15435v1","category":"astro-ph.HE"}
{"created":"2024-05-24 10:27:04","title":"Planar Cycle-Extendable Graphs","abstract":"For most problems pertaining to perfect matchings, one may restrict attention to matching covered graphs -- that is, connected nontrivial graphs with the property that each edge belongs to some perfect matching. There is extensive literature on these graphs that are also known as $1$-extendable graphs (since each edge extends to a perfect matching) including an ear decomposition theorem due to Lovasz and Plummer.   A cycle $C$ of a graph $G$ is conformal if $G-V(C)$ has a perfect matching; such cycles play an important role in the study of perfect matchings, especially when investigating the Pfaffian orientation problem. A matching covered graph $G$ is cycle-extendable if -- for each even cycle $C$ -- the cycle $C$ is conformal, or equivalently, each perfect matching of $C$ extends to a perfect matching of $G$, or equivalently, $C$ is the symmetric difference of two perfect matchings of $G$, or equivalently, $C$ extends to an ear decomposition of $G$. In the literature, these are also known as cycle-nice or as $1$-cycle resonant graphs.   Zhang, Wang, Yuan, Ng and Cheng [Discrete Mathematics, 345:7 (2022), 112876] provided a characterization of claw-free cycle-extendable graphs. Guo and Zhang [Discrete Mathematics, 275:1-3 (2004), 151-164] and independently Zhang and Li [Discrete Applied Mathematics, 160:13-14 (2012), 2069-2074], provided characterizations of bipartite planar cycle-extendable graphs. In this paper, we establish a characterization of all planar cycle-extendable graphs -- in terms of $K_2$ and four infinite families.","sentences":["For most problems pertaining to perfect matchings, one may restrict attention to matching covered graphs -- that is, connected nontrivial graphs with the property that each edge belongs to some perfect matching.","There is extensive literature on these graphs that are also known as $1$-extendable graphs (since each edge extends to a perfect matching) including an ear decomposition theorem due to Lovasz and Plummer.   ","A cycle $C$ of a graph $G$ is conformal if $G-V(C)$ has a perfect matching; such cycles play an important role in the study of perfect matchings, especially when investigating the Pfaffian orientation problem.","A matching covered graph $G$ is cycle-extendable if -- for each even cycle $C$ -- the cycle $C$ is conformal, or equivalently, each perfect matching of $C$ extends to a perfect matching of $G$, or equivalently, $C$ is the symmetric difference of two perfect matchings of $G$, or equivalently, $C$ extends to an ear decomposition of $G$. In the literature, these are also known as cycle-nice or as $1$-cycle resonant graphs.   ","Zhang, Wang, Yuan, Ng and Cheng","[Discrete Mathematics, 345:7 (2022), 112876] provided a characterization of claw-free cycle-extendable graphs.","Guo and Zhang","[Discrete Mathematics, 275:1-3 (2004), 151-164] and independently Zhang and Li","[Discrete Applied Mathematics, 160:13-14 (2012), 2069-2074], provided characterizations of bipartite planar cycle-extendable graphs.","In this paper, we establish a characterization of all planar cycle-extendable graphs -- in terms of $K_2$ and four infinite families."],"url":"http://arxiv.org/abs/2405.15416v1","category":"math.CO"}
{"created":"2024-05-24 09:42:12","title":"Influence of plasma instabilities on the propagation of electromagnetic cascades from distant blazars","abstract":"The propagation of very-high-energy gamma-rays (VHEGRs) in the extragalactic space offers the opportunity to study astrophysical phenomena not reproducible in laboratories. In particular, the deviation from predictions of the observed photon flux from distant sources at the GeV energy scale still represents an open problem. Commonly, this deviation is interpreted as the result of the deflection out of the line of sight of the source of the cascade-produced charged leptons by weak magnetic fields present in the intergalactic medium (IGM). However, plasma instabilities could have an effect on the energy- and momentum-distribution of the secondary electrons and positrons, modifying the development of electromagnetic cascades. In this work, we study the influence of plasma instabilities on the energy spectrum of electromagnetic cascades through a parametric study, performed with the Monte Carlo code CRPropa. We parameterize plasma instabilities with an energy loss length normalisation $\\lambda_0$ and a power law index $\\alpha$ of its electron/positron energy dependence. We simulate photon spectra at Earth for different blazar scenarios and find that plasma instabilities can reproduce the suppression in the GeV-photon flux of real astrophysical sources, such as 1ES 0229+200, when the energy loss length of electrons/positrons at 1.0 TeV is $\\lambda_0 \\simeq 100\\,\\text{kpc}$ and $\\alpha\\simeq0$. The energy fraction lost by the secondary electron pairs due to the instability is estimated to be about $1\\%$ over the typical interaction length of Inverse Compton scattering for these parameter values.","sentences":["The propagation of very-high-energy gamma-rays (VHEGRs) in the extragalactic space offers the opportunity to study astrophysical phenomena not reproducible in laboratories.","In particular, the deviation from predictions of the observed photon flux from distant sources at the GeV energy scale still represents an open problem.","Commonly, this deviation is interpreted as the result of the deflection out of the line of sight of the source of the cascade-produced charged leptons by weak magnetic fields present in the intergalactic medium (IGM).","However, plasma instabilities could have an effect on the energy- and momentum-distribution of the secondary electrons and positrons, modifying the development of electromagnetic cascades.","In this work, we study the influence of plasma instabilities on the energy spectrum of electromagnetic cascades through a parametric study, performed with the Monte Carlo code CRPropa.","We parameterize plasma instabilities with an energy loss length normalisation $\\lambda_0$ and a power law index $\\alpha$ of its electron/positron energy dependence.","We simulate photon spectra at Earth for different blazar scenarios and find that plasma instabilities can reproduce the suppression in the GeV-photon flux of real astrophysical sources, such as 1ES 0229+200, when the energy loss length of electrons/positrons at 1.0 TeV is $\\lambda_0 \\simeq 100\\,\\text{kpc}$ and $\\alpha\\simeq0$. The energy fraction lost by the secondary electron pairs due to the instability is estimated to be about $1\\%$ over the typical interaction length of Inverse Compton scattering for these parameter values."],"url":"http://arxiv.org/abs/2405.15390v1","category":"astro-ph.HE"}
{"created":"2024-05-24 08:43:04","title":"Unraveling the dust activity of naked-eye comet C/2022 E3 (ZTF)","abstract":"A morphological and photometric analysis of the naked-eye long-period comet C/2022 E3 (ZTF) before perihelion is presented in this study. The observation images taken by the Zwicky Transient Facility survey telescope from July 2022 to October 2022 show a gradually brightening dust coma and a tail with a clear structure. The morphology of the dust coma reveals nonsteady-state emission with an ejection velocity lower than 14 m s$^{-1}$ for particles larger than 100 um. According to the syndyne-synchrone analysis, dust particles larger than about 10 um contribute significantly to the observed tail. The model simulations of the 10 October 2022 image suggest that the radii of large particles lingering near the nucleus range from 0.1 mm to 1 mm. Assuming that the nucleus of comet E3 is a homogeneous sphere with an albedo of 0.1, the photometry analysis sets the lower and upper limits of the nucleus radius to be $0.81\\pm0.07$ km and $2.79\\pm0.01$ km, respectively. The dust production rates increased continuously from $241\\pm3$ kg s$^{-1}$ in July to $476\\pm9$ kg s$^{-1}$ in October. The dependence of the ejection velocity $v_{\\perp}$ perpendicular to the orbital plane of comet E3 on the particle size $a$ can be simplified as $v_{\\perp}\\propto a^{-1/2}$, which indicates that the dust emission is likely driven by gas. The water-production rate is inferred as $\\sim 368\\pm72$ kg s$^{-1}$ in October 2022, which is sustained by an equilibrium-sublimating area of $8.2\\times10^6$ m$^2$ at least. The comparative analysis of the characteristics of comet E3 with those of comets belonging to different types shows that the activity profile of long-period comet E3 surprisingly aligns more closely with those of short-period comets within a heliocentric distance range of about [1.7, 3.4] AU, where the images of comet E3 that we used in this study were taken.","sentences":["A morphological and photometric analysis of the naked-eye long-period comet C/2022 E3 (ZTF) before perihelion is presented in this study.","The observation images taken by the Zwicky Transient Facility survey telescope from July 2022 to October 2022 show a gradually brightening dust coma and a tail with a clear structure.","The morphology of the dust coma reveals nonsteady-state emission with an ejection velocity lower than 14 m s$^{-1}$ for particles larger than 100 um.","According to the syndyne-synchrone analysis, dust particles larger than about 10 um contribute significantly to the observed tail.","The model simulations of the 10 October 2022 image suggest that the radii of large particles lingering near the nucleus range from 0.1 mm to 1 mm.","Assuming that the nucleus of comet E3 is a homogeneous sphere with an albedo of 0.1, the photometry analysis sets the lower and upper limits of the nucleus radius to be $0.81\\pm0.07$ km and $2.79\\pm0.01$ km, respectively.","The dust production rates increased continuously from $241\\pm3$ kg s$^{-1}$ in July to $476\\pm9$ kg s$^{-1}$ in October.","The dependence of the ejection velocity $v_{\\perp}$ perpendicular to the orbital plane of comet E3 on the particle size $a$ can be simplified as $v_{\\perp}\\propto a^{-1/2}$, which indicates that the dust emission is likely driven by gas.","The water-production rate is inferred as $\\sim 368\\pm72$ kg s$^{-1}$ in October 2022, which is sustained by an equilibrium-sublimating area of $8.2\\times10^6$ m$^2$ at least.","The comparative analysis of the characteristics of comet E3 with those of comets belonging to different types shows that the activity profile of long-period comet E3 surprisingly aligns more closely with those of short-period comets within a heliocentric distance range of about [1.7, 3.4] AU, where the images of comet E3 that we used in this study were taken."],"url":"http://arxiv.org/abs/2405.15351v1","category":"astro-ph.EP"}
