{"created":"2024-05-01 17:59:20","title":"Self-Play Preference Optimization for Language Model Alignment","abstract":"Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \\textit{Self-Play Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.","sentences":["Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences.","Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment.","In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy.","Our approach, dubbed \\textit{Self-Play Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee.","Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO).","In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0.","It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard.","Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.)","from GPT-4 or other stronger language models."],"url":"http://arxiv.org/abs/2405.00675v1","category":"cs.LG"}
{"created":"2024-05-01 17:57:12","title":"Adapting Pretrained Networks for Image Quality Assessment on High Dynamic Range Displays","abstract":"Conventional image quality metrics (IQMs), such as PSNR and SSIM, are designed for perceptually uniform gamma-encoded pixel values and cannot be directly applied to perceptually non-uniform linear high-dynamic-range (HDR) colors. Similarly, most of the available datasets consist of standard-dynamic-range (SDR) images collected in standard and possibly uncontrolled viewing conditions. Popular pre-trained neural networks are likewise intended for SDR inputs, restricting their direct application to HDR content. On the other hand, training HDR models from scratch is challenging due to limited available HDR data. In this work, we explore more effective approaches for training deep learning-based models for image quality assessment (IQA) on HDR data. We leverage networks pre-trained on SDR data (source domain) and re-target these models to HDR (target domain) with additional fine-tuning and domain adaptation. We validate our methods on the available HDR IQA datasets, demonstrating that models trained with our combined recipe outperform previous baselines, converge much quicker, and reliably generalize to HDR inputs.","sentences":["Conventional image quality metrics (IQMs), such as PSNR and SSIM, are designed for perceptually uniform gamma-encoded pixel values and cannot be directly applied to perceptually non-uniform linear high-dynamic-range (HDR) colors.","Similarly, most of the available datasets consist of standard-dynamic-range (SDR) images collected in standard and possibly uncontrolled viewing conditions.","Popular pre-trained neural networks are likewise intended for SDR inputs, restricting their direct application to HDR content.","On the other hand, training HDR models from scratch is challenging due to limited available HDR data.","In this work, we explore more effective approaches for training deep learning-based models for image quality assessment (IQA) on HDR data.","We leverage networks pre-trained on SDR data (source domain) and re-target these models to HDR (target domain) with additional fine-tuning and domain adaptation.","We validate our methods on the available HDR IQA datasets, demonstrating that models trained with our combined recipe outperform previous baselines, converge much quicker, and reliably generalize to HDR inputs."],"url":"http://arxiv.org/abs/2405.00670v1","category":"cs.CV"}
{"created":"2024-05-01 17:50:37","title":"Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3","abstract":"This study presents a targeted model editing analysis focused on the latest large language model, Llama-3. We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions. We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing. Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits. With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing. This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance.","sentences":["This study presents a targeted model editing analysis focused on the latest large language model, Llama-3.","We explore the efficacy of popular model editing techniques - ROME, MEMIT, and EMMET, which are designed for precise layer interventions.","We identify the most effective layers for targeted edits through an evaluation that encompasses up to 4096 edits across three distinct strategies: sequential editing, batch editing, and a hybrid approach we call as sequential-batch editing.","Our findings indicate that increasing edit batch-sizes may degrade model performance more significantly than using smaller edit batches sequentially for equal number of edits.","With this, we argue that sequential model editing is an important component for scaling model editing methods and future research should focus on methods that combine both batched and sequential editing.","This observation suggests a potential limitation in current model editing methods which push towards bigger edit batch sizes, and we hope it paves way for future investigations into optimizing batch sizes and model editing performance."],"url":"http://arxiv.org/abs/2405.00664v1","category":"cs.CL"}
{"created":"2024-05-01 17:37:50","title":"RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization","abstract":"For long document summarization, discourse structure is important to discern the key content of the text and the differences in importance level between sentences. Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient fine-tuning strategies for long document summarization remains unexplored. Therefore, this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly incorporate RST into the LoRA model. Our empirical evaluation demonstrates that incorporating the type and uncertainty of rhetorical relations can complementarily enhance the performance of LoRA in summarization tasks. Furthermore, the best-performing variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning models, as confirmed by multiple automatic and human evaluations, and even surpasses previous state-of-the-art methods.","sentences":["For long document summarization, discourse structure is important to discern the key content of the text and the differences in importance level between sentences.","Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient fine-tuning strategies for long document summarization remains unexplored.","Therefore, this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly incorporate RST into the LoRA model.","Our empirical evaluation demonstrates that incorporating the type and uncertainty of rhetorical relations can complementarily enhance the performance of LoRA in summarization tasks.","Furthermore, the best-performing variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning models, as confirmed by multiple automatic and human evaluations, and even surpasses previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.00657v1","category":"cs.CL"}
{"created":"2024-05-01 17:32:17","title":"Particle scale anisotropy controls bulk properties in sheared granular materials","abstract":"The bulk dynamics of dense granular materials arise through a combination of particle-scale and mesoscale effects. Theoretical and numerical studies have shown that collective effects are created by particle-scale anisotropic structures such as grain connectivity (fabric), force transmission, and frictional mobilization, all of which influence bulk properties like bulk friction and the stress tensor through the Stress-Force-Fabric (SFF) relationship. To date, establishing the relevance of these effects to laboratory systems has remained elusive due to the challenge of measuring both normal and frictional contact forces at the particle scale. In this study, we perform experiments on a sheared photoelastic granular system in an quasi-2D annular (Couette) cell. During these experiments, we measure particle locations, contacts, and normal and frictional forces vectors during loading. We reconstruct the angular distributions of the contact and force vectors, and extract the corresponding emergent anisotropies for each of these metrics. Finally, we show that the SFF relation quantitatively predicts the relationship between particle scale anisotropies, the stress tensor components, and the bulk friction coefficient, capturing even transient behaviors. As such, this method shows promise for application to other dense particulate systems where fabric anisotropy can provide a useful measure of bulk friction.","sentences":["The bulk dynamics of dense granular materials arise through a combination of particle-scale and mesoscale effects.","Theoretical and numerical studies have shown that collective effects are created by particle-scale anisotropic structures such as grain connectivity (fabric), force transmission, and frictional mobilization, all of which influence bulk properties like bulk friction and the stress tensor through the Stress-Force-Fabric (SFF) relationship.","To date, establishing the relevance of these effects to laboratory systems has remained elusive due to the challenge of measuring both normal and frictional contact forces at the particle scale.","In this study, we perform experiments on a sheared photoelastic granular system in an quasi-2D annular (Couette) cell.","During these experiments, we measure particle locations, contacts, and normal and frictional forces vectors during loading.","We reconstruct the angular distributions of the contact and force vectors, and extract the corresponding emergent anisotropies for each of these metrics.","Finally, we show that the SFF relation quantitatively predicts the relationship between particle scale anisotropies, the stress tensor components, and the bulk friction coefficient, capturing even transient behaviors.","As such, this method shows promise for application to other dense particulate systems where fabric anisotropy can provide a useful measure of bulk friction."],"url":"http://arxiv.org/abs/2405.00653v1","category":"cond-mat.soft"}
{"created":"2024-05-01 17:27:11","title":"Grains of Saliency: Optimizing Saliency-based Training of Biometric Attack Detection Models","abstract":"Incorporating human-perceptual intelligence into model training has shown to increase the generalization capability of models in several difficult biometric tasks, such as presentation attack detection (PAD) and detection of synthetic samples. After the initial collection phase, human visual saliency (e.g., eye-tracking data, or handwritten annotations) can be integrated into model training through attention mechanisms, augmented training samples, or through human perception-related components of loss functions. Despite their successes, a vital, but seemingly neglected, aspect of any saliency-based training is the level of salience granularity (e.g., bounding boxes, single saliency maps, or saliency aggregated from multiple subjects) necessary to find a balance between reaping the full benefits of human saliency and the cost of its collection. In this paper, we explore several different levels of salience granularity and demonstrate that increased generalization capabilities of PAD and synthetic face detection can be achieved by using simple yet effective saliency post-processing techniques across several different CNNs.","sentences":["Incorporating human-perceptual intelligence into model training has shown to increase the generalization capability of models in several difficult biometric tasks, such as presentation attack detection (PAD) and detection of synthetic samples.","After the initial collection phase, human visual saliency (e.g., eye-tracking data, or handwritten annotations) can be integrated into model training through attention mechanisms, augmented training samples, or through human perception-related components of loss functions.","Despite their successes, a vital, but seemingly neglected, aspect of any saliency-based training is the level of salience granularity (e.g., bounding boxes, single saliency maps, or saliency aggregated from multiple subjects) necessary to find a balance between reaping the full benefits of human saliency and the cost of its collection.","In this paper, we explore several different levels of salience granularity and demonstrate that increased generalization capabilities of PAD and synthetic face detection can be achieved by using simple yet effective saliency post-processing techniques across several different CNNs."],"url":"http://arxiv.org/abs/2405.00650v1","category":"cs.CV"}
{"created":"2024-05-01 17:17:22","title":"ConstrainedZero: Chance-Constrained POMDP Planning using Learned Probabilistic Failure Surrogates and Adaptive Safety Constraints","abstract":"To plan safely in uncertain environments, agents must balance utility with safety constraints. Safe planning problems can be modeled as a chance-constrained partially observable Markov decision process (CC-POMDP) and solutions often use expensive rollouts or heuristics to estimate the optimal value and action-selection policy. This work introduces the ConstrainedZero policy iteration algorithm that solves CC-POMDPs in belief space by learning neural network approximations of the optimal value and policy with an additional network head that estimates the failure probability given a belief. This failure probability guides safe action selection during online Monte Carlo tree search (MCTS). To avoid overemphasizing search based on the failure estimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference to update the failure threshold during planning. The approach is tested on a safety-critical POMDP benchmark, an aircraft collision avoidance system, and the sustainability problem of safe CO$_2$ storage. Results show that by separating safety constraints from the objective we can achieve a target level of safety without optimizing the balance between rewards and costs.","sentences":["To plan safely in uncertain environments, agents must balance utility with safety constraints.","Safe planning problems can be modeled as a chance-constrained partially observable Markov decision process (CC-POMDP) and solutions often use expensive rollouts or heuristics to estimate the optimal value and action-selection policy.","This work introduces the ConstrainedZero policy iteration algorithm that solves CC-POMDPs in belief space by learning neural network approximations of the optimal value and policy with an additional network head that estimates the failure probability given a belief.","This failure probability guides safe action selection during online Monte Carlo tree search (MCTS).","To avoid overemphasizing search based on the failure estimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference to update the failure threshold during planning.","The approach is tested on a safety-critical POMDP benchmark, an aircraft collision avoidance system, and the sustainability problem of safe CO$_2$ storage.","Results show that by separating safety constraints from the objective we can achieve a target level of safety without optimizing the balance between rewards and costs."],"url":"http://arxiv.org/abs/2405.00644v1","category":"cs.AI"}
{"created":"2024-05-01 16:58:28","title":"When Quantization Affects Confidence of Large Language Models?","abstract":"Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs. This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss. Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.","sentences":["Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation.","Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs.","This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss.","Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models.","Secondly, we observe fluctuations in the impact on confidence across different scales.","Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place."],"url":"http://arxiv.org/abs/2405.00632v1","category":"cs.CL"}
{"created":"2024-05-01 16:54:12","title":"HUGO -- Highlighting Unseen Grid Options: Combining Deep Reinforcement Learning with a Heuristic Target Topology Approach","abstract":"With the growth of Renewable Energy (RE) generation, the operation of power grids has become increasingly complex. One solution is automated grid operation, where Deep Reinforcement Learning (DRL) has repeatedly shown significant potential in Learning to Run a Power Network (L2RPN) challenges. However, only individual actions at the substation level have been subjected to topology optimization by most existing DRL algorithms. In contrast, we propose a more holistic approach in this paper by proposing specific Target Topologies (TTs) as actions. These topologies are selected based on their robustness. As part of this paper, we present a search algorithm to find the TTs and upgrade our previously developed DRL agent CurriculumAgent (CAgent) to a novel topology agent. We compare the upgrade to the previous CAgent agent and can increase their scores significantly by 10%. Further, we achieve a 25% better median survival with our TTs included. Later analysis shows that almost all TTs are close to the base topology, explaining their robustness.","sentences":["With the growth of Renewable Energy (RE) generation, the operation of power grids has become increasingly complex.","One solution is automated grid operation, where Deep Reinforcement Learning (DRL) has repeatedly shown significant potential in Learning to Run a Power Network (L2RPN) challenges.","However, only individual actions at the substation level have been subjected to topology optimization by most existing DRL algorithms.","In contrast, we propose a more holistic approach in this paper by proposing specific Target Topologies (TTs) as actions.","These topologies are selected based on their robustness.","As part of this paper, we present a search algorithm to find the TTs and upgrade our previously developed DRL agent CurriculumAgent (CAgent) to a novel topology agent.","We compare the upgrade to the previous CAgent agent and can increase their scores significantly by 10%.","Further, we achieve a 25% better median survival with our TTs included.","Later analysis shows that almost all TTs are close to the base topology, explaining their robustness."],"url":"http://arxiv.org/abs/2405.00629v1","category":"cs.LG"}
{"created":"2024-05-01 16:43:55","title":"\"I'm Not Sure, But...\": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust","abstract":"Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., \"I'm not sure, but...\") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., \"It's not clear, but...\"), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.","sentences":["Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct.","To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users.","However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty.","We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine.","Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance.","We find that first-person expressions (e.g., \"I'm not sure, but...\") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy.","An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers.","While we observe similar effects for uncertainty expressed from a general perspective (e.g., \"It's not clear, but...\"), these effects are weaker and not statistically significant.","Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters.","This highlights the importance of user testing before deploying LLMs at scale."],"url":"http://arxiv.org/abs/2405.00623v1","category":"cs.HC"}
{"created":"2024-05-01 16:43:21","title":"Causal Evaluation of Language Models","abstract":"Causal reasoning is viewed as crucial for achieving human-level machine intelligence. Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for causal reasoning. In this work, we introduce Causal evaluation of Language Models (CaLM), which, to the best of our knowledge, is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models. First, we propose the CaLM framework, which establishes a foundational taxonomy consisting of four modules: causal target (i.e., what to evaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to measure the results), and error (i.e., how to analyze the bad results). This taxonomy defines a broad evaluation design space while systematically selecting criteria and priorities. Second, we compose the CaLM dataset, comprising 126,334 data samples, to provide curated sets of causal targets, adaptations, metrics, and errors, offering extensive coverage for diverse research pursuits. Third, we conduct an extensive evaluation of 28 leading language models on a core set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types. Fourth, we perform detailed analyses of the evaluation results across various dimensions (e.g., adaptation, scale). Fifth, we present 50 high-level empirical findings across 9 dimensions (e.g., model), providing valuable guidance for future language model development. Finally, we develop a multifaceted platform, including a website, leaderboards, datasets, and toolkits, to support scalable and adaptable assessments. We envision CaLM as an ever-evolving benchmark for the community, systematically updated with new causal targets, adaptations, models, metrics, and error types to reflect ongoing research advancements. Project website is at https://opencausalab.github.io/CaLM.","sentences":["Causal reasoning is viewed as crucial for achieving human-level machine intelligence.","Recent advances in language models have expanded the horizons of artificial intelligence across various domains, sparking inquiries into their potential for causal reasoning.","In this work, we introduce Causal evaluation of Language Models (CaLM), which, to the best of our knowledge, is the first comprehensive benchmark for evaluating the causal reasoning capabilities of language models.","First, we propose the CaLM framework, which establishes a foundational taxonomy consisting of four modules: causal target (i.e., what to evaluate), adaptation (i.e., how to obtain the results), metric (i.e., how to measure the results), and error (i.e., how to analyze the bad results).","This taxonomy defines a broad evaluation design space while systematically selecting criteria and priorities.","Second, we compose the CaLM dataset, comprising 126,334 data samples, to provide curated sets of causal targets, adaptations, metrics, and errors, offering extensive coverage for diverse research pursuits.","Third, we conduct an extensive evaluation of 28 leading language models on a core set of 92 causal targets, 9 adaptations, 7 metrics, and 12 error types.","Fourth, we perform detailed analyses of the evaluation results across various dimensions (e.g., adaptation, scale).","Fifth, we present 50 high-level empirical findings across 9 dimensions (e.g., model), providing valuable guidance for future language model development.","Finally, we develop a multifaceted platform, including a website, leaderboards, datasets, and toolkits, to support scalable and adaptable assessments.","We envision CaLM as an ever-evolving benchmark for the community, systematically updated with new causal targets, adaptations, models, metrics, and error types to reflect ongoing research advancements.","Project website is at https://opencausalab.github.io/CaLM."],"url":"http://arxiv.org/abs/2405.00622v1","category":"cs.CL"}
{"created":"2024-05-01 16:35:04","title":"Multigroup Robustness","abstract":"To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption. However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset. Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of multigroup robust algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption inside that subpopulation. When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related. Our techniques establish a new connection between multigroup fairness and robustness.","sentences":["To address the shortcomings of real-world datasets, robust learning algorithms have been designed to overcome arbitrary and indiscriminate data corruption.","However, practical processes of gathering data may lead to patterns of data corruption that are localized to specific partitions of the training dataset.","Motivated by critical applications where the learned model is deployed to make predictions about people from a rich collection of overlapping subpopulations, we initiate the study of multigroup robust algorithms whose robustness guarantees for each subpopulation only degrade with the amount of data corruption inside that subpopulation.","When the data corruption is not distributed uniformly over subpopulations, our algorithms provide more meaningful robustness guarantees than standard guarantees that are oblivious to how the data corruption and the affected subpopulations are related.","Our techniques establish a new connection between multigroup fairness and robustness."],"url":"http://arxiv.org/abs/2405.00614v1","category":"cs.LG"}
{"created":"2024-05-01 16:14:22","title":"Learning Expressive Disentangled Speech Representations with Soft Speech Units and Adversarial Style Augmentation","abstract":"Voice conversion is the task to transform voice characteristics of source speech while preserving content information. Nowadays, self-supervised representation learning models are increasingly utilized in content extraction. However, in these representations, a lot of hidden speaker information leads to timbre leakage while the prosodic information of hidden units lacks use. To address these issues, we propose a novel framework for expressive voice conversion called \"SAVC\" based on soft speech units from HuBert-soft. Taking soft speech units as input, we design an attribute encoder to extract content and prosody features respectively. Specifically, we first introduce statistic perturbation imposed by adversarial style augmentation to eliminate speaker information. Then the prosody is implicitly modeled on soft speech units with knowledge distillation. Experiment results show that the intelligibility and naturalness of converted speech outperform previous work.","sentences":["Voice conversion is the task to transform voice characteristics of source speech while preserving content information.","Nowadays, self-supervised representation learning models are increasingly utilized in content extraction.","However, in these representations, a lot of hidden speaker information leads to timbre leakage while the prosodic information of hidden units lacks use.","To address these issues, we propose a novel framework for expressive voice conversion called \"SAVC\" based on soft speech units from HuBert-soft.","Taking soft speech units as input, we design an attribute encoder to extract content and prosody features respectively.","Specifically, we first introduce statistic perturbation imposed by adversarial style augmentation to eliminate speaker information.","Then the prosody is implicitly modeled on soft speech units with knowledge distillation.","Experiment results show that the intelligibility and naturalness of converted speech outperform previous work."],"url":"http://arxiv.org/abs/2405.00603v1","category":"cs.SD"}
{"created":"2024-05-01 15:51:15","title":"Are Models Biased on Text without Gender-related Language?","abstract":"Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data. In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: Do language models still exhibit gender bias in non-stereotypical settings? To do so, we introduce UnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios. USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language. By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation. Surprisingly, we find low fairness across all 28 tested models. Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation. We release the full dataset and code at https://ucinlp.github.io/unstereo-eval.","sentences":["Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions.","A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that are present in the training data.","In this paper, we focus on bias where the effect from training data is unclear, and instead address the question: Do language models still exhibit gender bias in non-stereotypical settings?","To do so, we introduce UnStereoEval (USE), a novel framework tailored for investigating gender bias in stereotype-free scenarios.","USE defines a sentence-level score based on pretraining data statistics to determine if the sentence contain minimal word-gender associations.","To systematically benchmark the fairness of popular language models in stereotype-free scenarios, we utilize USE to automatically generate benchmarks without any gender-related language.","By leveraging USE's sentence-level score, we also repurpose prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation.","Surprisingly, we find low fairness across all 28 tested models.","Concretely, models demonstrate fair behavior in only 9%-41% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words.","These results raise important questions about where underlying model biases come from and highlight the need for more systematic and comprehensive bias evaluation.","We release the full dataset and code at https://ucinlp.github.io/unstereo-eval."],"url":"http://arxiv.org/abs/2405.00588v1","category":"cs.CL"}
{"created":"2024-05-01 15:30:41","title":"The Real, the Better: Aligning Large Language Models with Online Human Behaviors","abstract":"Large language model alignment is widely used and studied to avoid LLM producing unhelpful and harmful responses. However, the lengthy training process and predefined preference bias hinder adaptation to online diverse human preferences. To this end, this paper proposes an alignment framework, called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by directly leveraging real online human behaviors. By taking the generative adversarial framework, the generator is trained to respond following expected human behavior; while the discriminator tries to verify whether the triplets of query, response, and human behavior come from real online environments. Behavior modeling in natural-language form and the multi-model joint training mechanism enable an active and sustainable online alignment. Experimental results confirm the effectiveness of our proposed methods by both human and automatic evaluations.","sentences":["Large language model alignment is widely used and studied to avoid LLM producing unhelpful and harmful responses.","However, the lengthy training process and predefined preference bias hinder adaptation to online diverse human preferences.","To this end, this paper proposes an alignment framework, called Reinforcement Learning with Human Behavior (RLHB), to align LLMs by directly leveraging real online human behaviors.","By taking the generative adversarial framework, the generator is trained to respond following expected human behavior; while the discriminator tries to verify whether the triplets of query, response, and human behavior come from real online environments.","Behavior modeling in natural-language form and the multi-model joint training mechanism enable an active and sustainable online alignment.","Experimental results confirm the effectiveness of our proposed methods by both human and automatic evaluations."],"url":"http://arxiv.org/abs/2405.00578v1","category":"cs.CL"}
{"created":"2024-05-01 15:25:54","title":"EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos with Multi-modal Large Language Model","abstract":"Emotion AI is the ability of computers to understand human emotional states. Existing works have achieved promising progress, but two limitations remain to be solved: 1) Previous studies have been more focused on short sequential video emotion analysis while overlooking long sequential video. However, the emotions in short sequential videos only reflect instantaneous emotions, which may be deliberately guided or hidden. In contrast, long sequential videos can reveal authentic emotions; 2) Previous studies commonly utilize various signals such as facial, speech, and even sensitive biological signals (e.g., electrocardiogram). However, due to the increasing demand for privacy, developing Emotion AI without relying on sensitive signals is becoming important. To address the aforementioned limitations, in this paper, we construct a dataset for Emotion Analysis in Long-sequential and De-identity videos called EALD by collecting and processing the sequences of athletes' post-match interviews. In addition to providing annotations of the overall emotional state of each video, we also provide the Non-Facial Body Language (NFBL) annotations for each player. NFBL is an inner-driven emotional expression and can serve as an identity-free clue to understanding the emotional state. Moreover, we provide a simple but effective baseline for further research. More precisely, we evaluate the Multimodal Large Language Models (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs) to perform emotion analysis. Our experimental results demonstrate that: 1) MLLMs can achieve comparable, even better performance than the supervised single-modal models, even in a zero-shot scenario; 2) NFBL is an important cue in long sequential emotion analysis. EALD will be available on the open-source platform.","sentences":["Emotion AI is the ability of computers to understand human emotional states.","Existing works have achieved promising progress, but two limitations remain to be solved: 1) Previous studies have been more focused on short sequential video emotion analysis while overlooking long sequential video.","However, the emotions in short sequential videos only reflect instantaneous emotions, which may be deliberately guided or hidden.","In contrast, long sequential videos can reveal authentic emotions; 2) Previous studies commonly utilize various signals such as facial, speech, and even sensitive biological signals (e.g., electrocardiogram).","However, due to the increasing demand for privacy, developing Emotion AI without relying on sensitive signals is becoming important.","To address the aforementioned limitations, in this paper, we construct a dataset for Emotion Analysis in Long-sequential and De-identity videos called EALD by collecting and processing the sequences of athletes' post-match interviews.","In addition to providing annotations of the overall emotional state of each video, we also provide the Non-Facial Body Language (NFBL) annotations for each player.","NFBL is an inner-driven emotional expression and can serve as an identity-free clue to understanding the emotional state.","Moreover, we provide a simple but effective baseline for further research.","More precisely, we evaluate the Multimodal Large Language Models (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs) to perform emotion analysis.","Our experimental results demonstrate that: 1) MLLMs can achieve comparable, even better performance than the supervised single-modal models, even in a zero-shot scenario; 2) NFBL is an important cue in long sequential emotion analysis.","EALD will be available on the open-source platform."],"url":"http://arxiv.org/abs/2405.00574v1","category":"cs.CV"}
{"created":"2024-05-01 15:19:54","title":"Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) is a complex task that retrieves images using a query, which is configured with an image and a caption that describes desired modifications to that image. Supervised CIR approaches have shown strong performance, but their reliance on expensive manually-annotated datasets restricts their scalability and broader applicability. To address these issues, previous studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR) methods, which utilize a projection module to map images to word tokens. However, we conjecture that this approach has a downside: the projection module distorts the original image representation and confines the resulting composed embeddings to the text-side. In order to resolve this, we introduce a novel ZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly merge image and text representations by identifying an intermediate embedding of both. Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that fine-tunes the image encoder while keeping the text encoder fixed. TAT closes the modality gap between images and text, making the Slerp process much more effective. Notably, the TAT method is not only efficient in terms of the scale of the training dataset and training time, but it also serves as an excellent initial checkpoint for training supervised CIR models, thereby highlighting its wider potential. The integration of the Slerp-based ZS-CIR with a TAT-tuned model enables our approach to deliver state-of-the-art retrieval performance across CIR benchmarks.","sentences":["Composed Image Retrieval (CIR) is a complex task that retrieves images using a query, which is configured with an image and a caption that describes desired modifications to that image.","Supervised CIR approaches have shown strong performance, but their reliance on expensive manually-annotated datasets restricts their scalability and broader applicability.","To address these issues, previous studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR) methods, which utilize a projection module to map images to word tokens.","However, we conjecture that this approach has a downside: the projection module distorts the original image representation and confines the resulting composed embeddings to the text-side.","In order to resolve this, we introduce a novel ZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly merge image and text representations by identifying an intermediate embedding of both.","Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that fine-tunes the image encoder while keeping the text encoder fixed.","TAT closes the modality gap between images and text, making the Slerp process much more effective.","Notably, the TAT method is not only efficient in terms of the scale of the training dataset and training time, but it also serves as an excellent initial checkpoint for training supervised CIR models, thereby highlighting its wider potential.","The integration of the Slerp-based ZS-CIR with a TAT-tuned model enables our approach to deliver state-of-the-art retrieval performance across CIR benchmarks."],"url":"http://arxiv.org/abs/2405.00571v1","category":"cs.CV"}
{"created":"2024-05-01 15:19:19","title":"WEST GCN-LSTM: Weighted Stacked Spatio-Temporal Graph Neural Networks for Regional Traffic Forecasting","abstract":"Regional traffic forecasting is a critical challenge in urban mobility, with applications to various fields such as the Internet of Everything. In recent years, spatio-temporal graph neural networks have achieved state-of-the-art results in the context of numerous traffic forecasting challenges. This work aims at expanding upon the conventional spatio-temporal graph neural network architectures in a manner that may facilitate the inclusion of information regarding the examined regions, as well as the populations that traverse them, in order to establish a more efficient prediction model. The end-product of this scientific endeavour is a novel spatio-temporal graph neural network architecture that is referred to as WEST (WEighted STacked) GCN-LSTM. Furthermore, the inclusion of the aforementioned information is conducted via the use of two novel dedicated algorithms that are referred to as the Shared Borders Policy and the Adjustable Hops Policy. Through information fusion and distillation, the proposed solution manages to significantly outperform its competitors in the frame of an experimental evaluation that consists of 19 forecasting models, across several datasets. Finally, an additional ablation study determined that each of the components of the proposed solution contributes towards enhancing its overall performance.","sentences":["Regional traffic forecasting is a critical challenge in urban mobility, with applications to various fields such as the Internet of Everything.","In recent years, spatio-temporal graph neural networks have achieved state-of-the-art results in the context of numerous traffic forecasting challenges.","This work aims at expanding upon the conventional spatio-temporal graph neural network architectures in a manner that may facilitate the inclusion of information regarding the examined regions, as well as the populations that traverse them, in order to establish a more efficient prediction model.","The end-product of this scientific endeavour is a novel spatio-temporal graph neural network architecture that is referred to as WEST (WEighted STacked) GCN-LSTM.","Furthermore, the inclusion of the aforementioned information is conducted via the use of two novel dedicated algorithms that are referred to as the Shared Borders Policy and the Adjustable Hops Policy.","Through information fusion and distillation, the proposed solution manages to significantly outperform its competitors in the frame of an experimental evaluation that consists of 19 forecasting models, across several datasets.","Finally, an additional ablation study determined that each of the components of the proposed solution contributes towards enhancing its overall performance."],"url":"http://arxiv.org/abs/2405.00570v1","category":"cs.LG"}
{"created":"2024-05-01 15:18:12","title":"Powering In-Database Dynamic Model Slicing for Structured Data Analytics","abstract":"Relational database management systems (RDBMS) are widely used for the storage and retrieval of structured data. To derive insights beyond statistical aggregation, we typically have to extract specific subdatasets from the database using conventional database operations, and then apply deep neural networks (DNN) training and inference on these respective subdatasets in a separate machine learning system. The process can be prohibitively expensive, especially when there are a combinatorial number of subdatasets extracted for different analytical purposes. This calls for efficient in-database support of advanced analytical methods In this paper, we introduce LEADS, a novel SQL-aware dynamic model slicing technique to customize models for subdatasets specified by SQL queries. LEADS improves the predictive modeling of structured data via the mixture of experts (MoE) technique and maintains inference efficiency by a SQL-aware gating network. At the core of LEADS is the construction of a general model with multiple expert sub-models via MoE trained over the entire database. This SQL-aware MoE technique scales up the modeling capacity, enhances effectiveness, and preserves efficiency by activating only necessary experts via the gating network during inference. Additionally, we introduce two regularization terms during the training process of LEADS to strike a balance between effectiveness and efficiency. We also design and build an in-database inference system, called INDICES, to support end-to-end advanced structured data analytics by non-intrusively incorporating LEADS onto PostgreSQL. Our extensive experiments on real-world datasets demonstrate that LEADS consistently outperforms baseline models, and INDICES delivers effective in-database analytics with a considerable reduction in inference latency compared to traditional solutions.","sentences":["Relational database management systems (RDBMS) are widely used for the storage and retrieval of structured data.","To derive insights beyond statistical aggregation, we typically have to extract specific subdatasets from the database using conventional database operations, and then apply deep neural networks (DNN) training and inference on these respective subdatasets in a separate machine learning system.","The process can be prohibitively expensive, especially when there are a combinatorial number of subdatasets extracted for different analytical purposes.","This calls for efficient in-database support of advanced analytical methods In this paper, we introduce LEADS, a novel SQL-aware dynamic model slicing technique to customize models for subdatasets specified by SQL queries.","LEADS improves the predictive modeling of structured data via the mixture of experts (MoE) technique and maintains inference efficiency by a SQL-aware gating network.","At the core of LEADS is the construction of a general model with multiple expert sub-models via MoE trained over the entire database.","This SQL-aware MoE technique scales up the modeling capacity, enhances effectiveness, and preserves efficiency by activating only necessary experts via the gating network during inference.","Additionally, we introduce two regularization terms during the training process of LEADS to strike a balance between effectiveness and efficiency.","We also design and build an in-database inference system, called INDICES, to support end-to-end advanced structured data analytics by non-intrusively incorporating LEADS onto PostgreSQL.","Our extensive experiments on real-world datasets demonstrate that LEADS consistently outperforms baseline models, and INDICES delivers effective in-database analytics with a considerable reduction in inference latency compared to traditional solutions."],"url":"http://arxiv.org/abs/2405.00568v1","category":"cs.DB"}
{"created":"2024-05-01 15:07:32","title":"Informationally overcomplete measurements from generalized equiangular tight frames","abstract":"Informationally overcomplete measurements find important applications in quantum tomography and quantum state estimation. The most popular are maximal sets of mutually unbiased bases, for which trace relations between measurement operators are well known. In this paper, we introduce a more general class of informationally overcomplete POVMs that are generated by equiangular tight frames of arbitrary rank. This class provides a generalization of equiangular measurements to non-projective POVMs, which include rescaled mutually unbiased measurements and bases. We provide a method of their construction, analyze their symmetry properties, and provide examples for highly symmetric cases. In particular, we find a wide class of generalized equiangular measurements that are conical 2-designs, which allows us to derive the index of coincidence. Our results show benefits of considering a single informationally overcomplete measurement over informationally complete collections of POVMs.","sentences":["Informationally overcomplete measurements find important applications in quantum tomography and quantum state estimation.","The most popular are maximal sets of mutually unbiased bases, for which trace relations between measurement operators are well known.","In this paper, we introduce a more general class of informationally overcomplete POVMs that are generated by equiangular tight frames of arbitrary rank.","This class provides a generalization of equiangular measurements to non-projective POVMs, which include rescaled mutually unbiased measurements and bases.","We provide a method of their construction, analyze their symmetry properties, and provide examples for highly symmetric cases.","In particular, we find a wide class of generalized equiangular measurements that are conical 2-designs, which allows us to derive the index of coincidence.","Our results show benefits of considering a single informationally overcomplete measurement over informationally complete collections of POVMs."],"url":"http://arxiv.org/abs/2405.00560v1","category":"quant-ph"}
{"created":"2024-05-01 15:06:05","title":"Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment","abstract":"As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge, posing potential risks during deployment. Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer. To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT. This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production. It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development. Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies the mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency. The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency.","sentences":["As the capabilities of large language models (LLMs) have expanded dramatically, aligning these models with human values presents a significant challenge, posing potential risks during deployment.","Traditional alignment strategies rely heavily on human intervention, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), or on the self-alignment capacities of LLMs, which usually require a strong LLM's emergent ability to improve its original bad answer.","To address these challenges, we propose a novel self-alignment method that utilizes a Chain of Thought (CoT) approach, termed AlignCoT.","This method encompasses stages of Question Analysis, Answer Guidance, and Safe Answer production.","It is designed to enable LLMs to generate high-quality, safe responses throughout various stages of their development.","Furthermore, we introduce the Mixture of insighTful Experts (MoTE) architecture, which applies the mixture of experts to enhance each component of the AlignCoT process, markedly increasing alignment efficiency.","The MoTE approach not only outperforms existing methods in aligning LLMs with human values but also highlights the benefits of using self-generated data, revealing the dual benefits of improved alignment and training efficiency."],"url":"http://arxiv.org/abs/2405.00557v1","category":"cs.CL"}
{"created":"2024-05-01 14:59:24","title":"Swarm Learning: A Survey of Concepts, Applications, and Trends","abstract":"Deep learning models have raised privacy and security concerns due to their reliance on large datasets on central servers. As the number of Internet of Things (IoT) devices increases, artificial intelligence (AI) will be crucial for resource management, data processing, and knowledge acquisition. To address those issues, federated learning (FL) has introduced a novel approach to building a versatile, large-scale machine learning framework that operates in a decentralized and hardware-agnostic manner. However, FL faces network bandwidth limitations and data breaches. To reduce the central dependency in FL and increase scalability, swarm learning (SL) has been proposed in collaboration with Hewlett Packard Enterprise (HPE). SL represents a decentralized machine learning framework that leverages blockchain technology for secure, scalable, and private data management. A blockchain-based network enables the exchange and aggregation of model parameters among participants, thus mitigating the risk of a single point of failure and eliminating communication bottlenecks. To the best of our knowledge, this survey is the first to introduce the principles of Swarm Learning, its architectural design, and its fields of application. In addition, it highlights numerous research avenues that require further exploration by academic and industry communities to unlock the full potential and applications of SL.","sentences":["Deep learning models have raised privacy and security concerns due to their reliance on large datasets on central servers.","As the number of Internet of Things (IoT) devices increases, artificial intelligence (AI) will be crucial for resource management, data processing, and knowledge acquisition.","To address those issues, federated learning (FL) has introduced a novel approach to building a versatile, large-scale machine learning framework that operates in a decentralized and hardware-agnostic manner.","However, FL faces network bandwidth limitations and data breaches.","To reduce the central dependency in FL and increase scalability, swarm learning (SL) has been proposed in collaboration with Hewlett Packard Enterprise (HPE).","SL represents a decentralized machine learning framework that leverages blockchain technology for secure, scalable, and private data management.","A blockchain-based network enables the exchange and aggregation of model parameters among participants, thus mitigating the risk of a single point of failure and eliminating communication bottlenecks.","To the best of our knowledge, this survey is the first to introduce the principles of Swarm Learning, its architectural design, and its fields of application.","In addition, it highlights numerous research avenues that require further exploration by academic and industry communities to unlock the full potential and applications of SL."],"url":"http://arxiv.org/abs/2405.00556v1","category":"cs.LG"}
{"created":"2024-05-01 14:38:39","title":"Transport of topological defects in a biphasic mixture of active and passive nematic fluids","abstract":"Collectively moving cellular systems often contain a proportion of dead cells or non-motile genotypes. When mixed, nematically aligning motile and non-motile agents are known to segregate spontaneously. However, the role that topological defects and active stresses play in shaping the distribution of the two phases remains unresolved. In this study, we investigate the behaviour of a two-dimensional binary mixture of active and passive nematic fluids to understand how topological defects are transported between the two phases and, ultimately, how this leads to the segregation of topological charges. When the activity of the motile phase is large, and the tension at the interface of motile and non-motile phases is weak, we find that the active phase tends to accumulate $+1/2$ defects and expel $-1/2$ defects so that the motile phase develops a net positive charge. Conversely, when the activity of the motile phase is comparatively small and interfacial tension is strong, the opposite occurs so that the active phase develops a net negative charge. We then use these simulations to develop a physical intuition of the underlying processes that drive the charge segregation. Lastly, we quantify the sensitivity of this process on the other model parameters, by exploring the effect that anchoring strength, orientational elasticity, friction, and volume fraction of the motile phase have on topological charge segregation. As $+1/2$ and $-1/2$ defects have very different effects on interface morphology and fluid transport, this study offers new insights into the spontaneous pattern formation that occurs when motile and non-motile cells interact.","sentences":["Collectively moving cellular systems often contain a proportion of dead cells or non-motile genotypes.","When mixed, nematically aligning motile and non-motile agents are known to segregate spontaneously.","However, the role that topological defects and active stresses play in shaping the distribution of the two phases remains unresolved.","In this study, we investigate the behaviour of a two-dimensional binary mixture of active and passive nematic fluids to understand how topological defects are transported between the two phases and, ultimately, how this leads to the segregation of topological charges.","When the activity of the motile phase is large, and the tension at the interface of motile and non-motile phases is weak, we find that the active phase tends to accumulate $+1/2$ defects and expel $-1/2$ defects so that the motile phase develops a net positive charge.","Conversely, when the activity of the motile phase is comparatively small and interfacial tension is strong, the opposite occurs so that the active phase develops a net negative charge.","We then use these simulations to develop a physical intuition of the underlying processes that drive the charge segregation.","Lastly, we quantify the sensitivity of this process on the other model parameters, by exploring the effect that anchoring strength, orientational elasticity, friction, and volume fraction of the motile phase have on topological charge segregation.","As $+1/2$ and $-1/2$ defects have very different effects on interface morphology and fluid transport, this study offers new insights into the spontaneous pattern formation that occurs when motile and non-motile cells interact."],"url":"http://arxiv.org/abs/2405.00547v1","category":"physics.bio-ph"}
{"created":"2024-05-01 14:29:03","title":"New Benchmark Dataset and Fine-Grained Cross-Modal Fusion Framework for Vietnamese Multimodal Aspect-Category Sentiment Analysis","abstract":"The emergence of multimodal data on social media platforms presents new opportunities to better understand user sentiments toward a given aspect. However, existing multimodal datasets for Aspect-Category Sentiment Analysis (ACSA) often focus on textual annotations, neglecting fine-grained information in images. Consequently, these datasets fail to fully exploit the richness inherent in multimodal. To address this, we introduce a new Vietnamese multimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs with 14,618 fine-grained annotations for both text and image in the hotel domain. Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework (FCMF) that effectively learns both intra- and inter-modality interactions and then fuses these information to produce a unified multimodal representation. Experimental results show that our framework outperforms SOTA models on the ViMACSA dataset, achieving the highest F1 score of 79.73%. We also explore characteristics and challenges in Vietnamese multimodal sentiment analysis, including misspellings, abbreviations, and the complexities of the Vietnamese language. This work contributes both a benchmark dataset and a new framework that leverages fine-grained multimodal information to improve multimodal aspect-category sentiment analysis. Our dataset is available for research purposes: https://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis.","sentences":["The emergence of multimodal data on social media platforms presents new opportunities to better understand user sentiments toward a given aspect.","However, existing multimodal datasets for Aspect-Category Sentiment Analysis (ACSA) often focus on textual annotations, neglecting fine-grained information in images.","Consequently, these datasets fail to fully exploit the richness inherent in multimodal.","To address this, we introduce a new Vietnamese multimodal dataset, named ViMACSA, which consists of 4,876 text-image pairs with 14,618 fine-grained annotations for both text and image in the hotel domain.","Additionally, we propose a Fine-Grained Cross-Modal Fusion Framework (FCMF) that effectively learns both intra- and inter-modality interactions and then fuses these information to produce a unified multimodal representation.","Experimental results show that our framework outperforms SOTA models on the ViMACSA dataset, achieving the highest F1 score of 79.73%.","We also explore characteristics and challenges in Vietnamese multimodal sentiment analysis, including misspellings, abbreviations, and the complexities of the Vietnamese language.","This work contributes both a benchmark dataset and a new framework that leverages fine-grained multimodal information to improve multimodal aspect-category sentiment analysis.","Our dataset is available for research purposes: https://github.com/hoangquy18/Multimodal-Aspect-Category-Sentiment-Analysis."],"url":"http://arxiv.org/abs/2405.00543v1","category":"cs.CL"}
{"created":"2024-05-01 14:18:50","title":"A Legal Framework for Natural Language Processing Model Training in Portugal","abstract":"Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect. In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so. Instead, these models use large volumes of textual data to learn meaningful representations of human languages. Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged. Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations. Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications. In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development.","sentences":["Recent advances in deep learning have promoted the advent of many computational systems capable of performing intelligent actions that, until then, were restricted to the human intellect.","In the particular case of human languages, these advances allowed the introduction of applications like ChatGPT that are capable of generating coherent text without being explicitly programmed to do so.","Instead, these models use large volumes of textual data to learn meaningful representations of human languages.","Associated with these advances, concerns about copyright and data privacy infringements caused by these applications have emerged.","Despite these concerns, the pace at which new natural language processing applications continued to be developed largely outperformed the introduction of new regulations.","Today, communication barriers between legal experts and computer scientists motivate many unintentional legal infringements during the development of such applications.","In this paper, a multidisciplinary team intends to bridge this communication gap and promote more compliant Portuguese NLP research by presenting a series of everyday NLP use cases, while highlighting the Portuguese legislation that may arise during its development."],"url":"http://arxiv.org/abs/2405.00536v1","category":"cs.CL"}
{"created":"2024-05-01 14:05:52","title":"ULLER: A Unified Language for Learning and Reasoning","abstract":"The field of neuro-symbolic artificial intelligence (NeSy), which combines learning and reasoning, has recently experienced significant growth. There now are a wide variety of NeSy frameworks, each with its own specific language for expressing background knowledge and how to relate it to neural networks. This heterogeneity hinders accessibility for newcomers and makes comparing different NeSy frameworks challenging. We propose a unified language for NeSy, which we call ULLER, a Unified Language for LEarning and Reasoning. ULLER encompasses a wide variety of settings, while ensuring that knowledge described in it can be used in existing NeSy systems. ULLER has a neuro-symbolic first-order syntax for which we provide example semantics including classical, fuzzy, and probabilistic logics. We believe ULLER is a first step towards making NeSy research more accessible and comparable, paving the way for libraries that streamline training and evaluation across a multitude of semantics, knowledge bases, and NeSy systems.","sentences":["The field of neuro-symbolic artificial intelligence (NeSy), which combines learning and reasoning, has recently experienced significant growth.","There now are a wide variety of NeSy frameworks, each with its own specific language for expressing background knowledge and how to relate it to neural networks.","This heterogeneity hinders accessibility for newcomers and makes comparing different NeSy frameworks challenging.","We propose a unified language for NeSy, which we call ULLER, a Unified Language for LEarning and Reasoning.","ULLER encompasses a wide variety of settings, while ensuring that knowledge described in it can be used in existing NeSy systems.","ULLER has a neuro-symbolic first-order syntax for which we provide example semantics including classical, fuzzy, and probabilistic logics.","We believe ULLER is a first step towards making NeSy research more accessible and comparable, paving the way for libraries that streamline training and evaluation across a multitude of semantics, knowledge bases, and NeSy systems."],"url":"http://arxiv.org/abs/2405.00532v1","category":"cs.AI"}
{"created":"2024-05-01 14:01:22","title":"ChatBI: Towards Natural Language to Complex Business Intelligence SQL","abstract":"The Natural Language to SQL (NL2SQL) technology provides non-expert users who are unfamiliar with databases the opportunity to use SQL for data analysis.Converting Natural Language to Business Intelligence (NL2BI) is a popular practical scenario for NL2SQL in actual production systems. Compared to NL2SQL, NL2BI introduces more challenges.   In this paper, we propose ChatBI, a comprehensive and efficient technology for solving the NL2BI task. First, we analyze the interaction mode, an important module where NL2SQL and NL2BI differ in use, and design a smaller and cheaper model to match this interaction mode. In BI scenarios, tables contain a huge number of columns, making it impossible for existing NL2SQL methods that rely on Large Language Models (LLMs) for schema linking to proceed due to token limitations. The higher proportion of ambiguous columns in BI scenarios also makes schema linking difficult. ChatBI combines existing view technology in the database community to first decompose the schema linking problem into a Single View Selection problem and then uses a smaller and cheaper machine learning model to select the single view with a significantly reduced number of columns. The columns of this single view are then passed as the required columns for schema linking into the LLM. Finally, ChatBI proposes a phased process flow different from existing process flows, which allows ChatBI to generate SQL containing complex semantics and comparison relations more accurately.   We have deployed ChatBI on Baidu's data platform and integrated it into multiple product lines for large-scale production task evaluation. The obtained results highlight its superiority in practicality, versatility, and efficiency. At the same time, compared with the current mainstream NL2SQL technology under our real BI scenario data tables and queries, it also achieved the best results.","sentences":["The Natural Language to SQL (NL2SQL) technology provides non-expert users who are unfamiliar with databases the opportunity to use SQL for data analysis.","Converting Natural Language to Business Intelligence (NL2BI) is a popular practical scenario for NL2SQL in actual production systems.","Compared to NL2SQL, NL2BI introduces more challenges.   ","In this paper, we propose ChatBI, a comprehensive and efficient technology for solving the NL2BI task.","First, we analyze the interaction mode, an important module where NL2SQL and NL2BI differ in use, and design a smaller and cheaper model to match this interaction mode.","In BI scenarios, tables contain a huge number of columns, making it impossible for existing NL2SQL methods that rely on Large Language Models (LLMs) for schema linking to proceed due to token limitations.","The higher proportion of ambiguous columns in BI scenarios also makes schema linking difficult.","ChatBI combines existing view technology in the database community to first decompose the schema linking problem into a Single View Selection problem and then uses a smaller and cheaper machine learning model to select the single view with a significantly reduced number of columns.","The columns of this single view are then passed as the required columns for schema linking into the LLM.","Finally, ChatBI proposes a phased process flow different from existing process flows, which allows ChatBI to generate SQL containing complex semantics and comparison relations more accurately.   ","We have deployed ChatBI on Baidu's data platform and integrated it into multiple product lines for large-scale production task evaluation.","The obtained results highlight its superiority in practicality, versatility, and efficiency.","At the same time, compared with the current mainstream NL2SQL technology under our real BI scenario data tables and queries, it also achieved the best results."],"url":"http://arxiv.org/abs/2405.00527v1","category":"cs.DB"}
{"created":"2024-05-01 13:58:28","title":"FMLFS: A federated multi-label feature selection based on information theory in IoT environment","abstract":"In certain emerging applications such as health monitoring wearable and traffic monitoring systems, Internet-of-Things (IoT) devices generate or collect a huge amount of multi-label datasets. Within these datasets, each instance is linked to a set of labels. The presence of noisy, redundant, or irrelevant features in these datasets, along with the curse of dimensionality, poses challenges for multi-label classifiers. Feature selection (FS) proves to be an effective strategy in enhancing classifier performance and addressing these challenges. Yet, there is currently no existing distributed multi-label FS method documented in the literature that is suitable for distributed multi-label datasets within IoT environments. This paper introduces FMLFS, the first federated multi-label feature selection method. Here, mutual information between features and labels serves as the relevancy metric, while the correlation distance between features, derived from mutual information and joint entropy, is utilized as the redundancy measure. Following aggregation of these metrics on the edge server and employing Pareto-based bi-objective and crowding distance strategies, the sorted features are subsequently sent back to the IoT devices. The proposed method is evaluated through two scenarios: 1) transmitting reduced-size datasets to the edge server for centralized classifier usage, and 2) employing federated learning with reduced-size datasets. Evaluation across three metrics - performance, time complexity, and communication cost - demonstrates that FMLFS outperforms five other comparable methods in the literature and provides a good trade-off on three real-world datasets.","sentences":["In certain emerging applications such as health monitoring wearable and traffic monitoring systems, Internet-of-Things (IoT) devices generate or collect a huge amount of multi-label datasets.","Within these datasets, each instance is linked to a set of labels.","The presence of noisy, redundant, or irrelevant features in these datasets, along with the curse of dimensionality, poses challenges for multi-label classifiers.","Feature selection (FS) proves to be an effective strategy in enhancing classifier performance and addressing these challenges.","Yet, there is currently no existing distributed multi-label FS method documented in the literature that is suitable for distributed multi-label datasets within IoT environments.","This paper introduces FMLFS, the first federated multi-label feature selection method.","Here, mutual information between features and labels serves as the relevancy metric, while the correlation distance between features, derived from mutual information and joint entropy, is utilized as the redundancy measure.","Following aggregation of these metrics on the edge server and employing Pareto-based bi-objective and crowding distance strategies, the sorted features are subsequently sent back to the IoT devices.","The proposed method is evaluated through two scenarios: 1) transmitting reduced-size datasets to the edge server for centralized classifier usage, and 2) employing federated learning with reduced-size datasets.","Evaluation across three metrics - performance, time complexity, and communication cost - demonstrates that FMLFS outperforms five other comparable methods in the literature and provides a good trade-off on three real-world datasets."],"url":"http://arxiv.org/abs/2405.00524v1","category":"cs.LG"}
{"created":"2024-05-01 13:58:09","title":"CookingSense: A Culinary Knowledgebase with Multidisciplinary Assertions","abstract":"This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired. CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions. Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems. From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models. We also validate the quality and variety of assertions in CookingSense through qualitative analysis.","sentences":["This paper introduces CookingSense, a descriptive collection of knowledge assertions in the culinary domain extracted from various sources, including web data, scientific papers, and recipes, from which knowledge covering a broad range of aspects is acquired.","CookingSense is constructed through a series of dictionary-based filtering and language model-based semantic filtering techniques, which results in a rich knowledgebase of multidisciplinary food-related assertions.","Additionally, we present FoodBench, a novel benchmark to evaluate culinary decision support systems.","From evaluations with FoodBench, we empirically prove that CookingSense improves the performance of retrieval augmented language models.","We also validate the quality and variety of assertions in CookingSense through qualitative analysis."],"url":"http://arxiv.org/abs/2405.00523v1","category":"cs.AI"}
{"created":"2024-05-01 13:51:45","title":"Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning","abstract":"Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation. Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods. However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results. In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods. We also address a critical limitation in previous models' understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure. To rectify this, we propose methods to enhance true understanding and present a new baseline of results. Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a multimodal RL approach. This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks.","sentences":["Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation.","Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods.","However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results.","In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods.","We also address a critical limitation in previous models' understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure.","To rectify this, we propose methods to enhance true understanding and present a new baseline of results.","Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a multimodal RL approach.","This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks."],"url":"http://arxiv.org/abs/2405.00516v1","category":"cs.LG"}
{"created":"2024-05-01 13:20:37","title":"Using non-DESI data to confirm and strengthen the DESI 2024 spatially-flat $w_0w_a$CDM cosmological parameterization result","abstract":"We use a combination of Planck cosmic microwave background (CMB) anisotropy data and non-CMB data that include Pantheon+ type Ia supernovae, Hubble parameter [$H(z)$], growth factor ($f\\sigma_8$) measurements, and a collection of baryon acoustic oscillation (BAO) data, but not recent DESI 2024 BAO measurements, to confirm the DESI 2024 (DESI+CMB+PantheonPlus) data compilation support for dynamical dark energy with an evolving equation of state parameter $w(z) = w_0 + w_a z/(1+z)$. From our joint compilation of CMB and non-CMB data, in a spatially-flat cosmological model, we obtain $w_0 = -0.850 \\pm 0.059$ and $w_a = -0.59^{+0.26}_{-0.22}$ and find that this dynamical dark energy is favored over a cosmological constant by $\\sim 2\\sigma$. Our data constraints on the flat $w_0w_a$CDM model are slightly more restrictive than the DESI 2024 constraints, with the DESI 2024 and our values of $w_0$ and $w_a$ differing by $-0.27\\sigma$ and $0.44\\sigma$, respectively. Our data compilation slightly more strongly favors the flat $w_0w_a$CDM model over the flat $\\Lambda$CDM model than does the DESI 2024 data compilation.","sentences":["We use a combination of Planck cosmic microwave background (CMB) anisotropy data and non-CMB data that include Pantheon+ type Ia supernovae, Hubble parameter [$H(z)$], growth factor ($f\\sigma_8$) measurements, and a collection of baryon acoustic oscillation (BAO) data, but not recent DESI 2024 BAO measurements, to confirm the DESI 2024 (DESI+CMB+PantheonPlus) data compilation support for dynamical dark energy with an evolving equation of state parameter $w(z) = w_0 + w_a","z/(1+z)$. From our joint compilation of CMB and non-CMB data, in a spatially-flat cosmological model, we obtain $w_0 = -0.850 \\pm 0.059$ and $w_a = -0.59^{+0.26}_{-0.22}$ and find that this dynamical dark energy is favored over a cosmological constant by $\\sim 2\\sigma$. Our data constraints on the flat $w_0w_a$CDM model are slightly more restrictive than the DESI 2024 constraints, with the DESI 2024 and our values of $w_0$ and $w_a$ differing by $-0.27\\sigma$ and $0.44\\sigma$, respectively.","Our data compilation slightly more strongly favors the flat $w_0w_a$CDM model over the flat $\\Lambda$CDM model than does the DESI 2024 data compilation."],"url":"http://arxiv.org/abs/2405.00502v1","category":"astro-ph.CO"}
{"created":"2024-05-01 13:00:51","title":"GOLD: Geometry Problem Solver with Natural Language Description","abstract":"Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics. Current methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving. To tackle this issue, we present the Geometry problem sOlver with natural Language Description (GOLD) model. GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram. Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems. Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets. Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively.","sentences":["Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics.","Current methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving.","To tackle this issue, we present the Geometry problem sOlver with natural Language Description (GOLD) model.","GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram.","Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems.","Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets.","Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively."],"url":"http://arxiv.org/abs/2405.00494v1","category":"cs.AI"}
{"created":"2024-05-01 12:59:37","title":"Is Temperature the Creativity Parameter of Large Language Models?","abstract":"Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism. The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter. Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt. Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence. We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality. However, the influence of temperature on creativity is far more nuanced and weak than suggested by the \"creativity parameter\" claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher. Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter.","sentences":["Large language models (LLMs) are applied to all sorts of creative tasks, and their outputs vary from beautiful, to peculiar, to pastiche, into plain plagiarism.","The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter.","Here, we investigate this claim using a narrative generation task with a predetermined fixed context, model and prompt.","Specifically, we present an empirical analysis of the LLM output for different temperature values using four necessary conditions for creativity in narrative generation: novelty, typicality, cohesion, and coherence.","We find that temperature is weakly correlated with novelty, and unsurprisingly, moderately correlated with incoherence, but there is no relationship with either cohesion or typicality.","However, the influence of temperature on creativity is far more nuanced and weak than suggested by the \"creativity parameter\" claim; overall results suggest that the LLM generates slightly more novel outputs as temperatures get higher.","Finally, we discuss ideas to allow more controlled LLM creativity, rather than relying on chance via changing the temperature parameter."],"url":"http://arxiv.org/abs/2405.00492v1","category":"cs.CL"}
{"created":"2024-05-01 12:57:14","title":"On the Relevance of Byzantine Robust Optimization Against Data Poisoning","abstract":"The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called {\\em workers}). Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against {\\em data poisoning}and some {\\em faulty workers}. The problem of {\\em Byzantine ML} formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm. Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear. It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable. We prove that, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are, in a precise sense, optimal even under the weaker data poisoning threat model. Then, we study a generic data poisoning model wherein some workers have {\\em fully-poisonous local data}, i.e., their datasets are entirely corruptible, and the remainders have {\\em partially-poisonous local data}, i.e., only a fraction of their local datasets is corruptible. We prove that Byzantine-robust schemes yield optimal solutions against both these forms of data poisoning, and that the former is more harmful when workers have {\\em heterogeneous} local data.","sentences":["The success of machine learning (ML) has been intimately linked with the availability of large amounts of data, typically collected from heterogeneous sources and processed on vast networks of computing devices (also called {\\em workers}).","Beyond accuracy, the use of ML in critical domains such as healthcare and autonomous driving calls for robustness against {\\em data poisoning}and some {\\em faulty workers}.","The problem of {\\em Byzantine ML} formalizes these robustness issues by considering a distributed ML environment in which workers (storing a portion of the global dataset) can deviate arbitrarily from the prescribed algorithm.","Although the problem has attracted a lot of attention from a theoretical point of view, its practical importance for addressing realistic faults (where the behavior of any worker is locally constrained) remains unclear.","It has been argued that the seemingly weaker threat model where only workers' local datasets get poisoned is more reasonable.","We prove that, while tolerating a wider range of faulty behaviors, Byzantine ML yields solutions that are, in a precise sense, optimal even under the weaker data poisoning threat model.","Then, we study a generic data poisoning model wherein some workers have {\\em fully-poisonous local data}, i.e., their datasets are entirely corruptible, and the remainders have {\\em partially-poisonous local data}, i.e., only a fraction of their local datasets is corruptible.","We prove that Byzantine-robust schemes yield optimal solutions against both these forms of data poisoning, and that the former is more harmful when workers have {\\em heterogeneous} local data."],"url":"http://arxiv.org/abs/2405.00491v1","category":"cs.LG"}
{"created":"2024-05-01 12:56:14","title":"Explainable Automatic Grading with Neural Additive Models","abstract":"The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum. However, current state-of-the-art ASAG models are large neural networks (NN) often described as \"black box\", providing no explanation for which characteristics of an input are important for the produced output. This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade. To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model. We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response. We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric. We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering.","sentences":["The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum.","However, current state-of-the-art ASAG models are large neural networks (NN) often described as \"black box\", providing no explanation for which characteristics of an input are important for the produced output.","This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade.","To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model.","We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response.","We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric.","We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering."],"url":"http://arxiv.org/abs/2405.00489v1","category":"cs.LG"}
{"created":"2024-05-01 12:08:38","title":"Feature-Aware Noise Contrastive Learning For Unsupervised Red Panda Re-Identification","abstract":"To facilitate the re-identification (Re-ID) of individual animals, existing methods primarily focus on maximizing feature similarity within the same individual and enhancing distinctiveness between different individuals. However, most of them still rely on supervised learning and require substantial labeled data, which is challenging to obtain. To avoid this issue, we propose a Feature-Aware Noise Contrastive Learning (FANCL) method to explore an unsupervised learning solution, which is then validated on the task of red panda re-ID. FANCL employs a Feature-Aware Noise Addition module to produce noised images that conceal critical features and designs two contrastive learning modules to calculate the losses. Firstly, a feature consistency module is designed to bridge the gap between the original and noised features. Secondly, the neural networks are trained through a cluster contrastive learning module. Through these more challenging learning tasks, FANCL can adaptively extract deeper representations of red pandas. The experimental results on a set of red panda images collected in both indoor and outdoor environments prove that FANCL outperforms several related state-of-the-art unsupervised methods, achieving high performance comparable to supervised learning methods.","sentences":["To facilitate the re-identification (Re-ID) of individual animals, existing methods primarily focus on maximizing feature similarity within the same individual and enhancing distinctiveness between different individuals.","However, most of them still rely on supervised learning and require substantial labeled data, which is challenging to obtain.","To avoid this issue, we propose a Feature-Aware Noise Contrastive Learning (FANCL) method to explore an unsupervised learning solution, which is then validated on the task of red panda re-ID.","FANCL employs a Feature-Aware Noise Addition module to produce noised images that conceal critical features and designs two contrastive learning modules to calculate the losses.","Firstly, a feature consistency module is designed to bridge the gap between the original and noised features.","Secondly, the neural networks are trained through a cluster contrastive learning module.","Through these more challenging learning tasks, FANCL can adaptively extract deeper representations of red pandas.","The experimental results on a set of red panda images collected in both indoor and outdoor environments prove that FANCL outperforms several related state-of-the-art unsupervised methods, achieving high performance comparable to supervised learning methods."],"url":"http://arxiv.org/abs/2405.00468v1","category":"cs.CV"}
{"created":"2024-05-01 11:39:38","title":"Enhancing Surgical Robots with Embodied Intelligence for Autonomous Ultrasound Scanning","abstract":"Ultrasound robots are increasingly used in medical diagnostics and early disease screening. However, current ultrasound robots lack the intelligence to understand human intentions and instructions, hindering autonomous ultrasound scanning. To solve this problem, we propose a novel Ultrasound Embodied Intelligence system that equips ultrasound robots with the large language model (LLM) and domain knowledge, thereby improving the efficiency of ultrasound robots. Specifically, we first design an ultrasound operation knowledge database to add expertise in ultrasound scanning to the LLM, enabling the LLM to perform precise motion planning. Furthermore, we devise a dynamic ultrasound scanning strategy based on a \\textit{think-observe-execute} prompt engineering, allowing LLMs to dynamically adjust motion planning strategies during the scanning procedures. Extensive experiments demonstrate that our system significantly improves ultrasound scan efficiency and quality from verbal commands. This advancement in autonomous medical scanning technology contributes to non-invasive diagnostics and streamlined medical workflows.","sentences":["Ultrasound robots are increasingly used in medical diagnostics and early disease screening.","However, current ultrasound robots lack the intelligence to understand human intentions and instructions, hindering autonomous ultrasound scanning.","To solve this problem, we propose a novel Ultrasound Embodied Intelligence system that equips ultrasound robots with the large language model (LLM) and domain knowledge, thereby improving the efficiency of ultrasound robots.","Specifically, we first design an ultrasound operation knowledge database to add expertise in ultrasound scanning to the LLM, enabling the LLM to perform precise motion planning.","Furthermore, we devise a dynamic ultrasound scanning strategy based on a \\textit{think-observe-execute} prompt engineering, allowing LLMs to dynamically adjust motion planning strategies during the scanning procedures.","Extensive experiments demonstrate that our system significantly improves ultrasound scan efficiency and quality from verbal commands.","This advancement in autonomous medical scanning technology contributes to non-invasive diagnostics and streamlined medical workflows."],"url":"http://arxiv.org/abs/2405.00461v1","category":"cs.RO"}
{"created":"2024-05-01 11:36:08","title":"U.S. Election Hardens Hate Universe","abstract":"Local or national politics can trigger potentially dangerous hate in someone. But with a third of the world's population eligible to vote in elections in 2024 alone, we lack understanding of how individual-level hate multiplies up to hate behavior at the collective global scale. Here we show, based on the most recent U.S. election, that offline events are associated with a rapid adaptation of the global online hate universe that hardens (strengthens) both its network-of-networks structure and the 'flavors' of hate content that it collectively produces. Approximately 50 million potential voters in hate communities are drawn closer to each other and to the broad mainstream of approximately 2 billion others. It triggers new hate content at scale around immigration, ethnicity, and antisemitism that aligns with conspiracy theories about Jewish-led replacement before blending in hate around gender identity/sexual orientation, and religion. Telegram acts as a key hardening agent - yet is overlooked by U.S. Congressional hearings and new E.U. legislation. Because the hate universe has remained robust since 2020, anti-hate messaging surrounding not only upcoming elections but also other events like the war in Gaza, should pivot to blending multiple hate 'flavors' while targeting previously untouched social media structures.","sentences":["Local or national politics can trigger potentially dangerous hate in someone.","But with a third of the world's population eligible to vote in elections in 2024 alone, we lack understanding of how individual-level hate multiplies up to hate behavior at the collective global scale.","Here we show, based on the most recent U.S. election, that offline events are associated with a rapid adaptation of the global online hate universe that hardens (strengthens) both its network-of-networks structure and the 'flavors' of hate content that it collectively produces.","Approximately 50 million potential voters in hate communities are drawn closer to each other and to the broad mainstream of approximately 2 billion others.","It triggers new hate content at scale around immigration, ethnicity, and antisemitism that aligns with conspiracy theories about Jewish-led replacement before blending in hate around gender identity/sexual orientation, and religion.","Telegram acts as a key hardening agent - yet is overlooked by U.S. Congressional hearings and new E.U. legislation.","Because the hate universe has remained robust since 2020, anti-hate messaging surrounding not only upcoming elections but also other events like the war in Gaza, should pivot to blending multiple hate 'flavors' while targeting previously untouched social media structures."],"url":"http://arxiv.org/abs/2405.00459v1","category":"cs.SI"}
{"created":"2024-05-01 11:26:31","title":"Counterfactual Explanations for Deep Learning-Based Traffic Forecasting","abstract":"Deep learning models are widely used in traffic forecasting and have achieved state-of-the-art prediction accuracy. However, the black-box nature of those models makes the results difficult to interpret by users. This study aims to leverage an Explainable AI approach, counterfactual explanations, to enhance the explainability and usability of deep learning-based traffic forecasting models. Specifically, the goal is to elucidate relationships between various input contextual features and their corresponding predictions. We present a comprehensive framework that generates counterfactual explanations for traffic forecasting and provides usable insights through the proposed scenario-driven counterfactual explanations. The study first implements a deep learning model to predict traffic speed based on historical traffic data and contextual variables. Counterfactual explanations are then used to illuminate how alterations in these input variables affect predicted outcomes, thereby enhancing the transparency of the deep learning model. We investigated the impact of contextual features on traffic speed prediction under varying spatial and temporal conditions. The scenario-driven counterfactual explanations integrate two types of user-defined constraints, directional and weighting constraints, to tailor the search for counterfactual explanations to specific use cases. These tailored explanations benefit machine learning practitioners who aim to understand the model's learning mechanisms and domain experts who seek insights for real-world applications. The results showcase the effectiveness of counterfactual explanations in revealing traffic patterns learned by deep learning models, showing its potential for interpreting black-box deep learning models used for spatiotemporal predictions in general.","sentences":["Deep learning models are widely used in traffic forecasting and have achieved state-of-the-art prediction accuracy.","However, the black-box nature of those models makes the results difficult to interpret by users.","This study aims to leverage an Explainable AI approach, counterfactual explanations, to enhance the explainability and usability of deep learning-based traffic forecasting models.","Specifically, the goal is to elucidate relationships between various input contextual features and their corresponding predictions.","We present a comprehensive framework that generates counterfactual explanations for traffic forecasting and provides usable insights through the proposed scenario-driven counterfactual explanations.","The study first implements a deep learning model to predict traffic speed based on historical traffic data and contextual variables.","Counterfactual explanations are then used to illuminate how alterations in these input variables affect predicted outcomes, thereby enhancing the transparency of the deep learning model.","We investigated the impact of contextual features on traffic speed prediction under varying spatial and temporal conditions.","The scenario-driven counterfactual explanations integrate two types of user-defined constraints, directional and weighting constraints, to tailor the search for counterfactual explanations to specific use cases.","These tailored explanations benefit machine learning practitioners who aim to understand the model's learning mechanisms and domain experts who seek insights for real-world applications.","The results showcase the effectiveness of counterfactual explanations in revealing traffic patterns learned by deep learning models, showing its potential for interpreting black-box deep learning models used for spatiotemporal predictions in general."],"url":"http://arxiv.org/abs/2405.00456v1","category":"cs.LG"}
{"created":"2024-05-01 11:12:22","title":"Fuzzy Intelligent System for Student Software Project Evaluation","abstract":"Developing software projects allows students to put knowledge into practice and gain teamwork skills. However, assessing student performance in project-oriented courses poses significant challenges, particularly as the size of classes increases. The current paper introduces a fuzzy intelligent system designed to evaluate academic software projects using object-oriented programming and design course as an example. To establish evaluation criteria, we first conducted a survey of student project teams (n=31) and faculty (n=3) to identify key parameters and their applicable ranges. The selected criteria - clean code, use of inheritance, and functionality - were selected as essential for assessing the quality of academic software projects. These criteria were then represented as fuzzy variables with corresponding fuzzy sets. Collaborating with three experts, including one professor and two course instructors, we defined a set of fuzzy rules for a fuzzy inference system. This system processes the input criteria to produce a quantifiable measure of project success. The system demonstrated promising results in automating the evaluation of projects. Our approach standardizes project evaluations and helps to reduce the subjective bias in manual grading.","sentences":["Developing software projects allows students to put knowledge into practice and gain teamwork skills.","However, assessing student performance in project-oriented courses poses significant challenges, particularly as the size of classes increases.","The current paper introduces a fuzzy intelligent system designed to evaluate academic software projects using object-oriented programming and design course as an example.","To establish evaluation criteria, we first conducted a survey of student project teams (n=31) and faculty (n=3) to identify key parameters and their applicable ranges.","The selected criteria - clean code, use of inheritance, and functionality - were selected as essential for assessing the quality of academic software projects.","These criteria were then represented as fuzzy variables with corresponding fuzzy sets.","Collaborating with three experts, including one professor and two course instructors, we defined a set of fuzzy rules for a fuzzy inference system.","This system processes the input criteria to produce a quantifiable measure of project success.","The system demonstrated promising results in automating the evaluation of projects.","Our approach standardizes project evaluations and helps to reduce the subjective bias in manual grading."],"url":"http://arxiv.org/abs/2405.00453v1","category":"cs.CY"}
{"created":"2024-05-01 11:10:24","title":"Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning","abstract":"We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the critical importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and SciQ, with substantial percentage increases in accuracy to $80.7\\%$ (+$4.8\\%$), $32.2\\%$ (+$3.3\\%$), and $88.5\\%$ (+$7.7\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains.","sentences":["We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero.","Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals.","To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data.","The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data.","Theoretical analysis reveals the critical importance of using on-policy sampled data for successful self-improving.","Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models.","For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and SciQ, with substantial percentage increases in accuracy to $80.7\\%$ (+$4.8\\%$), $32.2\\%$ (+$3.3\\%$), and $88.5\\%$ (+$7.7\\%$), respectively.","Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains."],"url":"http://arxiv.org/abs/2405.00451v1","category":"cs.AI"}
{"created":"2024-05-01 11:08:26","title":"Quantum Global Minimum Finder based on Variational Quantum Search","abstract":"The search for global minima is a critical challenge across multiple fields including engineering, finance, and artificial intelligence, particularly with non-convex functions that feature multiple local optima, complicating optimization efforts. We introduce the Quantum Global Minimum Finder (QGMF), an innovative quantum computing approach that efficiently identifies global minima. QGMF combines binary search techniques to shift the objective function to a suitable position and then employs Variational Quantum Search to precisely locate the global minimum within this targeted subspace. Designed with a low-depth circuit architecture, QGMF is optimized for Noisy Intermediate-Scale Quantum (NISQ) devices, utilizing the logarithmic benefits of binary search to enhance scalability and efficiency. This work demonstrates the impact of QGMF in advancing the capabilities of quantum computing to overcome complex non-convex optimization challenges effectively.","sentences":["The search for global minima is a critical challenge across multiple fields including engineering, finance, and artificial intelligence, particularly with non-convex functions that feature multiple local optima, complicating optimization efforts.","We introduce the Quantum Global Minimum Finder (QGMF), an innovative quantum computing approach that efficiently identifies global minima.","QGMF combines binary search techniques to shift the objective function to a suitable position and then employs Variational Quantum Search to precisely locate the global minimum within this targeted subspace.","Designed with a low-depth circuit architecture, QGMF is optimized for Noisy Intermediate-Scale Quantum (NISQ) devices, utilizing the logarithmic benefits of binary search to enhance scalability and efficiency.","This work demonstrates the impact of QGMF in advancing the capabilities of quantum computing to overcome complex non-convex optimization challenges effectively."],"url":"http://arxiv.org/abs/2405.00450v1","category":"quant-ph"}
{"created":"2024-05-01 11:06:31","title":"RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models","abstract":"Prediction of road users' behaviors in the context of autonomous driving has gained considerable attention by the scientific community in the last years. Most works focus on predicting behaviors based on kinematic information alone, a simplification of the reality since road users are humans, and as such they are highly influenced by their surrounding context. In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans. In this work, we propose an explainable road users' behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph as well as on current evidence gathered in real time by onboard sensors. Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians' crossing actions; 2) Prediction of lane change maneuvers. In both cases, the performance attained surpasses the current state of the art in terms of anticipation and F1-score, showing a promising avenue for future research in this field.","sentences":["Prediction of road users' behaviors in the context of autonomous driving has gained considerable attention by the scientific community in the last years.","Most works focus on predicting behaviors based on kinematic information alone, a simplification of the reality since road users are humans, and as such they are highly influenced by their surrounding context.","In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans.","In this work, we propose an explainable road users' behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques.","For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph as well as on current evidence gathered in real time by onboard sensors.","Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians' crossing actions; 2) Prediction of lane change maneuvers.","In both cases, the performance attained surpasses the current state of the art in terms of anticipation and F1-score, showing a promising avenue for future research in this field."],"url":"http://arxiv.org/abs/2405.00449v1","category":"cs.LG"}
{"created":"2024-05-01 10:53:54","title":"Geometric Insights into Focal Loss: Reducing Curvature for Enhanced Model Calibration","abstract":"The key factor in implementing machine learning algorithms in decision-making situations is not only the accuracy of the model but also its confidence level. The confidence level of a model in a classification problem is often given by the output vector of a softmax function for convenience. However, these values are known to deviate significantly from the actual expected model confidence. This problem is called model calibration and has been studied extensively. One of the simplest techniques to tackle this task is focal loss, a generalization of cross-entropy by introducing one positive parameter. Although many related studies exist because of the simplicity of the idea and its formalization, the theoretical analysis of its behavior is still insufficient. In this study, our objective is to understand the behavior of focal loss by reinterpreting this function geometrically. Our analysis suggests that focal loss reduces the curvature of the loss surface in training the model. This indicates that curvature may be one of the essential factors in achieving model calibration. We design numerical experiments to support this conjecture to reveal the behavior of focal loss and the relationship between calibration performance and curvature.","sentences":["The key factor in implementing machine learning algorithms in decision-making situations is not only the accuracy of the model but also its confidence level.","The confidence level of a model in a classification problem is often given by the output vector of a softmax function for convenience.","However, these values are known to deviate significantly from the actual expected model confidence.","This problem is called model calibration and has been studied extensively.","One of the simplest techniques to tackle this task is focal loss, a generalization of cross-entropy by introducing one positive parameter.","Although many related studies exist because of the simplicity of the idea and its formalization, the theoretical analysis of its behavior is still insufficient.","In this study, our objective is to understand the behavior of focal loss by reinterpreting this function geometrically.","Our analysis suggests that focal loss reduces the curvature of the loss surface in training the model.","This indicates that curvature may be one of the essential factors in achieving model calibration.","We design numerical experiments to support this conjecture to reveal the behavior of focal loss and the relationship between calibration performance and curvature."],"url":"http://arxiv.org/abs/2405.00442v1","category":"stat.ML"}
{"created":"2024-05-01 10:33:36","title":"Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models","abstract":"Activity and parameter sparsity are two standard methods of making neural networks computationally more efficient. Event-based architectures such as spiking neural networks (SNNs) naturally exhibit activity sparsity, and many methods exist to sparsify their connectivity by pruning weights. While the effect of weight pruning on feed-forward SNNs has been previously studied for computer vision tasks, the effects of pruning for complex sequence tasks like language modeling are less well studied since SNNs have traditionally struggled to achieve meaningful performance on these tasks. Using a recently published SNN-like architecture that works well on small-scale language modeling, we study the effects of weight pruning when combined with activity sparsity. Specifically, we study the trade-off between the multiplicative efficiency gains the combination affords and its effect on task performance for language modeling. To dissect the effects of the two sparsities, we conduct a comparative analysis between densely activated models and sparsely activated event-based models across varying degrees of connectivity sparsity. We demonstrate that sparse activity and sparse connectivity complement each other without a proportional drop in task performance for an event-based neural network trained on the Penn Treebank and WikiText-2 language modeling datasets. Our results suggest sparsely connected event-based neural networks are promising candidates for effective and efficient sequence modeling.","sentences":["Activity and parameter sparsity are two standard methods of making neural networks computationally more efficient.","Event-based architectures such as spiking neural networks (SNNs) naturally exhibit activity sparsity, and many methods exist to sparsify their connectivity by pruning weights.","While the effect of weight pruning on feed-forward SNNs has been previously studied for computer vision tasks, the effects of pruning for complex sequence tasks like language modeling are less well studied since SNNs have traditionally struggled to achieve meaningful performance on these tasks.","Using a recently published SNN-like architecture that works well on small-scale language modeling, we study the effects of weight pruning when combined with activity sparsity.","Specifically, we study the trade-off between the multiplicative efficiency gains the combination affords and its effect on task performance for language modeling.","To dissect the effects of the two sparsities, we conduct a comparative analysis between densely activated models and sparsely activated event-based models across varying degrees of connectivity sparsity.","We demonstrate that sparse activity and sparse connectivity complement each other without a proportional drop in task performance for an event-based neural network trained on the Penn Treebank and WikiText-2 language modeling datasets.","Our results suggest sparsely connected event-based neural networks are promising candidates for effective and efficient sequence modeling."],"url":"http://arxiv.org/abs/2405.00433v1","category":"cs.LG"}
{"created":"2024-05-01 09:58:57","title":"Self-supervised Pre-training of Text Recognizers","abstract":"In this paper, we investigate self-supervised pre-training methods for document text recognition. Nowadays, large unlabeled datasets can be collected for many research tasks, including text recognition, but it is costly to annotate them. Therefore, methods utilizing unlabeled data are researched. We study self-supervised pre-training methods based on masked label prediction using three different approaches -- Feature Quantization, VQ-VAE, and Post-Quantized AE. We also investigate joint-embedding approaches with VICReg and NT-Xent objectives, for which we propose an image shifting technique to prevent model collapse where it relies solely on positional encoding while completely ignoring the input image. We perform our experiments on historical handwritten (Bentham) and historical printed datasets mainly to investigate the benefits of the self-supervised pre-training techniques with different amounts of annotated target domain data. We use transfer learning as strong baselines. The evaluation shows that the self-supervised pre-training on data from the target domain is very effective, but it struggles to outperform transfer learning from closely related domains. This paper is one of the first researches exploring self-supervised pre-training in document text recognition, and we believe that it will become a cornerstone for future research in this area. We made our implementation of the investigated methods publicly available at https://github.com/DCGM/pero-pretraining.","sentences":["In this paper, we investigate self-supervised pre-training methods for document text recognition.","Nowadays, large unlabeled datasets can be collected for many research tasks, including text recognition, but it is costly to annotate them.","Therefore, methods utilizing unlabeled data are researched.","We study self-supervised pre-training methods based on masked label prediction using three different approaches -- Feature Quantization, VQ-VAE, and Post-Quantized AE.","We also investigate joint-embedding approaches with VICReg and NT-Xent objectives, for which we propose an image shifting technique to prevent model collapse where it relies solely on positional encoding while completely ignoring the input image.","We perform our experiments on historical handwritten (Bentham) and historical printed datasets mainly to investigate the benefits of the self-supervised pre-training techniques with different amounts of annotated target domain data.","We use transfer learning as strong baselines.","The evaluation shows that the self-supervised pre-training on data from the target domain is very effective, but it struggles to outperform transfer learning from closely related domains.","This paper is one of the first researches exploring self-supervised pre-training in document text recognition, and we believe that it will become a cornerstone for future research in this area.","We made our implementation of the investigated methods publicly available at https://github.com/DCGM/pero-pretraining."],"url":"http://arxiv.org/abs/2405.00420v1","category":"cs.CV"}
{"created":"2024-05-01 09:57:34","title":"Detection of ransomware attacks using federated learning based on the CNN model","abstract":"Computing is still under a significant threat from ransomware, which necessitates prompt action to prevent it. Ransomware attacks can have a negative impact on how smart grids, particularly digital substations. In addition to examining a ransomware detection method using artificial intelligence (AI), this paper offers a ransomware attack modeling technique that targets the disrupted operation of a digital substation. The first, binary data is transformed into image data and fed into the convolution neural network model using federated learning. The experimental findings demonstrate that the suggested technique detects ransomware with a high accuracy rate.","sentences":["Computing is still under a significant threat from ransomware, which necessitates prompt action to prevent it.","Ransomware attacks can have a negative impact on how smart grids, particularly digital substations.","In addition to examining a ransomware detection method using artificial intelligence (AI), this paper offers a ransomware attack modeling technique that targets the disrupted operation of a digital substation.","The first, binary data is transformed into image data and fed into the convolution neural network model using federated learning.","The experimental findings demonstrate that the suggested technique detects ransomware with a high accuracy rate."],"url":"http://arxiv.org/abs/2405.00418v1","category":"cs.CR"}
{"created":"2024-05-01 09:55:31","title":"Conformal Risk Control for Ordinal Classification","abstract":"As a natural extension to the standard conformal prediction method, several conformal risk control methods have been recently developed and applied to various learning problems. In this work, we seek to control the conformal risk in expectation for ordinal classification tasks, which have broad applications to many real problems. For this purpose, we firstly formulated the ordinal classification task in the conformal risk control framework, and provided theoretic risk bounds of the risk control method. Then we proposed two types of loss functions specially designed for ordinal classification tasks, and developed corresponding algorithms to determine the prediction set for each case to control their risks at a desired level. We demonstrated the effectiveness of our proposed methods, and analyzed the difference between the two types of risks on three different datasets, including a simulated dataset, the UTKFace dataset and the diabetic retinopathy detection dataset.","sentences":["As a natural extension to the standard conformal prediction method, several conformal risk control methods have been recently developed and applied to various learning problems.","In this work, we seek to control the conformal risk in expectation for ordinal classification tasks, which have broad applications to many real problems.","For this purpose, we firstly formulated the ordinal classification task in the conformal risk control framework, and provided theoretic risk bounds of the risk control method.","Then we proposed two types of loss functions specially designed for ordinal classification tasks, and developed corresponding algorithms to determine the prediction set for each case to control their risks at a desired level.","We demonstrated the effectiveness of our proposed methods, and analyzed the difference between the two types of risks on three different datasets, including a simulated dataset, the UTKFace dataset and the diabetic retinopathy detection dataset."],"url":"http://arxiv.org/abs/2405.00417v1","category":"cs.LG"}
{"created":"2024-05-01 09:27:49","title":"Compressive Sensing Imaging Using Caustic Lens Mask Generated by Periodic Perturbation in a Ripple Tank","abstract":"Terahertz imaging shows significant potential across diverse fields, yet the cost-effectiveness of multi-pixel imaging equipment remains an obstacle for many researchers. To tackle this issue, the utilization of single-pixel imaging arises as a lower-cost option, however, the data collection process necessary for reconstructing images is time-consuming. Compressive Sensing offers a promising solution by enabling image generation with fewer measurements than required by Nyquist's theorem, yet long processing times remain an issue, especially for large-sized images. Our proposed solution to this issue involves using caustic lens effect induced by perturbations in a ripple tank as a sampling mask. The dynamic characteristics of the ripple tank introduce randomness into the sampling process, thereby reducing measurement time through exploitation of the inherent sparsity of THz band signals. In this study, a Convolutional Neural Network was used to conduct target classification, based on the distinctive signal patterns obtained via the caustic lens mask. The suggested classifier obtained a 95.16 % accuracy rate in differentiating targets resembling Latin letters.","sentences":["Terahertz imaging shows significant potential across diverse fields, yet the cost-effectiveness of multi-pixel imaging equipment remains an obstacle for many researchers.","To tackle this issue, the utilization of single-pixel imaging arises as a lower-cost option, however, the data collection process necessary for reconstructing images is time-consuming.","Compressive Sensing offers a promising solution by enabling image generation with fewer measurements than required by Nyquist's theorem, yet long processing times remain an issue, especially for large-sized images.","Our proposed solution to this issue involves using caustic lens effect induced by perturbations in a ripple tank as a sampling mask.","The dynamic characteristics of the ripple tank introduce randomness into the sampling process, thereby reducing measurement time through exploitation of the inherent sparsity of THz band signals.","In this study, a Convolutional Neural Network was used to conduct target classification, based on the distinctive signal patterns obtained via the caustic lens mask.","The suggested classifier obtained a 95.16 % accuracy rate in differentiating targets resembling Latin letters."],"url":"http://arxiv.org/abs/2405.00407v1","category":"eess.SP"}
{"created":"2024-05-01 08:50:08","title":"Trust Driven On-Demand Scheme for Client Deployment in Federated Learning","abstract":"Containerization technology plays a crucial role in Federated Learning (FL) setups, expanding the pool of potential clients and ensuring the availability of specific subsets for each learning iteration. However, doubts arise about the trustworthiness of devices deployed as clients in FL scenarios, especially when container deployment processes are involved. Addressing these challenges is important, particularly in managing potentially malicious clients capable of disrupting the learning process or compromising the entire model. In our research, we are motivated to integrate a trust element into the client selection and model deployment processes within our system architecture. This is a feature lacking in the initial client selection and deployment mechanism of the On-Demand architecture. We introduce a trust mechanism, named \"Trusted-On-Demand-FL\", which establishes a relationship of trust between the server and the pool of eligible clients. Utilizing Docker in our deployment strategy enables us to monitor and validate participant actions effectively, ensuring strict adherence to agreed-upon protocols while strengthening defenses against unauthorized data access or tampering. Our simulations rely on a continuous user behavior dataset, deploying an optimization model powered by a genetic algorithm to efficiently select clients for participation. By assigning trust values to individual clients and dynamically adjusting these values, combined with penalizing malicious clients through decreased trust scores, our proposed framework identifies and isolates harmful clients. This approach not only reduces disruptions to regular rounds but also minimizes instances of round dismissal, Consequently enhancing both system stability and security.","sentences":["Containerization technology plays a crucial role in Federated Learning (FL) setups, expanding the pool of potential clients and ensuring the availability of specific subsets for each learning iteration.","However, doubts arise about the trustworthiness of devices deployed as clients in FL scenarios, especially when container deployment processes are involved.","Addressing these challenges is important, particularly in managing potentially malicious clients capable of disrupting the learning process or compromising the entire model.","In our research, we are motivated to integrate a trust element into the client selection and model deployment processes within our system architecture.","This is a feature lacking in the initial client selection and deployment mechanism of the On-Demand architecture.","We introduce a trust mechanism, named \"Trusted-On-Demand-FL\", which establishes a relationship of trust between the server and the pool of eligible clients.","Utilizing Docker in our deployment strategy enables us to monitor and validate participant actions effectively, ensuring strict adherence to agreed-upon protocols while strengthening defenses against unauthorized data access or tampering.","Our simulations rely on a continuous user behavior dataset, deploying an optimization model powered by a genetic algorithm to efficiently select clients for participation.","By assigning trust values to individual clients and dynamically adjusting these values, combined with penalizing malicious clients through decreased trust scores, our proposed framework identifies and isolates harmful clients.","This approach not only reduces disruptions to regular rounds but also minimizes instances of round dismissal, Consequently enhancing both system stability and security."],"url":"http://arxiv.org/abs/2405.00395v1","category":"cs.CR"}
{"created":"2024-05-01 08:49:22","title":"Enhancing Mutual Trustworthiness in Federated Learning for Data-Rich Smart Cities","abstract":"Federated learning is a promising collaborative and privacy-preserving machine learning approach in data-rich smart cities. Nevertheless, the inherent heterogeneity of these urban environments presents a significant challenge in selecting trustworthy clients for collaborative model training. The usage of traditional approaches, such as the random client selection technique, poses several threats to the system's integrity due to the possibility of malicious client selection. Primarily, the existing literature focuses on assessing the trustworthiness of clients, neglecting the crucial aspect of trust in federated servers. To bridge this gap, in this work, we propose a novel framework that addresses the mutual trustworthiness in federated learning by considering the trust needs of both the client and the server. Our approach entails: (1) Creating preference functions for servers and clients, allowing them to rank each other based on trust scores, (2) Establishing a reputation-based recommendation system leveraging multiple clients to assess newly connected servers, (3) Assigning credibility scores to recommending devices for better server trustworthiness measurement, (4) Developing a trust assessment mechanism for smart devices using a statistical Interquartile Range (IQR) method, (5) Designing intelligent matching algorithms considering the preferences of both parties. Based on simulation and experimental results, our approach outperforms baseline methods by increasing trust levels, global model accuracy, and reducing non-trustworthy clients in the system.","sentences":["Federated learning is a promising collaborative and privacy-preserving machine learning approach in data-rich smart cities.","Nevertheless, the inherent heterogeneity of these urban environments presents a significant challenge in selecting trustworthy clients for collaborative model training.","The usage of traditional approaches, such as the random client selection technique, poses several threats to the system's integrity due to the possibility of malicious client selection.","Primarily, the existing literature focuses on assessing the trustworthiness of clients, neglecting the crucial aspect of trust in federated servers.","To bridge this gap, in this work, we propose a novel framework that addresses the mutual trustworthiness in federated learning by considering the trust needs of both the client and the server.","Our approach entails: (1) Creating preference functions for servers and clients, allowing them to rank each other based on trust scores, (2) Establishing a reputation-based recommendation system leveraging multiple clients to assess newly connected servers, (3)","Assigning credibility scores to recommending devices for better server trustworthiness measurement, (4) Developing a trust assessment mechanism for smart devices using a statistical Interquartile Range (IQR) method, (5) Designing intelligent matching algorithms considering the preferences of both parties.","Based on simulation and experimental results, our approach outperforms baseline methods by increasing trust levels, global model accuracy, and reducing non-trustworthy clients in the system."],"url":"http://arxiv.org/abs/2405.00394v1","category":"cs.GT"}
{"created":"2024-05-01 08:45:57","title":"Certified Adversarial Robustness of Machine Learning-based Malware Detectors via (De)Randomized Smoothing","abstract":"Deep learning-based malware detection systems are vulnerable to adversarial EXEmples - carefully-crafted malicious programs that evade detection with minimal perturbation. As such, the community is dedicating effort to develop mechanisms to defend against adversarial EXEmples. However, current randomized smoothing-based defenses are still vulnerable to attacks that inject blocks of adversarial content. In this paper, we introduce a certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist. Our method is inspired by (de)randomized smoothing which provides deterministic robustness certificates. During training, a base classifier is trained using subsets of continguous bytes. At inference time, our defense splits the executable into non-overlapping chunks, classifies each chunk independently, and computes the final prediction through majority voting to minimize the influence of injected content. Furthermore, we introduce a preprocessing step that fixes the size of the sections and headers to a multiple of the chunk size. As a consequence, the injected content is confined to an integer number of chunks without tampering the other chunks containing the real bytes of the input examples, allowing us to extend our certified robustness guarantees to content insertion attacks. We perform an extensive ablation study, by comparing our defense with randomized smoothing-based defenses against a plethora of content manipulation attacks and neural network architectures. Results show that our method exhibits unmatched robustness against strong content-insertion attacks, outperforming randomized smoothing-based defenses in the literature.","sentences":["Deep learning-based malware detection systems are vulnerable to adversarial EXEmples - carefully-crafted malicious programs that evade detection with minimal perturbation.","As such, the community is dedicating effort to develop mechanisms to defend against adversarial EXEmples.","However, current randomized smoothing-based defenses are still vulnerable to attacks that inject blocks of adversarial content.","In this paper, we introduce a certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist.","Our method is inspired by (de)randomized smoothing which provides deterministic robustness certificates.","During training, a base classifier is trained using subsets of continguous bytes.","At inference time, our defense splits the executable into non-overlapping chunks, classifies each chunk independently, and computes the final prediction through majority voting to minimize the influence of injected content.","Furthermore, we introduce a preprocessing step that fixes the size of the sections and headers to a multiple of the chunk size.","As a consequence, the injected content is confined to an integer number of chunks without tampering the other chunks containing the real bytes of the input examples, allowing us to extend our certified robustness guarantees to content insertion attacks.","We perform an extensive ablation study, by comparing our defense with randomized smoothing-based defenses against a plethora of content manipulation attacks and neural network architectures.","Results show that our method exhibits unmatched robustness against strong content-insertion attacks, outperforming randomized smoothing-based defenses in the literature."],"url":"http://arxiv.org/abs/2405.00392v1","category":"cs.CR"}
{"created":"2024-05-01 08:44:50","title":"Beamforming Inferring by Conditional WGAN-GP for Holographic Antenna Arrays","abstract":"The beamforming technology with large holographic antenna arrays is one of the key enablers for the next generation of wireless systems, which can significantly improve the spectral efficiency. However, the deployment of large antenna arrays implies high algorithm complexity and resource overhead at both receiver and transmitter ends. To address this issue, advanced technologies such as artificial intelligence have been developed to reduce beamforming overhead. Intuitively, if we can implement the near-optimal beamforming only using a tiny subset of the all channel information, the overhead for channel estimation and beamforming would be reduced significantly compared with the traditional beamforming methods that usually need full channel information and the inversion of large dimensional matrix. In light of this idea, we propose a novel scheme that utilizes Wasserstein generative adversarial network with gradient penalty to infer the full beamforming matrices based on very little of channel information. Simulation results confirm that it can accomplish comparable performance with the weighted minimum mean-square error algorithm, while reducing the overhead by over 50%.","sentences":["The beamforming technology with large holographic antenna arrays is one of the key enablers for the next generation of wireless systems, which can significantly improve the spectral efficiency.","However, the deployment of large antenna arrays implies high algorithm complexity and resource overhead at both receiver and transmitter ends.","To address this issue, advanced technologies such as artificial intelligence have been developed to reduce beamforming overhead.","Intuitively, if we can implement the near-optimal beamforming only using a tiny subset of the all channel information, the overhead for channel estimation and beamforming would be reduced significantly compared with the traditional beamforming methods that usually need full channel information and the inversion of large dimensional matrix.","In light of this idea, we propose a novel scheme that utilizes Wasserstein generative adversarial network with gradient penalty to infer the full beamforming matrices based on very little of channel information.","Simulation results confirm that it can accomplish comparable performance with the weighted minimum mean-square error algorithm, while reducing the overhead by over 50%."],"url":"http://arxiv.org/abs/2405.00391v1","category":"cs.IT"}
{"created":"2024-05-01 08:42:22","title":"Employing Federated Learning for Training Autonomous HVAC Systems","abstract":"Buildings account for 40 % of global energy consumption. A considerable portion of building energy consumption stems from heating, ventilation, and air conditioning (HVAC), and thus implementing smart, energy-efficient HVAC systems has the potential to significantly impact the course of climate change. In recent years, model-free reinforcement learning algorithms have been increasingly assessed for this purpose due to their ability to learn and adapt purely from experience. They have been shown to outperform classical controllers in terms of energy cost and consumption, as well as thermal comfort. However, their weakness lies in their relatively poor data efficiency, requiring long periods of training to reach acceptable policies, making them inapplicable to real-world controllers directly. Hence, common research goals are to improve the learning speed, as well as to improve their ability to generalize, in order to facilitate transfer learning to unseen building environments. In this paper, we take a federated learning approach to training the reinforcement learning controller of an HVAC system. A global control policy is learned by aggregating local policies trained on multiple data centers located in different climate zones. The goal of the policy is to simultaneously minimize energy consumption and maximize thermal comfort. The federated optimization strategy indirectly increases both the rate at which experience data is collected and the variation in the data. We demonstrate through experimental evaluation that these effects lead to a faster learning speed, as well as greater generalization capabilities in the federated policy compared to any individually trained policy.","sentences":["Buildings account for 40 % of global energy consumption.","A considerable portion of building energy consumption stems from heating, ventilation, and air conditioning (HVAC), and thus implementing smart, energy-efficient HVAC systems has the potential to significantly impact the course of climate change.","In recent years, model-free reinforcement learning algorithms have been increasingly assessed for this purpose due to their ability to learn and adapt purely from experience.","They have been shown to outperform classical controllers in terms of energy cost and consumption, as well as thermal comfort.","However, their weakness lies in their relatively poor data efficiency, requiring long periods of training to reach acceptable policies, making them inapplicable to real-world controllers directly.","Hence, common research goals are to improve the learning speed, as well as to improve their ability to generalize, in order to facilitate transfer learning to unseen building environments.","In this paper, we take a federated learning approach to training the reinforcement learning controller of an HVAC system.","A global control policy is learned by aggregating local policies trained on multiple data centers located in different climate zones.","The goal of the policy is to simultaneously minimize energy consumption and maximize thermal comfort.","The federated optimization strategy indirectly increases both the rate at which experience data is collected and the variation in the data.","We demonstrate through experimental evaluation that these effects lead to a faster learning speed, as well as greater generalization capabilities in the federated policy compared to any individually trained policy."],"url":"http://arxiv.org/abs/2405.00389v1","category":"math.OC"}
{"created":"2024-05-01 17:54:05","title":"RGB$\\leftrightarrow$X: Image decomposition and synthesis using material- and lighting-aware diffusion models","abstract":"The three areas of realistic forward rendering, per-pixel inverse rendering, and generative image synthesis may seem like separate and unrelated sub-fields of graphics and vision. However, recent work has demonstrated improved estimation of per-pixel intrinsic channels (albedo, roughness, metallicity) based on a diffusion architecture; we call this the RGB$\\rightarrow$X problem. We further show that the reverse problem of synthesizing realistic images given intrinsic channels, X$\\rightarrow$RGB, can also be addressed in a diffusion framework.   Focusing on the image domain of interior scenes, we introduce an improved diffusion model for RGB$\\rightarrow$X, which also estimates lighting, as well as the first diffusion X$\\rightarrow$RGB model capable of synthesizing realistic images from (full or partial) intrinsic channels. Our X$\\rightarrow$RGB model explores a middle ground between traditional rendering and generative models: we can specify only certain appearance properties that should be followed, and give freedom to the model to hallucinate a plausible version of the rest.   This flexibility makes it possible to use a mix of heterogeneous training datasets, which differ in the available channels. We use multiple existing datasets and extend them with our own synthetic and real data, resulting in a model capable of extracting scene properties better than previous work and of generating highly realistic images of interior scenes.","sentences":["The three areas of realistic forward rendering, per-pixel inverse rendering, and generative image synthesis may seem like separate and unrelated sub-fields of graphics and vision.","However, recent work has demonstrated improved estimation of per-pixel intrinsic channels (albedo, roughness, metallicity) based on a diffusion architecture; we call this the RGB$\\rightarrow$X problem.","We further show that the reverse problem of synthesizing realistic images given intrinsic channels, X$\\rightarrow$RGB, can also be addressed in a diffusion framework.   ","Focusing on the image domain of interior scenes, we introduce an improved diffusion model for RGB$\\rightarrow$X, which also estimates lighting, as well as the first diffusion X$\\rightarrow$RGB model capable of synthesizing realistic images from (full or partial) intrinsic channels.","Our X$\\rightarrow$RGB model explores a middle ground between traditional rendering and generative models: we can specify only certain appearance properties that should be followed, and give freedom to the model to hallucinate a plausible version of the rest.   ","This flexibility makes it possible to use a mix of heterogeneous training datasets, which differ in the available channels.","We use multiple existing datasets and extend them with our own synthetic and real data, resulting in a model capable of extracting scene properties better than previous work and of generating highly realistic images of interior scenes."],"url":"http://arxiv.org/abs/2405.00666v1","category":"cs.CV"}
{"created":"2024-05-01 17:50:16","title":"No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO","abstract":"Reinforcement learning (RL) is inherently rife with non-stationarity since the states and rewards the agent observes during training depend on its changing policy. Therefore, networks in deep RL must be capable of adapting to new observations and fitting new targets. However, previous works have observed that networks in off-policy deep value-based methods exhibit a decrease in representation rank, often correlated with an inability to continue learning or a collapse in performance. Although this phenomenon has generally been attributed to neural network learning under non-stationarity, it has been overlooked in on-policy policy optimization methods which are often thought capable of training indefinitely. In this work, we empirically study representation dynamics in Proximal Policy Optimization (PPO) on the Atari and MuJoCo environments, revealing that PPO agents are also affected by feature rank deterioration and loss of plasticity. We show that this is aggravated with stronger non-stationarity, ultimately driving the actor's performance to collapse, regardless of the performance of the critic. We draw connections between representation collapse, performance collapse, and trust region issues in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary loss, that along with other interventions shows that regularizing the representation dynamics improves the performance of PPO agents.","sentences":["Reinforcement learning (RL) is inherently rife with non-stationarity since the states and rewards the agent observes during training depend on its changing policy.","Therefore, networks in deep RL must be capable of adapting to new observations and fitting new targets.","However, previous works have observed that networks in off-policy deep value-based methods exhibit a decrease in representation rank, often correlated with an inability to continue learning or a collapse in performance.","Although this phenomenon has generally been attributed to neural network learning under non-stationarity, it has been overlooked in on-policy policy optimization methods which are often thought capable of training indefinitely.","In this work, we empirically study representation dynamics in Proximal Policy Optimization (PPO) on the Atari and MuJoCo environments, revealing that PPO agents are also affected by feature rank deterioration and loss of plasticity.","We show that this is aggravated with stronger non-stationarity, ultimately driving the actor's performance to collapse, regardless of the performance of the critic.","We draw connections between representation collapse, performance collapse, and trust region issues in PPO, and present Proximal Feature Optimization (PFO), a novel auxiliary loss, that along with other interventions shows that regularizing the representation dynamics improves the performance of PPO agents."],"url":"http://arxiv.org/abs/2405.00662v1","category":"cs.LG"}
{"created":"2024-05-01 17:47:44","title":"Connecting Infinity to Soft Factors","abstract":"In this note we study tree-level scattering amplitudes of gravitons under a natural deformation which in the large $z$ limit can be interpreted either as a $k$-hard-particle limit or as a $(n-k)$-soft-particle limit. When $k=2$ this becomes the standard BCFW deformation while for $k=3$ it leads to the Risager deformation. The hard- to soft-limit map we define motivates a way of computing the leading order behavior of amplitudes for large $z$ directly from soft limits. We check the proposal by applying the $k=3$ and $k=4$ versions to NMHV and N$^2$MHV gravity amplitudes respectively. The former reproduces in a few lines the result recently obtained by using CHY-like techniques in \\cite{BCL}. The N$^2$MHV formula is also remarkably simple and we give support for it using a CHY-like computation. In the $k=2$ case applied to any gravity amplitude, the multiple soft-limit analysis reproduces the correct ${\\cal O}(z^{-2})$ behavior while explicitly showing the source of the mysterious cancellation among Feynman diagrams that tames the behavior from the ${\\cal O}(z^{n-5})$ of individual Feynman diagrams down to the ${\\cal O}(z^{-2})$ of the amplitude.","sentences":["In this note we study tree-level scattering amplitudes of gravitons under a natural deformation which in the large $z$ limit can be interpreted either as a $k$-hard-particle limit or as a $(n-k)$-soft-particle limit.","When $k=2$ this becomes the standard BCFW deformation while for $k=3$ it leads to the Risager deformation.","The hard- to soft-limit map we define motivates a way of computing the leading order behavior of amplitudes for large $z$ directly from soft limits.","We check the proposal by applying the $k=3$ and $k=4$ versions to NMHV and N$^2$MHV gravity amplitudes respectively.","The former reproduces in a few lines the result recently obtained by using CHY-like techniques in \\cite{BCL}.","The N$^2$MHV formula is also remarkably simple and we give support for it using a CHY-like computation.","In the $k=2$ case applied to any gravity amplitude, the multiple soft-limit analysis reproduces the correct ${\\cal O}(z^{-2})$ behavior while explicitly showing the source of the mysterious cancellation among Feynman diagrams that tames the behavior from the ${\\cal O}(z^{n-5})$ of individual Feynman diagrams down to the ${\\cal O}(z^{-2})$ of the amplitude."],"url":"http://arxiv.org/abs/2405.00660v1","category":"hep-th"}
{"created":"2024-05-01 17:34:32","title":"Linearly simplified QAOA parameters and transferability","abstract":"Quantum Approximate Optimization Algorithm (QAOA) provides a way to solve combinatorial optimization problems using quantum computers. QAOA circuits consist of time evolution operators by the cost Hamiltonian and of state mixing operators, and embedded variational parameter for each operator is tuned so that the expectation value of the cost function is minimized. The optimization of the variational parameters is taken place on classical devices while the cost function is measured in the sense of quantum. To facilitate the classical optimization, there are several previous works on making decision strategies for optimal/initial parameters and on extracting similarities among instances. In our current work, we consider simplified QAOA parameters that take linear forms along with the depth in the circuit. Such a simplification, which would be suggested from an analogy to quantum annealing, leads to a drastic reduction of the parameter space from 2p to 4 dimensions with the any number of QAOA layers p. In addition, cost landscapes in the reduced parameter space have some stability on differing instances. This fact suggests that an optimal parameter set for a given instance can be transferred to other instances. In this paper we present some numerical results that are obtained for instances of the random Ising model and of the max-cut problem. The transferability of linearized parameters is demonstrated for randomly generated source and destination instances, and its dependence on features of the instances are investigated.","sentences":["Quantum Approximate Optimization Algorithm (QAOA) provides a way to solve combinatorial optimization problems using quantum computers.","QAOA circuits consist of time evolution operators by the cost Hamiltonian and of state mixing operators, and embedded variational parameter for each operator is tuned so that the expectation value of the cost function is minimized.","The optimization of the variational parameters is taken place on classical devices while the cost function is measured in the sense of quantum.","To facilitate the classical optimization, there are several previous works on making decision strategies for optimal/initial parameters and on extracting similarities among instances.","In our current work, we consider simplified QAOA parameters that take linear forms along with the depth in the circuit.","Such a simplification, which would be suggested from an analogy to quantum annealing, leads to a drastic reduction of the parameter space from 2p to 4 dimensions with the any number of QAOA layers p.","In addition, cost landscapes in the reduced parameter space have some stability on differing instances.","This fact suggests that an optimal parameter set for a given instance can be transferred to other instances.","In this paper we present some numerical results that are obtained for instances of the random Ising model and of the max-cut problem.","The transferability of linearized parameters is demonstrated for randomly generated source and destination instances, and its dependence on features of the instances are investigated."],"url":"http://arxiv.org/abs/2405.00655v1","category":"quant-ph"}
{"created":"2024-05-01 17:10:55","title":"From Empirical Observations to Universality: Dynamics of Deep Learning with Inputs Built on Gaussian mixture","abstract":"This study broadens the scope of theoretical frameworks in deep learning by delving into the dynamics of neural networks with inputs that demonstrate the structural characteristics to Gaussian Mixture (GM). We analyzed how the dynamics of neural networks under GM-structured inputs diverge from the predictions of conventional theories based on simple Gaussian structures. A revelation of our work is the observed convergence of neural network dynamics towards conventional theory even with standardized GM inputs, highlighting an unexpected universality. We found that standardization, especially in conjunction with certain nonlinear functions, plays a critical role in this phenomena. Consequently, despite the complex and varied nature of GM distributions, we demonstrate that neural networks exhibit asymptotic behaviors in line with predictions under simple Gaussian frameworks.","sentences":["This study broadens the scope of theoretical frameworks in deep learning by delving into the dynamics of neural networks with inputs that demonstrate the structural characteristics to Gaussian Mixture (GM).","We analyzed how the dynamics of neural networks under GM-structured inputs diverge from the predictions of conventional theories based on simple Gaussian structures.","A revelation of our work is the observed convergence of neural network dynamics towards conventional theory even with standardized GM inputs, highlighting an unexpected universality.","We found that standardization, especially in conjunction with certain nonlinear functions, plays a critical role in this phenomena.","Consequently, despite the complex and varied nature of GM distributions, we demonstrate that neural networks exhibit asymptotic behaviors in line with predictions under simple Gaussian frameworks."],"url":"http://arxiv.org/abs/2405.00642v1","category":"stat.ML"}
{"created":"2024-05-01 17:07:31","title":"Vacancy-mediated transport and segregation tendencies of solutes in FCC nickel under diffusional creep: A density functional theory study","abstract":"The Nabarro-Herring (N-H) diffusional creep theory postulates the vacancy-mediated transport of atoms under a stress gradient as the creep mechanism under low-stress and high-temperature conditions. In multicomponent alloys, we premise that this stress-assisted flow of vacancies to and from grain boundaries will produce elemental segregation. An observation of such segregation, validated with theoretical predictions, can provide the necessary experimental evidence for the occurrence of N-H creep. Theoretical calculations of the segregation tendencies via analyzing the dominant solute diffusion mechanisms and the difference in diffusivities of the elements are therefore essential. To this end, this study applies density functional theory calculations of migration barriers and solute-vacancy binding energies as input to the self-consistent mean field theory to assess the vacancy-mediated diffusion mechanisms, transport coefficients, and segregation tendencies of Co, Cr, Mo, Re, Ta, and W solutes in face-centered cubic Ni. We find Co, Re, and W to be slow diffusers at high temperatures and Cr, Mo, and Ta to be fast diffusers. Further analysis shows that the slow diffusers tend to always enrich at vacancy sinks over a wide range of temperatures. In contrast, the fast diffusers show a transition from depletion to enrichment as the temperature lowers. Furthermore, our analysis of the segregation tendencies under tensile hydrostatic strains shows that slow diffusers are largely unaffected by the strain and favor enrichment. On the other hand, the fast diffusers exhibit high sensitivity to strain and their segregation tendency can transition from depletion to enrichment at a given temperature. The transport coefficients calculated in this work are expected to serve as input to mesoscale microstructure models to provide a more rigorous assessment of solute segregation under N-H creep conditions.","sentences":["The Nabarro-Herring (N-H) diffusional creep theory postulates the vacancy-mediated transport of atoms under a stress gradient as the creep mechanism under low-stress and high-temperature conditions.","In multicomponent alloys, we premise that this stress-assisted flow of vacancies to and from grain boundaries will produce elemental segregation.","An observation of such segregation, validated with theoretical predictions, can provide the necessary experimental evidence for the occurrence of N-H creep.","Theoretical calculations of the segregation tendencies via analyzing the dominant solute diffusion mechanisms and the difference in diffusivities of the elements are therefore essential.","To this end, this study applies density functional theory calculations of migration barriers and solute-vacancy binding energies as input to the self-consistent mean field theory to assess the vacancy-mediated diffusion mechanisms, transport coefficients, and segregation tendencies of Co, Cr, Mo, Re, Ta, and W solutes in face-centered cubic Ni.","We find Co, Re, and W to be slow diffusers at high temperatures and Cr, Mo, and Ta to be fast diffusers.","Further analysis shows that the slow diffusers tend to always enrich at vacancy sinks over a wide range of temperatures.","In contrast, the fast diffusers show a transition from depletion to enrichment as the temperature lowers.","Furthermore, our analysis of the segregation tendencies under tensile hydrostatic strains shows that slow diffusers are largely unaffected by the strain and favor enrichment.","On the other hand, the fast diffusers exhibit high sensitivity to strain and their segregation tendency can transition from depletion to enrichment at a given temperature.","The transport coefficients calculated in this work are expected to serve as input to mesoscale microstructure models to provide a more rigorous assessment of solute segregation under N-H creep conditions."],"url":"http://arxiv.org/abs/2405.00639v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 16:49:54","title":"Koopman-based Deep Learning for Nonlinear System Estimation","abstract":"Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world. Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions. In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes. In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems. The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system. Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch.","sentences":["Nonlinear differential equations are encountered as models of fluid flow, spiking neurons, and many other systems of interest in the real world.","Common features of these systems are that their behaviors are difficult to describe exactly and invariably unmodeled dynamics present challenges in making precise predictions.","In many cases the models exhibit extremely complicated behavior due to bifurcations and chaotic regimes.","In this paper, we present a novel data-driven linear estimator that uses Koopman operator theory to extract finite-dimensional representations of complex nonlinear systems.","The extracted model is used together with a deep reinforcement learning network that learns the optimal stepwise actions to predict future states of the original nonlinear system.","Our estimator is also adaptive to a diffeomorphic transformation of the nonlinear system which enables transfer learning to compute state estimates of the transformed system without relearning from scratch."],"url":"http://arxiv.org/abs/2405.00627v1","category":"eess.SY"}
{"created":"2024-05-01 16:35:16","title":"Chemistry in externally FUV irradiated disks in the outskirts of the Orion Nebula","abstract":"Most stars are born in stellar clusters and their protoplanetary disks, which are the birthplaces of planets, can therefore be affected by the radiation of nearby massive stars. However, little is known about the chemistry of externally irradiated disks, including whether or not their properties are similar to the so-far better-studied isolated disks. Motivated by this question, we present ALMA Band 6 observations of two irradiated Class II protoplanetary disks in the outskirts of the Orion Nebula Cluster (ONC) to explore the chemical composition of disks exposed to (external) FUV radiation fields: the 216-0939 disk and the binary system 253-1536A/B, which are exposed to radiation fields of $10^2-10^3$ times the average interstellar radiation field. We detect lines from CO isotopologues, HCN, H$_2$CO, and C$_2$H toward both protoplanetary disks. Based on the observed disk-integrated line fluxes and flux ratios, we do not find significant differences between isolated and irradiated disks. The observed differences seem to be more closely related to the different stellar masses than to the external radiation field. This suggests that these disks are far enough away from the massive Trapezium stars, that their chemistry is no longer affected by external FUV radiation. Additional observations towards lower-mass disks and disks closer to the massive Trapezium stars are required to elucidate the level of external radiation required to make an impact on the chemistry of planet formation in different kinds of disks.","sentences":["Most stars are born in stellar clusters and their protoplanetary disks, which are the birthplaces of planets, can therefore be affected by the radiation of nearby massive stars.","However, little is known about the chemistry of externally irradiated disks, including whether or not their properties are similar to the so-far better-studied isolated disks.","Motivated by this question, we present ALMA Band 6 observations of two irradiated Class II protoplanetary disks in the outskirts of the Orion Nebula Cluster (ONC) to explore the chemical composition of disks exposed to (external) FUV radiation fields: the 216-0939 disk and the binary system 253-1536A/B, which are exposed to radiation fields of $10^2-10^3$ times the average interstellar radiation field.","We detect lines from CO isotopologues, HCN, H$_2$CO, and C$_2$H toward both protoplanetary disks.","Based on the observed disk-integrated line fluxes and flux ratios, we do not find significant differences between isolated and irradiated disks.","The observed differences seem to be more closely related to the different stellar masses than to the external radiation field.","This suggests that these disks are far enough away from the massive Trapezium stars, that their chemistry is no longer affected by external FUV radiation.","Additional observations towards lower-mass disks and disks closer to the massive Trapezium stars are required to elucidate the level of external radiation required to make an impact on the chemistry of planet formation in different kinds of disks."],"url":"http://arxiv.org/abs/2405.00615v1","category":"astro-ph.EP"}
{"created":"2024-05-01 16:06:48","title":"Non-abelian symmetry-resolved entanglement entropy","abstract":"We introduce a mathematical framework for symmetry-resolved entanglement entropy with a non-abelian symmetry group. To obtain a reduced density matrix that is block-diagonal in the non-abelian charges, we define subsystems operationally in terms of subalgebras of invariant observables. We derive exact formulas for the average and the variance of the typical entanglement entropy for the ensemble of random pure states with fixed non-abelian charges. We focus on compact, semisimple Lie groups. We show that, compared to the abelian case, new phenomena arise from the interplay of locality and non-abelian symmetry, such as the asymmetry of the entanglement entropy under subsystem exchange, which we show in detail by computing the Page curve of a many-body system with $SU(2)$ symmetry.","sentences":["We introduce a mathematical framework for symmetry-resolved entanglement entropy with a non-abelian symmetry group.","To obtain a reduced density matrix that is block-diagonal in the non-abelian charges, we define subsystems operationally in terms of subalgebras of invariant observables.","We derive exact formulas for the average and the variance of the typical entanglement entropy for the ensemble of random pure states with fixed non-abelian charges.","We focus on compact, semisimple Lie groups.","We show that, compared to the abelian case, new phenomena arise from the interplay of locality and non-abelian symmetry, such as the asymmetry of the entanglement entropy under subsystem exchange, which we show in detail by computing the Page curve of a many-body system with $SU(2)$ symmetry."],"url":"http://arxiv.org/abs/2405.00597v1","category":"quant-ph"}
{"created":"2024-05-01 16:04:42","title":"Unbundle-Rewrite-Rebundle: Runtime Detection and Rewriting of Privacy-Harming Code in JavaScript Bundles","abstract":"This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting privacy-harming portions of bundled JavaScript code, and rewriting that code at runtime to remove the privacy harming behavior without breaking the surrounding code or overall application. URR is a novel solution to the problem of JavaScript bundles, where websites pre-compile multiple code units into a single file, making it impossible for content filters and ad-blockers to differentiate between desired and unwanted resources. Where traditional content filtering tools rely on URLs, URR analyzes the code at the AST level, and replaces harmful AST sub-trees with privacy-and-functionality maintaining alternatives.   We present an open-sourced implementation of URR as a Firefox extension, and evaluate it against JavaScript bundles generated by the most popular bundling system (Webpack) deployed on the Tranco 10k. We measure the performance, measured by precision (1.00), recall (0.95), and speed (0.43s per-script) when detecting and rewriting three representative privacy harming libraries often included in JavaScript bundles, and find URR to be an effective approach to a large-and-growing blind spot unaddressed by current privacy tools.","sentences":["This work presents Unbundle-Rewrite-Rebundle (URR), a system for detecting privacy-harming portions of bundled JavaScript code, and rewriting that code at runtime to remove the privacy harming behavior without breaking the surrounding code or overall application.","URR is a novel solution to the problem of JavaScript bundles, where websites pre-compile multiple code units into a single file, making it impossible for content filters and ad-blockers to differentiate between desired and unwanted resources.","Where traditional content filtering tools rely on URLs, URR analyzes the code at the AST level, and replaces harmful AST sub-trees with privacy-and-functionality maintaining alternatives.   ","We present an open-sourced implementation of URR as a Firefox extension, and evaluate it against JavaScript bundles generated by the most popular bundling system (Webpack) deployed on the Tranco 10k.","We measure the performance, measured by precision (1.00), recall (0.95), and speed (0.43s per-script) when detecting and rewriting three representative privacy harming libraries often included in JavaScript bundles, and find URR to be an effective approach to a large-and-growing blind spot unaddressed by current privacy tools."],"url":"http://arxiv.org/abs/2405.00596v1","category":"cs.CR"}
{"created":"2024-05-01 15:55:25","title":"Nonlinear Poisson effect in affine semiflexible polymer networks","abstract":"Stretching an elastic material along one axis typically induces contraction along the transverse axes, a phenomenon known as the Poisson effect. From these strains, one can compute the specific volume, which generally either increases or, in the incompressible limit, remains constant as the material is stretched. However, in networks of semiflexible or stiff polymers, which are typically highly compressible yet stiffen significantly when stretched, one instead sees a significant reduction in specific volume under finite strains. This volume reduction is accompanied by increasing alignment of filaments along the strain axis and a nonlinear elastic response, with stiffening of the apparent Young's modulus. For semiflexible networks, in which entropic bending elasticity governs the linear elastic regime, the nonlinear Poisson effect is caused by the nonlinear force-extension relationship of the constituent filaments, which produces a highly asymmetric response of the constituent polymers to stretching and compression. The details of this relationship depend on the geometric and elastic properties of the underlying filaments, which can vary greatly in experimental systems. Here, we provide a comprehensive characterization of the nonlinear Poisson effect in an affine network model and explore the influence of filament properties on essential features of the macroscopic response, including strain-driven alignment and volume reduction.","sentences":["Stretching an elastic material along one axis typically induces contraction along the transverse axes, a phenomenon known as the Poisson effect.","From these strains, one can compute the specific volume, which generally either increases or, in the incompressible limit, remains constant as the material is stretched.","However, in networks of semiflexible or stiff polymers, which are typically highly compressible yet stiffen significantly when stretched, one instead sees a significant reduction in specific volume under finite strains.","This volume reduction is accompanied by increasing alignment of filaments along the strain axis and a nonlinear elastic response, with stiffening of the apparent Young's modulus.","For semiflexible networks, in which entropic bending elasticity governs the linear elastic regime, the nonlinear Poisson effect is caused by the nonlinear force-extension relationship of the constituent filaments, which produces a highly asymmetric response of the constituent polymers to stretching and compression.","The details of this relationship depend on the geometric and elastic properties of the underlying filaments, which can vary greatly in experimental systems.","Here, we provide a comprehensive characterization of the nonlinear Poisson effect in an affine network model and explore the influence of filament properties on essential features of the macroscopic response, including strain-driven alignment and volume reduction."],"url":"http://arxiv.org/abs/2405.00590v2","category":"cond-mat.soft"}
{"created":"2024-05-01 15:52:35","title":"Pretzel monoids","abstract":"We introduce an interesting class of left adequate monoids which we call pretzel monoids. These, on the one hand, are monoids of birooted graphs with respect to a natural `glue-and-fold' operation, and on the other hand, are shown to be defined in the category of left adequate monoids by a natural class of presentations. They are also shown to be the free idempotent-pure expansions of right cancellative monoids, making them, in some sense, the left adequate analogues of Margolis-Meakin expansions for inverse monoids. The construction recovers the second author's geometric model of free left adequate monoids when the right cancellative monoid is free.","sentences":["We introduce an interesting class of left adequate monoids which we call pretzel monoids.","These, on the one hand, are monoids of birooted graphs with respect to a natural `glue-and-fold' operation, and on the other hand, are shown to be defined in the category of left adequate monoids by a natural class of presentations.","They are also shown to be the free idempotent-pure expansions of right cancellative monoids, making them, in some sense, the left adequate analogues of Margolis-Meakin expansions for inverse monoids.","The construction recovers the second author's geometric model of free left adequate monoids when the right cancellative monoid is free."],"url":"http://arxiv.org/abs/2405.00589v1","category":"math.RA"}
{"created":"2024-05-01 15:50:16","title":"GraCo: Granularity-Controllable Interactive Segmentation","abstract":"Interactive Segmentation (IS) segments specific objects or parts in the image according to user input. Current IS pipelines fall into two categories: single-granularity output and multi-granularity output. The latter aims to alleviate the spatial ambiguity present in the former. However, the multi-granularity output pipeline suffers from limited interaction flexibility and produces redundant results. In this work, we introduce Granularity-Controllable Interactive Segmentation (GraCo), a novel approach that allows precise control of prediction granularity by introducing additional parameters to input. This enhances the customization of the interactive system and eliminates redundancy while resolving ambiguity. Nevertheless, the exorbitant cost of annotating multi-granularity masks and the lack of available datasets with granularity annotations make it difficult for models to acquire the necessary guidance to control output granularity. To address this problem, we design an any-granularity mask generator that exploits the semantic property of the pre-trained IS model to automatically generate abundant mask-granularity pairs without requiring additional manual annotation. Based on these pairs, we propose a granularity-controllable learning strategy that efficiently imparts the granularity controllability to the IS model. Extensive experiments on intricate scenarios at object and part levels demonstrate that our GraCo has significant advantages over previous methods. This highlights the potential of GraCo to be a flexible annotation tool, capable of adapting to diverse segmentation scenarios. The project page: https://zhao-yian.github.io/GraCo.","sentences":["Interactive Segmentation (IS) segments specific objects or parts in the image according to user input.","Current IS pipelines fall into two categories: single-granularity output and multi-granularity output.","The latter aims to alleviate the spatial ambiguity present in the former.","However, the multi-granularity output pipeline suffers from limited interaction flexibility and produces redundant results.","In this work, we introduce Granularity-Controllable Interactive Segmentation (GraCo), a novel approach that allows precise control of prediction granularity by introducing additional parameters to input.","This enhances the customization of the interactive system and eliminates redundancy while resolving ambiguity.","Nevertheless, the exorbitant cost of annotating multi-granularity masks and the lack of available datasets with granularity annotations make it difficult for models to acquire the necessary guidance to control output granularity.","To address this problem, we design an any-granularity mask generator that exploits the semantic property of the pre-trained IS model to automatically generate abundant mask-granularity pairs without requiring additional manual annotation.","Based on these pairs, we propose a granularity-controllable learning strategy that efficiently imparts the granularity controllability to the IS model.","Extensive experiments on intricate scenarios at object and part levels demonstrate that our GraCo has significant advantages over previous methods.","This highlights the potential of GraCo to be a flexible annotation tool, capable of adapting to diverse segmentation scenarios.","The project page: https://zhao-yian.github.io/GraCo."],"url":"http://arxiv.org/abs/2405.00587v1","category":"cs.CV"}
{"created":"2024-05-01 15:38:35","title":"Periodic nonlinear Schr\u00f6dinger equation with distributional potential and invariant measures","abstract":"In this paper, we continue some investigations on the periodic NLSE started by Lebowitz, Rose and Speer and by Bourgain with the addition of a distributional multiplicative potential. We prove that the equation is globally wellposed for a set of data of full normalized Gibbs measure, after suitable truncation in the focusing case. The set and the measure are invariant under the flow. The main ingredients used are Strichartz estimates on periodic NLS with distributional potential to obtain local well-posedness for low regularity initial data.","sentences":["In this paper, we continue some investigations on the periodic NLSE started by Lebowitz, Rose and Speer and by Bourgain with the addition of a distributional multiplicative potential.","We prove that the equation is globally wellposed for a set of data of full normalized Gibbs measure, after suitable truncation in the focusing case.","The set and the measure are invariant under the flow.","The main ingredients used are Strichartz estimates on periodic NLS with distributional potential to obtain local well-posedness for low regularity initial data."],"url":"http://arxiv.org/abs/2405.00583v1","category":"math.AP"}
{"created":"2024-05-01 15:35:53","title":"Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools","abstract":"The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.","sentences":["The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates.","Estimating the air change rate in indoor environments, however, remains challenging.","It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors.","In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties.","The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber.","Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach.","In addition, uncertainties in real-life contexts were quantified with an incremental variance {\\sigma} for the Wiener process.","This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal.","The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified.","A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi.","Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm."],"url":"http://arxiv.org/abs/2405.00582v2","category":"stat.AP"}
{"created":"2024-05-01 15:33:14","title":"Conformalized Tensor Completion with Riemannian Optimization","abstract":"Tensor data, or multi-dimensional array, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging. It is not uncommon to observe tensor data containing missing values and tensor completion aims at estimating the missing values given the partially observed tensor. Sufficient efforts have been spared on devising scalable tensor completion algorithms but few on quantifying the uncertainty of the estimator. In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry. We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter. We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm. Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval. We apply our method to the regional total electron content (TEC) reconstruction problem.","sentences":["Tensor data, or multi-dimensional array, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging.","It is not uncommon to observe tensor data containing missing values and tensor completion aims at estimating the missing values given the partially observed tensor.","Sufficient efforts have been spared on devising scalable tensor completion algorithms but few on quantifying the uncertainty of the estimator.","In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry.","We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter.","We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm.","Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval.","We apply our method to the regional total electron content (TEC) reconstruction problem."],"url":"http://arxiv.org/abs/2405.00581v1","category":"stat.ME"}
{"created":"2024-05-01 15:28:17","title":"Complex analytic solutions for the TQG model","abstract":"We present a condition under which the thermal quasi-geostrophic (TQG) model possesses a solution that is holomorphic in time with values in the Gevrey space of complex analytic functions. This can be seen as the complex extension of the work by Levermore and Oliver (1997) for the generalized Euler equation but applied to the TQG model.","sentences":["We present a condition under which the thermal quasi-geostrophic (TQG) model possesses a solution that is holomorphic in time with values in the Gevrey space of complex analytic functions.","This can be seen as the complex extension of the work by Levermore and Oliver (1997) for the generalized Euler equation but applied to the TQG model."],"url":"http://arxiv.org/abs/2405.00575v1","category":"math.AP"}
{"created":"2024-05-01 15:22:36","title":"JWST/NIRCam Detection of the Fomalhaut C Debris Disk in Scattered Light","abstract":"Observations of debris disks offer important insights into the formation and evolution of planetary systems. Though M dwarfs make up approximately 80% of nearby stars, very few M-dwarf debris disks have been studied in detail -- making it unclear how or if the information gleaned from studying debris disks around more massive stars extends to the more abundant M dwarf systems. We report the first scattered-light detection of the debris disk around the M4 star Fomalhaut C using JWST's Near Infrared Camera (NIRCam; 3.6$~\\mu$m and 4.4$~\\mu$m). This result adds to the prior sample of only four M-dwarf debris disks with detections in scattered light, and marks the latest spectral type and oldest star among them. The size and orientation of the disk in these data are generally consistent with the prior ALMA sub-mm detection. Though no companions are identified, these data provide strong constraints on their presence -- with sensitivity sufficient to recover sub-Saturn mass objects in the vicinity of the disk. This result illustrates the unique capability of JWST for uncovering elusive M-dwarf debris disks in scattered light, and lays the groundwork for deeper studies of such objects in the 2--5$~\\mu$m regime.","sentences":["Observations of debris disks offer important insights into the formation and evolution of planetary systems.","Though M dwarfs make up approximately 80% of nearby stars, very few M-dwarf debris disks have been studied in detail -- making it unclear how or if the information gleaned from studying debris disks around more massive stars extends to the more abundant M dwarf systems.","We report the first scattered-light detection of the debris disk around the M4 star Fomalhaut C using JWST's Near Infrared Camera (NIRCam; 3.6$~\\mu$m and 4.4$~\\mu$m).","This result adds to the prior sample of only four M-dwarf debris disks with detections in scattered light, and marks the latest spectral type and oldest star among them.","The size and orientation of the disk in these data are generally consistent with the prior ALMA sub-mm detection.","Though no companions are identified, these data provide strong constraints on their presence -- with sensitivity sufficient to recover sub-Saturn mass objects in the vicinity of the disk.","This result illustrates the unique capability of JWST for uncovering elusive M-dwarf debris disks in scattered light, and lays the groundwork for deeper studies of such objects in the 2--5$~\\mu$m regime."],"url":"http://arxiv.org/abs/2405.00573v1","category":"astro-ph.EP"}
{"created":"2024-05-01 15:17:40","title":"Remote Sensing Data Assimilation with a Chained Hydrologic-hydraulic Model for Flood Forecasting","abstract":"A chained hydrologic-hydraulic model is implemented using predicted runoff from a large-scale hydrologic model (namely ISBA-CTRIP) as inputs to local hydrodynamic models (TELEMAC-2D) to issue forecasts of water level and flood extent. The uncertainties in the hydrological forcing and in friction parameters are reduced by an Ensemble Kalman Filter that jointly assimilates in-situ water levels and flood extent maps derived from remote sensing observations. The data assimilation framework is cycled in a real-time forecasting configuration. A cycle consists of a reanalysis and a forecast phase. Over the analysis, observations up to the present are assimilated. An ensemble is then initialized from the last analyzed states and issued forecasts for next 36 hr. Three strategies of forcing data for this forecast are investigated: (i) using CTRIP runoff for reanalysis and forecast, (ii) using observed discharge for analysis, then CTRIP runoff for forecast and (iii) using observed discharge for reanalysis and keep a persistent discharge value for forecast. It was shown that the data assimilation strategy provides a reliable reanalysis in hindcast mode. The combination of observed discharge and CTRIP runoff provides the most accurate results. For all strategies, the quality of the forecast decreases as the lead time increases. When the errors in CTRIP forcing are non-stationary, the forecast capability may be reduced. This work demonstrates that the forcing provided by a hydrologic model, while imperfect, can be efficiently used as input to a hydraulic model to issue reanalysis and forecasts, thanks to the assimilation of in-situ and remote sensing observations.","sentences":["A chained hydrologic-hydraulic model is implemented using predicted runoff from a large-scale hydrologic model (namely ISBA-CTRIP) as inputs to local hydrodynamic models (TELEMAC-2D) to issue forecasts of water level and flood extent.","The uncertainties in the hydrological forcing and in friction parameters are reduced by an Ensemble Kalman Filter that jointly assimilates in-situ water levels and flood extent maps derived from remote sensing observations.","The data assimilation framework is cycled in a real-time forecasting configuration.","A cycle consists of a reanalysis and a forecast phase.","Over the analysis, observations up to the present are assimilated.","An ensemble is then initialized from the last analyzed states and issued forecasts for next 36 hr.","Three strategies of forcing data for this forecast are investigated: (i) using CTRIP runoff for reanalysis and forecast, (ii) using observed discharge for analysis, then CTRIP runoff for forecast and (iii) using observed discharge for reanalysis and keep a persistent discharge value for forecast.","It was shown that the data assimilation strategy provides a reliable reanalysis in hindcast mode.","The combination of observed discharge and CTRIP runoff provides the most accurate results.","For all strategies, the quality of the forecast decreases as the lead time increases.","When the errors in CTRIP forcing are non-stationary, the forecast capability may be reduced.","This work demonstrates that the forcing provided by a hydrologic model, while imperfect, can be efficiently used as input to a hydraulic model to issue reanalysis and forecasts, thanks to the assimilation of in-situ and remote sensing observations."],"url":"http://arxiv.org/abs/2405.00567v1","category":"eess.IV"}
{"created":"2024-05-01 15:12:19","title":"Contribution of PRIDE VLBI products to the joint JUICE-Europa Clipper moons' ephemerides solution","abstract":"In the coming decade, JUICE and Europa Clipper radio-science will yield the most accurate estimation to date of the Galilean moons' physical parameters and ephemerides. JUICE's PRIDE (Planetary Radio Interferometry and Doppler Experiment) will help achieve such a solution by providing VLBI (Very Long Baseline Interferometry) observations of the spacecraft's lateral position, complementing nominal radio-science measurements. We quantify how PRIDE VLBI can contribute to the moons' ephemerides determination, in terms of attainable solution improvement and validation opportunities. To this end, we simulated both single- and dual-spacecraft VLBI, exploiting the potential simultaneous tracking of JUICE and Europa Clipper. We considered various tracking and data quality scenarios, and compared the formal uncertainties obtained with and without VLBI. This was performed for both global and local (i.e., per-flyby) estimations of the moons' states, as achieving a global solution first requires proceeding arc-per-arc. We showed that both single- and dual-spacecraft VLBI only bring limited improvement to the global state estimation, but significantly contribute to the moons' normal points (i.e., local states at flyby times), most notably in the out-of-plane direction. Additionally, we designed a validation plan exploiting PRIDE VLBI to progressively validate the classical radio-science solution. By improving the local state estimations and offering various validation opportunities, PRIDE will be invaluable in overcoming possible dynamical modelling challenges. It can therefore play a key role in reconstructing a global solution for the Galilean moons' dynamics with the uncertainty levels promised by JUICE-Europa Clipper analyses. This, in turn, is critical to the accurate characterisation of tidal dissipation in the Jovian system, holding the key to the long-term evolution of the Galilean moons.","sentences":["In the coming decade, JUICE and Europa Clipper radio-science will yield the most accurate estimation to date of the Galilean moons' physical parameters and ephemerides.","JUICE's PRIDE (Planetary Radio Interferometry and Doppler Experiment) will help achieve such a solution by providing VLBI (Very Long Baseline Interferometry) observations of the spacecraft's lateral position, complementing nominal radio-science measurements.","We quantify how PRIDE VLBI can contribute to the moons' ephemerides determination, in terms of attainable solution improvement and validation opportunities.","To this end, we simulated both single- and dual-spacecraft VLBI, exploiting the potential simultaneous tracking of JUICE and Europa Clipper.","We considered various tracking and data quality scenarios, and compared the formal uncertainties obtained with and without VLBI.","This was performed for both global and local (i.e., per-flyby) estimations of the moons' states, as achieving a global solution first requires proceeding arc-per-arc.","We showed that both single- and dual-spacecraft VLBI only bring limited improvement to the global state estimation, but significantly contribute to the moons' normal points (i.e., local states at flyby times), most notably in the out-of-plane direction.","Additionally, we designed a validation plan exploiting PRIDE VLBI to progressively validate the classical radio-science solution.","By improving the local state estimations and offering various validation opportunities, PRIDE will be invaluable in overcoming possible dynamical modelling challenges.","It can therefore play a key role in reconstructing a global solution for the Galilean moons' dynamics with the uncertainty levels promised by JUICE-Europa Clipper analyses.","This, in turn, is critical to the accurate characterisation of tidal dissipation in the Jovian system, holding the key to the long-term evolution of the Galilean moons."],"url":"http://arxiv.org/abs/2405.00562v1","category":"astro-ph.EP"}
{"created":"2024-05-01 15:07:47","title":"A Taste for Variety","abstract":"A decision maker repeatedly chooses one of a finite set of actions. In each period, the decision maker's payoff depends on fixed basic payoff of the chosen action and the frequency with which the action has been chosen in the past. We analyze optimal strategies associated with three types of evaluations of infinite payoffs: discounted present value, the limit inferior, and the limit superior of the partial averages. We show that when the first two are the evaluation schemes, a stationary strategy can always achieve the best possible outcome. However, for the latter evaluation scheme, a stationary strategy can achieve the best outcome only if all actions that are chosen with strictly positive frequency by an optimal stationary strategy have the same basic payoff.","sentences":["A decision maker repeatedly chooses one of a finite set of actions.","In each period, the decision maker's payoff depends on fixed basic payoff of the chosen action and the frequency with which the action has been chosen in the past.","We analyze optimal strategies associated with three types of evaluations of infinite payoffs: discounted present value, the limit inferior, and the limit superior of the partial averages.","We show that when the first two are the evaluation schemes, a stationary strategy can always achieve the best possible outcome.","However, for the latter evaluation scheme, a stationary strategy can achieve the best outcome only if all actions that are chosen with strictly positive frequency by an optimal stationary strategy have the same basic payoff."],"url":"http://arxiv.org/abs/2405.00561v1","category":"econ.TH"}
{"created":"2024-05-01 17:05:22","title":"A Distributed Model Identification Algorithm for Multi-Agent Systems","abstract":"In this study, we investigate agent-based approach for system model identification with an emphasis on power distribution system applications. Departing from conventional practices of relying on historical data for offline model identification, we adopt an online update approach utilizing real-time data by employing the latest data points for gradient computation. This methodology offers advantages including a large reduction in the communication network's bandwidth requirements by minimizing the data exchanged at each iteration and enabling the model to adapt in real-time to disturbances. Furthermore, we extend our model identification process from linear frameworks to more complex non-linear convex models. This extension is validated through numerical studies demonstrating improved control performance for a synthetic IEEE test case.","sentences":["In this study, we investigate agent-based approach for system model identification with an emphasis on power distribution system applications.","Departing from conventional practices of relying on historical data for offline model identification, we adopt an online update approach utilizing real-time data by employing the latest data points for gradient computation.","This methodology offers advantages including a large reduction in the communication network's bandwidth requirements by minimizing the data exchanged at each iteration and enabling the model to adapt in real-time to disturbances.","Furthermore, we extend our model identification process from linear frameworks to more complex non-linear convex models.","This extension is validated through numerical studies demonstrating improved control performance for a synthetic IEEE test case."],"url":"http://arxiv.org/abs/2405.00637v1","category":"eess.SY"}
{"created":"2024-05-01 16:29:33","title":"Capillary-Assisted Printing of Droplets at a Solid-Like Liquid-Liquid Interface","abstract":"Capillary forces guide the motion of biomolecular condensates, water-borne insects, and breakfast cereal. These surface-mediated interactions can be harnessed to build units into materials with exotic properties deriving from mesoscale structure. Droplets are promising building blocks for these materials, finding applications in tissue engineering, adaptive optics, and structural colour. However, the instability of water droplets at many liquid-liquid interfaces hampers the use of capillarity for the assembly of droplet-based materials. Here, we use nanoparticle surfactants to form solid-like oil-water interfaces at which aqueous droplets sit for extended periods. We find that microlitre-sized droplets at these interfaces attract each other over millimetric scales. We rationalize this interaction with a modified theory of capillarity. Applying printing methods allows us to finely control initial droplet positions, from which they self-assemble into cellular materials. Finally, by functionalising the interface with gold nanoparticles, we use plasmon-assisted optofluidics to manipulate these droplet-based materials with temperature gradients.","sentences":["Capillary forces guide the motion of biomolecular condensates, water-borne insects, and breakfast cereal.","These surface-mediated interactions can be harnessed to build units into materials with exotic properties deriving from mesoscale structure.","Droplets are promising building blocks for these materials, finding applications in tissue engineering, adaptive optics, and structural colour.","However, the instability of water droplets at many liquid-liquid interfaces hampers the use of capillarity for the assembly of droplet-based materials.","Here, we use nanoparticle surfactants to form solid-like oil-water interfaces at which aqueous droplets sit for extended periods.","We find that microlitre-sized droplets at these interfaces attract each other over millimetric scales.","We rationalize this interaction with a modified theory of capillarity.","Applying printing methods allows us to finely control initial droplet positions, from which they self-assemble into cellular materials.","Finally, by functionalising the interface with gold nanoparticles, we use plasmon-assisted optofluidics to manipulate these droplet-based materials with temperature gradients."],"url":"http://arxiv.org/abs/2405.00609v2","category":"cond-mat.soft"}
{"created":"2024-05-01 15:58:55","title":"The origin of wall-shear stress fluctuations in wall-bounded turbulence","abstract":"The origin of wall shear-stress fluctuations in wall turbulence was studied through energy dissipation at the wall. While confirming the universality in wall dissipation at small inner scales, the dissipation at larger scales is a consequence of near-wall scale interactions. In particular, the energy transport from the universal small to larger scale strengthens with Reynolds number due to the growing number of intermediate scales associated with the log layer. We anticipate that these insights broadly apply to all canonical wall-bounded turbulence for sufficiently high Reynolds numbers.","sentences":["The origin of wall shear-stress fluctuations in wall turbulence was studied through energy dissipation at the wall.","While confirming the universality in wall dissipation at small inner scales, the dissipation at larger scales is a consequence of near-wall scale interactions.","In particular, the energy transport from the universal small to larger scale strengthens with Reynolds number due to the growing number of intermediate scales associated with the log layer.","We anticipate that these insights broadly apply to all canonical wall-bounded turbulence for sufficiently high Reynolds numbers."],"url":"http://arxiv.org/abs/2405.00591v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 15:18:37","title":"A novel central compact finite-difference scheme for third derivatives with high spectral resolution","abstract":"In this paper, we introduce a novel category of central compact schemes inspired by existing cell-node and cell-centered compact finite difference schemes, that offer a superior spectral resolution for solving the dispersive wave equation. In our approach, we leverage both the function values at the cell nodes and cell centers to calculate third-order spatial derivatives at the cell nodes. To compute spatial derivatives at the cell centers, we employ a technique that involves half-shifting the indices within the formula initially designed for the cell-nodes. In contrast to the conventional compact interpolation scheme, our proposed method effectively sidesteps the introduction of transfer errors. We employ the Taylor-series expansion-based method to calculate the finite difference coefficients. By conducting systematic Fourier analysis and numerical tests, we note that the methods exhibit exceptional characteristics such as high order, superior resolution, and low dissipation. Computational findings further illustrate the effectiveness of high-order compact schemes, particularly in addressing problems with a third derivative term.","sentences":["In this paper, we introduce a novel category of central compact schemes inspired by existing cell-node and cell-centered compact finite difference schemes, that offer a superior spectral resolution for solving the dispersive wave equation.","In our approach, we leverage both the function values at the cell nodes and cell centers to calculate third-order spatial derivatives at the cell nodes.","To compute spatial derivatives at the cell centers, we employ a technique that involves half-shifting the indices within the formula initially designed for the cell-nodes.","In contrast to the conventional compact interpolation scheme, our proposed method effectively sidesteps the introduction of transfer errors.","We employ the Taylor-series expansion-based method to calculate the finite difference coefficients.","By conducting systematic Fourier analysis and numerical tests, we note that the methods exhibit exceptional characteristics such as high order, superior resolution, and low dissipation.","Computational findings further illustrate the effectiveness of high-order compact schemes, particularly in addressing problems with a third derivative term."],"url":"http://arxiv.org/abs/2405.00569v2","category":"math.NA"}
{"created":"2024-05-01 15:17:27","title":"NumLLM: Numeric-Sensitive Large Language Model for Chinese Finance","abstract":"Recently, many works have proposed various financial large language models (FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on financial corpora. However, existing FinLLMs exhibit unsatisfactory performance in understanding financial text when numeric variables are involved in questions. In this paper, we propose a novel LLM, called numeric-sensitive large language model (NumLLM), for Chinese finance. We first construct a financial corpus from financial textbooks which is essential for improving numeric capability of LLMs during fine-tuning. After that, we train two individual low-rank adaptation (LoRA) modules by fine-tuning on our constructed financial corpus. One module is for adapting general-purpose LLMs to financial domain, and the other module is for enhancing the ability of NumLLM to understand financial text with numeric variables. Lastly, we merge the two LoRA modules into the foundation model to obtain NumLLM for inference. Experiments on financial question-answering benchmark show that NumLLM can boost the performance of the foundation model and can achieve the best overall performance compared to all baselines, on both numeric and non-numeric questions.","sentences":["Recently, many works have proposed various financial large language models (FinLLMs) by pre-training from scratch or fine-tuning open-sourced LLMs on financial corpora.","However, existing FinLLMs exhibit unsatisfactory performance in understanding financial text when numeric variables are involved in questions.","In this paper, we propose a novel LLM, called numeric-sensitive large language model (NumLLM), for Chinese finance.","We first construct a financial corpus from financial textbooks which is essential for improving numeric capability of LLMs during fine-tuning.","After that, we train two individual low-rank adaptation (LoRA) modules by fine-tuning on our constructed financial corpus.","One module is for adapting general-purpose LLMs to financial domain, and the other module is for enhancing the ability of NumLLM to understand financial text with numeric variables.","Lastly, we merge the two LoRA modules into the foundation model to obtain NumLLM for inference.","Experiments on financial question-answering benchmark show that NumLLM can boost the performance of the foundation model and can achieve the best overall performance compared to all baselines, on both numeric and non-numeric questions."],"url":"http://arxiv.org/abs/2405.00566v1","category":"cs.CE"}
{"created":"2024-05-01 15:14:52","title":"Resolution analysis of magnetically arrested disk simulations","abstract":"Polarisation measurements by the Event Horizon Telescope from M87$^{\\ast}$ and Sgr A$^\\ast$ suggest that there is a dynamically strong, ordered magnetic field, typical of what is expected of a magnetically arrested accretion disk (MAD). In such disks the strong poloidal magnetic field can suppress the accretion flow and cause episodic flux eruptions. Recent work shows that General Relativistic Magnetohydrodynamic (GRMHD) MAD simulations feature dynamics of turbulence and mixing instabilities that are becoming resolved at higher resolutions. We perform a convergence study of MADs exceeding the status quo by an order of magnitude in resolution. We use existing 3D simulations performed with the H-AMR code, up to resolution of 5376 x 2304 x 2304 in a logarithmic spherical-polar grid. We find consistent time-averaged disk properties across all resolutions. However, higher resolutions reveal signs of inward angular momentum transport attributed to turbulent convection, particularly evident when mixing instabilities occur at the surfaces of flux tubes during flux eruptions. Additionally, we see wave-like features in the jet sheath, which become more prominent at higher resolutions, that may induce mixing between jet and disk. At higher resolutions, we observe the sheath to be thinner, resulting in increased temperature, reduced magnetisation, and greater variability. Those differences could affect the dissipation of energy, that would eventually result in distinct observable radiative emission from high-resolution simulations. With higher resolutions, we can delve into crucial questions about horizon-scale physics and its impact on the dynamics and emission properties of larger-scale jets.","sentences":["Polarisation measurements by the Event Horizon Telescope from M87$^{\\ast}$ and Sgr A$^\\ast$ suggest that there is a dynamically strong, ordered magnetic field, typical of what is expected of a magnetically arrested accretion disk (MAD).","In such disks the strong poloidal magnetic field can suppress the accretion flow and cause episodic flux eruptions.","Recent work shows that General Relativistic Magnetohydrodynamic (GRMHD) MAD simulations feature dynamics of turbulence and mixing instabilities that are becoming resolved at higher resolutions.","We perform a convergence study of MADs exceeding the status quo by an order of magnitude in resolution.","We use existing 3D simulations performed with the H-AMR code, up to resolution of 5376 x 2304 x 2304 in a logarithmic spherical-polar grid.","We find consistent time-averaged disk properties across all resolutions.","However, higher resolutions reveal signs of inward angular momentum transport attributed to turbulent convection, particularly evident when mixing instabilities occur at the surfaces of flux tubes during flux eruptions.","Additionally, we see wave-like features in the jet sheath, which become more prominent at higher resolutions, that may induce mixing between jet and disk.","At higher resolutions, we observe the sheath to be thinner, resulting in increased temperature, reduced magnetisation, and greater variability.","Those differences could affect the dissipation of energy, that would eventually result in distinct observable radiative emission from high-resolution simulations.","With higher resolutions, we can delve into crucial questions about horizon-scale physics and its impact on the dynamics and emission properties of larger-scale jets."],"url":"http://arxiv.org/abs/2405.00564v1","category":"astro-ph.HE"}
{"created":"2024-05-01 14:27:42","title":"New Trends on the Systems Approach to Modeling SARS-CoV-2 Pandemics in a Globally Connected Planet","abstract":"This paper presents a critical analysis of the literature and perspective research ideas for modeling the epidemics caused by the SARS-CoV-2 virus. It goes beyond deterministic population dynamics to consider several key complexity features of the system under consideration. In particular, the multiscale features of the dynamics from contagion to the subsequent dynamics of competition between the immune system and the proliferating virus. Other topics addressed in this work include the propagation of epidemics in a territory, taking into account local transportation networks, the heterogeneity of the population, and the study of social and economic problems in populations involved in the spread of epidemics. The overall content aims to show how new mathematical tools can be developed to address the above topics and how mathematical models and simulations can contribute to the decision making of crisis managers.","sentences":["This paper presents a critical analysis of the literature and perspective research ideas for modeling the epidemics caused by the SARS-CoV-2 virus.","It goes beyond deterministic population dynamics to consider several key complexity features of the system under consideration.","In particular, the multiscale features of the dynamics from contagion to the subsequent dynamics of competition between the immune system and the proliferating virus.","Other topics addressed in this work include the propagation of epidemics in a territory, taking into account local transportation networks, the heterogeneity of the population, and the study of social and economic problems in populations involved in the spread of epidemics.","The overall content aims to show how new mathematical tools can be developed to address the above topics and how mathematical models and simulations can contribute to the decision making of crisis managers."],"url":"http://arxiv.org/abs/2405.00541v1","category":"q-bio.PE"}
{"created":"2024-05-01 14:06:11","title":"On the damping of spin waves","abstract":"We show that, in ideal-spin hydrodynamics, the components of the spin tensor follow damped wave equations. The damping rate is related to nonlocal collisions of the particles in the fluid, which enter at first order in $\\hbar$ in a semi-classical expansion. This rate provides an estimate for the timescale of spin equilibration and is computed by considering a system of spin-1/2 fermions subject to a quartic self-interaction. It is found that the relaxation times of the components of the spin tensor can become very large compared to the usual dissipative timescales of the system. Our results suggest that the spin degrees of freedom in a heavy-ion collision may not be in equilibrium by the time of freeze-out, and thus should be treated dynamically.","sentences":["We show that, in ideal-spin hydrodynamics, the components of the spin tensor follow damped wave equations.","The damping rate is related to nonlocal collisions of the particles in the fluid, which enter at first order in $\\hbar$ in a semi-classical expansion.","This rate provides an estimate for the timescale of spin equilibration and is computed by considering a system of spin-1/2 fermions subject to a quartic self-interaction.","It is found that the relaxation times of the components of the spin tensor can become very large compared to the usual dissipative timescales of the system.","Our results suggest that the spin degrees of freedom in a heavy-ion collision may not be in equilibrium by the time of freeze-out, and thus should be treated dynamically."],"url":"http://arxiv.org/abs/2405.00533v1","category":"nucl-th"}
{"created":"2024-05-01 13:49:09","title":"Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest Monitoring","abstract":"Image-level regression is an important task in Earth observation, where visual domain and label shifts are a core challenge hampering generalization. However, cross-domain regression with remote sensing data remains understudied due to the absence of suited datasets. We introduce a new dataset with aerial and satellite imagery in five countries with three forest-related regression tasks. To match real-world applicative interests, we compare methods through a restrictive setup where no prior on the target domain is available during training, and models are adapted with limited information during testing. Building on the assumption that ordered relationships generalize better, we propose manifold diffusion for regression as a strong baseline for transduction in low-data regimes. Our comparison highlights the comparative advantages of inductive and transductive methods in cross-domain regression.","sentences":["Image-level regression is an important task in Earth observation, where visual domain and label shifts are a core challenge hampering generalization.","However, cross-domain regression with remote sensing data remains understudied due to the absence of suited datasets.","We introduce a new dataset with aerial and satellite imagery in five countries with three forest-related regression tasks.","To match real-world applicative interests, we compare methods through a restrictive setup where no prior on the target domain is available during training, and models are adapted with limited information during testing.","Building on the assumption that ordered relationships generalize better, we propose manifold diffusion for regression as a strong baseline for transduction in low-data regimes.","Our comparison highlights the comparative advantages of inductive and transductive methods in cross-domain regression."],"url":"http://arxiv.org/abs/2405.00514v1","category":"cs.CV"}
{"created":"2024-05-01 13:45:59","title":"Quantum rates in dissipative systems with spatially varying friction","abstract":"We investigate whether making the friction spatially dependent introduces quantum effects into the thermal reaction rates for dissipative reactions. We calculate the quantum rates using the numerically exact multi-configuration time-dependent Hartree (MCTDH) method, as well as the approximate ring-polymer molecular dynamics (RPMD), ring-polymer instanton (RPI) methods, and classical mechanics. By conducting simulations across a wide range of temperatures and friction strengths, we can identify the various regimes that govern the reactive dynamics. At high temperatures, in addition to the spatial-diffusion and energy-diffusion regimes predicted by Kramer's rate theory, a (coherent) tunnelling-dominated regime is identified at low friction. At low temperatures, incoherent tunnelling dominates most of Kramer's curve, except at very low friction when coherent tunnelling becomes dominant. Unlike in classical mechanics, the bath's influence changes the equilibrium time-independent properties of the system, leading to a complex interplay between spatially dependent friction and nuclear quantum effects even at high temperatures. More specifically, we show that a realistic friction profile can lead to an increase (decrease) of the quantum (classical) rates with friction within the spatial-diffusion regime, showing that classical and quantum rates display qualitatively different behaviours. Except at very low frictions, we find that RPMD captures most of the quantum effects in the thermal reaction rates.","sentences":["We investigate whether making the friction spatially dependent introduces quantum effects into the thermal reaction rates for dissipative reactions.","We calculate the quantum rates using the numerically exact multi-configuration time-dependent Hartree (MCTDH) method, as well as the approximate ring-polymer molecular dynamics (RPMD), ring-polymer instanton (RPI) methods, and classical mechanics.","By conducting simulations across a wide range of temperatures and friction strengths, we can identify the various regimes that govern the reactive dynamics.","At high temperatures, in addition to the spatial-diffusion and energy-diffusion regimes predicted by Kramer's rate theory, a (coherent) tunnelling-dominated regime is identified at low friction.","At low temperatures, incoherent tunnelling dominates most of Kramer's curve, except at very low friction when coherent tunnelling becomes dominant.","Unlike in classical mechanics, the bath's influence changes the equilibrium time-independent properties of the system, leading to a complex interplay between spatially dependent friction and nuclear quantum effects even at high temperatures.","More specifically, we show that a realistic friction profile can lead to an increase (decrease) of the quantum (classical) rates with friction within the spatial-diffusion regime, showing that classical and quantum rates display qualitatively different behaviours.","Except at very low frictions, we find that RPMD captures most of the quantum effects in the thermal reaction rates."],"url":"http://arxiv.org/abs/2405.00512v1","category":"physics.chem-ph"}
{"created":"2024-05-01 12:46:57","title":"PackVFL: Efficient HE Packing for Vertical Federated Learning","abstract":"As an essential tool of secure distributed machine learning, vertical federated learning (VFL) based on homomorphic encryption (HE) suffers from severe efficiency problems due to data inflation and time-consuming operations. To this core, we propose PackVFL, an efficient VFL framework based on packed HE (PackedHE), to accelerate the existing HE-based VFL algorithms. PackVFL packs multiple cleartexts into one ciphertext and supports single-instruction-multiple-data (SIMD)-style parallelism. We focus on designing a high-performant matrix multiplication (MatMult) method since it takes up most of the ciphertext computation time in HE-based VFL. Besides, devising the MatMult method is also challenging for PackedHE because a slight difference in the packing way could predominantly affect its computation and communication costs. Without domain-specific design, directly applying SOTA MatMult methods is hard to achieve optimal.   Therefore, we make a three-fold design: 1) we systematically explore the current design space of MatMult and quantify the complexity of existing approaches to provide guidance; 2) we propose a hybrid MatMult method according to the unique characteristics of VFL; 3) we adaptively apply our hybrid method in representative VFL algorithms, leveraging distinctive algorithmic properties to further improve efficiency. As the batch size, feature dimension and model size of VFL scale up to large sizes, PackVFL consistently delivers enhanced performance. Empirically, PackVFL propels existing VFL algorithms to new heights, achieving up to a 51.52X end-to-end speedup. This represents a substantial 34.51X greater speedup compared to the direct application of SOTA MatMult methods.","sentences":["As an essential tool of secure distributed machine learning, vertical federated learning (VFL) based on homomorphic encryption (HE) suffers from severe efficiency problems due to data inflation and time-consuming operations.","To this core, we propose PackVFL, an efficient VFL framework based on packed HE (PackedHE), to accelerate the existing HE-based VFL algorithms.","PackVFL packs multiple cleartexts into one ciphertext and supports single-instruction-multiple-data (SIMD)-style parallelism.","We focus on designing a high-performant matrix multiplication (MatMult) method since it takes up most of the ciphertext computation time in HE-based VFL.","Besides, devising the MatMult method is also challenging for PackedHE because a slight difference in the packing way could predominantly affect its computation and communication costs.","Without domain-specific design, directly applying SOTA MatMult methods is hard to achieve optimal.   ","Therefore, we make a three-fold design: 1) we systematically explore the current design space of MatMult and quantify the complexity of existing approaches to provide guidance; 2) we propose a hybrid MatMult method according to the unique characteristics of VFL; 3) we adaptively apply our hybrid method in representative VFL algorithms, leveraging distinctive algorithmic properties to further improve efficiency.","As the batch size, feature dimension and model size of VFL scale up to large sizes, PackVFL consistently delivers enhanced performance.","Empirically, PackVFL propels existing VFL algorithms to new heights, achieving up to a 51.52X end-to-end speedup.","This represents a substantial 34.51X greater speedup compared to the direct application of SOTA MatMult methods."],"url":"http://arxiv.org/abs/2405.00482v1","category":"cs.CR"}
{"created":"2024-05-01 11:41:59","title":"Saturation Level of Ion Weibel Instability and Isotropization Length Scale in Electron-Ion Weibel-Mediated Shocks","abstract":"Ion Weibel instability is considered to be the dominant physics for the dissipation in high-Mach number astrophysical shocks such as supernova remnant shocks and gamma-ray burst shocks. We study the instability dependence on various parameters using theory and particle-in-cell simulations. We demonstrate that electron physics determines the saturation level of the Weibel-generated magnetic field, even though the instability is driven by the ions. We discuss the application to astrophysical and laboratory laser experiment environments to clarify the roles of the ion Weibel instability. We develop a model for the isotropization length scale in Weibel-mediated shocks and compare its value to other characteristic length scales of each system. We find that electron heating to near equipartition is crucial for the formation of ultra-relativistic Weibel-mediated shocks. On the other hand, our results imply that non-relativistic shocks in typical interstellar medium are not purely mediated by the Weibel instability.","sentences":["Ion Weibel instability is considered to be the dominant physics for the dissipation in high-Mach number astrophysical shocks such as supernova remnant shocks and gamma-ray burst shocks.","We study the instability dependence on various parameters using theory and particle-in-cell simulations.","We demonstrate that electron physics determines the saturation level of the Weibel-generated magnetic field, even though the instability is driven by the ions.","We discuss the application to astrophysical and laboratory laser experiment environments to clarify the roles of the ion Weibel instability.","We develop a model for the isotropization length scale in Weibel-mediated shocks and compare its value to other characteristic length scales of each system.","We find that electron heating to near equipartition is crucial for the formation of ultra-relativistic Weibel-mediated shocks.","On the other hand, our results imply that non-relativistic shocks in typical interstellar medium are not purely mediated by the Weibel instability."],"url":"http://arxiv.org/abs/2405.00462v1","category":"astro-ph.HE"}
{"created":"2024-05-01 11:29:22","title":"The Nucleus of a Compact Lie Group, and Support of Singularity Categories","abstract":"In this paper we adapt the notion of the nucleus defined by Benson, Carlson, and Robinson to compact Lie groups in non-modular characteristic. We show that it describes the singularities of the projective scheme of the cohomology of its classifying space. A notion of support for singularity categories of ring spectra (in the sense of Greenlees and Stevenson) is established, and is shown to be precisely the nucleus in this case, consistent with a conjecture of Benson and Greenlees for finite groups.","sentences":["In this paper we adapt the notion of the nucleus defined by Benson, Carlson, and Robinson to compact Lie groups in non-modular characteristic.","We show that it describes the singularities of the projective scheme of the cohomology of its classifying space.","A notion of support for singularity categories of ring spectra (in the sense of Greenlees and Stevenson) is established, and is shown to be precisely the nucleus in this case, consistent with a conjecture of Benson and Greenlees for finite groups."],"url":"http://arxiv.org/abs/2405.00457v1","category":"math.AT"}
{"created":"2024-05-01 10:26:08","title":"Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods","abstract":"Background and purpose: Deformable image registration (DIR) is a crucial tool in radiotherapy for extracting and modelling organ motion. However, when significant changes and sliding boundaries are present, it faces compromised accuracy and uncertainty, determining the subsequential contour propagation and dose accumulation procedures. Materials and methods: We propose an implicit neural representation (INR)-based approach modelling motion continuously in both space and time, named Continues-sPatial-Temporal DIR (CPT-DIR). This method uses a multilayer perception (MLP) network to map 3D coordinate (x,y,z) to its corresponding velocity vector (vx,vy,vz). The displacement vectors (dx,dy,dz) are then calculated by integrating velocity vectors over time. The MLP's parameters can rapidly adapt to new cases without pre-training, enhancing optimisation. The DIR's performance was tested on the DIR-Lab dataset of 10 lung 4DCT cases, using metrics of landmark accuracy (TRE), contour conformity (Dice) and image similarity (MAE). Results: The proposed CPT-DIR can reduce landmark TRE from 2.79mm to 0.99mm, outperforming B-splines' results for all cases. The MAE of the whole-body region improves from 35.46HU to 28.99HU. Furthermore, CPT-DIR surpasses B-splines for accuracy in the sliding boundary region, lowering MAE and increasing Dice coefficients for the ribcage from 65.65HU and 90.41% to 42.04HU and 90.56%, versus 75.40HU and 89.30% without registration. Meanwhile, CPT-DIR offers significant speed advantages, completing in under 15 seconds compared to a few minutes with the conventional B-splines method. Conclusion: Leveraging the continuous representations, the CPT-DIR method significantly enhances registration accuracy, automation and speed, outperforming traditional B-splines in landmark and contour precision, particularly in the challenging areas.","sentences":["Background and purpose: Deformable image registration (DIR) is a crucial tool in radiotherapy for extracting and modelling organ motion.","However, when significant changes and sliding boundaries are present, it faces compromised accuracy and uncertainty, determining the subsequential contour propagation and dose accumulation procedures.","Materials and methods: We propose an implicit neural representation (INR)-based approach modelling motion continuously in both space and time, named Continues-sPatial-Temporal DIR (CPT-DIR).","This method uses a multilayer perception (MLP) network to map 3D coordinate (x,y,z) to its corresponding velocity vector (vx,vy,vz).","The displacement vectors (dx,dy,dz) are then calculated by integrating velocity vectors over time.","The MLP's parameters can rapidly adapt to new cases without pre-training, enhancing optimisation.","The DIR's performance was tested on the DIR-Lab dataset of 10 lung 4DCT cases, using metrics of landmark accuracy (TRE), contour conformity (Dice) and image similarity (MAE).","Results:","The proposed CPT-DIR can reduce landmark TRE from 2.79mm to 0.99mm, outperforming B-splines' results for all cases.","The MAE of the whole-body region improves from 35.46HU to 28.99HU.","Furthermore, CPT-DIR surpasses B-splines for accuracy in the sliding boundary region, lowering MAE and increasing Dice coefficients for the ribcage from 65.65HU and 90.41% to 42.04HU and 90.56%, versus 75.40HU and 89.30% without registration.","Meanwhile, CPT-DIR offers significant speed advantages, completing in under 15 seconds compared to a few minutes with the conventional B-splines method.","Conclusion: Leveraging the continuous representations, the CPT-DIR method significantly enhances registration accuracy, automation and speed, outperforming traditional B-splines in landmark and contour precision, particularly in the challenging areas."],"url":"http://arxiv.org/abs/2405.00430v1","category":"physics.med-ph"}
{"created":"2024-05-01 17:59:45","title":"Spectrally Pruned Gaussian Fields with Neural Compensation","abstract":"Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality. However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory. We credit this high memory footprint to the lack of consideration for the relationship between primitives. In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation. On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals. On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights. We demonstrate the performance of SUNDAE with extensive results. For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset. Codes are publicly available at https://runyiyang.github.io/projects/SUNDAE/.","sentences":["Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered attention for its fast rendering speed and high rendering quality.","However, this comes with high memory consumption, e.g., a well-trained Gaussian field may utilize three million Gaussian primitives and over 700 MB of memory.","We credit this high memory footprint to the lack of consideration for the relationship between primitives.","In this paper, we propose a memory-efficient Gaussian field named SUNDAE with spectral pruning and neural compensation.","On one hand, we construct a graph on the set of Gaussian primitives to model their relationship and design a spectral down-sampling module to prune out primitives while preserving desired signals.","On the other hand, to compensate for the quality loss of pruning Gaussians, we exploit a lightweight neural network head to mix splatted features, which effectively compensates for quality losses while capturing the relationship between primitives in its weights.","We demonstrate the performance of SUNDAE with extensive results.","For example, SUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla Gaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB memory, on the Mip-NeRF360 dataset.","Codes are publicly available at https://runyiyang.github.io/projects/SUNDAE/."],"url":"http://arxiv.org/abs/2405.00676v1","category":"cs.CV"}
{"created":"2024-05-01 17:58:11","title":"Quantum algorithms for matrix geometric means","abstract":"Matrix geometric means between two positive definite matrices can be defined equivalently from distinct perspectives - as solutions to certain nonlinear systems of equations, as points along geodesics in Riemannian geometry, and as solutions to certain optimisation problems. This diversity already suggests the potential for varied applications, as well as acting as a bridge between different domains. Here we devise new quantum subroutines to efficiently prepare quantum unitary operators that embed the standard matrix geometric mean and its generalisations called the weighted matrix geometric mean. This enables the construction of solutions to the algebraic Riccati equation, which is an important class of nonlinear systems of equations that appears in machine learning, optimal control, estimation, and filtering. Using these subroutines, we present a new class of quantum learning algorithms called quantum geometric mean metric learning. This has applications in efficiently finding the best distance measure and solving classification problems in the weakly supervised limit and for anomaly detection, for both classical and quantum problems. We also show how our method can be generalised to a particular p^th-order system of nonlinear equations. These quantum subroutines for matrix geometric means are also useful in other areas of quantum information. For example, we show how to use them in the estimation of geometric Renyi relative entropies and the Uhlmann fidelity by means of the Fuchs-Caves observable. In particular, our quantum algorithms for estimating the Uhlmann and Matsumoto fidelities have optimal dependence on the precision. Finally, we provide a BQP-complete problem based on matrix geometric means that can be solved by our subroutines, thus characterising their computational capability.","sentences":["Matrix geometric means between two positive definite matrices can be defined equivalently from distinct perspectives - as solutions to certain nonlinear systems of equations, as points along geodesics in Riemannian geometry, and as solutions to certain optimisation problems.","This diversity already suggests the potential for varied applications, as well as acting as a bridge between different domains.","Here we devise new quantum subroutines to efficiently prepare quantum unitary operators that embed the standard matrix geometric mean and its generalisations called the weighted matrix geometric mean.","This enables the construction of solutions to the algebraic Riccati equation, which is an important class of nonlinear systems of equations that appears in machine learning, optimal control, estimation, and filtering.","Using these subroutines, we present a new class of quantum learning algorithms called quantum geometric mean metric learning.","This has applications in efficiently finding the best distance measure and solving classification problems in the weakly supervised limit and for anomaly detection, for both classical and quantum problems.","We also show how our method can be generalised to a particular p^th-order system of nonlinear equations.","These quantum subroutines for matrix geometric means are also useful in other areas of quantum information.","For example, we show how to use them in the estimation of geometric Renyi relative entropies and the Uhlmann fidelity by means of the Fuchs-Caves observable.","In particular, our quantum algorithms for estimating the Uhlmann and Matsumoto fidelities have optimal dependence on the precision.","Finally, we provide a BQP-complete problem based on matrix geometric means that can be solved by our subroutines, thus characterising their computational capability."],"url":"http://arxiv.org/abs/2405.00673v1","category":"quant-ph"}
{"created":"2024-05-01 17:54:48","title":"Environmental cosmic acceleration from a phase transition in the dark sector","abstract":"A new degravitation mechanism within the framework of scalar tensor gravity is proposed. The mechanism eliminates all constant contributions from the potential to the Friedmann equation, leaving only the kinematic and the dynamic terms of the potential to drive cosmic acceleration. We explore a scenario involving a density-triggered phase transition in the late-time universe, and argue that the resulting effective energy density and equation of state parameter can explain late-time cosmology when extrapolated to a region of the parameter space.","sentences":["A new degravitation mechanism within the framework of scalar tensor gravity is proposed.","The mechanism eliminates all constant contributions from the potential to the Friedmann equation, leaving only the kinematic and the dynamic terms of the potential to drive cosmic acceleration.","We explore a scenario involving a density-triggered phase transition in the late-time universe, and argue that the resulting effective energy density and equation of state parameter can explain late-time cosmology when extrapolated to a region of the parameter space."],"url":"http://arxiv.org/abs/2405.00668v1","category":"astro-ph.CO"}
{"created":"2024-05-01 17:54:31","title":"Clique packings in random graphs","abstract":"We consider the question of how many edge-disjoint near-maximal cliques may be found in the dense Erd\\H{o}s-R\\'enyi random graph $G(n,p)$. Recently Acan and Kahn showed that the largest such family contains only $O(n^2/(\\log{n})^3)$ cliques, with high probability, which disproved a conjecture of Alon and Spencer. We prove the corresponding lower bound, $\\Omega(n^2/(\\log{n})^3)$, by considering a random graph process which sequentially selects and deletes near-maximal cliques. To analyse this process we use the Differential Equation Method. We also give a new proof of the upper bound $O(n^2/(\\log{n})^3)$ and discuss the problem of the precise size of the largest such clique packing.","sentences":["We consider the question of how many edge-disjoint near-maximal cliques may be found in the dense Erd\\H{o}s-R\\'enyi random graph $G(n,p)$. Recently Acan and Kahn showed that the largest such family contains only $O(n^2/(\\log{n})^3)$ cliques, with high probability, which disproved a conjecture of Alon and Spencer.","We prove the corresponding lower bound, $\\Omega(n^2/(\\log{n})^3)$, by considering a random graph process which sequentially selects and deletes near-maximal cliques.","To analyse this process we use the Differential Equation Method.","We also give a new proof of the upper bound $O(n^2/(\\log{n})^3)$ and discuss the problem of the precise size of the largest such clique packing."],"url":"http://arxiv.org/abs/2405.00667v1","category":"math.CO"}
{"created":"2024-05-01 17:50:36","title":"Quantum cryptographic protocols with dual messaging system via 2D alternate quantum walks and genuine single particle entangled states","abstract":"Single-particle entangled states (SPES) can offer a more secure way of encoding and processing quantum information than their multi-particle counterparts. The SPES generated via a 2D alternate quantum-walk setup from initially separable states can be either 3-way or 2-way entangled. This letter shows that the generated genuine three-way and nonlocal two-way SPES can be used as cryptographic keys to securely encode two distinct messages simultaneously. We detail the message encryption-decryption steps and show the resilience of the 3-way and 2-way SPES-based cryptographic protocols against eavesdropper attacks like intercept-and-resend and man-in-the-middle. We also detail how these protocols can be experimentally realized using single photons, with the three degrees of freedom being OAM, path, and polarization. These have unparalleled security for quantum communication tasks. The ability to simultaneously encode two distinct messages using the generated SPES showcases the versatility and efficiency of the proposed cryptographic protocol. This capability could significantly improve the throughput of quantum communication systems.","sentences":["Single-particle entangled states (SPES) can offer a more secure way of encoding and processing quantum information than their multi-particle counterparts.","The SPES generated via a 2D alternate quantum-walk setup from initially separable states can be either 3-way or 2-way entangled.","This letter shows that the generated genuine three-way and nonlocal two-way SPES can be used as cryptographic keys to securely encode two distinct messages simultaneously.","We detail the message encryption-decryption steps and show the resilience of the 3-way and 2-way SPES-based cryptographic protocols against eavesdropper attacks like intercept-and-resend and man-in-the-middle.","We also detail how these protocols can be experimentally realized using single photons, with the three degrees of freedom being OAM, path, and polarization.","These have unparalleled security for quantum communication tasks.","The ability to simultaneously encode two distinct messages using the generated SPES showcases the versatility and efficiency of the proposed cryptographic protocol.","This capability could significantly improve the throughput of quantum communication systems."],"url":"http://arxiv.org/abs/2405.00663v1","category":"quant-ph"}
{"created":"2024-05-01 17:50:06","title":"Towards quantum gravity with neural networks: Solving quantum Hamilton constraints of 3d Euclidean gravity in the weak coupling limit","abstract":"We consider 3-dimensional Euclidean gravity in the weak coupling limit of Smolin and show that it is BF-theory with $\\text{U(1)}^3$ as a Lie group. The theory is quantised using loop quantum gravity methods. The kinematical degrees of freedom are truncated, on account of computational feasibility, by fixing a graph and deforming the algebra of the holonomies to impose a cutoff on the charge vectors. This leads to a quantum theory related to $\\text{U}_q \\text{(1)}^3$ BF-theory. The effect of imposing the cutoff on the charges is examined. We also implement the quantum volume operator of 3d loop quantum gravity. Most importantly we compare two constraints for the quantum model obtained: a master constraint enforcing curvature and Gauss constraint, as well as a combination of a quantum Hamilton constraint constructed using Thiemann's strategy and the Gauss master constraint. The two constraints are solved using the neural network quantum state ansatz, demonstrating its ability to explore models which are out of reach for exact numerical methods. The solutions spaces are quantitatively compared and although the forms of the constraints are radically different, the solutions turn out to have a surprisingly large overlap. We also investigate the behavior of the quantum volume in solutions to the constraints.","sentences":["We consider 3-dimensional Euclidean gravity in the weak coupling limit of Smolin and show that it is BF-theory with $\\text{U(1)}^3$ as a Lie group.","The theory is quantised using loop quantum gravity methods.","The kinematical degrees of freedom are truncated, on account of computational feasibility, by fixing a graph and deforming the algebra of the holonomies to impose a cutoff on the charge vectors.","This leads to a quantum theory related to $\\text{U}_q \\text{(1)}^3$ BF-theory.","The effect of imposing the cutoff on the charges is examined.","We also implement the quantum volume operator of 3d loop quantum gravity.","Most importantly we compare two constraints for the quantum model obtained: a master constraint enforcing curvature and Gauss constraint, as well as a combination of a quantum Hamilton constraint constructed using Thiemann's strategy and the Gauss master constraint.","The two constraints are solved using the neural network quantum state ansatz, demonstrating its ability to explore models which are out of reach for exact numerical methods.","The solutions spaces are quantitatively compared and although the forms of the constraints are radically different, the solutions turn out to have a surprisingly large overlap.","We also investigate the behavior of the quantum volume in solutions to the constraints."],"url":"http://arxiv.org/abs/2405.00661v1","category":"gr-qc"}
{"created":"2024-05-01 17:28:27","title":"The Geometry of Loop Spaces II: Corrections","abstract":"This paper contains corrections to Madea, Rosenberg, Torres-Ardila, \"The Geometry of Loop Spaces II: Characteristic Classes,\" Advances in Math. (287), 2016, 485-518. The main change is that results about $\\pi_1({\\rm Diff}(M))$ are replaced by results about $\\pi_1({\\rm Isom}(M))$, where Diff$(M)$, Isom$(M)$ refer to the diffeomorphism and isometry group of the manifold $M$.","sentences":["This paper contains corrections to Madea, Rosenberg, Torres-Ardila, \"The Geometry of Loop Spaces II: Characteristic Classes,\" Advances in Math.","(287), 2016, 485-518.","The main change is that results about $\\pi_1({\\rm Diff}(M))$ are replaced by results about $\\pi_1({\\rm Isom}(M))$, where Diff$(M)$, Isom$(M)$ refer to the diffeomorphism and isometry group of the manifold $M$."],"url":"http://arxiv.org/abs/2405.00651v2","category":"math.DG"}
{"created":"2024-05-01 17:18:46","title":"Gradient-based Automatic Per-Weight Mixed Precision Quantization for Neural Networks On-Chip","abstract":"Model size and inference speed at deployment time, are major challenges in many deep learning applications. A promising strategy to overcome these challenges is quantization. However, a straightforward uniform quantization to very low precision can result in significant accuracy loss. Mixed-precision quantization, based on the idea that certain parts of the network can accommodate lower precision without compromising performance compared to other parts, offers a potential solution. In this work, we present High Granularity Quantization (HGQ), an innovative quantization-aware training method designed to fine-tune the per-weight and per-activation precision in an automatic way for ultra-low latency and low power neural networks which are to be deployed on FPGAs. We demonstrate that HGQ can outperform existing methods by a substantial margin, achieving resource reduction by up to a factor of 20 and latency improvement by a factor of 5 while preserving accuracy.","sentences":["Model size and inference speed at deployment time, are major challenges in many deep learning applications.","A promising strategy to overcome these challenges is quantization.","However, a straightforward uniform quantization to very low precision can result in significant accuracy loss.","Mixed-precision quantization, based on the idea that certain parts of the network can accommodate lower precision without compromising performance compared to other parts, offers a potential solution.","In this work, we present High Granularity Quantization (HGQ), an innovative quantization-aware training method designed to fine-tune the per-weight and per-activation precision in an automatic way for ultra-low latency and low power neural networks which are to be deployed on FPGAs.","We demonstrate that HGQ can outperform existing methods by a substantial margin, achieving resource reduction by up to a factor of 20 and latency improvement by a factor of 5 while preserving accuracy."],"url":"http://arxiv.org/abs/2405.00645v1","category":"cs.LG"}
{"created":"2024-05-01 17:08:02","title":"Stochastic fluids with transport noise: Approximating diffusion from data using SVD and ensemble forecast back-propagation","abstract":"We introduce and test methods for the calibration of the diffusion term in Stochastic Partial Differential Equations (SPDEs) describing fluids. We take two approaches, one uses ideas from the singular value decomposition and the Biot-Savart law. The other backpropagates through an ensemble forecast, with respect to diffusion parameters, to minimise a probabilistic ensemble forecasting metric. We describe the approaches in the specific context of solutions to SPDEs describing the evolution of fluid particles, sometimes called inviscid vortex methods. The methods are tested in an idealised setting in which the reference data is a known realisation of the parameterised SPDE, and also using a forecast verification metric known as the Continuous Rank Probability Score (CRPS).","sentences":["We introduce and test methods for the calibration of the diffusion term in Stochastic Partial Differential Equations (SPDEs) describing fluids.","We take two approaches, one uses ideas from the singular value decomposition and the Biot-Savart law.","The other backpropagates through an ensemble forecast, with respect to diffusion parameters, to minimise a probabilistic ensemble forecasting metric.","We describe the approaches in the specific context of solutions to SPDEs describing the evolution of fluid particles, sometimes called inviscid vortex methods.","The methods are tested in an idealised setting in which the reference data is a known realisation of the parameterised SPDE, and also using a forecast verification metric known as the Continuous Rank Probability Score (CRPS)."],"url":"http://arxiv.org/abs/2405.00640v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 16:55:08","title":"Depth Priors in Removal Neural Radiance Fields","abstract":"Neural Radiance Fields (NeRF) have shown impressive results in 3D reconstruction and generating novel views. A key challenge within NeRF is the editing of reconstructed scenes, such as object removal, which requires maintaining consistency across multiple views and ensuring high-quality synthesised perspectives. Previous studies have incorporated depth priors, typically from LiDAR or sparse depth measurements provided by COLMAP, to improve the performance of object removal in NeRF. However, these methods are either costly or time-consuming. In this paper, we propose a novel approach that integrates monocular depth estimates with NeRF-based object removal models to significantly reduce time consumption and enhance the robustness and quality of scene generation and object removal. We conducted a thorough evaluation of COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy in depth map generation. Our findings suggest that COLMAP can serve as an effective alternative to a ground truth depth map where such information is missing or costly to obtain. Additionally, we integrated various monocular depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess their capacity to improve object removal performance. Our experimental results highlight the potential of monocular depth estimation to substantially improve NeRF applications.","sentences":["Neural Radiance Fields (NeRF) have shown impressive results in 3D reconstruction and generating novel views.","A key challenge within NeRF is the editing of reconstructed scenes, such as object removal, which requires maintaining consistency across multiple views and ensuring high-quality synthesised perspectives.","Previous studies have incorporated depth priors, typically from LiDAR or sparse depth measurements provided by COLMAP, to improve the performance of object removal in NeRF.","However, these methods are either costly or time-consuming.","In this paper, we propose a novel approach that integrates monocular depth estimates with NeRF-based object removal models to significantly reduce time consumption and enhance the robustness and quality of scene generation and object removal.","We conducted a thorough evaluation of COLMAP's dense depth reconstruction on the KITTI dataset to verify its accuracy in depth map generation.","Our findings suggest that COLMAP can serve as an effective alternative to a ground truth depth map where such information is missing or costly to obtain.","Additionally, we integrated various monocular depth estimation methods into the removal NeRF model, i.e., SpinNeRF, to assess their capacity to improve object removal performance.","Our experimental results highlight the potential of monocular depth estimation to substantially improve NeRF applications."],"url":"http://arxiv.org/abs/2405.00630v1","category":"cs.CV"}
{"created":"2024-05-01 16:47:23","title":"Hysteresis and Self-Oscillations in an Artificial Memristive Quantum Neuron","abstract":"We theoretically study an artificial neuron circuit containing a quantum memristor in the presence of relaxation and dephasing. The charge transport in the quantum element is realized via tunneling of a charge through a quantum particle which shuttles between two terminals -- a functionality reminiscent of classical diffusive memristors. We demonstrate that this physical principle enables hysteretic behavior of the current-voltage characteristics of the quantum device. In addition, being used in artificial neural circuit, the quantum switcher is able to generate self-sustained current oscillations. Our analysis reveals that these self-oscillations are triggered only in quantum regime with a moderate rate of relaxation, and cannot exist either in a purely coherent regime or at a very high decoherence. We investigate the hysteresis and instability leading to the onset of current self-oscillations and analyze their properties depending on the circuit parameters. Our results provide a generic approach to the use of quantum regimes for controlling hysteresis and generating self-oscillations.","sentences":["We theoretically study an artificial neuron circuit containing a quantum memristor in the presence of relaxation and dephasing.","The charge transport in the quantum element is realized via tunneling of a charge through a quantum particle which shuttles between two terminals -- a functionality reminiscent of classical diffusive memristors.","We demonstrate that this physical principle enables hysteretic behavior of the current-voltage characteristics of the quantum device.","In addition, being used in artificial neural circuit, the quantum switcher is able to generate self-sustained current oscillations.","Our analysis reveals that these self-oscillations are triggered only in quantum regime with a moderate rate of relaxation, and cannot exist either in a purely coherent regime or at a very high decoherence.","We investigate the hysteresis and instability leading to the onset of current self-oscillations and analyze their properties depending on the circuit parameters.","Our results provide a generic approach to the use of quantum regimes for controlling hysteresis and generating self-oscillations."],"url":"http://arxiv.org/abs/2405.00624v1","category":"quant-ph"}
{"created":"2024-05-01 16:03:20","title":"Infinitely Many Half-Volume Constant Mean Curvature Hypersurfaces via Min-Max Theory","abstract":"Let $(M^{n+1},g)$ be a closed Riemannian manifold of dimension $3\\le n+1\\le 5$. We show that, if the metric $g$ is generic, then $M$ contains infinitely many geometrically distinct constant mean curvature hypersurfaces, each enclosing half the volume of $M$. As an essential part of the proof, we develop an Almgren-Pitts type min-max theory for certain non-local functionals of the general form $$\\Omega \\mapsto \\operatorname{Area}(\\partial \\Omega) - \\int_\\Omega h + f(\\operatorname{Vol}(\\Omega)).$$","sentences":["Let $(M^{n+1},g)$ be a closed Riemannian manifold of dimension $3\\le n+1\\le 5$.","We show that, if the metric $g$ is generic, then $M$ contains infinitely many geometrically distinct constant mean curvature hypersurfaces, each enclosing half the volume of $M$. As an essential part of the proof, we develop an Almgren-Pitts type min-max theory for certain non-local functionals of the general form $$\\Omega \\mapsto \\operatorname{Area}(\\partial \\Omega) - \\int_\\Omega h + f(\\operatorname{Vol}(\\Omega)).$$"],"url":"http://arxiv.org/abs/2405.00595v1","category":"math.DG"}
{"created":"2024-05-01 15:59:00","title":"Scaling and renormalization in high-dimensional regression","abstract":"This paper presents a succinct derivation of the training and generalization performance of a variety of high-dimensional ridge regression models using the basic tools of random matrix theory and free probability. We provide an introduction and review of recent results on these topics, aimed at readers with backgrounds in physics and deep learning. Analytic formulas for the training and generalization errors are obtained in a few lines of algebra directly from the properties of the $S$-transform of free probability. This allows for a straightforward identification of the sources of power-law scaling in model performance. We compute the generalization error of a broad class of random feature models. We find that in all models, the $S$-transform corresponds to the train-test generalization gap, and yields an analogue of the generalized-cross-validation estimator. Using these techniques, we derive fine-grained bias-variance decompositions for a very general class of random feature models with structured covariates. These novel results allow us to discover a scaling regime for random feature models where the variance due to the features limits performance in the overparameterized setting. We also demonstrate how anisotropic weight structure in random feature models can limit performance and lead to nontrivial exponents for finite-width corrections in the overparameterized setting. Our results extend and provide a unifying perspective on earlier models of neural scaling laws.","sentences":["This paper presents a succinct derivation of the training and generalization performance of a variety of high-dimensional ridge regression models using the basic tools of random matrix theory and free probability.","We provide an introduction and review of recent results on these topics, aimed at readers with backgrounds in physics and deep learning.","Analytic formulas for the training and generalization errors are obtained in a few lines of algebra directly from the properties of the $S$-transform of free probability.","This allows for a straightforward identification of the sources of power-law scaling in model performance.","We compute the generalization error of a broad class of random feature models.","We find that in all models, the $S$-transform corresponds to the train-test generalization gap, and yields an analogue of the generalized-cross-validation estimator.","Using these techniques, we derive fine-grained bias-variance decompositions for a very general class of random feature models with structured covariates.","These novel results allow us to discover a scaling regime for random feature models where the variance due to the features limits performance in the overparameterized setting.","We also demonstrate how anisotropic weight structure in random feature models can limit performance and lead to nontrivial exponents for finite-width corrections in the overparameterized setting.","Our results extend and provide a unifying perspective on earlier models of neural scaling laws."],"url":"http://arxiv.org/abs/2405.00592v1","category":"stat.ML"}
{"created":"2024-05-01 15:29:55","title":"Discovering robust biomarkers of neurological disorders from functional MRI using graph neural networks: A Review","abstract":"Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets. Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder. In this review, we provide an overview of how GNN and model explainability techniques have been applied on fMRI datasets for disorder prediction tasks, with a particular emphasis on the robustness of biomarkers produced for neurodegenerative diseases and neuropsychiatric disorders. We found that while most studies have performant models, salient features highlighted in these studies vary greatly across studies on the same disorder and little has been done to evaluate their robustness. To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers. We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on improving the robustness of potential biomarkers discovered via GNNs.","sentences":["Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets.","Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder.","In this review, we provide an overview of how GNN and model explainability techniques have been applied on fMRI datasets for disorder prediction tasks, with a particular emphasis on the robustness of biomarkers produced for neurodegenerative diseases and neuropsychiatric disorders.","We found that while most studies have performant models, salient features highlighted in these studies vary greatly across studies on the same disorder and little has been done to evaluate their robustness.","To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers.","We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on improving the robustness of potential biomarkers discovered via GNNs."],"url":"http://arxiv.org/abs/2405.00577v1","category":"cs.LG"}
{"created":"2024-05-01 14:43:20","title":"Digital-analog quantum convolutional neural networks for image classification","abstract":"We propose digital-analog quantum kernels for enhancing the detection of complex features in the classification of images. We consider multipartite-entangled analog blocks, stemming from native Ising interactions in neutral-atom quantum processors, and individual operations as digital steps to implement the protocol. To further improving the detection of complex features, we apply multiple quantum kernels by varying the qubit connectivity according to the hardware constraints. An architecture that combines non-trainable quantum kernels and standard convolutional neural networks is used to classify realistic medical images, from breast cancer and pneumonia diseases, with a significantly reduced number of parameters. Despite this fact, the model exhibits better performance than its classical counterparts and achieves comparable metrics according to public benchmarks. These findings demonstrate the relevance of digital-analog encoding, paving the way for surpassing classical models in image recognition approaching us to quantum-advantage regimes.","sentences":["We propose digital-analog quantum kernels for enhancing the detection of complex features in the classification of images.","We consider multipartite-entangled analog blocks, stemming from native Ising interactions in neutral-atom quantum processors, and individual operations as digital steps to implement the protocol.","To further improving the detection of complex features, we apply multiple quantum kernels by varying the qubit connectivity according to the hardware constraints.","An architecture that combines non-trainable quantum kernels and standard convolutional neural networks is used to classify realistic medical images, from breast cancer and pneumonia diseases, with a significantly reduced number of parameters.","Despite this fact, the model exhibits better performance than its classical counterparts and achieves comparable metrics according to public benchmarks.","These findings demonstrate the relevance of digital-analog encoding, paving the way for surpassing classical models in image recognition approaching us to quantum-advantage regimes."],"url":"http://arxiv.org/abs/2405.00548v1","category":"quant-ph"}
{"created":"2024-05-01 14:01:48","title":"High-Order Block Toeplitz Inner-Bordering method for solving the Gelfand-Levitan-Marchenko equation","abstract":"We propose a high precision algorithm for solving the Gelfand-Levitan-Marchenko equation. The algorithm is based on the block version of the Toeplitz Inner-Bordering algorithm of Levinson's type. To approximate integrals, we use the high-precision one-sided and two-sided Gregory quadrature formulas. Also we use the Woodbury formula to construct a computational algorithm. This makes it possible to use the almost Toeplitz structure of the matrices for the fast calculations.","sentences":["We propose a high precision algorithm for solving the Gelfand-Levitan-Marchenko equation.","The algorithm is based on the block version of the Toeplitz Inner-Bordering algorithm of Levinson's type.","To approximate integrals, we use the high-precision one-sided and two-sided Gregory quadrature formulas.","Also we use the Woodbury formula to construct a computational algorithm.","This makes it possible to use the almost Toeplitz structure of the matrices for the fast calculations."],"url":"http://arxiv.org/abs/2405.00529v1","category":"math.NA"}
{"created":"2024-05-01 14:01:37","title":"Einstein-Hilbert gravity, higher derivatives and a scalar matter field","abstract":"The present paper extends two previous one's on pure gravity dealing with Einstein-Hilbert and higher derivatives by including a massless scalar field as representative of matter. We study the renormalization to all orders of perturbation theory, provide the Slavnov-Taylor identity, symmetric partial differential equations and derive finiteness properties in the Landau gauge. It is shown that beginning with one-loop negative norm states originating from higher derivatives disappear.","sentences":["The present paper extends two previous one's on pure gravity dealing with Einstein-Hilbert and higher derivatives by including a massless scalar field as representative of matter.","We study the renormalization to all orders of perturbation theory, provide the Slavnov-Taylor identity, symmetric partial differential equations and derive finiteness properties in the Landau gauge.","It is shown that beginning with one-loop negative norm states originating from higher derivatives disappear."],"url":"http://arxiv.org/abs/2405.00528v1","category":"hep-th"}
{"created":"2024-05-01 14:00:46","title":"Isospin violation effect and three-body decays of the $T_{cc}^{+}$ state","abstract":"In this work, we make a study of $T_{cc}^+$ state observed by the LHCb collaboration in 2021. In obtaining the effective potentials using the One-Boson-Exchange Potential Model we use an exponential form factor, and find that in the short and medium range, the contributions of the $\\pi$, $\\rho$ and $\\omega$ exchanges are comparable while in the long range the pion-exchange contribution is dominant. Based on the assumption that $T_{cc}^+$ is a loosely bound state of $D^*D$, we focus on its three-body decay using the meson-exchange method. Considering that the difference between the thresholds of $D^{*+}D^0$ and $D^{*0}D^+$ is even larger than the binding energy of $T_{cc}^+$, the isospin-breaking effect is amplified by the small binding energy of $T_{cc}^+$. Explicitly including such an isospin-breaking effect we obtain, by solving the Schr\\\"{o}dinger equation, that the probability of the isoscalar component is about $91\\%$ while that of the isovector component is around $9\\%$ for $T_{cc}^+$. Using the experimental value of the mass of $T_{cc}^+$ as an input, we obtain the wave function of $T_{cc}^+$ and further obtain its width via the three-body hadronic as well as the radiative decays. The total width we obtain is in agreement with the experimental value of the LHCb measurement with a unitarised Breit-Wigner profile. Conversely, the current results support the conclusion that $T_{cc}^+$ is a hadronic molecule of $D^*D$.","sentences":["In this work, we make a study of $T_{cc}^+$ state observed by the LHCb collaboration in 2021.","In obtaining the effective potentials using the One-Boson-Exchange Potential Model we use an exponential form factor, and find that in the short and medium range, the contributions of the $\\pi$, $\\rho$ and $\\omega$ exchanges are comparable while in the long range the pion-exchange contribution is dominant.","Based on the assumption that $T_{cc}^+$ is a loosely bound state of $D^*D$, we focus on its three-body decay using the meson-exchange method.","Considering that the difference between the thresholds of $D^{*+}D^0$ and $D^{*0}D^+$ is even larger than the binding energy of $T_{cc}^+$, the isospin-breaking effect is amplified by the small binding energy of $T_{cc}^+$. Explicitly including such an isospin-breaking effect we obtain, by solving the Schr\\\"{o}dinger equation, that the probability of the isoscalar component is about $91\\%$ while that of the isovector component is around $9\\%$ for $T_{cc}^+$. Using the experimental value of the mass of $T_{cc}^+$ as an input, we obtain the wave function of $T_{cc}^+$ and further obtain its width via the three-body hadronic as well as the radiative decays.","The total width we obtain is in agreement with the experimental value of the LHCb measurement with a unitarised Breit-Wigner profile.","Conversely, the current results support the conclusion that $T_{cc}^+$ is a hadronic molecule of $D^*D$."],"url":"http://arxiv.org/abs/2405.00525v1","category":"hep-ph"}
{"created":"2024-05-01 13:55:38","title":"Graph-Based Multivariate Multiscale Dispersion Entropy: Efficient Implementation and Applications to Real-World Network Data","abstract":"We introduce Multivariate Multiscale Graph-based Dispersion Entropy (mvDEG), a novel, computationally efficient method for analyzing multivariate time series data in graph and complex network frameworks, and demonstrate its application in real-world data. mvDEG effectively combines temporal dynamics with topological relationships, offering enhanced analysis compared to traditional nonlinear entropy methods. Its efficacy is established through testing on synthetic signals, such as uncorrelated and correlated noise, showcasing its adeptness in discerning various levels of dependency and complexity.   The robustness of mvDEG is further validated with real-world datasets, effectively differentiating various two-phase flow regimes and capturing distinct dynamics in weather data analysis. An important advancement of mvDEG is its computational efficiency. Our optimized algorithm displays a computational time that grows linearly with the number of vertices or nodes, in contrast to the exponential growth observed in classical methods. This efficiency is achieved through refined matrix power calculations that exploit matrix and Kronecker product properties, making our method faster than the state of the art. The significant acceleration in computational time positions mvDEG as a transformative tool for extensive and real-time applications, setting a new benchmark in the analysis of time series recorded at distributed locations and opening avenues for innovative applications.","sentences":["We introduce Multivariate Multiscale Graph-based Dispersion Entropy (mvDEG), a novel, computationally efficient method for analyzing multivariate time series data in graph and complex network frameworks, and demonstrate its application in real-world data.","mvDEG effectively combines temporal dynamics with topological relationships, offering enhanced analysis compared to traditional nonlinear entropy methods.","Its efficacy is established through testing on synthetic signals, such as uncorrelated and correlated noise, showcasing its adeptness in discerning various levels of dependency and complexity.   ","The robustness of mvDEG is further validated with real-world datasets, effectively differentiating various two-phase flow regimes and capturing distinct dynamics in weather data analysis.","An important advancement of mvDEG is its computational efficiency.","Our optimized algorithm displays a computational time that grows linearly with the number of vertices or nodes, in contrast to the exponential growth observed in classical methods.","This efficiency is achieved through refined matrix power calculations that exploit matrix and Kronecker product properties, making our method faster than the state of the art.","The significant acceleration in computational time positions mvDEG as a transformative tool for extensive and real-time applications, setting a new benchmark in the analysis of time series recorded at distributed locations and opening avenues for innovative applications."],"url":"http://arxiv.org/abs/2405.00518v1","category":"math.CO"}
{"created":"2024-05-01 13:38:03","title":"NeRF-Guided Unsupervised Learning of RGB-D Registration","abstract":"This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction.","sentences":["This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision.","Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision.","However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials.","In this paper, we present NeRF-UR, a novel frame-to-model optimization framework for unsupervised RGB-D registration.","Instead of frame-to-frame consistency, we leverage the neural radiance field (NeRF) as a global model of the scene and use the consistency between the input and the NeRF-rerendered frames for pose optimization.","This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model.","Furthermore, to bootstrap the NeRF optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model.","By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality.","Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch.","Code and models will be released for paper reproduction."],"url":"http://arxiv.org/abs/2405.00507v1","category":"cs.CV"}
{"created":"2024-05-01 13:20:21","title":"Pseudo-Riemannian symmetric spaces of signature (2,2)","abstract":"We study all four-dimensional simply-connected indecomposable non-semisimple pseudo-Riemannian symmetric spaces whose metric has signature (2,2). We present models and compute their isometry groups. We solve the problem of the existence or non-existence of compact quotients by properly acting discrete subgroups of the isometry group. This continues and completes earlier work by Maeta.","sentences":["We study all four-dimensional simply-connected indecomposable non-semisimple pseudo-Riemannian symmetric spaces whose metric has signature (2,2).","We present models and compute their isometry groups.","We solve the problem of the existence or non-existence of compact quotients by properly acting discrete subgroups of the isometry group.","This continues and completes earlier work by Maeta."],"url":"http://arxiv.org/abs/2405.00501v1","category":"math.DG"}
{"created":"2024-05-01 13:03:56","title":"The Loewner framework for parametric systems: Taming the curse of dimensionality","abstract":"The Loewner framework is an interpolatory framework for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. One main innovation established here is the construction of data-based realizations for any number of parameters. Equally importantly, we show how to alleviate the computational burden, by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \\times N$. This reduces the complexity from $O(N^3)$ to about $O(N^{1.4})$, thus taming the curse of dimensionality and making the solution scalable to very large data sets. To achieve this, a new generalized multivariate rational function realization is defined. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a coupled set of Sylvester equations. The null space of these Loewner matrices then allows the construction of the multivariate barycentric transfer function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic computational burden reduction. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.","sentences":["The Loewner framework is an interpolatory framework for the approximation of linear and nonlinear systems.","The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters.","One main innovation established here is the construction of data-based realizations for any number of parameters.","Equally importantly, we show how to alleviate the computational burden, by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \\times N$. This reduces the complexity from $O(N^3)$ to about $O(N^{1.4})$, thus taming the curse of dimensionality and making the solution scalable to very large data sets.","To achieve this, a new generalized multivariate rational function realization is defined.","Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a coupled set of Sylvester equations.","The null space of these Loewner matrices then allows the construction of the multivariate barycentric transfer function.","The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic computational burden reduction.","Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation.","Numerical examples highlight the effectiveness and scalability of the method."],"url":"http://arxiv.org/abs/2405.00495v1","category":"math.NA"}
{"created":"2024-05-01 12:23:16","title":"A Comprehensive Survey of Dynamic Graph Neural Networks: Models, Frameworks, Benchmarks, Experiments and Challenges","abstract":"Dynamic Graph Neural Networks (GNNs) combine temporal information with GNNs to capture structural, temporal, and contextual relationships in dynamic graphs simultaneously, leading to enhanced performance in various applications. As the demand for dynamic GNNs continues to grow, numerous models and frameworks have emerged to cater to different application needs. There is a pressing need for a comprehensive survey that evaluates the performance, strengths, and limitations of various approaches in this domain. This paper aims to fill this gap by offering a thorough comparative analysis and experimental evaluation of dynamic GNNs. It covers 81 dynamic GNN models with a novel taxonomy, 12 dynamic GNN training frameworks, and commonly used benchmarks. We also conduct experimental results from testing representative nine dynamic GNN models and three frameworks on six standard graph datasets. Evaluation metrics focus on convergence accuracy, training efficiency, and GPU memory usage, enabling a thorough comparison of performance across various models and frameworks. From the analysis and evaluation results, we identify key challenges and offer principles for future research to enhance the design of models and frameworks in the dynamic GNNs field.","sentences":["Dynamic Graph Neural Networks (GNNs) combine temporal information with GNNs to capture structural, temporal, and contextual relationships in dynamic graphs simultaneously, leading to enhanced performance in various applications.","As the demand for dynamic GNNs continues to grow, numerous models and frameworks have emerged to cater to different application needs.","There is a pressing need for a comprehensive survey that evaluates the performance, strengths, and limitations of various approaches in this domain.","This paper aims to fill this gap by offering a thorough comparative analysis and experimental evaluation of dynamic GNNs.","It covers 81 dynamic GNN models with a novel taxonomy, 12 dynamic GNN training frameworks, and commonly used benchmarks.","We also conduct experimental results from testing representative nine dynamic GNN models and three frameworks on six standard graph datasets.","Evaluation metrics focus on convergence accuracy, training efficiency, and GPU memory usage, enabling a thorough comparison of performance across various models and frameworks.","From the analysis and evaluation results, we identify key challenges and offer principles for future research to enhance the design of models and frameworks in the dynamic GNNs field."],"url":"http://arxiv.org/abs/2405.00476v1","category":"cs.LG"}
{"created":"2024-05-01 12:15:58","title":"DmADs-Net: Dense multiscale attention and depth-supervised network for medical image segmentation","abstract":"Deep learning has made important contributions to the development of medical image segmentation. Convolutional neural networks, as a crucial branch, have attracted strong attention from researchers. Through the tireless efforts of numerous researchers, convolutional neural networks have yielded numerous outstanding algorithms for processing medical images. The ideas and architectures of these algorithms have also provided important inspiration for the development of later technologies.Through extensive experimentation, we have found that currently mainstream deep learning algorithms are not always able to achieve ideal results when processing complex datasets and different types of datasets. These networks still have room for improvement in lesion localization and feature extraction. Therefore, we have created the Dense Multiscale Attention and Depth-Supervised Network (DmADs-Net).We use ResNet for feature extraction at different depths and create a Multi-scale Convolutional Feature Attention Block to improve the network's attention to weak feature information. The Local Feature Attention Block is created to enable enhanced local feature attention for high-level semantic information. In addition, in the feature fusion phase, a Feature Refinement and Fusion Block is created to enhance the fusion of different semantic information.We validated the performance of the network using five datasets of varying sizes and types. Results from comparative experiments show that DmADs-Net outperformed mainstream networks. Ablation experiments further demonstrated the effectiveness of the created modules and the rationality of the network architecture.","sentences":["Deep learning has made important contributions to the development of medical image segmentation.","Convolutional neural networks, as a crucial branch, have attracted strong attention from researchers.","Through the tireless efforts of numerous researchers, convolutional neural networks have yielded numerous outstanding algorithms for processing medical images.","The ideas and architectures of these algorithms have also provided important inspiration for the development of later technologies.","Through extensive experimentation, we have found that currently mainstream deep learning algorithms are not always able to achieve ideal results when processing complex datasets and different types of datasets.","These networks still have room for improvement in lesion localization and feature extraction.","Therefore, we have created the Dense Multiscale Attention and Depth-Supervised Network (DmADs-Net).We use ResNet for feature extraction at different depths and create a Multi-scale Convolutional Feature Attention Block to improve the network's attention to weak feature information.","The Local Feature Attention Block is created to enable enhanced local feature attention for high-level semantic information.","In addition, in the feature fusion phase, a Feature Refinement and Fusion Block is created to enhance the fusion of different semantic information.","We validated the performance of the network using five datasets of varying sizes and types.","Results from comparative experiments show that DmADs-Net outperformed mainstream networks.","Ablation experiments further demonstrated the effectiveness of the created modules and the rationality of the network architecture."],"url":"http://arxiv.org/abs/2405.00472v1","category":"eess.IV"}
{"created":"2024-05-01 12:12:59","title":"Exploiting Positional Bias for Query-Agnostic Generative Content in Search","abstract":"In recent years, neural ranking models (NRMs) have been shown to substantially outperform their lexical counterparts in text retrieval. In traditional search pipelines, a combination of features leads to well-defined behaviour. However, as neural approaches become increasingly prevalent as the final scoring component of engines or as standalone systems, their robustness to malicious text and, more generally, semantic perturbation needs to be better understood. We posit that the transformer attention mechanism can induce exploitable defects through positional bias in search models, leading to an attack that could generalise beyond a single query or topic. We demonstrate such defects by showing that non-relevant text--such as promotional content--can be easily injected into a document without adversely affecting its position in search results. Unlike previous gradient-based attacks, we demonstrate these biases in a query-agnostic fashion. In doing so, without the knowledge of topicality, we can still reduce the negative effects of non-relevant content injection by controlling injection position. Our experiments are conducted with simulated on-topic promotional text automatically generated by prompting LLMs with topical context from target documents. We find that contextualisation of a non-relevant text further reduces negative effects whilst likely circumventing existing content filtering mechanisms. In contrast, lexical models are found to be more resilient to such content injection attacks. We then investigate a simple yet effective compensation for the weaknesses of the NRMs in search, validating our hypotheses regarding transformer bias.","sentences":["In recent years, neural ranking models (NRMs) have been shown to substantially outperform their lexical counterparts in text retrieval.","In traditional search pipelines, a combination of features leads to well-defined behaviour.","However, as neural approaches become increasingly prevalent as the final scoring component of engines or as standalone systems, their robustness to malicious text and, more generally, semantic perturbation needs to be better understood.","We posit that the transformer attention mechanism can induce exploitable defects through positional bias in search models, leading to an attack that could generalise beyond a single query or topic.","We demonstrate such defects by showing that non-relevant text--such as promotional content--can be easily injected into a document without adversely affecting its position in search results.","Unlike previous gradient-based attacks, we demonstrate these biases in a query-agnostic fashion.","In doing so, without the knowledge of topicality, we can still reduce the negative effects of non-relevant content injection by controlling injection position.","Our experiments are conducted with simulated on-topic promotional text automatically generated by prompting LLMs with topical context from target documents.","We find that contextualisation of a non-relevant text further reduces negative effects whilst likely circumventing existing content filtering mechanisms.","In contrast, lexical models are found to be more resilient to such content injection attacks.","We then investigate a simple yet effective compensation for the weaknesses of the NRMs in search, validating our hypotheses regarding transformer bias."],"url":"http://arxiv.org/abs/2405.00469v1","category":"cs.IR"}
{"created":"2024-05-01 12:03:39","title":"Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable","abstract":"Foundational generative models should be traceable to protect their owners and facilitate safety regulation. To achieve this, traditional approaches embed identifiers based on supervisory trigger-response signals, which are commonly known as backdoor watermarks. They are prone to failure when the model is fine-tuned with nontrigger data. Our experiments show that this vulnerability is due to energetic changes in only a few 'busy' layers during fine-tuning. This yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes watermarks resilient to fine-tuning-based removal. The trigger-response pairs of AIAO samples across various neural network depths can be used to construct watermarked subpaths, employing Monte Carlo sampling to achieve stable verification results. In addition, unlike the existing methods of designing a backdoor for the input/output space of diffusion models, in our method, we propose to embed the backdoor into the feature space of sampled subpaths, where a mask-controlled trigger function is proposed to preserve the generation performance and ensure the invisibility of the embedded backdoor. Our empirical studies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm the robustness of AIAO; while the verification rates of other trigger-based methods fall from ~90% to ~70% after fine-tuning, those of our method remain consistently above 90%.","sentences":["Foundational generative models should be traceable to protect their owners and facilitate safety regulation.","To achieve this, traditional approaches embed identifiers based on supervisory trigger-response signals, which are commonly known as backdoor watermarks.","They are prone to failure when the model is fine-tuned with nontrigger data.","Our experiments show that this vulnerability is due to energetic changes in only a few 'busy' layers during fine-tuning.","This yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes watermarks resilient to fine-tuning-based removal.","The trigger-response pairs of AIAO samples across various neural network depths can be used to construct watermarked subpaths, employing Monte Carlo sampling to achieve stable verification results.","In addition, unlike the existing methods of designing a backdoor for the input/output space of diffusion models, in our method, we propose to embed the backdoor into the feature space of sampled subpaths, where a mask-controlled trigger function is proposed to preserve the generation performance and ensure the invisibility of the embedded backdoor.","Our empirical studies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm the robustness of AIAO; while the verification rates of other trigger-based methods fall from ~90% to ~70% after fine-tuning, those of our method remain consistently above 90%."],"url":"http://arxiv.org/abs/2405.00466v1","category":"cs.CV"}
{"created":"2024-05-01 11:17:58","title":"The Maxwell evolution equation of electromagnetic resonators: a mathematical proof with explicit derivation","abstract":"In a recent publication [1], the authors employ electromagnetic quasinormal-mode theory to establish an \"exact\" Maxwell evolution (EME) equation governing resonator dynamics. This resulting equation bears resemblance to the classical evolution equation proposed heuristically in the temporal coupled-mode theory (CMT), yet it also presents differences. One significant difference is that the driving force in the EME equation is proportional to the derivative of the incident field, whereas in the CMT evolution equation, it is proportional to the incident field itself. This unexpected finding was substantiated by a numerical test and a mathematical demonstration in [1]. The test remains undisputed in this study. However, the demonstration unfortunately relies on unjustified shortcuts. We promptly highlight these shortcuts while proposing an alternative demonstration that is direct and indisputable. The new demonstration provides a firm mathematical foundation for the EME equation and reinforces its credibility.","sentences":["In a recent publication [1], the authors employ electromagnetic quasinormal-mode theory to establish an \"exact\" Maxwell evolution (EME) equation governing resonator dynamics.","This resulting equation bears resemblance to the classical evolution equation proposed heuristically in the temporal coupled-mode theory (CMT), yet it also presents differences.","One significant difference is that the driving force in the EME equation is proportional to the derivative of the incident field, whereas in the CMT evolution equation, it is proportional to the incident field itself.","This unexpected finding was substantiated by a numerical test and a mathematical demonstration in [1].","The test remains undisputed in this study.","However, the demonstration unfortunately relies on unjustified shortcuts.","We promptly highlight these shortcuts while proposing an alternative demonstration that is direct and indisputable.","The new demonstration provides a firm mathematical foundation for the EME equation and reinforces its credibility."],"url":"http://arxiv.org/abs/2405.00455v1","category":"physics.optics"}
{"created":"2024-05-01 10:48:23","title":"Modeling Linear and Non-linear Layers: An MILP Approach Towards Finding Differential and Impossible Differential Propagations","abstract":"Symmetric key cryptography stands as a fundamental cornerstone in ensuring security within contemporary electronic communication frameworks. The cryptanalysis of classical symmetric key ciphers involves traditional methods and techniques aimed at breaking or analyzing these cryptographic systems. In the evaluation of new ciphers, the resistance against linear and differential cryptanalysis is commonly a key design criterion. The wide trail design technique for block ciphers facilitates the demonstration of security against linear and differential cryptanalysis. Assessing the scheme's security against differential attacks often involves determining the minimum number of active SBoxes for all rounds of a cipher. The propagation characteristics of a cryptographic component, such as an SBox, can be expressed using Boolean functions. Mixed Integer Linear Programming (MILP) proves to be a valuable technique for solving Boolean functions. We formulate a set of inequalities to model a Boolean function, which is subsequently solved by an MILP solver. To efficiently model a Boolean function and select a minimal set of inequalities, two key challenges must be addressed. We propose algorithms to address the second challenge, aiming to find more optimized linear and non-linear components. Our approaches are applied to modeling SBoxes (up to six bits) and EXOR operations with any number of inputs. Additionally, we introduce an MILP-based automatic tool for exploring differential and impossible differential propagations within a cipher. The tool is successfully applied to five lightweight block ciphers: Lilliput, GIFT64, SKINNY64, Klein, and MIBS.","sentences":["Symmetric key cryptography stands as a fundamental cornerstone in ensuring security within contemporary electronic communication frameworks.","The cryptanalysis of classical symmetric key ciphers involves traditional methods and techniques aimed at breaking or analyzing these cryptographic systems.","In the evaluation of new ciphers, the resistance against linear and differential cryptanalysis is commonly a key design criterion.","The wide trail design technique for block ciphers facilitates the demonstration of security against linear and differential cryptanalysis.","Assessing the scheme's security against differential attacks often involves determining the minimum number of active SBoxes for all rounds of a cipher.","The propagation characteristics of a cryptographic component, such as an SBox, can be expressed using Boolean functions.","Mixed Integer Linear Programming (MILP) proves to be a valuable technique for solving Boolean functions.","We formulate a set of inequalities to model a Boolean function, which is subsequently solved by an MILP solver.","To efficiently model a Boolean function and select a minimal set of inequalities, two key challenges must be addressed.","We propose algorithms to address the second challenge, aiming to find more optimized linear and non-linear components.","Our approaches are applied to modeling SBoxes (up to six bits) and EXOR operations with any number of inputs.","Additionally, we introduce an MILP-based automatic tool for exploring differential and impossible differential propagations within a cipher.","The tool is successfully applied to five lightweight block ciphers: Lilliput, GIFT64, SKINNY64, Klein, and MIBS."],"url":"http://arxiv.org/abs/2405.00441v1","category":"cs.CR"}
{"created":"2024-05-01 10:43:55","title":"MetaRM: Shifted Distributions Alignment via Meta-Learning","abstract":"The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM). However, as the training process progresses, the output distribution of the policy model shifts, leading to the RM's reduced ability to distinguish between responses. This issue is further compounded when the RM, trained on a specific data distribution, struggles to generalize to examples outside of that distribution. These two issues can be united as a challenge posed by the shifted distribution of the environment. To surmount this challenge, we introduce MetaRM, a method leveraging meta-learning to align the RM with the shifted environment distribution. MetaRM is designed to train the RM by minimizing data loss, particularly for data that can improve the differentiation ability to examples of the shifted target distribution. Extensive experiments demonstrate that MetaRM significantly improves the RM's distinguishing ability in iterative RLHF optimization, and also provides the capacity to identify subtle differences in out-of-distribution samples.","sentences":["The success of Reinforcement Learning from Human Feedback (RLHF) in language model alignment is critically dependent on the capability of the reward model (RM).","However, as the training process progresses, the output distribution of the policy model shifts, leading to the RM's reduced ability to distinguish between responses.","This issue is further compounded when the RM, trained on a specific data distribution, struggles to generalize to examples outside of that distribution.","These two issues can be united as a challenge posed by the shifted distribution of the environment.","To surmount this challenge, we introduce MetaRM, a method leveraging meta-learning to align the RM with the shifted environment distribution.","MetaRM is designed to train the RM by minimizing data loss, particularly for data that can improve the differentiation ability to examples of the shifted target distribution.","Extensive experiments demonstrate that MetaRM significantly improves the RM's distinguishing ability in iterative RLHF optimization, and also provides the capacity to identify subtle differences in out-of-distribution samples."],"url":"http://arxiv.org/abs/2405.00438v1","category":"cs.LG"}
{"created":"2024-05-01 09:59:40","title":"On the Incompressible Limit of Current-Vortex Sheets with or without Surface Tension","abstract":"This is the second part of the two-paper sequence, which aims to present a comprehensive study for compressible current-vortex sheets with or without surface tension in ideal compressible magnetohydrodynamics (MHD). The results of this paper are two-fold: First, we establish the zero-surface-tension limit of compressible current-vortex sheets under certain stability conditions on the free interface; Second, when the two-phase flows are isentropic and the density functions converge to the same constant as Mach number goes to zero, we can drop the boundedness assumption (with respect to Mach number) on high-order time derivatives by combining the paradifferential approach applied to the evolution equation of the free interface, the structure of wave equations for the total pressure and the anisotropic Sobolev spaces with suitable weights of Mach number. To our knowledge, this is the first result that rigorously justifies the incompressible limit for both compressible vortex sheets and free-surface ideal MHD flows.","sentences":["This is the second part of the two-paper sequence, which aims to present a comprehensive study for compressible current-vortex sheets with or without surface tension in ideal compressible magnetohydrodynamics (MHD).","The results of this paper are two-fold: First, we establish the zero-surface-tension limit of compressible current-vortex sheets under certain stability conditions on the free interface; Second, when the two-phase flows are isentropic and the density functions converge to the same constant as Mach number goes to zero, we can drop the boundedness assumption (with respect to Mach number) on high-order time derivatives by combining the paradifferential approach applied to the evolution equation of the free interface, the structure of wave equations for the total pressure and the anisotropic Sobolev spaces with suitable weights of Mach number.","To our knowledge, this is the first result that rigorously justifies the incompressible limit for both compressible vortex sheets and free-surface ideal MHD flows."],"url":"http://arxiv.org/abs/2405.00421v1","category":"math.AP"}
{"created":"2024-05-01 09:58:00","title":"The Serre spectral sequence of a Lie subalgebroid","abstract":"We study a spectral sequence approximating Lie algebroid cohomology associated to a Lie subalgebroid. This is a simultaneous generalisation of several classical constructions in differential geometry, including the Leray-Serre spectral sequence for de Rham cohomology associated to a fibration, the Hochschild-Serre spectral sequence for Lie algebras, and the Mackenzie spectral sequence for Lie algebroid extensions. We show that, for wide Lie subalgebroids, the spectral sequence converges to the Lie algebroid cohomology, and that, for Lie subalgebroids over proper submanifolds, the spectral sequence converges to the formal Lie algebroid cohomology. We discuss applications and recover several constructions in Poisson geometry in which this spectral sequence has appeared naturally in the literature.","sentences":["We study a spectral sequence approximating Lie algebroid cohomology associated to a Lie subalgebroid.","This is a simultaneous generalisation of several classical constructions in differential geometry, including the Leray-Serre spectral sequence for de Rham cohomology associated to a fibration, the Hochschild-Serre spectral sequence for Lie algebras, and the Mackenzie spectral sequence for Lie algebroid extensions.","We show that, for wide Lie subalgebroids, the spectral sequence converges to the Lie algebroid cohomology, and that, for Lie subalgebroids over proper submanifolds, the spectral sequence converges to the formal Lie algebroid cohomology.","We discuss applications and recover several constructions in Poisson geometry in which this spectral sequence has appeared naturally in the literature."],"url":"http://arxiv.org/abs/2405.00419v1","category":"math.DG"}
{"created":"2024-05-01 09:50:02","title":"Ergodicity for 2D Navier-Stokes equations with a degenerate pure jump noise","abstract":"In this paper, we establish the ergodicity for stochastic 2D Navier-Stokes equations driven by a highly degenerate pure jump L\\'evy noise. The noise could appear in as few as four directions. This gives an affirmative anwser to a longstanding problem. The case of Gaussian noise was treated in Hairer and Mattingly [\\emph{Ann. of Math.}, 164(3):993--1032, 2006]. To obtain the uniqueness of invariant measure, we use Malliavin calculus and anticipating stochastic calculus to establish the equi-continuity of the semigroup, the so-called {\\em e-property}, and prove some weak irreducibility of the solution process.","sentences":["In this paper, we establish the ergodicity for stochastic 2D Navier-Stokes equations driven by a highly degenerate pure jump L\\'evy noise.","The noise could appear in as few as four directions.","This gives an affirmative anwser to a longstanding problem.","The case of Gaussian noise was treated in Hairer and Mattingly [\\emph{Ann. of Math.}, 164(3):993--1032, 2006].","To obtain the uniqueness of invariant measure, we use Malliavin calculus and anticipating stochastic calculus to establish the equi-continuity of the semigroup, the so-called {\\em e-property}, and prove some weak irreducibility of the solution process."],"url":"http://arxiv.org/abs/2405.00414v1","category":"math.PR"}
{"created":"2024-05-01 09:49:20","title":"Structure of a fourth-order dispersive flow equation through the generalized Hasimoto transformation","abstract":"This paper focuses on a one-dimensional fourth-order nonlinear dispersive partial differential equation for curve flows on a K\\\"ahler manifold. The equation arises as a fourth-order extension of the one-dimensional Schr\\\"odinger flow equation, with physical and geometrical backgrounds. First, this paper presents a framework that can transform the equation into a system of fourth-order nonlinear dispersive partial differential-integral equations for complex-valued functions. This is achieved by developing the so-called generalized Hasimoto transformation, which enables us to handle general higher-dimensional compact K\\\"ahler manifolds. Second, this paper demonstrates the computations to obtain the explicit expression of the derived system for three examples of the compact K\\\"ahler manifolds, dealing with the complex Grassmannian as an example in detail.","sentences":["This paper focuses on a one-dimensional fourth-order nonlinear dispersive partial differential equation for curve flows on a K\\\"ahler manifold.","The equation arises as a fourth-order extension of the one-dimensional Schr\\\"odinger flow equation, with physical and geometrical backgrounds.","First, this paper presents a framework that can transform the equation into a system of fourth-order nonlinear dispersive partial differential-integral equations for complex-valued functions.","This is achieved by developing the so-called generalized Hasimoto transformation, which enables us to handle general higher-dimensional compact K\\\"ahler manifolds.","Second, this paper demonstrates the computations to obtain the explicit expression of the derived system for three examples of the compact K\\\"ahler manifolds, dealing with the complex Grassmannian as an example in detail."],"url":"http://arxiv.org/abs/2405.00412v1","category":"math.DG"}
{"created":"2024-05-01 09:12:22","title":"Maximal acceleration in Rainbow gravity","abstract":"In this paper, we derive maximal acceleration of a massive particle in Rainbow gravity. Using eight-dimensional phase-space metric compatible with Rainbow gravity, we obtain the maximal acceleration, valid up to first order in the Rainbow gravity parameter $\\eta$. Using the positivity condition on maximal acceleration, we find the upper bound on the Rainbow gravity parameter is of the order of $~10^{22}$ for positron and $10^{-44}$ for a black hole. After obtaining the expression for maximal acceleration for different choices of Rainbow functions, we derive corresponding modifications to Unruh temperature. Comparing with the observational value of the Unruh temperature, we find the upper bound on $\\eta$ as $~10^{32}$ for positron radiation. %and of the order of $10^{-100}$ for radiation from a black hole. We then derive geodesic equations for different choices of Rainbow functions and also obtain Newtonian limit of these geodesic equations. We find that the changes in the value of maximum acceleration, maximum temperature and Newtonian force equation are dependent on the choices of Rainbow functions.","sentences":["In this paper, we derive maximal acceleration of a massive particle in Rainbow gravity.","Using eight-dimensional phase-space metric compatible with Rainbow gravity, we obtain the maximal acceleration, valid up to first order in the Rainbow gravity parameter $\\eta$. Using the positivity condition on maximal acceleration, we find the upper bound on the Rainbow gravity parameter is of the order of $~10^{22}$ for positron and $10^{-44}$ for a black hole.","After obtaining the expression for maximal acceleration for different choices of Rainbow functions, we derive corresponding modifications to Unruh temperature.","Comparing with the observational value of the Unruh temperature, we find the upper bound on $\\eta$ as $~10^{32}$ for positron radiation.","%and of the order of $10^{-100}$ for radiation from a black hole.","We then derive geodesic equations for different choices of Rainbow functions and also obtain Newtonian limit of these geodesic equations.","We find that the changes in the value of maximum acceleration, maximum temperature and Newtonian force equation are dependent on the choices of Rainbow functions."],"url":"http://arxiv.org/abs/2405.00403v1","category":"gr-qc"}
{"created":"2024-05-01 09:06:30","title":"Optimized Drug Design using Multi-Objective Evolutionary Algorithms with SELFIES","abstract":"Computer aided drug design is a promising approach to reduce the tremendous costs, i.e. time and resources, for developing new medicinal drugs. It finds application in aiding the traversal of the vast chemical space of potentially useful compounds. In this paper, we deploy multi-objective evolutionary algorithms, namely NSGA-II, NSGA-III, and MOEA/D, for this purpose. At the same time, we used the SELFIES string representation method. In addition to the QED and SA score, we optimize compounds using the GuacaMol benchmark multi-objective task sets. Our results indicate that all three algorithms show converging behavior and successfully optimize the defined criteria whilst differing mainly in the number of potential solutions found. We observe that novel and promising candidates for synthesis are discovered among obtained compounds in the Pareto-sets.","sentences":["Computer aided drug design is a promising approach to reduce the tremendous costs, i.e. time and resources, for developing new medicinal drugs.","It finds application in aiding the traversal of the vast chemical space of potentially useful compounds.","In this paper, we deploy multi-objective evolutionary algorithms, namely NSGA-II, NSGA-III, and MOEA/D, for this purpose.","At the same time, we used the SELFIES string representation method.","In addition to the QED and SA score, we optimize compounds using the GuacaMol benchmark multi-objective task sets.","Our results indicate that all three algorithms show converging behavior and successfully optimize the defined criteria whilst differing mainly in the number of potential solutions found.","We observe that novel and promising candidates for synthesis are discovered among obtained compounds in the Pareto-sets."],"url":"http://arxiv.org/abs/2405.00401v1","category":"cs.NE"}
{"created":"2024-05-01 08:38:45","title":"Delamination Detection in Layered Waveguides using Ostrovsky Wave Packets","abstract":"We examine the scattering of Ostrovsky wave packets, generated from an incident solitary wave, in a two layered waveguide with a delamination in the centre and soft (imperfect) bonding either side of the centre. The layers of the waveguide are assumed to consist of different materials, and the strains are described by a system of coupled Boussinesq equations. A semi-analytical approach consisting of matched asymptotic multiple-scale expansions is applied, leading to Ostrovsky equations in soft bonded regions and Korteweg-de Vries equations in the delaminated region. This semi-analytical method has good agreement with direct numerical simulations, validating the approach.   In the delaminated regions, Ostrovsky wave packets evolve into a train of solitary waves, which subsequently evolve into Ostrovsky wave packets in the second bonded region. Analysis of the phase shift in the wave packet, introduced from the delaminated region, allows us to predict both the position and the length of the delamination; the first time this has been achieved using nonlinear waves. These results motivate experiments to validate the theoretical results, with the aim of creating a tool to monitor the integrity of layered structures.","sentences":["We examine the scattering of Ostrovsky wave packets, generated from an incident solitary wave, in a two layered waveguide with a delamination in the centre and soft (imperfect) bonding either side of the centre.","The layers of the waveguide are assumed to consist of different materials, and the strains are described by a system of coupled Boussinesq equations.","A semi-analytical approach consisting of matched asymptotic multiple-scale expansions is applied, leading to Ostrovsky equations in soft bonded regions and Korteweg-de Vries equations in the delaminated region.","This semi-analytical method has good agreement with direct numerical simulations, validating the approach.   ","In the delaminated regions, Ostrovsky wave packets evolve into a train of solitary waves, which subsequently evolve into Ostrovsky wave packets in the second bonded region.","Analysis of the phase shift in the wave packet, introduced from the delaminated region, allows us to predict both the position and the length of the delamination; the first time this has been achieved using nonlinear waves.","These results motivate experiments to validate the theoretical results, with the aim of creating a tool to monitor the integrity of layered structures."],"url":"http://arxiv.org/abs/2405.00388v1","category":"nlin.PS"}
{"created":"2024-05-01 08:25:32","title":"Modified least squares method and a review of its applications in machine learning and fractional differential/integral equations","abstract":"The least squares method provides the best-fit curve by minimizing the total squares error. In this work, we provide the modified least squares method based on the fractional orthogonal polynomials that belong to the space $M_{n}^{\\lambda} := \\text{span}\\{1,x^{\\lambda},x^{2\\lambda},\\ldots,x^{n\\lambda}\\},~\\lambda \\in (0,2]$. Numerical experiments demonstrate how to solve different problems using the modified least squares method. Moreover, the results show the advantage of the modified least squares method compared to the classical least squares method. Furthermore, we discuss the various applications of the modified least squares method in the fields like fractional differential/integral equations and machine learning.","sentences":["The least squares method provides the best-fit curve by minimizing the total squares error.","In this work, we provide the modified least squares method based on the fractional orthogonal polynomials that belong to the space $M_{n}^{\\lambda} := \\text{span}\\{1,x^{\\lambda},x^{2\\lambda},\\ldots,x^{n\\lambda}\\},~\\lambda \\in (0,2]$. Numerical experiments demonstrate how to solve different problems using the modified least squares method.","Moreover, the results show the advantage of the modified least squares method compared to the classical least squares method.","Furthermore, we discuss the various applications of the modified least squares method in the fields like fractional differential/integral equations and machine learning."],"url":"http://arxiv.org/abs/2405.00382v1","category":"math.NA"}
{"created":"2024-05-01 17:52:29","title":"Optimizing Profitability in Timely Gossip Networks","abstract":"We consider a communication system where a group of users, interconnected in a bidirectional gossip network, wishes to follow a time-varying source, e.g., updates on an event, in real-time. The users wish to maintain their expected version ages below a threshold, and can either rely on gossip from their neighbors or directly subscribe to a server publishing about the event, if the former option does not meet the timeliness requirements. The server wishes to maximize its profit by increasing subscriptions from users and minimizing event sampling frequency to reduce costs. This leads to a Stackelberg game between the server and the users where the sender is the leader deciding its sampling frequency and the users are the followers deciding their subscription strategies. We investigate equilibrium strategies for low-connectivity and high-connectivity topologies.","sentences":["We consider a communication system where a group of users, interconnected in a bidirectional gossip network, wishes to follow a time-varying source, e.g., updates on an event, in real-time.","The users wish to maintain their expected version ages below a threshold, and can either rely on gossip from their neighbors or directly subscribe to a server publishing about the event, if the former option does not meet the timeliness requirements.","The server wishes to maximize its profit by increasing subscriptions from users and minimizing event sampling frequency to reduce costs.","This leads to a Stackelberg game between the server and the users where the sender is the leader deciding its sampling frequency and the users are the followers deciding their subscription strategies.","We investigate equilibrium strategies for low-connectivity and high-connectivity topologies."],"url":"http://arxiv.org/abs/2405.00665v1","category":"cs.IT"}
{"created":"2024-05-01 17:35:54","title":"Shape optimization of slip-driven axisymmetric microswimmers","abstract":"In this work, we develop a computational framework that aims at simultaneously optimizing the shape and the slip velocity of an axisymmetric microswimmer suspended in a viscous fluid. We consider shapes of a given reduced volume that maximize the swimming efficiency, i.e., the (size-independent) ratio of the power loss arising from towing the rigid body of the same shape and size at the same translation velocity to the actual power loss incurred by swimming via the slip velocity. The optimal slip and efficiency (with shape fixed) are here given in terms of two Stokes flow solutions, and we then establish shape sensitivity formulas of adjoint-solution that provide objective function derivatives with respect to any set of shape parameters on the sole basis of the above two flow solutions. Our computational treatment relies on a fast and accurate boundary integral solver for solving all Stokes flow problems. We validate our analytic shape derivative formulas via comparisons against finite-difference gradient evaluations, and present several shape optimization examples.","sentences":["In this work, we develop a computational framework that aims at simultaneously optimizing the shape and the slip velocity of an axisymmetric microswimmer suspended in a viscous fluid.","We consider shapes of a given reduced volume that maximize the swimming efficiency, i.e., the (size-independent) ratio of the power loss arising from towing the rigid body of the same shape and size at the same translation velocity to the actual power loss incurred by swimming via the slip velocity.","The optimal slip and efficiency (with shape fixed) are here given in terms of two Stokes flow solutions, and we then establish shape sensitivity formulas of adjoint-solution that provide objective function derivatives with respect to any set of shape parameters on the sole basis of the above two flow solutions.","Our computational treatment relies on a fast and accurate boundary integral solver for solving all Stokes flow problems.","We validate our analytic shape derivative formulas via comparisons against finite-difference gradient evaluations, and present several shape optimization examples."],"url":"http://arxiv.org/abs/2405.00656v1","category":"math.OC"}
{"created":"2024-05-01 17:26:12","title":"Electronic Coherences in Molecules: The Projected Nuclear Quantum Momentum as a Hidden Agent","abstract":"Electronic coherences are key to understanding and controlling photo-induced molecular transformations. We identify a crucial quantum-mechanical feature of electron-nuclear correlation, the projected nuclear quantum momenta, essential to capture the correct coherence behavior. In simulations, we show that, unlike traditional trajectory-based schemes, exact-factorization-based methods approximate these correlation terms, and correctly capture electronic coherences in a range of situations, including their spatial dependence, an important aspect that influences subsequent electron dynamics and that is becoming accessible in more experiments.","sentences":["Electronic coherences are key to understanding and controlling photo-induced molecular transformations.","We identify a crucial quantum-mechanical feature of electron-nuclear correlation, the projected nuclear quantum momenta, essential to capture the correct coherence behavior.","In simulations, we show that, unlike traditional trajectory-based schemes, exact-factorization-based methods approximate these correlation terms, and correctly capture electronic coherences in a range of situations, including their spatial dependence, an important aspect that influences subsequent electron dynamics and that is becoming accessible in more experiments."],"url":"http://arxiv.org/abs/2405.00649v1","category":"physics.chem-ph"}
{"created":"2024-05-01 17:16:46","title":"Electronic and Optical Excitations in van der Waals Materials from a Non-Empirical Wannier-Localized Optimally-Tuned Screened Range-Separated Hybrid Functional","abstract":"Accurate prediction of electronic and optical excitations in van der Waals (vdW) materials is a long-standing challenge for density functional theory. The recently proposed Wannier-localized optimally-tuned screened range-separated hybrid (WOT-SRSH) functional has proven successful in non-empirical determination of electronic band gaps and optical absorption spectra for various covalent and ionic crystals. However, for vdW materials the tuning of the material- and structure-dependent functional parameters has, until now, only been attained semi-empirically. Here, we present a non-empirical WOT-SRSH approach applicable to vdW materials, with the optimal functional parameters transferable between monolayer and bulk. We apply this methodology to prototypical vdW materials: black phosphorus, molybdenum disulfide, and hexagonal boron nitride (in the latter case including zero-point renormalization). We show that the WOT-SRSH approach consistently achieves accuracy levels comparable to experiments and ab initio many-body perturbation theory (MBPT) calculations for band structures and optical absorption spectra, both on its own and as an optimal starting point for MBPT calculations.","sentences":["Accurate prediction of electronic and optical excitations in van der Waals (vdW) materials is a long-standing challenge for density functional theory.","The recently proposed Wannier-localized optimally-tuned screened range-separated hybrid (WOT-SRSH) functional has proven successful in non-empirical determination of electronic band gaps and optical absorption spectra for various covalent and ionic crystals.","However, for vdW materials the tuning of the material- and structure-dependent functional parameters has, until now, only been attained semi-empirically.","Here, we present a non-empirical WOT-SRSH approach applicable to vdW materials, with the optimal functional parameters transferable between monolayer and bulk.","We apply this methodology to prototypical vdW materials: black phosphorus, molybdenum disulfide, and hexagonal boron nitride (in the latter case including zero-point renormalization).","We show that the WOT-SRSH approach consistently achieves accuracy levels comparable to experiments and ab initio many-body perturbation theory (MBPT) calculations for band structures and optical absorption spectra, both on its own and as an optimal starting point for MBPT calculations."],"url":"http://arxiv.org/abs/2405.00643v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-01 16:48:28","title":"Queue-based Eco-Driving at Roundabouts with Reinforcement Learning","abstract":"We address eco-driving at roundabouts in mixed traffic to enhance traffic flow and traffic efficiency in urban areas. The aim is to proactively optimize speed of automated or non-automated connected vehicles (CVs), ensuring both an efficient approach and smooth entry into roundabouts. We incorporate the traffic situation ahead, i.e. preceding vehicles and waiting queues. Further, we develop two approaches: a rule-based and an Reinforcement Learning (RL) based eco-driving system, with both using the approach link and information from conflicting CVs for speed optimization. A fair comparison of rule-based and RL-based approaches is performed to explore RL as a viable alternative to classical optimization. Results show that both approaches outperform the baseline. Improvements significantly increase with growing traffic volumes, leading to best results on average being obtained at high volumes. Near capacity, performance deteriorates, indicating limited applicability at capacity limits. Examining different CV penetration rates, a decline in performance is observed, but with substantial results still being achieved at lower CV rates. RL agents can discover effective policies for speed optimization in dynamic roundabout settings, but they do not offer a substantial advantage over classical approaches, especially at higher traffic volumes or lower CV penetration rates.","sentences":["We address eco-driving at roundabouts in mixed traffic to enhance traffic flow and traffic efficiency in urban areas.","The aim is to proactively optimize speed of automated or non-automated connected vehicles (CVs), ensuring both an efficient approach and smooth entry into roundabouts.","We incorporate the traffic situation ahead, i.e. preceding vehicles and waiting queues.","Further, we develop two approaches: a rule-based and an Reinforcement Learning (RL) based eco-driving system, with both using the approach link and information from conflicting CVs for speed optimization.","A fair comparison of rule-based and RL-based approaches is performed to explore RL as a viable alternative to classical optimization.","Results show that both approaches outperform the baseline.","Improvements significantly increase with growing traffic volumes, leading to best results on average being obtained at high volumes.","Near capacity, performance deteriorates, indicating limited applicability at capacity limits.","Examining different CV penetration rates, a decline in performance is observed, but with substantial results still being achieved at lower CV rates.","RL agents can discover effective policies for speed optimization in dynamic roundabout settings, but they do not offer a substantial advantage over classical approaches, especially at higher traffic volumes or lower CV penetration rates."],"url":"http://arxiv.org/abs/2405.00625v1","category":"cs.LG"}
{"created":"2024-05-01 16:35:44","title":"An Expectation-Maximization Relaxed Method for Privacy Funnel","abstract":"The privacy funnel (PF) gives a framework of privacy-preserving data release, where the goal is to release useful data while also limiting the exposure of associated sensitive information. This framework has garnered significant interest due to its broad applications in characterization of the privacy-utility tradeoff. Hence, there is a strong motivation to develop numerical methods with high precision and theoretical convergence guarantees. In this paper, we propose a novel relaxation variant based on Jensen's inequality of the objective function for the computation of the PF problem. This model is proved to be equivalent to the original in terms of optimal solutions and optimal values. Based on our proposed model, we develop an accurate algorithm which only involves closed-form iterations. The convergence of our algorithm is theoretically guaranteed through descent estimation and Pinsker's inequality. Numerical results demonstrate the effectiveness of our proposed algorithm.","sentences":["The privacy funnel (PF) gives a framework of privacy-preserving data release, where the goal is to release useful data while also limiting the exposure of associated sensitive information.","This framework has garnered significant interest due to its broad applications in characterization of the privacy-utility tradeoff.","Hence, there is a strong motivation to develop numerical methods with high precision and theoretical convergence guarantees.","In this paper, we propose a novel relaxation variant based on Jensen's inequality of the objective function for the computation of the PF problem.","This model is proved to be equivalent to the original in terms of optimal solutions and optimal values.","Based on our proposed model, we develop an accurate algorithm which only involves closed-form iterations.","The convergence of our algorithm is theoretically guaranteed through descent estimation and Pinsker's inequality.","Numerical results demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2405.00616v1","category":"cs.IT"}
{"created":"2024-05-01 16:17:39","title":"A Preprocessing and Evaluation Toolbox for Trajectory Prediction Research on the Drone Datasets","abstract":"The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles. This paper highlights the need for standardizing the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this. Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualizing, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems. The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies. The toolbox is available at: https://github.com/westny/dronalize.","sentences":["The availability of high-quality datasets is crucial for the development of behavior prediction algorithms in autonomous vehicles.","This paper highlights the need for standardizing the use of certain datasets for motion forecasting research to simplify comparative analysis and proposes a set of tools and practices to achieve this.","Drawing on extensive experience and a comprehensive review of current literature, we summarize our proposals for preprocessing, visualizing, and evaluation in the form of an open-sourced toolbox designed for researchers working on trajectory prediction problems.","The clear specification of necessary preprocessing steps and evaluation metrics is intended to alleviate development efforts and facilitate the comparison of results across different studies.","The toolbox is available at: https://github.com/westny/dronalize."],"url":"http://arxiv.org/abs/2405.00604v1","category":"cs.RO"}
{"created":"2024-05-01 16:13:31","title":"How founder motivations, goals, and actions influence early trajectories of online communities","abstract":"Online communities offer their members various benefits, such as information access, social and emotional support, and entertainment. Despite the important role that founders play in shaping communities, prior research has focused primarily on what drives users to participate and contribute; the motivations and goals of founders remain underexplored. To uncover how and why online communities get started, we present findings from a survey of 951 recent founders of Reddit communities. We find that topical interest is the most common motivation for community creation, followed by motivations to exchange information, connect with others, and self-promote. Founders have heterogeneous goals for their nascent communities, but they tend to privilege community quality and engagement over sheer growth. These differences in founders' early attitudes towards their communities help predict not only the community-building actions that they pursue, but also the ability of their communities to attract visitors, contributors, and subscribers over the first 28 days. We end with a discussion of the implications for researchers, designers, and founders of online communities.","sentences":["Online communities offer their members various benefits, such as information access, social and emotional support, and entertainment.","Despite the important role that founders play in shaping communities, prior research has focused primarily on what drives users to participate and contribute; the motivations and goals of founders remain underexplored.","To uncover how and why online communities get started, we present findings from a survey of 951 recent founders of Reddit communities.","We find that topical interest is the most common motivation for community creation, followed by motivations to exchange information, connect with others, and self-promote.","Founders have heterogeneous goals for their nascent communities, but they tend to privilege community quality and engagement over sheer growth.","These differences in founders' early attitudes towards their communities help predict not only the community-building actions that they pursue, but also the ability of their communities to attract visitors, contributors, and subscribers over the first 28 days.","We end with a discussion of the implications for researchers, designers, and founders of online communities."],"url":"http://arxiv.org/abs/2405.00601v1","category":"cs.HC"}
{"created":"2024-05-01 15:40:38","title":"Construction of extremal Type II $\\mathbb{Z}_{8}$-codes via doubling method","abstract":"Extremal Type II $\\mathbb{Z}_{8}$-codes are a class of self-dual $\\mathbb{Z}_{8}$-codes with Euclidean weights divisible by $16$ and the largest possible minimum Euclidean weight for a given length. We introduce a doubling method for constructing a Type II $\\mathbb{Z}_{2k}$-code of length $n$ from a known Type II $\\mathbb{Z}_{2k}$-code of length $n$. Based on this method, we develop an algorithm to construct new extremal Type II $\\mathbb{Z}_8$-codes starting from an extremal Type II $\\mathbb{Z}_8$-code of type $(\\frac{n}{2},0,0)$ with an extremal $\\mathbb{Z}_4$-residue code and length $24, 32$ or $40$.   We construct at least ten new extremal Type II $\\mathbb{Z}_8$-codes of length $32$ and type $(15,1,1)$. Extremal Type II $\\mathbb{Z}_8$-codes of length $32$ of this type were not known before. Moreover, the binary residue codes of the constructed extremal $\\mathbb{Z}_8$-codes are optimal $[32,15]$ binary codes.","sentences":["Extremal Type II $\\mathbb{Z}_{8}$-codes are a class of self-dual $\\mathbb{Z}_{8}$-codes with Euclidean weights divisible by $16$ and the largest possible minimum Euclidean weight for a given length.","We introduce a doubling method for constructing a Type II $\\mathbb{Z}_{2k}$-code of length $n$ from a known Type II $\\mathbb{Z}_{2k}$-code of length $n$. Based on this method, we develop an algorithm to construct new extremal Type II $\\mathbb{Z}_8$-codes starting from an extremal Type II $\\mathbb{Z}_8$-code of type $(\\frac{n}{2},0,0)$ with an extremal $\\mathbb{Z}_4$-residue code and length $24, 32$ or $40$.   We construct at least ten new extremal Type II $\\mathbb{Z}_8$-codes of length $32$ and type $(15,1,1)$. Extremal Type II $\\mathbb{Z}_8$-codes of length $32$ of this type were not known before.","Moreover, the binary residue codes of the constructed extremal $\\mathbb{Z}_8$-codes are optimal $","[32,15]$ binary codes."],"url":"http://arxiv.org/abs/2405.00584v1","category":"cs.IT"}
{"created":"2024-05-01 15:32:22","title":"LEAP: Optimization Hierarchical Federated Learning on Non-IID Data with Coalition Formation Game","abstract":"Although Hierarchical Federated Learning (HFL) utilizes edge servers (ESs) to alleviate communication burdens, its model performance will be degraded by non-IID data and limited communication resources. Current works often assume that data is uniformly distributed, which however contradicts the heterogeneity of IoT. Solutions of additional model training to check the data distribution inevitably increases computational costs and the risk of privacy leakage. The challenges in solving these issues are how to reduce the impact of non-IID data without involving raw data and how to rationalize the communication resource allocation for addressing straggler problem. To tackle these challenges, we propose a novel optimization method based on coaLition formation gamE and grAdient Projection, called LEAP. Specifically, we combine edge data distribution with coalition formation game innovatively to adjust the correlations between clients and ESs dynamically, which ensures optimal correlations. We further capture the client heterogeneity to achieve the rational bandwidth allocation from coalition perception and determine the optimal transmission power within specified delay constraints at client level. Experimental results on four real datasets show that LEAP is able to achieve 20.62% improvement in model accuracy compared to the state-of-the-art baselines. Moreover, LEAP effectively reduce transmission energy consumption by at least about 2.24 times.","sentences":["Although Hierarchical Federated Learning (HFL) utilizes edge servers (ESs) to alleviate communication burdens, its model performance will be degraded by non-IID data and limited communication resources.","Current works often assume that data is uniformly distributed, which however contradicts the heterogeneity of IoT. Solutions of additional model training to check the data distribution inevitably increases computational costs and the risk of privacy leakage.","The challenges in solving these issues are how to reduce the impact of non-IID data without involving raw data and how to rationalize the communication resource allocation for addressing straggler problem.","To tackle these challenges, we propose a novel optimization method based on coaLition formation gamE and grAdient Projection, called LEAP.","Specifically, we combine edge data distribution with coalition formation game innovatively to adjust the correlations between clients and ESs dynamically, which ensures optimal correlations.","We further capture the client heterogeneity to achieve the rational bandwidth allocation from coalition perception and determine the optimal transmission power within specified delay constraints at client level.","Experimental results on four real datasets show that LEAP is able to achieve 20.62% improvement in model accuracy compared to the state-of-the-art baselines.","Moreover, LEAP effectively reduce transmission energy consumption by at least about 2.24 times."],"url":"http://arxiv.org/abs/2405.00579v1","category":"cs.GT"}
{"created":"2024-05-01 15:12:53","title":"Physics-Informed Acoustic Liner Optimization: Balancing Drag and Noise","abstract":"We present pore-resolved Direct Numerical Simulations (DNS) of turbulent flows grazing over acoustic liners with aerodynamically and/or acoustically optimized orifice configurations. Our DNS explore a large parameter space, studying various families of orifice geometries, including the influence of orifice shape, orientation, and the number of orifices. All flow cases show an increase in drag compared to the smooth wall. However, the added drag can be reduced by as much as $\\sim$55\\% as compared to conventional acoustic liners by simply altering the shape of the orifice or its orientation, in the case of a non-circular orifice. Complementary acoustic simulations demonstrate that this reduced drag may be achieved while maintaining the same noise reduction properties over a wide range of frequencies.","sentences":["We present pore-resolved Direct Numerical Simulations (DNS) of turbulent flows grazing over acoustic liners with aerodynamically and/or acoustically optimized orifice configurations.","Our DNS explore a large parameter space, studying various families of orifice geometries, including the influence of orifice shape, orientation, and the number of orifices.","All flow cases show an increase in drag compared to the smooth wall.","However, the added drag can be reduced by as much as $\\sim$55\\% as compared to conventional acoustic liners by simply altering the shape of the orifice or its orientation, in the case of a non-circular orifice.","Complementary acoustic simulations demonstrate that this reduced drag may be achieved while maintaining the same noise reduction properties over a wide range of frequencies."],"url":"http://arxiv.org/abs/2405.00563v1","category":"physics.flu-dyn"}
{"created":"2024-05-01 14:50:58","title":"Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs","abstract":"We present a novel approach for long-term human trajectory prediction, which is essential for long-horizon robot planning in human-populated environments. State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment. In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s. We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene. This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation. We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains. To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions. We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood (NLL) and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged baselines for a time horizon of 60s.","sentences":["We present a novel approach for long-term human trajectory prediction, which is essential for long-horizon robot planning in human-populated environments.","State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment.","In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s.","We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene.","This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation.","We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains.","To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions.","We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood (NLL) and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged baselines for a time horizon of 60s."],"url":"http://arxiv.org/abs/2405.00552v1","category":"cs.RO"}
{"created":"2024-05-01 14:31:43","title":"A Double Maximization Approach for Optimizing the LM Rate of Mismatched Decoding","abstract":"An approach is established for maximizing the Lower bound on the Mismatch capacity (hereafter abbreviated as LM rate), a key performance bound in mismatched decoding, by optimizing the channel input probability distribution. Under a fixed channel input probability distribution, the computation of the corresponding LM rate is a convex optimization problem. When optimizing the channel input probability distribution, however, the corresponding optimization problem adopts a max-min formulation, which is generally non-convex and is intractable with standard approaches. To solve this problem, a novel dual form of the LM rate is proposed, thereby transforming the max-min formulation into an equivalent double maximization formulation. This new formulation leads to a maximization problem setup wherein each individual optimization direction is convex. Consequently, an alternating maximization algorithm is established to solve the resultant maximization problem setup. Each step of the algorithm only involves a closed-form iteration, which is efficiently implemented with standard optimization procedures. Numerical experiments show the proposed approach for optimizing the LM rate leads to noticeable rate gains.","sentences":["An approach is established for maximizing the Lower bound on the Mismatch capacity (hereafter abbreviated as LM rate), a key performance bound in mismatched decoding, by optimizing the channel input probability distribution.","Under a fixed channel input probability distribution, the computation of the corresponding LM rate is a convex optimization problem.","When optimizing the channel input probability distribution, however, the corresponding optimization problem adopts a max-min formulation, which is generally non-convex and is intractable with standard approaches.","To solve this problem, a novel dual form of the LM rate is proposed, thereby transforming the max-min formulation into an equivalent double maximization formulation.","This new formulation leads to a maximization problem setup wherein each individual optimization direction is convex.","Consequently, an alternating maximization algorithm is established to solve the resultant maximization problem setup.","Each step of the algorithm only involves a closed-form iteration, which is efficiently implemented with standard optimization procedures.","Numerical experiments show the proposed approach for optimizing the LM rate leads to noticeable rate gains."],"url":"http://arxiv.org/abs/2405.00545v1","category":"cs.IT"}
{"created":"2024-05-01 14:19:30","title":"Quantifying Price Improvement in Order Flow Auctions","abstract":"This work introduces a framework for evaluating onchain order flow auctions (OFAs), emphasizing the metric of price improvement. Utilizing a set of open-source tools, our methodology systematically attributes price improvements to specific modifiable inputs of the system such as routing efficiency, gas optimization, and priority fee settings. When applied to leading Ethereum-based trading interfaces such as 1Inch and Uniswap, the results reveal that auction-enhanced interfaces can provide statistically significant improvements in trading outcomes, averaging 4-5 basis points in our sample. We further identify the sources of such price improvements to be added liquidity for large swaps. This research lays a foundation for future innovations in blockchain based trading platforms.","sentences":["This work introduces a framework for evaluating onchain order flow auctions (OFAs), emphasizing the metric of price improvement.","Utilizing a set of open-source tools, our methodology systematically attributes price improvements to specific modifiable inputs of the system such as routing efficiency, gas optimization, and priority fee settings.","When applied to leading Ethereum-based trading interfaces such as 1Inch and Uniswap, the results reveal that auction-enhanced interfaces can provide statistically significant improvements in trading outcomes, averaging 4-5 basis points in our sample.","We further identify the sources of such price improvements to be added liquidity for large swaps.","This research lays a foundation for future innovations in blockchain based trading platforms."],"url":"http://arxiv.org/abs/2405.00537v1","category":"q-fin.TR"}
{"created":"2024-05-01 13:58:01","title":"DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries Cryptocurrency Trend Forecasting","abstract":"In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities. Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences. This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data. Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets. By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\\% in prediction accuracy. This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies. Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation.","sentences":["In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities.","Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences.","This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data.","Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets.","By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\\% in prediction accuracy.","This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies.","Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation."],"url":"http://arxiv.org/abs/2405.00522v1","category":"econ.GN"}
{"created":"2024-05-01 13:51:39","title":"GAD-Generative Learning for HD Map-Free Autonomous Driving","abstract":"Deep-learning-based techniques have been widely adopted for autonomous driving software stacks for mass production in recent years, focusing primarily on perception modules, with some work extending this method to prediction modules. However, the downstream planning and control modules are still designed with hefty handcrafted rules, dominated by optimization-based methods such as quadratic programming or model predictive control. This results in a performance bottleneck for autonomous driving systems in that corner cases simply cannot be solved by enumerating hand-crafted rules. We present a deep-learning-based approach that brings prediction, decision, and planning modules together with the attempt to overcome the rule-based methods' deficiency in real-world applications of autonomous driving, especially for urban scenes. The DNN model we proposed is solely trained with 10 hours of human driver data, and it supports all mass-production ADAS features available on the market to date. This method is deployed onto a Jiyue test car with no modification to its factory-ready sensor set and compute platform. the feasibility, usability, and commercial potential are demonstrated in this article.","sentences":["Deep-learning-based techniques have been widely adopted for autonomous driving software stacks for mass production in recent years, focusing primarily on perception modules, with some work extending this method to prediction modules.","However, the downstream planning and control modules are still designed with hefty handcrafted rules, dominated by optimization-based methods such as quadratic programming or model predictive control.","This results in a performance bottleneck for autonomous driving systems in that corner cases simply cannot be solved by enumerating hand-crafted rules.","We present a deep-learning-based approach that brings prediction, decision, and planning modules together with the attempt to overcome the rule-based methods' deficiency in real-world applications of autonomous driving, especially for urban scenes.","The DNN model we proposed is solely trained with 10 hours of human driver data, and it supports all mass-production ADAS features available on the market to date.","This method is deployed onto a Jiyue test car with no modification to its factory-ready sensor set and compute platform.","the feasibility, usability, and commercial potential are demonstrated in this article."],"url":"http://arxiv.org/abs/2405.00515v1","category":"cs.RO"}
{"created":"2024-05-01 12:50:32","title":"Time and frequency domain low order, low frequency approximation of mechanical systems","abstract":"Control design for linear, time-invariant mechanical systems typically requires an accurate low-order approximation in the low frequency range. For example a series expansion of the transfer function around zero consisting of a mass, velocity, and compliance term. Because computing such a series expansion of the transfer function can be cumbersome, a new method to compute low-order approximations of mechanical systems is developed in this paper. The method does not require an explicit expression for the transfer function, which is not always available for infinite-dimensional systems. The advantages of the proposed method is demonstrated in three examples.","sentences":["Control design for linear, time-invariant mechanical systems typically requires an accurate low-order approximation in the low frequency range.","For example a series expansion of the transfer function around zero consisting of a mass, velocity, and compliance term.","Because computing such a series expansion of the transfer function can be cumbersome, a new method to compute low-order approximations of mechanical systems is developed in this paper.","The method does not require an explicit expression for the transfer function, which is not always available for infinite-dimensional systems.","The advantages of the proposed method is demonstrated in three examples."],"url":"http://arxiv.org/abs/2405.00486v1","category":"math.OC"}
{"created":"2024-05-01 12:49:38","title":"Spin Hamiltonians in the Modulated Momenta of Light","abstract":"Photonic solvers that are able to find the ground states of different spin Hamiltonians can be used to study many interactive physical systems and combinatorial optimization problems. Here, we establish a real-and-momentum space correspondence of spin Hamiltonians by spatial light transport. The real-space spin interaction is determined by modulating the momentum-space flow of light. This principle is formulated as a generalized Plancherel theorem, allowing us to implement a simple optical simulator that can find the ground states for any displacement-dependent spin interactions. Particularly, we use this principle to reveal the exotic magnetic phase diagram from a J1-J2-J3 model, and we also observe the vortex-mediated Berezinskii-Kosterlitz-Thouless dynamics from the XY model. These experiments exhibit high calculation precision by subtly controlling spin interactions from the momentum space of light, offering a promising scheme to explore novel physical effects.","sentences":["Photonic solvers that are able to find the ground states of different spin Hamiltonians can be used to study many interactive physical systems and combinatorial optimization problems.","Here, we establish a real-and-momentum space correspondence of spin Hamiltonians by spatial light transport.","The real-space spin interaction is determined by modulating the momentum-space flow of light.","This principle is formulated as a generalized Plancherel theorem, allowing us to implement a simple optical simulator that can find the ground states for any displacement-dependent spin interactions.","Particularly, we use this principle to reveal the exotic magnetic phase diagram from a J1-J2-J3 model, and we also observe the vortex-mediated Berezinskii-Kosterlitz-Thouless dynamics from the XY model.","These experiments exhibit high calculation precision by subtly controlling spin interactions from the momentum space of light, offering a promising scheme to explore novel physical effects."],"url":"http://arxiv.org/abs/2405.00484v1","category":"physics.optics"}
{"created":"2024-05-01 12:39:35","title":"Enhanced Visual Question Answering: A Comparative Analysis and Textual Feature Extraction Via Convolutions","abstract":"Visual Question Answering (VQA) has emerged as a highly engaging field in recent years, attracting increasing research efforts aiming to enhance VQA accuracy through the deployment of advanced models such as Transformers. Despite this growing interest, there has been limited exploration into the comparative analysis and impact of textual modalities within VQA, particularly in terms of model complexity and its effect on performance. In this work, we conduct a comprehensive comparison between complex textual models that leverage long dependency mechanisms and simpler models focusing on local textual features within a well-established VQA framework. Our findings reveal that employing complex textual encoders is not invariably the optimal approach for the VQA-v2 dataset. Motivated by this insight, we introduce an improved model, ConvGRU, which incorporates convolutional layers to enhance the representation of question text. Tested on the VQA-v2 dataset, ConvGRU achieves better performance without substantially increasing parameter complexity.","sentences":["Visual Question Answering (VQA) has emerged as a highly engaging field in recent years, attracting increasing research efforts aiming to enhance VQA accuracy through the deployment of advanced models such as Transformers.","Despite this growing interest, there has been limited exploration into the comparative analysis and impact of textual modalities within VQA, particularly in terms of model complexity and its effect on performance.","In this work, we conduct a comprehensive comparison between complex textual models that leverage long dependency mechanisms and simpler models focusing on local textual features within a well-established VQA framework.","Our findings reveal that employing complex textual encoders is not invariably the optimal approach for the VQA-v2 dataset.","Motivated by this insight, we introduce an improved model, ConvGRU, which incorporates convolutional layers to enhance the representation of question text.","Tested on the VQA-v2 dataset, ConvGRU achieves better performance without substantially increasing parameter complexity."],"url":"http://arxiv.org/abs/2405.00479v1","category":"cs.CV"}
{"created":"2024-05-01 12:21:29","title":"On Convergence of Discrete Schemes for Computing the Rate-Distortion Function of Continuous Source","abstract":"Computing the rate-distortion function for continuous sources is commonly regarded as a standard continuous optimization problem. When numerically addressing this problem, a typical approach involves discretizing the source space and subsequently solving the associated discrete problem. However, existing literature has predominantly concentrated on the convergence analysis of solving discrete problems, usually neglecting the convergence relationship between the original continuous optimization and its associated discrete counterpart. This neglect is not rigorous, since the solution of a discrete problem does not necessarily imply convergence to the solution of the original continuous problem, especially for non-linear problems. To address this gap, our study employs rigorous mathematical analysis, which constructs a series of finite-dimensional spaces approximating the infinite-dimensional space of the probability measure, establishing that solutions from discrete schemes converge to those from the continuous problems.","sentences":["Computing the rate-distortion function for continuous sources is commonly regarded as a standard continuous optimization problem.","When numerically addressing this problem, a typical approach involves discretizing the source space and subsequently solving the associated discrete problem.","However, existing literature has predominantly concentrated on the convergence analysis of solving discrete problems, usually neglecting the convergence relationship between the original continuous optimization and its associated discrete counterpart.","This neglect is not rigorous, since the solution of a discrete problem does not necessarily imply convergence to the solution of the original continuous problem, especially for non-linear problems.","To address this gap, our study employs rigorous mathematical analysis, which constructs a series of finite-dimensional spaces approximating the infinite-dimensional space of the probability measure, establishing that solutions from discrete schemes converge to those from the continuous problems."],"url":"http://arxiv.org/abs/2405.00474v1","category":"cs.IT"}
{"created":"2024-05-01 11:58:48","title":"On the best constants of Schur multipliers of second order divided difference functions","abstract":"We give a new proof of the boundedness of bilinear Schur multipliers of second order divided difference functions, as obtained earlier by Potapov, Skripka and Sukochev in their proof of Koplienko's conjecture on the existence of higher order spectral shift functions. Our proof is based on recent methods involving bilinear transference and the H\\\"ormander-Mikhlin-Schur multiplier theorem. Our approach provides a significant sharpening of the known asymptotic bounds of bilinear Schur multipliers of second order divided difference functions. Furthermore, we give a new lower bound of these bilinear Schur multipliers, giving again a fundamental improvement on the best known bounds obtained by Coine, Le Merdy, Potapov, Sukochev and Tomskova.   More precisely, we prove that for $f \\in C^2(\\mathbb{R})$ and $1 < p, p_1, p_2 < \\infty$ with $\\frac{1}{p} = \\frac{1}{p_1} + \\frac{1}{p_2}$ we have \\[ \\Vert M_{f^{[2]}}: S_{p_1} \\times S_{p_2} \\rightarrow S_p \\Vert \\lesssim \\Vert f'' \\Vert_\\infty D(p, p_1, p_2), \\] where the constant $D(p, p_1, p_2)$ is specified in Theorem 7.1 and $D(p, 2p, 2p) \\approx p^4 p^\\ast$ with $p^\\ast$ the H\\\"older conjugate of $p$. We further show that for $f(\\lambda) = \\lambda \\vert \\lambda \\vert$, $\\lambda \\in \\mathbb{R}$, for every $1 < p < \\infty$ we have \\[ p^2 p^\\ast \\lesssim \\Vert M_{f^{[2]}}: S_{2p} \\times S_{2p} \\rightarrow S_p \\Vert. \\] Here $f^{[2]}$ is the second order divided difference function of $f$ with $M_{f^{[2]}}$ the associated Schur multiplier. In particular it follows that our estimate $D(p, 2p, 2p)$ is optimal for $p \\searrow 1$.","sentences":["We give a new proof of the boundedness of bilinear Schur multipliers of second order divided difference functions, as obtained earlier by Potapov, Skripka and Sukochev in their proof of Koplienko's conjecture on the existence of higher order spectral shift functions.","Our proof is based on recent methods involving bilinear transference and the H\\\"ormander-Mikhlin-Schur multiplier theorem.","Our approach provides a significant sharpening of the known asymptotic bounds of bilinear Schur multipliers of second order divided difference functions.","Furthermore, we give a new lower bound of these bilinear Schur multipliers, giving again a fundamental improvement on the best known bounds obtained by Coine, Le Merdy, Potapov, Sukochev and Tomskova.   ","More precisely, we prove that for $f \\in C^2(\\mathbb{R})$ and $1 < p, p_1, p_2 < \\infty$ with $\\frac{1}{p} = \\frac{1}{p_1} + \\frac{1}{p_2}$ we have \\[ \\Vert M_{f^{[2]}}: S_{p_1} \\times S_{p_2} \\rightarrow S_p \\Vert \\lesssim \\Vert f'' \\Vert_\\infty D(p, p_1, p_2), \\] where the constant $D(p, p_1, p_2)$ is specified in Theorem 7.1 and $D(p, 2p, 2p)","\\approx p^4 p^\\ast$ with $p^\\ast$ the H\\\"older conjugate of $p$. We further show that for $f(\\lambda) = \\lambda \\vert \\lambda \\vert$, $\\lambda \\in \\mathbb{R}$, for every $1 < p < \\infty$ we have \\[ p^2 p^\\ast \\lesssim \\Vert M_{f^{[2]}}: S_{2p} \\times S_{2p} \\rightarrow S_p \\Vert.","\\]","Here $f^{[2]}$ is the second order divided difference function of $f$ with $M_{f^{[2]}}$ the associated Schur multiplier.","In particular it follows that our estimate $D(p, 2p, 2p)$ is optimal for $p \\searrow 1$."],"url":"http://arxiv.org/abs/2405.00464v1","category":"math.CA"}
{"created":"2024-05-01 11:03:11","title":"A Modelling Framework for Energy-Management and Eco-Driving Problems using Convex Relaxations","abstract":"This paper presents a convex optimization framework for eco-driving and vehicle energy management problems. We will first show that several types of eco-driving and vehicle energy management problems can be modelled using the same notions of energy storage buffers and energy storage converters that are connected to a power network. It will be shown that these problems can be formulated as optimization problems with linear cost functions and linear dynamics, and nonlinear constraints representing the power converters. We will show that under some mild conditions, the (non-convex) optimization problem has the same (globally) optimal solution as a convex relaxation. This means that the problems can be solved efficiently and that the solution is guaranteed to be globally optimal. Finally, a numerical example of the eco-driving problem is used to illustrate this claim.","sentences":["This paper presents a convex optimization framework for eco-driving and vehicle energy management problems.","We will first show that several types of eco-driving and vehicle energy management problems can be modelled using the same notions of energy storage buffers and energy storage converters that are connected to a power network.","It will be shown that these problems can be formulated as optimization problems with linear cost functions and linear dynamics, and nonlinear constraints representing the power converters.","We will show that under some mild conditions, the (non-convex) optimization problem has the same (globally) optimal solution as a convex relaxation.","This means that the problems can be solved efficiently and that the solution is guaranteed to be globally optimal.","Finally, a numerical example of the eco-driving problem is used to illustrate this claim."],"url":"http://arxiv.org/abs/2405.00447v1","category":"math.OC"}
{"created":"2024-05-01 10:15:45","title":"Quantum Monte Carlo study of the phase diagram of the two-dimensional uniform electron liquid","abstract":"We present a study of spin-unpolarized and spin-polarized two-dimensional uniform electron liquids using variational and diffusion quantum Monte Carlo (VMC and DMC) methods with Slater-Jastrow-backflow trial wave functions. Ground-state VMC and DMC energies are obtained in the density range $1 \\leq r_\\text{s} \\leq 40$. Single-particle and many-body finite-size errors are corrected using canonical-ensemble twist-averaged boundary conditions and extrapolation of twist-averaged energies to the thermodynamic limit of infinite system size. System-size-dependent errors in Slater-Jastrow-backflow DMC energies caused by partially converged VMC energy minimization calculations are discussed. We find that, for $1 \\leq r_\\text{s} \\leq 5$, optimizing the backflow function at each twist lowers the twist-averaged DMC energy at finite system size. However, nonsystematic system-size-dependent effects remain in the DMC energies, which can be partially removed by extrapolation from multiple finite system sizes to infinite system size. We attribute these nonsystematic effects to the close competition between fluid and defected crystal phases at different system sizes at low density. The DMC energies in the thermodynamic limit are used to parameterize a local spin density approximation correlation functional for inhomogeneous electron systems. Our zero-temperature phase diagram shows a single transition from a paramagnetic fluid to a hexagonal Wigner crystal at $r_\\text{s}=35(1)$, with no region of stability for a ferromagnetic fluid.","sentences":["We present a study of spin-unpolarized and spin-polarized two-dimensional uniform electron liquids using variational and diffusion quantum Monte Carlo (VMC and DMC) methods with Slater-Jastrow-backflow trial wave functions.","Ground-state VMC and DMC energies are obtained in the density range $1 \\leq r_\\text{s} \\leq 40$.","Single-particle and many-body finite-size errors are corrected using canonical-ensemble twist-averaged boundary conditions and extrapolation of twist-averaged energies to the thermodynamic limit of infinite system size.","System-size-dependent errors in Slater-Jastrow-backflow DMC energies caused by partially converged VMC energy minimization calculations are discussed.","We find that, for $1 \\leq r_\\text{s} \\leq 5$, optimizing the backflow function at each twist lowers the twist-averaged DMC energy at finite system size.","However, nonsystematic system-size-dependent effects remain in the DMC energies, which can be partially removed by extrapolation from multiple finite system sizes to infinite system size.","We attribute these nonsystematic effects to the close competition between fluid and defected crystal phases at different system sizes at low density.","The DMC energies in the thermodynamic limit are used to parameterize a local spin density approximation correlation functional for inhomogeneous electron systems.","Our zero-temperature phase diagram shows a single transition from a paramagnetic fluid to a hexagonal Wigner crystal at $r_\\text{s}=35(1)$, with no region of stability for a ferromagnetic fluid."],"url":"http://arxiv.org/abs/2405.00425v1","category":"cond-mat.str-el"}
