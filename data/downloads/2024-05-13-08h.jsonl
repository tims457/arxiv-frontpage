{"created":"2024-05-10 17:59:04","title":"Value Augmented Sampling for Language Model Alignment and Personalization","abstract":"Aligning Large Language Models (LLMs) to cater to different human preferences, learning new skills, and unlearning harmful behavior is an important problem. Search-based methods, such as Best-of-N or Monte-Carlo Tree Search, are performant, but impractical for LLM adaptation due to their high inference cost. On the other hand, using Reinforcement Learning (RL) for adaptation is computationally efficient, but performs worse due to the optimization challenges in co-training the value function and the policy. We present a new framework for reward optimization, Value Augmented Sampling (VAS), that can maximize different reward functions using data sampled from only the initial, frozen LLM. VAS solves for the optimal reward-maximizing policy without co-training the policy and the value function, making the optimization stable, outperforming established baselines, such as PPO and DPO, on standard benchmarks, and achieving comparable results to Best-of-128 with lower inference cost. Unlike existing RL methods that require changing the weights of the LLM, VAS does not require access to the weights of the pre-trained LLM. Thus, it can even adapt LLMs (e.g., ChatGPT), which are available only as APIs. In addition, our algorithm unlocks the new capability of composing several rewards and controlling the extent of each one during deployment time, paving the road ahead for the future of aligned, personalized LLMs.","sentences":["Aligning Large Language Models (LLMs) to cater to different human preferences, learning new skills, and unlearning harmful behavior is an important problem.","Search-based methods, such as Best-of-N or Monte-Carlo Tree Search, are performant, but impractical for LLM adaptation due to their high inference cost.","On the other hand, using Reinforcement Learning (RL) for adaptation is computationally efficient, but performs worse due to the optimization challenges in co-training the value function and the policy.","We present a new framework for reward optimization, Value Augmented Sampling (VAS), that can maximize different reward functions using data sampled from only the initial, frozen LLM.","VAS solves for the optimal reward-maximizing policy without co-training the policy and the value function, making the optimization stable, outperforming established baselines, such as PPO and DPO, on standard benchmarks, and achieving comparable results to Best-of-128 with lower inference cost.","Unlike existing RL methods that require changing the weights of the LLM, VAS does not require access to the weights of the pre-trained LLM.","Thus, it can even adapt LLMs (e.g., ChatGPT), which are available only as APIs.","In addition, our algorithm unlocks the new capability of composing several rewards and controlling the extent of each one during deployment time, paving the road ahead for the future of aligned, personalized LLMs."],"url":"http://arxiv.org/abs/2405.06639v1","category":"cs.LG"}
{"created":"2024-05-10 17:53:05","title":"Federated Document Visual Question Answering: A Pilot Study","abstract":"An important handicap of document analysis research is that documents tend to be copyrighted or contain private information, which prohibits their open publication and the creation of centralised, large-scale document datasets. Instead, documents are scattered in private data silos, making extensive training over heterogeneous data a tedious task. In this work, we explore the use of a federated learning (FL) scheme as a way to train a shared model on decentralised private document data. We focus on the problem of Document VQA, a task particularly suited to this approach, as the type of reasoning capabilities required from the model can be quite different in diverse domains. Enabling training over heterogeneous document datasets can thus substantially enrich DocVQA models. We assemble existing DocVQA datasets from diverse domains to reflect the data heterogeneity in real-world applications. We explore the self-pretraining technique in this multi-modal setting, where the same data is used for both pretraining and finetuning, making it relevant for privacy preservation. We further propose combining self-pretraining with a Federated DocVQA training method using centralized adaptive optimization that outperforms the FedAvg baseline. With extensive experiments, we also present a multi-faceted analysis on training DocVQA models with FL, which provides insights for future research on this task. We show that our pretraining strategies can effectively learn and scale up under federated training with diverse DocVQA datasets and tuning hyperparameters is essential for practical document tasks under federation.","sentences":["An important handicap of document analysis research is that documents tend to be copyrighted or contain private information, which prohibits their open publication and the creation of centralised, large-scale document datasets.","Instead, documents are scattered in private data silos, making extensive training over heterogeneous data a tedious task.","In this work, we explore the use of a federated learning (FL) scheme as a way to train a shared model on decentralised private document data.","We focus on the problem of Document VQA, a task particularly suited to this approach, as the type of reasoning capabilities required from the model can be quite different in diverse domains.","Enabling training over heterogeneous document datasets can thus substantially enrich DocVQA models.","We assemble existing DocVQA datasets from diverse domains to reflect the data heterogeneity in real-world applications.","We explore the self-pretraining technique in this multi-modal setting, where the same data is used for both pretraining and finetuning, making it relevant for privacy preservation.","We further propose combining self-pretraining with a Federated DocVQA training method using centralized adaptive optimization that outperforms the FedAvg baseline.","With extensive experiments, we also present a multi-faceted analysis on training DocVQA models with FL, which provides insights for future research on this task.","We show that our pretraining strategies can effectively learn and scale up under federated training with diverse DocVQA datasets and tuning hyperparameters is essential for practical document tasks under federation."],"url":"http://arxiv.org/abs/2405.06636v1","category":"cs.CV"}
{"created":"2024-05-10 17:51:35","title":"Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark","abstract":"We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual Network Analysis (VNA) tasks on small-scale graphs. We evaluate the Vision Language Models (VLMs) on 5 tasks related to three foundational network science concepts: identifying nodes of maximal degree on a rendered graph, identifying whether signed triads are balanced or unbalanced, and counting components. The tasks are structured to be easy for a human who understands the underlying graph theoretic concepts, and can all be solved by counting the appropriate elements in graphs. We find that while GPT-4 consistently outperforms LLaVa, both models struggle with every visual network analysis task we propose. We publicly release the first benchmark for the evaluation of VLMs on foundational VNA tasks.","sentences":["We evaluate the zero-shot ability of GPT-4 and LLaVa to perform simple Visual Network Analysis (VNA) tasks on small-scale graphs.","We evaluate the Vision Language Models (VLMs) on 5 tasks related to three foundational network science concepts: identifying nodes of maximal degree on a rendered graph, identifying whether signed triads are balanced or unbalanced, and counting components.","The tasks are structured to be easy for a human who understands the underlying graph theoretic concepts, and can all be solved by counting the appropriate elements in graphs.","We find that while GPT-4 consistently outperforms LLaVa, both models struggle with every visual network analysis task we propose.","We publicly release the first benchmark for the evaluation of VLMs on foundational VNA tasks."],"url":"http://arxiv.org/abs/2405.06634v1","category":"cs.CV"}
{"created":"2024-05-10 17:51:04","title":"Testing $f(T)$ cosmologies with HII Hubble diagram and CMB distance priors","abstract":"In this work, we present independent determinations of cosmological parameters and new constraints on $f(T)$ cosmologies, employing two new catalogs related to HII galaxy Hubble and CMB distance priors, along with the local standard measurements, SNIa, $H(z)$ measurements, growth rate data (RSD), and BAO baselines. We found that the marginalised best-fit C.L. $H_0$ and $\\sigma_8$ parameters within these cosmologies can relax the current cosmological tensions using HIIG data. This produces a larger range of admissible values for the current Hubble constant, and when all baselines are considered, the uncertainty bands for $H_0$ and the matter density parameter reduce significantly.","sentences":["In this work, we present independent determinations of cosmological parameters and new constraints on $f(T)$ cosmologies, employing two new catalogs related to HII galaxy Hubble and CMB distance priors, along with the local standard measurements, SNIa, $H(z)$ measurements, growth rate data (RSD), and BAO baselines.","We found that the marginalised best-fit C.L. $H_0$ and $\\sigma_8$ parameters within these cosmologies can relax the current cosmological tensions using HIIG data.","This produces a larger range of admissible values for the current Hubble constant, and when all baselines are considered, the uncertainty bands for $H_0$ and the matter density parameter reduce significantly."],"url":"http://arxiv.org/abs/2405.06633v1","category":"astro-ph.CO"}
{"created":"2024-05-10 17:50:35","title":"QSpace - An open-source tensor library for Abelian and non-Abelian symmetries","abstract":"This is the documentation for the tensor library QSpace (v4.0), a toolbox to exploit `quantum symmetry spaces' in tensor network states in the quantum many-body context. QSpace permits arbitrary combinations of symmetries including the abelian symmetries $\\mathbb{Z}_n$ and $U(1)$, as well as all non-abelian symmetries based on the semisimple classical Lie algebras: $A_n$, $B_n$, $C_n$, and $D_n$, or respectively, the special unitary group SU($n$), the odd orthogonal group SO($2n+1$), the symplectic group Sp($2n$), and the even orthogonal group SO($2n$). The code (C++ embedded via the MEX interface into Matlab) is available open source as of QSpace v4.0 at https://bitbucket.org/qspace4u/ under the Apache 2.0 license.   QSpace is designed as a bottom-up approach for non-abelian symmetries. It starts from the defining representation and the respective Lie algebra. By explicitly computing and tabulating generalized Clebsch-Gordan coefficient tensors, QSpace is versatile in the type of operations that it can perform across all symmetries. At the level of an application, much of the symmetry-related details are hidden within the QSpace C++ core libraries. Hence when developing tensor network algorithms with QSpace, these can be coded (nearly) as if there are no symmetries at all, despite being able to fully exploit general non-abelian symmetries.","sentences":["This is the documentation for the tensor library QSpace (v4.0), a toolbox to exploit `quantum symmetry spaces' in tensor network states in the quantum many-body context.","QSpace permits arbitrary combinations of symmetries including the abelian symmetries $\\mathbb{Z}_n$ and $U(1)$, as well as all non-abelian symmetries based on the semisimple classical Lie algebras: $A_n$, $B_n$, $C_n$, and $D_n$, or respectively, the special unitary group SU($n$), the odd orthogonal group SO($2n+1$), the symplectic group Sp($2n$), and the even orthogonal group SO($2n$).","The code (C++ embedded via the MEX interface into Matlab) is available open source as of QSpace v4.0 at https://bitbucket.org/qspace4u/ under the Apache 2.0 license.   ","QSpace is designed as a bottom-up approach for non-abelian symmetries.","It starts from the defining representation and the respective Lie algebra.","By explicitly computing and tabulating generalized Clebsch-Gordan coefficient tensors, QSpace is versatile in the type of operations that it can perform across all symmetries.","At the level of an application, much of the symmetry-related details are hidden within the QSpace C++ core libraries.","Hence when developing tensor network algorithms with QSpace, these can be coded (nearly) as if there are no symmetries at all, despite being able to fully exploit general non-abelian symmetries."],"url":"http://arxiv.org/abs/2405.06632v1","category":"cond-mat.str-el"}
{"created":"2024-05-10 17:45:22","title":"A General Hierarchy of Charges at Null Infinity via the Todd Polynomials","abstract":"We give a general procedure for constructing an extended phase space for Yang-Mills theory at null infinity, capable of handling the asymptotic symmetries and construction of charges responsible for sub$^n$-leading soft theorems at all orders. The procedure is coordinate and gauge-choice independent, and can be fed into the calculation of both tree and loop-level soft limits. We find a hierarchy in the extended phase space controlled by the Bernoulli numbers arising in Todd genus computations. We give an explicit example of a calculation at tree level, in radial gauge, where we also uncover recursion relations at all orders for the equations of motion and charges.","sentences":["We give a general procedure for constructing an extended phase space for Yang-Mills theory at null infinity, capable of handling the asymptotic symmetries and construction of charges responsible for sub$^n$-leading soft theorems at all orders.","The procedure is coordinate and gauge-choice independent, and can be fed into the calculation of both tree and loop-level soft limits.","We find a hierarchy in the extended phase space controlled by the Bernoulli numbers arising in Todd genus computations.","We give an explicit example of a calculation at tree level, in radial gauge, where we also uncover recursion relations at all orders for the equations of motion and charges."],"url":"http://arxiv.org/abs/2405.06629v1","category":"hep-th"}
{"created":"2024-05-10 17:41:38","title":"Optimal epidemic control by social distancing and vaccination of an infection structured by time since infection: the covid-19 case study","abstract":"Motivated by the issue of COVID-19 mitigation, in this work we tackle the general problem of optimally controlling an epidemic outbreak of a communicable disease structured by time since exposure, by the aid of two types of control instruments namely, social distancing and vaccination by a vaccine at least partly effective in protecting from infection. Effective vaccines are assumed to be made available only in a subsequent period of the epidemic so that - in the first period - epidemic control only relies on social distancing, as it happened for the COVID-19 pandemic. By our analyses, we could prove the existence of (at least) one optimal control pair, we derived first-order necessary conditions for optimality, and proved some useful properties of such optimal solutions. A worked example provides a number of further insights on the relationships between key control and epidemic parameters.","sentences":["Motivated by the issue of COVID-19 mitigation, in this work we tackle the general problem of optimally controlling an epidemic outbreak of a communicable disease structured by time since exposure, by the aid of two types of control instruments namely, social distancing and vaccination by a vaccine at least partly effective in protecting from infection.","Effective vaccines are assumed to be made available only in a subsequent period of the epidemic so that - in the first period - epidemic control only relies on social distancing, as it happened for the COVID-19 pandemic.","By our analyses, we could prove the existence of (at least) one optimal control pair, we derived first-order necessary conditions for optimality, and proved some useful properties of such optimal solutions.","A worked example provides a number of further insights on the relationships between key control and epidemic parameters."],"url":"http://arxiv.org/abs/2405.06628v1","category":"q-bio.PE"}
{"created":"2024-05-10 17:40:24","title":"Conformal Validity Guarantees Exist for Any Data Distribution","abstract":"As machine learning (ML) gains widespread adoption, practitioners are increasingly seeking means to quantify and control the risk these systems incur. This challenge is especially salient when ML systems have autonomy to collect their own data, such as in black-box optimization and active learning, where their actions induce sequential feedback-loop shifts in the data distribution. Conformal prediction has emerged as a promising approach to uncertainty and risk quantification, but existing variants either fail to accommodate sequences of data-dependent shifts, or do not fully exploit the fact that agent-induced shift is under our control. In this work we prove that conformal prediction can theoretically be extended to \\textit{any} joint data distribution, not just exchangeable or quasi-exchangeable ones, although it is exceedingly impractical to compute in the most general case. For practical applications, we outline a procedure for deriving specific conformal algorithms for any data distribution, and we use this procedure to derive tractable algorithms for a series of agent-induced covariate shifts. We evaluate the proposed algorithms empirically on synthetic black-box optimization and active learning tasks.","sentences":["As machine learning (ML) gains widespread adoption, practitioners are increasingly seeking means to quantify and control the risk these systems incur.","This challenge is especially salient when ML systems have autonomy to collect their own data, such as in black-box optimization and active learning, where their actions induce sequential feedback-loop shifts in the data distribution.","Conformal prediction has emerged as a promising approach to uncertainty and risk quantification, but existing variants either fail to accommodate sequences of data-dependent shifts, or do not fully exploit the fact that agent-induced shift is under our control.","In this work we prove that conformal prediction can theoretically be extended to \\textit{any} joint data distribution, not just exchangeable or quasi-exchangeable ones, although it is exceedingly impractical to compute in the most general case.","For practical applications, we outline a procedure for deriving specific conformal algorithms for any data distribution, and we use this procedure to derive tractable algorithms for a series of agent-induced covariate shifts.","We evaluate the proposed algorithms empirically on synthetic black-box optimization and active learning tasks."],"url":"http://arxiv.org/abs/2405.06627v1","category":"cs.LG"}
{"created":"2024-05-10 17:40:02","title":"Characterizing the Accuracy - Efficiency Trade-off of Low-rank Decomposition in Language Models","abstract":"Large language models (LLMs) have emerged and presented their general problem-solving capabilities with one model. However, the model size has increased dramatically with billions of parameters to enable such broad problem-solving capabilities. In addition, due to the dominance of matrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model size ratio is significantly lower than that of CNNs. This shift pushes LLMs from a computation-bound regime to a memory-bound regime. Therefore, optimizing the memory footprint and traffic is an important optimization direction for LLMs today.   Model compression methods such as quantization and parameter pruning have been actively explored for achieving the memory footprint and traffic optimization. However, the accuracy-efficiency trade-off of rank pruning for LLMs is not well-understood yet. Therefore, we characterize the accuracy-efficiency trade-off of a low-rank decomposition method, specifically Tucker decomposition, on recent language models, including an open-source LLM, Llama 2.   We formalize the low-rank decomposition design space and show that the decomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B). To navigate such a vast design space, we formulate the design space and perform thorough case studies of accuracy-efficiency trade-offs using six widely used LLM benchmarks on BERT and Llama 2 models. Our results show that we can achieve a 9\\% model size reduction with minimal accuracy drops, which range from 4\\%p to 10\\%p, depending on the difficulty of the benchmark, without any retraining to recover accuracy after decomposition. The results show that low-rank decomposition can be a promising direction for LLM-based applications that require real-time service in scale (e.g., AI agent assist and real-time coding assistant), where the latency is as important as the model accuracy.","sentences":["Large language models (LLMs) have emerged and presented their general problem-solving capabilities with one model.","However, the model size has increased dramatically with billions of parameters to enable such broad problem-solving capabilities.","In addition, due to the dominance of matrix-matrix and matrix-vector multiplications in LLMs, the compute-to-model size ratio is significantly lower than that of CNNs.","This shift pushes LLMs from a computation-bound regime to a memory-bound regime.","Therefore, optimizing the memory footprint and traffic is an important optimization direction for LLMs today.   ","Model compression methods such as quantization and parameter pruning have been actively explored for achieving the memory footprint and traffic optimization.","However, the accuracy-efficiency trade-off of rank pruning for LLMs is not well-understood yet.","Therefore, we characterize the accuracy-efficiency trade-off of a low-rank decomposition method, specifically Tucker decomposition, on recent language models, including an open-source LLM, Llama 2.   ","We formalize the low-rank decomposition design space and show that the decomposition design space is enormous (e.g., O($2^{37}$) for Llama2-7B).","To navigate such a vast design space, we formulate the design space and perform thorough case studies of accuracy-efficiency trade-offs using six widely used LLM benchmarks on BERT and Llama 2 models.","Our results show that we can achieve a 9\\% model size reduction with minimal accuracy drops, which range from 4\\%p to 10\\%p, depending on the difficulty of the benchmark, without any retraining to recover accuracy after decomposition.","The results show that low-rank decomposition can be a promising direction for LLM-based applications that require real-time service in scale (e.g., AI agent assist and real-time coding assistant), where the latency is as important as the model accuracy."],"url":"http://arxiv.org/abs/2405.06626v1","category":"cs.LG"}
{"created":"2024-05-10 17:39:36","title":"Strong existence for free-discontinuity problems with non-standard growth","abstract":"An Ahlfors-type regularity result for free-discontinuity energies defined on the space $SBV^{\\varphi}$ of special functions of bounded variation with $\\varphi$-growth, where $\\varphi$ is a generalized Orlicz function, is proved. Our analysis expands on the regularity theory for minimizers of a class of free-discontinuity problems in the non-standard growth case.","sentences":["An Ahlfors-type regularity result for free-discontinuity energies defined on the space $SBV^{\\varphi}$ of special functions of bounded variation with $\\varphi$-growth, where $\\varphi$ is a generalized Orlicz function, is proved.","Our analysis expands on the regularity theory for minimizers of a class of free-discontinuity problems in the non-standard growth case."],"url":"http://arxiv.org/abs/2405.06625v1","category":"math.AP"}
{"created":"2024-05-10 17:38:32","title":"Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems","abstract":"Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.","sentences":["Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts.","In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI.","The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees.","This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model).","We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them.","We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches."],"url":"http://arxiv.org/abs/2405.06624v1","category":"cs.AI"}
{"created":"2024-05-10 17:36:11","title":"Dynamic programming principle and computable prices in financial market models with transaction costs","abstract":"How to compute (super) hedging costs in rather general fi- nancial market models with transaction costs in discrete-time ? Despite the huge literature on this topic, most of results are characterizations of the super-hedging prices while it remains difficult to deduce numerical procedure to estimate them. We establish here a dynamic programming principle and we prove that it is possible to implement it under some conditions on the conditional supports of the price and volume processes for a large class of market models including convex costs such as order books but also non convex costs, e.g. fixed cost models.","sentences":["How to compute (super) hedging costs in rather general fi- nancial market models with transaction costs in discrete-time ?","Despite the huge literature on this topic, most of results are characterizations of the super-hedging prices while it remains difficult to deduce numerical procedure to estimate them.","We establish here a dynamic programming principle and we prove that it is possible to implement it under some conditions on the conditional supports of the price and volume processes for a large class of market models including convex costs such as order books but also non convex costs, e.g. fixed cost models."],"url":"http://arxiv.org/abs/2405.06623v1","category":"math.PR"}
{"created":"2024-05-10 17:35:16","title":"Faster entanglement production driven by quantum resonance in many-body rotors","abstract":"Quantum resonance in the paradigmatic kicked rotor model is a purely quantum effect that ignores the state of underlying classical chaos. In this work, the effect of quantum resonance on entanglement generation in the $N$-interacting kicked rotors is studied. We show a compelling feature: entanglement growth is superlinear in time until the timescale $t^*$, beyond which the entanglement production slows down to a logarithmic profile with superimposed oscillations. Notably, we find that at resonance, the entanglement dynamics is independent of the kick strength of rotors, but depends solely on the interaction strength. By mapping positional interaction to momentum space and analytically calculating the linear entropy, we elucidate the underlying mechanism driving these distinct growth profiles. The analytical findings are in excellent agreement with the numerical simulations performed for two- and three-interacting kicked rotors. Our results are amenable to an experimental realization on ultracold atom setup.","sentences":["Quantum resonance in the paradigmatic kicked rotor model is a purely quantum effect that ignores the state of underlying classical chaos.","In this work, the effect of quantum resonance on entanglement generation in the $N$-interacting kicked rotors is studied.","We show a compelling feature: entanglement growth is superlinear in time until the timescale $t^*$, beyond which the entanglement production slows down to a logarithmic profile with superimposed oscillations.","Notably, we find that at resonance, the entanglement dynamics is independent of the kick strength of rotors, but depends solely on the interaction strength.","By mapping positional interaction to momentum space and analytically calculating the linear entropy, we elucidate the underlying mechanism driving these distinct growth profiles.","The analytical findings are in excellent agreement with the numerical simulations performed for two- and three-interacting kicked rotors.","Our results are amenable to an experimental realization on ultracold atom setup."],"url":"http://arxiv.org/abs/2405.06622v1","category":"quant-ph"}
{"created":"2024-05-10 17:29:06","title":"A new diagnostic for the null test of dynamical dark energy in light of DESI 2024 and other BAO data","abstract":"We introduce a new diagnostic for the null test of dynamical dark energy. This diagnostic is useful, especially when we include anisotropic baryon acoustic oscillation (BAO) data in an analysis, to quantify the deviations from the standard $\\Lambda$CDM model. This null test is independent of any late-time cosmological model or parametrization. With this, we study the evidence for dynamical dark energy in light of Dark Energy Spectroscopic Instrument (DESI) 2024 data combined with cosmic microwave background (CMB) observations of the Planck 2018 mission and local $H_0$ measurements. We find low (around 0.6$\\sigma$) to moderate (around 1$\\sigma$) evidence for dynamical dark energy which is not that significant. Although we get individual deviations at around 2$\\sigma$ at the effective redshift 0.51 of the DESI 2024 data, the average deviations combining all redshift points are almost independent of the inclusion or exclusion of the data at effective redshift 0.51 (the differences are only around 0.3$\\sigma$). We get almost similar results for other non-DESI BAO data. The evidences are almost independent of the early-time physics and these evidences have very low dependence on $H_0$ values. The evidence of dynamical dark energy, obtained in this analysis, is not consistent with the DESI key paper results with the $w_0w_a$CDM model, but interestingly, these are almost consistent with the DESI's results with $z_pw_p$CDM model.","sentences":["We introduce a new diagnostic for the null test of dynamical dark energy.","This diagnostic is useful, especially when we include anisotropic baryon acoustic oscillation (BAO) data in an analysis, to quantify the deviations from the standard $\\Lambda$CDM model.","This null test is independent of any late-time cosmological model or parametrization.","With this, we study the evidence for dynamical dark energy in light of Dark Energy Spectroscopic Instrument (DESI) 2024 data combined with cosmic microwave background (CMB) observations of the Planck 2018 mission and local $H_0$ measurements.","We find low (around 0.6$\\sigma$) to moderate (around 1$\\sigma$) evidence for dynamical dark energy which is not that significant.","Although we get individual deviations at around 2$\\sigma$ at the effective redshift 0.51 of the DESI 2024 data, the average deviations combining all redshift points are almost independent of the inclusion or exclusion of the data at effective redshift 0.51 (the differences are only around 0.3$\\sigma$).","We get almost similar results for other non-DESI BAO data.","The evidences are almost independent of the early-time physics and these evidences have very low dependence on $H_0$ values.","The evidence of dynamical dark energy, obtained in this analysis, is not consistent with the DESI key paper results with the $w_0w_a$CDM model, but interestingly, these are almost consistent with the DESI's results with $z_pw_p$CDM model."],"url":"http://arxiv.org/abs/2405.06618v1","category":"astro-ph.CO"}
{"created":"2024-05-10 17:15:24","title":"\"We are at the mercy of others' opinion\": Supporting Blind People in Recreational Window Shopping with AI-infused Technology","abstract":"Engaging in recreational activities in public spaces poses challenges for blind people, often involving dependency on sighted help. Window shopping is a key recreational activity that remains inaccessible. In this paper, we investigate the information needs, challenges, and current approaches blind people have to recreational window shopping to inform the design of existing wayfinding and navigation technology for supporting blind shoppers in exploration and serendipitous discovery. We conduct a formative study with a total of 18 blind participants that include both focus groups (N=8) and interviews for requirements analysis (N=10). We find that there is a desire for push notifications of promotional information and pull notifications about shops of interest such as the targeted audience of a brand. Information about obstacles and points-of-interest required customization depending on one's mobility aid as well as presence of a crowd, children, and wheelchair users. We translate these findings into specific information modalities and rendering in the context of two existing AI-infused assistive applications: NavCog (a turn-by-turn navigation app) and Cabot (a navigation robot).","sentences":["Engaging in recreational activities in public spaces poses challenges for blind people, often involving dependency on sighted help.","Window shopping is a key recreational activity that remains inaccessible.","In this paper, we investigate the information needs, challenges, and current approaches blind people have to recreational window shopping to inform the design of existing wayfinding and navigation technology for supporting blind shoppers in exploration and serendipitous discovery.","We conduct a formative study with a total of 18 blind participants that include both focus groups (N=8) and interviews for requirements analysis (N=10).","We find that there is a desire for push notifications of promotional information and pull notifications about shops of interest such as the targeted audience of a brand.","Information about obstacles and points-of-interest required customization depending on one's mobility aid as well as presence of a crowd, children, and wheelchair users.","We translate these findings into specific information modalities and rendering in the context of two existing AI-infused assistive applications: NavCog (a turn-by-turn navigation app) and Cabot (a navigation robot)."],"url":"http://arxiv.org/abs/2405.06611v1","category":"cs.HC"}
{"created":"2024-05-10 17:13:33","title":"Realistic models of general-relativistic differentially rotating stars","abstract":"General-relativistic equilibria of differentially rotating stars are expected in a number of astrophysical scenarios, from core-collapse supernovae to the remnant of binary neutron-star mergers. The latter, in particular, have been the subject of extensive studies where they were modeled with a variety of laws of differential rotation with varying degree of realism. Starting from accurate and fully general-relativistic simulations of binary neutron-star mergers with various equations of state and mass ratios, we establish the time when the merger remnant has reached a quasi-stationary equilibrium and extract in this way realistic profiles of differential rotation. This allows us to explore how well traditional laws reproduce such differential-rotation properties and to derive new laws of differential rotation that better match the numerical data in the low-density Keplerian regions of the remnant. In this way, we have obtained a novel and somewhat surprising result: the dynamical stability line to quasi-radial oscillations computed from the turning-point criterion can have a slope that is not necessarily negative with respect to the central rest-mass density, as previously found with traditional differential-rotation laws. Indeed, for stellar models reproducing well the properties of the merger remnants, the slope is actually positive, thus reflecting remnants with angular momentum at large distances from the rotation axis, and hence with cores having higher central rest-mass densities and slower rotation rates.","sentences":["General-relativistic equilibria of differentially rotating stars are expected in a number of astrophysical scenarios, from core-collapse supernovae to the remnant of binary neutron-star mergers.","The latter, in particular, have been the subject of extensive studies where they were modeled with a variety of laws of differential rotation with varying degree of realism.","Starting from accurate and fully general-relativistic simulations of binary neutron-star mergers with various equations of state and mass ratios, we establish the time when the merger remnant has reached a quasi-stationary equilibrium and extract in this way realistic profiles of differential rotation.","This allows us to explore how well traditional laws reproduce such differential-rotation properties and to derive new laws of differential rotation that better match the numerical data in the low-density Keplerian regions of the remnant.","In this way, we have obtained a novel and somewhat surprising result: the dynamical stability line to quasi-radial oscillations computed from the turning-point criterion can have a slope that is not necessarily negative with respect to the central rest-mass density, as previously found with traditional differential-rotation laws.","Indeed, for stellar models reproducing well the properties of the merger remnants, the slope is actually positive, thus reflecting remnants with angular momentum at large distances from the rotation axis, and hence with cores having higher central rest-mass densities and slower rotation rates."],"url":"http://arxiv.org/abs/2405.06609v1","category":"gr-qc"}
{"created":"2024-05-10 17:12:48","title":"Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter Simulation","abstract":"We introduce a novel machine learning method developed for the fast simulation of calorimeter detector response, adapting vector-quantized variational autoencoder (VQ-VAE). Our model adopts a two-stage generation strategy: initially compressing geometry-aware calorimeter data into a discrete latent space, followed by the application of a sequence model to learn and generate the latent tokens. Extensive experimentation on the Calo-challenge dataset underscores the efficiency of our approach, showcasing a remarkable improvement in the generation speed compared with conventional method by a factor of 2000. Remarkably, our model achieves the generation of calorimeter showers within milliseconds. Furthermore, comprehensive quantitative evaluations across various metrics are performed to validate physics performance of generation.","sentences":["We introduce a novel machine learning method developed for the fast simulation of calorimeter detector response, adapting vector-quantized variational autoencoder (VQ-VAE).","Our model adopts a two-stage generation strategy: initially compressing geometry-aware calorimeter data into a discrete latent space, followed by the application of a sequence model to learn and generate the latent tokens.","Extensive experimentation on the Calo-challenge dataset underscores the efficiency of our approach, showcasing a remarkable improvement in the generation speed compared with conventional method by a factor of 2000.","Remarkably, our model achieves the generation of calorimeter showers within milliseconds.","Furthermore, comprehensive quantitative evaluations across various metrics are performed to validate physics performance of generation."],"url":"http://arxiv.org/abs/2405.06605v1","category":"physics.ins-det"}
{"created":"2024-05-10 17:09:52","title":"Inflationary Gravitational Wave Spectral Shapes as test for Low-Scale Leptogenesis","abstract":"We study thermal and non-thermal resonant leptogenesis in a general setting where a heavy scalar $\\phi$ decays to right-handed neutrinos (RHNs) whose further out-of-equilibrium decay generates the required lepton asymmetry. Domination of the energy budget of the Universe by the $\\phi$ or the RHNs alters the evolution history of the primordial gravitational waves (PGW), of inflationary origin, which re-enter the horizon after inflation, modifying the spectral shape. The decays of $\\phi$ and RHNs release entropy into the early Universe while nearly degenerate RHNs facilitate low and intermediate scale leptogenesis. We show that depending on the coupling $y_R$ of $\\phi$ to radiation species, RHNs can achieve thermal abundance before decaying, which gives rise to thermal leptogenesis. A characteristic damping of the GW spectrum resulting in two knee-like features or one knee-like feature would provide evidence for low-scale thermal and non-thermal leptogenesis respectively. We explore the parameter space for the lightest right-handed neutrino mass $M_1\\in[10^2,10^{14}]$ GeV and washout parameter $K$ that depends on the light-heavy neutrino Yukawa couplings $\\lambda$, in the weak ($K < 1$) and strong ($K > 1$) washout regimes. The resulting novel features compatible with observed baryon asymmetry are detectable by future experiments like LISA and ET. By estimating signal-to-noise ratio (SNR) for upcoming GW experiments, we investigate the effect of the scalar mass $M_\\phi$ and reheating temperature $T_\\phi$, which depends on the $\\phi-N$ Yukawa couplings $y_N$.","sentences":["We study thermal and non-thermal resonant leptogenesis in a general setting where a heavy scalar $\\phi$ decays to right-handed neutrinos (RHNs) whose further out-of-equilibrium decay generates the required lepton asymmetry.","Domination of the energy budget of the Universe by the $\\phi$ or the RHNs alters the evolution history of the primordial gravitational waves (PGW), of inflationary origin, which re-enter the horizon after inflation, modifying the spectral shape.","The decays of $\\phi$ and RHNs release entropy into the early Universe while nearly degenerate RHNs facilitate low and intermediate scale leptogenesis.","We show that depending on the coupling $y_R$ of $\\phi$ to radiation species, RHNs can achieve thermal abundance before decaying, which gives rise to thermal leptogenesis.","A characteristic damping of the GW spectrum resulting in two knee-like features or one knee-like feature would provide evidence for low-scale thermal and non-thermal leptogenesis respectively.","We explore the parameter space for the lightest right-handed neutrino mass $M_1\\in[10^2,10^{14}]$ GeV and washout parameter $K$ that depends on the light-heavy neutrino Yukawa couplings $\\lambda$, in the weak ($K < 1$) and strong ($K > 1$) washout regimes.","The resulting novel features compatible with observed baryon asymmetry are detectable by future experiments like LISA and ET.","By estimating signal-to-noise ratio (SNR) for upcoming GW experiments, we investigate the effect of the scalar mass $M_\\phi$ and reheating temperature $T_\\phi$, which depends on the $\\phi-N$ Yukawa couplings $y_N$."],"url":"http://arxiv.org/abs/2405.06603v1","category":"hep-ph"}
{"created":"2024-05-10 17:03:27","title":"A numerical code for the analysis of magnetic white dwarf spectra that includes field effects on the chemical equilibrium","abstract":"We present a new magnetic atmosphere model code for obtaining synthetic spectral fluxes of hydrogen-rich magnetic white dwarfs. To date, observed spectra have been analyzed with models that neglet the magnetic field effects on the atomic populations. In this work, we incorporate to state-of-art theory into the evaluation of numerical densities of atoms, free electrons and ions in local thermodynamical equilibrium under the action of a magnetic field. The energy distribution of atoms is rigorously evaluated for arbitrary field strength. This energy pattern includes from tightly bound states to metastable or truly bound, highly excited states embedded in the continuum, i.e., over the first Landau level. Finite nuclear mass effects and the coupling between the internal atomic structure and the motion of the atom across the magnetic field are also considered. Synthetic fluxes are generated with integrations of numerical solutions of polarized radiative transfer over the visible stellar disk using a spherical $t$-design method. The atmosphere code is tested with observations from the Sloan Digital Sky Survey for a group of known magnetic white dwarfs. Physical stellar parameters are obtained from least-squares fits to the observed energy distribution and compared with results of previous works. We show that the use of zerofield ionization equilibrium in spectral analysis can lead to underestimated effective temperatures for high magnetic white dwarfs.","sentences":["We present a new magnetic atmosphere model code for obtaining synthetic spectral fluxes of hydrogen-rich magnetic white dwarfs.","To date, observed spectra have been analyzed with models that neglet the magnetic field effects on the atomic populations.","In this work, we incorporate to state-of-art theory into the evaluation of numerical densities of atoms, free electrons and ions in local thermodynamical equilibrium under the action of a magnetic field.","The energy distribution of atoms is rigorously evaluated for arbitrary field strength.","This energy pattern includes from tightly bound states to metastable or truly bound, highly excited states embedded in the continuum, i.e., over the first Landau level.","Finite nuclear mass effects and the coupling between the internal atomic structure and the motion of the atom across the magnetic field are also considered.","Synthetic fluxes are generated with integrations of numerical solutions of polarized radiative transfer over the visible stellar disk using a spherical $t$-design method.","The atmosphere code is tested with observations from the Sloan Digital Sky Survey for a group of known magnetic white dwarfs.","Physical stellar parameters are obtained from least-squares fits to the observed energy distribution and compared with results of previous works.","We show that the use of zerofield ionization equilibrium in spectral analysis can lead to underestimated effective temperatures for high magnetic white dwarfs."],"url":"http://arxiv.org/abs/2405.06601v1","category":"astro-ph.SR"}
{"created":"2024-05-10 16:56:53","title":"A Lightweight Transformer for Remote Sensing Image Change Captioning","abstract":"Remote sensing image change captioning (RSICC) aims to automatically generate sentences that describe content differences in remote sensing bitemporal images. Recently, attention-based transformers have become a prevalent idea for capturing the features of global change. However, existing transformer-based RSICC methods face challenges, e.g., high parameters and high computational complexity caused by the self-attention operation in the transformer encoder component. To alleviate these issues, this paper proposes a Sparse Focus Transformer (SFT) for the RSICC task. Specifically, the SFT network consists of three main components, i.e. a high-level features extractor based on a convolutional neural network (CNN), a sparse focus attention mechanism-based transformer encoder network designed to locate and capture changing regions in dual-temporal images, and a description decoder that embeds images and words to generate sentences for captioning differences. The proposed SFT network can reduce the parameter number and computational complexity by incorporating a sparse attention mechanism within the transformer encoder network. Experimental results on various datasets demonstrate that even with a reduction of over 90\\% in parameters and computational complexity for the transformer encoder, our proposed network can still obtain competitive performance compared to other state-of-the-art RSICC methods. The code can be available at","sentences":["Remote sensing image change captioning (RSICC) aims to automatically generate sentences that describe content differences in remote sensing bitemporal images.","Recently, attention-based transformers have become a prevalent idea for capturing the features of global change.","However, existing transformer-based RSICC methods face challenges, e.g., high parameters and high computational complexity caused by the self-attention operation in the transformer encoder component.","To alleviate these issues, this paper proposes a Sparse Focus Transformer (SFT) for the RSICC task.","Specifically, the SFT network consists of three main components, i.e. a high-level features extractor based on a convolutional neural network (CNN), a sparse focus attention mechanism-based transformer encoder network designed to locate and capture changing regions in dual-temporal images, and a description decoder that embeds images and words to generate sentences for captioning differences.","The proposed SFT network can reduce the parameter number and computational complexity by incorporating a sparse attention mechanism within the transformer encoder network.","Experimental results on various datasets demonstrate that even with a reduction of over 90\\% in parameters and computational complexity for the transformer encoder, our proposed network can still obtain competitive performance compared to other state-of-the-art RSICC methods.","The code can be available at"],"url":"http://arxiv.org/abs/2405.06598v1","category":"cs.CV"}
{"created":"2024-05-10 16:48:44","title":"Non-Uniform Spatial Alignment Errors in sUAS Imagery From Wide-Area Disasters","abstract":"This work presents the first quantitative study of alignment errors between small uncrewed aerial systems (sUAS) geospatial imagery and a priori building polygons and finds that alignment errors are non-uniform and irregular. The work also introduces a publicly available dataset of imagery, building polygons, and human-generated and curated adjustments that can be used to evaluate existing strategies for aligning building polygons with sUAS imagery. There are no efforts that have aligned pre-existing spatial data with sUAS imagery, and thus, there is no clear state of practice. However, this effort and analysis show that the translational alignment errors present in this type of data, averaging 82px and an intersection over the union of 0.65, which would induce further errors and biases in downstream machine learning systems unless addressed. This study identifies and analyzes the translational alignment errors of 21,619 building polygons in fifty-one orthomosaic images, covering 16787.2 Acres (26.23 square miles), constructed from sUAS raw imagery from nine wide-area disasters (Hurricane Ian, Hurricane Harvey, Hurricane Michael, Hurricane Ida, Hurricane Idalia, Hurricane Laura, the Mayfield Tornado, the Musset Bayou Fire, and the Kilauea Eruption). The analysis finds no uniformity among the angle and distance metrics of the building polygon alignments as they present an average degree variance of 0.4 and an average pixel distance variance of 0.45. This work alerts the sUAS community to the problem of spatial alignment and that a simple linear transform, often used to align satellite imagery, will not be sufficient to align spatial data in sUAS orthomosaic imagery.","sentences":["This work presents the first quantitative study of alignment errors between small uncrewed aerial systems (sUAS) geospatial imagery and a priori building polygons and finds that alignment errors are non-uniform and irregular.","The work also introduces a publicly available dataset of imagery, building polygons, and human-generated and curated adjustments that can be used to evaluate existing strategies for aligning building polygons with sUAS imagery.","There are no efforts that have aligned pre-existing spatial data with sUAS imagery, and thus, there is no clear state of practice.","However, this effort and analysis show that the translational alignment errors present in this type of data, averaging 82px and an intersection over the union of 0.65, which would induce further errors and biases in downstream machine learning systems unless addressed.","This study identifies and analyzes the translational alignment errors of 21,619 building polygons in fifty-one orthomosaic images, covering 16787.2 Acres (26.23 square miles), constructed from sUAS raw imagery from nine wide-area disasters (Hurricane Ian, Hurricane Harvey, Hurricane Michael, Hurricane Ida, Hurricane Idalia, Hurricane Laura, the Mayfield Tornado, the Musset Bayou Fire, and the Kilauea Eruption).","The analysis finds no uniformity among the angle and distance metrics of the building polygon alignments as they present an average degree variance of 0.4 and an average pixel distance variance of 0.45.","This work alerts the sUAS community to the problem of spatial alignment and that a simple linear transform, often used to align satellite imagery, will not be sufficient to align spatial data in sUAS orthomosaic imagery."],"url":"http://arxiv.org/abs/2405.06593v1","category":"cs.CV"}
{"created":"2024-05-10 16:44:23","title":"Robotic Stroke Motion Following the Shape of the Human Back: Motion Generation and Psychological Effects","abstract":"In this study, to perform the robotic stroke motions following the shape of the human back similar to the stroke motions by humans, in contrast to the conventional robotic stroke motion with a linear trajectory, we propose a trajectory generation method for a robotic stroke motion following the shape of the human back. We confirmed that the accuracy of the method's trajectory was close to that of the actual stroking motion by a human. Furthermore, we conducted a subjective experiment to evaluate the psychological effects of the proposed stroke motion in contrast to those of the conventional stroke motion with a linear trajectory. The experimental results showed that the actual stroke motion following the shape of the human back tended to evoke more pleasant and active feelings than the conventional stroke motion.","sentences":["In this study, to perform the robotic stroke motions following the shape of the human back similar to the stroke motions by humans, in contrast to the conventional robotic stroke motion with a linear trajectory, we propose a trajectory generation method for a robotic stroke motion following the shape of the human back.","We confirmed that the accuracy of the method's trajectory was close to that of the actual stroking motion by a human.","Furthermore, we conducted a subjective experiment to evaluate the psychological effects of the proposed stroke motion in contrast to those of the conventional stroke motion with a linear trajectory.","The experimental results showed that the actual stroke motion following the shape of the human back tended to evoke more pleasant and active feelings than the conventional stroke motion."],"url":"http://arxiv.org/abs/2405.06588v1","category":"cs.RO"}
{"created":"2024-05-10 16:42:25","title":"Enhancing Weakly Supervised Semantic Segmentation with Multi-modal Foundation Models: An End-to-End Approach","abstract":"Semantic segmentation is a core computer vision problem, but the high costs of data annotation have hindered its wide application. Weakly-Supervised Semantic Segmentation (WSSS) offers a cost-efficient workaround to extensive labeling in comparison to fully-supervised methods by using partial or incomplete labels. Existing WSSS methods have difficulties in learning the boundaries of objects leading to poor segmentation results. We propose a novel and effective framework that addresses these issues by leveraging visual foundation models inside the bounding box. Adopting a two-stage WSSS framework, our proposed network consists of a pseudo-label generation module and a segmentation module. The first stage leverages Segment Anything Model (SAM) to generate high-quality pseudo-labels. To alleviate the problem of delineating precise boundaries, we adopt SAM inside the bounding box with the help of another pre-trained foundation model (e.g., Grounding-DINO). Furthermore, we eliminate the necessity of using the supervision of image labels, by employing CLIP in classification. Then in the second stage, the generated high-quality pseudo-labels are used to train an off-the-shelf segmenter that achieves the state-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014.","sentences":["Semantic segmentation is a core computer vision problem, but the high costs of data annotation have hindered its wide application.","Weakly-Supervised Semantic Segmentation (WSSS) offers a cost-efficient workaround to extensive labeling in comparison to fully-supervised methods by using partial or incomplete labels.","Existing WSSS methods have difficulties in learning the boundaries of objects leading to poor segmentation results.","We propose a novel and effective framework that addresses these issues by leveraging visual foundation models inside the bounding box.","Adopting a two-stage WSSS framework, our proposed network consists of a pseudo-label generation module and a segmentation module.","The first stage leverages Segment Anything Model (SAM) to generate high-quality pseudo-labels.","To alleviate the problem of delineating precise boundaries, we adopt SAM inside the bounding box with the help of another pre-trained foundation model (e.g., Grounding-DINO).","Furthermore, we eliminate the necessity of using the supervision of image labels, by employing CLIP in classification.","Then in the second stage, the generated high-quality pseudo-labels are used to train an off-the-shelf segmenter that achieves the state-of-the-art performance on PASCAL VOC 2012 and MS COCO 2014."],"url":"http://arxiv.org/abs/2405.06586v1","category":"cs.CV"}
{"created":"2024-05-10 16:35:37","title":"$RLL$-realization of two-parameter quantum affine algebra in type $D_n^{(1)}$","abstract":"We obtain the basic $R$-matrix of the two-parameter Quantum group $U=U_{r,s}\\mathcal(\\frak{so}_{2n})$ via its weight representation theory and then determine its $R$-matrix with spectral parameters for the two-parameter quantum affine algebra $U=U_{r,s}\\mathcal(\\widehat{\\mathfrak{so}_{2n}})$. Using the Gauss decomposition of the $R$-matrix realization of $U=U_{r,s}\\mathcal(\\frak{so}_{2n})$, we study the commutation relations of the Gaussian generators and finally arrive at its $RLL$-formalism of the Drinfeld realization of two-parameter quantum affine algebra $U=U_{r,s}\\mathcal(\\widehat{\\mathfrak{so}_{2n}})$.","sentences":["We obtain the basic $R$-matrix of the two-parameter Quantum group $U=U_{r,s}\\mathcal(\\frak{so}_{2n})$ via its weight representation theory and then determine its $R$-matrix with spectral parameters for the two-parameter quantum affine algebra $U=U_{r,s}\\mathcal(\\widehat{\\mathfrak{so}_{2n}})$. Using the Gauss decomposition of the $R$-matrix realization of $U=U_{r,s}\\mathcal(\\frak{so}_{2n})$, we study the commutation relations of the Gaussian generators and finally arrive at its $RLL$-formalism of the Drinfeld realization of two-parameter quantum affine algebra $U=U_{r,s}\\mathcal(\\widehat{\\mathfrak{so}_{2n}})$."],"url":"http://arxiv.org/abs/2405.06581v1","category":"math.QA"}
{"created":"2024-05-10 16:34:42","title":"Continuous-variable Quantum Boltzmann Machine","abstract":"We propose a continuous-variable quantum Boltzmann machine (CVQBM) using a powerful energy-based neural network. It can be realized experimentally on a continuous-variable (CV) photonic quantum computer. We used a CV quantum imaginary time evolution (QITE) algorithm to prepare the essential thermal state and then designed the CVQBM to proficiently generate continuous probability distributions. We applied our method to both classical and quantum data. Using real-world classical data, such as synthetic aperture radar (SAR) images, we generated probability distributions. For quantum data, we used the output of CV quantum circuits. We obtained high fidelity and low Kuller-Leibler (KL) divergence showing that our CVQBM learns distributions from given data well and generates data sampling from that distribution efficiently. We also discussed the experimental feasibility of our proposed CVQBM. Our method can be applied to a wide range of real-world problems by choosing an appropriate target distribution (corresponding to, e.g., SAR images, medical images, and risk management in finance). Moreover, our CVQBM is versatile and could be programmed to perform tasks beyond generation, such as anomaly detection.","sentences":["We propose a continuous-variable quantum Boltzmann machine (CVQBM) using a powerful energy-based neural network.","It can be realized experimentally on a continuous-variable (CV) photonic quantum computer.","We used a CV quantum imaginary time evolution (QITE) algorithm to prepare the essential thermal state and then designed the CVQBM to proficiently generate continuous probability distributions.","We applied our method to both classical and quantum data.","Using real-world classical data, such as synthetic aperture radar (SAR) images, we generated probability distributions.","For quantum data, we used the output of CV quantum circuits.","We obtained high fidelity and low Kuller-Leibler (KL) divergence showing that our CVQBM learns distributions from given data well and generates data sampling from that distribution efficiently.","We also discussed the experimental feasibility of our proposed CVQBM.","Our method can be applied to a wide range of real-world problems by choosing an appropriate target distribution (corresponding to, e.g., SAR images, medical images, and risk management in finance).","Moreover, our CVQBM is versatile and could be programmed to perform tasks beyond generation, such as anomaly detection."],"url":"http://arxiv.org/abs/2405.06580v1","category":"quant-ph"}
{"created":"2024-05-10 16:33:57","title":"Hierarchical Learned Risk-Aware Planning Framework for Human Driving Modeling","abstract":"This paper presents a novel approach to modeling human driving behavior, designed for use in evaluating autonomous vehicle control systems in a simulation environments. Our methodology leverages a hierarchical forward-looking, risk-aware estimation framework with learned parameters to generate human-like driving trajectories, accommodating multiple driver levels determined by model parameters. This approach is grounded in multimodal trajectory prediction, using a deep neural network with LSTM-based social pooling to predict the trajectories of surrounding vehicles. These trajectories are used to compute forward-looking risk assessments along the ego vehicle's path, guiding its navigation. Our method aims to replicate human driving behaviors by learning parameters that emulate human decision-making during driving. We ensure that our model exhibits robust generalization capabilities by conducting simulations, employing real-world driving data to validate the accuracy of our approach in modeling human behavior. The results reveal that our model effectively captures human behavior, showcasing its versatility in modeling human drivers in diverse highway scenarios.","sentences":["This paper presents a novel approach to modeling human driving behavior, designed for use in evaluating autonomous vehicle control systems in a simulation environments.","Our methodology leverages a hierarchical forward-looking, risk-aware estimation framework with learned parameters to generate human-like driving trajectories, accommodating multiple driver levels determined by model parameters.","This approach is grounded in multimodal trajectory prediction, using a deep neural network with LSTM-based social pooling to predict the trajectories of surrounding vehicles.","These trajectories are used to compute forward-looking risk assessments along the ego vehicle's path, guiding its navigation.","Our method aims to replicate human driving behaviors by learning parameters that emulate human decision-making during driving.","We ensure that our model exhibits robust generalization capabilities by conducting simulations, employing real-world driving data to validate the accuracy of our approach in modeling human behavior.","The results reveal that our model effectively captures human behavior, showcasing its versatility in modeling human drivers in diverse highway scenarios."],"url":"http://arxiv.org/abs/2405.06578v1","category":"cs.RO"}
{"created":"2024-05-10 16:25:14","title":"ExoplANETS-A: A VO database for host stars and planetary systems: The effect of XUV on planet atmospheres","abstract":"ExoplANETS-A is an EU Horizon-2020 project with the primary objective of establishing new knowledge on exoplanet atmospheres. Intimately related to this topic is the study of the host-stars radiative properties in order to understand the environment in which exoplanets lie.   The aim of this work is to exploit archived data from space-based observatories and other public sources to produce uniform sets of stellar data that can establish new insight on the influence of the host star on the planetary atmosphere. We have compiled X-ray and UV luminosities, which affect the formation and the atmospheric properties of the planets, and stellar parameters, which impact the retrieval process of the planetary-atmosphere's properties and its errors.   Our sample is formed of all transiting-exoplanet systems observed by HST or Spitzer. It includes 205 exoplanets and their 114 host-stars. We have built a catalogue with information extracted from public, online archives augmented by quantities derived by the Exoplanets-A work. With this catalogue we have implemented an online database which also includes X-ray and OHP spectra and TESS light curves. In addition, we have developed a tool, exoVOSA, which is able to fit the spectral energy distribution of exoplanets.   We give an example of using the database to study the effects of the host-star high-energy emission on the exoplanet atmosphere. The sample has a planet radius valley which is located at 1.8 Earth radii, in agreement with previous studies. Multiplanet systems in our sample were used to test the photoevaporation model and we find that out of 14 systems, only one significant case poses a contradiction to it (K2-3). In summary, the exoplanet and stellar resources compiled and generated by ExoplANETS-A form a sound basis for current JWST observations and for future work in the era of Ariel.","sentences":["ExoplANETS-A is an EU Horizon-2020 project with the primary objective of establishing new knowledge on exoplanet atmospheres.","Intimately related to this topic is the study of the host-stars radiative properties in order to understand the environment in which exoplanets lie.   ","The aim of this work is to exploit archived data from space-based observatories and other public sources to produce uniform sets of stellar data that can establish new insight on the influence of the host star on the planetary atmosphere.","We have compiled X-ray and UV luminosities, which affect the formation and the atmospheric properties of the planets, and stellar parameters, which impact the retrieval process of the planetary-atmosphere's properties and its errors.   ","Our sample is formed of all transiting-exoplanet systems observed by HST or Spitzer.","It includes 205 exoplanets and their 114 host-stars.","We have built a catalogue with information extracted from public, online archives augmented by quantities derived by the Exoplanets-A work.","With this catalogue we have implemented an online database which also includes X-ray and OHP spectra and TESS light curves.","In addition, we have developed a tool, exoVOSA, which is able to fit the spectral energy distribution of exoplanets.   ","We give an example of using the database to study the effects of the host-star high-energy emission on the exoplanet atmosphere.","The sample has a planet radius valley which is located at 1.8 Earth radii, in agreement with previous studies.","Multiplanet systems in our sample were used to test the photoevaporation model and we find that out of 14 systems, only one significant case poses a contradiction to it (K2-3).","In summary, the exoplanet and stellar resources compiled and generated by ExoplANETS-A form a sound basis for current JWST observations and for future work in the era of Ariel."],"url":"http://arxiv.org/abs/2405.06577v1","category":"astro-ph.SR"}
{"created":"2024-05-10 16:22:46","title":"Algorithms for partial wave amplitudes under covariant $L$-$S$ scheme","abstract":"With the continuous accumulation of data from hadron collision experiments, efficient Partial Wave Analysis tools are indispensable for constructing a clear hadron spectrum. Currently, automated computations of scattering amplitudes primarily rely on the helicity scheme and covariant effective Lagrangian method. The automated calculations under the covariant $L$-$S$ scheme, which is one of the commonly used partial wave analysis schemes, have not been fully realized. In this work, we provide a general algorithm for computing partial wave amplitudes under the covariant $L$-$S$ scheme. This will lay the foundation for automated computation of partial wave amplitudes under the covariant $L$-$S$ scheme.","sentences":["With the continuous accumulation of data from hadron collision experiments, efficient Partial Wave Analysis tools are indispensable for constructing a clear hadron spectrum.","Currently, automated computations of scattering amplitudes primarily rely on the helicity scheme and covariant effective Lagrangian method.","The automated calculations under the covariant $L$-$S$ scheme, which is one of the commonly used partial wave analysis schemes, have not been fully realized.","In this work, we provide a general algorithm for computing partial wave amplitudes under the covariant $L$-$S$ scheme.","This will lay the foundation for automated computation of partial wave amplitudes under the covariant $L$-$S$ scheme."],"url":"http://arxiv.org/abs/2405.06576v1","category":"hep-ph"}
{"created":"2024-05-10 16:22:33","title":"No-Regret is not enough! Bandits with General Constraints through Adaptive Regret Minimization","abstract":"In the bandits with knapsacks framework (BwK) the learner has $m$ resource-consumption (packing) constraints. We focus on the generalization of BwK in which the learner has a set of general long-term constraints. The goal of the learner is to maximize their cumulative reward, while at the same time achieving small cumulative constraints violations. In this scenario, there exist simple instances where conventional methods for BwK fail to yield sublinear violations of constraints. We show that it is possible to circumvent this issue by requiring the primal and dual algorithm to be weakly adaptive. Indeed, even in absence on any information on the Slater's parameter $\\rho$ characterizing the problem, the interplay between weakly adaptive primal and dual regret minimizers yields a \"self-bounding\" property of dual variables. In particular, their norm remains suitably upper bounded across the entire time horizon even without explicit projection steps. By exploiting this property, we provide best-of-both-worlds guarantees for stochastic and adversarial inputs. In the first case, we show that the algorithm guarantees sublinear regret. In the latter case, we establish a tight competitive ratio of $\\rho/(1+\\rho)$. In both settings, constraints violations are guaranteed to be sublinear in time. Finally, this results allow us to obtain new result for the problem of contextual bandits with linear constraints, providing the first no-$\\alpha$-regret guarantees for adversarial contexts.","sentences":["In the bandits with knapsacks framework (BwK) the learner has $m$ resource-consumption (packing) constraints.","We focus on the generalization of BwK in which the learner has a set of general long-term constraints.","The goal of the learner is to maximize their cumulative reward, while at the same time achieving small cumulative constraints violations.","In this scenario, there exist simple instances where conventional methods for BwK fail to yield sublinear violations of constraints.","We show that it is possible to circumvent this issue by requiring the primal and dual algorithm to be weakly adaptive.","Indeed, even in absence on any information on the Slater's parameter $\\rho$ characterizing the problem, the interplay between weakly adaptive primal and dual regret minimizers yields a \"self-bounding\" property of dual variables.","In particular, their norm remains suitably upper bounded across the entire time horizon even without explicit projection steps.","By exploiting this property, we provide best-of-both-worlds guarantees for stochastic and adversarial inputs.","In the first case, we show that the algorithm guarantees sublinear regret.","In the latter case, we establish a tight competitive ratio of $\\rho/(1+\\rho)$. In both settings, constraints violations are guaranteed to be sublinear in time.","Finally, this results allow us to obtain new result for the problem of contextual bandits with linear constraints, providing the first no-$\\alpha$-regret guarantees for adversarial contexts."],"url":"http://arxiv.org/abs/2405.06575v1","category":"cs.LG"}
{"created":"2024-05-10 16:20:11","title":"Deep video representation learning: a survey","abstract":"This paper provides a review on representation learning for videos. We classify recent spatiotemporal feature learning methods for sequential visual data and compare their pros and cons for general video analysis. Building effective features for videos is a fundamental problem in computer vision tasks involving video analysis and understanding. Existing features can be generally categorized into spatial and temporal features. Their effectiveness under variations of illumination, occlusion, view and background are discussed. Finally, we discuss the remaining challenges in existing deep video representation learning studies.","sentences":["This paper provides a review on representation learning for videos.","We classify recent spatiotemporal feature learning methods for sequential visual data and compare their pros and cons for general video analysis.","Building effective features for videos is a fundamental problem in computer vision tasks involving video analysis and understanding.","Existing features can be generally categorized into spatial and temporal features.","Their effectiveness under variations of illumination, occlusion, view and background are discussed.","Finally, we discuss the remaining challenges in existing deep video representation learning studies."],"url":"http://arxiv.org/abs/2405.06574v1","category":"cs.CV"}
{"created":"2024-05-10 16:18:49","title":"An Investigation of Incorporating Mamba for Speech Enhancement","abstract":"This work aims to study a scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. We exploit a Mamba-based regression model to characterize speech signals and build an SE system upon Mamba, termed SEMamba. We explore the properties of Mamba by integrating it as the core model in both basic and advanced SE systems, along with utilizing signal-level distances as well as metric-oriented loss functions. SEMamba demonstrates promising results and attains a PESQ score of 3.55 on the VoiceBank-DEMAND dataset. When combined with the perceptual contrast stretching technique, the proposed SEMamba yields a new state-of-the-art PESQ score of 3.69.","sentences":["This work aims to study a scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task.","We exploit a Mamba-based regression model to characterize speech signals and build an SE system upon Mamba, termed SEMamba.","We explore the properties of Mamba by integrating it as the core model in both basic and advanced SE systems, along with utilizing signal-level distances as well as metric-oriented loss functions.","SEMamba demonstrates promising results and attains a PESQ score of 3.55 on the VoiceBank-DEMAND dataset.","When combined with the perceptual contrast stretching technique, the proposed SEMamba yields a new state-of-the-art PESQ score of 3.69."],"url":"http://arxiv.org/abs/2405.06573v1","category":"cs.SD"}
{"created":"2024-05-10 16:17:29","title":"SPERO: Simultaneous Power/EM Side-channel Dataset Using Real-time and Oscilloscope Setups","abstract":"Cryptosystem implementations often disclose information regarding a secret key due to correlations with side channels such as power consumption, timing variations, and electromagnetic emissions. Since power and EM channels can leak distinct information, the combination of EM and power channels could increase side-channel attack efficiency. In this paper, we develop a miniature dual-channel side-channel detection platform, named RASCv3 to successfully extract subkeys from both unmasked and masked AES modules. For the unmasked AES, we combine EM and power channels by using mutual information to extract the secret key in real-time mode and the experiment result shows that less measurements-to-disclosure (MTD) is used than the last version (RASCv2). Further, we adopt RASCv3 to collect EM/Power traces from the masked AES module and successfully extract the secret key from the masked AES module in fewer power/EM/dual channel traces. In the end, we generate an ASCAD format dataset named SPERO, which consists of EM and power traces collected simultaneously during unmasked/masked AES module doing encryption and upload to the community for future use.","sentences":["Cryptosystem implementations often disclose information regarding a secret key due to correlations with side channels such as power consumption, timing variations, and electromagnetic emissions.","Since power and EM channels can leak distinct information, the combination of EM and power channels could increase side-channel attack efficiency.","In this paper, we develop a miniature dual-channel side-channel detection platform, named RASCv3 to successfully extract subkeys from both unmasked and masked AES modules.","For the unmasked AES, we combine EM and power channels by using mutual information to extract the secret key in real-time mode and the experiment result shows that less measurements-to-disclosure (MTD) is used than the last version (RASCv2).","Further, we adopt RASCv3 to collect EM/Power traces from the masked AES module and successfully extract the secret key from the masked AES module in fewer power/EM/dual channel traces.","In the end, we generate an ASCAD format dataset named SPERO, which consists of EM and power traces collected simultaneously during unmasked/masked AES module doing encryption and upload to the community for future use."],"url":"http://arxiv.org/abs/2405.06571v1","category":"cs.CE"}
{"created":"2024-05-10 16:14:45","title":"The Impact of Financial Literacy, Social Capital, and Financial Technology on Financial Inclusion of Indonesian Students","abstract":"This study aims to analyze the impact of financial literacy, social capital and financial technology on financial inclusion. The research method used a quantitative research method, in which questionnaires were distributed to 100 active students in the economics faculty at 7 private colleges in Tangerang, Indonesia. Based on the results of data processing using SPSS version 23, it results that financial literacy, social capital and financial technology partially have a positive and significant influence on financial inclusion. The results of this study provide input that financial literacy needs to be increased because it is not yet equivalent to financial inclusion, and reducing the gap between financial literacy and financial inclusion is only 2.74%. Another benefit of this research is to give an understanding to students that students should be independent actors or users of financial technology products and that students should become pioneers in delivering financial knowledge, financial behavior and financial attitudes to the wider community.","sentences":["This study aims to analyze the impact of financial literacy, social capital and financial technology on financial inclusion.","The research method used a quantitative research method, in which questionnaires were distributed to 100 active students in the economics faculty at 7 private colleges in Tangerang, Indonesia.","Based on the results of data processing using SPSS version 23, it results that financial literacy, social capital and financial technology partially have a positive and significant influence on financial inclusion.","The results of this study provide input that financial literacy needs to be increased because it is not yet equivalent to financial inclusion, and reducing the gap between financial literacy and financial inclusion is only 2.74%.","Another benefit of this research is to give an understanding to students that students should be independent actors or users of financial technology products and that students should become pioneers in delivering financial knowledge, financial behavior and financial attitudes to the wider community."],"url":"http://arxiv.org/abs/2405.06570v1","category":"q-fin.GN"}
{"created":"2024-05-10 16:12:01","title":"The Role of Topological Photon Spheres in Constraining the Parameters of Black Holes","abstract":"In this paper, we investigate the topological photon sphere from two distinct perspectives. In the first view, we examine the existence and characteristics of topological photon(anti-photon)spheres for black holes with different structures, such as Einstein-Young-Mills non-minimal, AdS black holes surrounded by Chaplygin-like dark fluid, and Bardeen-like black holes in Einstein-Gauss-Bonnet gravity. Furthermore, we delve into the deeper perspective of the necessity of photon spheres for super-compact gravitational structures such as black holes. By leveraging this necessity, we propose a classification of the parameter space of black hole models based on the existence and positioning of photon spheres. This approach enables the determination of parameter ranges that delineate whether a solution represents a black hole or a naked singularity. In essence, the paper illustrates the utility of the photon sphere as a notable test for establishing the permissible and non-permissible parameter ranges within specific theories of black hole solutions.","sentences":["In this paper, we investigate the topological photon sphere from two distinct perspectives.","In the first view, we examine the existence and characteristics of topological photon(anti-photon)spheres for black holes with different structures, such as Einstein-Young-Mills non-minimal, AdS black holes surrounded by Chaplygin-like dark fluid, and Bardeen-like black holes in Einstein-Gauss-Bonnet gravity.","Furthermore, we delve into the deeper perspective of the necessity of photon spheres for super-compact gravitational structures such as black holes.","By leveraging this necessity, we propose a classification of the parameter space of black hole models based on the existence and positioning of photon spheres.","This approach enables the determination of parameter ranges that delineate whether a solution represents a black hole or a naked singularity.","In essence, the paper illustrates the utility of the photon sphere as a notable test for establishing the permissible and non-permissible parameter ranges within specific theories of black hole solutions."],"url":"http://arxiv.org/abs/2405.06568v1","category":"gr-qc"}
{"created":"2024-05-10 16:12:00","title":"Generation of multi-photon Fock states at telecommunication wavelength using picosecond pulsed light","abstract":"Multi-photon Fock states have diverse applications such as optical quantum information processing. For the implementation of quantum information processing, it is desirable that Fock states be generated within the telecommunication wavelength band, particularly in the C-band (1530-1565 nm). This is because mature optical communication technologies can be leveraged for the transmission, manipulation, and detection. Additionally, to achieve high-speed quantum information processing, it is desirable for Fock states to be generated in short optical pulses, as this allows embedding lots of information in the time domain. In this paper, we report the first generation of picosecond pulsed multi-photon Fock states (single-photon and two-photon states) in the C-band with Wigner negativities, which are verified by pulsed homodyne tomography. In our experimental setup, we utilize a single-pixel superconducting nanostrip photon-number-resolving detector (SNSPD), which is expected to facilitate the high-rate generation of various quantum states. This capability stems from the high temporal resolution of SNSPDs (50 ps in our case) allowing us to increase the repetition frequency of pulsed light from the conventional MHz range to the GHz range, although in this experiment the repetition frequency is limited to 10 MHz due to the bandwidth of the homodyne detector. Consequently, our experimental setup is anticipated to serve as a prototype of a high-speed optical quantum state generator for ultrafast quantum information processing at telecommunication wavelength.","sentences":["Multi-photon Fock states have diverse applications such as optical quantum information processing.","For the implementation of quantum information processing, it is desirable that Fock states be generated within the telecommunication wavelength band, particularly in the C-band (1530-1565 nm).","This is because mature optical communication technologies can be leveraged for the transmission, manipulation, and detection.","Additionally, to achieve high-speed quantum information processing, it is desirable for Fock states to be generated in short optical pulses, as this allows embedding lots of information in the time domain.","In this paper, we report the first generation of picosecond pulsed multi-photon Fock states (single-photon and two-photon states) in the C-band with Wigner negativities, which are verified by pulsed homodyne tomography.","In our experimental setup, we utilize a single-pixel superconducting nanostrip photon-number-resolving detector (SNSPD), which is expected to facilitate the high-rate generation of various quantum states.","This capability stems from the high temporal resolution of SNSPDs (50 ps in our case) allowing us to increase the repetition frequency of pulsed light from the conventional MHz range to the GHz range, although in this experiment the repetition frequency is limited to 10 MHz due to the bandwidth of the homodyne detector.","Consequently, our experimental setup is anticipated to serve as a prototype of a high-speed optical quantum state generator for ultrafast quantum information processing at telecommunication wavelength."],"url":"http://arxiv.org/abs/2405.06567v1","category":"quant-ph"}
{"created":"2024-05-10 16:09:55","title":"Theory of interacting vector dark energy and fluid","abstract":"In this work, we study interaction between dark energy and dark matter, where dark energy is described by a massive vector field, and dark matter is modelled as a fluid. We present new interaction term, which affects only perturbations and can give interesting phenomenology. Then we present a general Lagrangian for the interacting vector dark energy with dark matter. For the dark energy, we choose Proca theory with $G_{3}$ term to study its phenomenological consequence. For this model, we explore both background and perturbation dynamics. We also present the no-ghost condition for tensor modes and scalar modes. Subsequently, we also study the evolution of the overdensities of both baryon and cold dark matter in the high$-k$ limit. We show that the effective gravitational coupling is modified for cold dark matter and baryon. We also choose a simple concrete model and numerically show a suppression in the growth of cold dark matter overdensity.","sentences":["In this work, we study interaction between dark energy and dark matter, where dark energy is described by a massive vector field, and dark matter is modelled as a fluid.","We present new interaction term, which affects only perturbations and can give interesting phenomenology.","Then we present a general Lagrangian for the interacting vector dark energy with dark matter.","For the dark energy, we choose Proca theory with $G_{3}$ term to study its phenomenological consequence.","For this model, we explore both background and perturbation dynamics.","We also present the no-ghost condition for tensor modes and scalar modes.","Subsequently, we also study the evolution of the overdensities of both baryon and cold dark matter in the high$-k$ limit.","We show that the effective gravitational coupling is modified for cold dark matter and baryon.","We also choose a simple concrete model and numerically show a suppression in the growth of cold dark matter overdensity."],"url":"http://arxiv.org/abs/2405.06565v1","category":"gr-qc"}
{"created":"2024-05-10 16:08:33","title":"Generalised Hydrodynamics of $\\mathrm{T\\bar{T}}$-Deformed Integrable Quantum Field Theories","abstract":"In this paper we evaluate the averages of conserved densities and currents associated to charges of generic spin in (1+1)-dimensional massive integrable Quantum Field Theories perturbed by the irrelevant $\\mathrm{T\\bar{T}}$ operator. By making use of the Thermodynamic Bethe Ansatz approach and of the theory of Generalised Hydrodynamics, we study the non-equilibrium steady state averages of conserved densities and currents in a partitioning protocol. We show that in particular limits, averages can be evaluated exactly in terms of quantities known from the unperturbed theory. In the massless limit we recover known results for the energy and momentum currents and generalise those to any higher spin conserved quantities. We extend some of our results to perturbations of the generalised $\\mathrm{T\\bar{T}}$ type. For the massive free fermion theory, we find an analytic expression for the effective inverse temperature after at $\\mathrm{T\\bar{T}}$ perturbation in terms of the bare inverse temperature by making use of Lambert's $W$ function.","sentences":["In this paper we evaluate the averages of conserved densities and currents associated to charges of generic spin in (1+1)-dimensional massive integrable Quantum Field Theories perturbed by the irrelevant $\\mathrm{T\\bar{T}}$ operator.","By making use of the Thermodynamic Bethe Ansatz approach and of the theory of Generalised Hydrodynamics, we study the non-equilibrium steady state averages of conserved densities and currents in a partitioning protocol.","We show that in particular limits, averages can be evaluated exactly in terms of quantities known from the unperturbed theory.","In the massless limit we recover known results for the energy and momentum currents and generalise those to any higher spin conserved quantities.","We extend some of our results to perturbations of the generalised $\\mathrm{T\\bar{T}}$ type.","For the massive free fermion theory, we find an analytic expression for the effective inverse temperature after at $\\mathrm{T\\bar{T}}$ perturbation in terms of the bare inverse temperature by making use of Lambert's $W$ function."],"url":"http://arxiv.org/abs/2405.06564v1","category":"hep-th"}
{"created":"2024-05-10 16:06:43","title":"What Can Natural Language Processing Do for Peer Review?","abstract":"The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified by reviewing at AI conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review. Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond.","sentences":["The number of scientific articles produced every year is growing rapidly.","Providing quality control over them is crucial for scientists and, ultimately, for the public good.","In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field.","Peer review is widely used, yet it is hard, time-consuming, and prone to error.","Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing.","As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace.","Yet, where exactly is help needed, where can NLP help, and where should it stand aside?","The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance.","We discuss peer review as a general process, exemplified by reviewing at AI conferences.","We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work.","We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues.","To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review.","Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward.","We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond."],"url":"http://arxiv.org/abs/2405.06563v1","category":"cs.CL"}
{"created":"2024-05-10 16:02:40","title":"Quantum Optics with Recoiled Free Electrons","abstract":"Quantum states of light play a key role in modern quantum science, but creating hybrid quantum light-matter states remains a challenge. A promising basis for the creation of hybrid states is the interaction of free electrons with photons, which has so far been largely implemented without taking into account electron quantum recoil effects. We provide an analytical quantum electrodynamics-based framework for quantum optics with recoiled electrons and introduce a single recoil parameter $\\sigma$. With this framework, we show how to generate photon and electron-photon Bell, Greenberger-Horne-Zeilinger (GHZ) and NOON states, coherent states, squeezed vacuum (including bright squeezed vacuum) and twin beams. We analyze the transition between these states and predict a new class of photon and electron-photon quantum states shaped with the photon recoil effect (recoil-induced shaping). These results have wide potential applications including quantum computing and communication with photons and free electrons, and open up a novel avenue for ultrafast electron microscopy and next-generation free-electron sources.","sentences":["Quantum states of light play a key role in modern quantum science, but creating hybrid quantum light-matter states remains a challenge.","A promising basis for the creation of hybrid states is the interaction of free electrons with photons, which has so far been largely implemented without taking into account electron quantum recoil effects.","We provide an analytical quantum electrodynamics-based framework for quantum optics with recoiled electrons and introduce a single recoil parameter $\\sigma$. With this framework, we show how to generate photon and electron-photon Bell, Greenberger-Horne-Zeilinger (GHZ) and NOON states, coherent states, squeezed vacuum (including bright squeezed vacuum) and twin beams.","We analyze the transition between these states and predict a new class of photon and electron-photon quantum states shaped with the photon recoil effect (recoil-induced shaping).","These results have wide potential applications including quantum computing and communication with photons and free electrons, and open up a novel avenue for ultrafast electron microscopy and next-generation free-electron sources."],"url":"http://arxiv.org/abs/2405.06560v1","category":"quant-ph"}
{"created":"2024-05-10 15:56:08","title":"Tradeoffs among Action Taking Policies Matter in Active Sequential Multi-Hypothesis Testing: the Optimal Error Exponent Region","abstract":"Reliability of sequential hypothesis testing can be greatly improved when decision maker is given the freedom to adaptively take an action that determines the distribution of the current collected sample. Such advantage of sampling adaptivity has been realized since Chernoff's seminal paper in 1959. While a large body of works have explored and investigated the gain of adaptivity, in the general multiple-hypothesis setting, the fundamental limits of individual error probabilities have not been fully understood. In particular, in the asymptotic regime as the expected stopping time tends to infinity, the error exponents are only characterized in specific cases, such as that of the total error probability. In this paper, we consider a general setup of active sequential multiple-hypothesis testing where at each time slot, a temporally varying subset of data sources (out of a known set) emerges from which the decision maker can select to collect samples, subject to a family of expected selection budget constraints. The selection of sources, understood as the \"action\" at each time slot, is constrained in a predefined action space. At the end of each time slot, the decision maker either decides to make the inference on the $M$ hypotheses, or continues to observe the data sources for the next time slot. The optimal tradeoffs among $M(M-1)$ types of error exponents are characterized, and the achievable region is shown to be a convex polytope. A companion asymptotically optimal test that strikes the balance between exploration and exploitation is proposed to achieve any target error exponents within the region. To the best of our knowledge, this is the first time in the literature to identify such tradeoffs among error exponents, and it uncovers the tension among different action taking policies even in the basic setting of Chernoff.","sentences":["Reliability of sequential hypothesis testing can be greatly improved when decision maker is given the freedom to adaptively take an action that determines the distribution of the current collected sample.","Such advantage of sampling adaptivity has been realized since Chernoff's seminal paper in 1959.","While a large body of works have explored and investigated the gain of adaptivity, in the general multiple-hypothesis setting, the fundamental limits of individual error probabilities have not been fully understood.","In particular, in the asymptotic regime as the expected stopping time tends to infinity, the error exponents are only characterized in specific cases, such as that of the total error probability.","In this paper, we consider a general setup of active sequential multiple-hypothesis testing where at each time slot, a temporally varying subset of data sources (out of a known set) emerges from which the decision maker can select to collect samples, subject to a family of expected selection budget constraints.","The selection of sources, understood as the \"action\" at each time slot, is constrained in a predefined action space.","At the end of each time slot, the decision maker either decides to make the inference on the $M$ hypotheses, or continues to observe the data sources for the next time slot.","The optimal tradeoffs among $M(M-1)$ types of error exponents are characterized, and the achievable region is shown to be a convex polytope.","A companion asymptotically optimal test that strikes the balance between exploration and exploitation is proposed to achieve any target error exponents within the region.","To the best of our knowledge, this is the first time in the literature to identify such tradeoffs among error exponents, and it uncovers the tension among different action taking policies even in the basic setting of Chernoff."],"url":"http://arxiv.org/abs/2405.06554v1","category":"cs.IT"}
{"created":"2024-05-10 15:54:55","title":"Scalable Property Valuation Models via Graph-based Deep Learning","abstract":"This paper aims to enrich the capabilities of existing deep learning-based automated valuation models through an efficient graph representation of peer dependencies, thus capturing intricate spatial relationships. In particular, we develop two novel graph neural network models that effectively identify sequences of neighboring houses with similar features, employing different message passing algorithms. The first strategy consider standard spatial graph convolutions, while the second one utilizes transformer graph convolutions. This approach confers scalability to the modeling process. The experimental evaluation is conducted using a proprietary dataset comprising approximately 200,000 houses located in Santiago, Chile. We show that employing tailored graph neural networks significantly improves the accuracy of house price prediction, especially when utilizing transformer convolutional message passing layers.","sentences":["This paper aims to enrich the capabilities of existing deep learning-based automated valuation models through an efficient graph representation of peer dependencies, thus capturing intricate spatial relationships.","In particular, we develop two novel graph neural network models that effectively identify sequences of neighboring houses with similar features, employing different message passing algorithms.","The first strategy consider standard spatial graph convolutions, while the second one utilizes transformer graph convolutions.","This approach confers scalability to the modeling process.","The experimental evaluation is conducted using a proprietary dataset comprising approximately 200,000 houses located in Santiago, Chile.","We show that employing tailored graph neural networks significantly improves the accuracy of house price prediction, especially when utilizing transformer convolutional message passing layers."],"url":"http://arxiv.org/abs/2405.06553v1","category":"cs.LG"}
{"created":"2024-05-10 15:44:11","title":"OneTo3D: One Image to Re-editable Dynamic 3D Model and Video Generation","abstract":"One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image. Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields. As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions. However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video. To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video. We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability. Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature. Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions. Here we will analyze the detailed implementation methods and theories analyses. Relative comparisons and conclusions will be presented. The project code is open source.","sentences":["One image to editable dynamic 3D model and video generation is novel direction and change in the research area of single image to 3D representation or 3D reconstruction of image.","Gaussian Splatting has demonstrated its advantages in implicit 3D reconstruction, compared with the original Neural Radiance Fields.","As the rapid development of technologies and principles, people tried to used the Stable Diffusion models to generate targeted models with text instructions.","However, using the normal implicit machine learning methods is hard to gain the precise motions and actions control, further more, it is difficult to generate a long content and semantic continuous 3D video.","To address this issue, we propose the OneTo3D, a method and theory to used one single image to generate the editable 3D model and generate the targeted semantic continuous time-unlimited 3D video.","We used a normal basic Gaussian Splatting model to generate the 3D model from a single image, which requires less volume of video memory and computer calculation ability.","Subsequently, we designed an automatic generation and self-adaptive binding mechanism for the object armature.","Combined with the re-editable motions and actions analyzing and controlling algorithm we proposed, we can achieve a better performance than the SOTA projects in the area of building the 3D model precise motions and actions control, and generating a stable semantic continuous time-unlimited 3D video with the input text instructions.","Here we will analyze the detailed implementation methods and theories analyses.","Relative comparisons and conclusions will be presented.","The project code is open source."],"url":"http://arxiv.org/abs/2405.06547v1","category":"cs.CV"}
{"created":"2024-05-10 15:43:17","title":"Sharp analysis of out-of-distribution error for \"importance-weighted\" estimators in the overparameterized regime","abstract":"Overparameterized models that achieve zero training error are observed to generalize well on average, but degrade in performance when faced with data that is under-represented in the training sample. In this work, we study an overparameterized Gaussian mixture model imbued with a spurious feature, and sharply analyze the in-distribution and out-of-distribution test error of a cost-sensitive interpolating solution that incorporates \"importance weights\". Compared to recent work Wang et al. (2021), Behnia et al. (2022), our analysis is sharp with matching upper and lower bounds, and significantly weakens required assumptions on data dimensionality. Our error characterizations also apply to any choice of importance weights and unveil a novel tradeoff between worst-case robustness to distribution shift and average accuracy as a function of the importance weight magnitude.","sentences":["Overparameterized models that achieve zero training error are observed to generalize well on average, but degrade in performance when faced with data that is under-represented in the training sample.","In this work, we study an overparameterized Gaussian mixture model imbued with a spurious feature, and sharply analyze the in-distribution and out-of-distribution test error of a cost-sensitive interpolating solution that incorporates \"importance weights\".","Compared to recent work Wang et al. (2021), Behnia et al. (2022), our analysis is sharp with matching upper and lower bounds, and significantly weakens required assumptions on data dimensionality.","Our error characterizations also apply to any choice of importance weights and unveil a novel tradeoff between worst-case robustness to distribution shift and average accuracy as a function of the importance weight magnitude."],"url":"http://arxiv.org/abs/2405.06546v1","category":"stat.ML"}
{"created":"2024-05-10 15:40:50","title":"Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities across various domains, although their susceptibility to hallucination poses significant challenges for their deployment in critical areas such as healthcare. To address this issue, retrieving relevant facts from knowledge graphs (KGs) is considered a promising method. Existing KG-augmented approaches tend to be resource-intensive, requiring multiple rounds of retrieval and verification for each factoid, which impedes their application in real-world scenarios.   In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR) to augment the factuality of LLMs' responses with less retrieval efforts in the medical field. Our approach leverages the attribution of next-token predictive probability distributions across different tokens, and various model layers to primarily identify tokens with a high potential for hallucination, reducing verification rounds by refining knowledge triples associated with these tokens. Moreover, we rectify inaccurate content using retrieved knowledge in the post-processing stage, which improves the truthfulness of generated responses. Experimental results on a medical dataset demonstrate that our approach can enhance the factual capability of LLMs across various foundational models as evidenced by the highest scores on truthfulness.","sentences":["Large language models (LLMs) have demonstrated remarkable capabilities across various domains, although their susceptibility to hallucination poses significant challenges for their deployment in critical areas such as healthcare.","To address this issue, retrieving relevant facts from knowledge graphs (KGs) is considered a promising method.","Existing KG-augmented approaches tend to be resource-intensive, requiring multiple rounds of retrieval and verification for each factoid, which impedes their application in real-world scenarios.   ","In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR) to augment the factuality of LLMs' responses with less retrieval efforts in the medical field.","Our approach leverages the attribution of next-token predictive probability distributions across different tokens, and various model layers to primarily identify tokens with a high potential for hallucination, reducing verification rounds by refining knowledge triples associated with these tokens.","Moreover, we rectify inaccurate content using retrieved knowledge in the post-processing stage, which improves the truthfulness of generated responses.","Experimental results on a medical dataset demonstrate that our approach can enhance the factual capability of LLMs across various foundational models as evidenced by the highest scores on truthfulness."],"url":"http://arxiv.org/abs/2405.06545v1","category":"cs.CL"}
{"created":"2024-05-10 15:40:18","title":"Efficient distributed inner product estimation via Pauli sampling","abstract":"Cross-platform verification is the task of comparing the output states produced by different physical platforms using solely local quantum operations and classical communication. While protocols have previously been suggested for this task, their exponential sample complexity renders them unpractical even for intermediate-scale quantum systems. In this work, we propose a novel protocol for this task based on Pauli sampling, a subroutine which generates Paulis distributed according to their weight in the expansion of a quantum state in the Pauli basis. We show that our protocols for both Pauli sampling and cross-platform verification are efficient for quantum states with low magic and entanglement (i.e., of the order $O(\\log n)$). Conversely, we show super-polynomial lower bounds on the complexity of both tasks for states with $\\omega(\\log n)$ magic and entanglement. Interestingly, when considering states with real amplitudes the requirements of our protocol for cross-platform verification can be significantly weakened.","sentences":["Cross-platform verification is the task of comparing the output states produced by different physical platforms using solely local quantum operations and classical communication.","While protocols have previously been suggested for this task, their exponential sample complexity renders them unpractical even for intermediate-scale quantum systems.","In this work, we propose a novel protocol for this task based on Pauli sampling, a subroutine which generates Paulis distributed according to their weight in the expansion of a quantum state in the Pauli basis.","We show that our protocols for both Pauli sampling and cross-platform verification are efficient for quantum states with low magic and entanglement (i.e., of the order $O(\\log n)$).","Conversely, we show super-polynomial lower bounds on the complexity of both tasks for states with $\\omega(\\log n)$ magic and entanglement.","Interestingly, when considering states with real amplitudes the requirements of our protocol for cross-platform verification can be significantly weakened."],"url":"http://arxiv.org/abs/2405.06544v1","category":"quant-ph"}
{"created":"2024-05-10 15:39:37","title":"Good Things Come in Trees: Emotion and Context Aware Behaviour Trees for Ethical Robotic Decision-Making","abstract":"Emotions guide our decision making process and yet have been little explored in practical ethical decision making scenarios. In this challenge, we explore emotions and how they can influence ethical decision making in a home robot context: which fetch requests should a robot execute, and why or why not? We discuss, in particular, two aspects of emotion: (1) somatic markers: objects to be retrieved are tagged as negative (dangerous, e.g. knives or mind-altering, e.g. medicine with overdose potential), providing a quick heuristic for where to focus attention to avoid the classic Frame Problem of artificial intelligence, (2) emotion inference: users' valence and arousal levels are taken into account in defining how and when a robot should respond to a human's requests, e.g. to carefully consider giving dangerous items to users experiencing intense emotions. Our emotion-based approach builds a foundation for the primary consideration of Safety, and is complemented by policies that support overriding based on Context (e.g. age of user, allergies) and Privacy (e.g. administrator settings). Transparency is another key aspect of our solution. Our solution is defined using behaviour trees, towards an implementable design that can provide reasoning information in real-time.","sentences":["Emotions guide our decision making process and yet have been little explored in practical ethical decision making scenarios.","In this challenge, we explore emotions and how they can influence ethical decision making in a home robot context: which fetch requests should a robot execute, and why or why not?","We discuss, in particular, two aspects of emotion: (1) somatic markers: objects to be retrieved are tagged as negative (dangerous, e.g. knives or mind-altering, e.g. medicine with overdose potential), providing a quick heuristic for where to focus attention to avoid the classic Frame Problem of artificial intelligence, (2) emotion inference: users' valence and arousal levels are taken into account in defining how and when a robot should respond to a human's requests, e.g. to carefully consider giving dangerous items to users experiencing intense emotions.","Our emotion-based approach builds a foundation for the primary consideration of Safety, and is complemented by policies that support overriding based on Context (e.g. age of user, allergies) and Privacy (e.g. administrator settings).","Transparency is another key aspect of our solution.","Our solution is defined using behaviour trees, towards an implementable design that can provide reasoning information in real-time."],"url":"http://arxiv.org/abs/2405.06543v1","category":"cs.RO"}
{"created":"2024-05-10 15:38:04","title":"Microstructures and anti-phase boundaries in long-range lattice systems","abstract":"We study the effect of long-range interactions in non-convex one-dimensional lattice systems in the simplified yet meaningful assumption that the relevant long-range interactions are between $M$-neighbours for some $M\\ge 2$ and are convex. If short-range interactions are non-convex we then have a competition between short-range oscillations and long-range ordering. In the case of a double-well nearest-neighbour potential, thanks to a recent result by Braides, Causin, Solci and Truskinovsky, we are able to show that such a competition generates $M$-periodic minimizers whose arrangements are driven by an interfacial energy. Given $M$, the shape of such minimizers is universal, and independent of the details of the energies, but the number and shapes of such minimizers increases as $M$ diverges.","sentences":["We study the effect of long-range interactions in non-convex one-dimensional lattice systems in the simplified yet meaningful assumption that the relevant long-range interactions are between $M$-neighbours for some $M\\ge 2$ and are convex.","If short-range interactions are non-convex we then have a competition between short-range oscillations and long-range ordering.","In the case of a double-well nearest-neighbour potential, thanks to a recent result by Braides, Causin, Solci and Truskinovsky, we are able to show that such a competition generates $M$-periodic minimizers whose arrangements are driven by an interfacial energy.","Given $M$, the shape of such minimizers is universal, and independent of the details of the energies, but the number and shapes of such minimizers increases as $M$ diverges."],"url":"http://arxiv.org/abs/2405.06542v1","category":"math.AP"}
{"created":"2024-05-10 15:36:56","title":"ATSumm: Auxiliary information enhanced approach for abstractive disaster Tweet Summarization with sparse training data","abstract":"The abundance of situational information on Twitter poses a challenge for users to manually discern vital and relevant information during disasters. A concise and human-interpretable overview of this information helps decision-makers in implementing efficient and quick disaster response. Existing abstractive summarization approaches can be categorized as sentence-based or key-phrase-based approaches. This paper focuses on sentence-based approach, which is typically implemented as a dual-phase procedure in literature. The initial phase, known as the extractive phase, involves identifying the most relevant tweets. The subsequent phase, referred to as the abstractive phase, entails generating a more human-interpretable summary. In this study, we adopt the methodology from prior research for the extractive phase. For the abstractive phase of summarization, most existing approaches employ deep learning-based frameworks, which can either be pre-trained or require training from scratch. However, to achieve the appropriate level of performance, it is imperative to have substantial training data for both methods, which is not readily available. This work presents an Abstractive Tweet Summarizer (ATSumm) that effectively addresses the issue of data sparsity by using auxiliary information. We introduced the Auxiliary Pointer Generator Network (AuxPGN) model, which utilizes a unique attention mechanism called Key-phrase attention. This attention mechanism incorporates auxiliary information in the form of key-phrases and their corresponding importance scores from the input tweets. We evaluate the proposed approach by comparing it with 10 state-of-the-art approaches across 13 disaster datasets. The evaluation results indicate that ATSumm achieves superior performance compared to state-of-the-art approaches, with improvement of 4-80% in ROUGE-N F1-score.","sentences":["The abundance of situational information on Twitter poses a challenge for users to manually discern vital and relevant information during disasters.","A concise and human-interpretable overview of this information helps decision-makers in implementing efficient and quick disaster response.","Existing abstractive summarization approaches can be categorized as sentence-based or key-phrase-based approaches.","This paper focuses on sentence-based approach, which is typically implemented as a dual-phase procedure in literature.","The initial phase, known as the extractive phase, involves identifying the most relevant tweets.","The subsequent phase, referred to as the abstractive phase, entails generating a more human-interpretable summary.","In this study, we adopt the methodology from prior research for the extractive phase.","For the abstractive phase of summarization, most existing approaches employ deep learning-based frameworks, which can either be pre-trained or require training from scratch.","However, to achieve the appropriate level of performance, it is imperative to have substantial training data for both methods, which is not readily available.","This work presents an Abstractive Tweet Summarizer (ATSumm) that effectively addresses the issue of data sparsity by using auxiliary information.","We introduced the Auxiliary Pointer Generator Network (AuxPGN) model, which utilizes a unique attention mechanism called Key-phrase attention.","This attention mechanism incorporates auxiliary information in the form of key-phrases and their corresponding importance scores from the input tweets.","We evaluate the proposed approach by comparing it with 10 state-of-the-art approaches across 13 disaster datasets.","The evaluation results indicate that ATSumm achieves superior performance compared to state-of-the-art approaches, with improvement of 4-80% in ROUGE-N F1-score."],"url":"http://arxiv.org/abs/2405.06541v1","category":"cs.CL"}
{"created":"2024-05-10 15:33:41","title":"Gradient Descent for Noisy Optimization","abstract":"We study the use of gradient descent with backtracking line search (GD-BLS) to solve the noisy optimization problem $\\theta_\\star:=\\mathrm{argmin}_{\\theta\\in\\mathbb{R}^d} \\mathbb{E}[f(\\theta,Z)]$, imposing that the function $F(\\theta):=\\mathbb{E}[f(\\theta,Z)]$ is strictly convex. Assuming that $\\mathbb{E}[\\|\\nabla_\\theta f(\\theta_\\star,Z)\\|^2]<\\infty$ and that objective function is locally $L$-smooth, we first prove that GD-BLS allows to estimate $\\theta_\\star$ with an error of size $\\mathcal{O}_{\\mathbb{P}}(B^{-0.25})$, where $B$ is the available computational budget. We then show that we can improve upon this rate by stopping the optimization process earlier when the gradient of the objective function is sufficiently close to zero, and use the residual computational budget to optimize, again with GD-BLS, a finer approximation of $F$. By iteratively applying this strategy $J$ times, we establish that we can estimate $\\theta_\\star$ with an error of size $\\mathcal{O}_{\\mathbb{P}}(B^{-\\frac{1}{2}(1-\\delta^{J})})$, where $\\delta\\in(1/2,1)$ is a user-specified parameter. More generally, we show that if $\\mathbb{E}[\\|\\nabla_\\theta f(\\theta_\\star,Z)\\|^{1+\\alpha}]<\\infty$ for some known $\\alpha\\in (0,1]$ then this approach allows to learn $\\theta_\\star$ with an error of size $\\mathcal{O}_{\\mathbb{P}}(B^{-\\frac{\\alpha}{1+\\alpha}(1-\\delta^{J})})$, where $\\delta\\in(2\\alpha/(1+3\\alpha),1)$ is a tuning parameter. Beyond knowing $\\alpha$, achieving the aforementioned convergence rates do not require to tune the algorithms parameters according to the specific functions $F$ and $f$ at hand, and we exhibit a simple noisy optimization problem for which stochastic gradient is not guaranteed to converge while the algorithms discussed in this work are.","sentences":["We study the use of gradient descent with backtracking line search (GD-BLS) to solve the noisy optimization problem $\\theta_\\star:=\\mathrm{argmin}_{\\theta\\in\\mathbb{R}^d} \\mathbb{E}[f(\\theta,Z)]$, imposing that the function $F(\\theta):=\\mathbb{E}[f(\\theta,Z)]$ is strictly convex.","Assuming that $\\mathbb{E}[\\|\\nabla_\\theta f(\\theta_\\star,Z)\\|^2]<\\infty$ and that objective function is locally $L$-smooth, we first prove that GD-BLS allows to estimate $\\theta_\\star$ with an error of size $\\mathcal{O}_{\\mathbb{P}}(B^{-0.25})$, where $B$ is the available computational budget.","We then show that we can improve upon this rate by stopping the optimization process earlier when the gradient of the objective function is sufficiently close to zero, and use the residual computational budget to optimize, again with GD-BLS, a finer approximation of $F$. By iteratively applying this strategy $J$ times, we establish that we can estimate $\\theta_\\star$ with an error of size $\\mathcal{O}_{\\mathbb{P}}(B^{-\\frac{1}{2}(1-\\delta^{J})})$, where $\\delta\\in(1/2,1)$ is a user-specified parameter.","More generally, we show that if $\\mathbb{E}[\\|\\nabla_\\theta f(\\theta_\\star,Z)\\|^{1+\\alpha}]<\\infty$ for some known $\\alpha\\in (0,1]$ then this approach allows to learn $\\theta_\\star$ with an error of size $\\mathcal{O}_{\\mathbb{P}}(B^{-\\frac{\\alpha}{1+\\alpha}(1-\\delta^{J})})$, where $\\delta\\in(2\\alpha/(1+3\\alpha),1)$ is a tuning parameter.","Beyond knowing $\\alpha$, achieving the aforementioned convergence rates do not require to tune the algorithms parameters according to the specific functions $F$ and $f$ at hand, and we exhibit a simple noisy optimization problem for which stochastic gradient is not guaranteed to converge while the algorithms discussed in this work are."],"url":"http://arxiv.org/abs/2405.06539v1","category":"math.OC"}
{"created":"2024-05-10 15:31:49","title":"Optical magnetoelectric effect in the polar honeycomb antiferromagnet Fe2Mo3O8","abstract":"The lack of both time-reversal and spatial inversion symmetry in polar magnets is a prerequisite for the occurrence of optical magnetoelectric effects such as nonreciprocal directional dichroism with the potential for the realization of optical diodes. In particular, antiferromagnetic materials with magnetic excitations in the THz range such as Fe2Mo3O8 are promising candidates for next-generation spintronic applications. In a combined experimental and theoretical effort we investigated the THz excitations of the polar honeycomb antiferromagnet Fe2Mo3O8 in external magnetic fields and their nonreciprocal directional dichroism, together with the temperature dependence of the electronic transitions in the mid- and near-infrared frequency range. Using an advanced single-ion approach for the Fe ions, we are able to describe optical excitations from the THz to the near-infrared frequency range quantitatively and successfully model the observed nonreciprocal directional dichroism in the THz regime.","sentences":["The lack of both time-reversal and spatial inversion symmetry in polar magnets is a prerequisite for the occurrence of optical magnetoelectric effects such as nonreciprocal directional dichroism with the potential for the realization of optical diodes.","In particular, antiferromagnetic materials with magnetic excitations in the THz range such as Fe2Mo3O8 are promising candidates for next-generation spintronic applications.","In a combined experimental and theoretical effort we investigated the THz excitations of the polar honeycomb antiferromagnet Fe2Mo3O8 in external magnetic fields and their nonreciprocal directional dichroism, together with the temperature dependence of the electronic transitions in the mid- and near-infrared frequency range.","Using an advanced single-ion approach for the Fe ions, we are able to describe optical excitations from the THz to the near-infrared frequency range quantitatively and successfully model the observed nonreciprocal directional dichroism in the THz regime."],"url":"http://arxiv.org/abs/2405.06538v1","category":"cond-mat.str-el"}
{"created":"2024-05-10 15:27:35","title":"Controllable Image Generation With Composed Parallel Token Prediction","abstract":"Compositional image generation requires models to generalise well in situations where two or more input concepts do not necessarily appear together in training (compositional generalisation). Despite recent progress in compositional image generation via composing continuous sampling processes such as diffusion and energy-based models, composing discrete generative processes has remained an open challenge, with the promise of providing improvements in efficiency, interpretability and simplicity. To this end, we propose a formulation for controllable conditional generation of images via composing the log-probability outputs of discrete generative models of the latent space. Our approach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art generation accuracy in three distinct settings (FFHQ, Positional CLEVR and Relational CLEVR) while attaining competitive Fr\\'echet Inception Distance (FID) scores. Our method attains an average generation accuracy of $80.71\\%$ across the studied settings. Our method also outperforms the next-best approach (ranked by accuracy) in terms of FID in seven out of nine experiments, with an average FID of $24.23$ (an average improvement of $-9.58$). Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous compositional methods on our hardware. We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image) in addition to offering an interpretable dimension of controllability via concept weighting. We further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model without any fine-tuning, allowing for fine-grained control of text-to-image generation.","sentences":["Compositional image generation requires models to generalise well in situations where two or more input concepts do not necessarily appear together in training (compositional generalisation).","Despite recent progress in compositional image generation via composing continuous sampling processes such as diffusion and energy-based models, composing discrete generative processes has remained an open challenge, with the promise of providing improvements in efficiency, interpretability and simplicity.","To this end, we propose a formulation for controllable conditional generation of images via composing the log-probability outputs of discrete generative models of the latent space.","Our approach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art generation accuracy in three distinct settings (FFHQ, Positional CLEVR and Relational CLEVR) while attaining competitive Fr\\'echet Inception Distance (FID) scores.","Our method attains an average generation accuracy of $80.71\\%$ across the studied settings.","Our method also outperforms the next-best approach (ranked by accuracy) in terms of FID in seven out of nine experiments, with an average FID of $24.23$ (an average improvement of $-9.58$).","Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous compositional methods on our hardware.","We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image) in addition to offering an interpretable dimension of controllability via concept weighting.","We further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model without any fine-tuning, allowing for fine-grained control of text-to-image generation."],"url":"http://arxiv.org/abs/2405.06535v1","category":"cs.CV"}
{"created":"2024-05-10 15:20:07","title":"Bridging Rayleigh-Jeans and Bose-Einstein condensation of a guided fluid of light with positive and negative temperatures","abstract":"We consider the free propagation geometry of a light beam (or fluid of light) in a multimode waveguide. As a result of the effective photon-photon interactions, the photon fluid thermalizes to an equilibrium state during its conservative propagation. In this configuration, Rayleigh-Jeans (RJ) thermalization and condensation of classical light waves have been recently observed experimentally in graded index multimode optical fibers characterized by a 2D parabolic trapping potential. As well-known, the properties of RJ condensation differ substantially from those of Bose-Einstein (BE) condensation: The condensate fraction decreases quadratically with the temperature for BE condensation, while it decreases linearly for RJ condensation. Furthermore, for quantum particles the heat capacity tends to zero at small temperatures, and it takes a constant value in the classical particle limit at high temperatures. This is in contrast with classical RJ waves, where the specific heat takes a constant value at small temperatures, and tends to vanish above the condensation transition in the normal (uncondensed) state. Here, we reconcile the thermodynamic properties of BE and RJ condensation: By introducing a frequency cut-off inherent to light propagation in a waveguide, we derive generalized expressions of the thermodynamic properties that include the RJ and BE limits as particular cases. We extend the approach to encompass negative temperatures. In contrast to positive temperatures, the specific heat does not display a singular behavior at negative temperatures, reflecting the non-critical nature of the transition to a macroscopic population of the highest energy level. Our work contributes to understanding the quantum-to-classical crossover in the equilibrium properties of light, within a versatile experimental platform based on nonlinear optical propagation in multimode waveguides.","sentences":["We consider the free propagation geometry of a light beam (or fluid of light) in a multimode waveguide.","As a result of the effective photon-photon interactions, the photon fluid thermalizes to an equilibrium state during its conservative propagation.","In this configuration, Rayleigh-Jeans (RJ) thermalization and condensation of classical light waves have been recently observed experimentally in graded index multimode optical fibers characterized by a 2D parabolic trapping potential.","As well-known, the properties of RJ condensation differ substantially from those of Bose-Einstein (BE) condensation: The condensate fraction decreases quadratically with the temperature for BE condensation, while it decreases linearly for RJ condensation.","Furthermore, for quantum particles the heat capacity tends to zero at small temperatures, and it takes a constant value in the classical particle limit at high temperatures.","This is in contrast with classical RJ waves, where the specific heat takes a constant value at small temperatures, and tends to vanish above the condensation transition in the normal (uncondensed) state.","Here, we reconcile the thermodynamic properties of BE and RJ condensation: By introducing a frequency cut-off inherent to light propagation in a waveguide, we derive generalized expressions of the thermodynamic properties that include the RJ and BE limits as particular cases.","We extend the approach to encompass negative temperatures.","In contrast to positive temperatures, the specific heat does not display a singular behavior at negative temperatures, reflecting the non-critical nature of the transition to a macroscopic population of the highest energy level.","Our work contributes to understanding the quantum-to-classical crossover in the equilibrium properties of light, within a versatile experimental platform based on nonlinear optical propagation in multimode waveguides."],"url":"http://arxiv.org/abs/2405.06531v1","category":"physics.optics"}
{"created":"2024-05-10 15:10:20","title":"Prompting Large Language Models with Knowledge Graphs for Question Answering Involving Long-tail Facts","abstract":"Although Large Language Models (LLMs) are effective in performing various NLP tasks, they still struggle to handle tasks that require extensive, real-world knowledge, especially when dealing with long-tail facts (facts related to long-tail entities). This limitation highlights the need to supplement LLMs with non-parametric knowledge. To address this issue, we analysed the effects of different types of non-parametric knowledge, including textual passage and knowledge graphs (KGs). Since LLMs have probably seen the majority of factual question-answering datasets already, to facilitate our analysis, we proposed a fully automatic pipeline for creating a benchmark that requires knowledge of long-tail facts for answering the involved questions. Using this pipeline, we introduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different knowledge settings using the proposed benchmark. Our experiments show that LLMs alone struggle with answering these questions, especially when the long-tail level is high or rich knowledge is required. Nonetheless, the performance of the same models improved significantly when they were prompted with non-parametric knowledge. We observed that, in most cases, prompting LLMs with KG triples surpasses passage-based prompting using a state-of-the-art retriever. In addition, while prompting LLMs with both KG triples and documents does not consistently improve knowledge coverage, it can dramatically reduce hallucinations in the generated content.","sentences":["Although Large Language Models (LLMs) are effective in performing various NLP tasks, they still struggle to handle tasks that require extensive, real-world knowledge, especially when dealing with long-tail facts (facts related to long-tail entities).","This limitation highlights the need to supplement LLMs with non-parametric knowledge.","To address this issue, we analysed the effects of different types of non-parametric knowledge, including textual passage and knowledge graphs (KGs).","Since LLMs have probably seen the majority of factual question-answering datasets already, to facilitate our analysis, we proposed a fully automatic pipeline for creating a benchmark that requires knowledge of long-tail facts for answering the involved questions.","Using this pipeline, we introduce the LTGen benchmark.","We evaluate state-of-the-art LLMs in different knowledge settings using the proposed benchmark.","Our experiments show that LLMs alone struggle with answering these questions, especially when the long-tail level is high or rich knowledge is required.","Nonetheless, the performance of the same models improved significantly when they were prompted with non-parametric knowledge.","We observed that, in most cases, prompting LLMs with KG triples surpasses passage-based prompting using a state-of-the-art retriever.","In addition, while prompting LLMs with both KG triples and documents does not consistently improve knowledge coverage, it can dramatically reduce hallucinations in the generated content."],"url":"http://arxiv.org/abs/2405.06524v1","category":"cs.CL"}
{"created":"2024-05-10 15:06:53","title":"Heterogeneous Graph Neural Networks with Loss-decrease-aware Curriculum Learning","abstract":"In recent years, heterogeneous graph neural networks (HGNNs) have achieved excellent performance in handling heterogeneous information networks (HINs). Curriculum learning is a machine learning strategy where training examples are presented to a model in a structured order, starting with easy examples and gradually increasing difficulty, aiming to improve learning efficiency and generalization. To better exploit the rich information in HINs, previous methods have started to explore the use of curriculum learning strategy to train HGNNs. Specifically, these works utilize the absolute value of the loss at each training epoch to evaluate the learning difficulty of each training sample. However, the relative loss, rather than the absolute value of loss, reveals the learning difficulty. Therefore, we propose a novel loss-decrease-aware training schedule (LDTS). LDTS uses the trend of loss decrease between each training epoch to better evaluating the difficulty of training samples, thereby enhancing the curriculum learning of HGNNs for downstream tasks. Additionally, we propose a sampling strategy to alleviate training imbalance issues. Our method further demonstrate the efficacy of curriculum learning in enhancing HGNNs capabilities. We call our method Loss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN). The code is public at https://github.com/wangyili00/LDHGNN.","sentences":["In recent years, heterogeneous graph neural networks (HGNNs) have achieved excellent performance in handling heterogeneous information networks (HINs).","Curriculum learning is a machine learning strategy where training examples are presented to a model in a structured order, starting with easy examples and gradually increasing difficulty, aiming to improve learning efficiency and generalization.","To better exploit the rich information in HINs, previous methods have started to explore the use of curriculum learning strategy to train HGNNs.","Specifically, these works utilize the absolute value of the loss at each training epoch to evaluate the learning difficulty of each training sample.","However, the relative loss, rather than the absolute value of loss, reveals the learning difficulty.","Therefore, we propose a novel loss-decrease-aware training schedule (LDTS).","LDTS uses the trend of loss decrease between each training epoch to better evaluating the difficulty of training samples, thereby enhancing the curriculum learning of HGNNs for downstream tasks.","Additionally, we propose a sampling strategy to alleviate training imbalance issues.","Our method further demonstrate the efficacy of curriculum learning in enhancing HGNNs capabilities.","We call our method Loss-decrease-aware Heterogeneous Graph Neural Networks (LDHGNN).","The code is public at https://github.com/wangyili00/LDHGNN."],"url":"http://arxiv.org/abs/2405.06522v1","category":"cs.LG"}
{"created":"2024-05-10 15:00:25","title":"Diameter of Commuting Graphs of Lie Algebras","abstract":"In this paper, we study the connectedness of the commuting graph of a general Lie algebra and provide a process to determine whether the commuting graph is connected or not, as well as to compute an upper bound for its diameter. In addition, we will examine the connectedness and diameter of the commuting graphs of some remarkable classes of Lie algebras, including: (1) a class of Lie algebras with one- or two-dimensional derived algebras; and (2) a class of solvable Lie algebras over the real field of dimension up to $4$.","sentences":["In this paper, we study the connectedness of the commuting graph of a general Lie algebra and provide a process to determine whether the commuting graph is connected or not, as well as to compute an upper bound for its diameter.","In addition, we will examine the connectedness and diameter of the commuting graphs of some remarkable classes of Lie algebras, including: (1) a class of Lie algebras with one- or two-dimensional derived algebras; and (2) a class of solvable Lie algebras over the real field of dimension up to $4$."],"url":"http://arxiv.org/abs/2405.06521v1","category":"math.RA"}
{"created":"2024-05-10 15:00:24","title":"Origin and Evolution of Angular Momentum of Class II Disks","abstract":"Context. While Class II stars have already accreted most of their mass, the continued inflow of fresh material via Bondi-Hoyle accretion acts as an additional mass reservoir for their circumstellar disks. This may explain the observed accretion rates of pre-main- sequence (PMS) stars, as well as observational inconsistencies in the mass and angular momentum balance of their disks. Aims. Using a new simulation that reproduces the stellar IMF, we want to quantify the role of Bondi-Hoyle accretion in the formation of Class II disks, as well address the prospect of its observational detection with the James Webb Space Telescope (JWST). Methods. We study the mass and angular momentum of the accreting gas using passively-advected tracer particles in the simulation, and we carry out radiative transfer calculations of near-infrared scattering to generate synthetic JWST observations of Bondi-Hoyle trails of PMS stars. Results. Gas accreting on Class II PMS stars approximately 1 Myr after their formation has enough mass and angular momentum to strongly affect the evolution of the preexisting disks. The accreted angular momentum is large enough to also explain the observed size of Class II disks. The orientation of the angular momentum vector can differ significantly from that of the previously accreted gas, which may result in a significant disk warping or misalignment. We also predict that JWST observations of Class II stars will be able to detect Bondi-Hoyle trails with a 90% success rate with only 2 min exposure time, if stars with accretion rates \\dot{M} > 5e-10 Msol/yr and luminosity of L > 0.5 Lsol are selected.","sentences":["Context.","While Class II stars have already accreted most of their mass, the continued inflow of fresh material via Bondi-Hoyle accretion acts as an additional mass reservoir for their circumstellar disks.","This may explain the observed accretion rates of pre-main- sequence (PMS) stars, as well as observational inconsistencies in the mass and angular momentum balance of their disks.","Aims.","Using a new simulation that reproduces the stellar IMF, we want to quantify the role of Bondi-Hoyle accretion in the formation of Class II disks, as well address the prospect of its observational detection with the James Webb Space Telescope (JWST).","Methods.","We study the mass and angular momentum of the accreting gas using passively-advected tracer particles in the simulation, and we carry out radiative transfer calculations of near-infrared scattering to generate synthetic JWST observations of Bondi-Hoyle trails of PMS stars.","Results.","Gas accreting on Class II PMS stars approximately 1 Myr after their formation has enough mass and angular momentum to strongly affect the evolution of the preexisting disks.","The accreted angular momentum is large enough to also explain the observed size of Class II disks.","The orientation of the angular momentum vector can differ significantly from that of the previously accreted gas, which may result in a significant disk warping or misalignment.","We also predict that JWST observations of Class II stars will be able to detect Bondi-Hoyle trails with a 90% success rate with only 2 min exposure time, if stars with accretion rates \\dot{M} >","5e-10 Msol/yr and luminosity of L > 0.5 Lsol are selected."],"url":"http://arxiv.org/abs/2405.06520v1","category":"astro-ph.SR"}
{"created":"2024-05-10 14:58:12","title":"Long time evolution of concentrated vortex rings with large radius","abstract":"We study the time evolution of an incompressible fluid with axial symmetry without swirl when the vorticity is sharply concentrated on $N$ annuli of radii of the order of $r_0$ and thickness $\\varepsilon$. We prove that when $r_0= |\\log \\varepsilon|^\\alpha$, $\\alpha>1$, the vorticity field of the fluid converges for $\\varepsilon \\to 0$ to the point vortex model, in an interval of time which diverges as $\\log|\\log\\varepsilon|$. This generalizes previous result by Cavallaro and Marchioro in [J. Math. Phys. 62, 053102, (2021)], that assumed $\\alpha>2$ and in which the convergence was proved for short times only.","sentences":["We study the time evolution of an incompressible fluid with axial symmetry without swirl when the vorticity is sharply concentrated on $N$ annuli of radii of the order of $r_0$ and thickness $\\varepsilon$. We prove that when $r_0= |\\log \\varepsilon|^\\alpha$, $\\alpha>1$, the vorticity field of the fluid converges for $\\varepsilon \\to 0$ to the point vortex model, in an interval of time which diverges as $\\log|\\log\\varepsilon|$. This generalizes previous result by Cavallaro and Marchioro in [J. Math.","Phys. 62, 053102, (2021)], that assumed $\\alpha>2$ and in which the convergence was proved for short times only."],"url":"http://arxiv.org/abs/2405.06518v1","category":"math.AP"}
{"created":"2024-05-10 14:54:11","title":"Holographic RG flows in a 3d gauged supergravity at finite temperature","abstract":"In this paper we consider finite-temperature holographic RG flows in $D=3$ $\\mathcal{N}=(2,0)$ gauged truncated supergravity coupled to a sigma model with a hyperbolic target space. In the context of the holographic duality, fixed points (CFTs) at finite temperature are described by AdS black holes. We come from the gravity EOM to a 3d autonomous dynamical system, which critical points can be related to fixed points of dual field theories. Near-horizon black hole solutions correspond to infinite points of this system. We use Poincar\\'e transformations to project the system on $\\mathbf{R}^3$ into the 3d unit cylinder such that the infinite points are mapped onto the boundary of the cylinder. We explore numerically the space of solutions. We show that the exact RG flow at zero temperature is the separatrix for asymptotically AdS black hole solutions if the potential has one extremum, while for the potential with three extrema the separatrices are RG flows between AdS fixed points. We find near-horizon analytical solutions for asymptotically AdS black holes using the dynamical equations. We also present a method for constructing full analytical solutions.","sentences":["In this paper we consider finite-temperature holographic RG flows in $D=3$ $\\mathcal{N}=(2,0)$ gauged truncated supergravity coupled to a sigma model with a hyperbolic target space.","In the context of the holographic duality, fixed points (CFTs) at finite temperature are described by AdS black holes.","We come from the gravity EOM to a 3d autonomous dynamical system, which critical points can be related to fixed points of dual field theories.","Near-horizon black hole solutions correspond to infinite points of this system.","We use Poincar\\'e transformations to project the system on $\\mathbf{R}^3$ into the 3d unit cylinder such that the infinite points are mapped onto the boundary of the cylinder.","We explore numerically the space of solutions.","We show that the exact RG flow at zero temperature is the separatrix for asymptotically AdS black hole solutions if the potential has one extremum, while for the potential with three extrema the separatrices are RG flows between AdS fixed points.","We find near-horizon analytical solutions for asymptotically AdS black holes using the dynamical equations.","We also present a method for constructing full analytical solutions."],"url":"http://arxiv.org/abs/2405.06515v1","category":"hep-th"}
{"created":"2024-05-10 14:48:48","title":"Sensing Gravity with Polarized Electromagnetic Radiation","abstract":"Polarization wiggling is a direct gravitational effect exerted by a gravitational field on any electromagnetic radiation traversing it. This effect is investigated in linear gravity for spacetimes with flat or conformally flat backgrounds. First, we show how the polarization wiggle rates can be computed in the conformal frame and transformed to the physical frame. Then it is shown that polarization wiggling is not sensitive to scalar perturbations to the metric, while vector and tensor perturbations do induce polarization wiggling. This poses two natural questions: Can polarized electromagnetic radiation be used to measure vectorial and tensorial components of gravitational fields directly? And if so, how? Next, polarization wiggling is studied for an arbitrary vector perturbation to the spacetime metric. In a stationary spacetime, the polarization wiggle rate is proportional to the difference in frame dragging rates between radiation emission and measurement events. We show how this can be used to measure the angular momentum of a gravitational source if the emitter orbits the gravitational source on a known orbit. Next, the effect of a gravitational tensor mode with arbitrary polarization is studied. Finally, the polarization wiggling effect induced by a gravitational tensor mode is analyzed. Two cases are demonstrated: A spacetime with a flat Minkowski background and an expanding cosmology with a conformally flat background. In both cases, the polarization wiggling frequency equals the frequency of the gravitational tensor mode, while the other state parameters are encoded in the polarization wiggling amplitude and phase of the polarized radiation. We show that measurements of polarization wiggling frequency, amplitude and phase of polarized radiation from different directions enables all state parameters of a gravitational tensor mode to be determined.","sentences":["Polarization wiggling is a direct gravitational effect exerted by a gravitational field on any electromagnetic radiation traversing it.","This effect is investigated in linear gravity for spacetimes with flat or conformally flat backgrounds.","First, we show how the polarization wiggle rates can be computed in the conformal frame and transformed to the physical frame.","Then it is shown that polarization wiggling is not sensitive to scalar perturbations to the metric, while vector and tensor perturbations do induce polarization wiggling.","This poses two natural questions: Can polarized electromagnetic radiation be used to measure vectorial and tensorial components of gravitational fields directly?","And if so, how?","Next, polarization wiggling is studied for an arbitrary vector perturbation to the spacetime metric.","In a stationary spacetime, the polarization wiggle rate is proportional to the difference in frame dragging rates between radiation emission and measurement events.","We show how this can be used to measure the angular momentum of a gravitational source if the emitter orbits the gravitational source on a known orbit.","Next, the effect of a gravitational tensor mode with arbitrary polarization is studied.","Finally, the polarization wiggling effect induced by a gravitational tensor mode is analyzed.","Two cases are demonstrated: A spacetime with a flat Minkowski background and an expanding cosmology with a conformally flat background.","In both cases, the polarization wiggling frequency equals the frequency of the gravitational tensor mode, while the other state parameters are encoded in the polarization wiggling amplitude and phase of the polarized radiation.","We show that measurements of polarization wiggling frequency, amplitude and phase of polarized radiation from different directions enables all state parameters of a gravitational tensor mode to be determined."],"url":"http://arxiv.org/abs/2405.06513v1","category":"gr-qc"}
{"created":"2024-05-10 14:47:19","title":"Linear dynamical systems with continuous weight functions","abstract":"In discrete-time linear dynamical systems (LDSs), a linear map is repeatedly applied to an initial vector yielding a sequence of vectors called the orbit of the system. A weight function assigning weights to the points in the orbit can be used to model quantitative aspects, such as resource consumption, of a system modelled by an LDS. This paper addresses the problems to compute the mean payoff, the total accumulated weight, and the discounted accumulated weight of the orbit under continuous weight functions and polynomial weight functions as a special case. Besides general LDSs, the special cases of stochastic LDSs and of LDSs with bounded orbits are considered. Furthermore, the problem of deciding whether an energy constraint is satisfied by the weighted orbit, i.e., whether the accumulated weight never drops below a given bound, is analysed.","sentences":["In discrete-time linear dynamical systems (LDSs), a linear map is repeatedly applied to an initial vector yielding a sequence of vectors called the orbit of the system.","A weight function assigning weights to the points in the orbit can be used to model quantitative aspects, such as resource consumption, of a system modelled by an LDS.","This paper addresses the problems to compute the mean payoff, the total accumulated weight, and the discounted accumulated weight of the orbit under continuous weight functions and polynomial weight functions as a special case.","Besides general LDSs, the special cases of stochastic LDSs and of LDSs with bounded orbits are considered.","Furthermore, the problem of deciding whether an energy constraint is satisfied by the weighted orbit, i.e., whether the accumulated weight never drops below a given bound, is analysed."],"url":"http://arxiv.org/abs/2405.06512v1","category":"math.DS"}
{"created":"2024-05-10 14:44:04","title":"UniDM: A Unified Framework for Data Manipulation with Large Language Models","abstract":"Designing effective data manipulation methods is a long standing problem in data lakes. Traditional methods, which rely on rules or machine learning models, require extensive human efforts on training data collection and tuning models. Recent methods apply Large Language Models (LLMs) to resolve multiple data manipulation tasks. They exhibit bright benefits in terms of performance but still require customized designs to fit each specific task. This is very costly and can not catch up with the requirements of big data lake platforms. In this paper, inspired by the cross-task generality of LLMs on NLP tasks, we pave the first step to design an automatic and general solution to tackle with data manipulation tasks. We propose UniDM, a unified framework which establishes a new paradigm to process data manipulation tasks using LLMs. UniDM formalizes a number of data manipulation tasks in a unified form and abstracts three main general steps to solve each task. We develop an automatic context retrieval to allow the LLMs to retrieve data from data lakes, potentially containing evidence and factual information. For each step, we design effective prompts to guide LLMs to produce high quality results. By our comprehensive evaluation on a variety of benchmarks, our UniDM exhibits great generality and state-of-the-art performance on a wide variety of data manipulation tasks.","sentences":["Designing effective data manipulation methods is a long standing problem in data lakes.","Traditional methods, which rely on rules or machine learning models, require extensive human efforts on training data collection and tuning models.","Recent methods apply Large Language Models (LLMs) to resolve multiple data manipulation tasks.","They exhibit bright benefits in terms of performance but still require customized designs to fit each specific task.","This is very costly and can not catch up with the requirements of big data lake platforms.","In this paper, inspired by the cross-task generality of LLMs on NLP tasks, we pave the first step to design an automatic and general solution to tackle with data manipulation tasks.","We propose UniDM, a unified framework which establishes a new paradigm to process data manipulation tasks using LLMs.","UniDM formalizes a number of data manipulation tasks in a unified form and abstracts three main general steps to solve each task.","We develop an automatic context retrieval to allow the LLMs to retrieve data from data lakes, potentially containing evidence and factual information.","For each step, we design effective prompts to guide LLMs to produce high quality results.","By our comprehensive evaluation on a variety of benchmarks, our UniDM exhibits great generality and state-of-the-art performance on a wide variety of data manipulation tasks."],"url":"http://arxiv.org/abs/2405.06510v1","category":"cs.AI"}
{"created":"2024-05-10 14:42:38","title":"Constraints on primordial lepton asymmetries with full neutrino transport","abstract":"Primordial neutrino-antineutrino asymmetries can be constrained through big-bang nucleosynthesis (BBN) relic abundances and cosmic microwave background (CMB) anisotropies, both observables being sensitive to neutrino properties. The latter constraint, which is due to gravitational effects from all neutrino flavors, is very minute since it is at least quadratic in the asymmetries. On the contrary, the constraints from primordial abundances presently dominate, although these abundances are almost only sensitive to the electron flavor asymmetry. It is generally assumed that neutrino asymmetries are sufficiently averaged by flavor oscillations prior to BBN, which allows to constrain a common primordial neutrino asymmetry at the epoch of BBN. This simplified approach suffers two caveats that we deal with in this article, combining a neutrino evolution code and BBN calculation throughout the MeV era. First, flavor \"equilibration\" is not true in general, therefore an accurate dynamical evolution of asymmetries is needed to connect experimental observables to the primordial asymmetries. Second, the approximate averaging of asymmetries through flavor oscillations is associated to a reheating of the primordial plasma. It is therefore crucial to correctly describe the interplay between flavor equilibration and neutrino decoupling, as an energy redistribution prior to decoupling does not significantly alter the final effective number of neutrino species' value. Overall, we find that the space of allowed initial asymmetries is generically unbound when using currently available primordial abundances and CMB measurements. We forecast constraints using future CMB experiment capabilities, which should reverse this experimental misfortune.","sentences":["Primordial neutrino-antineutrino asymmetries can be constrained through big-bang nucleosynthesis (BBN) relic abundances and cosmic microwave background (CMB) anisotropies, both observables being sensitive to neutrino properties.","The latter constraint, which is due to gravitational effects from all neutrino flavors, is very minute since it is at least quadratic in the asymmetries.","On the contrary, the constraints from primordial abundances presently dominate, although these abundances are almost only sensitive to the electron flavor asymmetry.","It is generally assumed that neutrino asymmetries are sufficiently averaged by flavor oscillations prior to BBN, which allows to constrain a common primordial neutrino asymmetry at the epoch of BBN.","This simplified approach suffers two caveats that we deal with in this article, combining a neutrino evolution code and BBN calculation throughout the MeV era.","First, flavor \"equilibration\" is not true in general, therefore an accurate dynamical evolution of asymmetries is needed to connect experimental observables to the primordial asymmetries.","Second, the approximate averaging of asymmetries through flavor oscillations is associated to a reheating of the primordial plasma.","It is therefore crucial to correctly describe the interplay between flavor equilibration and neutrino decoupling, as an energy redistribution prior to decoupling does not significantly alter the final effective number of neutrino species' value.","Overall, we find that the space of allowed initial asymmetries is generically unbound when using currently available primordial abundances and CMB measurements.","We forecast constraints using future CMB experiment capabilities, which should reverse this experimental misfortune."],"url":"http://arxiv.org/abs/2405.06509v1","category":"hep-ph"}
{"created":"2024-05-10 14:39:30","title":"Simple crowd dynamics to generate complex temporal contact networks","abstract":"Empirical contact networks or interaction networks demonstrate peculiar characteristics stemming from the fundamental social, psychological, physical mechanisms governing human interactions. Although these mechanisms are complex, we test whether we are able to reproduce some dynamical properties of these empirical networks from relatively simple models. In this study, we perform simulations for a range of 2D models of particle dynamics, namely the Random Walk, Active Brownian Particles, and Vicsek models, to generate artificial contact networks. We investigate temporal properties of these contact networks: the distributions of contact durations, inter-contact durations and number of contact per pair of particle. We demonstrate that the distribution of inter-contact durations can be recovered by the dynamics of these simple crowd particle models, and show that it is simply related to the well-know first-return process, which explains the -3/2 exponent that is found in both the numerical models and empirical contact networks.","sentences":["Empirical contact networks or interaction networks demonstrate peculiar characteristics stemming from the fundamental social, psychological, physical mechanisms governing human interactions.","Although these mechanisms are complex, we test whether we are able to reproduce some dynamical properties of these empirical networks from relatively simple models.","In this study, we perform simulations for a range of 2D models of particle dynamics, namely the Random Walk, Active Brownian Particles, and Vicsek models, to generate artificial contact networks.","We investigate temporal properties of these contact networks: the distributions of contact durations, inter-contact durations and number of contact per pair of particle.","We demonstrate that the distribution of inter-contact durations can be recovered by the dynamics of these simple crowd particle models, and show that it is simply related to the well-know first-return process, which explains the -3/2 exponent that is found in both the numerical models and empirical contact networks."],"url":"http://arxiv.org/abs/2405.06508v1","category":"physics.soc-ph"}
{"created":"2024-05-10 14:37:24","title":"Hal: A Language-General Framework for Analysis of User-Specified Monotone Frameworks","abstract":"Writing dataflow analyzers requires both language and domain-specificity. That is to say, each programming language and each program property requires its own analyzer. To enable a streamlined, user-driven approach to dataflow analyzers, we introduce the theoretical framework for a user-specified dataflow analysis. This framework is constructed in such a way that the user has to specify as little as possible, while the analyzer infers and computes everything else, including interprocedural embellishments. This theoretical framework was also implemented in Java, where users can specify a program property alongside minimal extra information to induce a dataflow analysis. This framework (both theoretical and in implementation) is language-general, meaning that it is independent of syntax and semantics (as all necessary syntactic and semantic information is provided by the user, and this information is provided only once for a given language). In this paper, we introduce basic notions of intraprocedural and interprocedural dataflow analyses, the proposed \"Implicit Monotone Framework,\" and a rigorous framework for partial functions as a property space.","sentences":["Writing dataflow analyzers requires both language and domain-specificity.","That is to say, each programming language and each program property requires its own analyzer.","To enable a streamlined, user-driven approach to dataflow analyzers, we introduce the theoretical framework for a user-specified dataflow analysis.","This framework is constructed in such a way that the user has to specify as little as possible, while the analyzer infers and computes everything else, including interprocedural embellishments.","This theoretical framework was also implemented in Java, where users can specify a program property alongside minimal extra information to induce a dataflow analysis.","This framework (both theoretical and in implementation) is language-general, meaning that it is independent of syntax and semantics (as all necessary syntactic and semantic information is provided by the user, and this information is provided only once for a given language).","In this paper, we introduce basic notions of intraprocedural and interprocedural dataflow analyses, the proposed \"Implicit Monotone Framework,\" and a rigorous framework for partial functions as a property space."],"url":"http://arxiv.org/abs/2405.06505v1","category":"cs.PL"}
{"created":"2024-05-10 14:29:51","title":"Multi-Target Unsupervised Domain Adaptation for Semantic Segmentation without External Data","abstract":"Multi-target unsupervised domain adaptation (UDA) aims to learn a unified model to address the domain shift between multiple target domains. Due to the difficulty of obtaining annotations for dense predictions, it has recently been introduced into cross-domain semantic segmentation. However, most existing solutions require labeled data from the source domain and unlabeled data from multiple target domains concurrently during training. Collectively, we refer to this data as \"external\". When faced with new unlabeled data from an unseen target domain, these solutions either do not generalize well or require retraining from scratch on all data. To address these challenges, we introduce a new strategy called \"multi-target UDA without external data\" for semantic segmentation. Specifically, the segmentation model is initially trained on the external data. Then, it is adapted to a new unseen target domain without accessing any external data. This approach is thus more scalable than existing solutions and remains applicable when external data is inaccessible. We demonstrate this strategy using a simple method that incorporates self-distillation and adversarial learning, where knowledge acquired from the external data is preserved during adaptation through \"one-way\" adversarial learning. Extensive experiments in several synthetic-to-real and real-to-real adaptation settings on four benchmark urban driving datasets show that our method significantly outperforms current state-of-the-art solutions, even in the absence of external data. Our source code is available online (https://github.com/YonghaoXu/UT-KD).","sentences":["Multi-target unsupervised domain adaptation (UDA) aims to learn a unified model to address the domain shift between multiple target domains.","Due to the difficulty of obtaining annotations for dense predictions, it has recently been introduced into cross-domain semantic segmentation.","However, most existing solutions require labeled data from the source domain and unlabeled data from multiple target domains concurrently during training.","Collectively, we refer to this data as \"external\".","When faced with new unlabeled data from an unseen target domain, these solutions either do not generalize well or require retraining from scratch on all data.","To address these challenges, we introduce a new strategy called \"multi-target UDA without external data\" for semantic segmentation.","Specifically, the segmentation model is initially trained on the external data.","Then, it is adapted to a new unseen target domain without accessing any external data.","This approach is thus more scalable than existing solutions and remains applicable when external data is inaccessible.","We demonstrate this strategy using a simple method that incorporates self-distillation and adversarial learning, where knowledge acquired from the external data is preserved during adaptation through \"one-way\" adversarial learning.","Extensive experiments in several synthetic-to-real and real-to-real adaptation settings on four benchmark urban driving datasets show that our method significantly outperforms current state-of-the-art solutions, even in the absence of external data.","Our source code is available online (https://github.com/YonghaoXu/UT-KD)."],"url":"http://arxiv.org/abs/2405.06502v1","category":"cs.CV"}
{"created":"2024-05-10 14:29:51","title":"Optimal transport of measures via autonomous vector fields","abstract":"We study the problem of transporting one probability measure to another via an autonomous velocity field. We rely on tools from the theory of optimal transport. In one space-dimension, we solve a linear homogeneous functional equation to construct a suitable autonomous vector field that realizes the (unique) monotone transport map as the time-$1$ map of its flow. Generically, this vector field can be chosen to be Lipschitz continuous. We then use Sudakov's disintegration approach to deal with the multi-dimensional case by reducing it to a family of one-dimensional problems.","sentences":["We study the problem of transporting one probability measure to another via an autonomous velocity field.","We rely on tools from the theory of optimal transport.","In one space-dimension, we solve a linear homogeneous functional equation to construct a suitable autonomous vector field that realizes the (unique) monotone transport map as the time-$1$ map of its flow.","Generically, this vector field can be chosen to be Lipschitz continuous.","We then use Sudakov's disintegration approach to deal with the multi-dimensional case by reducing it to a family of one-dimensional problems."],"url":"http://arxiv.org/abs/2405.06503v1","category":"math.OC"}
{"created":"2024-05-10 14:26:46","title":"Testing Strong Gravitational Lensing Effects of Supermassive Black Holes with String-Inspired Metric, EHT Constraints and Parameter Estimation","abstract":"We examine and compare the gravitational lensing, in the strong field limit, for the spherically symmetric string-inspired Euler-Heisenberg black holes, characterized by additional parameters $Q^2$ and $\\alpha-\\beta$, representing magnetic charge and coupling constant, respectively. Our analysis reveals a reduction in the photon sphere radius $x_{ps}$, critical impact parameter $u_{ps}$ and angular position $\\theta_\\infty$ with increasing magnitude of $Q^2$ and $\\alpha-\\beta$. Consequently, the value of these quantities is consistently lower than that of its GR equivalents. Further, the ratio $r_{mag}$ of the flux of the first image to all others decreases with $Q^2$ and $\\alpha-\\beta$. Unlike Schwarzschild black holes, string-inspired Euler-Heisenberg black holes have a smaller deflection angle $\\alpha_D$, which decreases even more as $Q^2$ increases. Moreover, the time delay for Sgr A* and M87* can reach up to $~11.302$ and $~17085.1$ minutes, respectively, at $Q^2=0.1$ and $\\alpha-\\beta=-1$, deviating from Schwarzschild black holes by $~0.194$ and $~293.6$ minutes which are not very significant. For Sgr A* and M87*, we determine $\\theta_\\infty$ to range within $(23.81, 26.28)~\\mu as$ and $(17.89, 19.78)~\\mu as$ respectively, with angular separations $s$ ranging from $(3.33-5.67)~nas$ for Sgr A* and $(2.51-4.26)~nas$ for M87*. EHT bounds on the $\\theta_{sh}$ of Sgr A* and M87* within the $1\\sigma$ region, bound the parameters $Q^2$ and $\\alpha-\\beta$ as: for Sgr A* $0.29278 \\le Q^2 \\le 0.60778$ and for M87* $0 < Q^2 \\le 0.08473$, but in both the cases we found no bound on the parameter $\\alpha-\\beta$. We also estimate the parameters $\\alpha-\\beta$ and $Q^2$ associated with string-inspired Euler-Heisenberg black holes using the EHT observation results of Sgr A* and M87*.","sentences":["We examine and compare the gravitational lensing, in the strong field limit, for the spherically symmetric string-inspired Euler-Heisenberg black holes, characterized by additional parameters $Q^2$ and $\\alpha-\\beta$, representing magnetic charge and coupling constant, respectively.","Our analysis reveals a reduction in the photon sphere radius $x_{ps}$, critical impact parameter $u_{ps}$ and angular position $\\theta_\\infty$ with increasing magnitude of $Q^2$ and $\\alpha-\\beta$.","Consequently, the value of these quantities is consistently lower than that of its GR equivalents.","Further, the ratio $r_{mag}$ of the flux of the first image to all others decreases with $Q^2$ and $\\alpha-\\beta$. Unlike Schwarzschild black holes, string-inspired Euler-Heisenberg black holes have a smaller deflection angle $\\alpha_D$, which decreases even more as $Q^2$ increases.","Moreover, the time delay for Sgr A* and M87* can reach up to $~11.302$ and $~17085.1$ minutes, respectively, at $Q^2=0.1$ and $\\alpha-\\beta=-1$, deviating from Schwarzschild black holes by $~0.194$ and $~293.6$ minutes which are not very significant.","For Sgr A* and M87*, we determine $\\theta_\\infty$ to range within $(23.81, 26.28)~\\mu as$ and $(17.89, 19.78)~\\mu as$ respectively, with angular separations $s$ ranging from $(3.33-5.67)~nas$ for Sgr A* and $(2.51-4.26)~nas$ for M87*.","EHT bounds on the $\\theta_{sh}$ of Sgr A* and M87* within the $1\\sigma$ region, bound the parameters $Q^2$ and $\\alpha-\\beta$ as: for Sgr A* $0.29278 \\le Q^2 \\le 0.60778$ and for M87* $0 <","Q^2 \\le 0.08473$, but in both the cases we found no bound on the parameter $\\alpha-\\beta$.","We also estimate the parameters $\\alpha-\\beta$ and $Q^2$ associated with string-inspired Euler-Heisenberg black holes using the EHT observation results of Sgr A* and M87*."],"url":"http://arxiv.org/abs/2405.06501v1","category":"gr-qc"}
{"created":"2024-05-10 14:23:43","title":"Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based Method for Evaluating Chess Strategies from Textbooks","abstract":"The chess domain is well-suited for creating an artificial intelligence (AI) system that mimics real-world challenges, including decision-making. Throughout the years, minimal attention has been paid to investigating insights derived from unstructured chess data sources. In this study, we examine the complicated relationships between multiple referenced moves in a chess-teaching textbook, and propose a novel method designed to encapsulate chess knowledge derived from move-action phrases. This study investigates the feasibility of using a modified sentiment analysis method as a means for evaluating chess moves based on text. Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents an advancement in evaluating the sentiment associated with referenced chess moves. By extracting insights from move-action phrases, our approach aims to provide a more fine-grained and contextually aware `chess move'-based sentiment classification. Through empirical experiments and analysis, we evaluate the performance of our fine-tuned ABSA model, presenting results that confirm the efficiency of our approach in advancing aspect-based sentiment classification within the chess domain. This research contributes to the area of game-playing by machines and shows the practical applicability of leveraging NLP techniques to understand the context of strategic games.","sentences":["The chess domain is well-suited for creating an artificial intelligence (AI) system that mimics real-world challenges, including decision-making.","Throughout the years, minimal attention has been paid to investigating insights derived from unstructured chess data sources.","In this study, we examine the complicated relationships between multiple referenced moves in a chess-teaching textbook, and propose a novel method designed to encapsulate chess knowledge derived from move-action phrases.","This study investigates the feasibility of using a modified sentiment analysis method as a means for evaluating chess moves based on text.","Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents an advancement in evaluating the sentiment associated with referenced chess moves.","By extracting insights from move-action phrases, our approach aims to provide a more fine-grained and contextually aware `chess move'-based sentiment classification.","Through empirical experiments and analysis, we evaluate the performance of our fine-tuned ABSA model, presenting results that confirm the efficiency of our approach in advancing aspect-based sentiment classification within the chess domain.","This research contributes to the area of game-playing by machines and shows the practical applicability of leveraging NLP techniques to understand the context of strategic games."],"url":"http://arxiv.org/abs/2405.06499v1","category":"cs.CL"}
{"created":"2024-05-10 14:21:08","title":"Implementation Study of Cost-Effective Verification for Pietrzak's Verifiable Delay Function in Ethereum Smart Contracts","abstract":"Verifiable Delay Function (VDF) is a cryptographic concept that ensures a minimum delay before output through sequential processing, which is resistant to parallel computing. Among the two well-known VDF protocols, Wesolowski and Pietrzak VDF, we focus on the Pietrzak VDF due to its computational efficiency and suitability for blockchain environments. Pietrzak's approach uses a recursive proof verification with the halving protocol, offering a practical alternative despite the longer proof length than Wesolowski's approach. Given the scarcity of research on practical VDF verification implementation, especially within smart contracts, this paper aims to implement cost-effective verification for the Pietrzak VDF in an Ethereum-based environment without compromising the VDF verification's integrity and reliability. Firstly, we propose generalized proof generation and verification algorithms for potential efficiency improvement. Secondly, we categorize and measure the gas cost of each part in a transaction for VDF verification. Thirdly, based on the analysis, we theoretically predict the optimized proof construction. Finally, we demonstrate the theoretical prediction matches the implementation results. Furthermore, our research shows that the proof length of the Pietrzak VDF is generated under 8 KB with the security level of 2048 bits, much smaller than the previous expectation. This implies that the Pietrzak VDF can be practically used for cryptographic applications on blockchains.","sentences":["Verifiable Delay Function (VDF) is a cryptographic concept that ensures a minimum delay before output through sequential processing, which is resistant to parallel computing.","Among the two well-known VDF protocols, Wesolowski and Pietrzak VDF, we focus on the Pietrzak VDF due to its computational efficiency and suitability for blockchain environments.","Pietrzak's approach uses a recursive proof verification with the halving protocol, offering a practical alternative despite the longer proof length than Wesolowski's approach.","Given the scarcity of research on practical VDF verification implementation, especially within smart contracts, this paper aims to implement cost-effective verification for the Pietrzak VDF in an Ethereum-based environment without compromising the VDF verification's integrity and reliability.","Firstly, we propose generalized proof generation and verification algorithms for potential efficiency improvement.","Secondly, we categorize and measure the gas cost of each part in a transaction for VDF verification.","Thirdly, based on the analysis, we theoretically predict the optimized proof construction.","Finally, we demonstrate the theoretical prediction matches the implementation results.","Furthermore, our research shows that the proof length of the Pietrzak VDF is generated under 8 KB with the security level of 2048 bits, much smaller than the previous expectation.","This implies that the Pietrzak VDF can be practically used for cryptographic applications on blockchains."],"url":"http://arxiv.org/abs/2405.06498v1","category":"cs.CR"}
{"created":"2024-05-10 14:19:15","title":"Storypark: Leveraging Large Language Models to Enhance Children Story Learning Through Child-AI collaboration Storytelling","abstract":"Interactive storytelling has been widely adopted by educators in teaching activities of young children. Such a teaching method combines storytelling with active child participation, benefiting their expressive abilities, creative thinking, and understanding of stories. Interactive storytelling requires facilitators to unidirectionally narrate the story content and encourage children's participation in story plot creation and interpretation of central themes through multi-sensory interactive methods such as questioning and drawing. However, providing tailored guidance based on diverse feedback from children during interactive storytelling poses challenges for most facilitators. These challenges include expanding story plot development based on children's ideas, using drawings to visualize children's thoughts, and interpreting the story's central themes based on children's thinking. This necessitates facilitators to possess strong imaginative, associative, domain knowledge, and drawing skills. Large language models have demonstrated their potential in facilitating responsive and participatory dialogues, offering new design possibilities to address the challenges faced by facilitators in interactive storytelling. In this study, our goal is to leverage large language models to design an interactive storytelling system that provides children with plot frameworks and interpretations of central themes during the interactive storytelling process. Through user experiments involving 20 child participants, we evaluate this interactive system's usability, learning effectiveness, and user experience. The user study shows that Storypark improves learning outcomes in understanding story key ideas, generalization, and transfer. And high engagement and willingness to use of participants demonstrate that StoryPark provides children with a positive learning experience.","sentences":["Interactive storytelling has been widely adopted by educators in teaching activities of young children.","Such a teaching method combines storytelling with active child participation, benefiting their expressive abilities, creative thinking, and understanding of stories.","Interactive storytelling requires facilitators to unidirectionally narrate the story content and encourage children's participation in story plot creation and interpretation of central themes through multi-sensory interactive methods such as questioning and drawing.","However, providing tailored guidance based on diverse feedback from children during interactive storytelling poses challenges for most facilitators.","These challenges include expanding story plot development based on children's ideas, using drawings to visualize children's thoughts, and interpreting the story's central themes based on children's thinking.","This necessitates facilitators to possess strong imaginative, associative, domain knowledge, and drawing skills.","Large language models have demonstrated their potential in facilitating responsive and participatory dialogues, offering new design possibilities to address the challenges faced by facilitators in interactive storytelling.","In this study, our goal is to leverage large language models to design an interactive storytelling system that provides children with plot frameworks and interpretations of central themes during the interactive storytelling process.","Through user experiments involving 20 child participants, we evaluate this interactive system's usability, learning effectiveness, and user experience.","The user study shows that Storypark improves learning outcomes in understanding story key ideas, generalization, and transfer.","And high engagement and willingness to use of participants demonstrate that StoryPark provides children with a positive learning experience."],"url":"http://arxiv.org/abs/2405.06495v1","category":"cs.HC"}
{"created":"2024-05-10 14:07:58","title":"Improving Deep Learning Model Calibration for Cardiac Applications using Deterministic Uncertainty Networks and Uncertainty-aware Training","abstract":"Improving calibration performance in deep learning (DL) classification models is important when planning the use of DL in a decision-support setting. In such a scenario, a confident wrong prediction could lead to a lack of trust and/or harm in a high-risk application. We evaluate the impact on accuracy and calibration of two types of approach that aim to improve DL classification model calibration: deterministic uncertainty methods (DUM) and uncertainty-aware training. Specifically, we test the performance of three DUMs and two uncertainty-aware training approaches as well as their combinations. To evaluate their utility, we use two realistic clinical applications from the field of cardiac imaging: artefact detection from phase contrast cardiac magnetic resonance (CMR) and disease diagnosis from the public ACDC CMR dataset. Our results indicate that both DUMs and uncertainty-aware training can improve both accuracy and calibration in both of our applications, with DUMs generally offering the best improvements. We also investigate the combination of the two approaches, resulting in a novel deterministic uncertainty-aware training approach. This provides further improvements for some combinations of DUMs and uncertainty-aware training approaches.","sentences":["Improving calibration performance in deep learning (DL) classification models is important when planning the use of DL in a decision-support setting.","In such a scenario, a confident wrong prediction could lead to a lack of trust and/or harm in a high-risk application.","We evaluate the impact on accuracy and calibration of two types of approach that aim to improve DL classification model calibration: deterministic uncertainty methods (DUM) and uncertainty-aware training.","Specifically, we test the performance of three DUMs and two uncertainty-aware training approaches as well as their combinations.","To evaluate their utility, we use two realistic clinical applications from the field of cardiac imaging: artefact detection from phase contrast cardiac magnetic resonance (CMR) and disease diagnosis from the public ACDC CMR dataset.","Our results indicate that both DUMs and uncertainty-aware training can improve both accuracy and calibration in both of our applications, with DUMs generally offering the best improvements.","We also investigate the combination of the two approaches, resulting in a novel deterministic uncertainty-aware training approach.","This provides further improvements for some combinations of DUMs and uncertainty-aware training approaches."],"url":"http://arxiv.org/abs/2405.06487v1","category":"cs.LG"}
{"created":"2024-05-10 14:07:29","title":"Solving Quantified Boolean Formulas with Few Existential Variables","abstract":"The quantified Boolean formula (QBF) problem is an important decision problem generally viewed as the archetype for PSPACE-completeness. Many problems of central interest in AI are in general not included in NP, e.g., planning, model checking, and non-monotonic reasoning, and for such problems QBF has successfully been used as a modelling tool. However, solvers for QBF are not as advanced as state of the art SAT solvers, which has prevented QBF from becoming a universal modelling language for PSPACE-complete problems. A theoretical explanation is that QBF (as well as many other PSPACE-complete problems) lacks natural parameters} guaranteeing fixed-parameter tractability (FPT).   In this paper we tackle this problem and consider a simple but overlooked parameter: the number of existentially quantified variables. This natural parameter is virtually unexplored in the literature which one might find surprising given the general scarcity of FPT algorithms for QBF. Via this parameterization we then develop a novel FPT algorithm applicable to QBF instances in conjunctive normal form (CNF) of bounded clause length. We complement this by a W[1]-hardness result for QBF in CNF of unbounded clause length as well as sharper lower bounds for the bounded arity case under the (strong) exponential-time hypothesis.","sentences":["The quantified Boolean formula (QBF) problem is an important decision problem generally viewed as the archetype for PSPACE-completeness.","Many problems of central interest in AI are in general not included in NP, e.g., planning, model checking, and non-monotonic reasoning, and for such problems QBF has successfully been used as a modelling tool.","However, solvers for QBF are not as advanced as state of the art SAT solvers, which has prevented QBF from becoming a universal modelling language for PSPACE-complete problems.","A theoretical explanation is that QBF (as well as many other PSPACE-complete problems) lacks natural parameters} guaranteeing fixed-parameter tractability (FPT).   ","In this paper we tackle this problem and consider a simple but overlooked parameter: the number of existentially quantified variables.","This natural parameter is virtually unexplored in the literature which one might find surprising given the general scarcity of FPT algorithms for QBF.","Via this parameterization we then develop a novel FPT algorithm applicable to QBF instances in conjunctive normal form (CNF) of bounded clause length.","We complement this by a W[1]-hardness result for QBF in CNF of unbounded clause length as well as sharper lower bounds for the bounded arity case under the (strong) exponential-time hypothesis."],"url":"http://arxiv.org/abs/2405.06485v1","category":"cs.CC"}
{"created":"2024-05-10 14:03:37","title":"LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing","abstract":"This paper describes our participation in SemEval 2024 Task 3, which focused on Multimodal Emotion Cause Analysis in Conversations. We developed an early prototype for an end-to-end system that uses graph-based methods from dependency parsing to identify causal emotion relations in multi-party conversations. Our model comprises a neural transformer-based encoder for contextualizing multimodal conversation data and a graph-based decoder for generating the adjacency matrix scores of the causal graph. We ranked 7th out of 15 valid and official submissions for Subtask 1, using textual inputs only. We also discuss our participation in Subtask 2 during post-evaluation using multi-modal inputs.","sentences":["This paper describes our participation in SemEval 2024 Task 3, which focused on Multimodal Emotion Cause Analysis in Conversations.","We developed an early prototype for an end-to-end system that uses graph-based methods from dependency parsing to identify causal emotion relations in multi-party conversations.","Our model comprises a neural transformer-based encoder for contextualizing multimodal conversation data and a graph-based decoder for generating the adjacency matrix scores of the causal graph.","We ranked 7th out of 15 valid and official submissions for Subtask 1, using textual inputs only.","We also discuss our participation in Subtask 2 during post-evaluation using multi-modal inputs."],"url":"http://arxiv.org/abs/2405.06483v1","category":"cs.CL"}
{"created":"2024-05-10 14:00:41","title":"On RadCom channel capacity for V2V applications","abstract":"The use of millimiter wave (mmWave) for communication and sensing purposes is one of the functions powered by Next Generation Vehicle-to-Anything (V2X) networks. The arrival of IEEE~802.11bd, which is able to operate in the 60 GHz band, opens the doors of Integrated Sensing and Communications (ISAC) to vehicular networks. Similarly, Radar-based Communications (RadCom) proposes the use of the radar spectrum for communication puproses. In this paper, we perform an analysis of the channel capacity for different configurations of RadCom, showing its potential to offload the V2X spectrum for bumper-to-bumper V2X applications. We finalize with a discussion on the potential for ISAC from both the 802.11bd and RadCom approaches.","sentences":["The use of millimiter wave (mmWave) for communication and sensing purposes is one of the functions powered by Next Generation Vehicle-to-Anything (V2X) networks.","The arrival of IEEE~802.11bd, which is able to operate in the 60 GHz band, opens the doors of Integrated Sensing and Communications (ISAC) to vehicular networks.","Similarly, Radar-based Communications (RadCom) proposes the use of the radar spectrum for communication puproses.","In this paper, we perform an analysis of the channel capacity for different configurations of RadCom, showing its potential to offload the V2X spectrum for bumper-to-bumper V2X applications.","We finalize with a discussion on the potential for ISAC from both the 802.11bd and RadCom approaches."],"url":"http://arxiv.org/abs/2405.06482v1","category":"cs.IT"}
{"created":"2024-05-10 13:53:46","title":"Attention is all they need: Cognitive science and the (techno)political economy of attention in humans and machines","abstract":"This paper critically analyses the \"attention economy\" within the framework of cognitive science and techno-political economics, as applied to both human and machine interactions. We explore how current business models, particularly in digital platform capitalism, harness user engagement by strategically shaping attentional patterns. These platforms utilize advanced AI and massive data analytics to enhance user engagement, creating a cycle of attention capture and data extraction. We review contemporary (neuro)cognitive theories of attention and platform engagement design techniques and criticize classical cognitivist and behaviourist theories for their inadequacies in addressing the potential harms of such engagement on user autonomy and wellbeing. 4E approaches to cognitive science, instead, emphasizing the embodied, extended, enactive, and ecological aspects of cognition, offer us an intrinsic normative standpoint and a more integrated understanding of how attentional patterns are actively constituted by adaptive digital environments. By examining the precarious nature of habit formation in digital contexts, we reveal the techno-economic underpinnings that threaten personal autonomy by disaggregating habits away from the individual, into an AI managed collection of behavioural patterns. Our current predicament suggests the necessity of a paradigm shift towards an ecology of attention. This shift aims to foster environments that respect and preserve human cognitive and social capacities, countering the exploitative tendencies of cognitive capitalism.","sentences":["This paper critically analyses the \"attention economy\" within the framework of cognitive science and techno-political economics, as applied to both human and machine interactions.","We explore how current business models, particularly in digital platform capitalism, harness user engagement by strategically shaping attentional patterns.","These platforms utilize advanced AI and massive data analytics to enhance user engagement, creating a cycle of attention capture and data extraction.","We review contemporary (neuro)cognitive theories of attention and platform engagement design techniques and criticize classical cognitivist and behaviourist theories for their inadequacies in addressing the potential harms of such engagement on user autonomy and wellbeing.","4E approaches to cognitive science, instead, emphasizing the embodied, extended, enactive, and ecological aspects of cognition, offer us an intrinsic normative standpoint and a more integrated understanding of how attentional patterns are actively constituted by adaptive digital environments.","By examining the precarious nature of habit formation in digital contexts, we reveal the techno-economic underpinnings that threaten personal autonomy by disaggregating habits away from the individual, into an AI managed collection of behavioural patterns.","Our current predicament suggests the necessity of a paradigm shift towards an ecology of attention.","This shift aims to foster environments that respect and preserve human cognitive and social capacities, countering the exploitative tendencies of cognitive capitalism."],"url":"http://arxiv.org/abs/2405.06478v1","category":"cs.CY"}
{"created":"2024-05-10 13:45:54","title":"Is the panel fair? Evaluating panel compositions through network analysis. The case of research assessments in Italy","abstract":"In research evaluation, the fair representation of panels is usually defined in terms of observable characteristics of scholars such as gender or affiliations. An an empirical strategy is proposed for exploring hidden connections between panellists such that, despite the respect of formal requirements, the panel could be considered alike as unfair with respect to the representation of diversity of research approaches and methodologies. The case study regards the three panels selected to evaluate research in economics, statistics and business during the Italian research assessment exercises. The first two panels were appointed directly by the governmental agency responsible for the evaluation, while the third was randomly selected. Hence the third panel can be considered as a control for evaluating about the fairness of the others. The fair representation is explored by comparing the networks of panellists based on their co-authorship relations, the networks based on journals in which they published and the networks based on their affiliated institutions (universities, research centres and newspapers). The results show that the members of the first two panels had connections much higher than the members of the control group. Hence the composition of the first two panels should be considered as unfair, as the results of the research assessments.","sentences":["In research evaluation, the fair representation of panels is usually defined in terms of observable characteristics of scholars such as gender or affiliations.","An an empirical strategy is proposed for exploring hidden connections between panellists such that, despite the respect of formal requirements, the panel could be considered alike as unfair with respect to the representation of diversity of research approaches and methodologies.","The case study regards the three panels selected to evaluate research in economics, statistics and business during the Italian research assessment exercises.","The first two panels were appointed directly by the governmental agency responsible for the evaluation, while the third was randomly selected.","Hence the third panel can be considered as a control for evaluating about the fairness of the others.","The fair representation is explored by comparing the networks of panellists based on their co-authorship relations, the networks based on journals in which they published and the networks based on their affiliated institutions (universities, research centres and newspapers).","The results show that the members of the first two panels had connections much higher than the members of the control group.","Hence the composition of the first two panels should be considered as unfair, as the results of the research assessments."],"url":"http://arxiv.org/abs/2405.06476v1","category":"econ.GN"}
{"created":"2024-05-10 13:44:36","title":"The Fyodorov-Hiary-Keating Conjecture on Mesoscopic Intervals","abstract":"We derive precise upper bounds for the maximum of the Riemann zeta function on short intervals on the critical line, showing for any $\\theta\\in(-1,0]$, the set of $t\\in [T,2T]$ for which $$\\max_{|h|\\leq \\log^\\theta T}|\\zeta(\\tfrac{1}{2}+it+ih)|>\\exp\\bigg({y+\\sqrt{(\\log\\log T)|\\theta|/2}\\cdot {S}}\\bigg)\\frac{(\\log T)^{(1+\\theta)}}{(\\log\\log T)^{3/4}}$$ is bounded above by $Cy\\exp({-2y-y^2/((1+\\theta)\\log\\log T)})$ (where $S$ is a random variable that is approximately a standard Gaussian as $T$ tends to infinity). This settles a strong form of a conjecture of Fyodorov--Hiary--Keating in mesoscopic intervals which was only known in the leading order. Using similar techniques, we also derive upper bounds for the second moment of the zeta function on such intervals. Conditioning on the value of $S$, we show that for all $t\\in[T,2T]$ outside a set of order $o(T)$, $$\\frac{1}{\\log^\\theta T}\\int_{|h|\\in \\log^\\theta T} |\\zeta(\\tfrac{1}{2}+it+ih)|^2\\mathrm{d}h \\ll e^{2S}\\cdot \\left(\\frac{(\\log T)^{(1+\\theta)}}{\\sqrt{\\log\\log T}}\\right).$$ This proves a weak form of another conjecture of Fyodorov-Keating and generalizes a result of Harper, which is recovered at $\\theta = 0$ (in which case $S$ is defined to be zero). Our main tool is an adaptation of the recursive scheme introduced by one of the authors, Bourgade and Radziwi{\\l}{\\l} to mesoscopic intervals.","sentences":["We derive precise upper bounds for the maximum of the Riemann zeta function on short intervals on the critical line, showing for any $\\theta\\in(-1,0]$, the set of $t\\in [T,2T]$ for which $$\\max_{|h|\\leq \\log^\\theta T}|\\zeta(\\tfrac{1}{2}+it+ih)|>\\exp\\bigg({y+\\sqrt{(\\log\\log T)|\\theta|/2}\\cdot {S}}\\bigg)\\frac{(\\log T)^{(1+\\theta)}}{(\\log\\log T)^{3/4}}$$ is bounded above by $Cy\\exp({-2y-y^2/((1+\\theta)\\log\\log T)})$ (where $S$ is a random variable that is approximately a standard Gaussian as $T$ tends to infinity).","This settles a strong form of a conjecture of Fyodorov--Hiary--Keating in mesoscopic intervals which was only known in the leading order.","Using similar techniques, we also derive upper bounds for the second moment of the zeta function on such intervals.","Conditioning on the value of $S$, we show that for all $t\\in[T,2T]$ outside a set of order $o(T)$, $$\\frac{1}{\\log^\\theta T}\\int_{|h|\\in \\log^\\theta T} |\\zeta(\\tfrac{1}{2}+it+ih)|^2\\mathrm{d}h \\ll e^{2S}\\cdot \\left(\\frac{(\\log T)^{(1+\\theta)}}{\\sqrt{\\log\\log T}}\\right).$$","This proves a weak form of another conjecture of Fyodorov-Keating and generalizes a result of Harper, which is recovered at $\\theta = 0$ (in which case $S$ is defined to be zero).","Our main tool is an adaptation of the recursive scheme introduced by one of the authors, Bourgade and Radziwi{\\l}{\\l} to mesoscopic intervals."],"url":"http://arxiv.org/abs/2405.06474v1","category":"math.NT"}
{"created":"2024-05-10 13:36:16","title":"Renormalization Flow of Nonlinear Electrodynamics","abstract":"We study the renormalization flow of generic actions that depend on the invariants of the field strength tensor of an abelian gauge field. While the Maxwell action defines a Gaussian fixed point, we search for further non-Gaussian fixed points or rather fixed functions, i.e., globally existing Lagrangians of the invariants. Using standard small-field expansion techniques for the resulting functional flow equation, a large number of fixed points is obtained, which - in analogy to recent findings for a shift-symmetric scalar field - we consider as approximation artifacts. For the construction of a globally existing fixed function, we pay attention to the use of proper initial conditions. Parametrizing the latter by the photon anomalous dimension, both the coefficients of the weak-field expansion are fully determined and those of the large-field expansion can be matched such that a global fixed function can be constructed for magnetic fields. The anomalous dimension also governs the strong-field limit. Our results provide evidence for the existence of a continuum of non-Gaussian fixed points parametrized by a small positive anomalous dimension below a critical value. We discuss the implications of this result within various scenarios with and without additional matter. For the strong-field limit of the 1PI QED effective action, where the anomalous dimension is determined by electronic fluctuations, our result suggests the existence of a singularity free strong-field limit, circumventing the standard conclusions connected to the perturbative Landau pole.","sentences":["We study the renormalization flow of generic actions that depend on the invariants of the field strength tensor of an abelian gauge field.","While the Maxwell action defines a Gaussian fixed point, we search for further non-Gaussian fixed points or rather fixed functions, i.e., globally existing Lagrangians of the invariants.","Using standard small-field expansion techniques for the resulting functional flow equation, a large number of fixed points is obtained, which - in analogy to recent findings for a shift-symmetric scalar field - we consider as approximation artifacts.","For the construction of a globally existing fixed function, we pay attention to the use of proper initial conditions.","Parametrizing the latter by the photon anomalous dimension, both the coefficients of the weak-field expansion are fully determined and those of the large-field expansion can be matched such that a global fixed function can be constructed for magnetic fields.","The anomalous dimension also governs the strong-field limit.","Our results provide evidence for the existence of a continuum of non-Gaussian fixed points parametrized by a small positive anomalous dimension below a critical value.","We discuss the implications of this result within various scenarios with and without additional matter.","For the strong-field limit of the 1PI QED effective action, where the anomalous dimension is determined by electronic fluctuations, our result suggests the existence of a singularity free strong-field limit, circumventing the standard conclusions connected to the perturbative Landau pole."],"url":"http://arxiv.org/abs/2405.06472v1","category":"hep-th"}
{"created":"2024-05-10 13:30:52","title":"Solar fusion III: New data and theory for hydrogen-burning stars","abstract":"In stars that lie on the main sequence in the Hertzsprung Russel diagram, like our sun, hydrogen is fused to helium in a number of nuclear reaction chains and series, such as the proton-proton chain and the carbon-nitrogen-oxygen cycles. Precisely determined thermonuclear rates of these reactions lie at the foundation of the standard solar model. This review, the third decadal evaluation of the nuclear physics of hydrogen-burning stars, is motivated by the great advances made in recent years by solar neutrino observatories, putting experimental knowledge of the proton-proton chain neutrino fluxes in the few-percent precision range. The basis of the review is a one-week community meeting held in July 2022 in Berkeley, California, and many subsequent digital meetings and exchanges. Each of the relevant reactions of solar and quiescent stellar hydrogen burning is reviewed here, from both theoretical and experimental perspectives. Recommendations for the state of the art of the astrophysical S-factor and its uncertainty are formulated for each of them. Several other topics of paramount importance for the solar model are reviewed, as well: recent and future neutrino experiments, electron screening, radiative opacities, and current and upcoming experimental facilities. In addition to reaction-specific recommendations, also general recommendations are formed.","sentences":["In stars that lie on the main sequence in the Hertzsprung Russel diagram, like our sun, hydrogen is fused to helium in a number of nuclear reaction chains and series, such as the proton-proton chain and the carbon-nitrogen-oxygen cycles.","Precisely determined thermonuclear rates of these reactions lie at the foundation of the standard solar model.","This review, the third decadal evaluation of the nuclear physics of hydrogen-burning stars, is motivated by the great advances made in recent years by solar neutrino observatories, putting experimental knowledge of the proton-proton chain neutrino fluxes in the few-percent precision range.","The basis of the review is a one-week community meeting held in July 2022 in Berkeley, California, and many subsequent digital meetings and exchanges.","Each of the relevant reactions of solar and quiescent stellar hydrogen burning is reviewed here, from both theoretical and experimental perspectives.","Recommendations for the state of the art of the astrophysical S-factor and its uncertainty are formulated for each of them.","Several other topics of paramount importance for the solar model are reviewed, as well: recent and future neutrino experiments, electron screening, radiative opacities, and current and upcoming experimental facilities.","In addition to reaction-specific recommendations, also general recommendations are formed."],"url":"http://arxiv.org/abs/2405.06470v1","category":"astro-ph.SR"}
{"created":"2024-05-10 13:30:05","title":"Model-Based Adaptive Control of Modular Multilevel Converters","abstract":"Electrical power conversions are common in a large variety of engineering applications. With reference to AC/DC and DC/AC power conversions, a strong research interest resides in multilevel converters, thanks to the many advantages they provide over standard two-level converters. In this paper, we first provide a power-oriented model of Modular Multilevel Converters (MMCs), followed by a detailed harmonic analysis. The model is given in the form of a block scheme that can be directly implemented in the Matlab/Simulink environment. The performed harmonic analysis gives a deep and exact understanding of the different terms affecting the evolution of the voltage trajectories in the upper and lower arms of the converter. Next, we propose a new model-based adaptive control scheme for MMCs. The proposed control allows to determine the optimal average capacitor voltages reference in real-time, thus allowing to properly track the desired load current while minimizing the harmonic content in the generated load current itself.","sentences":["Electrical power conversions are common in a large variety of engineering applications.","With reference to AC/DC and DC/AC power conversions, a strong research interest resides in multilevel converters, thanks to the many advantages they provide over standard two-level converters.","In this paper, we first provide a power-oriented model of Modular Multilevel Converters (MMCs), followed by a detailed harmonic analysis.","The model is given in the form of a block scheme that can be directly implemented in the Matlab/Simulink environment.","The performed harmonic analysis gives a deep and exact understanding of the different terms affecting the evolution of the voltage trajectories in the upper and lower arms of the converter.","Next, we propose a new model-based adaptive control scheme for MMCs.","The proposed control allows to determine the optimal average capacitor voltages reference in real-time, thus allowing to properly track the desired load current while minimizing the harmonic content in the generated load current itself."],"url":"http://arxiv.org/abs/2405.06469v1","category":"eess.SY"}
{"created":"2024-05-10 13:27:32","title":"Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification","abstract":"The task of medical image recognition is notably complicated by the presence of varied and multiple pathological indications, presenting a unique challenge in multi-label classification with unseen labels. This complexity underlines the need for computer-aided diagnosis methods employing multi-label zero-shot learning. Recent advancements in pre-trained vision-language models (VLMs) have showcased notable zero-shot classification abilities on medical images. However, these methods have limitations on leveraging extensive pre-trained knowledge from broader image datasets, and often depend on manual prompt construction by expert radiologists. By automating the process of prompt tuning, prompt learning techniques have emerged as an efficient way to adapt VLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in performing class-specific prompts on unseen categories, limiting generalizability in fine-grained scenarios. To overcome these constraints, we introduce a novel prompt generation approach inspirited by text generation in natural language processing (NLP). Our method, named Pseudo-Prompt Generating (PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring a RNN-based decoder, PsPG autoregressively generates class-tailored embedding vectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label chest radiograph datasets affirm the superiority of our approach against leading medical vision-language and multi-label prompt learning methods. The source code is available at https://github.com/fallingnight/PsPG","sentences":["The task of medical image recognition is notably complicated by the presence of varied and multiple pathological indications, presenting a unique challenge in multi-label classification with unseen labels.","This complexity underlines the need for computer-aided diagnosis methods employing multi-label zero-shot learning.","Recent advancements in pre-trained vision-language models (VLMs) have showcased notable zero-shot classification abilities on medical images.","However, these methods have limitations on leveraging extensive pre-trained knowledge from broader image datasets, and often depend on manual prompt construction by expert radiologists.","By automating the process of prompt tuning, prompt learning techniques have emerged as an efficient way to adapt VLMs to downstream tasks.","Yet, existing CoOp-based strategies fall short in performing class-specific prompts on unseen categories, limiting generalizability in fine-grained scenarios.","To overcome these constraints, we introduce a novel prompt generation approach inspirited by text generation in natural language processing (NLP).","Our method, named Pseudo-Prompt Generating (PsPG), capitalizes on the priori knowledge of multi-modal features.","Featuring a RNN-based decoder, PsPG autoregressively generates class-tailored embedding vectors, i.e., pseudo-prompts.","Comparative evaluations on various multi-label chest radiograph datasets affirm the superiority of our approach against leading medical vision-language and multi-label prompt learning methods.","The source code is available at https://github.com/fallingnight/PsPG"],"url":"http://arxiv.org/abs/2405.06468v1","category":"cs.CV"}
{"created":"2024-05-10 13:16:23","title":"Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers","abstract":"Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\\int_s^t W_r \\, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax).   Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBT's time complexity is logarithmic in the tolerance parameter $\\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\\varepsilon$ apart.   We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.","sentences":["Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs).","To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically.","However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\\int_s^t W_r \\, dr$.","With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments.","A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax).   ","Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation.","Based on binary search, the VBT's time complexity is logarithmic in the tolerance parameter $\\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\\varepsilon$ apart.   ","We present two applications of adaptive high order solvers enabled by our new VBT.","Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping.","We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations."],"url":"http://arxiv.org/abs/2405.06464v1","category":"math.NA"}
{"created":"2024-05-10 13:13:46","title":"SketchDream: Sketch-based Text-to-3D Generation and Editing","abstract":"Existing text-based 3D generation methods generate attractive results but lack detailed geometry control. Sketches, known for their conciseness and expressiveness, have contributed to intuitive 3D modeling but are confined to producing texture-less mesh models within predefined categories. Integrating sketch and text simultaneously for 3D generation promises enhanced control over geometry and appearance but faces challenges from 2D-to-3D translation ambiguity and multi-modal condition integration. Moreover, further editing of 3D models in arbitrary views will give users more freedom to customize their models. However, it is difficult to achieve high generation quality, preserve unedited regions, and manage proper interactions between shape components. To solve the above issues, we propose a text-driven 3D content generation and editing method, SketchDream, which supports NeRF generation from given hand-drawn sketches and achieves free-view sketch-based local editing. To tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view image generation diffusion model, which leverages depth guidance to establish spatial correspondence. A 3D ControlNet with a 3D attention module is utilized to control multi-view images and ensure their 3D consistency. To support local editing, we further propose a coarse-to-fine editing approach: the coarse phase analyzes component interactions and provides 3D masks to label edited regions, while the fine stage generates realistic results with refined details by local enhancement. Extensive experiments validate that our method generates higher-quality results compared with a combination of 2D ControlNet and image-to-3D generation techniques and achieves detailed control compared with existing diffusion-based 3D editing approaches.","sentences":["Existing text-based 3D generation methods generate attractive results but lack detailed geometry control.","Sketches, known for their conciseness and expressiveness, have contributed to intuitive 3D modeling but are confined to producing texture-less mesh models within predefined categories.","Integrating sketch and text simultaneously for 3D generation promises enhanced control over geometry and appearance but faces challenges from 2D-to-3D translation ambiguity and multi-modal condition integration.","Moreover, further editing of 3D models in arbitrary views will give users more freedom to customize their models.","However, it is difficult to achieve high generation quality, preserve unedited regions, and manage proper interactions between shape components.","To solve the above issues, we propose a text-driven 3D content generation and editing method, SketchDream, which supports NeRF generation from given hand-drawn sketches and achieves free-view sketch-based local editing.","To tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view image generation diffusion model, which leverages depth guidance to establish spatial correspondence.","A 3D ControlNet with a 3D attention module is utilized to control multi-view images and ensure their 3D consistency.","To support local editing, we further propose a coarse-to-fine editing approach: the coarse phase analyzes component interactions and provides 3D masks to label edited regions, while the fine stage generates realistic results with refined details by local enhancement.","Extensive experiments validate that our method generates higher-quality results compared with a combination of 2D ControlNet and image-to-3D generation techniques and achieves detailed control compared with existing diffusion-based 3D editing approaches."],"url":"http://arxiv.org/abs/2405.06461v1","category":"cs.GR"}
{"created":"2024-05-10 13:10:55","title":"Are EEG-to-Text Models Working?","abstract":"This work critically analyzes existing models for open-vocabulary EEG-to-Text translation. We identify a crucial limitation: previous studies often employed implicit teacher-forcing during evaluation, artificially inflating performance metrics. Additionally, they lacked a critical benchmark - comparing model performance on pure noise inputs. We propose a methodology to differentiate between models that truly learn from EEG signals and those that simply memorize training data. Our analysis reveals that model performance on noise data can be comparable to that on EEG data. These findings highlight the need for stricter evaluation practices in EEG-to-Text research, emphasizing transparent reporting and rigorous benchmarking with noise inputs. This approach will lead to more reliable assessments of model capabilities and pave the way for robust EEG-to-Text communication systems.","sentences":["This work critically analyzes existing models for open-vocabulary EEG-to-Text translation.","We identify a crucial limitation: previous studies often employed implicit teacher-forcing during evaluation, artificially inflating performance metrics.","Additionally, they lacked a critical benchmark - comparing model performance on pure noise inputs.","We propose a methodology to differentiate between models that truly learn from EEG signals and those that simply memorize training data.","Our analysis reveals that model performance on noise data can be comparable to that on EEG data.","These findings highlight the need for stricter evaluation practices in EEG-to-Text research, emphasizing transparent reporting and rigorous benchmarking with noise inputs.","This approach will lead to more reliable assessments of model capabilities and pave the way for robust EEG-to-Text communication systems."],"url":"http://arxiv.org/abs/2405.06459v1","category":"cs.CL"}
{"created":"2024-05-10 13:04:21","title":"E2TP: Element to Tuple Prompting Improves Aspect Sentiment Tuple Prediction","abstract":"Generative approaches have significantly influenced Aspect-Based Sentiment Analysis (ABSA), garnering considerable attention. However, existing studies often predict target text components monolithically, neglecting the benefits of utilizing single elements for tuple prediction. In this paper, we introduce Element to Tuple Prompting (E2TP), employing a two-step architecture. The former step focuses on predicting single elements, while the latter step completes the process by mapping these predicted elements to their corresponding tuples. E2TP is inspired by human problem-solving, breaking down tasks into manageable parts, using the first step's output as a guide in the second step. Within this strategy, three types of paradigms, namely E2TP($diet$), E2TP($f_1$), and E2TP($f_2$), are designed to facilitate the training process. Beyond in-domain task-specific experiments, our paper addresses cross-domain scenarios, demonstrating the effectiveness and generalizability of the approach. By conducting a comprehensive analysis on various benchmarks, we show that E2TP achieves new state-of-the-art results in nearly all cases.","sentences":["Generative approaches have significantly influenced Aspect-Based Sentiment Analysis (ABSA), garnering considerable attention.","However, existing studies often predict target text components monolithically, neglecting the benefits of utilizing single elements for tuple prediction.","In this paper, we introduce Element to Tuple Prompting (E2TP), employing a two-step architecture.","The former step focuses on predicting single elements, while the latter step completes the process by mapping these predicted elements to their corresponding tuples.","E2TP is inspired by human problem-solving, breaking down tasks into manageable parts, using the first step's output as a guide in the second step.","Within this strategy, three types of paradigms, namely E2TP($diet$), E2TP($f_1$), and E2TP($f_2$), are designed to facilitate the training process.","Beyond in-domain task-specific experiments, our paper addresses cross-domain scenarios, demonstrating the effectiveness and generalizability of the approach.","By conducting a comprehensive analysis on various benchmarks, we show that E2TP achieves new state-of-the-art results in nearly all cases."],"url":"http://arxiv.org/abs/2405.06454v1","category":"cs.CL"}
{"created":"2024-05-10 13:02:32","title":"Searching for linear structures in the failure of the Stone-Weierstrass theorem","abstract":"We analyze the existence of vector spaces of large dimension inside the set $\\mathcal{C}(L, \\K) \\setminus \\overline{\\mathcal{A}}$, where $L$ is a compact Hausdorff space and $\\mathcal{A}$ is a self-adjoint subalgebra of $\\mathcal C(L, \\K)$ that vanishes nowhere on $L$ and does not necessarily separate the points of $L$. The results depend strongly on an equivalence relation that is defined on the algebra $\\mathcal{A}$, denoted by $\\sim_{\\mathcal{A}}$, and a cardinal number that depends on $\\sim_{\\mathcal{A}}$ which we call the order of $\\sim_{\\mathcal{A}}$. We then introduce two different cases, when the order of $\\sim_{\\mathcal{A}}$ is finite or infinite. In the finite case, we show that $\\mathcal{C}(L, \\K) \\setminus \\overline{\\mathcal{A}}$ is $n$-lineable but not $(n+1)$-lineable with $n$ being the order of $\\sim_{\\mathcal{A}}$. On the other hand, when the order of $\\sim_{\\mathcal{A}}$ is infinite, we obtain general results assuming, for instance, that the codimension of the closure of $\\mathcal{A}$ is infinite or when $L$ is sequentially compact. To be more precise, we introduce the notion of the Stone-Weiestrass character of $L$ which is closely related to the topological weight of $L$ and allows us to describe the lineability of $\\mathcal{C}(L, \\K) \\setminus \\overline{\\mathcal{A}}$ in terms of the Stone-Weierstrass character of subsets of $\\sim_{\\mathcal A}$. We also prove, in the classical case, that $(\\mathcal{C}(\\partial{D}, \\C) \\setminus \\overline{\\mbox{Pol}(\\partial{D})}) \\cup \\{0\\}$ (where $\\mbox{Pol}(\\partial{D})$ is the set of all complex polynomials in one variable restricted to the boundary of the unit disk) contains an isometric copy of $\\text{Hol}(\\partial{D})$ and is strongly $\\mathfrak c$-algebrable, extending previous results from the literature.","sentences":["We analyze the existence of vector spaces of large dimension inside the set $\\mathcal{C}(L, \\K) \\setminus \\overline{\\mathcal{A}}$, where $L$ is a compact Hausdorff space and $\\mathcal{A}$ is a self-adjoint subalgebra of $\\mathcal C(L, \\K)$ that vanishes nowhere on $L$ and does not necessarily separate the points of $L$. The results depend strongly on an equivalence relation that is defined on the algebra $\\mathcal{A}$, denoted by $\\sim_{\\mathcal{A}}$, and a cardinal number that depends on $\\sim_{\\mathcal{A}}$ which we call the order of $\\sim_{\\mathcal{A}}$. We then introduce two different cases, when the order of $\\sim_{\\mathcal{A}}$ is finite or infinite.","In the finite case, we show that $\\mathcal{C}(L, \\K) \\setminus \\overline{\\mathcal{A}}$ is $n$-lineable but not $(n+1)$-lineable with $n$ being the order of $\\sim_{\\mathcal{A}}$. On the other hand, when the order of $\\sim_{\\mathcal{A}}$ is infinite, we obtain general results assuming, for instance, that the codimension of the closure of $\\mathcal{A}$ is infinite or when $L$ is sequentially compact.","To be more precise, we introduce the notion of the Stone-Weiestrass character of $L$ which is closely related to the topological weight of $L$ and allows us to describe the lineability of $\\mathcal{C}(L, \\K) \\setminus \\overline{\\mathcal{A}}$ in terms of the Stone-Weierstrass character of subsets of $\\sim_{\\mathcal A}$. We also prove, in the classical case, that $(\\mathcal{C}(\\partial{D}, \\C) \\setminus \\overline{\\mbox{Pol}(\\partial{D})}) \\cup \\{0\\}$ (where $\\mbox{Pol}(\\partial{D})$ is the set of all complex polynomials in one variable restricted to the boundary of the unit disk) contains an isometric copy of $\\text{Hol}(\\partial{D})$ and is strongly $\\mathfrak c$-algebrable, extending previous results from the literature."],"url":"http://arxiv.org/abs/2405.06453v1","category":"math.FA"}
{"created":"2024-05-10 13:02:23","title":"Integer partitions detect the primes","abstract":"We show that integer partitions, the fundamental building blocks in additive number theory, detect prime numbers in an unexpected way. Answering a question of Schneider, we show that the primes are the solutions to special equations in partition functions. For example, an integer $n\\geq 2$ is prime if and only if   $$   (3n^3 - 13n^2 + 18n - 8)M_1(n) + (12n^2 - 120n + 212)M_2(n) -960M_3(n) = 0,   $$   where the $M_a(n)$ are MacMahon's well-studied partition functions. More generally, for \"MacMahonesque\" partition functions $M_{\\vec{a}}(n),$ we prove that there are infinitely many such prime detecting equations with constant coefficients, such as   $$   80M_{(1,1,1)}(n)-12M_{(2,0,1)}(n)+12M_{(2,1,0)}(n)+\\dots-12M_{(1,3)}(n)-39M_{(3,1)}(n)=0.   $$","sentences":["We show that integer partitions, the fundamental building blocks in additive number theory, detect prime numbers in an unexpected way.","Answering a question of Schneider, we show that the primes are the solutions to special equations in partition functions.","For example, an integer $n\\geq 2$ is prime if and only if   $$   (3n^3 - 13n^2 + 18n - 8)M_1(n) + (12n^2 - 120n + 212)M_2(n) -960M_3(n)","= 0,   $$   where the $M_a(n)$ are MacMahon's well-studied partition functions.","More generally, for \"MacMahonesque\" partition functions $M_{\\vec{a}}(n),$ we prove that there are infinitely many such prime detecting equations with constant coefficients, such as   $$   80M_{(1,1,1)}(n)-12M_{(2,0,1)}(n)+12M_{(2,1,0)}(n)+\\dots-12M_{(1,3)}(n)-39M_{(3,1)}(n)=0.   ","$$"],"url":"http://arxiv.org/abs/2405.06451v1","category":"math.NT"}
{"created":"2024-05-10 12:56:16","title":"Maps between spherical group rings","abstract":"We prove that for finitely generated abelian groups $A$ and $B$, the space of $\\mathbb{E}_\\infty$-ring maps between the spherical groups rings $\\mathbb{S}[A] \\to \\mathbb{S}[B]$ is equivalent to the discrete set of group homomorphisms $A \\to B$. We also prove generalizations where the sphere is replaced by other ring spectra, e.g. we give a formula for the strict units in group rings of the form $R[A]$ for $A$ a finite $p$-group and $R$ $p$-completely chromatically complete.","sentences":["We prove that for finitely generated abelian groups $A$ and $B$, the space of $\\mathbb{E}_\\infty$-ring maps between the spherical groups rings $\\mathbb{S}[A] \\to \\mathbb{S}[B]$ is equivalent to the discrete set of group homomorphisms $A \\to B$. We also prove generalizations where the sphere is replaced by other ring spectra, e.g. we give a formula for the strict units in group rings of the form $R[A]$ for $A$ a finite $p$-group and $R$ $p$-completely chromatically complete."],"url":"http://arxiv.org/abs/2405.06448v1","category":"math.AT"}
{"created":"2024-05-10 12:48:57","title":"Residual-based Attention Physics-informed Neural Networks for Efficient Spatio-Temporal Lifetime Assessment of Transformers Operated in Renewable Power Plants","abstract":"Transformers are vital assets for the reliable and efficient operation of power and energy systems. They support the integration of renewables to the grid through improved grid stability and operation efficiency. Monitoring the health of transformers is essential to ensure grid reliability and efficiency. Thermal insulation ageing is a key transformer failure mode, which is generally tracked by monitoring the hotspot temperature (HST). However, HST measurement is complex and expensive and often estimated from indirect measurements. Existing computationally-efficient HST models focus on space-agnostic thermal models, providing worst-case HST estimates. This article introduces an efficient spatio-temporal model for transformer winding temperature and ageing estimation, which leverages physics-based partial differential equations (PDEs) with data-driven Neural Networks (NN) in a Physics Informed Neural Networks (PINNs) configuration to improve prediction accuracy and acquire spatio-temporal resolution. The computational efficiency of the PINN model is improved through the implementation of the Residual-Based Attention scheme that accelerates the PINN model convergence. PINN based oil temperature predictions are used to estimate spatio-temporal transformer winding temperature values, which are validated through PDE resolution models and fiber optic sensor measurements, respectively. Furthermore, the spatio-temporal transformer ageing model is inferred, aiding transformer health management decision-making and providing insights into localized thermal ageing phenomena in the transformer insulation. Results are validated with a distribution transformer operated on a floating photovoltaic power plant.","sentences":["Transformers are vital assets for the reliable and efficient operation of power and energy systems.","They support the integration of renewables to the grid through improved grid stability and operation efficiency.","Monitoring the health of transformers is essential to ensure grid reliability and efficiency.","Thermal insulation ageing is a key transformer failure mode, which is generally tracked by monitoring the hotspot temperature (HST).","However, HST measurement is complex and expensive and often estimated from indirect measurements.","Existing computationally-efficient HST models focus on space-agnostic thermal models, providing worst-case HST estimates.","This article introduces an efficient spatio-temporal model for transformer winding temperature and ageing estimation, which leverages physics-based partial differential equations (PDEs) with data-driven Neural Networks (NN) in a Physics Informed Neural Networks (PINNs) configuration to improve prediction accuracy and acquire spatio-temporal resolution.","The computational efficiency of the PINN model is improved through the implementation of the Residual-Based Attention scheme that accelerates the PINN model convergence.","PINN based oil temperature predictions are used to estimate spatio-temporal transformer winding temperature values, which are validated through PDE resolution models and fiber optic sensor measurements, respectively.","Furthermore, the spatio-temporal transformer ageing model is inferred, aiding transformer health management decision-making and providing insights into localized thermal ageing phenomena in the transformer insulation.","Results are validated with a distribution transformer operated on a floating photovoltaic power plant."],"url":"http://arxiv.org/abs/2405.06443v1","category":"cs.LG"}
{"created":"2024-05-10 12:44:05","title":"Optimal Beamforming of RIS-Aided Wireless Communications: An Alternating Inner Product Maximization Approach","abstract":"This paper investigates a general discrete $\\ell_p$-norm maximization problem, with the power enhancement at steering directions through reconfigurable intelligent surfaces (RISs) as an instance. We propose a mathematically concise iterative framework composed of alternating inner product maximizations, well-suited for addressing $\\ell_1$- and $\\ell_2$-norm maximizations with either discrete or continuous uni-modular variable constraints. The iteration is proven to be monotonically non-decreasing. Moreover, this framework exhibits a distinctive capability to mitigate performance degradation due to discrete quantization, establishing it as the first post-rounding lifting approach applicable to any algorithm intended for the continuous solution. Additionally, as an integral component of the alternating iterations framework, we present a divide-and-sort (DaS) method to tackle the discrete inner product maximization problem. In the realm of $\\ell_\\infty$-norm maximization with discrete uni-modular constraints, the DaS ensures the identification of the global optimum with polynomial search complexity. We validate the effectiveness of the alternating inner product maximization framework in beamforming through RISs using both numerical experiments and field trials on prototypes. The results demonstrate that the proposed approach achieves higher power enhancement and outperforms other competitors. Finally, we show that discrete phase configurations with moderate quantization bits (e.g., 4-bit) exhibit comparable performance to continuous configurations in terms of power gains.","sentences":["This paper investigates a general discrete $\\ell_p$-norm maximization problem, with the power enhancement at steering directions through reconfigurable intelligent surfaces (RISs) as an instance.","We propose a mathematically concise iterative framework composed of alternating inner product maximizations, well-suited for addressing $\\ell_1$- and $\\ell_2$-norm maximizations with either discrete or continuous uni-modular variable constraints.","The iteration is proven to be monotonically non-decreasing.","Moreover, this framework exhibits a distinctive capability to mitigate performance degradation due to discrete quantization, establishing it as the first post-rounding lifting approach applicable to any algorithm intended for the continuous solution.","Additionally, as an integral component of the alternating iterations framework, we present a divide-and-sort (DaS) method to tackle the discrete inner product maximization problem.","In the realm of $\\ell_\\infty$-norm maximization with discrete uni-modular constraints, the DaS ensures the identification of the global optimum with polynomial search complexity.","We validate the effectiveness of the alternating inner product maximization framework in beamforming through RISs using both numerical experiments and field trials on prototypes.","The results demonstrate that the proposed approach achieves higher power enhancement and outperforms other competitors.","Finally, we show that discrete phase configurations with moderate quantization bits (e.g., 4-bit) exhibit comparable performance to continuous configurations in terms of power gains."],"url":"http://arxiv.org/abs/2405.06442v1","category":"cs.IT"}
{"created":"2024-05-10 12:43:07","title":"Lateral plasmonic superlattice in strongly dissipative regime","abstract":"We calculate transmission coefficient, $\\mathcal T,$ of terahertz radiation through lateral plasmonic superlattice with a unit cell consisting of two regions with different plasma wave velocities, $s_1$ and $s_2$ ($s_1 > s_2$). We generalize theory developed earlier for resonant case to the non-resonant regime, when the scattering rate, $\\gamma,$ is large compared to fundamental gate-tunable frequencies $\\omega_{1,2}$ of plasma oscillations in both regions. We find that absorption, and consequently $\\mathcal T$, strongly depends on density modulation amplitude and on the frequency of the incoming radiation. We describe evolution of the absorption with increasing of radiation frequency from the quasi-static regime of very low frequency to the high-frequency regime, identify several dissipation regimes and find analytical expression for absorption, and, accordingly, for $\\mathcal T,$ in these regimes. A general phase diagram of non-resonant regime in the plane $(\\omega,\\omega_2)$ for fixed $\\omega_1$ is constructed. Most importantly, $\\mathcal T$ sharply depends on the gate voltages and frequency. In particular, for $\\omega_2 \\ll \\omega_1,$ $\\mathcal T$ strongly varies on the very small frequency scale, $\\delta \\omega \\ll \\gamma,$ determined by the Maxwell relaxation, $\\delta \\omega \\sim \\omega_1^2/\\gamma,$ so that the superlattice shows high responsivity within the frequency band $\\delta \\omega.$","sentences":["We calculate transmission coefficient, $\\mathcal T,$ of terahertz radiation through lateral plasmonic superlattice with a unit cell consisting of two regions with different plasma wave velocities, $s_1$ and $s_2$ ($s_1 > s_2$).","We generalize theory developed earlier for resonant case to the non-resonant regime, when the scattering rate, $\\gamma,$ is large compared to fundamental gate-tunable frequencies $\\omega_{1,2}$ of plasma oscillations in both regions.","We find that absorption, and consequently $\\mathcal T$, strongly depends on density modulation amplitude and on the frequency of the incoming radiation.","We describe evolution of the absorption with increasing of radiation frequency from the quasi-static regime of very low frequency to the high-frequency regime, identify several dissipation regimes and find analytical expression for absorption, and, accordingly, for $\\mathcal T,$ in these regimes.","A general phase diagram of non-resonant regime in the plane $(\\omega,\\omega_2)$ for fixed $\\omega_1$ is constructed.","Most importantly, $\\mathcal T$ sharply depends on the gate voltages and frequency.","In particular, for $\\omega_2 \\ll \\omega_1,$ $\\mathcal T$ strongly varies on the very small frequency scale, $\\delta \\omega \\ll \\gamma,$ determined by the Maxwell relaxation, $\\delta \\omega \\sim \\omega_1^2/\\gamma,$ so that the superlattice shows high responsivity within the frequency band $\\delta \\omega.$"],"url":"http://arxiv.org/abs/2405.06441v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-10 12:41:16","title":"Industrial Application of the Shapley value-based Redispatch Cost Allocation to Large-Scale Power Grids requires AC Optimal Power Flow","abstract":"A burgeoning topic in the current energy transition are the huge costs of redispatch congestion management (CM) in large transmission systems. One of the German transmission system operators (TSOs) raised the critical inquiry of how to allocate the redispatch costs amongst TSOs in an equitable and beneficial way. Previously, a Shapley value-based approach has been introduced on small test grids, using the linear DC approximation of optimal power flow (OPF). However, within the application of CM, its feasibility and accuracy for large-scale power grids and its impact on the computed congestions remain uncertain. Therefore, this study investigates the applicability of the DC OPF compared to the exact AC OPF with regard to the Shapley values, for both small and large-scale grids. Numerical simulation shows significant differences in the congested lines, the overall redispatch costs, and the Shapley values. These findings suggest that for future CM, the TSOs should further investigate AC OPF solutions.","sentences":["A burgeoning topic in the current energy transition are the huge costs of redispatch congestion management (CM) in large transmission systems.","One of the German transmission system operators (TSOs) raised the critical inquiry of how to allocate the redispatch costs amongst TSOs in an equitable and beneficial way.","Previously, a Shapley value-based approach has been introduced on small test grids, using the linear DC approximation of optimal power flow (OPF).","However, within the application of CM, its feasibility and accuracy for large-scale power grids and its impact on the computed congestions remain uncertain.","Therefore, this study investigates the applicability of the DC OPF compared to the exact AC OPF with regard to the Shapley values, for both small and large-scale grids.","Numerical simulation shows significant differences in the congested lines, the overall redispatch costs, and the Shapley values.","These findings suggest that for future CM, the TSOs should further investigate AC OPF solutions."],"url":"http://arxiv.org/abs/2405.06439v1","category":"eess.SY"}
{"created":"2024-05-10 12:38:48","title":"Generalized van Trees inequality: Local minimax bounds for non-smooth functionals and irregular statistical models","abstract":"In a decision-theoretic framework, minimax lower bound provides the worst-case performance of estimators relative to a given class of statistical models. For parametric and semiparametric models, the H\\'{a}jek--Le Cam local asymptotic minimax (LAM) theorem provides the optimal and sharp asymptotic lower bound. Despite its relative generality, this result comes with limitations as it only applies to the estimation of differentiable functionals under regular statistical models. On the other hand, non-asymptotic minimax lower bounds, such as those based on the reduction to hypothesis testing, do not often yield sharp asymptotic constants. Inspired by the recent improvement of the van Trees inequality and related methods in the literature, we provide new non-asymptotic minimax lower bounds under minimal regularity assumptions, which imply sharp asymptotic constants. The proposed lower bounds do not require the differentiability of functionals or regularity of statistical models, extending the efficiency theory to broader situations where standard approaches fail. Additionally, new lower bounds provide non-asymptotic constants, which can shed light on more refined fundamental limits of estimation in finite samples. We demonstrate that new lower bounds recover many classical results, including the LAM theorem and semiparametric efficiency bounds. We also illustrate the use of the new lower bound by deriving the local minimax lower bound for estimating the density at a point and directionally differentiable parameters.","sentences":["In a decision-theoretic framework, minimax lower bound provides the worst-case performance of estimators relative to a given class of statistical models.","For parametric and semiparametric models, the H\\'{a}jek--Le Cam local asymptotic minimax (LAM) theorem provides the optimal and sharp asymptotic lower bound.","Despite its relative generality, this result comes with limitations as it only applies to the estimation of differentiable functionals under regular statistical models.","On the other hand, non-asymptotic minimax lower bounds, such as those based on the reduction to hypothesis testing, do not often yield sharp asymptotic constants.","Inspired by the recent improvement of the van Trees inequality and related methods in the literature, we provide new non-asymptotic minimax lower bounds under minimal regularity assumptions, which imply sharp asymptotic constants.","The proposed lower bounds do not require the differentiability of functionals or regularity of statistical models, extending the efficiency theory to broader situations where standard approaches fail.","Additionally, new lower bounds provide non-asymptotic constants, which can shed light on more refined fundamental limits of estimation in finite samples.","We demonstrate that new lower bounds recover many classical results, including the LAM theorem and semiparametric efficiency bounds.","We also illustrate the use of the new lower bound by deriving the local minimax lower bound for estimating the density at a point and directionally differentiable parameters."],"url":"http://arxiv.org/abs/2405.06437v1","category":"math.ST"}
{"created":"2024-05-10 12:20:12","title":"Probing orbits of stellar mass objects deep in galactic nuclei with quasi-periodic eruptions -- II: population analysis","abstract":"Quasi-periodic eruptions (QPEs) are intense repeating soft X-ray bursts with recurrence times about a few hours to a few weeks from galactic nuclei. Though the debates on the origin of QPEs have not completely settled down, more and more analyses favor the interpretation that QPEs are the result of collisions between a stellar mass object (a stellar mass black hole or a main sequence star) and an accretion disk around a supermassive black hole (SMBH) in galactic nuclei. If this interpretation is correct, QPEs will be invaluable in probing the orbits of stellar mass objects in the vicinity of SMBHs, and further inferring the formation of extreme mass ratio inspirals (EMRIs), one of the major targets of spaceborne gravitational wave missions. In this work, we extended the EMRI orbital analysis in Paper I arXiv:2401.11190 to all the known QPE sources with more than $6$ flares observed. Among all the analyzed 5 QPE sources, two distinct EMRI populations are identified: 4 EMRIs are of low orbital eccentricity (consistent with 0) which should be born in the wet EMRI formation channel, and 1 mildly eccentric EMRI (with $e= 0.25^{+0.18}_{-0.20}$ at 2-$\\sigma$ confidence level) is consistent with the predictions of both the dry loss-cone formation channel and the Hills mechanism.","sentences":["Quasi-periodic eruptions (QPEs) are intense repeating soft X-ray bursts with recurrence times about a few hours to a few weeks from galactic nuclei.","Though the debates on the origin of QPEs have not completely settled down, more and more analyses favor the interpretation that QPEs are the result of collisions between a stellar mass object (a stellar mass black hole or a main sequence star) and an accretion disk around a supermassive black hole (SMBH) in galactic nuclei.","If this interpretation is correct, QPEs will be invaluable in probing the orbits of stellar mass objects in the vicinity of SMBHs, and further inferring the formation of extreme mass ratio inspirals (EMRIs), one of the major targets of spaceborne gravitational wave missions.","In this work, we extended the EMRI orbital analysis in Paper I arXiv:2401.11190 to all the known QPE sources with more than $6$ flares observed.","Among all the analyzed 5 QPE sources, two distinct EMRI populations are identified: 4 EMRIs are of low orbital eccentricity (consistent with 0) which should be born in the wet EMRI formation channel, and 1 mildly eccentric EMRI (with $e= 0.25^{+0.18}_{-0.20}$ at 2-$\\sigma$ confidence level) is consistent with the predictions of both the dry loss-cone formation channel and the Hills mechanism."],"url":"http://arxiv.org/abs/2405.06429v1","category":"astro-ph.HE"}
{"created":"2024-05-10 12:16:18","title":"Generation of Ultra-Collimated Polarized Attosecond $\u03b3-$Rays via Beam Instabilities","abstract":"Polarized attosecond $\\gamma-$rays may offer excitation and hyperfine tracking of reactions relevant to nuclear physics, astrophysics, high-energy physics, etc. However, unfortunately, generation of a feasible and easy-to-deploy source is still a great challenge. Here, we put forward a novel method for producing ultra-collimated high-brilliance polarized attosecond $\\gamma-$rays via the interaction of an unpolarized electron beam with a solid-density plasma. As a relativistic electron beam enters a solid-density plasma, it can be modulated into high-density clusters via the self-modulation instability of itself and further into attosecond slices due to its own hosing instability. This is accompanied by the generation of similar pulse-width $\\gamma-$slices via nonlinear Compton scattering. The severe hosing instability breaks the symmetry of the excited electromagnetic fields, resulting in net linear polarization of $\\gamma-$slices, which challenges the conventional perception that the interaction of an axially symmetric unpolarized electron beam with a uniform plasma cannot generate polarized radiation. In addition, we also obtain high-quality electron microbunches which may serve as an alternative source for prebunched free-electron lasers.","sentences":["Polarized attosecond $\\gamma-$rays may offer excitation and hyperfine tracking of reactions relevant to nuclear physics, astrophysics, high-energy physics, etc.","However, unfortunately, generation of a feasible and easy-to-deploy source is still a great challenge.","Here, we put forward a novel method for producing ultra-collimated high-brilliance polarized attosecond $\\gamma-$rays via the interaction of an unpolarized electron beam with a solid-density plasma.","As a relativistic electron beam enters a solid-density plasma, it can be modulated into high-density clusters via the self-modulation instability of itself and further into attosecond slices due to its own hosing instability.","This is accompanied by the generation of similar pulse-width $\\gamma-$slices via nonlinear Compton scattering.","The severe hosing instability breaks the symmetry of the excited electromagnetic fields, resulting in net linear polarization of $\\gamma-$slices, which challenges the conventional perception that the interaction of an axially symmetric unpolarized electron beam with a uniform plasma cannot generate polarized radiation.","In addition, we also obtain high-quality electron microbunches which may serve as an alternative source for prebunched free-electron lasers."],"url":"http://arxiv.org/abs/2405.06426v1","category":"physics.plasm-ph"}
{"created":"2024-05-10 12:14:11","title":"Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation","abstract":"Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts. This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult. To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation. Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty. Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training. Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench. These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models.","sentences":["Assessing response quality to instructions in language models is vital but challenging due to the complexity of human language across different contexts.","This complexity often results in ambiguous or inconsistent interpretations, making accurate assessment difficult.","To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation.","Trained with preference datasets, our uncertainty-enabled proxy not only scores rewards for responses but also evaluates their inherent uncertainty.","Empirical results demonstrate significant benefits of incorporating the proposed proxy into language model training.","Our method boosts the instruction following capability of language models by refining data curation for training and improving policy optimization objectives, thereby surpassing existing methods by a large margin on benchmarks such as Vicuna and MT-bench.","These findings highlight that our proposed approach substantially advances language model training and paves a new way of harnessing uncertainty within language models."],"url":"http://arxiv.org/abs/2405.06424v1","category":"cs.CL"}
{"created":"2024-05-10 12:13:54","title":"Carleson Operators on Doubling Metric Measure Spaces","abstract":"We prove a new generalization of a theorem of Carleson, namely bounds for a generalized Carleson operator on doubling metric measure spaces. Additionally, we explicitly reduce Carleson's classical result on pointwise convergence of Fourier series to this new theorem. Both proofs are presented in great detail, suitable as a blueprint for computer verification using the current capabilities of the software package Lean. Note that even Carleson's classical result has not yet been computer-verified.","sentences":["We prove a new generalization of a theorem of Carleson, namely bounds for a generalized Carleson operator on doubling metric measure spaces.","Additionally, we explicitly reduce Carleson's classical result on pointwise convergence of Fourier series to this new theorem.","Both proofs are presented in great detail, suitable as a blueprint for computer verification using the current capabilities of the software package Lean.","Note that even Carleson's classical result has not yet been computer-verified."],"url":"http://arxiv.org/abs/2405.06423v1","category":"math.CA"}
{"created":"2024-05-10 12:12:38","title":"Contextual Affordances for Safe Exploration in Robotic Scenarios","abstract":"Robotics has been a popular field of research in the past few decades, with much success in industrial applications such as manufacturing and logistics. This success is led by clearly defined use cases and controlled operating environments. However, robotics has yet to make a large impact in domestic settings. This is due in part to the difficulty and complexity of designing mass-manufactured robots that can succeed in the variety of homes and environments that humans live in and that can operate safely in close proximity to humans. This paper explores the use of contextual affordances to enable safe exploration and learning in robotic scenarios targeted in the home. In particular, we propose a simple state representation that allows us to extend contextual affordances to larger state spaces and showcase how affordances can improve the success and convergence rate of a reinforcement learning algorithm in simulation. Our results suggest that after further iterations, it is possible to consider the implementation of this approach in a real robot manipulator. Furthermore, in the long term, this work could be the foundation for future explorations of human-robot interactions in complex domestic environments. This could be possible once state-of-the-art robot manipulators achieve the required level of dexterity for the described affordances in this paper.","sentences":["Robotics has been a popular field of research in the past few decades, with much success in industrial applications such as manufacturing and logistics.","This success is led by clearly defined use cases and controlled operating environments.","However, robotics has yet to make a large impact in domestic settings.","This is due in part to the difficulty and complexity of designing mass-manufactured robots that can succeed in the variety of homes and environments that humans live in and that can operate safely in close proximity to humans.","This paper explores the use of contextual affordances to enable safe exploration and learning in robotic scenarios targeted in the home.","In particular, we propose a simple state representation that allows us to extend contextual affordances to larger state spaces and showcase how affordances can improve the success and convergence rate of a reinforcement learning algorithm in simulation.","Our results suggest that after further iterations, it is possible to consider the implementation of this approach in a real robot manipulator.","Furthermore, in the long term, this work could be the foundation for future explorations of human-robot interactions in complex domestic environments.","This could be possible once state-of-the-art robot manipulators achieve the required level of dexterity for the described affordances in this paper."],"url":"http://arxiv.org/abs/2405.06422v1","category":"cs.RO"}
{"created":"2024-05-10 12:10:22","title":"Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting","abstract":"In real-world scenarios, time series forecasting often demands timeliness, making research on model backbones a perennially hot topic. To meet these performance demands, we propose a novel backbone from the perspective of information fusion. Introducing the Basic Probability Assignment (BPA) Module and the Time Evidence Fusion Network (TEFN), based on evidence theory, allows us to achieve superior performance. On the other hand, the perspective of multi-source information fusion effectively improves the accuracy of forecasting. Due to the fact that BPA is generated by fuzzy theory, TEFN also has considerable interpretability. In real data experiments, the TEFN partially achieved state-of-the-art, with low errors comparable to PatchTST, and operating efficiency surpass performance models such as Dlinear. Meanwhile, TEFN has high robustness and small error fluctuations in the random hyperparameter selection. TEFN is not a model that achieves the ultimate in single aspect, but a model that balances performance, accuracy, stability, and interpretability.","sentences":["In real-world scenarios, time series forecasting often demands timeliness, making research on model backbones a perennially hot topic.","To meet these performance demands, we propose a novel backbone from the perspective of information fusion.","Introducing the Basic Probability Assignment (BPA) Module and the Time Evidence Fusion Network (TEFN), based on evidence theory, allows us to achieve superior performance.","On the other hand, the perspective of multi-source information fusion effectively improves the accuracy of forecasting.","Due to the fact that BPA is generated by fuzzy theory, TEFN also has considerable interpretability.","In real data experiments, the TEFN partially achieved state-of-the-art, with low errors comparable to PatchTST, and operating efficiency surpass performance models such as Dlinear.","Meanwhile, TEFN has high robustness and small error fluctuations in the random hyperparameter selection.","TEFN is not a model that achieves the ultimate in single aspect, but a model that balances performance, accuracy, stability, and interpretability."],"url":"http://arxiv.org/abs/2405.06419v1","category":"cs.LG"}
{"created":"2024-05-10 12:03:53","title":"PAC-Bayesian Generalization Bounds for Knowledge Graph Representation Learning","abstract":"While a number of knowledge graph representation learning (KGRL) methods have been proposed over the past decade, very few theoretical analyses have been conducted on them. In this paper, we present the first PAC-Bayesian generalization bounds for KGRL methods. To analyze a broad class of KGRL models, we propose a generic framework named ReED (Relation-aware Encoder-Decoder), which consists of a relation-aware message passing encoder and a triplet classification decoder. Our ReED framework can express at least 15 different existing KGRL models, including not only graph neural network-based models such as R-GCN and CompGCN but also shallow-architecture models such as RotatE and ANALOGY. Our generalization bounds for the ReED framework provide theoretical grounds for the commonly used tricks in KGRL, e.g., parameter-sharing and weight normalization schemes, and guide desirable design choices for practical KGRL methods. We empirically show that the critical factors in our generalization bounds can explain actual generalization errors on three real-world knowledge graphs.","sentences":["While a number of knowledge graph representation learning (KGRL) methods have been proposed over the past decade, very few theoretical analyses have been conducted on them.","In this paper, we present the first PAC-Bayesian generalization bounds for KGRL methods.","To analyze a broad class of KGRL models, we propose a generic framework named ReED (Relation-aware Encoder-Decoder), which consists of a relation-aware message passing encoder and a triplet classification decoder.","Our ReED framework can express at least 15 different existing KGRL models, including not only graph neural network-based models such as R-GCN and CompGCN but also shallow-architecture models such as RotatE and ANALOGY.","Our generalization bounds for the ReED framework provide theoretical grounds for the commonly used tricks in KGRL, e.g., parameter-sharing and weight normalization schemes, and guide desirable design choices for practical KGRL methods.","We empirically show that the critical factors in our generalization bounds can explain actual generalization errors on three real-world knowledge graphs."],"url":"http://arxiv.org/abs/2405.06418v1","category":"cs.LG"}
{"created":"2024-05-10 11:55:27","title":"Generalization analysis with deep ReLU networks for metric and similarity learning","abstract":"While considerable theoretical progress has been devoted to the study of metric and similarity learning, the generalization mystery is still missing. In this paper, we study the generalization performance of metric and similarity learning by leveraging the specific structure of the true metric (the target function). Specifically, by deriving the explicit form of the true metric for metric and similarity learning with the hinge loss, we construct a structured deep ReLU neural network as an approximation of the true metric, whose approximation ability relies on the network complexity. Here, the network complexity corresponds to the depth, the number of nonzero weights and the computation units of the network. Consider the hypothesis space which consists of the structured deep ReLU networks, we develop the excess generalization error bounds for a metric and similarity learning problem by estimating the approximation error and the estimation error carefully. An optimal excess risk rate is derived by choosing the proper capacity of the constructed hypothesis space. To the best of our knowledge, this is the first-ever-known generalization analysis providing the excess generalization error for metric and similarity learning. In addition, we investigate the properties of the true metric of metric and similarity learning with general losses.","sentences":["While considerable theoretical progress has been devoted to the study of metric and similarity learning, the generalization mystery is still missing.","In this paper, we study the generalization performance of metric and similarity learning by leveraging the specific structure of the true metric (the target function).","Specifically, by deriving the explicit form of the true metric for metric and similarity learning with the hinge loss, we construct a structured deep ReLU neural network as an approximation of the true metric, whose approximation ability relies on the network complexity.","Here, the network complexity corresponds to the depth, the number of nonzero weights and the computation units of the network.","Consider the hypothesis space which consists of the structured deep ReLU networks, we develop the excess generalization error bounds for a metric and similarity learning problem by estimating the approximation error and the estimation error carefully.","An optimal excess risk rate is derived by choosing the proper capacity of the constructed hypothesis space.","To the best of our knowledge, this is the first-ever-known generalization analysis providing the excess generalization error for metric and similarity learning.","In addition, we investigate the properties of the true metric of metric and similarity learning with general losses."],"url":"http://arxiv.org/abs/2405.06415v1","category":"stat.ML"}
{"created":"2024-05-10 11:53:53","title":"Can Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?","abstract":"Intelligent Tutoring Systems (ITSs) often contain an automated feedback component, which provides a predefined feedback message to students when they detect a predefined error. To such a feedback component, we often resort to template-based approaches. These approaches require significant effort from human experts to detect a limited number of possible student errors and provide corresponding feedback. This limitation is exemplified in open-ended math questions, where there can be a large number of different incorrect errors. In our work, we examine the capabilities of large language models (LLMs) to generate feedback for open-ended math questions, similar to that of an established ITS that uses a template-based approach. We fine-tune both open-source and proprietary LLMs on real student responses and corresponding ITS-provided feedback. We measure the quality of the generated feedback using text similarity metrics. We find that open-source and proprietary models both show promise in replicating the feedback they see during training, but do not generalize well to previously unseen student errors. These results suggest that despite being able to learn the formatting of feedback, LLMs are not able to fully understand mathematical errors made by students.","sentences":["Intelligent Tutoring Systems (ITSs) often contain an automated feedback component, which provides a predefined feedback message to students when they detect a predefined error.","To such a feedback component, we often resort to template-based approaches.","These approaches require significant effort from human experts to detect a limited number of possible student errors and provide corresponding feedback.","This limitation is exemplified in open-ended math questions, where there can be a large number of different incorrect errors.","In our work, we examine the capabilities of large language models (LLMs) to generate feedback for open-ended math questions, similar to that of an established ITS that uses a template-based approach.","We fine-tune both open-source and proprietary LLMs on real student responses and corresponding ITS-provided feedback.","We measure the quality of the generated feedback using text similarity metrics.","We find that open-source and proprietary models both show promise in replicating the feedback they see during training, but do not generalize well to previously unseen student errors.","These results suggest that despite being able to learn the formatting of feedback, LLMs are not able to fully understand mathematical errors made by students."],"url":"http://arxiv.org/abs/2405.06414v1","category":"cs.CL"}
{"created":"2024-05-10 11:52:53","title":"Multi-level Personalized Federated Learning on Heterogeneous and Long-Tailed Data","abstract":"Federated learning (FL) offers a privacy-centric distributed learning framework, enabling model training on individual clients and central aggregation without necessitating data exchange. Nonetheless, FL implementations often suffer from non-i.i.d. and long-tailed class distributions across mobile applications, e.g., autonomous vehicles, which leads models to overfitting as local training may converge to sub-optimal. In our study, we explore the impact of data heterogeneity on model bias and introduce an innovative personalized FL framework, Multi-level Personalized Federated Learning (MuPFL), which leverages the hierarchical architecture of FL to fully harness computational resources at various levels. This framework integrates three pivotal modules: Biased Activation Value Dropout (BAVD) to mitigate overfitting and accelerate training; Adaptive Cluster-based Model Update (ACMU) to refine local models ensuring coherent global aggregation; and Prior Knowledge-assisted Classifier Fine-tuning (PKCF) to bolster classification and personalize models in accord with skewed local data with shared knowledge. Extensive experiments on diverse real-world datasets for image classification and semantic segmentation validate that MuPFL consistently outperforms state-of-the-art baselines, even under extreme non-i.i.d. and long-tail conditions, which enhances accuracy by as much as 7.39% and accelerates training by up to 80% at most, marking significant advancements in both efficiency and effectiveness.","sentences":["Federated learning (FL) offers a privacy-centric distributed learning framework, enabling model training on individual clients and central aggregation without necessitating data exchange.","Nonetheless, FL implementations often suffer from non-i.i.d. and long-tailed class distributions across mobile applications, e.g., autonomous vehicles, which leads models to overfitting as local training may converge to sub-optimal.","In our study, we explore the impact of data heterogeneity on model bias and introduce an innovative personalized FL framework, Multi-level Personalized Federated Learning (MuPFL), which leverages the hierarchical architecture of FL to fully harness computational resources at various levels.","This framework integrates three pivotal modules: Biased Activation Value Dropout (BAVD) to mitigate overfitting and accelerate training; Adaptive Cluster-based Model Update (ACMU) to refine local models ensuring coherent global aggregation; and Prior Knowledge-assisted Classifier Fine-tuning (PKCF) to bolster classification and personalize models in accord with skewed local data with shared knowledge.","Extensive experiments on diverse real-world datasets for image classification and semantic segmentation validate that MuPFL consistently outperforms state-of-the-art baselines, even under extreme non-i.i.d. and long-tail conditions, which enhances accuracy by as much as 7.39% and accelerates training by up to 80% at most, marking significant advancements in both efficiency and effectiveness."],"url":"http://arxiv.org/abs/2405.06413v1","category":"cs.AI"}
{"created":"2024-05-10 11:47:14","title":"Mixing and ergodicity of compositions of inner functions","abstract":"We study ergodic and mixing properties of non-autonomous dynamics on the unit circle generated by inner functions fixing the origin.","sentences":["We study ergodic and mixing properties of non-autonomous dynamics on the unit circle generated by inner functions fixing the origin."],"url":"http://arxiv.org/abs/2405.06411v1","category":"math.DS"}
{"created":"2024-05-10 11:43:35","title":"Visualizing Neural Network Imagination","abstract":"In certain situations, neural networks will represent environment states in their hidden activations. Our goal is to visualize what environment states the networks are representing. We experiment with a recurrent neural network (RNN) architecture with a decoder network at the end. After training, we apply the decoder to the intermediate representations of the network to visualize what they represent. We define a quantitative interpretability metric and use it to demonstrate that hidden states can be highly interpretable on a simple task. We also develop autoencoder and adversarial techniques and show that benefit interpretability.","sentences":["In certain situations, neural networks will represent environment states in their hidden activations.","Our goal is to visualize what environment states the networks are representing.","We experiment with a recurrent neural network (RNN) architecture with a decoder network at the end.","After training, we apply the decoder to the intermediate representations of the network to visualize what they represent.","We define a quantitative interpretability metric and use it to demonstrate that hidden states can be highly interpretable on a simple task.","We also develop autoencoder and adversarial techniques and show that benefit interpretability."],"url":"http://arxiv.org/abs/2405.06409v1","category":"cs.LG"}
{"created":"2024-05-10 11:42:06","title":"Gravitating Scalarons with Inverted Higgs Potential","abstract":"Previously, a class of regular and asymptotically flat gravitating scalar solitons (scalarons) has been constructed in the Einstein--Klein--Gordon (EKG) theory by adopting a phantom field with Higgs-like potential where the kinetic term has the wrong sign and the scalaron possesses the negative Arnowitt--Deser--Misner (ADM) mass as a consequence. In this paper, we demonstrate that the use of the phantom field can be avoided by inverting the Higgs-like potential in the EKG system when the kinetic term has a proper sign, such that the corresponding gravitating scalaron can possess the positive ADM mass. We systematically study the basic properties of the gravitating scalaron, such as the ADM mass, the energy conditions, the geodesics of test particles, etc. Moreover, we find that it can be smoothly connected to the counterpart hairy black hole solutions from our recent work in the small horizon limit.","sentences":["Previously, a class of regular and asymptotically flat gravitating scalar solitons (scalarons) has been constructed in the Einstein--Klein--Gordon (EKG) theory by adopting a phantom field with Higgs-like potential where the kinetic term has the wrong sign and the scalaron possesses the negative Arnowitt--Deser--Misner (ADM) mass as a consequence.","In this paper, we demonstrate that the use of the phantom field can be avoided by inverting the Higgs-like potential in the EKG system when the kinetic term has a proper sign, such that the corresponding gravitating scalaron can possess the positive ADM mass.","We systematically study the basic properties of the gravitating scalaron, such as the ADM mass, the energy conditions, the geodesics of test particles, etc.","Moreover, we find that it can be smoothly connected to the counterpart hairy black hole solutions from our recent work in the small horizon limit."],"url":"http://arxiv.org/abs/2405.06407v1","category":"gr-qc"}
{"created":"2024-05-10 11:34:47","title":"Inclusive content reduces racial and gender biases, yet non-inclusive content dominates popular media outlets","abstract":"Images are often termed as representations of perceived reality. As such, racial and gender biases in popular media imagery could play a vital role in shaping people's perceptions of society. While inquiries into such biases have examined the frequency at which different racial and gender groups appear in different forms of media, the literature still lacks a large-scale longitudinal study that further examines the manner in which these groups are portrayed. To fill this gap, we examine three media forms, namely fashion magazines, movie posters, and advertisements. To do so, we collect a large dataset comprising over 300,000 images spanning over five decades and utilize state-of-the-art machine learning models to not only classify race and gender but also identify the posture, emotional state, and body composition of the person featured in each image. We find that racial minorities appear far less frequently than their White counterparts, and when they do appear, they are portrayed less prominently and tend to convey more negative emotions. We also find that women are more likely to be portrayed with their full bodies in images, whereas men are more frequently presented with their faces. This disparity exemplifies face-ism, where emphasizing faces over bodies has been linked to perceptions of higher competence and intelligence. Finally, through a series of survey experiments, we show that exposure to inclusive content-rather than racially and gender-homogenized content -- significantly reduces perception biases towards minorities in areas such as household income, hiring merit, beauty standards, leadership positions, and the representation of women in the workplace. Taken together, our findings demonstrate that racial and gender biases in media continue to be an ongoing problem that may exacerbate existing stereotypes.","sentences":["Images are often termed as representations of perceived reality.","As such, racial and gender biases in popular media imagery could play a vital role in shaping people's perceptions of society.","While inquiries into such biases have examined the frequency at which different racial and gender groups appear in different forms of media, the literature still lacks a large-scale longitudinal study that further examines the manner in which these groups are portrayed.","To fill this gap, we examine three media forms, namely fashion magazines, movie posters, and advertisements.","To do so, we collect a large dataset comprising over 300,000 images spanning over five decades and utilize state-of-the-art machine learning models to not only classify race and gender but also identify the posture, emotional state, and body composition of the person featured in each image.","We find that racial minorities appear far less frequently than their White counterparts, and when they do appear, they are portrayed less prominently and tend to convey more negative emotions.","We also find that women are more likely to be portrayed with their full bodies in images, whereas men are more frequently presented with their faces.","This disparity exemplifies face-ism, where emphasizing faces over bodies has been linked to perceptions of higher competence and intelligence.","Finally, through a series of survey experiments, we show that exposure to inclusive content-rather than racially and gender-homogenized content -- significantly reduces perception biases towards minorities in areas such as household income, hiring merit, beauty standards, leadership positions, and the representation of women in the workplace.","Taken together, our findings demonstrate that racial and gender biases in media continue to be an ongoing problem that may exacerbate existing stereotypes."],"url":"http://arxiv.org/abs/2405.06404v1","category":"cs.CY"}
{"created":"2024-05-10 11:23:44","title":"Nuclear rainbow of the symmetric nucleus-nucleus system: Interchange of the nearside and farside scattering","abstract":"Extensive elastic scattering data measured at energies around 10 to 20 MeV/nucleon for some identical systems, like 12C+12C and 16O+16O, exhibit the nuclear rainbow pattern of broad Airy oscillations of the cross section at medium and large angles. Due to the identity of the scattered projectile and recoiled target, the rainbow pattern at angles around and beyond $\\theta_{\\rm c.m.}\\approx 90^\\circ$ is strongly deteriorated by the boson exchange. The nuclear rainbow features in the identical-particles elastic scattering discussed so far are based on the nearside-farside (NF) decomposition of the scattering amplitude given by an optical model calculation neglecting the projectile-target exchange symmetry. Moreover, the NF decomposition method was developed in the 70s by Fuller for nonidentical systems only, and the details of how the exchange symmetry of an identical system affects the evolution of nuclear rainbow remain unexplored. Therefore, the Fuller method is generalized in this work for the elastic scattering of two identical (spin-zero) nuclei, with the projectile-target exchange symmetry taken explicitly into account. The results obtained for elastic 12C+12C and 16O+16O scattering at low energies show the exchange symmetry results in a symmetric interchange of the nearside and farside patterns at angles passing $\\theta_{\\rm c.m.}=90^\\circ$, which requires a more subtle interpretation of nuclear rainbow. We also found that a similar NF interchange occurs in a nonidentical nucleus-nucleus system with the core-core symmetry, where the elastic cross section at backward angles is due mainly to the elastic transfer of cluster or nucleon between two identical cores. This interesting effect is illustrated in the elastic 16O+12C scattering at low energies where the elastic $\\alpha$ transfer between two 12C cores has been proven to enhance the elastic cross section at backward angles.","sentences":["Extensive elastic scattering data measured at energies around 10 to 20 MeV/nucleon for some identical systems, like 12C+12C and 16O+16O, exhibit the nuclear rainbow pattern of broad Airy oscillations of the cross section at medium and large angles.","Due to the identity of the scattered projectile and recoiled target, the rainbow pattern at angles around and beyond $\\theta_{\\rm c.m.}\\approx","90^\\circ$ is strongly deteriorated by the boson exchange.","The nuclear rainbow features in the identical-particles elastic scattering discussed so far are based on the nearside-farside (NF) decomposition of the scattering amplitude given by an optical model calculation neglecting the projectile-target exchange symmetry.","Moreover, the NF decomposition method was developed in the 70s by Fuller for nonidentical systems only, and the details of how the exchange symmetry of an identical system affects the evolution of nuclear rainbow remain unexplored.","Therefore, the Fuller method is generalized in this work for the elastic scattering of two identical (spin-zero) nuclei, with the projectile-target exchange symmetry taken explicitly into account.","The results obtained for elastic 12C+12C and 16O+16O scattering at low energies show the exchange symmetry results in a symmetric interchange of the nearside and farside patterns at angles passing $\\theta_{\\rm c.m.}=90^\\circ$, which requires a more subtle interpretation of nuclear rainbow.","We also found that a similar NF interchange occurs in a nonidentical nucleus-nucleus system with the core-core symmetry, where the elastic cross section at backward angles is due mainly to the elastic transfer of cluster or nucleon between two identical cores.","This interesting effect is illustrated in the elastic 16O+12C scattering at low energies where the elastic $\\alpha$ transfer between two 12C cores has been proven to enhance the elastic cross section at backward angles."],"url":"http://arxiv.org/abs/2405.06400v1","category":"nucl-th"}
{"created":"2024-05-10 11:22:31","title":"Program Synthesis using Inductive Logic Programming for the Abstraction and Reasoning Corpus","abstract":"The Abstraction and Reasoning Corpus (ARC) is a general artificial intelligence benchmark that is currently unsolvable by any Machine Learning method, including Large Language Models (LLMs). It demands strong generalization and reasoning capabilities which are known to be weaknesses of Neural Network based systems. In this work, we propose a Program Synthesis system that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to solve ARC. We have manually defined a simple Domain Specific Language (DSL) that corresponds to a small set of object-centric abstractions relevant to ARC. This is the Background Knowledge used by ILP to create Logic Programs that provide reasoning capabilities to our system. The full system is capable of generalize to unseen tasks, since ILP can create Logic Program(s) from few examples, in the case of ARC: pairs of Input-Output grids examples for each task. These Logic Programs are able to generate Objects present in the Output grid and the combination of these can form a complete program that transforms an Input grid into an Output grid. We randomly chose some tasks from ARC that dont require more than the small number of the Object primitives we implemented and show that given only these, our system can solve tasks that require each, such different reasoning.","sentences":["The Abstraction and Reasoning Corpus (ARC) is a general artificial intelligence benchmark that is currently unsolvable by any Machine Learning method, including Large Language Models (LLMs).","It demands strong generalization and reasoning capabilities which are known to be weaknesses of Neural Network based systems.","In this work, we propose a Program Synthesis system that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to solve ARC.","We have manually defined a simple Domain Specific Language (DSL) that corresponds to a small set of object-centric abstractions relevant to ARC.","This is the Background Knowledge used by ILP to create Logic Programs that provide reasoning capabilities to our system.","The full system is capable of generalize to unseen tasks, since ILP can create Logic Program(s) from few examples, in the case of ARC: pairs of Input-Output grids examples for each task.","These Logic Programs are able to generate Objects present in the Output grid and the combination of these can form a complete program that transforms an Input grid into an Output grid.","We randomly chose some tasks from ARC that dont require more than the small number of the Object primitives we implemented and show that given only these, our system can solve tasks that require each, such different reasoning."],"url":"http://arxiv.org/abs/2405.06399v1","category":"cs.LG"}
{"created":"2024-05-10 11:11:29","title":"Fitness-Based Growth of Directed Networks with Hierarchy","abstract":"Growing attention has been brought to the fact that many real directed networks exhibit hierarchy and directionality as measured through techniques like Trophic Analysis and non-normality. We propose a simple growing network model where the probability of connecting to a node is defined by a preferential attachment mechanism based on degree and the difference in fitness between nodes. In particular, we show how mechanisms such as degree-based preferential attachment and node fitness interactions can lead to the emergence of the spectrum of hierarchy and directionality observed in real networks. In this work, we study various features of this model relating to network hierarchy, as measured by Trophic Analysis. This includes (I) how preferential attachment can lead to network hierarchy, (II) how scale-free degree distributions and network hierarchy can coexist, (III) the correlation between node fitness and trophic level, (IV) how the fitness parameters can predict trophic incoherence and how the trophic level difference distribution compares to the fitness difference distribution, (V) the relationship between trophic level and degree imbalance and the unique role of nodes at the ends of the fitness hierarchy and (VI) how fitness interactions and degree-based preferential attachment can interplay to generate networks of varying coherence and degree distribution. We also provide an example of the intuition this work enables in the analysis of a real historical network. This work provides insight into simple mechanisms which can give rise to hierarchy in directed networks and quantifies the usefulness and limitations of using Trophic Analysis as an analysis tool for real networks.","sentences":["Growing attention has been brought to the fact that many real directed networks exhibit hierarchy and directionality as measured through techniques like Trophic Analysis and non-normality.","We propose a simple growing network model where the probability of connecting to a node is defined by a preferential attachment mechanism based on degree and the difference in fitness between nodes.","In particular, we show how mechanisms such as degree-based preferential attachment and node fitness interactions can lead to the emergence of the spectrum of hierarchy and directionality observed in real networks.","In this work, we study various features of this model relating to network hierarchy, as measured by Trophic Analysis.","This includes (I) how preferential attachment can lead to network hierarchy, (II) how scale-free degree distributions and network hierarchy can coexist, (III) the correlation between node fitness and trophic level, (IV) how the fitness parameters can predict trophic incoherence and how the trophic level difference distribution compares to the fitness difference distribution, (V) the relationship between trophic level and degree imbalance and the unique role of nodes at the ends of the fitness hierarchy and (VI) how fitness interactions and degree-based preferential attachment can interplay to generate networks of varying coherence and degree distribution.","We also provide an example of the intuition this work enables in the analysis of a real historical network.","This work provides insight into simple mechanisms which can give rise to hierarchy in directed networks and quantifies the usefulness and limitations of using Trophic Analysis as an analysis tool for real networks."],"url":"http://arxiv.org/abs/2405.06395v1","category":"physics.soc-ph"}
{"created":"2024-05-10 11:08:20","title":"Memory Mosaics","abstract":"Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent ways. We demonstrate these capabilities on toy examples and we also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks.","sentences":["Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest.","Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities.","Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent ways.","We demonstrate these capabilities on toy examples and we also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks."],"url":"http://arxiv.org/abs/2405.06394v1","category":"cs.LG"}
{"created":"2024-05-10 11:05:17","title":"Levitons in correlated nano-scale systems","abstract":"in nanoscale systems in the presence of single-electron excitations generated by Lorentzian voltage drives, termed \\textit{Levitons}. These excitations allow to realize the analog of quantum optics experiments using electrons instead of photons. Importantly, electrons in condensed matter systems are strongly affected by the presence of different types of non-trivial correlations, with no counterpart in the domain of photonic quantum optics. After providing a short introduction about Levitons in non-interacting systems, we focus on how they operate in the presence of two types of strong electronic correlations in nanoscale systems, such as those arising in the fractional quantum Hall effect or in superconducting systems. Specifically, we consider Levitons in a quantum Hall bar of the fractional quantum Hall effect, pinched by a quantum point contact, where anyons with fractional charge and statistics tunnel between opposite edges. In this case, a Leviton-Leviton interaction can be induced by the strongly correlated background. Concerning the effect of superconducting correlations on Levitons, we show that, in a normal metal system coupled to BCS superconductors, half integer Levitons minimize the excess noise in the Andreev regime. Interestingly, energy-entangled electron states can be realized on-demand in this type of hybrid setups by exploiting crossed Andreev reflection. The results exposed in this review have potential applications in the context of quantum information and computation with single-electron flying qubits.","sentences":["in nanoscale systems in the presence of single-electron excitations generated by Lorentzian voltage drives, termed \\textit{Levitons}.","These excitations allow to realize the analog of quantum optics experiments using electrons instead of photons.","Importantly, electrons in condensed matter systems are strongly affected by the presence of different types of non-trivial correlations, with no counterpart in the domain of photonic quantum optics.","After providing a short introduction about Levitons in non-interacting systems, we focus on how they operate in the presence of two types of strong electronic correlations in nanoscale systems, such as those arising in the fractional quantum Hall effect or in superconducting systems.","Specifically, we consider Levitons in a quantum Hall bar of the fractional quantum Hall effect, pinched by a quantum point contact, where anyons with fractional charge and statistics tunnel between opposite edges.","In this case, a Leviton-Leviton interaction can be induced by the strongly correlated background.","Concerning the effect of superconducting correlations on Levitons, we show that, in a normal metal system coupled to BCS superconductors, half integer Levitons minimize the excess noise in the Andreev regime.","Interestingly, energy-entangled electron states can be realized on-demand in this type of hybrid setups by exploiting crossed Andreev reflection.","The results exposed in this review have potential applications in the context of quantum information and computation with single-electron flying qubits."],"url":"http://arxiv.org/abs/2405.06392v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-10 11:03:15","title":"Primordial Black Hole clusters, phenomenology & implications","abstract":"We present direct N-body simulations of black-hole-only clusters with up to $2 \\cdot 10^4$ compact objects, zero natal spin and no primordial binaries as predicted by various primordial black hole (PBH) Dark Matter models. The clusters' evolution is computed using ${\\tt NBODY6\\!+\\!+GPU}$, including the effects of the tidal field of the galaxy, the kicks of black hole mergers and orbit-averaged energy loss by gravitational radiation of binaries. We investigate clusters with four initial mass distributions, three of which attempt to model a generic PBH scenario using a lognormal mass distribution and a fourth one that can be directly linked to a monochromatic PBH scenario when accretion is considered. More specifically, we dive into the clusters' internal dynamics, describing their expansion and evaporation, along with the resultant binary black hole mergers. We also compare several simulations with and without black hole merger kicks and find modelling implications for the probability of hierarchical mergers.","sentences":["We present direct N-body simulations of black-hole-only clusters with up to $2 \\cdot 10^4$ compact objects, zero natal spin and no primordial binaries as predicted by various primordial black hole (PBH) Dark Matter models.","The clusters' evolution is computed using ${\\tt NBODY6\\!+\\!+GPU}$, including the effects of the tidal field of the galaxy, the kicks of black hole mergers and orbit-averaged energy loss by gravitational radiation of binaries.","We investigate clusters with four initial mass distributions, three of which attempt to model a generic PBH scenario using a lognormal mass distribution and a fourth one that can be directly linked to a monochromatic PBH scenario when accretion is considered.","More specifically, we dive into the clusters' internal dynamics, describing their expansion and evaporation, along with the resultant binary black hole mergers.","We also compare several simulations with and without black hole merger kicks and find modelling implications for the probability of hierarchical mergers."],"url":"http://arxiv.org/abs/2405.06391v1","category":"astro-ph.CO"}
{"created":"2024-05-10 10:58:30","title":"Quasinormal Frequencies of Fields with Various Spin in the Quantum Oppenheimer-Snyder Model of Black Holes","abstract":"A recent development involves an intriguing model of a quantum-corrected black hole, established through the application of the quantum Oppenheimer-Snyder model within loop quantum cosmology [Lewandowski et al., Phys. Rev. Lett. 130 (2023) 10, 101501]. Employing both time-domain integration and the WKB approach, we compute the quasinormal frequencies for scalar, electromagnetic, and neutrino perturbations in these quantum-corrected black holes. Our analysis reveals that while the real oscillation frequencies undergo only minor adjustments due to the quantum parameter, the damping rate experiences a significant decrease as a result of its influence. We also deduce the analytic formula for quasinormal frequencies in the eikonal limit and show that the correspondence between the null geodesics and eikoanl quasinormal modes holds in this case.","sentences":["A recent development involves an intriguing model of a quantum-corrected black hole, established through the application of the quantum Oppenheimer-Snyder model within loop quantum cosmology","[Lewandowski et al., Phys.","Rev. Lett.","130 (2023) 10, 101501].","Employing both time-domain integration and the WKB approach, we compute the quasinormal frequencies for scalar, electromagnetic, and neutrino perturbations in these quantum-corrected black holes.","Our analysis reveals that while the real oscillation frequencies undergo only minor adjustments due to the quantum parameter, the damping rate experiences a significant decrease as a result of its influence.","We also deduce the analytic formula for quasinormal frequencies in the eikonal limit and show that the correspondence between the null geodesics and eikoanl quasinormal modes holds in this case."],"url":"http://arxiv.org/abs/2405.06390v1","category":"gr-qc"}
{"created":"2024-05-10 10:52:22","title":"Continual Novel Class Discovery via Feature Enhancement and Adaptation","abstract":"Continual Novel Class Discovery (CNCD) aims to continually discover novel classes without labels while maintaining the recognition capability for previously learned classes. The main challenges faced by CNCD include the feature-discrepancy problem, the inter-session confusion problem, etc. In this paper, we propose a novel Feature Enhancement and Adaptation method for the CNCD to tackle the above challenges, which consists of a guide-to-novel framework, a centroid-to-samples similarity constraint (CSS), and a boundary-aware prototype constraint (BAP). More specifically, the guide-to-novel framework is established to continually discover novel classes under the guidance of prior distribution. Afterward, the CSS is designed to constrain the relationship between centroid-to-samples similarities of different classes, thereby enhancing the distinctiveness of features among novel classes. Finally, the BAP is proposed to keep novel class features aware of the positions of other class prototypes during incremental sessions, and better adapt novel class features to the shared feature space. Experimental results on three benchmark datasets demonstrate the superiority of our method, especially in more challenging protocols with more incremental sessions.","sentences":["Continual Novel Class Discovery (CNCD) aims to continually discover novel classes without labels while maintaining the recognition capability for previously learned classes.","The main challenges faced by CNCD include the feature-discrepancy problem, the inter-session confusion problem, etc.","In this paper, we propose a novel Feature Enhancement and Adaptation method for the CNCD to tackle the above challenges, which consists of a guide-to-novel framework, a centroid-to-samples similarity constraint (CSS), and a boundary-aware prototype constraint (BAP).","More specifically, the guide-to-novel framework is established to continually discover novel classes under the guidance of prior distribution.","Afterward, the CSS is designed to constrain the relationship between centroid-to-samples similarities of different classes, thereby enhancing the distinctiveness of features among novel classes.","Finally, the BAP is proposed to keep novel class features aware of the positions of other class prototypes during incremental sessions, and better adapt novel class features to the shared feature space.","Experimental results on three benchmark datasets demonstrate the superiority of our method, especially in more challenging protocols with more incremental sessions."],"url":"http://arxiv.org/abs/2405.06389v1","category":"cs.CV"}
{"created":"2024-05-10 10:44:29","title":"How to Augment for Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models?","abstract":"Atmospheric turbulence poses a significant challenge to the performance of object detection models. Turbulence causes distortions, blurring, and noise in images by bending and scattering light rays due to variations in the refractive index of air. This results in non-rigid geometric distortions and temporal fluctuations in the electromagnetic radiation received by optical systems. This paper explores the effectiveness of turbulence image augmentation techniques in improving the accuracy and robustness of thermal-adapted and deep learning-based object detection models under atmospheric turbulence. Three distinct approximation-based turbulence simulators (geometric, Zernike-based, and P2S) are employed to generate turbulent training and test datasets. The performance of three state-of-the-art deep learning-based object detection models: RTMDet-x, DINO-4scale, and YOLOv8-x, is employed on these turbulent datasets with and without turbulence augmentation during training. The results demonstrate that utilizing turbulence-specific augmentations during model training can significantly improve detection accuracy and robustness against distorted turbulent images. Turbulence augmentation enhances performance even for a non-turbulent test set.","sentences":["Atmospheric turbulence poses a significant challenge to the performance of object detection models.","Turbulence causes distortions, blurring, and noise in images by bending and scattering light rays due to variations in the refractive index of air.","This results in non-rigid geometric distortions and temporal fluctuations in the electromagnetic radiation received by optical systems.","This paper explores the effectiveness of turbulence image augmentation techniques in improving the accuracy and robustness of thermal-adapted and deep learning-based object detection models under atmospheric turbulence.","Three distinct approximation-based turbulence simulators (geometric, Zernike-based, and P2S) are employed to generate turbulent training and test datasets.","The performance of three state-of-the-art deep learning-based object detection models: RTMDet-x, DINO-4scale, and YOLOv8-x, is employed on these turbulent datasets with and without turbulence augmentation during training.","The results demonstrate that utilizing turbulence-specific augmentations during model training can significantly improve detection accuracy and robustness against distorted turbulent images.","Turbulence augmentation enhances performance even for a non-turbulent test set."],"url":"http://arxiv.org/abs/2405.06383v1","category":"cs.CV"}
{"created":"2024-05-10 10:42:52","title":"Autoresonant laser acceleration of electrons in a strongly magnetized plasma solenoid","abstract":"Direct laser acceleration of electrons is considered in a strongly magnetized plasmoid with the magnetic field strength allowing for reaching the auto-resonance. The plasmoid may be optically created by irradiation of specially designed targets with an auxiliary intense laser beam at the previous stage of interaction in a possible all-optical setup. Specifics of the strongly magnetized plasma solenoid may be critically important for the resonant processes where a small deviation of the parameters destroys the matching conditions. The process of the autoresonant electron acceleration is analyzed for different configurations inherent in the possible realizations of the setup, estimates for the efficiency of acceleration and resonance magnetic fields are proposed. Despite the modifications of the autoresonance conditions and the presence of collective effects, generation of energetic electron bunches in a realistic setup may be possible and effective.","sentences":["Direct laser acceleration of electrons is considered in a strongly magnetized plasmoid with the magnetic field strength allowing for reaching the auto-resonance.","The plasmoid may be optically created by irradiation of specially designed targets with an auxiliary intense laser beam at the previous stage of interaction in a possible all-optical setup.","Specifics of the strongly magnetized plasma solenoid may be critically important for the resonant processes where a small deviation of the parameters destroys the matching conditions.","The process of the autoresonant electron acceleration is analyzed for different configurations inherent in the possible realizations of the setup, estimates for the efficiency of acceleration and resonance magnetic fields are proposed.","Despite the modifications of the autoresonance conditions and the presence of collective effects, generation of energetic electron bunches in a realistic setup may be possible and effective."],"url":"http://arxiv.org/abs/2405.06381v1","category":"physics.plasm-ph"}
{"created":"2024-05-10 10:25:21","title":"The Evolution of Language and Human Rationality","abstract":"If language evolved by sexual selection to display superior intelligence, then we require conversational skills, to impress other people, gain high social status, and get a mate. Conversational skills include a Theory of Mind, a sense of self, self esteem and social emotions. To be impressive, we must converse fluently and fast. The syntax of an utterance is defined by fast unification of feature structures. The pragmatic skills of conversation are also learned and deployed as feature structures; we rehearse conversations as verbal thoughts. Many aspects of our mental lives (such as our Theory of Mind, and our social emotions) work by fast, pre conscious unification of learned feature structures, rather than rational deliberation. As we think, we use the Fast Theory of Mind to infer (unreliably) how a Shadow Audience will regard what we think, say, and do. These forces, which determine our motivations and actions, are less rational and deliberate than we like to suppose","sentences":["If language evolved by sexual selection to display superior intelligence, then we require conversational skills, to impress other people, gain high social status, and get a mate.","Conversational skills include a Theory of Mind, a sense of self, self esteem and social emotions.","To be impressive, we must converse fluently and fast.","The syntax of an utterance is defined by fast unification of feature structures.","The pragmatic skills of conversation are also learned and deployed as feature structures; we rehearse conversations as verbal thoughts.","Many aspects of our mental lives (such as our Theory of Mind, and our social emotions) work by fast, pre conscious unification of learned feature structures, rather than rational deliberation.","As we think, we use the Fast Theory of Mind to infer (unreliably) how a Shadow Audience will regard what we think, say, and do.","These forces, which determine our motivations and actions, are less rational and deliberate than we like to suppose"],"url":"http://arxiv.org/abs/2405.06377v1","category":"q-bio.NC"}
{"created":"2024-05-10 10:19:14","title":"LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play","abstract":"Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.","sentences":["Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions.","To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives.","To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers.","Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs.","We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study.","Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics."],"url":"http://arxiv.org/abs/2405.06373v1","category":"cs.CL"}
{"created":"2024-05-10 10:16:27","title":"Intelligent Duty Cycling Management and Wake-up for Energy Harvesting IoT Networks with Correlated Activity","abstract":"This paper presents an approach for energy-neutral Internet of Things (IoT) scenarios where the IoT devices (IoTDs) rely entirely on their energy harvesting capabilities to sustain operation. We use a Markov chain to represent the operation and transmission states of the IoTDs, a modulated Poisson process to model their energy harvesting process, and a discrete-time Markov chain to model their battery state. The aim is to efficiently manage the duty cycling of the IoTDs, so as to prolong their battery life and reduce instances of low-energy availability. We propose a duty-cycling management based on K- nearest neighbors, aiming to strike a trade-off between energy efficiency and detection accuracy. This is done by incorporating spatial and temporal correlations among IoTDs' activity, as well as their energy harvesting capabilities. We also allow the base station to wake up specific IoTDs if more information about an event is needed upon initial detection. Our proposed scheme shows significant improvements in energy savings and performance, with up to 11 times lower misdetection probability and 50\\% lower energy consumption for high-density scenarios compared to a random duty cycling benchmark.","sentences":["This paper presents an approach for energy-neutral Internet of Things (IoT) scenarios where the IoT devices (IoTDs) rely entirely on their energy harvesting capabilities to sustain operation.","We use a Markov chain to represent the operation and transmission states of the IoTDs, a modulated Poisson process to model their energy harvesting process, and a discrete-time Markov chain to model their battery state.","The aim is to efficiently manage the duty cycling of the IoTDs, so as to prolong their battery life and reduce instances of low-energy availability.","We propose a duty-cycling management based on K- nearest neighbors, aiming to strike a trade-off between energy efficiency and detection accuracy.","This is done by incorporating spatial and temporal correlations among IoTDs' activity, as well as their energy harvesting capabilities.","We also allow the base station to wake up specific IoTDs if more information about an event is needed upon initial detection.","Our proposed scheme shows significant improvements in energy savings and performance, with up to 11 times lower misdetection probability and 50\\% lower energy consumption for high-density scenarios compared to a random duty cycling benchmark."],"url":"http://arxiv.org/abs/2405.06372v1","category":"eess.SY"}
{"created":"2024-05-10 10:13:19","title":"Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns","abstract":"Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice. While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear. This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on secure software development. We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers. We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development. Our analysis of the interviews and Reddit posts finds that despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection. Their overall mistrust leads to checking AI suggestions in similar ways to human code, although they expect improvements and, therefore, a heavier use for security tasks in the future. We conclude with recommendations for software professionals to critically check AI suggestions, AI creators to improve suggestion security and capabilities for ethical security tasks, and academic researchers to consider general-purpose AI in software development.","sentences":["Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice.","While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear.","This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on secure software development.","We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers.","We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development.","Our analysis of the interviews and Reddit posts finds that despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection.","Their overall mistrust leads to checking AI suggestions in similar ways to human code, although they expect improvements and, therefore, a heavier use for security tasks in the future.","We conclude with recommendations for software professionals to critically check AI suggestions, AI creators to improve suggestion security and capabilities for ethical security tasks, and academic researchers to consider general-purpose AI in software development."],"url":"http://arxiv.org/abs/2405.06371v1","category":"cs.CR"}
{"created":"2024-05-10 10:10:37","title":"DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under Differentially Private Federated Learning using Dynamic Low-Rank Adaptation","abstract":"Federated learning (FL) allows clients in an Internet of Things (IoT) system to collaboratively train a global model without sharing their local data with a server. However, clients' contributions to the server can still leak sensitive information. Differential privacy (DP) addresses such leakage by providing formal privacy guarantees, with mechanisms that add randomness to the clients' contributions. The randomness makes it infeasible to train large transformer-based models, common in modern IoT systems. In this work, we empirically evaluate the practicality of fine-tuning large scale on-device transformer-based models with differential privacy in a federated learning system. We conduct comprehensive experiments on various system properties for tasks spanning a multitude of domains: speech recognition, computer vision (CV) and natural language understanding (NLU). Our results show that full fine-tuning under differentially private federated learning (DP-FL) generally leads to huge performance degradation which can be alleviated by reducing the dimensionality of contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks of existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA) consistently outperforms other methods. An even more promising approach, DyLoRA, which makes the low rank variable, when naively combined with FL would straightforwardly break differential privacy. We therefore propose an adaptation method that can be combined with differential privacy and call it DP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word error rate (WER) increase due to DP to less than 2% and 7% respectively with 1 million clients and a stringent privacy budget of {\\epsilon}=2.","sentences":["Federated learning (FL) allows clients in an Internet of Things (IoT) system to collaboratively train a global model without sharing their local data with a server.","However, clients' contributions to the server can still leak sensitive information.","Differential privacy (DP) addresses such leakage by providing formal privacy guarantees, with mechanisms that add randomness to the clients' contributions.","The randomness makes it infeasible to train large transformer-based models, common in modern IoT systems.","In this work, we empirically evaluate the practicality of fine-tuning large scale on-device transformer-based models with differential privacy in a federated learning system.","We conduct comprehensive experiments on various system properties for tasks spanning a multitude of domains: speech recognition, computer vision (CV) and natural language understanding (NLU).","Our results show that full fine-tuning under differentially private federated learning (DP-FL) generally leads to huge performance degradation which can be alleviated by reducing the dimensionality of contributions through parameter-efficient fine-tuning (PEFT).","Our benchmarks of existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA) consistently outperforms other methods.","An even more promising approach, DyLoRA, which makes the low rank variable, when naively combined with FL would straightforwardly break differential privacy.","We therefore propose an adaptation method that can be combined with differential privacy and call it DP-DyLoRA.","Finally, we are able to reduce the accuracy degradation and word error rate (WER) increase due to DP to less than 2% and 7% respectively with 1 million clients and a stringent privacy budget of {\\epsilon}=2."],"url":"http://arxiv.org/abs/2405.06368v1","category":"cs.LG"}
{"created":"2024-05-10 10:06:11","title":"Accounting for selection biases in population analyses: equivalence of the in-likelihood and post-processing approaches","abstract":"In this paper I show the equivalence, under appropriate assumptions, of two alternative methods to account for the presence of selection biases (also called selection effects) in population studies: one is to include the selection effects in the likelihood directly; the other follows the procedure of first inferring the observed distribution and then removing selection effects a posteriori. Moreover, I investigate a potential bias allegedly induced by the latter approach: I show that this procedure, if applied under the appropriate assumptions, does not produce the aforementioned bias.","sentences":["In this paper I show the equivalence, under appropriate assumptions, of two alternative methods to account for the presence of selection biases (also called selection effects) in population studies: one is to include the selection effects in the likelihood directly; the other follows the procedure of first inferring the observed distribution and then removing selection effects a posteriori.","Moreover, I investigate a potential bias allegedly induced by the latter approach: I show that this procedure, if applied under the appropriate assumptions, does not produce the aforementioned bias."],"url":"http://arxiv.org/abs/2405.06366v1","category":"stat.ME"}
{"created":"2024-05-10 09:58:47","title":"Projection by Convolution: Optimal Sample Complexity for Reinforcement Learning in Continuous-Space MDPs","abstract":"We consider the problem of learning an $\\varepsilon$-optimal policy in a general class of continuous-space Markov decision processes (MDPs) having smooth Bellman operators. Given access to a generative model, we achieve rate-optimal sample complexity by performing a simple, \\emph{perturbed} version of least-squares value iteration with orthogonal trigonometric polynomials as features. Key to our solution is a novel projection technique based on ideas from harmonic analysis. Our~$\\widetilde{\\mathcal{O}}(\\epsilon^{-2-d/(\\nu+1)})$ sample complexity, where $d$ is the dimension of the state-action space and $\\nu$ the order of smoothness, recovers the state-of-the-art result of discretization approaches for the special case of Lipschitz MDPs $(\\nu=0)$. At the same time, for $\\nu\\to\\infty$, it recovers and greatly generalizes the $\\mathcal{O}(\\epsilon^{-2})$ rate of low-rank MDPs, which are more amenable to regression approaches. In this sense, our result bridges the gap between two popular but conflicting perspectives on continuous-space MDPs.","sentences":["We consider the problem of learning an $\\varepsilon$-optimal policy in a general class of continuous-space Markov decision processes (MDPs) having smooth Bellman operators.","Given access to a generative model, we achieve rate-optimal sample complexity by performing a simple, \\emph{perturbed} version of least-squares value iteration with orthogonal trigonometric polynomials as features.","Key to our solution is a novel projection technique based on ideas from harmonic analysis.","Our~$\\widetilde{\\mathcal{O}}(\\epsilon^{-2-d/(\\nu+1)})$ sample complexity, where $d$ is the dimension of the state-action space and $\\nu$ the order of smoothness, recovers the state-of-the-art result of discretization approaches for the special case of Lipschitz MDPs $(\\nu=0)$. At the same time, for $\\nu\\to\\infty$, it recovers and greatly generalizes the $\\mathcal{O}(\\epsilon^{-2})$ rate of low-rank MDPs, which are more amenable to regression approaches.","In this sense, our result bridges the gap between two popular but conflicting perspectives on continuous-space MDPs."],"url":"http://arxiv.org/abs/2405.06363v1","category":"cs.LG"}
{"created":"2024-05-10 09:46:08","title":"Madelung Mechanics and Superoscillations","abstract":"In single-particle Madelung mechanics, the single-particle quantum state $\\Psi(\\vec{x},t) = R(\\vec{x},t) e^{iS(\\vec{x},t)/\\hbar}$ is interpreted as comprising an entire conserved fluid of classical point particles, with local density $R(\\vec{x},t)^2$ and local momentum $\\vec{\\nabla}S(\\vec{x},t)$ (where $R$ and $S$ are real). The Schr\\\"{o}dinger equation gives rise to the continuity equation for the fluid, and the Hamilton-Jacobi equation for particles of the fluid, which includes a new density-dependent quantum potential energy term $Q(\\vec{x},t) = -\\frac{\\hbar^2}{2m}\\frac{\\vec{\\nabla}R(\\vec{x},t)}{R(\\vec{x},t)}$, which is all that makes the fluid behavior nonclassical. In particular, the quantum potential can become negative and create a nonclassical boost in the kinetic energy. This boost is related to superoscillations in the wavefunction, where the local frequency of $\\Psi$ exceeds its global band limit. Berry showed that for states of definite energy $E$, the regions of superoscillation are exactly the regions where $Q(\\vec{x},t)<0$. For energy superposition states with band-limit $E_+$, the situation is slightly more complicated, and the bound is no longer $Q(\\vec{x},t)<0$. However, the fluid model provides a definite local energy for each fluid particle which allows us to define a local band limit for superoscillation, and with this definition, all regions of superoscillation are again regions where $Q(\\vec{x},t)<0$ for general superpositions. Detailed examples are given which illustrate the role of the quantum potential and superoscillations in a range of scenarios.","sentences":["In single-particle Madelung mechanics, the single-particle quantum state $\\Psi(\\vec{x},t) = R(\\vec{x},t) e^{iS(\\vec{x},t)/\\hbar}$ is interpreted as comprising an entire conserved fluid of classical point particles, with local density $R(\\vec{x},t)^2$ and local momentum $\\vec{\\nabla}S(\\vec{x},t)$ (where $R$ and $S$ are real).","The Schr\\\"{o}dinger equation gives rise to the continuity equation for the fluid, and the Hamilton-Jacobi equation for particles of the fluid, which includes a new density-dependent quantum potential energy term $Q(\\vec{x},t) = -\\frac{\\hbar^2}{2m}\\frac{\\vec{\\nabla}R(\\vec{x},t)}{R(\\vec{x},t)}$, which is all that makes the fluid behavior nonclassical.","In particular, the quantum potential can become negative and create a nonclassical boost in the kinetic energy.","This boost is related to superoscillations in the wavefunction, where the local frequency of $\\Psi$ exceeds its global band limit.","Berry showed that for states of definite energy $E$, the regions of superoscillation are exactly the regions where $Q(\\vec{x},t)<0$. For energy superposition states with band-limit $E_+$, the situation is slightly more complicated, and the bound is no longer $Q(\\vec{x},t)<0$. However, the fluid model provides a definite local energy for each fluid particle which allows us to define a local band limit for superoscillation, and with this definition, all regions of superoscillation are again regions where $Q(\\vec{x},t)<0$ for general superpositions.","Detailed examples are given which illustrate the role of the quantum potential and superoscillations in a range of scenarios."],"url":"http://arxiv.org/abs/2405.06358v1","category":"quant-ph"}
{"created":"2024-05-10 09:39:12","title":"CRATOR: a Dark Web Crawler","abstract":"Dark web crawling is a complex process that involves specific methodologies and techniques to navigate the Tor network and extract data from hidden services. This study proposes a general dark web crawler designed to extract pages handling security protocols, such as captchas, efficiently. Our approach uses a combination of seed URL lists, link analysis, and scanning to discover new content. We also incorporate methods for user-agent rotation and proxy usage to maintain anonymity and avoid detection. We evaluate the effectiveness of our crawler using metrics such as coverage, performance and robustness. Our results demonstrate that our crawler effectively extracts pages handling security protocols while maintaining anonymity and avoiding detection. Our proposed dark web crawler can be used for various applications, including threat intelligence, cybersecurity, and online investigations.","sentences":["Dark web crawling is a complex process that involves specific methodologies and techniques to navigate the Tor network and extract data from hidden services.","This study proposes a general dark web crawler designed to extract pages handling security protocols, such as captchas, efficiently.","Our approach uses a combination of seed URL lists, link analysis, and scanning to discover new content.","We also incorporate methods for user-agent rotation and proxy usage to maintain anonymity and avoid detection.","We evaluate the effectiveness of our crawler using metrics such as coverage, performance and robustness.","Our results demonstrate that our crawler effectively extracts pages handling security protocols while maintaining anonymity and avoiding detection.","Our proposed dark web crawler can be used for various applications, including threat intelligence, cybersecurity, and online investigations."],"url":"http://arxiv.org/abs/2405.06356v1","category":"cs.CR"}
{"created":"2024-05-10 09:38:50","title":"Switched Vector Field-based Guidance for General Reference Path Following in Planar Environment","abstract":"Reference path following is a key component in the functioning of almost all engineered autonomous agents. Among several path following guidance methods in existing literature, vector-field-based guidance approach has got wide attention because of its simplicity and guarantee of stability under a broad class of scenarios. However, the usage of same cross-track-error-dependent structure of desired vector field in most of the existing literature irrespective of instantaneous cross-track error and course angle of unmanned vehicle makes it quite restrictive in attaining faster convergence and also leads to infeasibly high turn rate command for many scenarios. To this end, this paper presents a novel switched vector field-based guidance for following a general reference path, in which the structure of the desired vector field depends on instantaneous cross-track-error and vehicle's course angle. While the developed method ensures faster convergence, it also ensures that the guidance command always stays within a realistic threshold satisfying its curvature constraint, thus making it more real-life implementable for autonomous vehicles with kino-dynamic constraints. Theoretical analysis for convergence of the developed guidance scheme is presented. Possibilities of undesirable chattering at phase transitions are also eliminated. Numerical simulation studies are presented to validate the satisfactory performance of the developed algorithm.","sentences":["Reference path following is a key component in the functioning of almost all engineered autonomous agents.","Among several path following guidance methods in existing literature, vector-field-based guidance approach has got wide attention because of its simplicity and guarantee of stability under a broad class of scenarios.","However, the usage of same cross-track-error-dependent structure of desired vector field in most of the existing literature irrespective of instantaneous cross-track error and course angle of unmanned vehicle makes it quite restrictive in attaining faster convergence and also leads to infeasibly high turn rate command for many scenarios.","To this end, this paper presents a novel switched vector field-based guidance for following a general reference path, in which the structure of the desired vector field depends on instantaneous cross-track-error and vehicle's course angle.","While the developed method ensures faster convergence, it also ensures that the guidance command always stays within a realistic threshold satisfying its curvature constraint, thus making it more real-life implementable for autonomous vehicles with kino-dynamic constraints.","Theoretical analysis for convergence of the developed guidance scheme is presented.","Possibilities of undesirable chattering at phase transitions are also eliminated.","Numerical simulation studies are presented to validate the satisfactory performance of the developed algorithm."],"url":"http://arxiv.org/abs/2405.06355v1","category":"eess.SY"}
{"created":"2024-05-10 09:37:36","title":"KeepOriginalAugment: Single Image-based Better Information-Preserving Data Augmentation Approach","abstract":"Advanced image data augmentation techniques play a pivotal role in enhancing the training of models for diverse computer vision tasks. Notably, SalfMix and KeepAugment have emerged as popular strategies, showcasing their efficacy in boosting model performance. However, SalfMix reliance on duplicating salient features poses a risk of overfitting, potentially compromising the model's generalization capabilities. Conversely, KeepAugment, which selectively preserves salient regions and augments non-salient ones, introduces a domain shift that hinders the exchange of crucial contextual information, impeding overall model understanding. In response to these challenges, we introduce KeepOriginalAugment, a novel data augmentation approach. This method intelligently incorporates the most salient region within the non-salient area, allowing augmentation to be applied to either region. Striking a balance between data diversity and information preservation, KeepOriginalAugment enables models to leverage both diverse salient and non-salient regions, leading to enhanced performance. We explore three strategies for determining the placement of the salient region minimum, maximum, or random and investigate swapping perspective strategies to decide which part (salient or non-salient) undergoes augmentation. Our experimental evaluations, conducted on classification datasets such as CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate the superior performance of KeepOriginalAugment compared to existing state-of-the-art techniques.","sentences":["Advanced image data augmentation techniques play a pivotal role in enhancing the training of models for diverse computer vision tasks.","Notably, SalfMix and KeepAugment have emerged as popular strategies, showcasing their efficacy in boosting model performance.","However, SalfMix reliance on duplicating salient features poses a risk of overfitting, potentially compromising the model's generalization capabilities.","Conversely, KeepAugment, which selectively preserves salient regions and augments non-salient ones, introduces a domain shift that hinders the exchange of crucial contextual information, impeding overall model understanding.","In response to these challenges, we introduce KeepOriginalAugment, a novel data augmentation approach.","This method intelligently incorporates the most salient region within the non-salient area, allowing augmentation to be applied to either region.","Striking a balance between data diversity and information preservation, KeepOriginalAugment enables models to leverage both diverse salient and non-salient regions, leading to enhanced performance.","We explore three strategies for determining the placement of the salient region minimum, maximum, or random and investigate swapping perspective strategies to decide which part (salient or non-salient) undergoes augmentation.","Our experimental evaluations, conducted on classification datasets such as CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate the superior performance of KeepOriginalAugment compared to existing state-of-the-art techniques."],"url":"http://arxiv.org/abs/2405.06354v1","category":"cs.CV"}
{"created":"2024-05-10 09:31:54","title":"Next generation clinical trials: Seamless designs and master protocols","abstract":"Background: Drug development is often inefficient, costly and lengthy, yet it is essential for evaluating the safety and efficacy of new interventions. Compared with other disease areas, this is particularly true for Phase II / III cancer clinical trials where high attrition rates and reduced regulatory approvals are being seen. In response to these challenges, seamless clinical trials and master protocols have emerged to streamline the drug development process. Methods: Seamless clinical trials, characterized by their ability to transition seamlessly from one phase to another, can lead to accelerating the development of promising therapies while Master protocols provide a framework for investigating multiple treatment options and patient subgroups within a single trial. Results: We discuss the advantages of these methods through real trial examples and the principals that lead to their success while also acknowledging the associated regulatory considerations and challenges. Conclusion: Seamless designs and Master protocols have the potential to improve confirmatory clinical trials. In the disease area of cancer, this ultimately means that patients can receive life-saving treatments sooner.","sentences":["Background: Drug development is often inefficient, costly and lengthy, yet it is essential for evaluating the safety and efficacy of new interventions.","Compared with other disease areas, this is particularly true for Phase II / III cancer clinical trials where high attrition rates and reduced regulatory approvals are being seen.","In response to these challenges, seamless clinical trials and master protocols have emerged to streamline the drug development process.","Methods: Seamless clinical trials, characterized by their ability to transition seamlessly from one phase to another, can lead to accelerating the development of promising therapies while Master protocols provide a framework for investigating multiple treatment options and patient subgroups within a single trial.","Results:","We discuss the advantages of these methods through real trial examples and the principals that lead to their success while also acknowledging the associated regulatory considerations and challenges.","Conclusion: Seamless designs and Master protocols have the potential to improve confirmatory clinical trials.","In the disease area of cancer, this ultimately means that patients can receive life-saving treatments sooner."],"url":"http://arxiv.org/abs/2405.06353v1","category":"stat.ME"}
{"created":"2024-05-10 09:31:18","title":"Energy in critical collapse","abstract":"We study the energy issue in critical collapse of a spherically symmetric scalar field. It is found that in critical collapse, the contribution from the material energy is greater than that from the gravitational energy. The quantity $m/r$ plays an important role in identifying the formation of apparent horizon in gravitational collapse, where $m$ is the Misner-Sharp mass and $r$ the areal radius. We observe that in critical collapse, the maximum value of $m/r$ fluctuates between $2/15$ and $4/15$. This denotes a large gap between critical collapse and black hole formation for which the criterion is $m/r=1/2$.","sentences":["We study the energy issue in critical collapse of a spherically symmetric scalar field.","It is found that in critical collapse, the contribution from the material energy is greater than that from the gravitational energy.","The quantity $m/r$ plays an important role in identifying the formation of apparent horizon in gravitational collapse, where $m$ is the Misner-Sharp mass and $r$ the areal radius.","We observe that in critical collapse, the maximum value of $m/r$ fluctuates between $2/15$ and $4/15$. This denotes a large gap between critical collapse and black hole formation for which the criterion is $m/r=1/2$."],"url":"http://arxiv.org/abs/2405.06351v1","category":"gr-qc"}
{"created":"2024-05-10 09:28:20","title":"Local topology and perestroikas in protein structure and folding dynamics","abstract":"Methods of local topology are introduced to the field of protein physics. This is achieved by explaining how the folding and unfolding processes of a globular protein alter the local topology of the protein's C-alpha backbone through conformational bifurcations. The mathematical formulation builds on the concept of Arnol'd's perestroikas, by extending it to piecewise linear chains using the discrete Frenet frame formalism. In the low-temperature folded phase, the backbone geometry generalizes the concept of a Peano curve, with its modular building blocks modeled by soliton solutions of a discretized nonlinear Schroedinger equation. The onset of thermal unfolding begins when perestroikas change the flattening and branch points that determine the centers of solitons. When temperature increases, the perestroikas cascade, which leads to a progressive disintegration of the modular structures. The folding and unfolding processes are quantitatively characterized by a correlation function that describes the evolution of perestroikas under temperature changes. The approach provides a comprehensive framework for understanding the Physics of protein folding and unfolding transitions, contributing to the broader field of protein structure and dynamics.","sentences":["Methods of local topology are introduced to the field of protein physics.","This is achieved by explaining how the folding and unfolding processes of a globular protein alter the local topology of the protein's C-alpha backbone through conformational bifurcations.","The mathematical formulation builds on the concept of Arnol'd's perestroikas, by extending it to piecewise linear chains using the discrete Frenet frame formalism.","In the low-temperature folded phase, the backbone geometry generalizes the concept of a Peano curve, with its modular building blocks modeled by soliton solutions of a discretized nonlinear Schroedinger equation.","The onset of thermal unfolding begins when perestroikas change the flattening and branch points that determine the centers of solitons.","When temperature increases, the perestroikas cascade, which leads to a progressive disintegration of the modular structures.","The folding and unfolding processes are quantitatively characterized by a correlation function that describes the evolution of perestroikas under temperature changes.","The approach provides a comprehensive framework for understanding the Physics of protein folding and unfolding transitions, contributing to the broader field of protein structure and dynamics."],"url":"http://arxiv.org/abs/2405.06348v1","category":"physics.bio-ph"}
{"created":"2024-05-10 09:26:54","title":"Building Trust in AI-Driven Decision Making for Cyber-Physical Systems (CPS): A Comprehensive Review","abstract":"Recent advancements in technology have led to the emergence of Cyber-Physical Systems (CPS), which seamlessly integrate the cyber and physical domains in various sectors such as agriculture, autonomous systems, and healthcare. This integration presents opportunities for enhanced efficiency and automation through the utilization of artificial intelligence (AI) and machine learning (ML). However, the complexity of CPS brings forth challenges related to transparency, bias, and trust in AI-enabled decision-making processes. This research explores the significance of AI and ML in enabling CPS in these domains and addresses the challenges associated with interpreting and trusting AI systems within CPS. Specifically, the role of explainable AI (XAI) in enhancing trustworthiness and reliability in AI-enabled decision-making processes is discussed. Key challenges such as transparency, security, and privacy are identified, along with the necessity of building trust through transparency, accountability, and ethical considerations.","sentences":["Recent advancements in technology have led to the emergence of Cyber-Physical Systems (CPS), which seamlessly integrate the cyber and physical domains in various sectors such as agriculture, autonomous systems, and healthcare.","This integration presents opportunities for enhanced efficiency and automation through the utilization of artificial intelligence (AI) and machine learning (ML).","However, the complexity of CPS brings forth challenges related to transparency, bias, and trust in AI-enabled decision-making processes.","This research explores the significance of AI and ML in enabling CPS in these domains and addresses the challenges associated with interpreting and trusting AI systems within CPS.","Specifically, the role of explainable AI (XAI) in enhancing trustworthiness and reliability in AI-enabled decision-making processes is discussed.","Key challenges such as transparency, security, and privacy are identified, along with the necessity of building trust through transparency, accountability, and ethical considerations."],"url":"http://arxiv.org/abs/2405.06347v1","category":"eess.SP"}
{"created":"2024-05-10 09:20:09","title":"Controlled molecule injector for cold, dense, and pure molecular beams at the European x-ray free-electron laser","abstract":"A permanently available molecular-beam injection setup for controlled molecules (COMO) was installed and commissioned at the small quantum systems (SQS) instrument at the European x-ray free-electron laser (EuXFEL). A $b$-type electrostatic deflector allows for pure state-, size-, and isomer-selected samples of polar molecules and clusters. The source provides a rotationally cold ($T\\approx1$~K) and dense ($\\rho\\approx10^8$~cm$^{-3}$) molecular beam with pulse durations up to 100~\\us generated by a new version of the Even-Lavie valve. Here, a performance overview of the COMO setup is presented along with characterization experiments performed both, with an optical laser at the Center for Free-Electron-Laser Science and with x-rays at EuXFEL under burst-mode operation. COMO was designed to be attached to different instruments at the EuXFEL, in particular at the small quantum systems (SQS) and single particles, clusters, and biomolecules (SPB) instruments. This advanced controlled-molecules injection setup enables XFEL studies using highly defined samples with soft and hard x-ray FEL radiation for applications ranging from atomic, molecular, and cluster physics to elementary processes in chemistry and biology.","sentences":["A permanently available molecular-beam injection setup for controlled molecules (COMO) was installed and commissioned at the small quantum systems (SQS) instrument at the European x-ray free-electron laser (EuXFEL).","A $b$-type electrostatic deflector allows for pure state-, size-, and isomer-selected samples of polar molecules and clusters.","The source provides a rotationally cold ($T\\approx1$~K) and dense ($\\rho\\approx10^8$~cm$^{-3}$) molecular beam with pulse durations up to 100~\\us generated by a new version of the Even-Lavie valve.","Here, a performance overview of the COMO setup is presented along with characterization experiments performed both, with an optical laser at the Center for Free-Electron-Laser Science and with x-rays at EuXFEL under burst-mode operation.","COMO was designed to be attached to different instruments at the EuXFEL, in particular at the small quantum systems (SQS) and single particles, clusters, and biomolecules (SPB) instruments.","This advanced controlled-molecules injection setup enables XFEL studies using highly defined samples with soft and hard x-ray FEL radiation for applications ranging from atomic, molecular, and cluster physics to elementary processes in chemistry and biology."],"url":"http://arxiv.org/abs/2405.06344v1","category":"physics.chem-ph"}
{"created":"2024-05-10 09:11:41","title":"M3DIS - A grid of 3D radiation-hydrodynamics stellar atmosphere models for stellar surveys","abstract":"Large-scale stellar surveys, such as SDSS-V, 4MOST, WEAVE, and PLATO, require accurate atmospheric models and synthetic spectra of stars for accurate analyses of fundamental stellar parameters and chemical abundances. The primary goal of our work is to develop a new approach to solve radiation-hydrodynamics (RHD) and generate model stellar spectra in a self-consistent and highly efficient framework. We build upon the Copenhagen legacy RHD code, the MULTI3D non-local thermodynamic equilibrium (NLTE) code, and the DISPATCH high-performance framework. The new approach allows us to calculate 3D RHD models of stellar atmospheres on timescales of a few thousand CPU hours and to perform subsequent spectrum synthesis in local thermodynamic equilibrium (LTE) or NLTE for the desired physical conditions within the parameter space of FGK-type stars. We compare the 3D RHD solar model with other available models and validate its performance against solar observations, including the centre-to-limb variation of intensities and key solar diagnostic lines of H and Fe. We show that the performance of the new code allows to overcome the main bottleneck in 3D NLTE spectroscopy and enables calculations of multi-dimensional grids of synthetic stellar observables for comparison with modern astronomical observations.","sentences":["Large-scale stellar surveys, such as SDSS-V, 4MOST, WEAVE, and PLATO, require accurate atmospheric models and synthetic spectra of stars for accurate analyses of fundamental stellar parameters and chemical abundances.","The primary goal of our work is to develop a new approach to solve radiation-hydrodynamics (RHD) and generate model stellar spectra in a self-consistent and highly efficient framework.","We build upon the Copenhagen legacy RHD code, the MULTI3D non-local thermodynamic equilibrium (NLTE) code, and the DISPATCH high-performance framework.","The new approach allows us to calculate 3D RHD models of stellar atmospheres on timescales of a few thousand CPU hours and to perform subsequent spectrum synthesis in local thermodynamic equilibrium (LTE) or NLTE for the desired physical conditions within the parameter space of FGK-type stars.","We compare the 3D RHD solar model with other available models and validate its performance against solar observations, including the centre-to-limb variation of intensities and key solar diagnostic lines of H and Fe.","We show that the performance of the new code allows to overcome the main bottleneck in 3D NLTE spectroscopy and enables calculations of multi-dimensional grids of synthetic stellar observables for comparison with modern astronomical observations."],"url":"http://arxiv.org/abs/2405.06338v1","category":"astro-ph.SR"}
{"created":"2024-05-10 09:09:53","title":"Efficient End-to-End Detection of 6-DoF Grasps for Robotic Bin Picking","abstract":"Bin picking is an important building block for many robotic systems, in logistics, production or in household use-cases. In recent years, machine learning methods for the prediction of 6-DoF grasps on diverse and unknown objects have shown promising progress. However, existing approaches only consider a single ground truth grasp orientation at a grasp location during training and therefore can only predict limited grasp orientations which leads to a reduced number of feasible grasps in bin picking with restricted reachability. In this paper, we propose a novel approach for learning dense and diverse 6-DoF grasps for parallel-jaw grippers in robotic bin picking. We introduce a parameterized grasp distribution model based on Power-Spherical distributions that enables a training based on all possible ground truth samples. Thereby, we also consider the grasp uncertainty enhancing the model's robustness to noisy inputs. As a result, given a single top-down view depth image, our model can generate diverse grasps with multiple collision-free grasp orientations. Experimental evaluations in simulation and on a real robotic bin picking setup demonstrate the model's ability to generalize across various object categories achieving an object clearing rate of around $90 \\%$ in simulation and real-world experiments. We also outperform state of the art approaches. Moreover, the proposed approach exhibits its usability in real robot experiments without any refinement steps, even when only trained on a synthetic dataset, due to the probabilistic grasp distribution modeling.","sentences":["Bin picking is an important building block for many robotic systems, in logistics, production or in household use-cases.","In recent years, machine learning methods for the prediction of 6-DoF grasps on diverse and unknown objects have shown promising progress.","However, existing approaches only consider a single ground truth grasp orientation at a grasp location during training and therefore can only predict limited grasp orientations which leads to a reduced number of feasible grasps in bin picking with restricted reachability.","In this paper, we propose a novel approach for learning dense and diverse 6-DoF grasps for parallel-jaw grippers in robotic bin picking.","We introduce a parameterized grasp distribution model based on Power-Spherical distributions that enables a training based on all possible ground truth samples.","Thereby, we also consider the grasp uncertainty enhancing the model's robustness to noisy inputs.","As a result, given a single top-down view depth image, our model can generate diverse grasps with multiple collision-free grasp orientations.","Experimental evaluations in simulation and on a real robotic bin picking setup demonstrate the model's ability to generalize across various object categories achieving an object clearing rate of around $90 \\%$ in simulation and real-world experiments.","We also outperform state of the art approaches.","Moreover, the proposed approach exhibits its usability in real robot experiments without any refinement steps, even when only trained on a synthetic dataset, due to the probabilistic grasp distribution modeling."],"url":"http://arxiv.org/abs/2405.06336v1","category":"cs.RO"}
{"created":"2024-05-10 09:08:16","title":"A universal phenomenology of charge-spin interconversion and dynamics in diffusive systems with spin-orbit coupling","abstract":"We present an effective field theory for a unified description of transport in normal and superconducting metals in the presence of generic spin-orbit coupling (SOC). The structure of the quantum kinetic theory in the diffusive regime is determined by a set of fundamental constraints -- charge conjugation symmetry, the causality principle, and the crystal symmetry of a material. These symmetries uniquely fix the action of the Keldysh non-linear $\\sigma$ model (NLSM), which at the saddle point yields the quantum kinetic Usadel-type equation. Our phenomenological approach is reminiscent of the Ginzburg-Landau theory, but is valid for superconductors in the whole temperature range, describes the diffusive transport in the normal state, and naturally captures the effects of superconducting fluctuations. As an application, we derive the NLSM and quantum transport equations which include all effects of spin-orbit coupling, allowed by the crystal symmetry, for example, the spin Hall, spin current swapping or spin-galvanic effects. Our approach can be extended to systems with broken time reversal symmetry, as well as to the description of hybrid interfaces, where the spin-charge interconversion can be enhanced due to strong interfacial SOC.","sentences":["We present an effective field theory for a unified description of transport in normal and superconducting metals in the presence of generic spin-orbit coupling (SOC).","The structure of the quantum kinetic theory in the diffusive regime is determined by a set of fundamental constraints -- charge conjugation symmetry, the causality principle, and the crystal symmetry of a material.","These symmetries uniquely fix the action of the Keldysh non-linear $\\sigma$ model (NLSM), which at the saddle point yields the quantum kinetic Usadel-type equation.","Our phenomenological approach is reminiscent of the Ginzburg-Landau theory, but is valid for superconductors in the whole temperature range, describes the diffusive transport in the normal state, and naturally captures the effects of superconducting fluctuations.","As an application, we derive the NLSM and quantum transport equations which include all effects of spin-orbit coupling, allowed by the crystal symmetry, for example, the spin Hall, spin current swapping or spin-galvanic effects.","Our approach can be extended to systems with broken time reversal symmetry, as well as to the description of hybrid interfaces, where the spin-charge interconversion can be enhanced due to strong interfacial SOC."],"url":"http://arxiv.org/abs/2405.06334v1","category":"cond-mat.supr-con"}
{"created":"2024-05-10 09:03:27","title":"LMD3: Language Model Data Density Dependence","abstract":"We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation. Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that increasing the support in the training distribution for specific test queries results in a measurable increase in density, which is also a significant predictor of the performance increase caused by the intervention. Experiments with pretraining data demonstrate that we can explain a significant fraction of the variance in model perplexity via density measurements. We conclude that our framework can provide statistical evidence of the dependence of a target model's predictions on subsets of its training data, and can more generally be used to characterize the support (or lack thereof) in the training data for a given test task.","sentences":["We develop a methodology for analyzing language model task performance at the individual example level based on training data density estimation.","Experiments with paraphrasing as a controlled intervention on finetuning data demonstrate that increasing the support in the training distribution for specific test queries results in a measurable increase in density, which is also a significant predictor of the performance increase caused by the intervention.","Experiments with pretraining data demonstrate that we can explain a significant fraction of the variance in model perplexity via density measurements.","We conclude that our framework can provide statistical evidence of the dependence of a target model's predictions on subsets of its training data, and can more generally be used to characterize the support (or lack thereof) in the training data for a given test task."],"url":"http://arxiv.org/abs/2405.06331v1","category":"cs.LG"}
{"created":"2024-05-10 09:03:12","title":"Interpretable Multi-task Learning with Shared Variable Embeddings","abstract":"This paper proposes a general interpretable predictive system with shared information. The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure. Embeddings of input and output variables in a common space are obtained, where the input embeddings are produced through attending to a set of shared embeddings, reused across tasks. All the embeddings are treated as model parameters and learned. Specific restrictions on the space of shared embedings and the sparsity of the attention mechanism are considered. Experiments show that the introduction of shared embeddings does not deteriorate the results obtained from a vanilla variable embeddings method. We run a number of further ablations. Inducing sparsity in the attention mechanism leads to both an increase in accuracy and a significant decrease in the number of training steps required. Shared embeddings provide a measure of interpretability in terms of both a qualitative assessment and the ability to map specific shared embeddings to pre-defined concepts that are not tailored to the considered model. There seems to be a trade-off between accuracy and interpretability. The basic shared embeddings method favors interpretability, whereas the sparse attention method promotes accuracy. The results lead to the conclusion that variable embedding methods may be extended with shared information to provide increased interpretability and accuracy.","sentences":["This paper proposes a general interpretable predictive system with shared information.","The system is able to perform predictions in a multi-task setting where distinct tasks are not bound to have the same input/output structure.","Embeddings of input and output variables in a common space are obtained, where the input embeddings are produced through attending to a set of shared embeddings, reused across tasks.","All the embeddings are treated as model parameters and learned.","Specific restrictions on the space of shared embedings and the sparsity of the attention mechanism are considered.","Experiments show that the introduction of shared embeddings does not deteriorate the results obtained from a vanilla variable embeddings method.","We run a number of further ablations.","Inducing sparsity in the attention mechanism leads to both an increase in accuracy and a significant decrease in the number of training steps required.","Shared embeddings provide a measure of interpretability in terms of both a qualitative assessment and the ability to map specific shared embeddings to pre-defined concepts that are not tailored to the considered model.","There seems to be a trade-off between accuracy and interpretability.","The basic shared embeddings method favors interpretability, whereas the sparse attention method promotes accuracy.","The results lead to the conclusion that variable embedding methods may be extended with shared information to provide increased interpretability and accuracy."],"url":"http://arxiv.org/abs/2405.06330v1","category":"cs.LG"}
{"created":"2024-05-10 09:01:14","title":"ChatGPTest: opportunities and cautionary tales of utilizing AI for questionnaire pretesting","abstract":"The rapid advancements in generative artificial intelligence have opened up new avenues for enhancing various aspects of research, including the design and evaluation of survey questionnaires. However, the recent pioneering applications have not considered questionnaire pretesting. This article explores the use of GPT models as a useful tool for pretesting survey questionnaires, particularly in the early stages of survey design. Illustrated with two applications, the article suggests incorporating GPT feedback as an additional stage before human pretesting, potentially reducing successive iterations. The article also emphasizes the indispensable role of researchers' judgment in interpreting and implementing AI-generated feedback.","sentences":["The rapid advancements in generative artificial intelligence have opened up new avenues for enhancing various aspects of research, including the design and evaluation of survey questionnaires.","However, the recent pioneering applications have not considered questionnaire pretesting.","This article explores the use of GPT models as a useful tool for pretesting survey questionnaires, particularly in the early stages of survey design.","Illustrated with two applications, the article suggests incorporating GPT feedback as an additional stage before human pretesting, potentially reducing successive iterations.","The article also emphasizes the indispensable role of researchers' judgment in interpreting and implementing AI-generated feedback."],"url":"http://arxiv.org/abs/2405.06329v1","category":"cs.CY"}
{"created":"2024-05-10 09:01:08","title":"On solving Schroedingers equation with integrals over classical paths","abstract":"We show that the Schroedinger equation of quantum physics can be solved using the classical Hamilton-Jacobi action dynamics, extending a key result of Feynman applicable only to quadratic Lagrangians. This is made possible by two developments. The first is incorporating geometric constraints directly in the classical least action problem, in effect replacing in part the probabilistic setting by the non-uniqueness of solutions of the constrained problem. For instance, in the double slit experiment or for a particle in a box, spatial inequality constraints create Dirac constraint forces, which lead to multiple path solutions. The second development is a spatial rescaling of clocks, specifically designed to achieve a general equivalence between Schroedinger and Hamilton-Jacobi representations. These developments leave the results of associated Feynman path integrals unchanged, but they can greatly simplify their computation as only classical paths need to be included in the integrals, and time-slicing is avoided altogether. They also suggest a smooth transition between physics across scales.","sentences":["We show that the Schroedinger equation of quantum physics can be solved using the classical Hamilton-Jacobi action dynamics, extending a key result of Feynman applicable only to quadratic Lagrangians.","This is made possible by two developments.","The first is incorporating geometric constraints directly in the classical least action problem, in effect replacing in part the probabilistic setting by the non-uniqueness of solutions of the constrained problem.","For instance, in the double slit experiment or for a particle in a box, spatial inequality constraints create Dirac constraint forces, which lead to multiple path solutions.","The second development is a spatial rescaling of clocks, specifically designed to achieve a general equivalence between Schroedinger and Hamilton-Jacobi representations.","These developments leave the results of associated Feynman path integrals unchanged, but they can greatly simplify their computation as only classical paths need to be included in the integrals, and time-slicing is avoided altogether.","They also suggest a smooth transition between physics across scales."],"url":"http://arxiv.org/abs/2405.06328v1","category":"quant-ph"}
{"created":"2024-05-10 08:51:10","title":"Inequivalence of stochastic and Bohmian arrival times in time-of-flight experiments","abstract":"Motivated by a recent prediction [Com. Phys., 6, 195 (2023)] that time-of-flight experiments with ultracold atoms could test different interpretations of quantum mechanics, this work investigates the arrival times predicted by the stochastic interpretation, whereby quantum particles follow definite but non-deterministic and non-differentiable trajectories. The distribution of arrival times is obtained from a Fokker-Planck equation, and confirmed by direct simulation of trajectories. It is found to be in general different from the distribution predicted by the Bohmian interpretation, in which quantum particles follow definite deterministic and differentiable trajectories. This result suggests that trajectory-based interpretations of quantum mechanics could be experimentally discriminated.","sentences":["Motivated by a recent prediction [Com. Phys., 6, 195 (2023)] that time-of-flight experiments with ultracold atoms could test different interpretations of quantum mechanics, this work investigates the arrival times predicted by the stochastic interpretation, whereby quantum particles follow definite but non-deterministic and non-differentiable trajectories.","The distribution of arrival times is obtained from a Fokker-Planck equation, and confirmed by direct simulation of trajectories.","It is found to be in general different from the distribution predicted by the Bohmian interpretation, in which quantum particles follow definite deterministic and differentiable trajectories.","This result suggests that trajectory-based interpretations of quantum mechanics could be experimentally discriminated."],"url":"http://arxiv.org/abs/2405.06324v1","category":"quant-ph"}
{"created":"2024-05-10 08:50:08","title":"Open Access Battle Damage Detection via Pixel-Wise T-Test on Sentinel-1 Imagery","abstract":"In the context of recent, highly destructive conflicts in Gaza and Ukraine, reliable estimates of building damage are essential for an informed public discourse, human rights monitoring, and humanitarian aid provision. Given the contentious nature of conflict damage assessment, these estimates must be fully reproducible, explainable, and derived from open access data. This paper introduces a new method for building damage detection-- the Pixel-Wise T-Test (PWTT)-- that satisfies these conditions. Using a combination of freely-available synthetic aperture radar imagery and statistical change detection, the PWTT generates accurate conflict damage estimates across a wide area at regular time intervals. Accuracy is assessed using an original dataset of over half a million labeled building footprints spanning 12 cities across Ukraine, Palestine, Syria, and Iraq. Despite being simple and lightweight, the algorithm achieves building-level accuracy statistics (AUC=0.88 across Ukraine, 0.81 in Gaza) rivalling state of the art methods that use deep learning and high resolution imagery. The workflow is open source and deployed entirely within the Google Earth Engine environment, allowing for the generation of interactive Battle Damage Dashboards for Ukraine and Gaza that update in near-real time, allowing the public and humanitarian practitioners to immediately get estimates of damaged buildings in a given area.","sentences":["In the context of recent, highly destructive conflicts in Gaza and Ukraine, reliable estimates of building damage are essential for an informed public discourse, human rights monitoring, and humanitarian aid provision.","Given the contentious nature of conflict damage assessment, these estimates must be fully reproducible, explainable, and derived from open access data.","This paper introduces a new method for building damage detection-- the Pixel-Wise T-Test (PWTT)-- that satisfies these conditions.","Using a combination of freely-available synthetic aperture radar imagery and statistical change detection, the PWTT generates accurate conflict damage estimates across a wide area at regular time intervals.","Accuracy is assessed using an original dataset of over half a million labeled building footprints spanning 12 cities across Ukraine, Palestine, Syria, and Iraq.","Despite being simple and lightweight, the algorithm achieves building-level accuracy statistics (AUC=0.88 across Ukraine, 0.81 in Gaza) rivalling state of the art methods that use deep learning and high resolution imagery.","The workflow is open source and deployed entirely within the Google Earth Engine environment, allowing for the generation of interactive Battle Damage Dashboards for Ukraine and Gaza that update in near-real time, allowing the public and humanitarian practitioners to immediately get estimates of damaged buildings in a given area."],"url":"http://arxiv.org/abs/2405.06323v1","category":"cs.CV"}
{"created":"2024-05-10 08:48:18","title":"Laser-assisted radiative recombination beyond the dipole approximation","abstract":"A comprehensive theoretical approach to describe the electron-ion radiative recombination in the presence of intense, short laser pulses, which accounts for nondipole corrections is presented. It is based on the relativistic Coulomb-Volkov solution describing an electron in a combined Coulomb potential and a laser field, which is systematically expanded in powers of $1/c$. Thus, it allows us to trace the origin of nondipole effects observed in the spectrum of emitted radiation. Hence, as we demonstrate for high-frequency pulses assisting the process, a significant extension of the cutoff and asymmetry in angular distributions of the emitted radiation can be attributed to the electron recoil off the laser pulse. In addition, we investigate a possibility of enhancing the efficiency of the generated high-energy radiation by chirping the pulse.","sentences":["A comprehensive theoretical approach to describe the electron-ion radiative recombination in the presence of intense, short laser pulses, which accounts for nondipole corrections is presented.","It is based on the relativistic Coulomb-Volkov solution describing an electron in a combined Coulomb potential and a laser field, which is systematically expanded in powers of $1/c$.","Thus, it allows us to trace the origin of nondipole effects observed in the spectrum of emitted radiation.","Hence, as we demonstrate for high-frequency pulses assisting the process, a significant extension of the cutoff and asymmetry in angular distributions of the emitted radiation can be attributed to the electron recoil off the laser pulse.","In addition, we investigate a possibility of enhancing the efficiency of the generated high-energy radiation by chirping the pulse."],"url":"http://arxiv.org/abs/2405.06322v1","category":"quant-ph"}
{"created":"2024-05-10 08:48:03","title":"Correlation Dimension of Natural Language in a Statistical Manifold","abstract":"The correlation dimension of natural language is measured by applying the Grassberger-Procaccia algorithm to high-dimensional sequences produced by a large-scale language model. This method, previously studied only in a Euclidean space, is reformulated in a statistical manifold via the Fisher-Rao distance. Language exhibits a multifractal, with global self-similarity and a universal dimension around 6.5, which is smaller than those of simple discrete random sequences and larger than that of a Barab\\'asi-Albert process. Long memory is the key to producing self-similarity. Our method is applicable to any probabilistic model of real-world discrete sequences, and we show an application to music data.","sentences":["The correlation dimension of natural language is measured by applying the Grassberger-Procaccia algorithm to high-dimensional sequences produced by a large-scale language model.","This method, previously studied only in a Euclidean space, is reformulated in a statistical manifold via the Fisher-Rao distance.","Language exhibits a multifractal, with global self-similarity and a universal dimension around 6.5, which is smaller than those of simple discrete random sequences and larger than that of a Barab\\'asi-Albert process.","Long memory is the key to producing self-similarity.","Our method is applicable to any probabilistic model of real-world discrete sequences, and we show an application to music data."],"url":"http://arxiv.org/abs/2405.06321v1","category":"cs.CL"}
{"created":"2024-05-10 08:39:11","title":"Metasurfaces with Full Control over Asymmetric Transmission of Light","abstract":"The study of optical systems with asymmetric responses has grown significantly due to their broad application potential in various fields. In particular, Janus metasurfaces are notable for their ability to control light asymmetrically at the pixel level within thin films. However, previous demonstrations were restricted to the partial control of asymmetric transmission for a limited set of input polarizations, focusing primarily on scalar functionalities. Here, we introduce optical metasurfaces consisting of bi-layer silicon nanostructures that can achieve a fully generalized form of asymmetric transmission for any possible input polarization. Their designs owe much to our theoretical model of asymmetric optical transmission in reciprocal systems that explicates the relationship between front- and back-side Jones matrices for general cases, revealing a fundamental correlation between the polarization-direction channels of opposing sides of incidence. To practically circumvent this constraint, we propose a method to partition transmission space, enabling the realization of four distinct vector functionalities within the target volume. As a proof of concept, we experimentally demonstrate polarization-direction-multiplexed Janus vector holograms generating four vector holograms. When integrated with computational vector polarizer arrays, this approach facilitates optical encryption with a high level of obscurity. We anticipate that our mathematical framework and novel material systems for generalized asymmetric transmission may pave the way for applications such as optical computations, sensing, and imaging.","sentences":["The study of optical systems with asymmetric responses has grown significantly due to their broad application potential in various fields.","In particular, Janus metasurfaces are notable for their ability to control light asymmetrically at the pixel level within thin films.","However, previous demonstrations were restricted to the partial control of asymmetric transmission for a limited set of input polarizations, focusing primarily on scalar functionalities.","Here, we introduce optical metasurfaces consisting of bi-layer silicon nanostructures that can achieve a fully generalized form of asymmetric transmission for any possible input polarization.","Their designs owe much to our theoretical model of asymmetric optical transmission in reciprocal systems that explicates the relationship between front- and back-side Jones matrices for general cases, revealing a fundamental correlation between the polarization-direction channels of opposing sides of incidence.","To practically circumvent this constraint, we propose a method to partition transmission space, enabling the realization of four distinct vector functionalities within the target volume.","As a proof of concept, we experimentally demonstrate polarization-direction-multiplexed Janus vector holograms generating four vector holograms.","When integrated with computational vector polarizer arrays, this approach facilitates optical encryption with a high level of obscurity.","We anticipate that our mathematical framework and novel material systems for generalized asymmetric transmission may pave the way for applications such as optical computations, sensing, and imaging."],"url":"http://arxiv.org/abs/2405.06318v1","category":"physics.optics"}
{"created":"2024-05-10 08:36:15","title":"Applications of the Painlev\u00e9-Kuratowski convergence: Lipschitz functions with converging Clarke subdifferentials and convergence of sets defined by converging equations","abstract":"In this note we investigate two kinds of applications of the Painlev\\'e-Kuratowski convergence of closed sets in analysis that are motivated also by questions from singularity theory. First, we generalise to Lipschitz functions the classical theorem stating that given a sequence of smooth functions with locally uniformly convergent derivatives, we obtain the local uniform convergence of the functions themselves (provided they were convergent at one point). Next we turn to the study of the behaviour of the fibres of a given function. We prove some general real counterparts of the Hurwitz theorem from complex analysis stating that the local uniform convergence of holomorphic functions implies the convergence of their sets of zeros. From the point of view of singularity theory our two theorems concern the convergence of the sets when their descriptions are convergent. They are also of interest in approximation theory.","sentences":["In this note we investigate two kinds of applications of the Painlev\\'e-Kuratowski convergence of closed sets in analysis that are motivated also by questions from singularity theory.","First, we generalise to Lipschitz functions the classical theorem stating that given a sequence of smooth functions with locally uniformly convergent derivatives, we obtain the local uniform convergence of the functions themselves (provided they were convergent at one point).","Next we turn to the study of the behaviour of the fibres of a given function.","We prove some general real counterparts of the Hurwitz theorem from complex analysis stating that the local uniform convergence of holomorphic functions implies the convergence of their sets of zeros.","From the point of view of singularity theory our two theorems concern the convergence of the sets when their descriptions are convergent.","They are also of interest in approximation theory."],"url":"http://arxiv.org/abs/2405.06314v1","category":"math.GT"}
{"created":"2024-05-10 08:36:14","title":"Long-Time Asymptotics of the Sliced-Wasserstein Flow","abstract":"The sliced-Wasserstein flow is an evolution equation where a probability density evolves in time, advected by a velocity field computed as the average among directions in the unit sphere of the optimal transport displacements from its 1D projections to the projections of a fixed target measure. This flow happens to be the gradient flow in the usual Wasserstein space of the squared sliced-Wasserstein distance to the target. We consider the question whether in long-time the flow converges to the target (providing a positive result when the target is Gaussian) and the question of the long-time limit of the flow map obtained by following the trajectories of each particle. We prove that this limit is in general not the optimal transport map from the starting measure to the target. Both questions come from the folklore about sliced-Wasserstein and had never been properly treated.","sentences":["The sliced-Wasserstein flow is an evolution equation where a probability density evolves in time, advected by a velocity field computed as the average among directions in the unit sphere of the optimal transport displacements from its 1D projections to the projections of a fixed target measure.","This flow happens to be the gradient flow in the usual Wasserstein space of the squared sliced-Wasserstein distance to the target.","We consider the question whether in long-time the flow converges to the target (providing a positive result when the target is Gaussian) and the question of the long-time limit of the flow map obtained by following the trajectories of each particle.","We prove that this limit is in general not the optimal transport map from the starting measure to the target.","Both questions come from the folklore about sliced-Wasserstein and had never been properly treated."],"url":"http://arxiv.org/abs/2405.06313v1","category":"math.OC"}
{"created":"2024-05-10 08:34:46","title":"FedGCS: A Generative Framework for Efficient Client Selection in Federated Learning via Gradient-based Optimization","abstract":"Federated Learning faces significant challenges in statistical and system heterogeneity, along with high energy consumption, necessitating efficient client selection strategies. Traditional approaches, including heuristic and learning-based methods, fall short of addressing these complexities holistically. In response, we propose FedGCS, a novel generative client selection framework that innovatively recasts the client selection process as a generative task. Drawing inspiration from the methodologies used in large language models, FedGCS efficiently encodes abundant decision-making knowledge within a continuous representation space, enabling efficient gradient-based optimization to search for optimal client selection that will be finally output via generation. The framework comprises four steps: (1) automatic collection of diverse \"selection-score\" pair data using classical client selection methods; (2) training an encoder-evaluator-decoder framework on this data to construct a continuous representation space; (3) employing gradient-based optimization in this space for optimal client selection; (4) generating the final optimal client selection via using beam search for the well-trained decoder. FedGCS outperforms traditional methods by being more comprehensive, generalizable, and efficient, simultaneously optimizing for model performance, latency, and energy consumption. The effectiveness of FedGCS is proven through extensive experimental analyses.","sentences":["Federated Learning faces significant challenges in statistical and system heterogeneity, along with high energy consumption, necessitating efficient client selection strategies.","Traditional approaches, including heuristic and learning-based methods, fall short of addressing these complexities holistically.","In response, we propose FedGCS, a novel generative client selection framework that innovatively recasts the client selection process as a generative task.","Drawing inspiration from the methodologies used in large language models, FedGCS efficiently encodes abundant decision-making knowledge within a continuous representation space, enabling efficient gradient-based optimization to search for optimal client selection that will be finally output via generation.","The framework comprises four steps: (1) automatic collection of diverse \"selection-score\" pair data using classical client selection methods; (2) training an encoder-evaluator-decoder framework on this data to construct a continuous representation space; (3) employing gradient-based optimization in this space for optimal client selection; (4) generating the final optimal client selection via using beam search for the well-trained decoder.","FedGCS outperforms traditional methods by being more comprehensive, generalizable, and efficient, simultaneously optimizing for model performance, latency, and energy consumption.","The effectiveness of FedGCS is proven through extensive experimental analyses."],"url":"http://arxiv.org/abs/2405.06312v1","category":"cs.LG"}
{"created":"2024-05-10 08:34:45","title":"All-Optical Manipulation of Band Gap Dynamics via Electron-Phonon Coupling","abstract":"The electron-phonon coupling (EPC) is a ubiquitous interaction in condensed systems and plays a vital role in shaping the electronic properties of materials. Yet, achieving coherent manipulation of electron-phonon coupling has posed a considerable challenge. Here, employing time-resolved high-harmonic generation (tr-HHG) spectroscopy, we demonstrate the coherent manipulation of bandgap dynamics in a BaF2 crystal by precisely controlling the EPC using ultrashort light pulses. The tr-HHG spectrum perturbed by a triply degenerate phonon mode T2g, exhibits simultaneously a remarkable two-dimensional (2D) sensitivity, namely intensity domain in addition to the previously reported energy domain. The dynamic compression and enhancement of the harmonics in the intensity domain showed a {\\pi}/2 phase shift compared to the manifestation of shifts of the harmonics in the energy domain, an astounding example of a physical phenomenon being observed simultaneously in two different perspectives. To complement our experimental observations, we employed a quantum model that incorporates the EPC, successfully reproducing the results. In addition, we demonstrated complete control over the EPC strength and initial phase of the coherent phonon oscillations by varying the incident electric field polarization over crystal orientation. Our findings lay a foundation for future investigations aiming to harness and exploit the remarkable potential of EPC in solid-state systems.","sentences":["The electron-phonon coupling (EPC) is a ubiquitous interaction in condensed systems and plays a vital role in shaping the electronic properties of materials.","Yet, achieving coherent manipulation of electron-phonon coupling has posed a considerable challenge.","Here, employing time-resolved high-harmonic generation (tr-HHG) spectroscopy, we demonstrate the coherent manipulation of bandgap dynamics in a BaF2 crystal by precisely controlling the EPC using ultrashort light pulses.","The tr-HHG spectrum perturbed by a triply degenerate phonon mode T2g, exhibits simultaneously a remarkable two-dimensional (2D) sensitivity, namely intensity domain in addition to the previously reported energy domain.","The dynamic compression and enhancement of the harmonics in the intensity domain showed a {\\pi}/2 phase shift compared to the manifestation of shifts of the harmonics in the energy domain, an astounding example of a physical phenomenon being observed simultaneously in two different perspectives.","To complement our experimental observations, we employed a quantum model that incorporates the EPC, successfully reproducing the results.","In addition, we demonstrated complete control over the EPC strength and initial phase of the coherent phonon oscillations by varying the incident electric field polarization over crystal orientation.","Our findings lay a foundation for future investigations aiming to harness and exploit the remarkable potential of EPC in solid-state systems."],"url":"http://arxiv.org/abs/2405.06311v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 08:32:07","title":"Smooth Sensitivity for Geo-Privacy","abstract":"Suppose each user $i$ holds a private value $x_i$ in some metric space $(U, \\mathrm{dist})$, and an untrusted data analyst wishes to compute $\\sum_i f(x_i)$ for some function $f : U \\rightarrow \\mathbb{R}$ by asking each user to send in a privatized $f(x_i)$. This is a fundamental problem in privacy-preserving population analytics, and the local model of differential privacy (LDP) is the predominant model under which the problem has been studied. However, LDP requires any two different $x_i, x'_i$ to be $\\varepsilon$-distinguishable, which can be overly strong for geometric/numerical data. On the other hand, Geo-Privacy (GP) stipulates that the level of distinguishability be proportional to $\\mathrm{dist}(x_i, x_i')$, providing an attractive alternative notion of personal data privacy in a metric space. However, existing GP mechanisms for this problem, which add a uniform noise to either $x_i$ or $f(x_i)$, are not satisfactory. In this paper, we generalize the smooth sensitivity framework from Differential Privacy to Geo-Privacy, which allows us to add noise tailored to the hardness of the given instance. We provide definitions, mechanisms, and a generic procedure for computing the smooth sensitivity under GP equipped with a general metric. Then we present three applications: one-way and two-way threshold functions, and Gaussian kernel density estimation, to demonstrate the applicability and utility of our smooth sensitivity framework.","sentences":["Suppose each user $i$ holds a private value $x_i$ in some metric space $(U, \\mathrm{dist})$, and an untrusted data analyst wishes to compute $\\sum_i f(x_i)$ for some function $f : U \\rightarrow \\mathbb{R}$ by asking each user to send in a privatized $f(x_i)$. This is a fundamental problem in privacy-preserving population analytics, and the local model of differential privacy (LDP) is the predominant model under which the problem has been studied.","However, LDP requires any two different $x_i, x'_i$ to be $\\varepsilon$-distinguishable, which can be overly strong for geometric/numerical data.","On the other hand, Geo-Privacy (GP) stipulates that the level of distinguishability be proportional to $\\mathrm{dist}(x_i, x_i')$, providing an attractive alternative notion of personal data privacy in a metric space.","However, existing GP mechanisms for this problem, which add a uniform noise to either $x_i$ or $f(x_i)$, are not satisfactory.","In this paper, we generalize the smooth sensitivity framework from Differential Privacy to Geo-Privacy, which allows us to add noise tailored to the hardness of the given instance.","We provide definitions, mechanisms, and a generic procedure for computing the smooth sensitivity under GP equipped with a general metric.","Then we present three applications: one-way and two-way threshold functions, and Gaussian kernel density estimation, to demonstrate the applicability and utility of our smooth sensitivity framework."],"url":"http://arxiv.org/abs/2405.06307v1","category":"cs.CR"}
{"created":"2024-05-10 08:31:04","title":"A NLP Approach to \"Review Bombing\" in Metacritic PC Videogames User Ratings","abstract":"Many videogames suffer \"review bombing\" -a large volume of unusually low scores that in many cases do not reflect the real quality of the product- when rated by users. By taking Metacritic's 50,000+ user score aggregations for PC games in English language, we use a Natural Language Processing (NLP) approach to try to understand the main words and concepts appearing in such cases, reaching a 0.88 accuracy on a validation set when distinguishing between just bad ratings and review bombings. By uncovering and analyzing the patterns driving this phenomenon, these results could be used to further mitigate these situations.","sentences":["Many videogames suffer \"review bombing\" -a large volume of unusually low scores that in many cases do not reflect the real quality of the product- when rated by users.","By taking Metacritic's 50,000+ user score aggregations for PC games in English language, we use a Natural Language Processing (NLP) approach to try to understand the main words and concepts appearing in such cases, reaching a 0.88 accuracy on a validation set when distinguishing between just bad ratings and review bombings.","By uncovering and analyzing the patterns driving this phenomenon, these results could be used to further mitigate these situations."],"url":"http://arxiv.org/abs/2405.06306v1","category":"cs.CL"}
{"created":"2024-05-10 08:22:49","title":"Cooperative ISAC Networks: Opportunities and Challenges","abstract":"The integration of sensing and communication (ISAC) emerges as a cornerstone technology for the forth upcoming sixth generation era, seamlessly incorporating sensing functionality into wireless networks as a native capability. The main challenges in efficient ISAC are constituted by its limited sensing and communication coverage, as well as severe inter-cell interference. Network-level ISAC relying on multi-cell cooperation is capable of effectively expanding both the sensing and communication (S&C) coverage and of providing extra degrees of freedom (DoF) for realizing increased integration gains between S&C. In this work, we provide new considerations for ISAC networks, including new metrics, the optimization of the DoF, cooperation regimes, and highlight new S&C tradeoffs. Then, we discuss a suite of cooperative S&C architectures both at the task, as well as data, and signal levels. Furthermore, the interplay between S&C at the network level is investigated and promising research directions are outlined.","sentences":["The integration of sensing and communication (ISAC) emerges as a cornerstone technology for the forth upcoming sixth generation era, seamlessly incorporating sensing functionality into wireless networks as a native capability.","The main challenges in efficient ISAC are constituted by its limited sensing and communication coverage, as well as severe inter-cell interference.","Network-level ISAC relying on multi-cell cooperation is capable of effectively expanding both the sensing and communication (S&C) coverage and of providing extra degrees of freedom (DoF) for realizing increased integration gains between S&C. In this work, we provide new considerations for ISAC networks, including new metrics, the optimization of the DoF, cooperation regimes, and highlight new S&C tradeoffs.","Then, we discuss a suite of cooperative S&C architectures both at the task, as well as data, and signal levels.","Furthermore, the interplay between S&C at the network level is investigated and promising research directions are outlined."],"url":"http://arxiv.org/abs/2405.06305v1","category":"eess.SP"}
{"created":"2024-05-10 16:36:59","title":"The Role of Learning Algorithms in Collective Action","abstract":"Collective action in Machine Learning is the study of the control that a coordinated group can have over machine learning algorithms. While previous research has concentrated on assessing the impact of collectives against Bayes optimal classifiers, this perspective is limited, given that in reality, classifiers seldom achieve Bayes optimality and are influenced by the choice of learning algorithms along with their inherent inductive biases. In this work, we initiate the study of how the choice of the learning algorithm plays a role in the success of a collective in practical settings. Specifically, we focus on distributionally robust algorithms (DRO), popular for improving a worst group error, and on the popular stochastic gradient descent (SGD), due to its inductive bias for \"simpler\" functions. Our empirical results, supported by a theoretical foundation, show that the effective size and success of the collective are highly dependent on properties of the learning algorithm. This highlights the necessity of taking the learning algorithm into account when studying the impact of collective action in Machine learning.","sentences":["Collective action in Machine Learning is the study of the control that a coordinated group can have over machine learning algorithms.","While previous research has concentrated on assessing the impact of collectives against Bayes optimal classifiers, this perspective is limited, given that in reality, classifiers seldom achieve Bayes optimality and are influenced by the choice of learning algorithms along with their inherent inductive biases.","In this work, we initiate the study of how the choice of the learning algorithm plays a role in the success of a collective in practical settings.","Specifically, we focus on distributionally robust algorithms (DRO), popular for improving a worst group error, and on the popular stochastic gradient descent (SGD), due to its inductive bias for \"simpler\" functions.","Our empirical results, supported by a theoretical foundation, show that the effective size and success of the collective are highly dependent on properties of the learning algorithm.","This highlights the necessity of taking the learning algorithm into account when studying the impact of collective action in Machine learning."],"url":"http://arxiv.org/abs/2405.06582v1","category":"cs.LG"}
{"created":"2024-05-10 15:57:01","title":"Search for time-dependent $CP$ violation in $D^0 \\rightarrow \u03c0^+ \u03c0^- \u03c0^0$ decays","abstract":"A measurement of time-dependent $CP$ violation in $D^0 \\rightarrow \\pi^+ \\pi^- \\pi^0$ decays using a $pp$ collision data sample collected by the LHCb experiment in 2012 and from 2015 to 2018, corresponding to an integrated luminosity of 7.7$\\,\\mathrm{fb}^{-1}$, is presented. The initial flavour of each $D^0$ candidate is determined from the charge of the pion produced in the $D^*(2010)^+ \\rightarrow D^0 \\pi^+$ decay. The decay $D^0 \\rightarrow K^- \\pi^+ \\pi^0$ is used as a control channel to validate the measurement procedure. The gradient of the time-dependent $CP$ asymmetry, $\\Delta Y$, in $D^0 \\rightarrow \\pi^+ \\pi^- \\pi^0$ decays is measured to be \\begin{equation*}   \\Delta Y = (-1.3 \\pm 6.3 \\pm 2.4) \\times 10^{-4}, \\end{equation*} where the first uncertainty is statistical and the second is systematic, which is compatible with $CP$ conservation.","sentences":["A measurement of time-dependent $CP$ violation in $D^0 \\rightarrow \\pi^+ \\pi^- \\pi^0$ decays using a $pp$ collision data sample collected by the LHCb experiment in 2012 and from 2015 to 2018, corresponding to an integrated luminosity of 7.7$\\,\\mathrm{fb}^{-1}$, is presented.","The initial flavour of each $D^0$ candidate is determined from the charge of the pion produced in the $D^*(2010)^+ \\rightarrow D^0 \\pi^+$ decay.","The decay $D^0 \\rightarrow K^- \\pi^+ \\pi^0$ is used as a control channel to validate the measurement procedure.","The gradient of the time-dependent $CP$ asymmetry, $\\Delta Y$, in $D^0 \\rightarrow \\pi^+ \\pi^- \\pi^0$ decays is measured to be \\begin{equation*}   \\Delta Y = (-1.3 \\pm 6.3 \\pm 2.4)","\\times 10^{-4}, \\end{equation*} where the first uncertainty is statistical and the second is systematic, which is compatible with $CP$ conservation."],"url":"http://arxiv.org/abs/2405.06556v1","category":"hep-ex"}
{"created":"2024-05-10 13:11:07","title":"ProCIS: A Benchmark for Proactive Retrieval in Conversations","abstract":"The field of conversational information seeking, which is rapidly gaining interest in both academia and industry, is changing how we interact with search engines through natural language interactions. Existing datasets and methods are mostly evaluating reactive conversational information seeking systems that solely provide response to every query from the user. We identify a gap in building and evaluating proactive conversational information seeking systems that can monitor a multi-party human conversation and proactively engage in the conversation at an opportune moment by retrieving useful resources and suggestions. In this paper, we introduce a large-scale dataset for proactive document retrieval that consists of over 2.8 million conversations. We conduct crowdsourcing experiments to obtain high-quality and relatively complete relevance judgments through depth-k pooling. We also collect annotations related to the parts of the conversation that are related to each document, enabling us to evaluate proactive retrieval systems. We introduce normalized proactive discounted cumulative gain (npDCG) for evaluating these systems, and further provide benchmark results for a wide range of models, including a novel model we developed for this task. We believe that the developed dataset, called ProCIS, paves the path towards developing proactive conversational information seeking systems.","sentences":["The field of conversational information seeking, which is rapidly gaining interest in both academia and industry, is changing how we interact with search engines through natural language interactions.","Existing datasets and methods are mostly evaluating reactive conversational information seeking systems that solely provide response to every query from the user.","We identify a gap in building and evaluating proactive conversational information seeking systems that can monitor a multi-party human conversation and proactively engage in the conversation at an opportune moment by retrieving useful resources and suggestions.","In this paper, we introduce a large-scale dataset for proactive document retrieval that consists of over 2.8 million conversations.","We conduct crowdsourcing experiments to obtain high-quality and relatively complete relevance judgments through depth-k pooling.","We also collect annotations related to the parts of the conversation that are related to each document, enabling us to evaluate proactive retrieval systems.","We introduce normalized proactive discounted cumulative gain (npDCG) for evaluating these systems, and further provide benchmark results for a wide range of models, including a novel model we developed for this task.","We believe that the developed dataset, called ProCIS, paves the path towards developing proactive conversational information seeking systems."],"url":"http://arxiv.org/abs/2405.06460v1","category":"cs.IR"}
{"created":"2024-05-10 11:05:59","title":"Measurement of the ${e}^{+}{e}^{-}\\to p \\bar{p}\u03c0^{0}$ cross section at $\\sqrt{s}=2.1000-3.0800$ GeV","abstract":"The process $e^{+}e^{-}\\to p\\bar{p}\\pi^{0}$ is studied at 20 center-of-mass energies ranging from 2.1000 to 3.0800 GeV using 636.8 pb$^{-1}$ of data collected with the BESIII detector operating at the BEPCII collider. The Born cross sections for $e^{+}e^{-}\\to p\\bar{p}\\pi^{0}$ are measured with high precision. Since the lowest center-of-mass energy, 2.1000 GeV, is less than 90 MeV above the $p\\bar{p}\\pi^0$ energy threshold, we can probe the threshold behavior for this reaction. However, no anomalous threshold enhancement is found in the cross sections for $e^{+}e^{-}\\to p\\bar{p}\\pi^{0}$.","sentences":["The process $e^{+}e^{-}\\to p\\bar{p}\\pi^{0}$ is studied at 20 center-of-mass energies ranging from 2.1000 to 3.0800 GeV using 636.8 pb$^{-1}$ of data collected with the BESIII detector operating at the BEPCII collider.","The Born cross sections for $e^{+}e^{-}\\to p\\bar{p}\\pi^{0}$ are measured with high precision.","Since the lowest center-of-mass energy, 2.1000 GeV, is less than 90 MeV above the $p\\bar{p}\\pi^0$ energy threshold, we can probe the threshold behavior for this reaction.","However, no anomalous threshold enhancement is found in the cross sections for $e^{+}e^{-}\\to p\\bar{p}\\pi^{0}$."],"url":"http://arxiv.org/abs/2405.06393v1","category":"hep-ex"}
{"created":"2024-05-10 09:29:39","title":"Three short-period Earth-sized planets around M dwarfs discovered by TESS: TOI-5720b, TOI-6008b and TOI-6086b","abstract":"One of the main goals of the NASA's TESS (Transiting Exoplanet Survey Satellite) mission is the discovery of Earth-like planets around nearby M-dwarf stars. Here, we present the discovery and validation of three new short-period Earth-sized planets orbiting nearby M-dwarfs: TOI- 5720b, TOI-6008b and TOI-6086b. We combined TESS data, ground-based multi-color light curves, ground-based optical and near-infrared spectroscopy, and Subaru/IRD RVs data to validate the planetary candidates and constrain the physical parameters of the systems. In addition, we used archival images, high-resolution imaging, and statistical validation techniques to support the planetary validation. TOI-5720b is a planet with a radius of Rp=1.09 Re orbiting a nearby (23 pc) M2.5 host, with an orbital period of P=1.43 days. It has an equilibrium temperature of Teq=708 K and an incident flux of Sp=41.7 Se. TOI-6008b has a period of P=0.86 day, a radius of Rp=1.03 Re, an equilibrium temperature of Teq=707 K and an incident flux of Sp=41.5 Se. The host star (TOI-6008) is a nearby (36 pc) M5 with an effective temperature of Teff=3075 K. Based on the RV measurements collected with Subaru/IRD, we set a 3-sigma upper limit of Mp<4 M_Earth, thus ruling out a star or brown dwarf as the transiting companion. TOI-6086b orbits its nearby (31 pc) M3 host star (Teff=3200 K) every 1.39 days, and has a radius of Rp=1.18 Re, an equilibrium temperature of Teq=634 K and an incident flux of Sp=26.8 Se. Additional high precision radial velocity measurements are needed to derive the planetary masses and bulk densities, and to search for additional planets in the systems. Moreover, short-period earth-sized planets orbiting around nearby M-dwarfs are suitable targets for atmospheric characterization with the James Webb Space Telescope (JWST) through transmission and emission spectroscopy, and phase curve photometry.","sentences":["One of the main goals of the NASA's TESS (Transiting Exoplanet Survey Satellite) mission is the discovery of Earth-like planets around nearby M-dwarf stars.","Here, we present the discovery and validation of three new short-period Earth-sized planets orbiting nearby M-dwarfs: TOI- 5720b, TOI-6008b and TOI-6086b.","We combined TESS data, ground-based multi-color light curves, ground-based optical and near-infrared spectroscopy, and Subaru/IRD RVs data to validate the planetary candidates and constrain the physical parameters of the systems.","In addition, we used archival images, high-resolution imaging, and statistical validation techniques to support the planetary validation.","TOI-5720b is a planet with a radius of Rp=1.09 Re orbiting a nearby (23 pc) M2.5 host, with an orbital period of P=1.43 days.","It has an equilibrium temperature of Teq=708 K and an incident flux of Sp=41.7 Se.","TOI-6008b has a period of P=0.86 day, a radius of Rp=1.03 Re, an equilibrium temperature of Teq=707 K and an incident flux of Sp=41.5 Se.","The host star (TOI-6008) is a nearby (36 pc) M5 with an effective temperature of Teff=3075 K. Based on the RV measurements collected with Subaru/IRD, we set a 3-sigma upper limit of Mp<4 M_Earth, thus ruling out a star or brown dwarf as the transiting companion.","TOI-6086b orbits its nearby (31 pc) M3 host star (Teff=3200 K) every 1.39 days, and has a radius of Rp=1.18","Re, an equilibrium temperature of Teq=634 K and an incident flux of Sp=26.8 Se.","Additional high precision radial velocity measurements are needed to derive the planetary masses and bulk densities, and to search for additional planets in the systems.","Moreover, short-period earth-sized planets orbiting around nearby M-dwarfs are suitable targets for atmospheric characterization with the James Webb Space Telescope (JWST) through transmission and emission spectroscopy, and phase curve photometry."],"url":"http://arxiv.org/abs/2405.06350v1","category":"astro-ph.EP"}
{"created":"2024-05-10 09:26:12","title":"Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology","abstract":"Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South. This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally. Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies. Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences. Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches. This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies. Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages. By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North.","sentences":["Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South.","This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally.","Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies.","Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences.","Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches.","This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies.","Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages.","By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North."],"url":"http://arxiv.org/abs/2405.06346v1","category":"cs.CL"}
{"created":"2024-05-10 09:08:15","title":"Random Batch Ewald Method for Dielectrically Confined Coulomb Systems","abstract":"Quasi two-dimensional Coulomb systems have drawn widespread interest. The reduced symmetry of these systems leads to complex collective behaviors, yet simultaneously poses significant challenges for particle-based simulations. In this paper, a novel method is presented for efficiently simulate a collection of charges confined in doubly-periodic slabs, with the extension to scenarios involving dielectric jumps at slab boundaries. Unlike existing methods, the method is insensitive to the aspect ratio of simulation box, and it achieves optimal O(N) complexity and strong scalability, thanks to the random batch Ewald (RBE) approach. Moreover, the additional cost for polarization contributions, represented as image reflection series, is reduced to a negligible cost via combining the RBE with an efficient structure factor coefficient re-calibration technique in k-space. Explicit formulas for optimal parameter choices of the algorithm are provided through error estimates, together with a rigorous proof. Finally, we demonstrate the accuracy, efficiency and scalability of our method, called RBE2D, via numerical tests across a variety of prototype systems. An excellent agreement between the RBE2D and the PPPM method is observed, with a significant reduction in the computational cost and strong scalability, demonstrating that it is a promising method for a broad range of charged systems under quasi-2D confinement.","sentences":["Quasi two-dimensional Coulomb systems have drawn widespread interest.","The reduced symmetry of these systems leads to complex collective behaviors, yet simultaneously poses significant challenges for particle-based simulations.","In this paper, a novel method is presented for efficiently simulate a collection of charges confined in doubly-periodic slabs, with the extension to scenarios involving dielectric jumps at slab boundaries.","Unlike existing methods, the method is insensitive to the aspect ratio of simulation box, and it achieves optimal O(N) complexity and strong scalability, thanks to the random batch Ewald (RBE) approach.","Moreover, the additional cost for polarization contributions, represented as image reflection series, is reduced to a negligible cost via combining the RBE with an efficient structure factor coefficient re-calibration technique in k-space.","Explicit formulas for optimal parameter choices of the algorithm are provided through error estimates, together with a rigorous proof.","Finally, we demonstrate the accuracy, efficiency and scalability of our method, called RBE2D, via numerical tests across a variety of prototype systems.","An excellent agreement between the RBE2D and the PPPM method is observed, with a significant reduction in the computational cost and strong scalability, demonstrating that it is a promising method for a broad range of charged systems under quasi-2D confinement."],"url":"http://arxiv.org/abs/2405.06333v1","category":"math.NA"}
{"created":"2024-05-10 17:59:54","title":"On Existence of Latency Optimal Uncoded Storage Schemes in Geo-Distributed Data Storage Systems","abstract":"We consider the problem of geographically distributed data storage in a network of servers (or nodes) where the nodes are connected to each other via communication links having certain round-trip times (RTTs). Each node serves a specific set of clients, where a client can request for any of the files available in the distributed system. The parent node provides the requested file if available locally; else it contacts other nodes that have the data needed to retrieve the requested file. This inter-node communication incurs a delay resulting in a certain latency in servicing the data request. The worst-case latency incurred at a servicing node and the system average latency are important performance metrics of a storage system, which depend not only on inter-node RTTs, but also on how the data is stored across the nodes. Data files could be placed in the nodes as they are, i.e., in uncoded fashion, or can be coded and placed. This paper provides the necessary and sufficient conditions for the existence of uncoded storage schemes that are optimal in terms of both per-node worst-case latency and system average latency. In addition, the paper provides efficient binary storage codes for a specific case where optimal uncoded schemes do not exist.","sentences":["We consider the problem of geographically distributed data storage in a network of servers (or nodes) where the nodes are connected to each other via communication links having certain round-trip times (RTTs).","Each node serves a specific set of clients, where a client can request for any of the files available in the distributed system.","The parent node provides the requested file if available locally; else it contacts other nodes that have the data needed to retrieve the requested file.","This inter-node communication incurs a delay resulting in a certain latency in servicing the data request.","The worst-case latency incurred at a servicing node and the system average latency are important performance metrics of a storage system, which depend not only on inter-node RTTs, but also on how the data is stored across the nodes.","Data files could be placed in the nodes as they are, i.e., in uncoded fashion, or can be coded and placed.","This paper provides the necessary and sufficient conditions for the existence of uncoded storage schemes that are optimal in terms of both per-node worst-case latency and system average latency.","In addition, the paper provides efficient binary storage codes for a specific case where optimal uncoded schemes do not exist."],"url":"http://arxiv.org/abs/2405.06641v1","category":"cs.IT"}
{"created":"2024-05-10 17:30:24","title":"Interfacial spin-orbit coupling driven enhancement of superconductivity by parallel magnetic field","abstract":"It is commonly believed that time reversal symmetry breaking perturbations such as magnetic field or scattering on magnetic impurities destruct superconductivity and suppress the critical temperature of the superconducting phase transition. However, in the recent experimental studies significant enhancement of superconducting critical temperature by parallel magnetic field was found for several thin superconducting systems [H.J. Gardner, et al., Nature Physics {\\bf 7}, 895 (2011); T. Asaba, et al., Scientific Reports {\\bf 8}, 1 (2018) ]. Here we present a possible explanation of the observed phenomenon showing that combination of interfacial spin-orbit interaction and external magnetic field can cause the increase of the superconducting critical temperature of thin superconducting film in wide range of magnetic fields. We use the Ginzburg-Landau formalism taking into account Lifshitz invariant originated from Rashba spin-orbit interaction, which is also responsible for the superconducting diode effect. We also calculate the modification of the Little-Parks oscillations of the critical temperature of hollow superconducting cylinder at the presence of Rashba interfacial spin-orbit interaction.","sentences":["It is commonly believed that time reversal symmetry breaking perturbations such as magnetic field or scattering on magnetic impurities destruct superconductivity and suppress the critical temperature of the superconducting phase transition.","However, in the recent experimental studies significant enhancement of superconducting critical temperature by parallel magnetic field was found for several thin superconducting systems","[H.J. Gardner, et al., Nature Physics {\\bf 7}, 895 (2011); T. Asaba, et al., Scientific Reports {\\bf 8}, 1 (2018) ].","Here we present a possible explanation of the observed phenomenon showing that combination of interfacial spin-orbit interaction and external magnetic field can cause the increase of the superconducting critical temperature of thin superconducting film in wide range of magnetic fields.","We use the Ginzburg-Landau formalism taking into account Lifshitz invariant originated from Rashba spin-orbit interaction, which is also responsible for the superconducting diode effect.","We also calculate the modification of the Little-Parks oscillations of the critical temperature of hollow superconducting cylinder at the presence of Rashba interfacial spin-orbit interaction."],"url":"http://arxiv.org/abs/2405.06619v1","category":"cond-mat.supr-con"}
{"created":"2024-05-10 17:27:11","title":"The Coupled Impacts of Atmospheric Composition and Obliquity on the Climate Dynamics of TRAPPIST-1e","abstract":"Planets in multi-planet systems are expected to migrate inward as near-resonant chains, thus allowing them to undergo gravitational planet-planet interactions and possibly maintain a non-zero obliquity. The TRAPPIST-1 system is in such a near-resonant configuration, making it plausible that TRAPPIST-1e has a non-zero obliquity. In this work, we use the ExoCAM GCM to study the possible climates of TRAPPIST-1e at varying obliquities and atmospheric compositions. We vary obliquity from 0$^\\circ$ to 90$^\\circ$ and the partial pressure of carbon dioxide from 0.0004 bars (modern Earth-like) to 1 bar. We find that models with a higher obliquity are hotter overall and have a smaller day-night temperature contrast than the lower obliquity models, which is consistent with previous studies. Most significantly, the super-rotating high-altitude jet becomes sub-rotating at high obliquity, thus impacting cloud and surface temperature patterns. As the amount of carbon dioxide increases, the climate of TRAPPIST-1e becomes hotter, cloudier, and less variable. From modeled thermal phase curves, we find that the impact of obliquity could potentially have observable consequences due to the effect of cloud coverage on the outgoing longwave radiation.","sentences":["Planets in multi-planet systems are expected to migrate inward as near-resonant chains, thus allowing them to undergo gravitational planet-planet interactions and possibly maintain a non-zero obliquity.","The TRAPPIST-1 system is in such a near-resonant configuration, making it plausible that TRAPPIST-1e has a non-zero obliquity.","In this work, we use the ExoCAM GCM to study the possible climates of TRAPPIST-1e at varying obliquities and atmospheric compositions.","We vary obliquity from 0$^\\circ$ to 90$^\\circ$ and the partial pressure of carbon dioxide from 0.0004 bars (modern Earth-like) to 1 bar.","We find that models with a higher obliquity are hotter overall and have a smaller day-night temperature contrast than the lower obliquity models, which is consistent with previous studies.","Most significantly, the super-rotating high-altitude jet becomes sub-rotating at high obliquity, thus impacting cloud and surface temperature patterns.","As the amount of carbon dioxide increases, the climate of TRAPPIST-1e becomes hotter, cloudier, and less variable.","From modeled thermal phase curves, we find that the impact of obliquity could potentially have observable consequences due to the effect of cloud coverage on the outgoing longwave radiation."],"url":"http://arxiv.org/abs/2405.06615v1","category":"astro-ph.EP"}
{"created":"2024-05-10 17:25:47","title":"An explicit granular-mechanics approach to marine sediment acoustics","abstract":"Here we theoretically and computationally study the frequency dependence of phase speed and attenuation for marine sediments from the perspective of granular mechanics. We leverage recent theoretical insights from the granular physics community as well as discrete-element method simulations, where the granular material is treated as a packing of discrete objects that interact via pairwise forces. These pairwise forces include both repulsive contact forces as well as dissipative terms which may include losses from the fluid as well as losses from inelasticity at grain-grain contacts. We show that the structure of disordered granular packings leads to anomalous scaling laws for frequency-dependent phase speed and attenuation that do not follow from a continuum treatment. Our results demonstrate that granular packing structure, which is not explicitly considered in existing models, may play a crucial role in a complete theory of sediment acoustics. While this simple approach does not explicitly treat sound propagation or inertial effects in the interstitial fluid, it provides a starting point for future models that include these and other more complex features.","sentences":["Here we theoretically and computationally study the frequency dependence of phase speed and attenuation for marine sediments from the perspective of granular mechanics.","We leverage recent theoretical insights from the granular physics community as well as discrete-element method simulations, where the granular material is treated as a packing of discrete objects that interact via pairwise forces.","These pairwise forces include both repulsive contact forces as well as dissipative terms which may include losses from the fluid as well as losses from inelasticity at grain-grain contacts.","We show that the structure of disordered granular packings leads to anomalous scaling laws for frequency-dependent phase speed and attenuation that do not follow from a continuum treatment.","Our results demonstrate that granular packing structure, which is not explicitly considered in existing models, may play a crucial role in a complete theory of sediment acoustics.","While this simple approach does not explicitly treat sound propagation or inertial effects in the interstitial fluid, it provides a starting point for future models that include these and other more complex features."],"url":"http://arxiv.org/abs/2405.06614v1","category":"cond-mat.soft"}
{"created":"2024-05-10 17:13:33","title":"Dual-band bandpass filter derived from the transformation of a single-band bandpass filter","abstract":"The recent proliferation of personal wireless communication devices is driving the need for multi-band frequency selective components including multiplexers and dual-band filters. This paper presents a simple technique for transforming a single-band bandpass filter (BPF) into a dual-band BPF. A second order (two-pole) single-band bandpass filter was chosen for this research, giving rise to a fourth order (four-pole) dual-band bandpass filter after the proposed filter transformation. Both filters were then implemented using the compact U-shaped microstrip resonator for improved device miniaturization. The proposed work features a centre frequency of 1.4 GHz for the single-band bandpass filter, with a span of 3.4% fractional bandwidth. The dual-band bandpass filter operates at 1.35 and 1.45 GHz. The design implementation employs the commercially available Rogers RT/Duroid 6010LM substrate, having a dissipation factor of 0.0023, dielectric constant of 10.7, diel thickness (h) of 1.27 mm, and top/bottom cladding of 35 microns. The results reported for the theoretical and practical designs show good agreement and improved performance when compared to similar research works in literature. The practical responses of the prototype dual-band BPF indicate a good return loss of better than 18 dB across both bands, and an insertion loss of better than 0.1 dB.","sentences":["The recent proliferation of personal wireless communication devices is driving the need for multi-band frequency selective components including multiplexers and dual-band filters.","This paper presents a simple technique for transforming a single-band bandpass filter (BPF) into a dual-band BPF.","A second order (two-pole) single-band bandpass filter was chosen for this research, giving rise to a fourth order (four-pole) dual-band bandpass filter after the proposed filter transformation.","Both filters were then implemented using the compact U-shaped microstrip resonator for improved device miniaturization.","The proposed work features a centre frequency of 1.4 GHz for the single-band bandpass filter, with a span of 3.4% fractional bandwidth.","The dual-band bandpass filter operates at 1.35 and 1.45 GHz.","The design implementation employs the commercially available Rogers RT/Duroid 6010LM substrate, having a dissipation factor of 0.0023, dielectric constant of 10.7, diel thickness (h) of 1.27 mm, and top/bottom cladding of 35 microns.","The results reported for the theoretical and practical designs show good agreement and improved performance when compared to similar research works in literature.","The practical responses of the prototype dual-band BPF indicate a good return loss of better than 18 dB across both bands, and an insertion loss of better than 0.1 dB."],"url":"http://arxiv.org/abs/2405.06608v1","category":"eess.SY"}
{"created":"2024-05-10 17:00:04","title":"Multi-Object Tracking in the Dark","abstract":"Low-light scenes are prevalent in real-world applications (e.g. autonomous driving and surveillance at night). Recently, multi-object tracking in various practical use cases have received much attention, but multi-object tracking in dark scenes is rarely considered. In this paper, we focus on multi-object tracking in dark scenes. To address the lack of datasets, we first build a Low-light Multi-Object Tracking (LMOT) dataset. LMOT provides well-aligned low-light video pairs captured by our dual-camera system, and high-quality multi-object tracking annotations for all videos. Then, we propose a low-light multi-object tracking method, termed as LTrack. We introduce the adaptive low-pass downsample module to enhance low-frequency components of images outside the sensor noises. The degradation suppression learning strategy enables the model to learn invariant information under noise disturbance and image quality degradation. These components improve the robustness of multi-object tracking in dark scenes. We conducted a comprehensive analysis of our LMOT dataset and proposed LTrack. Experimental results demonstrate the superiority of the proposed method and its competitiveness in real night low-light scenes. Dataset and Code: https: //github.com/ying-fu/LMOT","sentences":["Low-light scenes are prevalent in real-world applications (e.g. autonomous driving and surveillance at night).","Recently, multi-object tracking in various practical use cases have received much attention, but multi-object tracking in dark scenes is rarely considered.","In this paper, we focus on multi-object tracking in dark scenes.","To address the lack of datasets, we first build a Low-light Multi-Object Tracking (LMOT) dataset.","LMOT provides well-aligned low-light video pairs captured by our dual-camera system, and high-quality multi-object tracking annotations for all videos.","Then, we propose a low-light multi-object tracking method, termed as LTrack.","We introduce the adaptive low-pass downsample module to enhance low-frequency components of images outside the sensor noises.","The degradation suppression learning strategy enables the model to learn invariant information under noise disturbance and image quality degradation.","These components improve the robustness of multi-object tracking in dark scenes.","We conducted a comprehensive analysis of our LMOT dataset and proposed LTrack.","Experimental results demonstrate the superiority of the proposed method and its competitiveness in real night low-light scenes.","Dataset and Code: https: //github.com/ying-fu/LMOT"],"url":"http://arxiv.org/abs/2405.06600v1","category":"cs.CV"}
{"created":"2024-05-10 16:58:15","title":"Ice phase classification made easy with score-based denoising","abstract":"Accurate identification of ice phases is essential for understanding various physicochemical phenomena. However, such classification for structures simulated with molecular dynamics is complicated by the complex symmetries of ice polymorphs and thermal fluctuations. For this purpose, both traditional order parameters and data-driven machine learning approaches have been employed, but they often rely on expert intuition, specific geometric information, or large training datasets. In this work, we present an unsupervised phase classification framework that combines a score-based denoiser model with a subsequent model-free classification method to accurately identify ice phases. The denoiser model is trained on perturbed synthetic data of ideal reference structures, eliminating the need for large datasets and labeling efforts. The classification step utilizes the Smooth Overlap of Atomic Positions (SOAP) descriptors as the atomic fingerprint, ensuring Euclidean symmetries and transferability to various structural systems. Our approach achieves a remarkable 100\\% accuracy in distinguishing ice phases of test trajectories using only seven ideal reference structures of ice phases as model inputs. This demonstrates the generalizability of the score-based denoiser model in facilitating phase identification for complex molecular systems. The proposed classification strategy can be broadly applied to investigate structural evolution and phase identification for a wide range of materials, offering new insights into the fundamental understanding of water and other complex systems.","sentences":["Accurate identification of ice phases is essential for understanding various physicochemical phenomena.","However, such classification for structures simulated with molecular dynamics is complicated by the complex symmetries of ice polymorphs and thermal fluctuations.","For this purpose, both traditional order parameters and data-driven machine learning approaches have been employed, but they often rely on expert intuition, specific geometric information, or large training datasets.","In this work, we present an unsupervised phase classification framework that combines a score-based denoiser model with a subsequent model-free classification method to accurately identify ice phases.","The denoiser model is trained on perturbed synthetic data of ideal reference structures, eliminating the need for large datasets and labeling efforts.","The classification step utilizes the Smooth Overlap of Atomic Positions (SOAP) descriptors as the atomic fingerprint, ensuring Euclidean symmetries and transferability to various structural systems.","Our approach achieves a remarkable 100\\% accuracy in distinguishing ice phases of test trajectories using only seven ideal reference structures of ice phases as model inputs.","This demonstrates the generalizability of the score-based denoiser model in facilitating phase identification for complex molecular systems.","The proposed classification strategy can be broadly applied to investigate structural evolution and phase identification for a wide range of materials, offering new insights into the fundamental understanding of water and other complex systems."],"url":"http://arxiv.org/abs/2405.06599v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 16:46:55","title":"Perturbations of Q-balls: from spectral structure to radiation pressure","abstract":"We investigate Q-balls in a 1+1 dimensional complex scalar field theory. We find that the relaxation of a squashed Q-ball is dominated by the decay of a normal mode through nonlinear coupling to scattering modes and a long-lasting quasi-normal mode. We also study how these Q-balls behave when exposed to scalar radiation, finding that for certain conditions they can experience negative radiation pressure.","sentences":["We investigate Q-balls in a 1+1 dimensional complex scalar field theory.","We find that the relaxation of a squashed Q-ball is dominated by the decay of a normal mode through nonlinear coupling to scattering modes and a long-lasting quasi-normal mode.","We also study how these Q-balls behave when exposed to scalar radiation, finding that for certain conditions they can experience negative radiation pressure."],"url":"http://arxiv.org/abs/2405.06591v1","category":"hep-th"}
{"created":"2024-05-10 16:39:38","title":"Huygens Synchronization of Three Aligned Clocks","abstract":"This study examines the synchronization of three identical oscillators arranged in an array and coupled by small impacts, wherein each oscillator interacts solely with its nearest neighbor. The synchronized state, which is asymptotically stable, is characterized by phase opposition among alternating oscillators. We analyze the system using a non-linear discrete dynamical system based on a difference equation derived from the iteration of a plane diffeomorphism. We illustrate these results with the application to a system of three aligned Andronov clocks, showcasing their applicability to a broad range of oscillator systems.","sentences":["This study examines the synchronization of three identical oscillators arranged in an array and coupled by small impacts, wherein each oscillator interacts solely with its nearest neighbor.","The synchronized state, which is asymptotically stable, is characterized by phase opposition among alternating oscillators.","We analyze the system using a non-linear discrete dynamical system based on a difference equation derived from the iteration of a plane diffeomorphism.","We illustrate these results with the application to a system of three aligned Andronov clocks, showcasing their applicability to a broad range of oscillator systems."],"url":"http://arxiv.org/abs/2405.06585v1","category":"math.DS"}
{"created":"2024-05-10 16:12:35","title":"Efficient Federated Low Rank Matrix Completion","abstract":"In this work, we develop and analyze a Gradient Descent (GD) based solution, called Alternating GD and Minimization (AltGDmin), for efficiently solving the low rank matrix completion (LRMC) in a federated setting. LRMC involves recovering an $n \\times q$ rank-$r$ matrix $\\Xstar$ from a subset of its entries when $r \\ll \\min(n,q)$. Our theoretical guarantees (iteration and sample complexity bounds) imply that AltGDmin is the most communication-efficient solution in a federated setting, is one of the fastest, and has the second best sample complexity among all iterative solutions to LRMC. In addition, we also prove two important corollaries. (a) We provide a guarantee for AltGDmin for solving the noisy LRMC problem. (b) We show how our lemmas can be used to provide an improved sample complexity guarantee for AltMin, which is the fastest centralized solution.","sentences":["In this work, we develop and analyze a Gradient Descent (GD) based solution, called Alternating GD and Minimization (AltGDmin), for efficiently solving the low rank matrix completion (LRMC) in a federated setting.","LRMC involves recovering an $n \\times q$ rank-$r$ matrix $\\Xstar$ from a subset of its entries when $r \\ll \\min(n,q)$. Our theoretical guarantees (iteration and sample complexity bounds) imply that AltGDmin is the most communication-efficient solution in a federated setting, is one of the fastest, and has the second best sample complexity among all iterative solutions to LRMC.","In addition, we also prove two important corollaries.","(a) We provide a guarantee for AltGDmin for solving the noisy LRMC problem.","(b) We show how our lemmas can be used to provide an improved sample complexity guarantee for AltMin, which is the fastest centralized solution."],"url":"http://arxiv.org/abs/2405.06569v1","category":"cs.LG"}
{"created":"2024-05-10 16:02:41","title":"Reservoir Computing Benchmarks: a review, a taxonomy, some best practices","abstract":"Reservoir Computing is an Unconventional Computation model to perform computation on various different substrates, such as RNNs or physical materials. The method takes a \"black-box\" approach, training only the outputs of the system it is built on. As such, evaluating the computational capacity of these systems can be challenging. We review and critique the evaluation methods used in the field of Reservoir Computing. We introduce a categorisation of benchmark tasks. We review multiple examples of benchmarks from the literature as applied to reservoir computing, and note their strengths and shortcomings. We suggest ways in which benchmarks and their uses may be improved to the benefit of the reservoir computing community","sentences":["Reservoir Computing is an Unconventional Computation model to perform computation on various different substrates, such as RNNs or physical materials.","The method takes a \"black-box\" approach, training only the outputs of the system it is built on.","As such, evaluating the computational capacity of these systems can be challenging.","We review and critique the evaluation methods used in the field of Reservoir Computing.","We introduce a categorisation of benchmark tasks.","We review multiple examples of benchmarks from the literature as applied to reservoir computing, and note their strengths and shortcomings.","We suggest ways in which benchmarks and their uses may be improved to the benefit of the reservoir computing community"],"url":"http://arxiv.org/abs/2405.06561v1","category":"cs.ET"}
{"created":"2024-05-10 15:46:38","title":"Highly Sensitive Ratiometric Fluorescent Fiber Matrixes for Oxygen Sensing with Micrometer-Spatial Resolution","abstract":"Oxygen (O2)-sensing matrices are promising tools for the live monitoring of extracellular O2 consumption levels in long-term cell cultures. In this study, ratiometric O2-sensing membranes were prepared by electrospinning, an easy, low-cost, scalable, and robust method for fabricating nanofibers. Poly({\\epsilon}-caprolactone) and poly(dimethyl)siloxane polymers were blended with tris(4,7-diphenyl-1,10-phenanthroline) ruthenium(II) dichloride, which was used as the O2-sensing probe, and rhodamine B isothiocyanate, which was used as the reference dye. The functionalized scaffolds were morphologically characterized by scanning electron microscopy, and their physicochemical profiles were obtained by Fourier transform infrared spectroscopy, thermogravimetric analysis, and water contact angle measurement. The sensing capabilities were investigated by confocal laser scanning microscopy, performing photobleaching, reversibility, and calibration curve studies toward different dissolved O2 (DO) concentrations. Electrospun sensing nanofibers showed a high response to changes in DO concentrations in the physiological-pathological range from 0.5 to 20% and good stability under ratiometric imaging. In addition, the sensing systems were highly biocompatible for cell growth promoting adhesiveness and growth of three cancer cell lines, namely metastatic melanoma cell line SK-MEL2, breast cancer cell line MCF-7, and pancreatic ductal adenocarcinoma cell line Panc-1, thus recreating a suitable biological environment in vitro. These O2-sensing biomaterials can potentially measure alterations in cell metabolism caused by changes in ambient O2 content during drug testing/validation and tissue regeneration processes.","sentences":["Oxygen (O2)-sensing matrices are promising tools for the live monitoring of extracellular O2 consumption levels in long-term cell cultures.","In this study, ratiometric O2-sensing membranes were prepared by electrospinning, an easy, low-cost, scalable, and robust method for fabricating nanofibers.","Poly({\\epsilon}-caprolactone) and poly(dimethyl)siloxane polymers were blended with tris(4,7-diphenyl-1,10-phenanthroline) ruthenium(II) dichloride, which was used as the O2-sensing probe, and rhodamine B isothiocyanate, which was used as the reference dye.","The functionalized scaffolds were morphologically characterized by scanning electron microscopy, and their physicochemical profiles were obtained by Fourier transform infrared spectroscopy, thermogravimetric analysis, and water contact angle measurement.","The sensing capabilities were investigated by confocal laser scanning microscopy, performing photobleaching, reversibility, and calibration curve studies toward different dissolved O2 (DO) concentrations.","Electrospun sensing nanofibers showed a high response to changes in DO concentrations in the physiological-pathological range from 0.5 to 20% and good stability under ratiometric imaging.","In addition, the sensing systems were highly biocompatible for cell growth promoting adhesiveness and growth of three cancer cell lines, namely metastatic melanoma cell line SK-MEL2, breast cancer cell line MCF-7, and pancreatic ductal adenocarcinoma cell line Panc-1, thus recreating a suitable biological environment in vitro.","These O2-sensing biomaterials can potentially measure alterations in cell metabolism caused by changes in ambient O2 content during drug testing/validation and tissue regeneration processes."],"url":"http://arxiv.org/abs/2405.06550v1","category":"physics.bio-ph"}
{"created":"2024-05-10 15:34:01","title":"Separating States in Astronomical Sources Using Hidden Markov Models: With a Case Study of Flaring and Quiescence on EV Lac","abstract":"We present a new method to distinguish between different states (e.g., high and low, quiescent and flaring) in astronomical sources with count data. The method models the underlying physical process as latent variables following a continuous-space Markov chain that determines the expected Poisson counts in observed light curves in multiple passbands. For the underlying state process, we consider several autoregressive processes, yielding continuous-space hidden Markov models of varying complexity. Under these models, we can infer the state that the object is in at any given time. The state predictions from these models are then dichotomized with the help of a finite-mixture model to produce state classifications. We apply these techniques to X-ray data from the active dMe flare star EV Lac, splitting the data into quiescent and flaring states. We find that a first-order vector autoregressive process efficiently separates flaring from quiescence: flaring occurs over 30-40% of the observation durations, a well-defined persistent quiescent state can be identified, and the flaring state is characterized by higher temperatures and emission measures.","sentences":["We present a new method to distinguish between different states (e.g., high and low, quiescent and flaring) in astronomical sources with count data.","The method models the underlying physical process as latent variables following a continuous-space Markov chain that determines the expected Poisson counts in observed light curves in multiple passbands.","For the underlying state process, we consider several autoregressive processes, yielding continuous-space hidden Markov models of varying complexity.","Under these models, we can infer the state that the object is in at any given time.","The state predictions from these models are then dichotomized with the help of a finite-mixture model to produce state classifications.","We apply these techniques to X-ray data from the active dMe flare star EV Lac, splitting the data into quiescent and flaring states.","We find that a first-order vector autoregressive process efficiently separates flaring from quiescence: flaring occurs over 30-40% of the observation durations, a well-defined persistent quiescent state can be identified, and the flaring state is characterized by higher temperatures and emission measures."],"url":"http://arxiv.org/abs/2405.06540v1","category":"astro-ph.SR"}
{"created":"2024-05-10 15:27:43","title":"Mesh Denoising Transformer","abstract":"Mesh denoising, aimed at removing noise from input meshes while preserving their feature structures, is a practical yet challenging task. Despite the remarkable progress in learning-based mesh denoising methodologies in recent years, their network designs often encounter two principal drawbacks: a dependence on single-modal geometric representations, which fall short in capturing the multifaceted attributes of meshes, and a lack of effective global feature aggregation, hindering their ability to fully understand the mesh's comprehensive structure. To tackle these issues, we propose SurfaceFormer, a pioneering Transformer-based mesh denoising framework. Our first contribution is the development of a new representation known as Local Surface Descriptor, which is crafted by establishing polar systems on each mesh face, followed by sampling points from adjacent surfaces using geodesics. The normals of these points are organized into 2D patches, mimicking images to capture local geometric intricacies, whereas the poles and vertex coordinates are consolidated into a point cloud to embody spatial information. This advancement surmounts the hurdles posed by the irregular and non-Euclidean characteristics of mesh data, facilitating a smooth integration with Transformer architecture. Next, we propose a dual-stream structure consisting of a Geometric Encoder branch and a Spatial Encoder branch, which jointly encode local geometry details and spatial information to fully explore multimodal information for mesh denoising. A subsequent Denoising Transformer module receives the multimodal information and achieves efficient global feature aggregation through self-attention operators. Our experimental evaluations demonstrate that this novel approach outperforms existing state-of-the-art methods in both objective and subjective assessments, marking a significant leap forward in mesh denoising.","sentences":["Mesh denoising, aimed at removing noise from input meshes while preserving their feature structures, is a practical yet challenging task.","Despite the remarkable progress in learning-based mesh denoising methodologies in recent years, their network designs often encounter two principal drawbacks: a dependence on single-modal geometric representations, which fall short in capturing the multifaceted attributes of meshes, and a lack of effective global feature aggregation, hindering their ability to fully understand the mesh's comprehensive structure.","To tackle these issues, we propose SurfaceFormer, a pioneering Transformer-based mesh denoising framework.","Our first contribution is the development of a new representation known as Local Surface Descriptor, which is crafted by establishing polar systems on each mesh face, followed by sampling points from adjacent surfaces using geodesics.","The normals of these points are organized into 2D patches, mimicking images to capture local geometric intricacies, whereas the poles and vertex coordinates are consolidated into a point cloud to embody spatial information.","This advancement surmounts the hurdles posed by the irregular and non-Euclidean characteristics of mesh data, facilitating a smooth integration with Transformer architecture.","Next, we propose a dual-stream structure consisting of a Geometric Encoder branch and a Spatial Encoder branch, which jointly encode local geometry details and spatial information to fully explore multimodal information for mesh denoising.","A subsequent Denoising Transformer module receives the multimodal information and achieves efficient global feature aggregation through self-attention operators.","Our experimental evaluations demonstrate that this novel approach outperforms existing state-of-the-art methods in both objective and subjective assessments, marking a significant leap forward in mesh denoising."],"url":"http://arxiv.org/abs/2405.06536v1","category":"cs.CV"}
{"created":"2024-05-10 15:22:00","title":"A posteriori error estimates based on multilevel decompositions with large problems on the coarsest level","abstract":"Multilevel methods represent a powerful approach in numerical solution of partial differential equations. The multilevel structure can also be used to construct estimates for total and algebraic errors of computed approximations. This paper deals with residual-based error estimates that are based on properties of quasi-interpolation operators, stable-splittings, or frames. We focus on the settings where the system matrix on the coarsest level is still large and the associated terms in the estimates can only be approximated. We show that the way in which the error term associated with the coarsest level is approximated is substantial. It can significantly affect both the efficiency (accuracy) of the overall error estimates and their robustness with respect to the size of the coarsest problem. The newly proposed approximation of the coarsest-level term is based on using the conjugate gradient method with an appropriate stopping criterion. We prove that the resulting estimates are efficient and robust with respect to the size of the coarsest-level problem. Numerical experiments illustrate the theoretical findings.","sentences":["Multilevel methods represent a powerful approach in numerical solution of partial differential equations.","The multilevel structure can also be used to construct estimates for total and algebraic errors of computed approximations.","This paper deals with residual-based error estimates that are based on properties of quasi-interpolation operators, stable-splittings, or frames.","We focus on the settings where the system matrix on the coarsest level is still large and the associated terms in the estimates can only be approximated.","We show that the way in which the error term associated with the coarsest level is approximated is substantial.","It can significantly affect both the efficiency (accuracy) of the overall error estimates and their robustness with respect to the size of the coarsest problem.","The newly proposed approximation of the coarsest-level term is based on using the conjugate gradient method with an appropriate stopping criterion.","We prove that the resulting estimates are efficient and robust with respect to the size of the coarsest-level problem.","Numerical experiments illustrate the theoretical findings."],"url":"http://arxiv.org/abs/2405.06532v1","category":"math.NA"}
{"created":"2024-05-10 15:09:54","title":"Forms in prime variables and differing degrees","abstract":"Let $F_1,\\ldots,F_R$ be homogeneous polynomials with integer coefficients in $n$ variables with differing degrees. Write $\\boldsymbol{F}=(F_1,\\ldots,F_R)$ with $D$ being the maximal degree. Suppose that $\\boldsymbol{F}$ is a nonsingular system and $n\\ge D^2 4^{D+6}R^5$. We prove an asymptotic formula for the number of prime solutions to $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$, whose main term is positive if (i) $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ has a nonsingular solution over the $p$-adic units $\\mathbb{U}_p$ for all primes $p$, and (ii) $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ has a nonsingular solution in the open cube $(0,1)^n$. This can be viewed as a smooth local-global principle for $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ with differing degrees. It follows that, under (i) and (ii), the set of prime solutions to $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ is Zariski dense in the set of its solutions.","sentences":["Let $F_1,\\ldots,F_R$ be homogeneous polynomials with integer coefficients in $n$ variables with differing degrees.","Write $\\boldsymbol{F}=(F_1,\\ldots,F_R)$ with $D$ being the maximal degree.","Suppose that $\\boldsymbol{F}$ is a nonsingular system and $n\\ge D^2","4^{D+6}R^5$. We prove an asymptotic formula for the number of prime solutions to $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$, whose main term is positive if (i) $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ has a nonsingular solution over the $p$-adic units $\\mathbb{U}_p$ for all primes $p$, and (ii) $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ has a nonsingular solution in the open cube $(0,1)^n$. This can be viewed as a smooth local-global principle for $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ with differing degrees.","It follows that, under (i) and (ii), the set of prime solutions to $\\boldsymbol{F}(\\boldsymbol{x})=\\boldsymbol{0}$ is Zariski dense in the set of its solutions."],"url":"http://arxiv.org/abs/2405.06523v1","category":"math.NT"}
{"created":"2024-05-10 14:57:06","title":"Virial Theorem and Its Applications in Instability of Two-Phase Water-Wave","abstract":"In this paper, we analyze the dynamics of two layers of immiscible, inviscid, incompressible, and irrotational fluids through a full nonlinear system. Our goal is to establish a virial theorem and prove the polynomial growth of slope and curvature of the interface over time when the fluid below is no denser than the one above. These phenomena, known as Rayleigh-Taylor instability and Kelvin-Helmholtz instability, will be proved for a broad class of regular initial data, including the case of 2D overlapping interface.","sentences":["In this paper, we analyze the dynamics of two layers of immiscible, inviscid, incompressible, and irrotational fluids through a full nonlinear system.","Our goal is to establish a virial theorem and prove the polynomial growth of slope and curvature of the interface over time when the fluid below is no denser than the one above.","These phenomena, known as Rayleigh-Taylor instability and Kelvin-Helmholtz instability, will be proved for a broad class of regular initial data, including the case of 2D overlapping interface."],"url":"http://arxiv.org/abs/2405.06517v1","category":"math.AP"}
{"created":"2024-05-10 14:54:48","title":"An Efficient Algorithm for Sum-Rate Maximization in Fluid Antenna-Assisted ISAC System","abstract":"In this letter, we investigate the fluid antenna (FA)-assisted integrated sensing and communication (ISAC) system, where communication and radar sensing employ the co-waveform design. Specifically, we focus on the beamformer design and antenna position configuration to realize a higher communication rate while guaranteeing the minimum radar probing power. Different from existing beamformer algorithms, we propose an efficient proximal distance algorithm (PDA) to solve the multiuser sum-rate maximization problem with radar sensing constraint to obtain the closed-form beamforming vector. In addition, we develop an extrapolated projected gradient (EPG) algorithm to obtain a better antenna location configuration for FA to enhance the ISAC performance. Numerical results show that the considered FA-assisted ISAC system enjoys a higher sum-rate by the proposed algorithm, compared with that in existing non-FA ISAC systems.","sentences":["In this letter, we investigate the fluid antenna (FA)-assisted integrated sensing and communication (ISAC) system, where communication and radar sensing employ the co-waveform design.","Specifically, we focus on the beamformer design and antenna position configuration to realize a higher communication rate while guaranteeing the minimum radar probing power.","Different from existing beamformer algorithms, we propose an efficient proximal distance algorithm (PDA) to solve the multiuser sum-rate maximization problem with radar sensing constraint to obtain the closed-form beamforming vector.","In addition, we develop an extrapolated projected gradient (EPG) algorithm to obtain a better antenna location configuration for FA to enhance the ISAC performance.","Numerical results show that the considered FA-assisted ISAC system enjoys a higher sum-rate by the proposed algorithm, compared with that in existing non-FA ISAC systems."],"url":"http://arxiv.org/abs/2405.06516v1","category":"cs.IT"}
{"created":"2024-05-10 14:18:21","title":"Experimental identification of topological defects in 2D colloidal glass","abstract":"Topological defects are singularities in the order parameter space that are mathematically described by topological invariants and cannot be removed by continuous transformations. These defects play a significant role in various fields, ranging from cosmology to solid-state physics and biological matter. The definition of these irregularities requires an ordered reference configuration, leading to decades of debate about the existence of topological defects in disordered systems, such as glasses. Recently, it has been proposed that well-defined topological defects might emerge in the dynamical properties of glasses under deformation, potentially relating to their plastic behavior. In this study, we investigate a two-dimensional colloidal glass system composed of particles interacting by an effective magnetic repulsion. We reveal the presence of topological defects in the eigenspace of the vibrational frequencies of this experimental 2D amorphous solid. The vibrational density of states of this 2D amorphous material exhibits distinct glassy properties such as the presence of a boson peak anomaly. Through extensive numerical and theoretical quantitative analysis, we establish a robust positive correlation between the vibrational characteristics and the total number of topological defects. We furthermore show that defects of opposite charge tend to pair together and prove their local statistical correlation with the \"soft spots\", the regions more prone to plastic flow. This work provides the experimental confirmation for the existence of topological defects in disordered systems revealing a complex interplay between topology, disorder, and vibrational behavior.","sentences":["Topological defects are singularities in the order parameter space that are mathematically described by topological invariants and cannot be removed by continuous transformations.","These defects play a significant role in various fields, ranging from cosmology to solid-state physics and biological matter.","The definition of these irregularities requires an ordered reference configuration, leading to decades of debate about the existence of topological defects in disordered systems, such as glasses.","Recently, it has been proposed that well-defined topological defects might emerge in the dynamical properties of glasses under deformation, potentially relating to their plastic behavior.","In this study, we investigate a two-dimensional colloidal glass system composed of particles interacting by an effective magnetic repulsion.","We reveal the presence of topological defects in the eigenspace of the vibrational frequencies of this experimental 2D amorphous solid.","The vibrational density of states of this 2D amorphous material exhibits distinct glassy properties such as the presence of a boson peak anomaly.","Through extensive numerical and theoretical quantitative analysis, we establish a robust positive correlation between the vibrational characteristics and the total number of topological defects.","We furthermore show that defects of opposite charge tend to pair together and prove their local statistical correlation with the \"soft spots\", the regions more prone to plastic flow.","This work provides the experimental confirmation for the existence of topological defects in disordered systems revealing a complex interplay between topology, disorder, and vibrational behavior."],"url":"http://arxiv.org/abs/2405.06494v1","category":"cond-mat.soft"}
{"created":"2024-05-10 14:13:21","title":"A Note on an Inferentialist Approach to Resource Semantics","abstract":"A central concept within informatics is in modelling such systems for the purpose of reasoning (perhaps automated) about their behaviour and properties. To this end, one requires an interpretation of logical formulae in terms of the resources and states of the system; such an interpretation is called a 'resource semantics' of the logic. This paper shows how 'inferentialism' -- the view that meaning is given in terms of inferential behaviour -- enables a versatile and expressive framework for resource semantics. Specifically, how inferentialism seamlessly incorporates the assertion-based approach of the logic of Bunched Implications, foundational in program verification (e.g., as the basis of Separation Logic), and the renowned number-of-uses reading of Linear Logic. This integration enables reasoning about shared and separated resources in intuitive and familiar ways, as well as about the composition and interfacing of system components.","sentences":["A central concept within informatics is in modelling such systems for the purpose of reasoning (perhaps automated) about their behaviour and properties.","To this end, one requires an interpretation of logical formulae in terms of the resources and states of the system; such an interpretation is called a 'resource semantics' of the logic.","This paper shows how 'inferentialism' -- the view that meaning is given in terms of inferential behaviour -- enables a versatile and expressive framework for resource semantics.","Specifically, how inferentialism seamlessly incorporates the assertion-based approach of the logic of Bunched Implications, foundational in program verification (e.g., as the basis of Separation Logic), and the renowned number-of-uses reading of Linear Logic.","This integration enables reasoning about shared and separated resources in intuitive and familiar ways, as well as about the composition and interfacing of system components."],"url":"http://arxiv.org/abs/2405.06491v1","category":"cs.LO"}
{"created":"2024-05-10 14:13:02","title":"An Open Source Stochastic Unit Commitment Tool using the PyPSA-Framework","abstract":"This paper presents an open source stochastic unit commitment (UC) optimization tool, which is available on GitHub. In addition, it presents an example use case in which UC optimization is done for a waste-to-energy plant with heat storage and a battery energy storage system (BESS) in Germany, under uncertain day-ahead and balancing power (aFRR) market prices as well as heat load uncertainty. The tool consists of multiple modular extensions for the Python for Power System Analysis (PyPSA) framework, namely the implementation of market and bidding mechanisms, stochastic optimization and multistaging.","sentences":["This paper presents an open source stochastic unit commitment (UC) optimization tool, which is available on GitHub.","In addition, it presents an example use case in which UC optimization is done for a waste-to-energy plant with heat storage and a battery energy storage system (BESS) in Germany, under uncertain day-ahead and balancing power (aFRR) market prices as well as heat load uncertainty.","The tool consists of multiple modular extensions for the Python for Power System Analysis (PyPSA) framework, namely the implementation of market and bidding mechanisms, stochastic optimization and multistaging."],"url":"http://arxiv.org/abs/2405.06490v1","category":"math.OC"}
{"created":"2024-05-10 14:07:31","title":"Bayesian Analysis of Molecular Emission and Dust Continuum of Protoplanetary Disks","abstract":"The Mid-InfraRed Instrument (MIRI) on board the James Webb Space Telescope (JWST) probes the chemistry and dust mineralogy of the inner regions of protoplanetary disks. The observed spectra are unprecedented in their detail, complicating interpretations which are mainly based on manual continuum subtraction and 0D slab models. We investigate the physical conditions under which the gas emits in protoplanetary disks. Based on MIRI spectra, we apply a full Bayesian analysis that provides the posterior distributions of dust and molecular properties. For doing so, we introduce the Dust Continuum Kit with Line emission from Gas (DuCKLinG), a model describing the molecular line emission and the dust continuum simultaneously without large computational cost. The dust model is based on work by Juhasz et al. (2009, 2010). The molecular emission is based on LTE slab models, but with radial gradients in column densities and temperatures. The model is compared to observations using Bayesian analysis. We benchmark this model to a complex thermo-chemical ProDiMo model and fit the MIRI spectrum of GWLup. We find that the retrieved molecular conditions from DuCKLinG fall within the true values from ProDiMo. The column densities retrieved by Grant et al. (2023) fall within the retrieved ranges in this study for all examined molecules (CO2, H2O, HCN, and C2H2). Similar overlap is found for the temperatures with only the temperature range of HCN not including the previously found value. This discrepancy may be due to the simultaneous fitting of all molecules compared to the step-by-step fitting of the previous study. There is statistically significant evidence for radial temperature and column density gradients for H2O and CO2 compared to the constant temperature and column density assumed in the 0D slab models. Additionally, HCN and C2H2 emit from a small region with near constant conditions.","sentences":["The Mid-InfraRed Instrument (MIRI) on board the James Webb Space Telescope (JWST) probes the chemistry and dust mineralogy of the inner regions of protoplanetary disks.","The observed spectra are unprecedented in their detail, complicating interpretations which are mainly based on manual continuum subtraction and 0D slab models.","We investigate the physical conditions under which the gas emits in protoplanetary disks.","Based on MIRI spectra, we apply a full Bayesian analysis that provides the posterior distributions of dust and molecular properties.","For doing so, we introduce the Dust Continuum Kit with Line emission from Gas (DuCKLinG), a model describing the molecular line emission and the dust continuum simultaneously without large computational cost.","The dust model is based on work by Juhasz et al.","(2009, 2010).","The molecular emission is based on LTE slab models, but with radial gradients in column densities and temperatures.","The model is compared to observations using Bayesian analysis.","We benchmark this model to a complex thermo-chemical ProDiMo model and fit the MIRI spectrum of GWLup.","We find that the retrieved molecular conditions from DuCKLinG fall within the true values from ProDiMo.","The column densities retrieved by Grant et al. (2023) fall within the retrieved ranges in this study for all examined molecules (CO2, H2O, HCN, and C2H2).","Similar overlap is found for the temperatures with only the temperature range of HCN not including the previously found value.","This discrepancy may be due to the simultaneous fitting of all molecules compared to the step-by-step fitting of the previous study.","There is statistically significant evidence for radial temperature and column density gradients for H2O and CO2 compared to the constant temperature and column density assumed in the 0D slab models.","Additionally, HCN and C2H2 emit from a small region with near constant conditions."],"url":"http://arxiv.org/abs/2405.06486v1","category":"astro-ph.EP"}
{"created":"2024-05-10 14:06:46","title":"Non-Linear Dynamics and Critical Phenomena in the Holographic Landscape of Weyl Semimetals","abstract":"This paper analyzes critical exponents in a holographic Weyl semi-metal (WSM) using the $D3/D7$ brane setup. We study the non-linear behavior of the longitudinal current $J$ interacting with an external electric field $E$ at zero and finite temperatures. At zero temperature, we identify a potential quantum phase transition in the $J$-$E$ relationship driven by critical background parameters. At zero temperature, we pinpoint a potential quantum phase transition in the $J$-$E$ relationship, driven by a critical ratio of background parameters. This transition is a unique reconnection phenomenon, emerging from the interplay between WSM-like and ordinary nonlinear conducting behaviors, signaling a quantum phase transition. At nonzero temperature, with dissipation, the system exhibits both first- and second-order phase transitions by varying the electric field and the axial anomaly. We introduce longitudinal conductivity as an order parameter for the current-driven phase transition. Remarkably, our numerical analysis indicates critical exponents in this non-equilibrium phase transition that resemble the mean-field values found in metallic systems. This study sheds light on critical phenomena in non-equilibrium states, offering new insights into the quantum critical behavior of holographic systems and the nonlinear dynamics in WSMs, with broader implications for quantum phase transitions in condensed matter physics and topological materials.","sentences":["This paper analyzes critical exponents in a holographic Weyl semi-metal (WSM) using the $D3/D7$ brane setup.","We study the non-linear behavior of the longitudinal current $J$ interacting with an external electric field $E$ at zero and finite temperatures.","At zero temperature, we identify a potential quantum phase transition in the $J$-$E$ relationship driven by critical background parameters.","At zero temperature, we pinpoint a potential quantum phase transition in the $J$-$E$ relationship, driven by a critical ratio of background parameters.","This transition is a unique reconnection phenomenon, emerging from the interplay between WSM-like and ordinary nonlinear conducting behaviors, signaling a quantum phase transition.","At nonzero temperature, with dissipation, the system exhibits both first- and second-order phase transitions by varying the electric field and the axial anomaly.","We introduce longitudinal conductivity as an order parameter for the current-driven phase transition.","Remarkably, our numerical analysis indicates critical exponents in this non-equilibrium phase transition that resemble the mean-field values found in metallic systems.","This study sheds light on critical phenomena in non-equilibrium states, offering new insights into the quantum critical behavior of holographic systems and the nonlinear dynamics in WSMs, with broader implications for quantum phase transitions in condensed matter physics and topological materials."],"url":"http://arxiv.org/abs/2405.06484v1","category":"hep-th"}
{"created":"2024-05-10 13:39:22","title":"Autonomous Driving with a Deep Dual-Model Solution for Steering and Braking Control","abstract":"The technology of autonomous driving is currently attracting a great deal of interest in both research and industry. In this paper, we present a deep learning dual-model solution that uses two deep neural networks for combined braking and steering in autonomous vehicles. Steering control is achieved by applying the NVIDIA's PilotNet model to predict the steering wheel angle, while braking control relies on the use of MobileNet SSD. Both models rely on a single front-facing camera for image input. The MobileNet SSD model is suitable for devices with constrained resources, whereas PilotNet struggles to operate efficiently on smaller devices with limited resources. To make it suitable for such devices, we modified the PilotNet model using our own original network design and reduced the number of model parameters and its memory footprint by approximately 60%. The inference latency has also been reduced, making the model more suitable to operate on resource-constrained devices. The modified PilotNet model achieves similar loss and accuracy compared to the original PilotNet model. When evaluated in a simulated environment, both autonomous driving systems, one using the modified PilotNet model and the other using the original PilotNet model for steering, show similar levels of autonomous driving performance.","sentences":["The technology of autonomous driving is currently attracting a great deal of interest in both research and industry.","In this paper, we present a deep learning dual-model solution that uses two deep neural networks for combined braking and steering in autonomous vehicles.","Steering control is achieved by applying the NVIDIA's PilotNet model to predict the steering wheel angle, while braking control relies on the use of MobileNet SSD.","Both models rely on a single front-facing camera for image input.","The MobileNet SSD model is suitable for devices with constrained resources, whereas PilotNet struggles to operate efficiently on smaller devices with limited resources.","To make it suitable for such devices, we modified the PilotNet model using our own original network design and reduced the number of model parameters and its memory footprint by approximately 60%.","The inference latency has also been reduced, making the model more suitable to operate on resource-constrained devices.","The modified PilotNet model achieves similar loss and accuracy compared to the original PilotNet model.","When evaluated in a simulated environment, both autonomous driving systems, one using the modified PilotNet model and the other using the original PilotNet model for steering, show similar levels of autonomous driving performance."],"url":"http://arxiv.org/abs/2405.06473v1","category":"cs.RO"}
{"created":"2024-05-10 13:25:30","title":"Typical dimension and absolute continuity for classes of dynamically defined measures, Part II : exposition and extensions","abstract":"This paper is partly an exposition, and partly an extension of our work [1] to the multiparameter case. We consider certain classes of parametrized dynamically defined measures. These are push-forwards, under the natural projection, of ergodic measures for parametrized families of smooth iterated function systems (IFS) on the line. Under some assumptions, most crucially, a transversality condition, we obtain formulas for the Hausdorff dimension of the measure and absolute continuity for almost every parameter in the appropriate parameter region. The main novelty of [1] and the present paper is that not only the IFS, but also the ergodic measure in the symbolic space, whose push-forward we consider, depends on the parameter. This includes many interesting families of measures, in particular, invariant measures for IFS's with place-dependent probabilities and natural (equilibrium) measures for smooth IFS's. One of the goals of this paper is to present an exposition of [1] in a more reader-friendly way, emphasizing the ideas and proof strategies, but omitting the more technical parts. This exposition/survey is based in part on the series of lectures by K\\'aroly Simon at the Summer School \"Dynamics and Fractals\" in 2023 at the Banach Center, Warsaw. The main new feature, compared to [1], is that we consider multi-parameter families; in other words, the set of parameters is allowed to be multi-dimensional. This broadens the scope of applications. A new application considered here is to a class of Furstenberg-like measures.   [1] B. B\\'ar\\'any, K. Simon, B. Solomyak and A. \\'Spiewak: Typical absolute continuity for classes of dynamically defined measures. Advances in Mathematics, Volume 399, 2022, 108258, ISSN 0001-8708, https://doi.org/10.1016/j.aim.2022.108258.","sentences":["This paper is partly an exposition, and partly an extension of our work [1] to the multiparameter case.","We consider certain classes of parametrized dynamically defined measures.","These are push-forwards, under the natural projection, of ergodic measures for parametrized families of smooth iterated function systems (IFS) on the line.","Under some assumptions, most crucially, a transversality condition, we obtain formulas for the Hausdorff dimension of the measure and absolute continuity for almost every parameter in the appropriate parameter region.","The main novelty of [1] and the present paper is that not only the IFS, but also the ergodic measure in the symbolic space, whose push-forward we consider, depends on the parameter.","This includes many interesting families of measures, in particular, invariant measures for IFS's with place-dependent probabilities and natural (equilibrium) measures for smooth IFS's.","One of the goals of this paper is to present an exposition of [1] in a more reader-friendly way, emphasizing the ideas and proof strategies, but omitting the more technical parts.","This exposition/survey is based in part on the series of lectures by K\\'aroly Simon at the Summer School \"Dynamics and Fractals\" in 2023 at the Banach Center, Warsaw.","The main new feature, compared to [1], is that we consider multi-parameter families; in other words, the set of parameters is allowed to be multi-dimensional.","This broadens the scope of applications.","A new application considered here is to a class of Furstenberg-like measures.   ","[1] B. B\\'ar\\'any, K. Simon, B. Solomyak and A. \\'Spiewak: Typical absolute continuity for classes of dynamically defined measures.","Advances in Mathematics, Volume 399, 2022, 108258, ISSN 0001-8708, https://doi.org/10.1016/j.aim.2022.108258."],"url":"http://arxiv.org/abs/2405.06466v1","category":"math.DS"}
{"created":"2024-05-10 13:23:06","title":"Quantum-Accurate Machine Learning Potentials for Metal-Organic Frameworks using Temperature Driven Active Learning","abstract":"Understanding how structural flexibility affects the properties of metal-organic frameworks (MOFs) is crucial for the design of better MOFs for targeted applications. Flexible MOFs can be studied with molecular dynamics simulations, whose accuracy depends on the force-field used to describe the interatomic interactions. Density functional theory (DFT) and quantum-chemistry methods are highly accurate, but the computational overheads limit their use in long time-dependent simulations for large systems. In contrast, classical force fields usually struggle with the description of coordination bonds.   In this work we develop a DFT-accurate machine-learning spectral neighbor analysis potential, trained on DFT energies, forces and stress tensors, for two representative MOFs, namely ZIF-8 and MOF-5. Their structural and vibrational properties are then studied as a function of temperature and tightly compared with available experimental data. Most importantly, we demonstrate an active-learning algorithm, based on mapping the relevant internal coordinates, which drastically reduces the number of training data to be computed at the DFT level. Thus, the workflow presented here appears as an efficient strategy for the study of flexible MOFs with DFT accuracy, but at a fraction of the DFT computational cost.","sentences":["Understanding how structural flexibility affects the properties of metal-organic frameworks (MOFs) is crucial for the design of better MOFs for targeted applications.","Flexible MOFs can be studied with molecular dynamics simulations, whose accuracy depends on the force-field used to describe the interatomic interactions.","Density functional theory (DFT) and quantum-chemistry methods are highly accurate, but the computational overheads limit their use in long time-dependent simulations for large systems.","In contrast, classical force fields usually struggle with the description of coordination bonds.   ","In this work we develop a DFT-accurate machine-learning spectral neighbor analysis potential, trained on DFT energies, forces and stress tensors, for two representative MOFs, namely ZIF-8 and MOF-5.","Their structural and vibrational properties are then studied as a function of temperature and tightly compared with available experimental data.","Most importantly, we demonstrate an active-learning algorithm, based on mapping the relevant internal coordinates, which drastically reduces the number of training data to be computed at the DFT level.","Thus, the workflow presented here appears as an efficient strategy for the study of flexible MOFs with DFT accuracy, but at a fraction of the DFT computational cost."],"url":"http://arxiv.org/abs/2405.06465v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 13:15:42","title":"MRSegmentator: Robust Multi-Modality Segmentation of 40 Classes in MRI and CT Sequences","abstract":"Purpose: To introduce a deep learning model capable of multi-organ segmentation in MRI scans, offering a solution to the current limitations in MRI analysis due to challenges in resolution, standardized intensity values, and variability in sequences.   Materials and Methods: he model was trained on 1,200 manually annotated MRI scans from the UK Biobank, 221 in-house MRI scans and 1228 CT scans, leveraging cross-modality transfer learning from CT segmentation models. A human-in-the-loop annotation workflow was employed to efficiently create high-quality segmentations. The model's performance was evaluated on NAKO and the AMOS22 dataset containing 600 and 60 MRI examinations. Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD) was used to assess segmentation accuracy. The model will be open sourced.   Results: The model showcased high accuracy in segmenting well-defined organs, achieving Dice Similarity Coefficient (DSC) scores of 0.97 for the right and left lungs, and 0.95 for the heart. It also demonstrated robustness in organs like the liver (DSC: 0.96) and kidneys (DSC: 0.95 left, 0.95 right), which present more variability. However, segmentation of smaller and complex structures such as the portal and splenic veins (DSC: 0.54) and adrenal glands (DSC: 0.65 left, 0.61 right) revealed the need for further model optimization.   Conclusion: The proposed model is a robust, tool for accurate segmentation of 40 anatomical structures in MRI and CT images. By leveraging cross-modality learning and interactive annotation, the model achieves strong performance and generalizability across diverse datasets, making it a valuable resource for researchers and clinicians. It is open source and can be downloaded from https://github.com/hhaentze/MRSegmentator.","sentences":["Purpose: To introduce a deep learning model capable of multi-organ segmentation in MRI scans, offering a solution to the current limitations in MRI analysis due to challenges in resolution, standardized intensity values, and variability in sequences.   ","Materials and Methods: he model was trained on 1,200 manually annotated MRI scans from the UK Biobank, 221 in-house MRI scans and 1228 CT scans, leveraging cross-modality transfer learning from CT segmentation models.","A human-in-the-loop annotation workflow was employed to efficiently create high-quality segmentations.","The model's performance was evaluated on NAKO and the AMOS22 dataset containing 600 and 60 MRI examinations.","Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD) was used to assess segmentation accuracy.","The model will be open sourced.   ","Results:","The model showcased high accuracy in segmenting well-defined organs, achieving Dice Similarity Coefficient (DSC) scores of 0.97 for the right and left lungs, and 0.95 for the heart.","It also demonstrated robustness in organs like the liver (DSC: 0.96) and kidneys (DSC: 0.95 left, 0.95 right), which present more variability.","However, segmentation of smaller and complex structures such as the portal and splenic veins (DSC: 0.54) and adrenal glands (DSC: 0.65 left, 0.61 right) revealed the need for further model optimization.   ","Conclusion: The proposed model is a robust, tool for accurate segmentation of 40 anatomical structures in MRI and CT images.","By leveraging cross-modality learning and interactive annotation, the model achieves strong performance and generalizability across diverse datasets, making it a valuable resource for researchers and clinicians.","It is open source and can be downloaded from https://github.com/hhaentze/MRSegmentator."],"url":"http://arxiv.org/abs/2405.06463v1","category":"eess.IV"}
{"created":"2024-05-10 13:08:05","title":"IETI-based Low-Rank method for PDE-constrained optimization","abstract":"Isogeometric Analysis (IgA) is a versatile method for the discretization of partial differential equations on complex domains, which arise in various applications of science and engineering. Some complex geometries can be better described as a computational domain by a multi-patch approach, where each patch is determined by a tensor product Non-Uniform Rational Basis Splines (NURBS) parameterization. This allows on the one hand to consider the problem of the complex assembly of mass or stiffness matrices (or tensors) over the whole geometry locally on the individual smaller patches, and on the other hand it is possible to perform local mesh refinements independently on each patch, allowing efficient local refinement in regions of high activity where higher accuracy is required, while coarser meshes can be used elsewhere. Furthermore, the information about differing material models or properties that are to apply in a subdomain of the geometry can be included in the patch in which this subdomain is located. For this it must be ensured that the approximate solution is continuous over the entire computational domain and therefore at the interfaces of two (or more) patches. The most promising approach for this problem, which transfers the idea of Finite Element Tearing and Interconnecting (FETI) methods into the isogeometric setup, was the IsogEometric Tearing and Interconnecting (IETI) method, where by introducing a constraints matrix and associated Lagrange multipliers and formulating it into a dual problem, depending only on the Lagrange multipliers, continuity at the interfaces was ensured in solving the resulting system. In this paper we illustrate that low-rank methods based on the tensor-train format can be generalised for a multi-patch IgA setup, which follows the IETI idea.","sentences":["Isogeometric Analysis (IgA) is a versatile method for the discretization of partial differential equations on complex domains, which arise in various applications of science and engineering.","Some complex geometries can be better described as a computational domain by a multi-patch approach, where each patch is determined by a tensor product Non-Uniform Rational Basis Splines (NURBS) parameterization.","This allows on the one hand to consider the problem of the complex assembly of mass or stiffness matrices (or tensors) over the whole geometry locally on the individual smaller patches, and on the other hand it is possible to perform local mesh refinements independently on each patch, allowing efficient local refinement in regions of high activity where higher accuracy is required, while coarser meshes can be used elsewhere.","Furthermore, the information about differing material models or properties that are to apply in a subdomain of the geometry can be included in the patch in which this subdomain is located.","For this it must be ensured that the approximate solution is continuous over the entire computational domain and therefore at the interfaces of two (or more) patches.","The most promising approach for this problem, which transfers the idea of Finite Element Tearing and Interconnecting (FETI) methods into the isogeometric setup, was the IsogEometric Tearing and Interconnecting (IETI) method, where by introducing a constraints matrix and associated Lagrange multipliers and formulating it into a dual problem, depending only on the Lagrange multipliers, continuity at the interfaces was ensured in solving the resulting system.","In this paper we illustrate that low-rank methods based on the tensor-train format can be generalised for a multi-patch IgA setup, which follows the IETI idea."],"url":"http://arxiv.org/abs/2405.06458v1","category":"math.NA"}
{"created":"2024-05-10 13:05:27","title":"Managing Forensic Recovery in the Cloud","abstract":"As organisations move away from locally hosted computer services toward Cloud platforms, there is a corresponding need to ensure the forensic integrity of such instances. The primary reasons for concern are (i) the locus of responsibility, and (ii) the associated risk of legal sanction and financial penalty. Building upon previously proposed techniques for intrusion monitoring, we highlight the multi-level interpretation problem, propose enhanced monitoring of Cloud-based systems at diverse operational and data storage level as a basis for review of historical change across the hosted system and afford scope to identify any data impact from hostile action or 'friendly fire'.","sentences":["As organisations move away from locally hosted computer services toward Cloud platforms, there is a corresponding need to ensure the forensic integrity of such instances.","The primary reasons for concern are (i) the locus of responsibility, and (ii) the associated risk of legal sanction and financial penalty.","Building upon previously proposed techniques for intrusion monitoring, we highlight the multi-level interpretation problem, propose enhanced monitoring of Cloud-based systems at diverse operational and data storage level as a basis for review of historical change across the hosted system and afford scope to identify any data impact from hostile action or 'friendly fire'."],"url":"http://arxiv.org/abs/2405.06455v1","category":"cs.DC"}
{"created":"2024-05-10 12:52:46","title":"Sandboxing Adoption in Open Source Ecosystems","abstract":"Sandboxing mechanisms allow developers to limit how much access applications have to resources, following the least-privilege principle. However, it's not clear how much and in what ways developers are using these mechanisms. This study looks at the use of Seccomp, Landlock, Capsicum, Pledge, and Unveil in all packages of four open-source operating systems. We found that less than 1% of packages directly use these mechanisms, but many more indirectly use them. Examining how developers apply these mechanisms reveals interesting usage patterns, such as cases where developers simplify their sandbox implementation. It also highlights challenges that may be hindering the widespread adoption of sandboxing mechanisms.","sentences":["Sandboxing mechanisms allow developers to limit how much access applications have to resources, following the least-privilege principle.","However, it's not clear how much and in what ways developers are using these mechanisms.","This study looks at the use of Seccomp, Landlock, Capsicum, Pledge, and Unveil in all packages of four open-source operating systems.","We found that less than 1% of packages directly use these mechanisms, but many more indirectly use them.","Examining how developers apply these mechanisms reveals interesting usage patterns, such as cases where developers simplify their sandbox implementation.","It also highlights challenges that may be hindering the widespread adoption of sandboxing mechanisms."],"url":"http://arxiv.org/abs/2405.06447v1","category":"cs.SE"}
{"created":"2024-05-10 12:50:52","title":"Systematic interval observer design for linear systems","abstract":"We first propose systematic and comprehensive interval observer designs for linear time-invariant systems, under standard assumptions. Historically, such designs rely on transformations with certain limitations into a form that is Metzler (for continuous time) or non-negative (for discrete time). We show that they can be effectively replaced with a linear time-invariant transformation that can be easily computed offline. Then, we propose the extension to the time-varying setting, where conventional transformations lack guaranteed outcomes. Academic examples are presented to illustrate our methods.","sentences":["We first propose systematic and comprehensive interval observer designs for linear time-invariant systems, under standard assumptions.","Historically, such designs rely on transformations with certain limitations into a form that is Metzler (for continuous time) or non-negative (for discrete time).","We show that they can be effectively replaced with a linear time-invariant transformation that can be easily computed offline.","Then, we propose the extension to the time-varying setting, where conventional transformations lack guaranteed outcomes.","Academic examples are presented to illustrate our methods."],"url":"http://arxiv.org/abs/2405.06445v1","category":"eess.SY"}
{"created":"2024-05-10 12:42:57","title":"Low-rank quantics tensor train representations of Feynman diagrams for multiorbital electron-phonon models","abstract":"Feynman diagrams are an essential tool for simulating strongly correlated electron systems. However, stochastic quantum Monte Carlo (QMC) sampling suffers from the sign problem, e.g., when solving a multiorbital quantum impurity model. Recently, two approaches have been proposed for efficient numerical treatment of Feynman diagrams: Tensor Cross Interpolation (TCI) for replacing the stochastic sampling and the Quantics Tensor Train (QTT) representation for compressing space-time dependence. Combining these approaches, we find low-rank structures in weak-coupling Feynman diagrams for a multiorbital electron-phonon model and demonstrate their efficient numerical integrations with exponential resolution in time and exponential convergence of error with respect to computational cost.","sentences":["Feynman diagrams are an essential tool for simulating strongly correlated electron systems.","However, stochastic quantum Monte Carlo (QMC) sampling suffers from the sign problem, e.g., when solving a multiorbital quantum impurity model.","Recently, two approaches have been proposed for efficient numerical treatment of Feynman diagrams: Tensor Cross Interpolation (TCI) for replacing the stochastic sampling and the Quantics Tensor Train (QTT) representation for compressing space-time dependence.","Combining these approaches, we find low-rank structures in weak-coupling Feynman diagrams for a multiorbital electron-phonon model and demonstrate their efficient numerical integrations with exponential resolution in time and exponential convergence of error with respect to computational cost."],"url":"http://arxiv.org/abs/2405.06440v1","category":"cond-mat.str-el"}
{"created":"2024-05-10 12:31:18","title":"Deceleration of kicked objects due to the Galactic potential","abstract":"Various stellar objects experience a velocity kick at some point in their evolution. These include neutron stars and black holes at their birth or binary systems when one of the two components goes supernova. For most of these objects, the magnitude of the kick and its impact on the object dynamics remains a topic of debate. We investigate how kicks alter the velocity distribution of objects born in the Milky Way disc, both immediately after the kick and at later times, and whether these kicks are encoded in the observed population of Galactic neutron stars. We simulate the Galactic trajectories of point masses on circular orbits in the disc after being perturbed by an isotropic kick, with a Maxwellian distribution of magnitudes with $\\sigma=265$ km/s. Then, we simulate the motion of these point masses for $200$ Myr. These trajectories are then evaluated, either for the Milky Way population as a whole or for those passing within two kiloparsecs of the Sun, to get the time evolution of the velocities. During the first $20$ Myr, the bulk velocity of kicked objects becomes temporarily aligned to the cylindrical radius, implying an anisotropy in the velocity orientations. Beyond this age, the velocity distribution shifts toward lower values and settles to a median of $\\sim200$ km/s. Around the Sun, the distribution also loses its upper tail, primarily due to unbound objects escaping the Galaxy. We compare this to the velocities of Galactic pulsars and find that pulsars show a similar evolution with characteristic age. The shift of the velocity distribution is due to bound objects spending most of their orbits at larger radii after the kick. They are, therefore, decelerated by the Galactic potential. We find the same deceleration to be predicted for nearby objects and the total population and conclude it is also observed in Galactic pulsars.","sentences":["Various stellar objects experience a velocity kick at some point in their evolution.","These include neutron stars and black holes at their birth or binary systems when one of the two components goes supernova.","For most of these objects, the magnitude of the kick and its impact on the object dynamics remains a topic of debate.","We investigate how kicks alter the velocity distribution of objects born in the Milky Way disc, both immediately after the kick and at later times, and whether these kicks are encoded in the observed population of Galactic neutron stars.","We simulate the Galactic trajectories of point masses on circular orbits in the disc after being perturbed by an isotropic kick, with a Maxwellian distribution of magnitudes with $\\sigma=265$ km/s. Then, we simulate the motion of these point masses for $200$ Myr.","These trajectories are then evaluated, either for the Milky Way population as a whole or for those passing within two kiloparsecs of the Sun, to get the time evolution of the velocities.","During the first $20$ Myr, the bulk velocity of kicked objects becomes temporarily aligned to the cylindrical radius, implying an anisotropy in the velocity orientations.","Beyond this age, the velocity distribution shifts toward lower values and settles to a median of $\\sim200$ km/s. Around the Sun, the distribution also loses its upper tail, primarily due to unbound objects escaping the Galaxy.","We compare this to the velocities of Galactic pulsars and find that pulsars show a similar evolution with characteristic age.","The shift of the velocity distribution is due to bound objects spending most of their orbits at larger radii after the kick.","They are, therefore, decelerated by the Galactic potential.","We find the same deceleration to be predicted for nearby objects and the total population and conclude it is also observed in Galactic pulsars."],"url":"http://arxiv.org/abs/2405.06436v1","category":"astro-ph.GA"}
{"created":"2024-05-10 12:24:09","title":"A parametrization algorithm to compute lower dimensional elliptic tori in Hamiltonian systems","abstract":"We present an algorithm for the construction of lower dimensional elliptic tori in parametric Hamiltonian systems by means of the parametrization method with the tangent and normal frequencies being prescribed. This requires that the Hamiltonian system has as many parameters as the dimension of the normal dynamics, and the algorithm must adjust these parameters. We illustrate the methodology with an implementation of the algorithm computing $2$--dimensional elliptic tori in a system of $4$ coupled pendula (4 degrees of freedom).","sentences":["We present an algorithm for the construction of lower dimensional elliptic tori in parametric Hamiltonian systems by means of the parametrization method with the tangent and normal frequencies being prescribed.","This requires that the Hamiltonian system has as many parameters as the dimension of the normal dynamics, and the algorithm must adjust these parameters.","We illustrate the methodology with an implementation of the algorithm computing $2$--dimensional elliptic tori in a system of $4$ coupled pendula (4 degrees of freedom)."],"url":"http://arxiv.org/abs/2405.06432v1","category":"math.DS"}
{"created":"2024-05-10 12:23:43","title":"A Comprehensive Study of an Oscillating Eclipsing Algol: Y Cam","abstract":"Y Cam is classified as one of the oscillating Eclipsing Algol (oEA) systems, which feature a $\\delta$ Scuti-type pulsating component alongside mass transfer phenomena. oEA systems are invaluable for probing the evolutionary processes and internal structures of binary components offering insights through their binary variations and oscillating. In this study, we conducted a comprehensive investigation of Y Cam utilizing high-quality photometric TESS data, and high-resolution ELODIE spectra. Through our analysis, we examined the radial velocity variation, performed binary modeling, and calculated the effective temperature values of binary components. The fundamental stellar parameters, such as mass and radius, were determined with an accuracy of $\\sim$ $2-6$ %. Furthermore, we examined the orbital period variation to assess the amount of mass transfer using the available minima times of the system and three new minima times obtained from TESS light curves. Analyzing the pulsation structure of the system with the TESS data revealed the dominant pulsation period and amplitude of the pulsating component to be 0.066 d and 4.65 mmag, respectively. Notably, we observed frequency modulations with the orbital period's frequency, along with variations in the amplitude of the highest amplitude frequency across different orbital phases. Remarkably, the amplitude reaches its peak at phases 0.5 and 1. These findings indicate a candidate of a tidally tilted pulsator. Consequently, we investigated the evolutionary status of the binary components using MESA binary evolution models, determining the age of the system to be 3.28 $\\pm$ 0.09 Gyr.","sentences":["Y Cam is classified as one of the oscillating Eclipsing Algol (oEA) systems, which feature a $\\delta$ Scuti-type pulsating component alongside mass transfer phenomena.","oEA systems are invaluable for probing the evolutionary processes and internal structures of binary components offering insights through their binary variations and oscillating.","In this study, we conducted a comprehensive investigation of Y Cam utilizing high-quality photometric TESS data, and high-resolution ELODIE spectra.","Through our analysis, we examined the radial velocity variation, performed binary modeling, and calculated the effective temperature values of binary components.","The fundamental stellar parameters, such as mass and radius, were determined with an accuracy of $\\sim$ $2-6$ %.","Furthermore, we examined the orbital period variation to assess the amount of mass transfer using the available minima times of the system and three new minima times obtained from TESS light curves.","Analyzing the pulsation structure of the system with the TESS data revealed the dominant pulsation period and amplitude of the pulsating component to be 0.066 d and 4.65 mmag, respectively.","Notably, we observed frequency modulations with the orbital period's frequency, along with variations in the amplitude of the highest amplitude frequency across different orbital phases.","Remarkably, the amplitude reaches its peak at phases 0.5 and 1.","These findings indicate a candidate of a tidally tilted pulsator.","Consequently, we investigated the evolutionary status of the binary components using MESA binary evolution models, determining the age of the system to be 3.28 $\\pm$ 0.09 Gyr."],"url":"http://arxiv.org/abs/2405.06431v1","category":"astro-ph.SR"}
{"created":"2024-05-10 12:15:02","title":"Koopman-Based Surrogate Modelling of Turbulent Rayleigh-B\u00e9nard Convection","abstract":"Several related works have introduced Koopman-based Machine Learning architectures as a surrogate model for dynamical systems. These architectures aim to learn non-linear measurements (also known as observables) of the system's state that evolve by a linear operator and are, therefore, amenable to model-based linear control techniques. So far, mainly simple systems have been targeted, and Koopman architectures as reduced-order models for more complex dynamics have not been fully explored. Hence, we use a Koopman-inspired architecture called the Linear Recurrent Autoencoder Network (LRAN) for learning reduced-order dynamics in convection flows of a Rayleigh B\\'enard Convection (RBC) system at different amounts of turbulence. The data is obtained from direct numerical simulations of the RBC system. A traditional fluid dynamics method, the Kernel Dynamic Mode Decomposition (KDMD), is used to compare the LRAN. For both methods, we performed hyperparameter sweeps to identify optimal settings. We used a Normalized Sum of Square Error measure for the quantitative evaluation of the models, and we also studied the model predictions qualitatively. We obtained more accurate predictions with the LRAN than with KDMD in the most turbulent setting. We conjecture that this is due to the LRAN's flexibility in learning complicated observables from data, thereby serving as a viable surrogate model for the main structure of fluid dynamics in turbulent convection settings. In contrast, KDMD was more effective in lower turbulence settings due to the repetitiveness of the convection flow. The feasibility of Koopman-based surrogate models for turbulent fluid flows opens possibilities for efficient model-based control techniques useful in a variety of industrial settings.","sentences":["Several related works have introduced Koopman-based Machine Learning architectures as a surrogate model for dynamical systems.","These architectures aim to learn non-linear measurements (also known as observables) of the system's state that evolve by a linear operator and are, therefore, amenable to model-based linear control techniques.","So far, mainly simple systems have been targeted, and Koopman architectures as reduced-order models for more complex dynamics have not been fully explored.","Hence, we use a Koopman-inspired architecture called the Linear Recurrent Autoencoder Network (LRAN) for learning reduced-order dynamics in convection flows of a Rayleigh B\\'enard Convection (RBC) system at different amounts of turbulence.","The data is obtained from direct numerical simulations of the RBC system.","A traditional fluid dynamics method, the Kernel Dynamic Mode Decomposition (KDMD), is used to compare the LRAN.","For both methods, we performed hyperparameter sweeps to identify optimal settings.","We used a Normalized Sum of Square Error measure for the quantitative evaluation of the models, and we also studied the model predictions qualitatively.","We obtained more accurate predictions with the LRAN than with KDMD in the most turbulent setting.","We conjecture that this is due to the LRAN's flexibility in learning complicated observables from data, thereby serving as a viable surrogate model for the main structure of fluid dynamics in turbulent convection settings.","In contrast, KDMD was more effective in lower turbulence settings due to the repetitiveness of the convection flow.","The feasibility of Koopman-based surrogate models for turbulent fluid flows opens possibilities for efficient model-based control techniques useful in a variety of industrial settings."],"url":"http://arxiv.org/abs/2405.06425v1","category":"cs.LG"}
{"created":"2024-05-10 12:10:59","title":"On the logical Janus-face of weak continuity","abstract":"Continuity is one of the most central notions in mathematics and related areas. An interesting related topic is decompositions of continuity, where continuity is shown to be equivalent to the combination of two or more weak continuity notions. Now, from a logical viewpoint, continuity is fairly tame in that its basic properties are readily established in rather weak logical systems. It is then a natural question whether the same holds for the aforementioned weak continuity notions. In this paper, we establish the Janus-face nature of weak continuity notions: we identify numerous weak continuity notions that are as tame as continuity, but also list a number of extremely wild weak continuity notions for which basic properties, like finding the supremum or evaluating the Riemann integral, are extremely hard to prove. To make our heuristic notions (tame, wild, weak) precise, we adopt Reverse Mathematics, a program in the foundation of mathematics that seeks to identify the minimal axioms that prove a given theorem. Particularly surprising and novel is that our `wild' theorems exist at the outer edges of Reverse Mathematics, i.e. while rather basic, they imply full second-order arithmetic.","sentences":["Continuity is one of the most central notions in mathematics and related areas.","An interesting related topic is decompositions of continuity, where continuity is shown to be equivalent to the combination of two or more weak continuity notions.","Now, from a logical viewpoint, continuity is fairly tame in that its basic properties are readily established in rather weak logical systems.","It is then a natural question whether the same holds for the aforementioned weak continuity notions.","In this paper, we establish the Janus-face nature of weak continuity notions: we identify numerous weak continuity notions that are as tame as continuity, but also list a number of extremely wild weak continuity notions for which basic properties, like finding the supremum or evaluating the Riemann integral, are extremely hard to prove.","To make our heuristic notions (tame, wild, weak) precise, we adopt Reverse Mathematics, a program in the foundation of mathematics that seeks to identify the minimal axioms that prove a given theorem.","Particularly surprising and novel is that our `wild' theorems exist at the outer edges of Reverse Mathematics, i.e. while rather basic, they imply full second-order arithmetic."],"url":"http://arxiv.org/abs/2405.06420v1","category":"math.LO"}
{"created":"2024-05-10 12:00:02","title":"Hadron Spectroscopy: Light, Strange Baryons","abstract":"The resonance mass spectra have been studied through a non-relativistic hypercentral Constituent Quark Model (hCQM) using a linear potential. Also, the effects of higher order correction terms (${\\cal{O}}(\\frac{1}{m})$, ${\\cal{O}}(\\frac{1}{m^{2}})$) have been studied for improvisation of the results. Other baryonic properties such as Regge trajectories, magnetic moment and decay widths have been considered. A detailed comparison with other approaches are discussed in the present review.","sentences":["The resonance mass spectra have been studied through a non-relativistic hypercentral Constituent Quark Model (hCQM) using a linear potential.","Also, the effects of higher order correction terms (${\\cal{O}}(\\frac{1}{m})$, ${\\cal{O}}(\\frac{1}{m^{2}})$) have been studied for improvisation of the results.","Other baryonic properties such as Regge trajectories, magnetic moment and decay widths have been considered.","A detailed comparison with other approaches are discussed in the present review."],"url":"http://arxiv.org/abs/2405.06417v1","category":"hep-ph"}
{"created":"2024-05-10 11:56:05","title":"Biaxial strain effects in 2D diamond formation from graphene stacks","abstract":"Discovering innovative methods to understand phase transitions, modify phase diagrams, and uncover novel synthesis routes poses significant and far-reaching challenges. In this study, we demonstrate the formation of nanodiamond-like sp3 carbon from few-layer graphene (FLG) stacks at room temperature and relatively low transition pressure (~7.0 GPa) due to chemical interaction with water and physical biaxial strain induced by substrate compression. By employing resonance Raman and optical absorption spectroscopies at high-pressure on FLG systems, utilizing van der Waals heterostructures (hBN/FLG) on different substrates (SiO2/Si and diamond), we originally unveiled the key role of biaxial strain. Ab initio molecular dynamics simulations corroborates the pivotal role of both water and biaxial strain in locally stabilizing sp3 carbon structures at the graphene-ice interface. This breakthrough directly enhances nanodiamond technology but also establishes biaxial strain engineering as a promising tool to explore novel phases of 2D nanomaterials.","sentences":["Discovering innovative methods to understand phase transitions, modify phase diagrams, and uncover novel synthesis routes poses significant and far-reaching challenges.","In this study, we demonstrate the formation of nanodiamond-like sp3 carbon from few-layer graphene (FLG) stacks at room temperature and relatively low transition pressure (~7.0 GPa) due to chemical interaction with water and physical biaxial strain induced by substrate compression.","By employing resonance Raman and optical absorption spectroscopies at high-pressure on FLG systems, utilizing van der Waals heterostructures (hBN/FLG) on different substrates (SiO2/Si and diamond), we originally unveiled the key role of biaxial strain.","Ab initio molecular dynamics simulations corroborates the pivotal role of both water and biaxial strain in locally stabilizing sp3 carbon structures at the graphene-ice interface.","This breakthrough directly enhances nanodiamond technology but also establishes biaxial strain engineering as a promising tool to explore novel phases of 2D nanomaterials."],"url":"http://arxiv.org/abs/2405.06416v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 11:52:40","title":"Solving the Turbine Balancing Problem using Quantum Annealing","abstract":"Quantum computing has the potential for disruptive change in many sectors of industry, especially in materials science and optimization. In this paper, we describe how the Turbine Balancing Problem can be solved with quantum computing, which is the NP-hard optimization problem of analytically balancing rotor blades in a single plane as found in turbine assembly. Small yet relevant instances occur in industry, which makes the problem interesting for early quantum computing benchmarks. We model it as a Quadratic Unconstrained Binary Optimization problem and compare the performance of a classical rule-based heuristic and D-Wave Systems' Quantum Annealer Advantage_system4.1. In this case study, we use real-world as well as synthetic datasets and observe that the quantum hardware significantly improves an actively used heuristic's solution for small-scale problem instances with bare disk imbalance in terms of solution quality. Motivated by this performance gain, we subsequently design a quantum-inspired classical heuristic based on simulated annealing that achieves extremely good results on all given problem instances, essentially solving the optimization problem sufficiently well for all considered datasets, according to industrial requirements.","sentences":["Quantum computing has the potential for disruptive change in many sectors of industry, especially in materials science and optimization.","In this paper, we describe how the Turbine Balancing Problem can be solved with quantum computing, which is the NP-hard optimization problem of analytically balancing rotor blades in a single plane as found in turbine assembly.","Small yet relevant instances occur in industry, which makes the problem interesting for early quantum computing benchmarks.","We model it as a Quadratic Unconstrained Binary Optimization problem and compare the performance of a classical rule-based heuristic and D-Wave Systems' Quantum Annealer Advantage_system4.1.","In this case study, we use real-world as well as synthetic datasets and observe that the quantum hardware significantly improves an actively used heuristic's solution for small-scale problem instances with bare disk imbalance in terms of solution quality.","Motivated by this performance gain, we subsequently design a quantum-inspired classical heuristic based on simulated annealing that achieves extremely good results on all given problem instances, essentially solving the optimization problem sufficiently well for all considered datasets, according to industrial requirements."],"url":"http://arxiv.org/abs/2405.06412v1","category":"quant-ph"}
{"created":"2024-05-10 11:41:46","title":"Transmission of a Pressure Signal through a Confined Bubble Array","abstract":"Pressure changes travel at an infinite sound speed in an incompressible ideal fluid, opposite to what happens in a bubbly liquid, where the presence of bubbles adds compressibility such that the sound speed becomes finite. Here, the transmission of a pressure signal through a confined bubble array is studied numerically. An axisymmetric Boundary Integral (BI) code was used to simulate a horizontal array of spherical bubbles inside a cylindrical container. On one end of the container, a piston is able to move in either a sinusoidal or impulsive way to excite the bubbles, whereas the other end of the cylinder is fixed. A one-dimensional model, hereafter called the Multiple Bagnold Problem (MBP) model, was developed to predict the behavior of the 3D system. A good agreement between the model and the simulations was found for initial excitation times and qualitatively good agreement for later times. The MBP model was subsequently used to obtain an expression for the sound speed in monodisperse systems, which depends on the ratio between the driving and the natural bubble frequency. Two regimes were found, one where the pressure signal travels unattenuated and the other where attenuation is present. Subsequently we turn to more complex, bidisperse systems, where the influence of the presence of bubbles of two different sizes is explored. Analytical expressions to predict the eigenfrequencies were obtained for monodisperse and bidisperse alternating systems. For bidisperse stacked systems the eigenfrequencies were computed numerically giving a good agreement with the MBP simulations. Finally, we discuss the propagation of a pressure signal through a polydisperse system.","sentences":["Pressure changes travel at an infinite sound speed in an incompressible ideal fluid, opposite to what happens in a bubbly liquid, where the presence of bubbles adds compressibility such that the sound speed becomes finite.","Here, the transmission of a pressure signal through a confined bubble array is studied numerically.","An axisymmetric Boundary Integral (BI) code was used to simulate a horizontal array of spherical bubbles inside a cylindrical container.","On one end of the container, a piston is able to move in either a sinusoidal or impulsive way to excite the bubbles, whereas the other end of the cylinder is fixed.","A one-dimensional model, hereafter called the Multiple Bagnold Problem (MBP) model, was developed to predict the behavior of the 3D system.","A good agreement between the model and the simulations was found for initial excitation times and qualitatively good agreement for later times.","The MBP model was subsequently used to obtain an expression for the sound speed in monodisperse systems, which depends on the ratio between the driving and the natural bubble frequency.","Two regimes were found, one where the pressure signal travels unattenuated and the other where attenuation is present.","Subsequently we turn to more complex, bidisperse systems, where the influence of the presence of bubbles of two different sizes is explored.","Analytical expressions to predict the eigenfrequencies were obtained for monodisperse and bidisperse alternating systems.","For bidisperse stacked systems the eigenfrequencies were computed numerically giving a good agreement with the MBP simulations.","Finally, we discuss the propagation of a pressure signal through a polydisperse system."],"url":"http://arxiv.org/abs/2405.06406v1","category":"physics.flu-dyn"}
{"created":"2024-05-10 11:30:31","title":"Stability and Optimal Control Study of a Stochastic Within-Host COVID-19 Model","abstract":"In this paper, we analyze the stochastic SIV model for COVID-19 at the within-host level and explore its dynamics. We first examine the existence and positivity of the solution of the stochastic model using Ito's formula and establish exponential stability. Next, we extend the model to a stochastic optimal control problem and investigate the effectiveness of control measures, specifically antiviral drugs and immunomodulators. Numerical simulations are conducted to complement the theoretical results in both cases. Simulation suggests that the noises play a crucial role in the dynamics of the disease and disease can be eradicated with appropriate values of the stochastic coefficient. Stochastic optimal control study suggests that the viral load count in an infected individual decreases with the optimal treatment rates and is most effective when the antiviral and immunomodulating drugs are administered together.","sentences":["In this paper, we analyze the stochastic SIV model for COVID-19 at the within-host level and explore its dynamics.","We first examine the existence and positivity of the solution of the stochastic model using Ito's formula and establish exponential stability.","Next, we extend the model to a stochastic optimal control problem and investigate the effectiveness of control measures, specifically antiviral drugs and immunomodulators.","Numerical simulations are conducted to complement the theoretical results in both cases.","Simulation suggests that the noises play a crucial role in the dynamics of the disease and disease can be eradicated with appropriate values of the stochastic coefficient.","Stochastic optimal control study suggests that the viral load count in an infected individual decreases with the optimal treatment rates and is most effective when the antiviral and immunomodulating drugs are administered together."],"url":"http://arxiv.org/abs/2405.06403v1","category":"math.DS"}
{"created":"2024-05-10 11:26:05","title":"Statistical mechanics from relational complex time with a pure state","abstract":"Thermodynamics and its quantum counterpart are traditionally described with statistical ensembles. Canonical typicality has related statistical mechanics for a system to ensembles of global energy eigen- states of system and its environment analyzing their cardinality. We show that the canonical density for a system emerges from a maximally entangled global state of system and environment through relational complex time evolution between system and environment without the need to maximize the entropy or to count states.","sentences":["Thermodynamics and its quantum counterpart are traditionally described with statistical ensembles.","Canonical typicality has related statistical mechanics for a system to ensembles of global energy eigen- states of system and its environment analyzing their cardinality.","We show that the canonical density for a system emerges from a maximally entangled global state of system and environment through relational complex time evolution between system and environment without the need to maximize the entropy or to count states."],"url":"http://arxiv.org/abs/2405.06401v1","category":"quant-ph"}
{"created":"2024-05-10 11:20:48","title":"Performance of UAV-based Cell-free mMIMO ISAC Networks: Tethered vs. Mobile","abstract":"The employment of unmanned aerial vehicles (UAVs) aligned with multistatic sensing in integrated sensing and communication (ISAC) systems can provide remarkable performance gains in sensing, by taking advantage of the cell-free massive multiple-input multiple-output (mMIMO) architecture. Under these considerations, in this paper, the achievable sensing signal-to-noise-plus-interference ratio (SINR) of a cell-free mMIMO ISAC UAV-based network is evaluated for two different deployments of UAVs, namely, mobile and tethered. In both scenarios, a transmit precoder that jointly optimizes the sensing and communication requirements subjected to power constraints is designed. Specifically, for the scenario with mobile UAVs, beyond the transmit precoding, we also optimize the position of the transmit UAVs through particle swarm optimization (PSO). The results show that, although tethered UAVs have a more efficient power allocation, the proposed position control algorithm for the mobile UAVs can achieve a superior gain in terms of sensing SINR.","sentences":["The employment of unmanned aerial vehicles (UAVs) aligned with multistatic sensing in integrated sensing and communication (ISAC) systems can provide remarkable performance gains in sensing, by taking advantage of the cell-free massive multiple-input multiple-output (mMIMO) architecture.","Under these considerations, in this paper, the achievable sensing signal-to-noise-plus-interference ratio (SINR) of a cell-free mMIMO ISAC UAV-based network is evaluated for two different deployments of UAVs, namely, mobile and tethered.","In both scenarios, a transmit precoder that jointly optimizes the sensing and communication requirements subjected to power constraints is designed.","Specifically, for the scenario with mobile UAVs, beyond the transmit precoding, we also optimize the position of the transmit UAVs through particle swarm optimization (PSO).","The results show that, although tethered UAVs have a more efficient power allocation, the proposed position control algorithm for the mobile UAVs can achieve a superior gain in terms of sensing SINR."],"url":"http://arxiv.org/abs/2405.06398v1","category":"eess.SP"}
{"created":"2024-05-10 10:49:41","title":"Scalable Computation of Inter-Core Bounds Through Exact Abstractions","abstract":"Real-time systems (RTSs) are at the heart of numerous safety-critical applications. An RTS typically consists of a set of real-time tasks (the software) that execute on a multicore shared-memory platform (the hardware) following a scheduling policy. In an RTS, computing inter-core bounds, i.e., bounds separating events produced by tasks on different cores, is crucial. While efficient techniques to over-approximate such bounds exist, little has been proposed to compute their exact values. Given an RTS with a set of cores C and a set of tasks T , under partitioned fixed- priority scheduling with limited preemption, a recent work by Foughali, Hladik and Zuepke (FHZ) models tasks with affinity c (i.e., allocated to core c in C) as a Uppaal timed automata (TA) network Nc. For each core c in C, Nc integrates blocking (due to data sharing) using tight analytical formulae. Through compositional model checking, FHZ achieved a substantial gain in scalability for bounds local to a core. However, computing inter-core bounds for some events of interest E, produced by a subset of tasks TE with different affinities CE, requires model checking the parallel composition of all TA networks Nc for each c in CE, which produces a large, often intractable, state space. In this paper, we present a new scalable approach based on exact abstractions to compute exact inter-core bounds in a schedulable RTS, under the assumption that tasks in TE have distinct affinities. We develop a novel algorithm, leveraging a new query that we implement in Uppaal, that computes for each TA network Nc in NE an abstraction A(Nc) preserving the exact intervals within which events occur on c, therefore drastically reducing the state space. The scalability of our approach is demonstrated on the WATERS 2017 industrial challenge, for which we efficiently compute various types of inter-core bounds where FHZ fails to scale.","sentences":["Real-time systems (RTSs) are at the heart of numerous safety-critical applications.","An RTS typically consists of a set of real-time tasks (the software) that execute on a multicore shared-memory platform (the hardware) following a scheduling policy.","In an RTS, computing inter-core bounds, i.e., bounds separating events produced by tasks on different cores, is crucial.","While efficient techniques to over-approximate such bounds exist, little has been proposed to compute their exact values.","Given an RTS with a set of cores C and a set of tasks T , under partitioned fixed- priority scheduling with limited preemption, a recent work by Foughali, Hladik and Zuepke (FHZ) models tasks with affinity c (i.e., allocated to core c in C) as a Uppaal timed automata (TA) network Nc.","For each core c in C, Nc integrates blocking (due to data sharing) using tight analytical formulae.","Through compositional model checking, FHZ achieved a substantial gain in scalability for bounds local to a core.","However, computing inter-core bounds for some events of interest E, produced by a subset of tasks TE with different affinities CE, requires model checking the parallel composition of all TA networks Nc for each c in CE, which produces a large, often intractable, state space.","In this paper, we present a new scalable approach based on exact abstractions to compute exact inter-core bounds in a schedulable RTS, under the assumption that tasks in TE have distinct affinities.","We develop a novel algorithm, leveraging a new query that we implement in Uppaal, that computes for each TA network Nc in NE an abstraction A(Nc) preserving the exact intervals within which events occur on c, therefore drastically reducing the state space.","The scalability of our approach is demonstrated on the WATERS 2017 industrial challenge, for which we efficiently compute various types of inter-core bounds where FHZ fails to scale."],"url":"http://arxiv.org/abs/2405.06387v1","category":"cs.FL"}
{"created":"2024-05-10 10:48:29","title":"Deconfinement and chiral restoration phase transition under rotation from holography in an anisotropic gravitational background","abstract":"We investigate the effects of rotation on deconfinement and chiral phase transitions in the framework of dynamical holographic QCD model. Instead of transforming to the rotating system by Lorentz boost, we construct an anisotropic gravitational background by incorporating the rotating boundary current. We firstly investigate the pure gluon system under rotation to extract deconfinement phase transition from the Polyakov loop then add 2-flavor probe for chiral restoration phase transition from the chiral condensate. It is observed that at low chemical potentials, the deconfinement phase transition of pure gluon system is of first order and the chiral phase transition of 2-flavor system is of crossover. Both the critical temperatures of deconfinement and chiral phase transitions decrease/increase with imaginary/real angular velocity ($\\Omega_I/\\Omega$) as $T/T_c\\sim 1- C_2 \\Omega_I^2$ and $T/T_c\\sim 1+ C_2 \\Omega^2$, which is consistent with lattice QCD results. In the temperature-chemical potential $T-\\mu$ phase diagram, the critical end point (CEP) moves towards regions of higher temperature and chemical potential with real angular velocity.","sentences":["We investigate the effects of rotation on deconfinement and chiral phase transitions in the framework of dynamical holographic QCD model.","Instead of transforming to the rotating system by Lorentz boost, we construct an anisotropic gravitational background by incorporating the rotating boundary current.","We firstly investigate the pure gluon system under rotation to extract deconfinement phase transition from the Polyakov loop then add 2-flavor probe for chiral restoration phase transition from the chiral condensate.","It is observed that at low chemical potentials, the deconfinement phase transition of pure gluon system is of first order and the chiral phase transition of 2-flavor system is of crossover.","Both the critical temperatures of deconfinement and chiral phase transitions decrease/increase with imaginary/real angular velocity ($\\Omega_I/\\Omega$) as $T/T_c\\sim","1- C_2 \\Omega_I^2$ and $T/T_c\\sim 1+ C_2 \\Omega^2$, which is consistent with lattice QCD results.","In the temperature-chemical potential $T-\\mu$ phase diagram, the critical end point (CEP) moves towards regions of higher temperature and chemical potential with real angular velocity."],"url":"http://arxiv.org/abs/2405.06386v1","category":"hep-ph"}
{"created":"2024-05-10 10:46:27","title":"Higher complex Sobolev spaces on complex manifolds","abstract":"We study higher complex Sobolev spaces and their corresponding functional capacities. In particular, we prove the Moser-Trudinger inequality for these spaces and discuss some relationships between these spaces and the complex Monge-Amp\\`{e}re equation.","sentences":["We study higher complex Sobolev spaces and their corresponding functional capacities.","In particular, we prove the Moser-Trudinger inequality for these spaces and discuss some relationships between these spaces and the complex Monge-Amp\\`{e}re equation."],"url":"http://arxiv.org/abs/2405.06385v1","category":"math.CV"}
{"created":"2024-05-10 10:46:19","title":"Statistical physics of complex systems: glasses, spin glasses, continuous constraint satisfaction problems, high-dimensional inference and neural networks","abstract":"The purpose of this manuscript is to review my recent activity on three main research topics. The first concerns the nature of low temperature amorphous solids and their relation with the spin glass transition in a magnetic field. This is the subject of the first chapter where I discuss a new model, the KHGPS model, which allows to make some progress. In the second chapter I review a second research line that concerns the study of the rigidity/jamming transitions in particle system models and their relation to constraint satisfaction and optimization problems in high dimension. Finally in the last chapter I review my activity on the problem of the dynamics of learning algorithms in high-dimensional inference and supervised learning problems.","sentences":["The purpose of this manuscript is to review my recent activity on three main research topics.","The first concerns the nature of low temperature amorphous solids and their relation with the spin glass transition in a magnetic field.","This is the subject of the first chapter where I discuss a new model, the KHGPS model, which allows to make some progress.","In the second chapter I review a second research line that concerns the study of the rigidity/jamming transitions in particle system models and their relation to constraint satisfaction and optimization problems in high dimension.","Finally in the last chapter I review my activity on the problem of the dynamics of learning algorithms in high-dimensional inference and supervised learning problems."],"url":"http://arxiv.org/abs/2405.06384v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-10 10:34:09","title":"(H,Li)$_{6}$Ru$_{2}$O$_{6}$ : a zero-field Ru$^{3+}$-based Kitaev Quantum Spin Liquid","abstract":"We report the synthesis and properties of (H,Li)$_{6}$Ru$_{2}$O$_{6}$, which is shown to be a $J_{\\text{eff}}=\\frac{1}{2}$ system made out of Ru$^{3+}$ moments in a honeycomb geometry. Bulk magnetization, heat capacity, nuclear magnetic resonance (NMR), and muon spin relaxation ($\\mu$SR) rule out the presence of static moments or any spin glass phase down to 500 mK. All techniques suggest a crossover to a liquid-like state below about 40 K. The $^{7}$Li nuclear magnetic resonance (NMR) shift data suggest a non-zero $T$-independent spin susceptibility at low $T$. In zero field, $C_m/T$ shows $T^{-1}$ divergence which is consistent with vacancy-induced effects on low-energy excitations of the pristine Kitaev spin liquid. With field, power-law variations in the $^{7}$Li NMR spin-lattice relaxation rate 1/T$_{1}$ and magnetic heat capacity $C_{m}$ show quantitatively new scaling behaviors. A two-step entropy release in heat capacity is also observed putatively from $Z_{2}$ flux (low-$T$ step) and itinerant Majorana fermions (high-$T$ step). Based on these findings, we propose that (H,Li)$_{6}$Ru$_{2}$O$_{6}$ realizes a Kitaev spin liquid with no evidence of inherent magnetic ordering in zero field unlike $\\alpha$-RuCl$_{3}$ where approximately $8$ Tesla field is required to suppress magnetic order.","sentences":["We report the synthesis and properties of (H,Li)$_{6}$Ru$_{2}$O$_{6}$, which is shown to be a $J_{\\text{eff}}=\\frac{1}{2}$ system made out of Ru$^{3+}$ moments in a honeycomb geometry.","Bulk magnetization, heat capacity, nuclear magnetic resonance (NMR), and muon spin relaxation ($\\mu$SR) rule out the presence of static moments or any spin glass phase down to 500 mK. All techniques suggest a crossover to a liquid-like state below about 40 K. The $^{7}$Li nuclear magnetic resonance (NMR) shift data suggest a non-zero $T$-independent spin susceptibility at low","$T$. In zero field, $C_m/T$ shows $T^{-1}$ divergence which is consistent with vacancy-induced effects on low-energy excitations of the pristine Kitaev spin liquid.","With field, power-law variations in the $^{7}$Li NMR spin-lattice relaxation rate 1/T$_{1}$ and magnetic heat capacity $C_{m}$ show quantitatively new scaling behaviors.","A two-step entropy release in heat capacity is also observed putatively from $Z_{2}$ flux (low-$T$ step) and itinerant Majorana fermions (high-$T$ step).","Based on these findings, we propose that (H,Li)$_{6}$Ru$_{2}$O$_{6}$ realizes a Kitaev spin liquid with no evidence of inherent magnetic ordering in zero field unlike $\\alpha$-RuCl$_{3}$ where approximately $8$ Tesla field is required to suppress magnetic order."],"url":"http://arxiv.org/abs/2405.06380v1","category":"cond-mat.str-el"}
{"created":"2024-05-10 10:01:10","title":"Control of the von Neumann Entropy for an Open Two-Qubit System Using Coherent and Incoherent Drives","abstract":"This article is devoted to developing an approach for manipulating the von Neumann entropy $S(\\rho(t))$ of an open two-qubit system with coherent control and incoherent control inducing time-dependent decoherence rates. The following goals are considered: (a) minimizing or maximizing the final entropy $S(\\rho(T))$; (b) steering $S(\\rho(T))$ to a given target value; (c) steering $S(\\rho(T))$ to a target value and satisfying the pointwise state constraint $S(\\rho(t)) \\leq \\overline{S}$ for a given $\\overline{S}$; (d) keeping $S(\\rho(t))$ constant at a given time interval. Under the Markovian dynamics determined by a Gorini--Kossakowski--Sudarshan--Lindblad type master equation, which contains coherent and incoherent controls, one- and two-step gradient projection methods and genetic algorithm have been adapted, taking into account the specifics of the objective functionals. The corresponding numerical results are provided and discussed.","sentences":["This article is devoted to developing an approach for manipulating the von Neumann entropy $S(\\rho(t))$ of an open two-qubit system with coherent control and incoherent control inducing time-dependent decoherence rates.","The following goals are considered: (a) minimizing or maximizing the final entropy $S(\\rho(T))$; (b) steering $S(\\rho(T))$ to a given target value; (c) steering $S(\\rho(T))$ to a target value and satisfying the pointwise state constraint $S(\\rho(t))","\\leq \\overline{S}$ for a given $\\overline{S}$; (d) keeping $S(\\rho(t))$ constant at a given time interval.","Under the Markovian dynamics determined by a Gorini--Kossakowski--Sudarshan--Lindblad type master equation, which contains coherent and incoherent controls, one- and two-step gradient projection methods and genetic algorithm have been adapted, taking into account the specifics of the objective functionals.","The corresponding numerical results are provided and discussed."],"url":"http://arxiv.org/abs/2405.06365v1","category":"quant-ph"}
{"created":"2024-05-10 10:00:43","title":"Electromagnetic Property Sensing in ISAC with Multiple Base Stations: Algorithm, Pilot Design,and Performance Analysis","abstract":"Integrated sensing and communication (ISAC) has opened up numerous game-changing opportunities for future wireless systems. In this paper, we develop a novel scheme that utilizes orthogonal frequency division multiplexing (OFDM) pilot signals to sense the electromagnetic (EM) property of the target and thus identify the materials of the target. Specifically, we first establish an EM wave propagation model with Maxwell equations, where the EM property of the target is captured by a closed-form expression of the channel. We then build the mathematical model for the relative permittivity and conductivity distribution (RPCD) within a predetermined region of interest shared by multiple base stations (BSs). Based on the EM wave propagation model, we propose an EM property sensing method, in which the RPCD can be reconstructed from compressive sensing techniques that exploits the joint sparsity structure of the EM property vector. We then develop a fusion algorithm to combine data from multiple BSs, which can enhance the reconstruction accuracy of EM property by efficiently integrating diverse measurements. Moreover, the fusion is performed at the feature level of RPCD and features low transmission overhead. We further design the pilot signals that can minimize the mutual coherence of the equivalent channels and enhance the diversity of incident EM wave patterns. Simulation results demonstrate the efficacy of the proposed method in achieving high-quality RPCD reconstruction and accurate material classification.","sentences":["Integrated sensing and communication (ISAC) has opened up numerous game-changing opportunities for future wireless systems.","In this paper, we develop a novel scheme that utilizes orthogonal frequency division multiplexing (OFDM) pilot signals to sense the electromagnetic (EM) property of the target and thus identify the materials of the target.","Specifically, we first establish an EM wave propagation model with Maxwell equations, where the EM property of the target is captured by a closed-form expression of the channel.","We then build the mathematical model for the relative permittivity and conductivity distribution (RPCD) within a predetermined region of interest shared by multiple base stations (BSs).","Based on the EM wave propagation model, we propose an EM property sensing method, in which the RPCD can be reconstructed from compressive sensing techniques that exploits the joint sparsity structure of the EM property vector.","We then develop a fusion algorithm to combine data from multiple BSs, which can enhance the reconstruction accuracy of EM property by efficiently integrating diverse measurements.","Moreover, the fusion is performed at the feature level of RPCD and features low transmission overhead.","We further design the pilot signals that can minimize the mutual coherence of the equivalent channels and enhance the diversity of incident EM wave patterns.","Simulation results demonstrate the efficacy of the proposed method in achieving high-quality RPCD reconstruction and accurate material classification."],"url":"http://arxiv.org/abs/2405.06364v1","category":"eess.SP"}
{"created":"2024-05-10 09:50:06","title":"Quantum Krylov-Subspace Method Based Linear Solver","abstract":"Despite the successful enhancement to the Harrow-Hassidim-Lloyd algorithm by Childs et al., who introduced the Fourier approach leveraging linear combinations of unitary operators, our research has identified non-trivial redundancies within this method. This finding points to a considerable potential for refinement. In this paper, we propose the quantum Krylov-subspace method (QKSM), which is a hybrid classical-quantum algorithm, to mitigate such redundancies. By integrating QKSM as a subroutine, we introduce the quantum Krylov-subspace method based linear solver that not only reduces computational redundancy but also enhances efficiency and accuracy. Extensive numerical experiments, conducted on systems with dimensions up to $2^{10} \\times 2^{10}$, have demonstrated a significant reduction in computational resources and have led to more precise approximations.","sentences":["Despite the successful enhancement to the Harrow-Hassidim-Lloyd algorithm by Childs et al., who introduced the Fourier approach leveraging linear combinations of unitary operators, our research has identified non-trivial redundancies within this method.","This finding points to a considerable potential for refinement.","In this paper, we propose the quantum Krylov-subspace method (QKSM), which is a hybrid classical-quantum algorithm, to mitigate such redundancies.","By integrating QKSM as a subroutine, we introduce the quantum Krylov-subspace method based linear solver that not only reduces computational redundancy but also enhances efficiency and accuracy.","Extensive numerical experiments, conducted on systems with dimensions up to $2^{10} \\times 2^{10}$, have demonstrated a significant reduction in computational resources and have led to more precise approximations."],"url":"http://arxiv.org/abs/2405.06359v1","category":"quant-ph"}
{"created":"2024-05-10 09:44:23","title":"Beyond Bell sampling: stabilizer state learning and quantum pseudorandomness lower bounds on qudits","abstract":"Bell sampling is a simple yet powerful measurement primitive that has recently attracted a lot of attention, and has proven to be a valuable tool in studying stabiliser states. Unfortunately, however, it is known that Bell sampling fails when used on qu\\emph{d}its of dimension $d>2$. In this paper, we explore and quantify the limitations of Bell sampling on qudits, and propose new quantum algorithms to circumvent the use of Bell sampling in solving two important problems: learning stabiliser states and providing pseudorandomness lower bounds on qudits. More specifically, as our first result, we characterise the output distribution corresponding to Bell sampling on copies of a stabiliser state and show that the output can be uniformly random, and hence reveal no information. As our second result, for $d=p$ prime we devise a quantum algorithm to identify an unknown stabiliser state in $(\\mathbb{C}^p)^{\\otimes n}$ that uses $O(n)$ copies of the input state and runs in time $O(n^4)$. As our third result, we provide a quantum algorithm that efficiently distinguishes a Haar-random state from a state with non-negligible stabiliser fidelity. As a corollary, any Clifford circuit on qudits of dimension $d$ using $O(\\log{n}/\\log{d})$ auxiliary non-Clifford single-qudit gates cannot prepare computationally pseudorandom quantum states.","sentences":["Bell sampling is a simple yet powerful measurement primitive that has recently attracted a lot of attention, and has proven to be a valuable tool in studying stabiliser states.","Unfortunately, however, it is known that Bell sampling fails when used on qu\\emph{d}its of dimension $d>2$. In this paper, we explore and quantify the limitations of Bell sampling on qudits, and propose new quantum algorithms to circumvent the use of Bell sampling in solving two important problems: learning stabiliser states and providing pseudorandomness lower bounds on qudits.","More specifically, as our first result, we characterise the output distribution corresponding to Bell sampling on copies of a stabiliser state and show that the output can be uniformly random, and hence reveal no information.","As our second result, for $d=p$ prime we devise a quantum algorithm to identify an unknown stabiliser state in $(\\mathbb{C}^p)^{\\otimes n}$ that uses $O(n)$ copies of the input state and runs in time $O(n^4)$. As our third result, we provide a quantum algorithm that efficiently distinguishes a Haar-random state from a state with non-negligible stabiliser fidelity.","As a corollary, any Clifford circuit on qudits of dimension $d$ using $O(\\log{n}/\\log{d})$ auxiliary non-Clifford single-qudit gates cannot prepare computationally pseudorandom quantum states."],"url":"http://arxiv.org/abs/2405.06357v1","category":"quant-ph"}
{"created":"2024-05-10 09:20:47","title":"Evaluating Adversarial Robustness in the Spatial Frequency Domain","abstract":"Convolutional Neural Networks (CNNs) have dominated the majority of computer vision tasks. However, CNNs' vulnerability to adversarial attacks has raised concerns about deploying these models to safety-critical applications. In contrast, the Human Visual System (HVS), which utilizes spatial frequency channels to process visual signals, is immune to adversarial attacks. As such, this paper presents an empirical study exploring the vulnerability of CNN models in the frequency domain. Specifically, we utilize the discrete cosine transform (DCT) to construct the Spatial-Frequency (SF) layer to produce a block-wise frequency spectrum of an input image and formulate Spatial Frequency CNNs (SF-CNNs) by replacing the initial feature extraction layers of widely-used CNN backbones with the SF layer. Through extensive experiments, we observe that SF-CNN models are more robust than their CNN counterparts under both white-box and black-box attacks. To further explain the robustness of SF-CNNs, we compare the SF layer with a trainable convolutional layer with identical kernel sizes using two mixing strategies to show that the lower frequency components contribute the most to the adversarial robustness of SF-CNNs. We believe our observations can guide the future design of robust CNN models.","sentences":["Convolutional Neural Networks (CNNs) have dominated the majority of computer vision tasks.","However, CNNs' vulnerability to adversarial attacks has raised concerns about deploying these models to safety-critical applications.","In contrast, the Human Visual System (HVS), which utilizes spatial frequency channels to process visual signals, is immune to adversarial attacks.","As such, this paper presents an empirical study exploring the vulnerability of CNN models in the frequency domain.","Specifically, we utilize the discrete cosine transform (DCT) to construct the Spatial-Frequency (SF) layer to produce a block-wise frequency spectrum of an input image and formulate Spatial Frequency CNNs (SF-CNNs) by replacing the initial feature extraction layers of widely-used CNN backbones with the SF layer.","Through extensive experiments, we observe that SF-CNN models are more robust than their CNN counterparts under both white-box and black-box attacks.","To further explain the robustness of SF-CNNs, we compare the SF layer with a trainable convolutional layer with identical kernel sizes using two mixing strategies to show that the lower frequency components contribute the most to the adversarial robustness of SF-CNNs.","We believe our observations can guide the future design of robust CNN models."],"url":"http://arxiv.org/abs/2405.06345v1","category":"cs.CV"}
{"created":"2024-05-10 09:04:08","title":"Solving maximally comonotone inclusion problems via an implicit Newton-like inertial dynamical system and its discretization","abstract":"This paper deals with an implicit Newton-like inertial dynamical system governed by a maximally comonotone inclusion problem in a Hilbert space. Under suitable conditions, we establish not only pointwise estimates and integral estimates for the velocity and the value of the associated Yosida regularization operator along the trajectory of the system, but also the weak convergence of the trajectory to a zero of the maximally comonotone operator. Moreover, a new inertial algorithm is developed via a time discretization of the proposed system. Our analysis reveals that the resulting discrete algorithm exhibits fast convergence properties matching the ones of the continuous time counterpart. Finally, the theoretical results are illustrated by numerical experiments.","sentences":["This paper deals with an implicit Newton-like inertial dynamical system governed by a maximally comonotone inclusion problem in a Hilbert space.","Under suitable conditions, we establish not only pointwise estimates and integral estimates for the velocity and the value of the associated Yosida regularization operator along the trajectory of the system, but also the weak convergence of the trajectory to a zero of the maximally comonotone operator.","Moreover, a new inertial algorithm is developed via a time discretization of the proposed system.","Our analysis reveals that the resulting discrete algorithm exhibits fast convergence properties matching the ones of the continuous time counterpart.","Finally, the theoretical results are illustrated by numerical experiments."],"url":"http://arxiv.org/abs/2405.06332v1","category":"math.OC"}
{"created":"2024-05-10 08:45:23","title":"Decoding Emotions in Abstract Art: Cognitive Plausibility of CLIP in Recognizing Color-Emotion Associations","abstract":"This study investigates the cognitive plausibility of a pretrained multimodal model, CLIP, in recognizing emotions evoked by abstract visual art. We employ a dataset comprising images with associated emotion labels and textual rationales of these labels provided by human annotators. We perform linguistic analyses of rationales, zero-shot emotion classification of images and rationales, apply similarity-based prediction of emotion, and investigate color-emotion associations. The relatively low, yet above baseline, accuracy in recognizing emotion for abstract images and rationales suggests that CLIP decodes emotional complexities in a manner not well aligned with human cognitive processes. Furthermore, we explore color-emotion interactions in images and rationales. Expected color-emotion associations, such as red relating to anger, are identified in images and texts annotated with emotion labels by both humans and CLIP, with the latter showing even stronger interactions. Our results highlight the disparity between human processing and machine processing when connecting image features and emotions.","sentences":["This study investigates the cognitive plausibility of a pretrained multimodal model, CLIP, in recognizing emotions evoked by abstract visual art.","We employ a dataset comprising images with associated emotion labels and textual rationales of these labels provided by human annotators.","We perform linguistic analyses of rationales, zero-shot emotion classification of images and rationales, apply similarity-based prediction of emotion, and investigate color-emotion associations.","The relatively low, yet above baseline, accuracy in recognizing emotion for abstract images and rationales suggests that CLIP decodes emotional complexities in a manner not well aligned with human cognitive processes.","Furthermore, we explore color-emotion interactions in images and rationales.","Expected color-emotion associations, such as red relating to anger, are identified in images and texts annotated with emotion labels by both humans and CLIP, with the latter showing even stronger interactions.","Our results highlight the disparity between human processing and machine processing when connecting image features and emotions."],"url":"http://arxiv.org/abs/2405.06319v1","category":"cs.CV"}
{"created":"2024-05-10 08:38:19","title":"Difference \"abc\" theorem for entire functions and Difference analogue of truncated version of Nevanlinna second main theorem","abstract":"In this paper, we focus on the difference analogue of Stothers-Mason theorem for entire functions of order less than 1, which can be seen as difference $abc$ theorem for entire functions with slow growth. We also obtain the difference analogue of truncated version of Nevanlinna second main theorem which reveals that a non-periodic meromorphic function with period one of finite order cannot have too many points with long height in the complex plane. Both theorems depend on new definitions of height of shifting poles and shifting zeros of a given meromorphic function in a domain.","sentences":["In this paper, we focus on the difference analogue of Stothers-Mason theorem for entire functions of order less than 1, which can be seen as difference $abc$ theorem for entire functions with slow growth.","We also obtain the difference analogue of truncated version of Nevanlinna second main theorem which reveals that a non-periodic meromorphic function with period one of finite order cannot have too many points with long height in the complex plane.","Both theorems depend on new definitions of height of shifting poles and shifting zeros of a given meromorphic function in a domain."],"url":"http://arxiv.org/abs/2405.06317v1","category":"math.CV"}
{"created":"2024-05-10 08:37:44","title":"Refined localization spaces, Kondratiev spaces with fractional smoothness and extension operators","abstract":"In this paper, we introduce Kondratiev spaces of fractional smoothness based on their close relation to refined localization spaces. Moreover, we investigate relations to other approaches leading to extensions of the scale of Kondratiev spaces with integer order of smoothness, based on complex interpolation, and give further results for complex interpolation of those function spaces. As it turns out to be one of the main tools in studying these spaces on domains of polyhedral type, certain aspects of the analysis of Stein's extension operator are revisited. Finally, as an application, we study Sobolev-type embeddings.","sentences":["In this paper, we introduce Kondratiev spaces of fractional smoothness based on their close relation to refined localization spaces.","Moreover, we investigate relations to other approaches leading to extensions of the scale of Kondratiev spaces with integer order of smoothness, based on complex interpolation, and give further results for complex interpolation of those function spaces.","As it turns out to be one of the main tools in studying these spaces on domains of polyhedral type, certain aspects of the analysis of Stein's extension operator are revisited.","Finally, as an application, we study Sobolev-type embeddings."],"url":"http://arxiv.org/abs/2405.06316v1","category":"math.NA"}
{"created":"2024-05-10 08:37:20","title":"A note on the $8\u03c0$ problem of J\u00e4ger-Luckhaus system","abstract":"It is proven that for any radially symmetric, nonnegative and continuous initial data with critical mass $8\\pi$, J\\\"ager-Luckhuas system, posed on the unit disk, admits a global classical solution, which is uniform-in-time bounded.","sentences":["It is proven that for any radially symmetric, nonnegative and continuous initial data with critical mass $8\\pi$, J\\\"ager-Luckhuas system, posed on the unit disk, admits a global classical solution, which is uniform-in-time bounded."],"url":"http://arxiv.org/abs/2405.06315v1","category":"math.AP"}
{"created":"2024-05-10 08:34:18","title":"The Discovery of Neptune Revisited","abstract":"The study of the differences detected between the observed and the predicted positions of Uranus taking only the ancient planets into account led to the discovery of planet Neptune in 1846. This event remains one of the best accomplishments ever achieved in the history of Astronomy and Classical Mechanics. In this paper, we study the perturbations in the orbit of Uranus due to Neptune and its effects from a modern numerical point of view of the $N$-body problem. The effects induced by Pluto in the orbit of Neptune, as the historical search for a ninth planet in the Solar System (recently boostered again with the hypothesis of the so-called Planet Nine) back in the days was propelled by some supposed small inconsistencies in the orbit of the ice giants, are also analyzed.","sentences":["The study of the differences detected between the observed and the predicted positions of Uranus taking only the ancient planets into account led to the discovery of planet Neptune in 1846.","This event remains one of the best accomplishments ever achieved in the history of Astronomy and Classical Mechanics.","In this paper, we study the perturbations in the orbit of Uranus due to Neptune and its effects from a modern numerical point of view of the $N$-body problem.","The effects induced by Pluto in the orbit of Neptune, as the historical search for a ninth planet in the Solar System (recently boostered again with the hypothesis of the so-called Planet Nine) back in the days was propelled by some supposed small inconsistencies in the orbit of the ice giants, are also analyzed."],"url":"http://arxiv.org/abs/2405.06310v1","category":"astro-ph.EP"}
{"created":"2024-05-10 17:35:15","title":"On Streaming Codes for Simultaneously Correcting Burst and Random Erasures","abstract":"Streaming codes are packet-level codes that recover dropped packets within a strict decoding-delay constraint. We study streaming codes over a sliding-window (SW) channel model which admits only those erasure patterns which allow either a single burst erasure of $\\le b$ packets along with $\\le e$ random packet erasures, or else, $\\le a$ random packet erasures, in any sliding-window of $w$ time slots. We determine the optimal rate of a streaming code constructed via the popular diagonal embedding (DE) technique over such a SW channel under delay constraint $\\tau=(w-1)$ and provide an $O(w)$ field size code construction. For the case $e>1$, we show that it is not possible to significantly reduce this field size requirement, assuming the well-known MDS conjecture. We then provide a block code construction whose DE yields a streaming code achieving the rate derived above, over a field of size sub-linear in $w,$ for a family of parameters having $e=1.$ We show the field size optimality of this construction for some parameters, and near-optimality for others under a sparsity constraint. Additionally, we derive an upper-bound on the $d_{\\text{min}}$ of a cyclic code and characterize cyclic codes which achieve this bound via their ability to simultaneously recover from burst and random erasures.","sentences":["Streaming codes are packet-level codes that recover dropped packets within a strict decoding-delay constraint.","We study streaming codes over a sliding-window (SW) channel model which admits only those erasure patterns which allow either a single burst erasure of $\\le b$ packets along with $\\le e$ random packet erasures, or else, $\\le a$ random packet erasures, in any sliding-window of $w$ time slots.","We determine the optimal rate of a streaming code constructed via the popular diagonal embedding (DE) technique over such a SW channel under delay constraint $\\tau=(w-1)$ and provide an $O(w)$ field size code construction.","For the case $e>1$, we show that it is not possible to significantly reduce this field size requirement, assuming the well-known MDS conjecture.","We then provide a block code construction whose DE yields a streaming code achieving the rate derived above, over a field of size sub-linear in $w,$ for a family of parameters having $e=1.$ We show the field size optimality of this construction for some parameters, and near-optimality for others under a sparsity constraint.","Additionally, we derive an upper-bound on the $d_{\\text{min}}$ of a cyclic code and characterize cyclic codes which achieve this bound via their ability to simultaneously recover from burst and random erasures."],"url":"http://arxiv.org/abs/2405.06621v1","category":"cs.IT"}
{"created":"2024-05-10 17:31:20","title":"Steps Toward Quantum Simulations of Hadronization and Energy-Loss in Dense Matter","abstract":"A framework for simulating the real-time dynamics of particles in dense matter using quantum computers is developed. As a demonstration, we perform classical simulations of heavy-hadrons propagating through a dense medium in the Schwinger model. Measurements of the time-dependent energy and charge density are used to identify mechanisms responsible for energy loss and hadron production (hadronization). A study of entanglement dynamics highlights the importance of quantum coherence between the particles that make up the dense medium. Throughout this work, care is taken to isolate, and remove, phenomena that arise solely from a finite lattice spacing. It is found that signatures of entanglement are more sensitive to lattice artifacts than other observables. Toward quantum simulations, we present an efficient method and the corresponding quantum circuits for preparing ground states in the presence of heavy mesons. These circuits are used to estimate the resources required to simulate in-medium energy loss and hadronization in the Schwinger model using quantum computers.","sentences":["A framework for simulating the real-time dynamics of particles in dense matter using quantum computers is developed.","As a demonstration, we perform classical simulations of heavy-hadrons propagating through a dense medium in the Schwinger model.","Measurements of the time-dependent energy and charge density are used to identify mechanisms responsible for energy loss and hadron production (hadronization).","A study of entanglement dynamics highlights the importance of quantum coherence between the particles that make up the dense medium.","Throughout this work, care is taken to isolate, and remove, phenomena that arise solely from a finite lattice spacing.","It is found that signatures of entanglement are more sensitive to lattice artifacts than other observables.","Toward quantum simulations, we present an efficient method and the corresponding quantum circuits for preparing ground states in the presence of heavy mesons.","These circuits are used to estimate the resources required to simulate in-medium energy loss and hadronization in the Schwinger model using quantum computers."],"url":"http://arxiv.org/abs/2405.06620v1","category":"quant-ph"}
{"created":"2024-05-10 17:28:34","title":"Optimal Uniform Circle Formation by Asynchronous Luminous Robots","abstract":"We study the {\\sc Uniform Circle Formation} ({\\sc UCF}) problem for a swarm of $n$ autonomous mobile robots operating in \\emph{Look-Compute-Move} (LCM) cycles on the Euclidean plane. We assume our robots are \\emph{luminous}, i.e. embedded with a persistent light that can assume a color chosen from a fixed palette, and \\emph{opaque}, i.e. not able to see beyond a collinear robot. Robots are said to \\emph{collide} if they share positions or their paths intersect within concurrent LCM cycles. To solve {\\sc UCF}, a swarm of $n$ robots must autonomously arrange themselves so that each robot occupies a vertex of the same regular $n$-gon not fixed in advance. In terms of efficiency, the goal is to design an algorithm that optimizes (or provides a tradeoff between) two fundamental performance metrics: \\emph{(i)} the execution time and \\emph{(ii)} the size of the color palette. There exists an $O(1)$-time $O(1)$-color algorithm for this problem under the fully synchronous and semi-synchronous schedulers and a $O(\\log\\log n)$-time $O(1)$-color or $O(1)$-time $O(\\sqrt{n})$-color algorithm under the asynchronous scheduler, avoiding collisions. In this paper, we develop a deterministic algorithm solving {\\sc UCF} avoiding collisions in $O(1)$-time with $O(1)$ colors under the asynchronous scheduler, which is asymptotically optimal with respect to both time and number of colors used, the first such result. Furthermore, the algorithm proposed here minimizes for the first time what we call the \\emph{computational SEC}, i.e. the smallest circular area where robots operate throughout the whole algorithm.","sentences":["We study the {\\sc Uniform Circle Formation} ({\\sc UCF}) problem for a swarm of $n$ autonomous mobile robots operating in \\emph{Look-Compute-Move} (LCM) cycles on the Euclidean plane.","We assume our robots are \\emph{luminous}, i.e. embedded with a persistent light that can assume a color chosen from a fixed palette, and \\emph{opaque}, i.e. not able to see beyond a collinear robot.","Robots are said to \\emph{collide} if they share positions or their paths intersect within concurrent LCM cycles.","To solve {\\sc UCF}, a swarm of $n$ robots must autonomously arrange themselves so that each robot occupies a vertex of the same regular $n$-gon not fixed in advance.","In terms of efficiency, the goal is to design an algorithm that optimizes (or provides a tradeoff between) two fundamental performance metrics: \\emph{(i)} the execution time and \\emph{(ii)} the size of the color palette.","There exists an $O(1)$-time $O(1)$-color algorithm for this problem under the fully synchronous and semi-synchronous schedulers and a $O(\\log\\log n)$-time $O(1)$-color or $O(1)$-time $O(\\sqrt{n})$-color algorithm under the asynchronous scheduler, avoiding collisions.","In this paper, we develop a deterministic algorithm solving {\\sc UCF} avoiding collisions in $O(1)$-time with $O(1)$ colors under the asynchronous scheduler, which is asymptotically optimal with respect to both time and number of colors used, the first such result.","Furthermore, the algorithm proposed here minimizes for the first time what we call the \\emph{computational SEC}, i.e. the smallest circular area where robots operate throughout the whole algorithm."],"url":"http://arxiv.org/abs/2405.06617v1","category":"cs.DC"}
{"created":"2024-05-10 17:13:01","title":"On Streaming Codes for Burst and Random Errors","abstract":"Streaming codes (SCs) are packet-level codes that recover erased packets within a strict decoding-delay deadline. Streaming codes for various packet erasure channel models such as sliding-window (SW) channel models that admit random or burst erasures in any SW of a fixed length have been studied in the literature, and the optimal rate as well as rate-optimal code constructions of SCs over such channel models are known. In this paper, we study error-correcting streaming codes ($\\text{SC}_{\\text{ERR}}$s), i.e., packet-level codes which recover erroneous packets within a delay constraint. We study $\\text{SC}_{\\text{ERR}}$s for two classes of SW channel models, one that admits random packet errors, and another that admits multiple bursts of packet errors, in any SW of a fixed length. For the case of random packet errors, we establish the equivalence of an $\\text{SC}_{\\text{ERR}}$ and a corresponding SC that recovers from random packet erasures, thus determining the optimal rate of an $\\text{SC}_{\\text{ERR}}$ for this setting, and providing a rate-optimal code construction for all parameters. We then focus on SCs that recover from multiple erasure bursts and derive a rate-upper-bound for such SCs. We show the necessity of a divisibility constraint for the existence of an SC constructed by the popular diagonal embedding technique, that achieves this rate-bound under a stringent delay requirement. We then show that a construction known in the literature achieves this rate-bound when the divisibility constraint is met. We further show the equivalence of the SCs considered and $\\text{SC}_{\\text{ERR}}$s for the setting of multiple error bursts, under a stringent delay requirement.","sentences":["Streaming codes (SCs) are packet-level codes that recover erased packets within a strict decoding-delay deadline.","Streaming codes for various packet erasure channel models such as sliding-window (SW) channel models that admit random or burst erasures in any SW of a fixed length have been studied in the literature, and the optimal rate as well as rate-optimal code constructions of SCs over such channel models are known.","In this paper, we study error-correcting streaming codes ($\\text{SC}_{\\text{ERR}}$s), i.e., packet-level codes which recover erroneous packets within a delay constraint.","We study $\\text{SC}_{\\text{ERR}}$s for two classes of SW channel models, one that admits random packet errors, and another that admits multiple bursts of packet errors, in any SW of a fixed length.","For the case of random packet errors, we establish the equivalence of an $\\text{SC}_{\\text{ERR}}$ and a corresponding SC that recovers from random packet erasures, thus determining the optimal rate of an $\\text{SC}_{\\text{ERR}}$ for this setting, and providing a rate-optimal code construction for all parameters.","We then focus on SCs that recover from multiple erasure bursts and derive a rate-upper-bound for such SCs.","We show the necessity of a divisibility constraint for the existence of an SC constructed by the popular diagonal embedding technique, that achieves this rate-bound under a stringent delay requirement.","We then show that a construction known in the literature achieves this rate-bound when the divisibility constraint is met.","We further show the equivalence of the SCs considered and $\\text{SC}_{\\text{ERR}}$s for the setting of multiple error bursts, under a stringent delay requirement."],"url":"http://arxiv.org/abs/2405.06606v1","category":"cs.IT"}
{"created":"2024-05-10 16:46:32","title":"Decomposing weather forecasting into advection and convection with neural networks","abstract":"Operational weather forecasting models have advanced for decades on both the explicit numerical solvers and the empirical physical parameterization schemes. However, the involved high computational costs and uncertainties in these existing schemes are requiring potential improvements through alternative machine learning methods. Previous works use a unified model to learn the dynamics and physics of the atmospheric model. Contrarily, we propose a simple yet effective machine learning model that learns the horizontal movement in the dynamical core and vertical movement in the physical parameterization separately. By replacing the advection with a graph attention network and the convection with a multi-layer perceptron, our model provides a new and efficient perspective to simulate the transition of variables in atmospheric models. We also assess the model's performance over a 5-day iterative forecasting. Under the same input variables and training methods, our model outperforms existing data-driven methods with a significantly-reduced number of parameters with a resolution of 5.625 deg. Overall, this work aims to contribute to the ongoing efforts that leverage machine learning techniques for improving both the accuracy and efficiency of global weather forecasting.","sentences":["Operational weather forecasting models have advanced for decades on both the explicit numerical solvers and the empirical physical parameterization schemes.","However, the involved high computational costs and uncertainties in these existing schemes are requiring potential improvements through alternative machine learning methods.","Previous works use a unified model to learn the dynamics and physics of the atmospheric model.","Contrarily, we propose a simple yet effective machine learning model that learns the horizontal movement in the dynamical core and vertical movement in the physical parameterization separately.","By replacing the advection with a graph attention network and the convection with a multi-layer perceptron, our model provides a new and efficient perspective to simulate the transition of variables in atmospheric models.","We also assess the model's performance over a 5-day iterative forecasting.","Under the same input variables and training methods, our model outperforms existing data-driven methods with a significantly-reduced number of parameters with a resolution of 5.625 deg.","Overall, this work aims to contribute to the ongoing efforts that leverage machine learning techniques for improving both the accuracy and efficiency of global weather forecasting."],"url":"http://arxiv.org/abs/2405.06590v1","category":"physics.ao-ph"}
{"created":"2024-05-10 16:39:26","title":"Private Repair of a Single Erasure in Reed-Solomon Codes","abstract":"We investigate the problem of privately recovering a single erasure for Reed-Solomon codes with low communication bandwidths. For an $[n,k]_{q^\\ell}$ code with $n-k\\geq q^{m}+t-1$, we construct a repair scheme that allows a client to recover an arbitrary codeword symbol without leaking its index to any set of $t$ colluding helper nodes at a repair bandwidth of $(n-1)(\\ell-m)$ sub-symbols in $\\mathbb{F}_q$. When $t=1$, this reduces to the bandwidth of existing repair schemes based on subspace polynomials. We prove the optimality of the proposed scheme when $n=q^\\ell$ under a reasonable assumption about the schemes being used. Our private repair scheme can also be transformed into a private retrieval scheme for data encoded by Reed-Solomon codes.","sentences":["We investigate the problem of privately recovering a single erasure for Reed-Solomon codes with low communication bandwidths.","For an $[n,k]_{q^\\ell}$ code with $n-k\\geq q^{m}+t-1$, we construct a repair scheme that allows a client to recover an arbitrary codeword symbol without leaking its index to any set of $t$ colluding helper nodes at a repair bandwidth of $(n-1)(\\ell-m)$ sub-symbols in $\\mathbb{F}_q$. When $t=1$, this reduces to the bandwidth of existing repair schemes based on subspace polynomials.","We prove the optimality of the proposed scheme when $n=q^\\ell$ under a reasonable assumption about the schemes being used.","Our private repair scheme can also be transformed into a private retrieval scheme for data encoded by Reed-Solomon codes."],"url":"http://arxiv.org/abs/2405.06583v1","category":"cs.IT"}
{"created":"2024-05-10 16:34:34","title":"Terahertz Antenna Impedance Matched to a Graphene Photodetector","abstract":"Developing low-power, high-sensitivity photodetectors for the terahertz (THz) band that operate at room temperature is an important challenge in optoelectronics. In this study, we introduce a photo-thermal-electric (PTE) effect detector based on quasi-free standing bilayer graphene (BLG) on a silicon carbide (SiC) substrate, designed for the THz frequency range. Our detector's performance hinges on a quasi-optical coupling scheme, which integrates an aspherical silicon lens, to optimize impedance matching between the THz antenna and the graphene p-n junction. At room temperature, we achieved a noise equivalent power (NEP) of less than 300 $pW/\\sqrt{Hz}$. Through an impedance matching analysis, we coupled a planar antenna with a graphene p-n junction, inserted in parallel to the nano-gap of the antenna, via two coupling capacitors. By adjusting the capacitors and the antenna arm length, we tailored the antenna's maximum infrared power absorption to specific frequencies. The sensitivity, spectral properties, and scalability of our material make it an ideal candidate for future development of far-infrared detectors operating at room temperature.","sentences":["Developing low-power, high-sensitivity photodetectors for the terahertz (THz) band that operate at room temperature is an important challenge in optoelectronics.","In this study, we introduce a photo-thermal-electric (PTE) effect detector based on quasi-free standing bilayer graphene (BLG) on a silicon carbide (SiC) substrate, designed for the THz frequency range.","Our detector's performance hinges on a quasi-optical coupling scheme, which integrates an aspherical silicon lens, to optimize impedance matching between the THz antenna and the graphene p-n junction.","At room temperature, we achieved a noise equivalent power (NEP) of less than 300 $pW/\\sqrt{Hz}$. Through an impedance matching analysis, we coupled a planar antenna with a graphene p-n junction, inserted in parallel to the nano-gap of the antenna, via two coupling capacitors.","By adjusting the capacitors and the antenna arm length, we tailored the antenna's maximum infrared power absorption to specific frequencies.","The sensitivity, spectral properties, and scalability of our material make it an ideal candidate for future development of far-infrared detectors operating at room temperature."],"url":"http://arxiv.org/abs/2405.06579v1","category":"physics.ins-det"}
{"created":"2024-05-10 15:56:37","title":"Interlayer couplings in homobilayer structures of MSi2X4 (M = Mo/W, X = N/P/As)","abstract":"We investigated the interlayer coupling effect in homobilayer structures of MSi2X4 with M = Mo/W and X = N/P/As. Through the combination of first-principles calculations and analytical formulations, the equilibrium interlayer distance, layer energy difference and interlayer hopping strength are obtained for all six MSi2X4 materials, which are found to be insensitive to the type of M atom but differ significantly between X = N and X = P/As. In homobilayers with close to 0{\\deg} twist angles, the interlayer charge redistribution introduces a stacking-dependent interlayer electrostatic potential with a magnitude reaching 0.1 eV in MSi2N4, suggesting that it can be an excellent candidate for studying the sliding ferroelectricity. The interlayer hopping strengths are found to be as large as several tens meV at valence band maxima positions K and {\\Gamma}, and ~ 1 meV at the conduction band edge K. The resultant layer-hybridizations vary in a large range under different stacking registries, which can be used to simulate honeycomb lattice models with both trivial and non-trivial band topologies.","sentences":["We investigated the interlayer coupling effect in homobilayer structures of MSi2X4 with M = Mo/W and X = N/P/As.","Through the combination of first-principles calculations and analytical formulations, the equilibrium interlayer distance, layer energy difference and interlayer hopping strength are obtained for all six MSi2X4 materials, which are found to be insensitive to the type of M atom but differ significantly between X = N and X = P/As.","In homobilayers with close to 0{\\deg} twist angles, the interlayer charge redistribution introduces a stacking-dependent interlayer electrostatic potential with a magnitude reaching 0.1 eV in MSi2N4, suggesting that it can be an excellent candidate for studying the sliding ferroelectricity.","The interlayer hopping strengths are found to be as large as several tens meV at valence band maxima positions K and {\\Gamma}, and ~ 1 meV at the conduction band edge K. The resultant layer-hybridizations vary in a large range under different stacking registries, which can be used to simulate honeycomb lattice models with both trivial and non-trivial band topologies."],"url":"http://arxiv.org/abs/2405.06555v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 15:49:01","title":"ADSumm: Annotated Ground-truth Summary Datasets for Disaster Tweet Summarization","abstract":"Online social media platforms, such as Twitter, provide valuable information during disaster events. Existing tweet disaster summarization approaches provide a summary of these events to aid government agencies, humanitarian organizations, etc., to ensure effective disaster response. In the literature, there are two types of approaches for disaster summarization, namely, supervised and unsupervised approaches. Although supervised approaches are typically more effective, they necessitate a sizable number of disaster event summaries for testing and training. However, there is a lack of good number of disaster summary datasets for training and evaluation. This motivates us to add more datasets to make supervised learning approaches more efficient. In this paper, we present ADSumm, which adds annotated ground-truth summaries for eight disaster events which consist of both natural and man-made disaster events belonging to seven different countries. Our experimental analysis shows that the newly added datasets improve the performance of the supervised summarization approaches by 8-28% in terms of ROUGE-N F1-score. Moreover, in newly annotated dataset, we have added a category label for each input tweet which helps to ensure good coverage from different categories in summary. Additionally, we have added two other features relevance label and key-phrase, which provide information about the quality of a tweet and explanation about the inclusion of the tweet into summary, respectively. For ground-truth summary creation, we provide the annotation procedure adapted in detail, which has not been described in existing literature. Experimental analysis shows the quality of ground-truth summary is very good with Coverage, Relevance and Diversity.","sentences":["Online social media platforms, such as Twitter, provide valuable information during disaster events.","Existing tweet disaster summarization approaches provide a summary of these events to aid government agencies, humanitarian organizations, etc., to ensure effective disaster response.","In the literature, there are two types of approaches for disaster summarization, namely, supervised and unsupervised approaches.","Although supervised approaches are typically more effective, they necessitate a sizable number of disaster event summaries for testing and training.","However, there is a lack of good number of disaster summary datasets for training and evaluation.","This motivates us to add more datasets to make supervised learning approaches more efficient.","In this paper, we present ADSumm, which adds annotated ground-truth summaries for eight disaster events which consist of both natural and man-made disaster events belonging to seven different countries.","Our experimental analysis shows that the newly added datasets improve the performance of the supervised summarization approaches by 8-28% in terms of ROUGE-N F1-score.","Moreover, in newly annotated dataset, we have added a category label for each input tweet which helps to ensure good coverage from different categories in summary.","Additionally, we have added two other features relevance label and key-phrase, which provide information about the quality of a tweet and explanation about the inclusion of the tweet into summary, respectively.","For ground-truth summary creation, we provide the annotation procedure adapted in detail, which has not been described in existing literature.","Experimental analysis shows the quality of ground-truth summary is very good with Coverage, Relevance and Diversity."],"url":"http://arxiv.org/abs/2405.06551v1","category":"cs.CL"}
{"created":"2024-05-10 15:45:41","title":"Usefulness of Quantum Entanglement for Enhancing Precision in Frequency Estimation","abstract":"We investigate strategies for reaching the ultimate limit on the precision of frequency estimation when the number of probes used in each run of the experiment is fixed. That limit is set by the quantum Cram\\'er-Rao bound (QCRB), which predicts that the use of maximally entangled probes enhances the estimation precision, when compared with the use of independent probes. However, the bound is only achievable if the statistical model used in the estimation remains identifiable throughout the procedure. This in turn sets different limits on the maximal sensing time used in each run of the estimation procedure, when entangled and independent probes are used. When those constraints are taken into account, one can show that, when the total number of probes and the total duration of the estimation process are counted as fixed resources, the use of entangled probes is, in fact, disadvantageous when compared with the use of independent probes. In order to counteract the limitations imposed on the sensing time by the requirement of identifiability of the statistical model, we propose a time-adaptive strategy, in which the sensing time is adequately increased at each step of the estimation process, calculate an attainable error bound for the strategy and discuss how to optimally choose its parameters in order to minimize that bound. We show that the proposed strategy leads to much better scaling of the estimation uncertainty with the total number of probes and the total sensing time than the traditional fixed-sensing-time strategy. We also show that, when the total number of probes and the total sensing time are counted as resources, independent probes and maximally entangled ones have now the same performance, in contrast to the non-adaptive strategy, where the use of independent is more advantageous than the use of maximally entangled ones.","sentences":["We investigate strategies for reaching the ultimate limit on the precision of frequency estimation when the number of probes used in each run of the experiment is fixed.","That limit is set by the quantum Cram\\'er-Rao bound (QCRB), which predicts that the use of maximally entangled probes enhances the estimation precision, when compared with the use of independent probes.","However, the bound is only achievable if the statistical model used in the estimation remains identifiable throughout the procedure.","This in turn sets different limits on the maximal sensing time used in each run of the estimation procedure, when entangled and independent probes are used.","When those constraints are taken into account, one can show that, when the total number of probes and the total duration of the estimation process are counted as fixed resources, the use of entangled probes is, in fact, disadvantageous when compared with the use of independent probes.","In order to counteract the limitations imposed on the sensing time by the requirement of identifiability of the statistical model, we propose a time-adaptive strategy, in which the sensing time is adequately increased at each step of the estimation process, calculate an attainable error bound for the strategy and discuss how to optimally choose its parameters in order to minimize that bound.","We show that the proposed strategy leads to much better scaling of the estimation uncertainty with the total number of probes and the total sensing time than the traditional fixed-sensing-time strategy.","We also show that, when the total number of probes and the total sensing time are counted as resources, independent probes and maximally entangled ones have now the same performance, in contrast to the non-adaptive strategy, where the use of independent is more advantageous than the use of maximally entangled ones."],"url":"http://arxiv.org/abs/2405.06548v1","category":"quant-ph"}
{"created":"2024-05-10 15:14:23","title":"Semantic and Spatial Adaptive Pixel-level Classifier for Semantic Segmentation","abstract":"Vanilla pixel-level classifiers for semantic segmentation are based on a certain paradigm, involving the inner product of fixed prototypes obtained from the training set and pixel features in the test image. This approach, however, encounters significant limitations, i.e., feature deviation in the semantic domain and information loss in the spatial domain. The former struggles with large intra-class variance among pixel features from different images, while the latter fails to utilize the structured information of semantic objects effectively. This leads to blurred mask boundaries as well as a deficiency of fine-grained recognition capability. In this paper, we propose a novel Semantic and Spatial Adaptive (SSA) classifier to address the above challenges. Specifically, we employ the coarse masks obtained from the fixed prototypes as a guide to adjust the fixed prototype towards the center of the semantic and spatial domains in the test image. The adapted prototypes in semantic and spatial domains are then simultaneously considered to accomplish classification decisions. In addition, we propose an online multi-domain distillation learning strategy to improve the adaption process. Experimental results on three publicly available benchmarks show that the proposed SSA significantly improves the segmentation performance of the baseline models with only a minimal increase in computational cost. Code is available at https://github.com/xwmaxwma/SSA.","sentences":["Vanilla pixel-level classifiers for semantic segmentation are based on a certain paradigm, involving the inner product of fixed prototypes obtained from the training set and pixel features in the test image.","This approach, however, encounters significant limitations, i.e., feature deviation in the semantic domain and information loss in the spatial domain.","The former struggles with large intra-class variance among pixel features from different images, while the latter fails to utilize the structured information of semantic objects effectively.","This leads to blurred mask boundaries as well as a deficiency of fine-grained recognition capability.","In this paper, we propose a novel Semantic and Spatial Adaptive (SSA) classifier to address the above challenges.","Specifically, we employ the coarse masks obtained from the fixed prototypes as a guide to adjust the fixed prototype towards the center of the semantic and spatial domains in the test image.","The adapted prototypes in semantic and spatial domains are then simultaneously considered to accomplish classification decisions.","In addition, we propose an online multi-domain distillation learning strategy to improve the adaption process.","Experimental results on three publicly available benchmarks show that the proposed SSA significantly improves the segmentation performance of the baseline models with only a minimal increase in computational cost.","Code is available at https://github.com/xwmaxwma/SSA."],"url":"http://arxiv.org/abs/2405.06525v1","category":"cs.CV"}
{"created":"2024-05-10 14:58:49","title":"Measures and Charges on the Places of $\\bar{\\mathbb Q}$","abstract":"Recent work of the author established dual representation theorems for certain vector spaces that arise in an important article of Allcock and Vaaler. These results constructed an object called a consistent map which acts like a measure on the set of places of $\\bar{\\mathbb Q}$, but is not a Borel measure on this space. We describe the appropriate ring of sets $\\mathcal R$ for which every consistent map arises from a measure on $\\mathcal R$. We further obtain the conditions under which a consistent map may be extended to a measure on the smallest algebra containing $\\mathcal R$.","sentences":["Recent work of the author established dual representation theorems for certain vector spaces that arise in an important article of Allcock and Vaaler.","These results constructed an object called a consistent map which acts like a measure on the set of places of $\\bar{\\mathbb Q}$, but is not a Borel measure on this space.","We describe the appropriate ring of sets $\\mathcal R$ for which every consistent map arises from a measure on $\\mathcal R$. We further obtain the conditions under which a consistent map may be extended to a measure on the smallest algebra containing $\\mathcal R$."],"url":"http://arxiv.org/abs/2405.06519v1","category":"math.NT"}
{"created":"2024-05-10 14:14:09","title":"Solar prominence diagnostics and their associated estimated errors from 1D NLTE Mg II h&k modelling","abstract":"Aims. We present further development of the rolling root mean square (rRMS) algorithm. These improvements consist of an increase in computational speed and an estimation of the uncertainty on the recovered diagnostics. This improvement is named the cross root mean square (xRMS) algorithm.   Methods. We used the quantile method to recover the statistics of the line profiles in order to study the evolution of the prominence observed by IRIS on 1 October 2019. We then introduced the improvements to rRMS. These improvements greatly increased the computational speed, and this increase in speed allowed us to use a large model grid. Thus, we utilised a grid of 23 940 models to recover the thermodynamic diagnostics. We used the 'good' (but not 'best') fitting models to recover an estimate of the uncertainty on the recovered diagnostics.   Results. The maximum line-of-sight (LOS) velocities were found to be 70 km/s. The line widths were mostly 0.4 {\\AA} with the asymmetries of most pixels around zero. The central temperature of the prominence was found to range from 10 kK to 20 kK, with uncertainties of approximately +/-5 to +/-15 kK. The central pressure was around 0.2 dyn/cm2, with uncertainties of +/-0.2 to +/-0.3 dyn/cm2. The ionisation degree ranged from 1 to 1000, with uncertainties mostly in the range +/-10 to +/-100. The electron density was mostly 10^10 /cm3, with uncertainties of mostly +/-10^9.   Conclusions. The new xRMS algorithm finds an estimation of the errors of the recovered thermodynamic properties. To our knowledge, this is the first attempt at systematically determining the errors from forward modelling. The large range of errors found may hint at the degeneracies present when using a single ion and/or species from forward modelling. In the future, co-aligned observations of more than one ion and/or species should be used to attempt to constrain this problem.","sentences":["Aims.","We present further development of the rolling root mean square (rRMS) algorithm.","These improvements consist of an increase in computational speed and an estimation of the uncertainty on the recovered diagnostics.","This improvement is named the cross root mean square (xRMS) algorithm.   Methods.","We used the quantile method to recover the statistics of the line profiles in order to study the evolution of the prominence observed by IRIS on 1 October 2019.","We then introduced the improvements to rRMS.","These improvements greatly increased the computational speed, and this increase in speed allowed us to use a large model grid.","Thus, we utilised a grid of 23 940 models to recover the thermodynamic diagnostics.","We used the 'good' (but not 'best') fitting models to recover an estimate of the uncertainty on the recovered diagnostics.   Results.","The maximum line-of-sight (LOS) velocities were found to be 70 km/s. The line widths were mostly 0.4 {\\AA} with the asymmetries of most pixels around zero.","The central temperature of the prominence was found to range from 10 kK to 20 kK, with uncertainties of approximately +/-5 to +/-15","kK.","The central pressure was around 0.2 dyn/cm2, with uncertainties of +/-0.2 to +/-0.3 dyn/cm2.","The ionisation degree ranged from 1 to 1000, with uncertainties mostly in the range +/-10 to +/-100.","The electron density was mostly 10^10 /cm3, with uncertainties of mostly +/-10^9.   Conclusions.","The new xRMS algorithm finds an estimation of the errors of the recovered thermodynamic properties.","To our knowledge, this is the first attempt at systematically determining the errors from forward modelling.","The large range of errors found may hint at the degeneracies present when using a single ion and/or species from forward modelling.","In the future, co-aligned observations of more than one ion and/or species should be used to attempt to constrain this problem."],"url":"http://arxiv.org/abs/2405.06492v1","category":"astro-ph.SR"}
{"created":"2024-05-10 14:12:26","title":"Forecasting constraints from surface brightness fluctuations in galaxy clusters","abstract":"Studies of surface brightness (SB) fluctuations in the intracluster medium (ICM) present an indirect estimate of turbulent pressure support and associated Mach numbers. While high resolution X-ray spectroscopy offer means to directly constrain line of sight gas motions, including those due to turbulence, such observations are relatively expensive and will be limited to nearby, bright clusters. In this respect, SB fluctuations are the most economical means to constrain turbulent motions at large cluster radii across a range of redshifts and masses.   To forecast what current and future X-ray and SZ facilities may achieve in SB fluctuation studies, I review and synthesize matters of accuracy and precision with respect to calculating power spectra of SB fluctuations, from which turbulent properties are derived. Balance concerns of power spectrum accuracy and precision across a range of spatial scales, I propose the use of three annuli with: [0,0.4] $R_{500}$, [0.4,1] $R_{500}$, and [1,1.5] $R_{500}$. Adopting these three regions, I calculate the uncertainty in the hydrostatic mass bias, $\\sigma_{b_{\\mathcal{M}}}$, can be achieved for various instruments in several scenarios. I find that $\\textit{Lynx}$ and AtLAST are competitive in their constraints at $R_{500}$, while AtLAST should perform better when constraining $\\sigma_{b_{\\mathcal{M}}}$ at $R_{200}$.","sentences":["Studies of surface brightness (SB) fluctuations in the intracluster medium (ICM) present an indirect estimate of turbulent pressure support and associated Mach numbers.","While high resolution X-ray spectroscopy offer means to directly constrain line of sight gas motions, including those due to turbulence, such observations are relatively expensive and will be limited to nearby, bright clusters.","In this respect, SB fluctuations are the most economical means to constrain turbulent motions at large cluster radii across a range of redshifts and masses.   ","To forecast what current and future X-ray and SZ facilities may achieve in SB fluctuation studies, I review and synthesize matters of accuracy and precision with respect to calculating power spectra of SB fluctuations, from which turbulent properties are derived.","Balance concerns of power spectrum accuracy and precision across a range of spatial scales, I propose the use of three annuli with: [0,0.4] $R_{500}$, [0.4,1] $R_{500}$, and [1,1.5] $R_{500}$. Adopting these three regions, I calculate the uncertainty in the hydrostatic mass bias, $\\sigma_{b_{\\mathcal{M}}}$, can be achieved for various instruments in several scenarios.","I find that $\\textit{Lynx}$ and AtLAST are competitive in their constraints at $R_{500}$, while AtLAST should perform better when constraining $\\sigma_{b_{\\mathcal{M}}}$ at $R_{200}$."],"url":"http://arxiv.org/abs/2405.06489v1","category":"astro-ph.CO"}
{"created":"2024-05-10 13:58:05","title":"Collisionless conduction in a high-beta plasma: a collision operator for whistler turbulence","abstract":"The regulation of electron heat transport in high-beta, weakly collisional, magnetized plasma is investigated. A temperature gradient oriented along a mean magnetic field can induce a kinetic heat-flux-driven whistler instability (HWI), which back-reacts on the transport by scattering electrons and impeding their flow. Previous analytical and numerical studies have shown that the heat flux for the saturated HWI scales as the inverse of electron beta. These numerical studies, however, had limited scale separation and consequently large fluctuation amplitudes, which calls into question their relevance at astrophysical scales. To this end, we perform a series of particle-in-cell simulations of the HWI across a range of electron beta and temperature-gradient length scales under two different physical setups. The saturated heat flux in all of our simulations follows the expected inverse-electron-beta scaling, supporting the robustness of the result. We also use our simulation results to develop and implement several methods to construct an effective collision operator for whistler turbulence. The results point to an issue with the standard quasi-linear explanation of HWI saturation, which is analogous to the well-known 90-degree scattering problem in the cosmic ray community. Despite this limitation, the methods developed here can serve as a blueprint for future work seeking to characterize the effective collisionality caused by kinetic instabilities.","sentences":["The regulation of electron heat transport in high-beta, weakly collisional, magnetized plasma is investigated.","A temperature gradient oriented along a mean magnetic field can induce a kinetic heat-flux-driven whistler instability (HWI), which back-reacts on the transport by scattering electrons and impeding their flow.","Previous analytical and numerical studies have shown that the heat flux for the saturated HWI scales as the inverse of electron beta.","These numerical studies, however, had limited scale separation and consequently large fluctuation amplitudes, which calls into question their relevance at astrophysical scales.","To this end, we perform a series of particle-in-cell simulations of the HWI across a range of electron beta and temperature-gradient length scales under two different physical setups.","The saturated heat flux in all of our simulations follows the expected inverse-electron-beta scaling, supporting the robustness of the result.","We also use our simulation results to develop and implement several methods to construct an effective collision operator for whistler turbulence.","The results point to an issue with the standard quasi-linear explanation of HWI saturation, which is analogous to the well-known 90-degree scattering problem in the cosmic ray community.","Despite this limitation, the methods developed here can serve as a blueprint for future work seeking to characterize the effective collisionality caused by kinetic instabilities."],"url":"http://arxiv.org/abs/2405.06481v1","category":"astro-ph.HE"}
{"created":"2024-05-10 13:55:08","title":"Informativeness of Weighted Conformal Prediction","abstract":"Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data. However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals. To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions. We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations.","sentences":["Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data.","However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals.","To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions.","We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations."],"url":"http://arxiv.org/abs/2405.06479v1","category":"stat.ME"}
{"created":"2024-05-10 13:49:55","title":"Asymptotic Normality of $U$-Statistics is Equivalent to Convergence in the Wasserstein Distance","abstract":"We prove the claim in the title under mild conditions which are usually satisfied when trying to establish asymptotic normality. We assume strictly stationary and absolutely regular data.","sentences":["We prove the claim in the title under mild conditions which are usually satisfied when trying to establish asymptotic normality.","We assume strictly stationary and absolutely regular data."],"url":"http://arxiv.org/abs/2405.06477v1","category":"math.ST"}
{"created":"2024-05-10 12:25:06","title":"Fair Mixed Effects Support Vector Machine","abstract":"To ensure unbiased and ethical automated predictions, fairness must be a core principle in machine learning applications. Fairness in machine learning aims to mitigate biases present in the training data and model imperfections that could lead to discriminatory outcomes. This is achieved by preventing the model from making decisions based on sensitive characteristics like ethnicity or sexual orientation. A fundamental assumption in machine learning is the independence of observations. However, this assumption often does not hold true for data describing social phenomena, where data points are often clustered based. Hence, if the machine learning models do not account for the cluster correlations, the results may be biased. Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest. We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously. With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions.","sentences":["To ensure unbiased and ethical automated predictions, fairness must be a core principle in machine learning applications.","Fairness in machine learning aims to mitigate biases present in the training data and model imperfections that could lead to discriminatory outcomes.","This is achieved by preventing the model from making decisions based on sensitive characteristics like ethnicity or sexual orientation.","A fundamental assumption in machine learning is the independence of observations.","However, this assumption often does not hold true for data describing social phenomena, where data points are often clustered based.","Hence, if the machine learning models do not account for the cluster correlations, the results may be biased.","Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest.","We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously.","With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions."],"url":"http://arxiv.org/abs/2405.06433v1","category":"cs.LG"}
{"created":"2024-05-10 12:18:56","title":"Weighted past and paired dynamic varentropy measures, their properties and usefulness","abstract":"We introduce two uncertainty measures, say weighted past varentropy (WPVE) and weighted paired dynamic varentropy (WPDVE). Several properties have been studied for these proposed measures. The effect of the monotone transformation for these measures have been discussed. We have obtained an upper bound of the WPVE using the weighted past Shannon entropy. A lower bound of the WPVE is also obtained. The WPVE has been studied for proportional reversed hazard rate (PRHR) models. Upper and lower bounds of the WPDVE have been derived. We propose non-parametric kernel estimates of the WPVE and WPDVE. Further, maximum likelihood estimation has been employed to estimate WPVE and WPDVE for an exponential population. A numerical simulation is provided to observe the behaviour of the proposed estimates. Finally, we have analysed a real data set and obtain the estimated values of WPVE.","sentences":["We introduce two uncertainty measures, say weighted past varentropy (WPVE) and weighted paired dynamic varentropy (WPDVE).","Several properties have been studied for these proposed measures.","The effect of the monotone transformation for these measures have been discussed.","We have obtained an upper bound of the WPVE using the weighted past Shannon entropy.","A lower bound of the WPVE is also obtained.","The WPVE has been studied for proportional reversed hazard rate (PRHR) models.","Upper and lower bounds of the WPDVE have been derived.","We propose non-parametric kernel estimates of the WPVE and WPDVE.","Further, maximum likelihood estimation has been employed to estimate WPVE and WPDVE for an exponential population.","A numerical simulation is provided to observe the behaviour of the proposed estimates.","Finally, we have analysed a real data set and obtain the estimated values of WPVE."],"url":"http://arxiv.org/abs/2405.06428v1","category":"math.ST"}
{"created":"2024-05-10 11:42:44","title":"I3DGS: Improve 3D Gaussian Splatting from Multiple Dimensions","abstract":"3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain an implicit neural learning rendering result than the traditional neural rendering technology but keep the more high-definition fast rendering speed. But it is still difficult to achieve a fast enough efficiency on 3D Gaussian Splatting for the practical applications. To Address this issue, we propose the I3DS, a synthetic model performance improvement evaluation solution and experiments test. From multiple and important levels or dimensions of the original 3D Gaussian Splatting, we made more than two thousand various kinds of experiments to test how the selected different items and components can make an impact on the training efficiency of the 3D Gaussian Splatting model. In this paper, we will share abundant and meaningful experiences and methods about how to improve the training, performance and the impacts caused by different items of the model. A special but normal Integer compression in base 95 and a floating-point compression in base 94 with ASCII encoding and decoding mechanism is presented. Many real and effective experiments and test results or phenomena will be recorded. After a series of reasonable fine-tuning, I3DS can gain excellent performance improvements than the previous one. The project code is available as open source.","sentences":["3D Gaussian Splatting is a novel method for 3D view synthesis, which can gain an implicit neural learning rendering result than the traditional neural rendering technology but keep the more high-definition fast rendering speed.","But it is still difficult to achieve a fast enough efficiency on 3D Gaussian Splatting for the practical applications.","To Address this issue, we propose the I3DS, a synthetic model performance improvement evaluation solution and experiments test.","From multiple and important levels or dimensions of the original 3D Gaussian Splatting, we made more than two thousand various kinds of experiments to test how the selected different items and components can make an impact on the training efficiency of the 3D Gaussian Splatting model.","In this paper, we will share abundant and meaningful experiences and methods about how to improve the training, performance and the impacts caused by different items of the model.","A special but normal Integer compression in base 95 and a floating-point compression in base 94 with ASCII encoding and decoding mechanism is presented.","Many real and effective experiments and test results or phenomena will be recorded.","After a series of reasonable fine-tuning, I3DS can gain excellent performance improvements than the previous one.","The project code is available as open source."],"url":"http://arxiv.org/abs/2405.06408v1","category":"cs.CV"}
{"created":"2024-05-10 09:18:21","title":"DarsakX: A Python Package for Designing and Analyzing Imaging Performance of X-ray Telescopes","abstract":"The imaging performance and sensitivity of an X-ray telescope when observing astrophysical sources are primarily governed by the optical design, geometrical uncertainties (figure errors, surface roughness, and mirror alignment inaccuracies), and the reflectivity properties of the X-ray reflecting mirror surface. To thoroughly evaluate the imaging performance of an X-ray telescope with an optical design similar to Wolter-1 optics, which comprises multiple shells with known geometrical uncertainties and mirror reflectivity properties, appropriate computational tools are essential. These tools are used to estimate the angular resolution and effective area for various source energies and locations and, more importantly, to assess the impact of figure errors on the telescope's imaging performance. Additionally, they can also be used to optimize optics geometry by modifying it in reference to the Wolter-1 optics, aiming to minimize the optical aberration associated with the Wolter-1 configuration. In this paper, we introduce DarsakX, a Python-based ray tracing computational tool specifically designed to estimate the imaging performance of a multi-shell X-ray telescope. DarsakX has the capability to simulate the impact of figure errors present in the axial direction of a mirror shell. The geometrical shape of the mirror shells can be defined as a combination of figure error with the base optics, such as Wolter-1 or Conical optics. Additionally, DarsakX allows the exploration of new optical designs involving two reflections similar to Wolter-1 optics but with an improved angular resolution for wide-field telescopes. Developed through an analytical approach, DarsakX ensures computational efficiency, enabling fast processing.","sentences":["The imaging performance and sensitivity of an X-ray telescope when observing astrophysical sources are primarily governed by the optical design, geometrical uncertainties (figure errors, surface roughness, and mirror alignment inaccuracies), and the reflectivity properties of the X-ray reflecting mirror surface.","To thoroughly evaluate the imaging performance of an X-ray telescope with an optical design similar to Wolter-1 optics, which comprises multiple shells with known geometrical uncertainties and mirror reflectivity properties, appropriate computational tools are essential.","These tools are used to estimate the angular resolution and effective area for various source energies and locations and, more importantly, to assess the impact of figure errors on the telescope's imaging performance.","Additionally, they can also be used to optimize optics geometry by modifying it in reference to the Wolter-1 optics, aiming to minimize the optical aberration associated with the Wolter-1 configuration.","In this paper, we introduce DarsakX, a Python-based ray tracing computational tool specifically designed to estimate the imaging performance of a multi-shell X-ray telescope.","DarsakX has the capability to simulate the impact of figure errors present in the axial direction of a mirror shell.","The geometrical shape of the mirror shells can be defined as a combination of figure error with the base optics, such as Wolter-1 or Conical optics.","Additionally, DarsakX allows the exploration of new optical designs involving two reflections similar to Wolter-1 optics but with an improved angular resolution for wide-field telescopes.","Developed through an analytical approach, DarsakX ensures computational efficiency, enabling fast processing."],"url":"http://arxiv.org/abs/2405.06343v1","category":"astro-ph.IM"}
{"created":"2024-05-10 09:10:51","title":"Regularization with optimal space-time priors","abstract":"We propose a variational regularization approach based on cylindrical shearlets to deal with dynamic imaging problems, with dynamic tomography as guiding example. The idea is that the mismatch term essentially integrates a sequence of separable, static problems, while the regularization term sees the non-stationary target as a spatio-temporal object. We motivate this approach by showing that cylindrical shearlets provide optimally sparse approximations for the class of cartoon-like videos, i.e., a class of functions useful to model spatio-temporal image sequences and videos, which we introduce extending the classic notion of cartoon-like images. To formulate our regularization model, we define cylindrical shearlet smoothness spaces, which is pivotal to obtain suitable embeddings in functional spaces. To complete our analysis, we prove that the proposed regularization strategy is well-defined, the solution of the minimisation problem exists and is unique (for $ p > 1$). Furthermore, we provide convergence rates (in terms of the symmetric Bregman distance) under deterministic and random noise conditions, and within the context of statistical inverse learning. We numerically validate our theoretical results using both simulated and measured dynamic tomography data, showing that our approach leads to a practical and robust reconstruction strategy.","sentences":["We propose a variational regularization approach based on cylindrical shearlets to deal with dynamic imaging problems, with dynamic tomography as guiding example.","The idea is that the mismatch term essentially integrates a sequence of separable, static problems, while the regularization term sees the non-stationary target as a spatio-temporal object.","We motivate this approach by showing that cylindrical shearlets provide optimally sparse approximations for the class of cartoon-like videos, i.e., a class of functions useful to model spatio-temporal image sequences and videos, which we introduce extending the classic notion of cartoon-like images.","To formulate our regularization model, we define cylindrical shearlet smoothness spaces, which is pivotal to obtain suitable embeddings in functional spaces.","To complete our analysis, we prove that the proposed regularization strategy is well-defined, the solution of the minimisation problem exists and is unique (for $ p > 1$).","Furthermore, we provide convergence rates (in terms of the symmetric Bregman distance) under deterministic and random noise conditions, and within the context of statistical inverse learning.","We numerically validate our theoretical results using both simulated and measured dynamic tomography data, showing that our approach leads to a practical and robust reconstruction strategy."],"url":"http://arxiv.org/abs/2405.06337v1","category":"math.NA"}
{"created":"2024-05-10 08:33:28","title":"Distinguishing articles in questionable and non-questionable journals using quantitative indicators associated with quality","abstract":"This study investigates the viability of distinguishing articles in questionable journals (QJs) from those in non-QJs on the basis of quantitative indicators typically associated with quality. Subsequently, I examine what can be deduced about the quality of articles in QJs based on the differences observed. I contrast the length of abstracts and full-texts, prevalence of spelling errors, text readability, number of references and citations, the size and internationality of the author team, the documentation of ethics and informed consent statements, and the presence erroneous decisions based on statistical errors in 1,714 articles from 31 QJs, 1,691 articles from 16 journals indexed in Web of Science (WoS), and 1,900 articles from 45 mid-tier journals, all in the field of psychology. The results suggest that QJ articles do diverge from the disciplinary standards set by peer-reviewed journals in psychology on quantitative indicators of quality that tend to reflect the effect of peer review and editorial processes. However, mid-tier and WoS journals are also affected by potential quality concerns, such as under-reporting of ethics and informed consent processes and the presence of errors in interpreting statistics. Further research is required to develop a comprehensive understanding of the quality of articles in QJs.","sentences":["This study investigates the viability of distinguishing articles in questionable journals (QJs) from those in non-QJs on the basis of quantitative indicators typically associated with quality.","Subsequently, I examine what can be deduced about the quality of articles in QJs based on the differences observed.","I contrast the length of abstracts and full-texts, prevalence of spelling errors, text readability, number of references and citations, the size and internationality of the author team, the documentation of ethics and informed consent statements, and the presence erroneous decisions based on statistical errors in 1,714 articles from 31 QJs, 1,691 articles from 16 journals indexed in Web of Science (WoS), and 1,900 articles from 45 mid-tier journals, all in the field of psychology.","The results suggest that QJ articles do diverge from the disciplinary standards set by peer-reviewed journals in psychology on quantitative indicators of quality that tend to reflect the effect of peer review and editorial processes.","However, mid-tier and WoS journals are also affected by potential quality concerns, such as under-reporting of ethics and informed consent processes and the presence of errors in interpreting statistics.","Further research is required to develop a comprehensive understanding of the quality of articles in QJs."],"url":"http://arxiv.org/abs/2405.06308v1","category":"cs.DL"}
{"created":"2024-05-10 14:38:02","title":"EcoEdgeTwin: Enhanced 6G Network via Mobile Edge Computing and Digital Twin Integration","abstract":"In the 6G era, integrating Mobile Edge Computing (MEC) and Digital Twin (DT) technologies presents a transformative approach to enhance network performance through predictive, adaptive control for energy-efficient, low-latency communication. This paper presents the EcoEdgeTwin model, an innovative framework that harnesses the synergy between MEC and DT technologies to ensure efficient network operation. We optimize the utility function within the EcoEdgeTwin model to balance enhancing users' Quality of Experience (QoE) and minimizing latency and energy consumption at edge servers. This approach ensures efficient and adaptable network operations, utilizing DT to synchronize and integrate real-time data seamlessly.   Our framework achieves this by implementing robust mechanisms for task offloading, service caching, and cost-effective service migration. Additionally, it manages energy consumption related to task processing, communication, and the influence of DT predictions, all essential for optimizing latency and minimizing energy usage.   Through the utility model, we also prioritize QoE, fostering a user-centric approach to network management that balances network efficiency with user satisfaction. A cornerstone of our approach is integrating the advantage actor-critic algorithm, marking a pioneering use of deep reinforcement learning for dynamic network management. This strategy addresses challenges in service mobility and network variability, ensuring optimal network performance matrices. Our extensive simulations demonstrate that compared to benchmark models lacking DT integration, EcoEdgeTwin framework significantly reduces energy usage and latency while enhancing QoE.","sentences":["In the 6G era, integrating Mobile Edge Computing (MEC) and Digital Twin (DT) technologies presents a transformative approach to enhance network performance through predictive, adaptive control for energy-efficient, low-latency communication.","This paper presents the EcoEdgeTwin model, an innovative framework that harnesses the synergy between MEC and DT technologies to ensure efficient network operation.","We optimize the utility function within the EcoEdgeTwin model to balance enhancing users' Quality of Experience (QoE) and minimizing latency and energy consumption at edge servers.","This approach ensures efficient and adaptable network operations, utilizing DT to synchronize and integrate real-time data seamlessly.   ","Our framework achieves this by implementing robust mechanisms for task offloading, service caching, and cost-effective service migration.","Additionally, it manages energy consumption related to task processing, communication, and the influence of DT predictions, all essential for optimizing latency and minimizing energy usage.   ","Through the utility model, we also prioritize QoE, fostering a user-centric approach to network management that balances network efficiency with user satisfaction.","A cornerstone of our approach is integrating the advantage actor-critic algorithm, marking a pioneering use of deep reinforcement learning for dynamic network management.","This strategy addresses challenges in service mobility and network variability, ensuring optimal network performance matrices.","Our extensive simulations demonstrate that compared to benchmark models lacking DT integration, EcoEdgeTwin framework significantly reduces energy usage and latency while enhancing QoE."],"url":"http://arxiv.org/abs/2405.06507v1","category":"cs.NI"}
{"created":"2024-05-10 17:59:08","title":"Linearizing Large Language Models","abstract":"Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.","sentences":["Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost.","However, their original formulation suffers from poor scaling and underperforms compute-matched transformers.","Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments.","Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets.","As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA).","We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget.","This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost.","We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models.","Our code and models can be found at https://github.com/TRI-ML/linear_open_lm."],"url":"http://arxiv.org/abs/2405.06640v1","category":"cs.CL"}
{"created":"2024-05-10 16:45:15","title":"Sensing force gradients with cavity optomechanics while evading backaction","abstract":"We study force gradient sensing by a coherently driven mechanical resonator with phase-sensitive detection of motion via the two-tone backaction evading measurement of cavity optomechanics. The response of the cavity to two coherent pumps is solved by numerical integration of the classical equations of motion, showing an extended region of monotonic response. We use Floquet theory to model the fluctuations, which rise only slightly above that of the usual backaction evading measurement in the presence of the mechanical drive. Our analysis indicates that this sensing technique is advantageous for applications such as Atomic Force Microscopy.","sentences":["We study force gradient sensing by a coherently driven mechanical resonator with phase-sensitive detection of motion via the two-tone backaction evading measurement of cavity optomechanics.","The response of the cavity to two coherent pumps is solved by numerical integration of the classical equations of motion, showing an extended region of monotonic response.","We use Floquet theory to model the fluctuations, which rise only slightly above that of the usual backaction evading measurement in the presence of the mechanical drive.","Our analysis indicates that this sensing technique is advantageous for applications such as Atomic Force Microscopy."],"url":"http://arxiv.org/abs/2405.06589v1","category":"quant-ph"}
{"created":"2024-05-10 15:58:04","title":"Steady electric currents in magnetized QCD and their use for the equation of state","abstract":"In this paper we study the emergence of steady electric currents in QCD as a response to a non-uniform magnetic background using lattice simulations with 2 + 1 quark flavors at the physical point, as well as leading-order chiral perturbation theory. Using these currents, we develop a novel method to determine the leading-order coefficient of the equation of state in a magnetic field expansion: the magnetic susceptibility of the QCD medium. We decompose the current expectation value into valence- and sea-quark contributions and demonstrate that the dominant contribution to the electric current is captured by the valence term alone, allowing for a comparably cheap determination of the susceptibility. Our continuum extrapolated lattice results for the equation of state confirm the findings of some of the existing studies in the literature, namely that the QCD medium behaves diamagnetically at low and paramagnetically at high temperatures.","sentences":["In this paper we study the emergence of steady electric currents in QCD as a response to a non-uniform magnetic background using lattice simulations with 2 + 1 quark flavors at the physical point, as well as leading-order chiral perturbation theory.","Using these currents, we develop a novel method to determine the leading-order coefficient of the equation of state in a magnetic field expansion: the magnetic susceptibility of the QCD medium.","We decompose the current expectation value into valence- and sea-quark contributions and demonstrate that the dominant contribution to the electric current is captured by the valence term alone, allowing for a comparably cheap determination of the susceptibility.","Our continuum extrapolated lattice results for the equation of state confirm the findings of some of the existing studies in the literature, namely that the QCD medium behaves diamagnetically at low and paramagnetically at high temperatures."],"url":"http://arxiv.org/abs/2405.06557v1","category":"hep-lat"}
{"created":"2024-05-10 15:22:27","title":"Existence and uniqueness of $t$-graphs of prescribed mean curvature in Heisenberg groups","abstract":"We study the prescribed mean curvature equation for $t$-graphs in a Riemannian Heisenberg group of arbitrary dimension. We characterize the existence of classical solutions in a bounded domain without imposing Dirichlet boundary data, and we provide conditions that guarantee uniqueness. Moreover, we extend previous results to solve the Dirichlet problem when the mean curvature is non-constant. Finally, by an approximation technique, we obtain solutions to the sub-Riemannian prescribed mean curvature equation.","sentences":["We study the prescribed mean curvature equation for $t$-graphs in a Riemannian Heisenberg group of arbitrary dimension.","We characterize the existence of classical solutions in a bounded domain without imposing Dirichlet boundary data, and we provide conditions that guarantee uniqueness.","Moreover, we extend previous results to solve the Dirichlet problem when the mean curvature is non-constant.","Finally, by an approximation technique, we obtain solutions to the sub-Riemannian prescribed mean curvature equation."],"url":"http://arxiv.org/abs/2405.06533v1","category":"math.DG"}
{"created":"2024-05-10 15:19:39","title":"The Morse property of limit functions appearing in mean field equations on surfaces with boundary","abstract":"In this paper we study the Morse property for functions related to limit functions of mean field equations on a smooth, compact surface $\\Sigma$ with boundary $\\partial\\Sigma$. Given a Riemannian metric $g$ on $\\Sigma$ we consider functions of the form   \\[   f_g(x) := \\sum_{i=1}^m\\sigma_i^2R^g(x_i)+\\sum_{i,j=1\\\\i\\ne j}^m\\sigma_i\\sigma_jG^g(x_i,x_j)+h(x_1,\\ldots,x_m),   \\]   where $\\sigma_i \\neq 0$ for $i=1,\\ldots,m$, $G^g$ is the Green function of the Laplace-Beltrami operator on $(\\Sigma,g)$ with Neumann boundary conditions, $R^g$ is the corresponding Robin function, and $h \\in \\mathcal{C}^{2}(\\Sigma^m,\\mathbb{R})$ is arbitrary. We prove that for any Riemannian metric $g$, there exists a metric $\\widetilde g$ which is arbitrarily close to $g$ and in the conformal class of $g$ such that $f_{\\widetilde g}$ is a Morse function. Furthermore we show that, if all $\\sigma_i>0$, then the set of Riemannian metrics for which $f_g$ is a Morse function is open and dense in the set of all Riemannian metrics.","sentences":["In this paper we study the Morse property for functions related to limit functions of mean field equations on a smooth, compact surface $\\Sigma$ with boundary $\\partial\\Sigma$. Given a Riemannian metric $g$ on $\\Sigma$ we consider functions of the form   \\[   f_g(x) := \\sum_{i=1}^m\\sigma_i^2R^g(x_i)+\\sum_{i,j=1\\\\i\\ne j}^m\\sigma_i\\sigma_jG^g(x_i,x_j)+h(x_1,\\ldots,x_m),   \\]   where $\\sigma_i \\neq 0$ for $i=1,\\ldots,m$, $G^g$ is the Green function of the Laplace-Beltrami operator on $(\\Sigma,g)$ with Neumann boundary conditions, $R^g$ is the corresponding Robin function, and $h \\in \\mathcal{C}^{2}(\\Sigma^m,\\mathbb{R})$ is arbitrary.","We prove that for any Riemannian metric $g$, there exists a metric $\\widetilde g$ which is arbitrarily close to $g$ and in the conformal class of $g$ such that $f_{\\widetilde g}$ is a Morse function.","Furthermore we show that, if all $\\sigma_i>0$, then the set of Riemannian metrics for which $f_g$ is a Morse function is open and dense in the set of all Riemannian metrics."],"url":"http://arxiv.org/abs/2405.06530v1","category":"math.DG"}
{"created":"2024-05-10 14:08:35","title":"Can Neural Networks learn Finite Elements?","abstract":"The aim of this note is to construct a neural network for which the linear finite element approximation of a simple one dimensional boundary value problem is a minimum of the cost function to find out if the neural network is able to reproduce the finite element approximation. The deepest goal is to shed some light on the problems one encounters when trying to use neural networks to approximate partial differential equations","sentences":["The aim of this note is to construct a neural network for which the linear finite element approximation of a simple one dimensional boundary value problem is a minimum of the cost function to find out if the neural network is able to reproduce the finite element approximation.","The deepest goal is to shed some light on the problems one encounters when trying to use neural networks to approximate partial differential equations"],"url":"http://arxiv.org/abs/2405.06488v1","category":"math.NA"}
{"created":"2024-05-10 12:27:45","title":"Photonic Neuromorphic Accelerator for Convolutional Neural Networks based on an Integrated Reconfigurable Mesh","abstract":"In this work, we present and experimentally validate a passive photonic-integrated neuromorphic accelerator that uses a hardware-friendly optical spectrum slicing technique through a reconfigurable silicon photonic mesh. The proposed scheme acts as an analogue convolutional engine, enabling information preprocessing in the optical domain, dimensionality reduction and extraction of spatio-temporal features. Numerical results demonstrate that utilizing only 7 passive photonic nodes, critical modules of a digital convolutional neural network can be replaced. As a result, a 98.6% accuracy on the MNIST dataset was achieved, with a power consumption reduction of at least 26% compared to digital CNNs. Experimental results confirm these findings, achieving 97.7% accuracy with only 3 passive nodes.","sentences":["In this work, we present and experimentally validate a passive photonic-integrated neuromorphic accelerator that uses a hardware-friendly optical spectrum slicing technique through a reconfigurable silicon photonic mesh.","The proposed scheme acts as an analogue convolutional engine, enabling information preprocessing in the optical domain, dimensionality reduction and extraction of spatio-temporal features.","Numerical results demonstrate that utilizing only 7 passive photonic nodes, critical modules of a digital convolutional neural network can be replaced.","As a result, a 98.6% accuracy on the MNIST dataset was achieved, with a power consumption reduction of at least 26% compared to digital CNNs.","Experimental results confirm these findings, achieving 97.7% accuracy with only 3 passive nodes."],"url":"http://arxiv.org/abs/2405.06434v1","category":"physics.optics"}
{"created":"2024-05-10 12:21:07","title":"Algebraic integers with continued fraction expansions containing palindromes and square roots with prescribed periods","abstract":"We prove that there exist infinitely many algebraic integers with continued fraction expansion of the kind $[a_0, \\overline{a_1, \\ldots, a_n, s}]$ where $(a_1, \\ldots, a_n)$ is a palindrome and $s \\in \\mathbb N_{\\geq1}$, characterizing all the algebraic integers with such expansions. We also provide an explicit method for finding $s$ and determining the corresponding algebraic integer. Moreover, we deal with the particular case $(a_1, \\ldots, a_n) = (m, \\ldots, m)$ describing the corresponding algebraic integers in terms of Fibonacci polynomials. We exploit these results for obtaining expansions of square roots of integers with prescribed periods and we also write explicitly the fundamental solutions of the corresponding Pell's equations.","sentences":["We prove that there exist infinitely many algebraic integers with continued fraction expansion of the kind $[a_0, \\overline{a_1, \\ldots, a_n, s}]$ where $(a_1, \\ldots, a_n)$ is a palindrome and $s \\in \\mathbb N_{\\geq1}$, characterizing all the algebraic integers with such expansions.","We also provide an explicit method for finding $s$ and determining the corresponding algebraic integer.","Moreover, we deal with the particular case $(a_1, \\ldots, a_n) = (m, \\ldots, m)$ describing the corresponding algebraic integers in terms of Fibonacci polynomials.","We exploit these results for obtaining expansions of square roots of integers with prescribed periods and we also write explicitly the fundamental solutions of the corresponding Pell's equations."],"url":"http://arxiv.org/abs/2405.06430v1","category":"math.NT"}
{"created":"2024-05-10 11:17:36","title":"Unbounded Hamilton-Jacobi-Bellman Equations with one co-dimensional discontinuities","abstract":"The aim of this work is to deal with a discontinuous Hamilton-Jacobi equation in the whole euclidian N-dimensional space, associated to a possibly unbounded optimal control problem. Here, the discontinuities are located on a hyperplane and the typical questions we address concern the existence and uniqueness of solutions, and of course the definition itself of solution. We consider viscosity solutions in the sense of Ishii. The convex Hamiltonians are associated to a control problem with specific cost and dynamics given on each side of the hyperplane. We assume that those are Lipschitz continuous but the main difficulty we deal with is that they are potentially unbounded, as well as the control spaces. Using Bellman's approach we construct two value functions which turn out to be the minimal and maximal solutions in the sense of Ishii. Moreover, we also build a whole family of value functions, which are still solutions in the sense of Ishii and connect continuously the minimal solution to the maximal one.","sentences":["The aim of this work is to deal with a discontinuous Hamilton-Jacobi equation in the whole euclidian N-dimensional space, associated to a possibly unbounded optimal control problem.","Here, the discontinuities are located on a hyperplane and the typical questions we address concern the existence and uniqueness of solutions, and of course the definition itself of solution.","We consider viscosity solutions in the sense of Ishii.","The convex Hamiltonians are associated to a control problem with specific cost and dynamics given on each side of the hyperplane.","We assume that those are Lipschitz continuous but the main difficulty we deal with is that they are potentially unbounded, as well as the control spaces.","Using Bellman's approach we construct two value functions which turn out to be the minimal and maximal solutions in the sense of Ishii.","Moreover, we also build a whole family of value functions, which are still solutions in the sense of Ishii and connect continuously the minimal solution to the maximal one."],"url":"http://arxiv.org/abs/2405.06396v1","category":"math.OC"}
{"created":"2024-05-10 10:26:48","title":"Short-Time Force Response during the Impact of a Droplet with Gas Bubbles","abstract":"The presence of gas or vapour bubbles may strongly influence the forces that occur during the impact of a liquid mass onto a solid. Here, we study this effect numerically, in a well-controlled manner, by simulating the short-time interaction between an impacting droplet and a solid surface, mediated by the gas layer between droplet and solid just before collision, in the presence and absence of bubbles. A boundary integral method is used to simulate the falling droplet, the mediating air layer is modeled using lubrication theory, whereas uniform gas bubbles are added to the droplet that obey a polytropic equation of state. We show that the presence of gas bubbles inside the droplet can have a significant influence on the force exerted on the substrate, even before touchdown. This is due to the transmission of load from the solid, through the gas layer and finally into the bubbly droplet, buffering the impact. We simulate different bubble configurations, modifying their number, size, shape and initial position. It is found that larger bubbles, as well as those close to the impact zone, dampen the collision more as compared to small bubbles or the ones that are far from the droplet's surface. In addition, multiple small bubbles are shown to have a similar or even greater effect as a single large bubble.","sentences":["The presence of gas or vapour bubbles may strongly influence the forces that occur during the impact of a liquid mass onto a solid.","Here, we study this effect numerically, in a well-controlled manner, by simulating the short-time interaction between an impacting droplet and a solid surface, mediated by the gas layer between droplet and solid just before collision, in the presence and absence of bubbles.","A boundary integral method is used to simulate the falling droplet, the mediating air layer is modeled using lubrication theory, whereas uniform gas bubbles are added to the droplet that obey a polytropic equation of state.","We show that the presence of gas bubbles inside the droplet can have a significant influence on the force exerted on the substrate, even before touchdown.","This is due to the transmission of load from the solid, through the gas layer and finally into the bubbly droplet, buffering the impact.","We simulate different bubble configurations, modifying their number, size, shape and initial position.","It is found that larger bubbles, as well as those close to the impact zone, dampen the collision more as compared to small bubbles or the ones that are far from the droplet's surface.","In addition, multiple small bubbles are shown to have a similar or even greater effect as a single large bubble."],"url":"http://arxiv.org/abs/2405.06378v1","category":"physics.flu-dyn"}
{"created":"2024-05-10 10:09:46","title":"Ultra-fast Digital DPC Yielding High Spatio-Temporal Resolution for Low-Dose Phase Characterisation","abstract":"In the scanning transmission electron microscope, both phase imaging of beam-sensitive materials and characterisation of a material's functional properties using in-situ experiments are becoming more widely available. As the practicable scan speed of 4D-STEM detectors improves, so too does the temporal resolution achievable for both differential phase contrast (DPC) and ptychography. However, the read-out burden of pixelated detectors, and the size of the gigabyte to terabyte sized data sets, remain a challenge for both temporal resolution and their practical adoption. In this work, we show that a high-fidelity DPC phase reconstruction can be achieved from both annular segmented detectors or pixelated arrays with relatively few elements using signal digitisation. Unlike conventional analog data, even at the fastest scan speeds, phase reconstructions from digitised DPC-segment images yield reliable data. Finally, dose fractionation by fast scanning and multi-framing allows for post-process binning of frame streams to balance signal-to-noise ratio and temporal resolution for low-dose phase imaging for in-situ experiments.","sentences":["In the scanning transmission electron microscope, both phase imaging of beam-sensitive materials and characterisation of a material's functional properties using in-situ experiments are becoming more widely available.","As the practicable scan speed of 4D-STEM detectors improves, so too does the temporal resolution achievable for both differential phase contrast (DPC) and ptychography.","However, the read-out burden of pixelated detectors, and the size of the gigabyte to terabyte sized data sets, remain a challenge for both temporal resolution and their practical adoption.","In this work, we show that a high-fidelity DPC phase reconstruction can be achieved from both annular segmented detectors or pixelated arrays with relatively few elements using signal digitisation.","Unlike conventional analog data, even at the fastest scan speeds, phase reconstructions from digitised DPC-segment images yield reliable data.","Finally, dose fractionation by fast scanning and multi-framing allows for post-process binning of frame streams to balance signal-to-noise ratio and temporal resolution for low-dose phase imaging for in-situ experiments."],"url":"http://arxiv.org/abs/2405.06367v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 08:33:42","title":"Viscosity Solutions of Second Order Path-Dependent Partial Differential Equations and Applications","abstract":"In this article, a notion of viscosity solutions is introduced for fully nonlinear second order path-dependent partial differential equations in the spirit of [Zhou, Ann. Appl. Probab., 33 (2023), 5564-5612]. We prove the existence, comparison principle, consistency and stability for the viscosity solutions. Application to path-dependent stochastic differential games is given.","sentences":["In this article, a notion of viscosity solutions is introduced for fully nonlinear second order path-dependent partial differential equations in the spirit of [Zhou, Ann.","Appl.","Probab., 33 (2023), 5564-5612].","We prove the existence, comparison principle, consistency and stability for the viscosity solutions.","Application to path-dependent stochastic differential games is given."],"url":"http://arxiv.org/abs/2405.06309v1","category":"math.PR"}
{"created":"2024-05-10 17:11:31","title":"Explaining Text Similarity in Transformer Models","abstract":"As Transformers have become state-of-the-art models for natural language processing (NLP) tasks, the need to understand and explain their predictions is increasingly apparent. Especially in unsupervised applications, such as information retrieval tasks, similarity models built on top of foundation model representations have been widely applied. However, their inner prediction mechanisms have mostly remained opaque. Recent advances in explainable AI have made it possible to mitigate these limitations by leveraging improved explanations for Transformers through layer-wise relevance propagation (LRP). Using BiLRP, an extension developed for computing second-order explanations in bilinear similarity models, we investigate which feature interactions drive similarity in NLP models. We validate the resulting explanations and demonstrate their utility in three corpus-level use cases, analyzing grammatical interactions, multilingual semantics, and biomedical text retrieval. Our findings contribute to a deeper understanding of different semantic similarity tasks and models, highlighting how novel explainable AI methods enable in-depth analyses and corpus-level insights.","sentences":["As Transformers have become state-of-the-art models for natural language processing (NLP) tasks, the need to understand and explain their predictions is increasingly apparent.","Especially in unsupervised applications, such as information retrieval tasks, similarity models built on top of foundation model representations have been widely applied.","However, their inner prediction mechanisms have mostly remained opaque.","Recent advances in explainable AI have made it possible to mitigate these limitations by leveraging improved explanations for Transformers through layer-wise relevance propagation (LRP).","Using BiLRP, an extension developed for computing second-order explanations in bilinear similarity models, we investigate which feature interactions drive similarity in NLP models.","We validate the resulting explanations and demonstrate their utility in three corpus-level use cases, analyzing grammatical interactions, multilingual semantics, and biomedical text retrieval.","Our findings contribute to a deeper understanding of different semantic similarity tasks and models, highlighting how novel explainable AI methods enable in-depth analyses and corpus-level insights."],"url":"http://arxiv.org/abs/2405.06604v1","category":"cs.CL"}
{"created":"2024-05-10 16:00:29","title":"Random matrix theory improved Fr\u00e9chet mean of symmetric positive definite matrices","abstract":"In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr\\'echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr\\'echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.","sentences":["In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr\\'echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means.","Such means are leveraged in numerous machine-learning tasks.","Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr\\'echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average.","Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.06558v1","category":"stat.ML"}
{"created":"2024-05-10 14:25:49","title":"Advantageous and disadvantageous inequality aversion can be taught through vicarious learning of others' preferences","abstract":"While enforcing egalitarian social norms is critical for human society, punishing social norm violators often incurs a cost to the self. This cost looms even larger when one can benefit from an unequal distribution of resources (i.e. advantageous inequity), as in receiving a higher salary than a colleague with the identical role. In the Ultimatum Game, a classic test bed for fairness norm enforcement, individuals rarely reject (punish) such unequal proposed divisions of resources because doing so entails a sacrifice of one's own benefit. Recent work has demonstrated that observing another's punitive responses to unfairness can efficiently alter the punitive preferences of an observer. It remains an open question, however, whether such contagion is powerful enough to impart advantageous inequity aversion to individuals. Using a variant of the Ultimatum Game in which participants are tasked with responding to fairness violations on behalf of another 'Teacher' - whose aversion to advantageous (versus disadvantageous) inequity was systematically manipulated-we probe whether individuals subsequently increase their punishment unfair after experiencing fairness violations on their own behalf. In two experiments, we found individuals can acquire aversion to advantageous inequity 'vicariously' through observing (and implementing) the Teacher's preferences. Computationally, these learning effects were best characterized by a model which learns the latent structure of the Teacher's preferences, rather than a simple Reinforcement Learning account. In summary, our study is the first to demonstrate that people can swiftly and readily acquire another's preferences for advantageous inequity, suggesting in turn that behavioral contagion may be one promising mechanism through which social norm enforcement - which people rarely implement in the case of advantageous inequality - can be enhanced.","sentences":["While enforcing egalitarian social norms is critical for human society, punishing social norm violators often incurs a cost to the self.","This cost looms even larger when one can benefit from an unequal distribution of resources (i.e. advantageous inequity), as in receiving a higher salary than a colleague with the identical role.","In the Ultimatum Game, a classic test bed for fairness norm enforcement, individuals rarely reject (punish) such unequal proposed divisions of resources because doing so entails a sacrifice of one's own benefit.","Recent work has demonstrated that observing another's punitive responses to unfairness can efficiently alter the punitive preferences of an observer.","It remains an open question, however, whether such contagion is powerful enough to impart advantageous inequity aversion to individuals.","Using a variant of the Ultimatum Game in which participants are tasked with responding to fairness violations on behalf of another 'Teacher' - whose aversion to advantageous (versus disadvantageous) inequity was systematically manipulated-we probe whether individuals subsequently increase their punishment unfair after experiencing fairness violations on their own behalf.","In two experiments, we found individuals can acquire aversion to advantageous inequity 'vicariously' through observing (and implementing) the Teacher's preferences.","Computationally, these learning effects were best characterized by a model which learns the latent structure of the Teacher's preferences, rather than a simple Reinforcement Learning account.","In summary, our study is the first to demonstrate that people can swiftly and readily acquire another's preferences for advantageous inequity, suggesting in turn that behavioral contagion may be one promising mechanism through which social norm enforcement - which people rarely implement in the case of advantageous inequality - can be enhanced."],"url":"http://arxiv.org/abs/2405.06500v1","category":"q-bio.NC"}
{"created":"2024-05-10 13:57:13","title":"Incentive-compatible Bandits: Importance Weighting No More","abstract":"We study the problem of incentive-compatible online learning with bandit feedback. In this class of problems, the experts are self-interested agents who might misrepresent their preferences with the goal of being selected most often. The goal is to devise algorithms which are simultaneously incentive-compatible, that is the experts are incentivised to report their true preferences, and have no regret with respect to the preferences of the best fixed expert in hindsight. \\citet{freeman2020no} propose an algorithm in the full information setting with optimal $O(\\sqrt{T \\log(K)})$ regret and $O(T^{2/3}(K\\log(K))^{1/3})$ regret in the bandit setting.   In this work we propose the first incentive-compatible algorithms that enjoy $O(\\sqrt{KT})$ regret bounds. We further demonstrate how simple loss-biasing allows the algorithm proposed in Freeman et al. 2020 to enjoy $\\tilde O(\\sqrt{KT})$ regret. As a byproduct of our approach we obtain the first bandit algorithm with nearly optimal regret bounds in the adversarial setting which works entirely on the observed loss sequence without the need for importance-weighted estimators. Finally, we provide an incentive-compatible algorithm that enjoys asymptotically optimal best-of-both-worlds regret guarantees, i.e., logarithmic regret in the stochastic regime as well as worst-case $O(\\sqrt{KT})$ regret.","sentences":["We study the problem of incentive-compatible online learning with bandit feedback.","In this class of problems, the experts are self-interested agents who might misrepresent their preferences with the goal of being selected most often.","The goal is to devise algorithms which are simultaneously incentive-compatible, that is the experts are incentivised to report their true preferences, and have no regret with respect to the preferences of the best fixed expert in hindsight.","\\citet{freeman2020no} propose an algorithm in the full information setting with optimal $O(\\sqrt{T \\log(K)})$ regret and $O(T^{2/3}(K\\log(K))^{1/3})$ regret in the bandit setting.   ","In this work we propose the first incentive-compatible algorithms that enjoy $O(\\sqrt{KT})$ regret bounds.","We further demonstrate how simple loss-biasing allows the algorithm proposed in Freeman et al. 2020 to enjoy $\\tilde O(\\sqrt{KT})$ regret.","As a byproduct of our approach we obtain the first bandit algorithm with nearly optimal regret bounds in the adversarial setting which works entirely on the observed loss sequence without the need for importance-weighted estimators.","Finally, we provide an incentive-compatible algorithm that enjoys asymptotically optimal best-of-both-worlds regret guarantees, i.e., logarithmic regret in the stochastic regime as well as worst-case $O(\\sqrt{KT})$ regret."],"url":"http://arxiv.org/abs/2405.06480v1","category":"cs.LG"}
{"created":"2024-05-10 13:25:39","title":"Attend, Distill, Detect: Attention-aware Entropy Distillation for Anomaly Detection","abstract":"Unsupervised anomaly detection encompasses diverse applications in industrial settings where a high-throughput and precision is imperative. Early works were centered around one-class-one-model paradigm, which poses significant challenges in large-scale production environments. Knowledge-distillation based multi-class anomaly detection promises a low latency with a reasonably good performance but with a significant drop as compared to one-class version. We propose a DCAM (Distributed Convolutional Attention Module) which improves the distillation process between teacher and student networks when there is a high variance among multiple classes or objects. Integrated multi-scale feature matching strategy to utilise a mixture of multi-level knowledge from the feature pyramid of the two networks, intuitively helping in detecting anomalies of varying sizes which is also an inherent problem in the multi-class scenario. Briefly, our DCAM module consists of Convolutional Attention blocks distributed across the feature maps of the student network, which essentially learns to masks the irrelevant information during student learning alleviating the \"cross-class interference\" problem. This process is accompanied by minimizing the relative entropy using KL-Divergence in Spatial dimension and a Channel-wise Cosine Similarity between the same feature maps of teacher and student. The losses enables to achieve scale-invariance and capture non-linear relationships. We also highlight that the DCAM module would only be used during training and not during inference as we only need the learned feature maps and losses for anomaly scoring and hence, gaining a performance gain of 3.92% than the multi-class baseline with a preserved latency.","sentences":["Unsupervised anomaly detection encompasses diverse applications in industrial settings where a high-throughput and precision is imperative.","Early works were centered around one-class-one-model paradigm, which poses significant challenges in large-scale production environments.","Knowledge-distillation based multi-class anomaly detection promises a low latency with a reasonably good performance but with a significant drop as compared to one-class version.","We propose a DCAM (Distributed Convolutional Attention Module) which improves the distillation process between teacher and student networks when there is a high variance among multiple classes or objects.","Integrated multi-scale feature matching strategy to utilise a mixture of multi-level knowledge from the feature pyramid of the two networks, intuitively helping in detecting anomalies of varying sizes which is also an inherent problem in the multi-class scenario.","Briefly, our DCAM module consists of Convolutional Attention blocks distributed across the feature maps of the student network, which essentially learns to masks the irrelevant information during student learning alleviating the \"cross-class interference\" problem.","This process is accompanied by minimizing the relative entropy using KL-Divergence in Spatial dimension and a Channel-wise Cosine Similarity between the same feature maps of teacher and student.","The losses enables to achieve scale-invariance and capture non-linear relationships.","We also highlight that the DCAM module would only be used during training and not during inference as we only need the learned feature maps and losses for anomaly scoring and hence, gaining a performance gain of 3.92% than the multi-class baseline with a preserved latency."],"url":"http://arxiv.org/abs/2405.06467v1","category":"cs.CV"}
{"created":"2024-05-10 11:35:19","title":"Inferring Skin-Brain-Skin Connections from Infodemiology Data using Dynamic Bayesian Networks","abstract":"The relationship between skin diseases and mental illnesses has been extensively studied using cross-sectional epidemiological data. Typically, such data can only measure association (rather than causation) and include only a subset of the diseases we may be interested in. In this paper, we complement the evidence from such analyses by learning an overarching causal network model over twelve health conditions from the Google Search Trends Symptoms public data set.   We learned the causal network model using a dynamic Bayesian network, which can represent both cyclic and acyclic causal relationships, is easy to interpret and accounts for the spatio-temporal trends in the data in a probabilistically rigorous way. The causal network confirms a large number of cyclic relationships between the selected health conditions and the interplay between skin and mental diseases. For acne, we observe a cyclic relationship with anxiety and attention deficit hyperactivity disorder (ADHD) and an indirect relationship with depression through sleep disorders. For dermatitis, we observe directed links to anxiety, depression and sleep disorders and a cyclic relationship with ADHD. We also observe a link between dermatitis and ADHD and a cyclic relationship between acne and ADHD. Furthermore, the network includes several direct connections between sleep disorders and other health conditions, highlighting the impact of the former on the overall health and well-being of the patient.   Mapping disease interplay, indirect relationships, and the key role of mediators, such as sleep disorders, will allow healthcare professionals to address disease management holistically and more effectively. Even if we consider all skin and mental diseases jointly, each disease subnetwork is unique, allowing for more targeted interventions.","sentences":["The relationship between skin diseases and mental illnesses has been extensively studied using cross-sectional epidemiological data.","Typically, such data can only measure association (rather than causation) and include only a subset of the diseases we may be interested in.","In this paper, we complement the evidence from such analyses by learning an overarching causal network model over twelve health conditions from the Google Search Trends Symptoms public data set.   ","We learned the causal network model using a dynamic Bayesian network, which can represent both cyclic and acyclic causal relationships, is easy to interpret and accounts for the spatio-temporal trends in the data in a probabilistically rigorous way.","The causal network confirms a large number of cyclic relationships between the selected health conditions and the interplay between skin and mental diseases.","For acne, we observe a cyclic relationship with anxiety and attention deficit hyperactivity disorder (ADHD) and an indirect relationship with depression through sleep disorders.","For dermatitis, we observe directed links to anxiety, depression and sleep disorders and a cyclic relationship with ADHD.","We also observe a link between dermatitis and ADHD and a cyclic relationship between acne and ADHD.","Furthermore, the network includes several direct connections between sleep disorders and other health conditions, highlighting the impact of the former on the overall health and well-being of the patient.   ","Mapping disease interplay, indirect relationships, and the key role of mediators, such as sleep disorders, will allow healthcare professionals to address disease management holistically and more effectively.","Even if we consider all skin and mental diseases jointly, each disease subnetwork is unique, allowing for more targeted interventions."],"url":"http://arxiv.org/abs/2405.06405v1","category":"stat.AP"}
{"created":"2024-05-10 09:56:02","title":"Certified $\\ell_2$ Attribution Robustness via Uniformly Smoothed Attributions","abstract":"Model attribution is a popular tool to explain the rationales behind model predictions. However, recent work suggests that the attributions are vulnerable to minute perturbations, which can be added to input samples to fool the attributions while maintaining the prediction outputs. Although empirical studies have shown positive performance via adversarial training, an effective certified defense method is eminently needed to understand the robustness of attributions. In this work, we propose to use uniform smoothing technique that augments the vanilla attributions by noises uniformly sampled from a certain space. It is proved that, for all perturbations within the attack region, the cosine similarity between uniformly smoothed attribution of perturbed sample and the unperturbed sample is guaranteed to be lower bounded. We also derive alternative formulations of the certification that is equivalent to the original one and provides the maximum size of perturbation or the minimum smoothing radius such that the attribution can not be perturbed. We evaluate the proposed method on three datasets and show that the proposed method can effectively protect the attributions from attacks, regardless of the architecture of networks, training schemes and the size of the datasets.","sentences":["Model attribution is a popular tool to explain the rationales behind model predictions.","However, recent work suggests that the attributions are vulnerable to minute perturbations, which can be added to input samples to fool the attributions while maintaining the prediction outputs.","Although empirical studies have shown positive performance via adversarial training, an effective certified defense method is eminently needed to understand the robustness of attributions.","In this work, we propose to use uniform smoothing technique that augments the vanilla attributions by noises uniformly sampled from a certain space.","It is proved that, for all perturbations within the attack region, the cosine similarity between uniformly smoothed attribution of perturbed sample and the unperturbed sample is guaranteed to be lower bounded.","We also derive alternative formulations of the certification that is equivalent to the original one and provides the maximum size of perturbation or the minimum smoothing radius such that the attribution can not be perturbed.","We evaluate the proposed method on three datasets and show that the proposed method can effectively protect the attributions from attacks, regardless of the architecture of networks, training schemes and the size of the datasets."],"url":"http://arxiv.org/abs/2405.06361v1","category":"cs.LG"}
{"created":"2024-05-10 15:24:10","title":"Automatic Structural Search of Tensor Network States including Entanglement Renormalization","abstract":"Tensor network (TN) states, including entanglement renormalization (ER), can encompass a wider variety of entangled states. When the entanglement structure of the quantum state of interest is non-uniform in real space, accurately representing the state with a limited number of degrees of freedom hinges on appropriately configuring the TN to align with the entanglement pattern. However, a proposal has yet to show a structural search of ER due to its high computational cost and the lack of flexibility in its algorithm. In this study, we conducted an optimal structural search of TN, including ER, based on the reconstruction of their local structures with respect to variational energy. Firstly, we demonstrated that our algorithm for the spin-$1/2$ tetramer singlets model could calculate exact ground energy using the multi-scale entanglement renormalization ansatz (MERA) structure as an initial TN structure. Subsequently, we applied our algorithm to the random XY models with the two initial structures: MERA and the suitable structure underlying the strong disordered renormalization group. We found that, in both cases, our algorithm achieves improvements in variational energy, fidelity, and entanglement entropy. The degree of improvement in these quantities is superior in the latter case compared to the former, suggesting that utilizing an existing TN design method as a preprocessing step is important for maximizing our algorithm's performance.","sentences":["Tensor network (TN) states, including entanglement renormalization (ER), can encompass a wider variety of entangled states.","When the entanglement structure of the quantum state of interest is non-uniform in real space, accurately representing the state with a limited number of degrees of freedom hinges on appropriately configuring the TN to align with the entanglement pattern.","However, a proposal has yet to show a structural search of ER due to its high computational cost and the lack of flexibility in its algorithm.","In this study, we conducted an optimal structural search of TN, including ER, based on the reconstruction of their local structures with respect to variational energy.","Firstly, we demonstrated that our algorithm for the spin-$1/2$ tetramer singlets model could calculate exact ground energy using the multi-scale entanglement renormalization ansatz (MERA) structure as an initial TN structure.","Subsequently, we applied our algorithm to the random XY models with the two initial structures: MERA and the suitable structure underlying the strong disordered renormalization group.","We found that, in both cases, our algorithm achieves improvements in variational energy, fidelity, and entanglement entropy.","The degree of improvement in these quantities is superior in the latter case compared to the former, suggesting that utilizing an existing TN design method as a preprocessing step is important for maximizing our algorithm's performance."],"url":"http://arxiv.org/abs/2405.06534v1","category":"quant-ph"}
{"created":"2024-05-10 15:19:19","title":"A Distributionally Robust Approach to Shannon Limits using the Wasserstein Distance","abstract":"We consider the rate-distortion function for lossy source compression, as well as the channel capacity for error correction, through the lens of distributional robustness. We assume that the distribution of the source or of the additive channel noise is unknown and lies within a Wasserstein-2 ambiguity set of a given radius centered around a specified nominal distribution, and we look for the worst-case asymptotically optimal coding rate over such an ambiguity set. Varying the radius of the ambiguity set allows us to interpolate between the worst-case and stochastic scenarios using probabilistic tools. Our problem setting fits into the paradigm of compound source / channel models introduced by Sakrison and Blackwell, respectively. This paper shows that if the nominal distribution is Gaussian, then so is the worst-case source / noise distribution, and the compound rate-distortion / channel capacity functions admit convex formulations with Linear Matrix Inequality (LMI) constraints. These formulations yield simple closed-form expressions in the scalar case, offering insights into the behavior of Shannon limits with the changing radius of the Wasserstein-2 ambiguity set.","sentences":["We consider the rate-distortion function for lossy source compression, as well as the channel capacity for error correction, through the lens of distributional robustness.","We assume that the distribution of the source or of the additive channel noise is unknown and lies within a Wasserstein-2 ambiguity set of a given radius centered around a specified nominal distribution, and we look for the worst-case asymptotically optimal coding rate over such an ambiguity set.","Varying the radius of the ambiguity set allows us to interpolate between the worst-case and stochastic scenarios using probabilistic tools.","Our problem setting fits into the paradigm of compound source / channel models introduced by Sakrison and Blackwell, respectively.","This paper shows that if the nominal distribution is Gaussian, then so is the worst-case source / noise distribution, and the compound rate-distortion / channel capacity functions admit convex formulations with Linear Matrix Inequality (LMI) constraints.","These formulations yield simple closed-form expressions in the scalar case, offering insights into the behavior of Shannon limits with the changing radius of the Wasserstein-2 ambiguity set."],"url":"http://arxiv.org/abs/2405.06528v1","category":"cs.IT"}
{"created":"2024-05-10 11:20:13","title":"Statistical divergences in high-dimensional hypothesis testing and a modern technique for estimating them","abstract":"Hypothesis testing in high dimensional data is a notoriously difficult problem without direct access to competing models' likelihood functions. This paper argues that statistical divergences can be used to quantify the difference between the population distributions of observed data and competing models, justifying their use as the basis of a hypothesis test. We go on to point out how modern techniques for functional optimization let us estimate many divergences, without the need for population likelihood functions, using samples from two distributions alone. We use a physics-based example to show how the proposed two-sample test can be implemented in practice, and discuss the necessary steps required to mature the ideas presented into an experimental framework.","sentences":["Hypothesis testing in high dimensional data is a notoriously difficult problem without direct access to competing models' likelihood functions.","This paper argues that statistical divergences can be used to quantify the difference between the population distributions of observed data and competing models, justifying their use as the basis of a hypothesis test.","We go on to point out how modern techniques for functional optimization let us estimate many divergences, without the need for population likelihood functions, using samples from two distributions alone.","We use a physics-based example to show how the proposed two-sample test can be implemented in practice, and discuss the necessary steps required to mature the ideas presented into an experimental framework."],"url":"http://arxiv.org/abs/2405.06397v1","category":"physics.data-an"}
{"created":"2024-05-10 10:29:23","title":"Entropic Bounds on the Average Length of Codes with a Space","abstract":"We consider the problem of constructing prefix-free codes in which a designated symbol, a space, can only appear at the end of codewords. We provide a linear-time algorithm to construct almost-optimal codes with this property, meaning that their average length differs from the minimum possible by at most one. We obtain our results by uncovering a relation between our class of codes and the class of one-to-one codes. Additionally, we derive upper and lower bounds to the average length of optimal prefix-free codes with a space in terms of the source entropy.","sentences":["We consider the problem of constructing prefix-free codes in which a designated symbol, a space, can only appear at the end of codewords.","We provide a linear-time algorithm to construct almost-optimal codes with this property, meaning that their average length differs from the minimum possible by at most one.","We obtain our results by uncovering a relation between our class of codes and the class of one-to-one codes.","Additionally, we derive upper and lower bounds to the average length of optimal prefix-free codes with a space in terms of the source entropy."],"url":"http://arxiv.org/abs/2405.06379v1","category":"cs.IT"}
{"created":"2024-05-10 09:18:17","title":"Compression-Realized Deep Structural Network for Video Quality Enhancement","abstract":"This paper focuses on the task of quality enhancement for compressed videos. Although deep network-based video restorers achieve impressive progress, most of the existing methods lack a structured design to optimally leverage the priors within compression codecs. Since the quality degradation of the video is primarily induced by the compression algorithm, a new paradigm is urgently needed for a more \"conscious\" process of quality enhancement. As a result, we propose the Compression-Realize Deep Structural Network (CRDS), introducing three inductive biases aligned with the three primary processes in the classic compression codec, merging the strengths of classical encoder architecture with deep network capabilities. Inspired by the residual extraction and domain transformation process in the codec, a pre-trained Latent Degradation Residual Auto-Encoder is proposed to transform video frames into a latent feature space, and the mutual neighborhood attention mechanism is integrated for precise motion estimation and residual extraction. Furthermore, drawing inspiration from the quantization noise distribution of the codec, CRDS proposes a novel Progressive Denoising framework with intermediate supervision that decomposes the quality enhancement into a series of simpler denoising sub-tasks. Experimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our approach surpasses state-of-the-art models.","sentences":["This paper focuses on the task of quality enhancement for compressed videos.","Although deep network-based video restorers achieve impressive progress, most of the existing methods lack a structured design to optimally leverage the priors within compression codecs.","Since the quality degradation of the video is primarily induced by the compression algorithm, a new paradigm is urgently needed for a more \"conscious\" process of quality enhancement.","As a result, we propose the Compression-Realize Deep Structural Network (CRDS), introducing three inductive biases aligned with the three primary processes in the classic compression codec, merging the strengths of classical encoder architecture with deep network capabilities.","Inspired by the residual extraction and domain transformation process in the codec, a pre-trained Latent Degradation Residual Auto-Encoder is proposed to transform video frames into a latent feature space, and the mutual neighborhood attention mechanism is integrated for precise motion estimation and residual extraction.","Furthermore, drawing inspiration from the quantization noise distribution of the codec, CRDS proposes a novel Progressive Denoising framework with intermediate supervision that decomposes the quality enhancement into a series of simpler denoising sub-tasks.","Experimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our approach surpasses state-of-the-art models."],"url":"http://arxiv.org/abs/2405.06342v1","category":"cs.CV"}
{"created":"2024-05-10 09:13:57","title":"Improving Transferable Targeted Adversarial Attack via Normalized Logit Calibration and Truncated Feature Mixing","abstract":"This paper aims to enhance the transferability of adversarial samples in targeted attacks, where attack success rates remain comparatively low. To achieve this objective, we propose two distinct techniques for improving the targeted transferability from the loss and feature aspects. First, in previous approaches, logit calibrations used in targeted attacks primarily focus on the logit margin between the targeted class and the untargeted classes among samples, neglecting the standard deviation of the logit. In contrast, we introduce a new normalized logit calibration method that jointly considers the logit margin and the standard deviation of logits. This approach effectively calibrates the logits, enhancing the targeted transferability. Second, previous studies have demonstrated that mixing the features of clean samples during optimization can significantly increase transferability. Building upon this, we further investigate a truncated feature mixing method to reduce the impact of the source training model, resulting in additional improvements. The truncated feature is determined by removing the Rank-1 feature associated with the largest singular value decomposed from the high-level convolutional layers of the clean sample. Extensive experiments conducted on the ImageNet-Compatible and CIFAR-10 datasets demonstrate the individual and mutual benefits of our proposed two components, which outperform the state-of-the-art methods by a large margin in black-box targeted attacks.","sentences":["This paper aims to enhance the transferability of adversarial samples in targeted attacks, where attack success rates remain comparatively low.","To achieve this objective, we propose two distinct techniques for improving the targeted transferability from the loss and feature aspects.","First, in previous approaches, logit calibrations used in targeted attacks primarily focus on the logit margin between the targeted class and the untargeted classes among samples, neglecting the standard deviation of the logit.","In contrast, we introduce a new normalized logit calibration method that jointly considers the logit margin and the standard deviation of logits.","This approach effectively calibrates the logits, enhancing the targeted transferability.","Second, previous studies have demonstrated that mixing the features of clean samples during optimization can significantly increase transferability.","Building upon this, we further investigate a truncated feature mixing method to reduce the impact of the source training model, resulting in additional improvements.","The truncated feature is determined by removing the Rank-1 feature associated with the largest singular value decomposed from the high-level convolutional layers of the clean sample.","Extensive experiments conducted on the ImageNet-Compatible and CIFAR-10 datasets demonstrate the individual and mutual benefits of our proposed two components, which outperform the state-of-the-art methods by a large margin in black-box targeted attacks."],"url":"http://arxiv.org/abs/2405.06340v1","category":"cs.CV"}
{"created":"2024-05-10 08:57:26","title":"Backward errors for multiple eigenpairs in structured and unstructured nonlinear eigenvalue problems","abstract":"Given a nonlinear matrix-valued function $F(\\lambda)$ and approximate eigenpairs $(\\lambda_i, v_i)$, we discuss how to determine the smallest perturbation $\\delta F$ such that $[F + \\delta F](\\lambda_i) v_i = 0$; we call the distance between the $F$ and $F + \\delta F$ the backward error for this set of approximate eigenpairs. We focus on the case where $F(\\lambda)$ is given as a linear combination of scalar functions multiplying matrix coefficients $F_i$, and the perturbation is done on the matrix coefficients. We provide inexpensive upper bounds, and a way to accurately compute the backward error by means of direct computations or through Riemannian optimization. We also discuss how the backward error can be determined when the $F_i$ have particular structures (such as symmetry, sparsity, or low-rank), and the perturbations are required to preserve them. For special cases (such as for symmetric coefficients), explicit and inexpensive formulas to compute the $\\delta F_i$ are also given.","sentences":["Given a nonlinear matrix-valued function $F(\\lambda)$ and approximate eigenpairs $(\\lambda_i, v_i)$, we discuss how to determine the smallest perturbation $\\delta F$ such that $[F + \\delta F](\\lambda_i) v_i","= 0$; we call the distance between the $F$ and $F + \\delta F$ the backward error for this set of approximate eigenpairs.","We focus on the case where $F(\\lambda)$ is given as a linear combination of scalar functions multiplying matrix coefficients $F_i$, and the perturbation is done on the matrix coefficients.","We provide inexpensive upper bounds, and a way to accurately compute the backward error by means of direct computations or through Riemannian optimization.","We also discuss how the backward error can be determined when the $F_i$ have particular structures (such as symmetry, sparsity, or low-rank), and the perturbations are required to preserve them.","For special cases (such as for symmetric coefficients), explicit and inexpensive formulas to compute the $\\delta F_i$ are also given."],"url":"http://arxiv.org/abs/2405.06327v1","category":"math.NA"}
{"created":"2024-05-10 17:58:24","title":"Assorted remarks on bending measures and energies for plates and shells, and their invariance properties","abstract":"In this note, we address several issues, including some raised in recent works and commentary, related to bending measures and energies for plates and shells, and certain of their invariance properties. We discuss the distinction between definitions and results in our and others' approaches, correct an error and citation oversights in our work, and provide additional brief observations regarding the relative size of energetic terms and the symmetrization of bending measures. Particular points of emphasis are a reiteration of some of the early history of dilation-invariant measures, the similarities between all such measures, and the non-dilation-invariance of our recently introduced bending measure for shells and curved rods. In the course of this discussion, we provide a simpler presentation of the elementary, but much overlooked, fact that the additional tangential stretch of material near the mid-surface of a thin body is the product of the mid-surface stretch and the change in curvature.","sentences":["In this note, we address several issues, including some raised in recent works and commentary, related to bending measures and energies for plates and shells, and certain of their invariance properties.","We discuss the distinction between definitions and results in our and others' approaches, correct an error and citation oversights in our work, and provide additional brief observations regarding the relative size of energetic terms and the symmetrization of bending measures.","Particular points of emphasis are a reiteration of some of the early history of dilation-invariant measures, the similarities between all such measures, and the non-dilation-invariance of our recently introduced bending measure for shells and curved rods.","In the course of this discussion, we provide a simpler presentation of the elementary, but much overlooked, fact that the additional tangential stretch of material near the mid-surface of a thin body is the product of the mid-surface stretch and the change in curvature."],"url":"http://arxiv.org/abs/2405.06638v1","category":"cond-mat.soft"}
{"created":"2024-05-10 17:48:16","title":"Nonlocal correlations transmitted between quantum dots via short topological superconductor","abstract":"We study the quasiparticle states and nonlocal correlations of a hybrid structure, comprising two quantum dots interconnected through a short-length topological superconducting nanowire hosting overlaping Majorana modes. We show that the hybridization between different components of this setup gives rise to the emergence of molecular states, which are responsible for nonlocal correlations. We inspect the resulting energy structure, focusing on the inter-dependence between the quasiparticles of individual quantum dots. We predict the existence of nonlocal effects, which could be accessed and probed by crossed Andreev reflection spectroscopy. Our study would be relevant to a recent experimental realization of the minimal Kitaev model [T. Dvir et al., Nature 614, 445 (2023)], by considering its hybrid structure with side-attached quantum dots.","sentences":["We study the quasiparticle states and nonlocal correlations of a hybrid structure, comprising two quantum dots interconnected through a short-length topological superconducting nanowire hosting overlaping Majorana modes.","We show that the hybridization between different components of this setup gives rise to the emergence of molecular states, which are responsible for nonlocal correlations.","We inspect the resulting energy structure, focusing on the inter-dependence between the quasiparticles of individual quantum dots.","We predict the existence of nonlocal effects, which could be accessed and probed by crossed Andreev reflection spectroscopy.","Our study would be relevant to a recent experimental realization of the minimal Kitaev model [T. Dvir et al., Nature 614, 445 (2023)], by considering its hybrid structure with side-attached quantum dots."],"url":"http://arxiv.org/abs/2405.06630v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-10 17:28:20","title":"Fast Mixing in Sparse Random Ising Models","abstract":"Motivated by the community detection problem in Bayesian inference, as well as the recent explosion of interest in spin glasses from statistical physics, we study the classical Glauber dynamics for sampling from Ising models with sparse random interactions. It is now well-known that when the interaction matrix has spectral diameter less than $1$, Glauber dynamics mixes in $O(n\\log n)$ steps. Unfortunately, such criteria fail dramatically for interactions supported on arguably the most well-studied sparse random graph: the Erd\\H{o}s--R\\'{e}nyi random graph $G(n,d/n)$, due to the presence of almost linearly many outlier eigenvalues of unbounded magnitude.   We prove that for the \\emph{Viana--Bray spin glass}, where the interactions are supported on $G(n,d/n)$ and randomly assigned $\\pm\\beta$, Glauber dynamics mixes in $n^{1+o(1)}$ time with high probability as long as $\\beta \\le O(1/\\sqrt{d})$, independent of $n$. We further extend our results to random graphs drawn according to the $2$-community stochastic block model, as well as when the interactions are given by a \"centered\" version of the adjacency matrix. The latter setting is particularly relevant for the inference problem in community detection. Indeed, we build on this result to demonstrate that Glauber dynamics succeeds at recovering communities in the stochastic block model in an upcoming paper.   The primary technical ingredient in our proof is showing that with high probability, a sparse random graph can be decomposed into two parts --- a \\emph{bulk} which behaves like a graph with bounded maximum degree and a well-behaved spectrum, and a \\emph{near-forest} with favorable pseudorandom properties. We then use this decomposition to design a localization procedure that interpolates to simpler Ising models supported only on the near-forest, and then execute a pathwise analysis to establish a modified log-Sobolev inequality.","sentences":["Motivated by the community detection problem in Bayesian inference, as well as the recent explosion of interest in spin glasses from statistical physics, we study the classical Glauber dynamics for sampling from Ising models with sparse random interactions.","It is now well-known that when the interaction matrix has spectral diameter less than $1$, Glauber dynamics mixes in $O(n\\log n)$ steps.","Unfortunately, such criteria fail dramatically for interactions supported on arguably the most well-studied sparse random graph: the Erd\\H{o}s--R\\'{e}nyi random graph $G(n,d/n)$, due to the presence of almost linearly many outlier eigenvalues of unbounded magnitude.   ","We prove that for the \\emph{Viana--Bray spin glass}, where the interactions are supported on $G(n,d/n)$ and randomly assigned $\\pm\\beta$, Glauber dynamics mixes in $n^{1+o(1)}$ time with high probability as long as $\\beta \\le O(1/\\sqrt{d})$, independent of $n$. We further extend our results to random graphs drawn according to the $2$-community stochastic block model, as well as when the interactions are given by a \"centered\" version of the adjacency matrix.","The latter setting is particularly relevant for the inference problem in community detection.","Indeed, we build on this result to demonstrate that Glauber dynamics succeeds at recovering communities in the stochastic block model in an upcoming paper.   ","The primary technical ingredient in our proof is showing that with high probability, a sparse random graph can be decomposed into two parts --- a \\emph{bulk} which behaves like a graph with bounded maximum degree and a well-behaved spectrum, and a \\emph{near-forest} with favorable pseudorandom properties.","We then use this decomposition to design a localization procedure that interpolates to simpler Ising models supported only on the near-forest, and then execute a pathwise analysis to establish a modified log-Sobolev inequality."],"url":"http://arxiv.org/abs/2405.06616v1","category":"math.PR"}
{"created":"2024-05-10 17:13:24","title":"SO(5) multicriticality in two-dimensional quantum magnets","abstract":"We resolve the nature of the quantum phase transition between a N\\'eel antiferromagnet and a valence-bond solid in two-dimensional spin-1/2 magnets. We study a class of $J$-$Q$ models, in which Heisenberg exchange $J$ competes with interactions $Q_n$ formed by products of $n$ singlet projectors on adjacent parallel lattice links. QMC simulations provide unambiguous evidence for first-order transitions, with the discontinuities increasing with $n$. For $n=2$ and $n=3$ models, the first-order signatures are very weak. On intermediate length scales, we extract well-defined scaling dimensions (critical exponents) that are common to the models with small $n$, indicating proximity to a quantum critical point. By combining two $Q$ terms, the transition can be tuned from weak to more strongly first-order. The two coexisting orders on the first-order line scale with a large exponent $\\beta \\approx 0.85$. This exponent and others are close to bounds for an SO($5$) symmetric CFT with a relevant SO($5$) singlet. We characterize the emergent SO($5$) symmetry by the scaling dimensions of its leading irrelevant perturbations. The large $\\beta$ value and a large correlation length exponent, $\\nu \\approx 1.4$, partially explain why the transition remains near-critical even quite far away from the critical point and in many different models without fine-tuning. In addition, we find that few-spin lattice operators are dominated by the SO($5$) violating field (the traceless symmetric tensor), and interactions involving many spins are required to observe strong effects of the relevant SO($5$) singlet. The exponent that had previously been identified with the divergent correlation length when crossing between the two phases does not have a corresponding CFT operator. We explain this emergent pseudocritical scale by a mechanism relying on a dangerously irrelevant SO($5$) perturbation.","sentences":["We resolve the nature of the quantum phase transition between a N\\'eel antiferromagnet and a valence-bond solid in two-dimensional spin-1/2 magnets.","We study a class of $J$-$Q$ models, in which Heisenberg exchange $J$ competes with interactions $Q_n$ formed by products of $n$ singlet projectors on adjacent parallel lattice links.","QMC simulations provide unambiguous evidence for first-order transitions, with the discontinuities increasing with $n$. For $n=2$ and $n=3$ models, the first-order signatures are very weak.","On intermediate length scales, we extract well-defined scaling dimensions (critical exponents) that are common to the models with small $n$, indicating proximity to a quantum critical point.","By combining two $Q$ terms, the transition can be tuned from weak to more strongly first-order.","The two coexisting orders on the first-order line scale with a large exponent $\\beta \\approx 0.85$.","This exponent and others are close to bounds for an SO($5$) symmetric CFT with a relevant SO($5$) singlet.","We characterize the emergent SO($5$) symmetry by the scaling dimensions of its leading irrelevant perturbations.","The large $\\beta$ value and a large correlation length exponent, $\\nu \\approx 1.4$, partially explain why the transition remains near-critical even quite far away from the critical point and in many different models without fine-tuning.","In addition, we find that few-spin lattice operators are dominated by the SO($5$) violating field (the traceless symmetric tensor), and interactions involving many spins are required to observe strong effects of the relevant SO($5$) singlet.","The exponent that had previously been identified with the divergent correlation length when crossing between the two phases does not have a corresponding CFT operator.","We explain this emergent pseudocritical scale by a mechanism relying on a dangerously irrelevant SO($5$) perturbation."],"url":"http://arxiv.org/abs/2405.06607v1","category":"cond-mat.str-el"}
{"created":"2024-05-10 17:06:56","title":"Advancing Precision Particle Background Estimation for Future X-ray Missions: Correlated Variability between AMS and Chandra/XMM-Newton","abstract":"Galactic cosmic ray (GCR) particles have a significant impact on the particle-induced background of X-ray observatories, and their flux exhibits substantial temporal variability, potentially influencing background levels. In this study, we present one-day binned high-energy reject rates derived from the Chandra-ACIS and XMM-Newton EPIC-pn instruments, serving as proxies for GCR particle flux. We systematically analyze the ACIS and EPIC-pn reject rates and compare them with the AMS proton flux. Our analysis initially reveals robust correlations between the AMS proton flux and the ACIS/EPIC-pn reject rates when binned over 27-day intervals. However, a closer examination reveals substantial fluctuations within each 27-day bin, indicating shorter-term variability. Upon daily binning, we observe finer. temporal structures in the datasets, demonstrating the presence of recurrent variations with periods of $\\sim$ 25 days and 23 days in ACIS and EPIC-pn reject rates, respectively, spanning the years 2014 to 2018. Notably, during the 2016--2017 period, we additionally detect periodicities of $\\sim$13.5 days and 9 days in the ACIS and EPIC-pn reject rates, respectively. Intriguingly, we observe a time lag of $\\sim$ 6 days between the AMS proton flux and the ACIS/EPIC-pn reject rates during the second half of 2016. This time lag is not visible before 2016 and aftern2017. The underlying physical mechanisms responsible for this time lag remain a subject of ongoing investigation.","sentences":["Galactic cosmic ray (GCR) particles have a significant impact on the particle-induced background of X-ray observatories, and their flux exhibits substantial temporal variability, potentially influencing background levels.","In this study, we present one-day binned high-energy reject rates derived from the Chandra-ACIS and XMM-Newton EPIC-pn instruments, serving as proxies for GCR particle flux.","We systematically analyze the ACIS and EPIC-pn reject rates and compare them with the AMS proton flux.","Our analysis initially reveals robust correlations between the AMS proton flux and the ACIS/EPIC-pn reject rates when binned over 27-day intervals.","However, a closer examination reveals substantial fluctuations within each 27-day bin, indicating shorter-term variability.","Upon daily binning, we observe finer.","temporal structures in the datasets, demonstrating the presence of recurrent variations with periods of $\\sim$ 25 days and 23 days in ACIS and EPIC-pn reject rates, respectively, spanning the years 2014 to 2018.","Notably, during the 2016--2017 period, we additionally detect periodicities of $\\sim$13.5 days and 9 days in the ACIS and EPIC-pn reject rates, respectively.","Intriguingly, we observe a time lag of $\\sim$ 6 days between the AMS proton flux and the ACIS/EPIC-pn reject rates during the second half of 2016.","This time lag is not visible before 2016 and aftern2017.","The underlying physical mechanisms responsible for this time lag remain a subject of ongoing investigation."],"url":"http://arxiv.org/abs/2405.06602v1","category":"astro-ph.HE"}
{"created":"2024-05-10 16:53:43","title":"The Non-Invertible Axial Symmetry in QED Comes Full Circle","abstract":"We revisit the possibility of constructing non-invertible topological defects for the axial symmetry of massless QED, despite its ABJ anomaly. Dressing the defects with a topological quantum field theory with mixed $U(1)$ and $\\mathbb{R}$-valued gauge fields, we are able to describe axial rotations of any rational or irrational angle. We confront our results with the existing proposals, in particular those that concern rational angles. We also provide the Symmetry TFT that reproduces the action of all such symmetry defects of QED. Finally, we discuss how similar techniques allow the study of condensation defects for a $\\mathbb{R}$ global symmetry.","sentences":["We revisit the possibility of constructing non-invertible topological defects for the axial symmetry of massless QED, despite its ABJ anomaly.","Dressing the defects with a topological quantum field theory with mixed $U(1)$ and $\\mathbb{R}$-valued gauge fields, we are able to describe axial rotations of any rational or irrational angle.","We confront our results with the existing proposals, in particular those that concern rational angles.","We also provide the Symmetry TFT that reproduces the action of all such symmetry defects of QED.","Finally, we discuss how similar techniques allow the study of condensation defects for a $\\mathbb{R}$ global symmetry."],"url":"http://arxiv.org/abs/2405.06596v1","category":"hep-th"}
{"created":"2024-05-10 16:52:20","title":"Atomic Quantum Technologies for Quantum Matter and Fundamental Physics Applications","abstract":"Physics is living an era of unprecedented cross-fertilization among the different areas of science. In this perspective review, we discuss the manifold impact that ultracold-atom quantum technologies can have in fundamental and applied science through platforms for quantum simulation, computation, metrology and sensing. We illustrate how the engineering of table-top experiments with atom technologies is engendering applications to understand problems in condensed matter and fundamental physics, cosmology and astrophysics, foundational aspects of quantum mechanics, quantum chemistry and the emerging field of quantum biology. We take the perspective of two main approaches, i.e. creating quantum analogues and building quantum simulators, highlighting that independently of the ultimate goal of a universal quantum computer to be met, the remarkable transformative effects of these achievements remain unchanged. We convey three main messages. First, atomic quantum technologies have enabled a new way in which quantum technologies are used for fundamental science, even beyond the advancement of knowledge, which is characterised by truly cross-disciplinary research, extended interplay between theoretical and experimental thinking, and intersectoral approach. Second, quantum many-body physics is taking the center stage in frontier's science. Third, quantum science progress will have capillary impact on society. Thus, the adoption of a responsible research and innovation approach to quantum technologies is mandatory, to accompany citizens in building awareness and future scaffolding. Following on all these reflections, this perspective review is aimed at scientists active or interested in interdisciplinary research, providing the reader with an overview of the current status of these wide fields of research where ultracold-atomic platforms play a vital role in their description and simulation.","sentences":["Physics is living an era of unprecedented cross-fertilization among the different areas of science.","In this perspective review, we discuss the manifold impact that ultracold-atom quantum technologies can have in fundamental and applied science through platforms for quantum simulation, computation, metrology and sensing.","We illustrate how the engineering of table-top experiments with atom technologies is engendering applications to understand problems in condensed matter and fundamental physics, cosmology and astrophysics, foundational aspects of quantum mechanics, quantum chemistry and the emerging field of quantum biology.","We take the perspective of two main approaches, i.e. creating quantum analogues and building quantum simulators, highlighting that independently of the ultimate goal of a universal quantum computer to be met, the remarkable transformative effects of these achievements remain unchanged.","We convey three main messages.","First, atomic quantum technologies have enabled a new way in which quantum technologies are used for fundamental science, even beyond the advancement of knowledge, which is characterised by truly cross-disciplinary research, extended interplay between theoretical and experimental thinking, and intersectoral approach.","Second, quantum many-body physics is taking the center stage in frontier's science.","Third, quantum science progress will have capillary impact on society.","Thus, the adoption of a responsible research and innovation approach to quantum technologies is mandatory, to accompany citizens in building awareness and future scaffolding.","Following on all these reflections, this perspective review is aimed at scientists active or interested in interdisciplinary research, providing the reader with an overview of the current status of these wide fields of research where ultracold-atomic platforms play a vital role in their description and simulation."],"url":"http://arxiv.org/abs/2405.06595v1","category":"quant-ph"}
{"created":"2024-05-10 16:48:09","title":"Density and inertia effects on two-dimensional active semiflexible filament suspensions","abstract":"We examine the influence of density on the transition between chain and spiral structures in planar assemblies of active semiflexible filaments, utilizing detailed numerical simulations. We focus on how increased density, and higher P\\'eclet numbers, affect the activity-induced transition spiral state in a semiflexible, self-avoiding active chain. Our findings show that increasing the density causes the spiral state to break up, reverting to a motile chain-like shape. This results in a density-dependent reentrant phase transition from spirals back to open chains. We attribute this phenomenon to an inertial effect observed at the single polymer level, where increased persistence length due to inertia has been shown in recent three-dimensional studies to cause polymers to open up. Our two-dimensional simulations further reveal that a reduction in the damping coefficient leads to partial unwinding of the spirals, forming longer arms. In suspension, interactions among these extended arms can trigger a complete unwinding of the spirals, driven by the combined effects of density and inertia.","sentences":["We examine the influence of density on the transition between chain and spiral structures in planar assemblies of active semiflexible filaments, utilizing detailed numerical simulations.","We focus on how increased density, and higher P\\'eclet numbers, affect the activity-induced transition spiral state in a semiflexible, self-avoiding active chain.","Our findings show that increasing the density causes the spiral state to break up, reverting to a motile chain-like shape.","This results in a density-dependent reentrant phase transition from spirals back to open chains.","We attribute this phenomenon to an inertial effect observed at the single polymer level, where increased persistence length due to inertia has been shown in recent three-dimensional studies to cause polymers to open up.","Our two-dimensional simulations further reveal that a reduction in the damping coefficient leads to partial unwinding of the spirals, forming longer arms.","In suspension, interactions among these extended arms can trigger a complete unwinding of the spirals, driven by the combined effects of density and inertia."],"url":"http://arxiv.org/abs/2405.06592v1","category":"cond-mat.soft"}
{"created":"2024-05-10 16:39:35","title":"How often does a cubic hypersurface have a rational point?","abstract":"A cubic hypersurface in $\\mathbb{P}^n$ defined over $\\mathbb{Q}$ is given by the vanishing locus of a cubic form $f$ in $n+1$ variables. It is conjectured that when $n \\geq 4$, such cubic hypersurfaces satisfy the Hasse principle. This is now known to hold on average due to recent work of Browning, Le Boudec, and Sawin. Using this result, we determine the proportion of cubic hypersurfaces in $\\mathbb{P}^n$, ordered by the height of $f$, with a rational point for $n \\geq 4$ explicitly as a product over primes $p$ of rational functions in $p$. In particular, this proportion is equal to 1 for cubic hypersurfaces in $\\mathbb{P}^n$ for $n \\geq 9$; for $100\\%$ of cubic hypersurfaces, this recovers a celebrated result of Heath-Brown that non-singular cubic forms in at least 10 variables have rational zeros. In the $n=3$ case, we give a precise conjecture for the proportion of cubic surfaces in $\\mathbb{P}^3$ with a rational point.","sentences":["A cubic hypersurface in $\\mathbb{P}^n$ defined over $\\mathbb{Q}$ is given by the vanishing locus of a cubic form $f$ in $n+1$ variables.","It is conjectured that when $n \\geq 4$, such cubic hypersurfaces satisfy the Hasse principle.","This is now known to hold on average due to recent work of Browning, Le Boudec, and Sawin.","Using this result, we determine the proportion of cubic hypersurfaces in $\\mathbb{P}^n$, ordered by the height of $f$, with a rational point for $n \\geq 4$ explicitly as a product over primes $p$ of rational functions in $p$. In particular, this proportion is equal to 1 for cubic hypersurfaces in $\\mathbb{P}^n$ for $n \\geq 9$; for $100\\%$ of cubic hypersurfaces, this recovers a celebrated result of Heath-Brown that non-singular cubic forms in at least 10 variables have rational zeros.","In the $n=3$ case, we give a precise conjecture for the proportion of cubic surfaces in $\\mathbb{P}^3$ with a rational point."],"url":"http://arxiv.org/abs/2405.06584v1","category":"math.NT"}
{"created":"2024-05-10 16:10:39","title":"Charge (in)stability and superradiance of Topological Stars","abstract":"We study linear massive scalar charged perturbations of Topological Stars in the fuzzball and in the black hole (Black String) regimes. The objects that naturally couple to the electric 3-form field strength of these solutions are charged strings, wound around the compact direction. We explore the possibility of instabilities of these solutions, in analogy with the charge instability already highlighted for other non-BPS geometries like JMaRT. This issue is addressed by calculating quasi-normal mode frequencies with a variety of techniques: WKB approximation, direct integration, Leaver method and by exploiting the recently discovered correspondence between black hole-fuzzball perturbation theory and quantum Seiberg-Witten curves. All mode frequencies we find have negative imaginary parts, implying an exponential decay in time. This suggests a linear stability of Topological Stars also in this new scenario. In addition, we study the charge superradiance for the Black String. We compute the amplification factor with the numerical integration method and a quantum Seiberg-Witten motivated definition including instantonic corrections.","sentences":["We study linear massive scalar charged perturbations of Topological Stars in the fuzzball and in the black hole (Black String) regimes.","The objects that naturally couple to the electric 3-form field strength of these solutions are charged strings, wound around the compact direction.","We explore the possibility of instabilities of these solutions, in analogy with the charge instability already highlighted for other non-BPS geometries like JMaRT.","This issue is addressed by calculating quasi-normal mode frequencies with a variety of techniques: WKB approximation, direct integration, Leaver method and by exploiting the recently discovered correspondence between black hole-fuzzball perturbation theory and quantum Seiberg-Witten curves.","All mode frequencies we find have negative imaginary parts, implying an exponential decay in time.","This suggests a linear stability of Topological Stars also in this new scenario.","In addition, we study the charge superradiance for the Black String.","We compute the amplification factor with the numerical integration method and a quantum Seiberg-Witten motivated definition including instantonic corrections."],"url":"http://arxiv.org/abs/2405.06566v1","category":"hep-th"}
{"created":"2024-05-10 15:50:31","title":"Non-Relativistic Intersecting Branes, Newton-Cartan Geometry and AdS/CFT","abstract":"We discuss non-relativistic variants of four-dimensional ${\\cal N}$=4 super-Yang-Mills theory obtained from generalised Newton-Cartan geometric limits of D3-branes in ten-dimensional spacetime. We argue that the natural interpretation of these limits is that they correspond to non-relativistic D1-branes or D3-branes intersecting the original D3-branes. The resulting gauge theories have dynamics that reduce to quantum mechanics on monopole moduli space or two-dimensional sigma-models on Hitchin moduli space respectively. We show that these theories possess interesting infinite-dimensional symmetries and we discuss the dual $AdS$ geometries.","sentences":["We discuss non-relativistic variants of four-dimensional ${\\cal N}$=4 super-Yang-Mills theory obtained from generalised Newton-Cartan geometric limits of D3-branes in ten-dimensional spacetime.","We argue that the natural interpretation of these limits is that they correspond to non-relativistic D1-branes or D3-branes intersecting the original D3-branes.","The resulting gauge theories have dynamics that reduce to quantum mechanics on monopole moduli space or two-dimensional sigma-models on Hitchin moduli space respectively.","We show that these theories possess interesting infinite-dimensional symmetries and we discuss the dual $AdS$ geometries."],"url":"http://arxiv.org/abs/2405.06552v1","category":"hep-th"}
{"created":"2024-05-10 15:31:19","title":"A self-consistent current response theory of jamming and vibrational modes in low-temperature amorphous solids","abstract":"We study amorphous solids with strong elastic disorder and find an un-jamming instability that exists, inter alia, in an harmonic model built using Euclidean random matrices (ERM). Employing the Zwanzig-Mori projection operator formalism and Gaussian factorization approximations, we develop a first-principles, self-consistent theory of transverse momentum correlations in athermal disordered materials, extending beyond the standard Born approximation. The vibrational anomalies in glass at low temperatures are recovered in the stable solid limit, and floppy modes lacking restoring forces are predicted in unstable states below the jamming transition. Near the un-jamming transition, the speed of sound $v_0^\\perp$ vanishes with $ \\propto \\sqrt{\\epsilon}$, where $\\epsilon$ denotes the distance from the critical point. Additionally, the density of states develops a plateau, independent of $\\epsilon$ above a frequency $\\omega_*$ which vanishes at the transition, $\\omega_*\\propto |\\epsilon|$. We identify a characteristic length scale in the un-jammed phase, $\\lambda_-^\\perp\\propto1/\\sqrt{\\epsilon}$, indicating the distance over which injected momentum remains correlated. We confirm the theoretical predictions with numerical solutions of a scalar ERM model, demonstrating overall good qualitative and partly quantitative agreement.","sentences":["We study amorphous solids with strong elastic disorder and find an un-jamming instability that exists, inter alia, in an harmonic model built using Euclidean random matrices (ERM).","Employing the Zwanzig-Mori projection operator formalism and Gaussian factorization approximations, we develop a first-principles, self-consistent theory of transverse momentum correlations in athermal disordered materials, extending beyond the standard Born approximation.","The vibrational anomalies in glass at low temperatures are recovered in the stable solid limit, and floppy modes lacking restoring forces are predicted in unstable states below the jamming transition.","Near the un-jamming transition, the speed of sound $v_0^\\perp$ vanishes with $ \\propto \\sqrt{\\epsilon}$, where $\\epsilon$ denotes the distance from the critical point.","Additionally, the density of states develops a plateau, independent of $\\epsilon$ above a frequency $\\omega_*$ which vanishes at the transition, $\\omega_*\\propto |\\epsilon|$. We identify a characteristic length scale in the un-jammed phase, $\\lambda_-^\\perp\\propto1/\\sqrt{\\epsilon}$, indicating the distance over which injected momentum remains correlated.","We confirm the theoretical predictions with numerical solutions of a scalar ERM model, demonstrating overall good qualitative and partly quantitative agreement."],"url":"http://arxiv.org/abs/2405.06537v1","category":"cond-mat.soft"}
{"created":"2024-05-10 15:19:31","title":"Amplitude bounds of steady rotational water waves","abstract":"We consider classical steady water waves with a free surface, a flat bottom and constant vorticity $\\gamma$. In the adverse case $\\gamma>0$ we prove that there is an absolute upper bound on the amplitude, independent of the physical constants, provided that $\\gamma$ is sufficiently small. In any favorable case $\\gamma\\leq0$ we present a new proof of such an absolute bound on the amplitude and prove that the amplitude tends to zero as $\\gamma$ tends to negative infinity.","sentences":["We consider classical steady water waves with a free surface, a flat bottom and constant vorticity $\\gamma$.","In the adverse case $\\gamma>0$ we prove that there is an absolute upper bound on the amplitude, independent of the physical constants, provided that $\\gamma$ is sufficiently small.","In any favorable case $\\gamma\\leq0$ we present a new proof of such an absolute bound on the amplitude and prove that the amplitude tends to zero as $\\gamma$ tends to negative infinity."],"url":"http://arxiv.org/abs/2405.06529v1","category":"math.AP"}
{"created":"2024-05-10 15:19:10","title":"First observation of single photons in a CRESST detector and new dark matter exclusion limits","abstract":"The main goal of the CRESST-III experiment is the direct detection of dark matter particles via their scattering off target nuclei in cryogenic detectors. In this work we present the results of a Silicon-On-Sapphire (SOS) detector with a mass of 0.6$\\,$g and an energy threshold of (6.7$\\, \\pm \\,$0.2)$\\,$eV with a baseline energy resolution of (1.0$\\, \\pm \\,$0.2)$\\,$eV. This allowed for a calibration via the detection of single luminescence photons in the eV-range, which could be observed in CRESST for the first time. We present new exclusion limits on the spin-independent and spin-dependent dark matter-nucleon cross section that extend to dark matter particle masses of less than 100$\\,$MeV/c$^{2}$.","sentences":["The main goal of the CRESST-III experiment is the direct detection of dark matter particles via their scattering off target nuclei in cryogenic detectors.","In this work we present the results of a Silicon-On-Sapphire (SOS) detector with a mass of 0.6$\\,$g and an energy threshold of (6.7$\\, \\pm \\,$0.2)$\\,$eV with a baseline energy resolution of (1.0$\\, \\pm \\,$0.2)$\\,$eV.","This allowed for a calibration via the detection of single luminescence photons in the eV-range, which could be observed in CRESST for the first time.","We present new exclusion limits on the spin-independent and spin-dependent dark matter-nucleon cross section that extend to dark matter particle masses of less than 100$\\,$MeV/c$^{2}$."],"url":"http://arxiv.org/abs/2405.06527v1","category":"astro-ph.CO"}
{"created":"2024-05-10 14:37:26","title":"Scattering of dark pions in Sp(4) gauge theory","abstract":"Analyzes of astrophysical data provide first hints on the self-interactions of dark matter at low energies. Lattice calculations of dark matter theories can be used to investigate them, especially in the case of strongly-interacting dark matter. We consider Sp(4) gauge theory with two fundamental fermions as a candidate theory. We compute the scattering phase shift for the scattering of two identical dark pions and determine the parameters of the effective range expansion. Our exploratory results in the supposedly most common interaction channel provide a lower limit for the dark matter mass when compared to astrophysical data. We also provide first benchmarks of velocity-weighted cross-sections in the relevant non-relativistic domain.","sentences":["Analyzes of astrophysical data provide first hints on the self-interactions of dark matter at low energies.","Lattice calculations of dark matter theories can be used to investigate them, especially in the case of strongly-interacting dark matter.","We consider Sp(4) gauge theory with two fundamental fermions as a candidate theory.","We compute the scattering phase shift for the scattering of two identical dark pions and determine the parameters of the effective range expansion.","Our exploratory results in the supposedly most common interaction channel provide a lower limit for the dark matter mass when compared to astrophysical data.","We also provide first benchmarks of velocity-weighted cross-sections in the relevant non-relativistic domain."],"url":"http://arxiv.org/abs/2405.06506v1","category":"hep-lat"}
{"created":"2024-05-10 14:33:36","title":"Rotational transitions induced by collisions of HD$^{+}$ ions with low energy electrons","abstract":"A series of Multichannel Quantum Defect Theory-based computations have been performed, in order to produce the cross sections of rotational transitions (excitations $N_{i}^{+}-2 \\rightarrow$ $N_{i}^{+}$, de-excitations $N_{i}^{+}$ $\\rightarrow$ $N_{i}^{+}-2$, with $N_{i}^{+}=2$ to $10$) and of their competitive process, the dissociative recombination, induced by collisions of HD$^+$ ions with electrons in the energy range $10^{-5}$ to 0.3 eV. Maxwell anisotropic rate coefficients, obtained from these cross sections in the conditions of the Heidelberg Test Storage Ring (TSR) experiments ($k_{B}T_{t}=2.8$ meV and $k_{B}T_{l}=45$ $\\mu$eV), have been reported for those processes in the same electronic energy range. Maxwell isotropic rate coefficients have been as well presented for electronic temperatures up to a few hundreds of Kelvins. Very good overall agreement is found between our results for rotational transitions and the former theoretical computations as well as with experiment. Furthermore, owing to the full rotational computations performed, the accuracy of the resulting dissociative recombination cross sections is considerably improved.","sentences":["A series of Multichannel Quantum Defect Theory-based computations have been performed, in order to produce the cross sections of rotational transitions (excitations $N_{i}^{+}-2 \\rightarrow$ $N_{i}^{+}$, de-excitations $N_{i}^{+}$ $\\rightarrow$ $N_{i}^{+}-2$, with $N_{i}^{+}=2$ to $10$) and of their competitive process, the dissociative recombination, induced by collisions of HD$^+$ ions with electrons in the energy range $10^{-5}$ to 0.3 eV. Maxwell anisotropic rate coefficients, obtained from these cross sections in the conditions of the Heidelberg Test Storage Ring (TSR) experiments ($k_{B}T_{t}=2.8$ meV and $k_{B}T_{l}=45$ $\\mu$eV), have been reported for those processes in the same electronic energy range.","Maxwell isotropic rate coefficients have been as well presented for electronic temperatures up to a few hundreds of Kelvins.","Very good overall agreement is found between our results for rotational transitions and the former theoretical computations as well as with experiment.","Furthermore, owing to the full rotational computations performed, the accuracy of the resulting dissociative recombination cross sections is considerably improved."],"url":"http://arxiv.org/abs/2405.06504v1","category":"astro-ph.IM"}
{"created":"2024-05-10 14:20:45","title":"Modular Invariant Slow Roll Inflation","abstract":"We propose new classes of inflation models based on the modular symmetry, where the modulus field $\\tau$ serves as the inflaton. We establish a connection between modular inflation and modular stabilization, wherein the modulus field rolls towards a fixed point along the boundary of the fundamental domain. We find the modular symmetry strongly constrain the possible shape of the potential and identify some parameter space where the inflation predictions agree with cosmic microwave background observations. The tensor-to-scalar ratio is predicted to be smaller than $10^{-6}$ in our models, while the running of spectral index is of the the order of $10^{-4}$.","sentences":["We propose new classes of inflation models based on the modular symmetry, where the modulus field $\\tau$ serves as the inflaton.","We establish a connection between modular inflation and modular stabilization, wherein the modulus field rolls towards a fixed point along the boundary of the fundamental domain.","We find the modular symmetry strongly constrain the possible shape of the potential and identify some parameter space where the inflation predictions agree with cosmic microwave background observations.","The tensor-to-scalar ratio is predicted to be smaller than $10^{-6}$ in our models, while the running of spectral index is of the the order of $10^{-4}$."],"url":"http://arxiv.org/abs/2405.06497v1","category":"hep-ph"}
{"created":"2024-05-10 14:16:45","title":"Unimolecular processes in diatomic carbon anions at high rotational excitation","abstract":"On the millisecond to second time scale, stored beams of diatomic carbon anions C$_2{}^-$ from a sputter ion source feature unimolecular decay of yet unexplained origin by electron emission and fragmentation. To account for the magnitude and time dependence of the experimental rates, levels with high rotational and vibrational excitation are modeled for the lowest electronic states of C$_2{}^-$, also including the lowest quartet potential. Energies, spontaneous radiative decay rates (including spin-forbidden quartet-level decay), and tunneling dissociation rates are determined for a large number of highly excited C$_2{}^-$ levels and their population in sputter-type ion sources is considered. For the quartet levels, the stability against autodetachment is addressed and recently calculated rates of rotationally assisted autodetachment are applied. Non-adiabatic vibrational autodetachment rates of high vibrational levels in the doublet C$_2{}^-$ ground potential are also calculated. The results are combined to model the experimental unimolecular decay signals. Comparison of the modeled to the experimental rates measured at the Croygenic Storage Ring (CSR) gives strong evidence that C$_2{}^-$ ions in quasi-stable levels of the quartet electronic states are the so far unidentified source of unimolecular decay.","sentences":["On the millisecond to second time scale, stored beams of diatomic carbon anions C$_2{}^-$ from a sputter ion source feature unimolecular decay of yet unexplained origin by electron emission and fragmentation.","To account for the magnitude and time dependence of the experimental rates, levels with high rotational and vibrational excitation are modeled for the lowest electronic states of C$_2{}^-$, also including the lowest quartet potential.","Energies, spontaneous radiative decay rates (including spin-forbidden quartet-level decay), and tunneling dissociation rates are determined for a large number of highly excited C$_2{}^-$ levels and their population in sputter-type ion sources is considered.","For the quartet levels, the stability against autodetachment is addressed and recently calculated rates of rotationally assisted autodetachment are applied.","Non-adiabatic vibrational autodetachment rates of high vibrational levels in the doublet C$_2{}^-$ ground potential are also calculated.","The results are combined to model the experimental unimolecular decay signals.","Comparison of the modeled to the experimental rates measured at the Croygenic Storage Ring (CSR) gives strong evidence that C$_2{}^-$ ions in quasi-stable levels of the quartet electronic states are the so far unidentified source of unimolecular decay."],"url":"http://arxiv.org/abs/2405.06493v1","category":"physics.chem-ph"}
{"created":"2024-05-10 13:35:46","title":"NO2 adsorption on GaN surface and its interaction with the yellow-luminescence-associated surface state","abstract":"Trapping of charge at surface states is a longstanding problem in GaN that hinders a full realization of its potential as a semiconductor for microelectronics. At least part of this charge originates in molecules adsorbed on the GaN surface. Multiple studies have addressed the adsorption of different substances, but the role of adsorbents in the charge-trapping mechanism remains unclear. Here, we show that the GaN surface selectively adsorbs nitrogen dioxide (NO2) existing in the air in trace amounts. NO2 appears to charge the yellow-luminescence-related surface state. Mild heat treatment in vacuum removes this surface charge, only to be re-absorbed on re-exposure to air. Selective exposure of vacuum-annealed GaN to NO2 reproduces a similar surface charge distribution, as does the exposure to air. Residual gas analysis of the gases desorbed during heat treatment in vacuum shows a large concentration of nitric oxide (NO) released from the surface. These observations suggest that NO2 is selectively adsorbed from the air, deleteriously affecting the electrical properties of air-exposed GaN. The trapping of free electrons as part of the NO2chemisorption process changes the surface charge density, resulting in a change in the surface band bending. Uncontrollable by nature, NO2 adsorption may significantly affect any GaN-based electronic device. However, as shown here, a rather mild heat treatment in vacuum restores the surface state occupancy of GaN to its intrinsic state. If attempted before passivation, this heat treatment may provide a possible solution to longstanding stability problems associated with surface charge trapping in GaN-based devices.","sentences":["Trapping of charge at surface states is a longstanding problem in GaN that hinders a full realization of its potential as a semiconductor for microelectronics.","At least part of this charge originates in molecules adsorbed on the GaN surface.","Multiple studies have addressed the adsorption of different substances, but the role of adsorbents in the charge-trapping mechanism remains unclear.","Here, we show that the GaN surface selectively adsorbs nitrogen dioxide (NO2) existing in the air in trace amounts.","NO2 appears to charge the yellow-luminescence-related surface state.","Mild heat treatment in vacuum removes this surface charge, only to be re-absorbed on re-exposure to air.","Selective exposure of vacuum-annealed GaN to NO2 reproduces a similar surface charge distribution, as does the exposure to air.","Residual gas analysis of the gases desorbed during heat treatment in vacuum shows a large concentration of nitric oxide (NO) released from the surface.","These observations suggest that NO2 is selectively adsorbed from the air, deleteriously affecting the electrical properties of air-exposed GaN.","The trapping of free electrons as part of the NO2chemisorption process changes the surface charge density, resulting in a change in the surface band bending.","Uncontrollable by nature, NO2 adsorption may significantly affect any GaN-based electronic device.","However, as shown here, a rather mild heat treatment in vacuum restores the surface state occupancy of GaN to its intrinsic state.","If attempted before passivation, this heat treatment may provide a possible solution to longstanding stability problems associated with surface charge trapping in GaN-based devices."],"url":"http://arxiv.org/abs/2405.06471v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 13:07:30","title":"Characterization of mood and emotion regulation in females with PMS/PMDD using near-infrared spectroscopy to assess prefrontal cerebral blood flow and the mood questionnaire","abstract":"Many sexually mature women experience premenstrual syndrome (PMS) or premenstrual dysphoric mood disorder (PMDD). Current approaches for managing PMS and PMDD rely on daily mental condition recording, which many discontinue due to its impracticality. Hence, there's a critical need for a simple, objective method to monitor mental symptoms. One of the principal symptoms of PMDD is a dysfunction in emotional regulation, which has been demonstrated through brain-function imaging measurements to involve hyperactivity in the amygdala and a decrease in functionality in the prefrontal cortex (PFC). However, most research has been focused on PMDD, leaving a gap in understanding of PMS. Near-infrared spectroscopy (NIRS) measures brain activity by spectroscopically determining the amount of hemoglobin in the blood vessels. This study aimed to characterize the emotional regulation function in PMS. We measured brain activity in the PFC region using NIRS when participants were presented with emotion-inducing pictures. Furthermore, moods highly associated with emotions were assessed through questionnaires. Forty-six participants were categorized into non-PMS, PMS, and PMDD groups based on the gynecologist's diagnosis. POMS2 scores revealed higher negative mood and lower positive mood in the follicular phase for the PMS group, while the PMDD group exhibited heightened negative mood during the luteal phase. NIRS results showed reduced emotional expression in the PMS group during both phases, while no significant differences were observed in the PMDD group compared to non-PMS. It was found that there are differences in the distribution of mood during the luteal and follicular phase and in cerebral blood flow responses to emotional stimuli between PMS and PMDD. These findings suggest the potential for providing individuals with awareness of PMS or PMDD through scores on the POMS2 and NIRS measurements.","sentences":["Many sexually mature women experience premenstrual syndrome (PMS) or premenstrual dysphoric mood disorder (PMDD).","Current approaches for managing PMS and PMDD rely on daily mental condition recording, which many discontinue due to its impracticality.","Hence, there's a critical need for a simple, objective method to monitor mental symptoms.","One of the principal symptoms of PMDD is a dysfunction in emotional regulation, which has been demonstrated through brain-function imaging measurements to involve hyperactivity in the amygdala and a decrease in functionality in the prefrontal cortex (PFC).","However, most research has been focused on PMDD, leaving a gap in understanding of PMS.","Near-infrared spectroscopy (NIRS) measures brain activity by spectroscopically determining the amount of hemoglobin in the blood vessels.","This study aimed to characterize the emotional regulation function in PMS.","We measured brain activity in the PFC region using NIRS when participants were presented with emotion-inducing pictures.","Furthermore, moods highly associated with emotions were assessed through questionnaires.","Forty-six participants were categorized into non-PMS, PMS, and PMDD groups based on the gynecologist's diagnosis.","POMS2 scores revealed higher negative mood and lower positive mood in the follicular phase for the PMS group, while the PMDD group exhibited heightened negative mood during the luteal phase.","NIRS results showed reduced emotional expression in the PMS group during both phases, while no significant differences were observed in the PMDD group compared to non-PMS.","It was found that there are differences in the distribution of mood during the luteal and follicular phase and in cerebral blood flow responses to emotional stimuli between PMS and PMDD.","These findings suggest the potential for providing individuals with awareness of PMS or PMDD through scores on the POMS2 and NIRS measurements."],"url":"http://arxiv.org/abs/2405.06457v1","category":"q-bio.OT"}
{"created":"2024-05-10 13:02:26","title":"A Review of Matrix Algebra for Power and Energy Applications","abstract":"This report presents a brief review of matrix algebra and its implementation in Julia for power and energy applications. First, we present basic examples of data visualization, followed by conventional operations with matrices and vectors. Then, we study quadratic forms and norms, two main concepts required in the convergence study of the power flow in power and energy applications. After that, we give good practices to create a neat code in Julia. There is an extensive set of examples available on the Internet related to these basic aspects, so we avoid repeating what is well documented. Hence, we show only basic examples to create our first scripts.","sentences":["This report presents a brief review of matrix algebra and its implementation in Julia for power and energy applications.","First, we present basic examples of data visualization, followed by conventional operations with matrices and vectors.","Then, we study quadratic forms and norms, two main concepts required in the convergence study of the power flow in power and energy applications.","After that, we give good practices to create a neat code in Julia.","There is an extensive set of examples available on the Internet related to these basic aspects, so we avoid repeating what is well documented.","Hence, we show only basic examples to create our first scripts."],"url":"http://arxiv.org/abs/2405.06452v1","category":"math.NA"}
{"created":"2024-05-10 12:50:30","title":"Soft quark effects on H+jet production at NLP accuracy","abstract":"We define soft quark operators that enable construction of colour-ordered helicity amplitudes out of the non-radiative ones. We explore these operators for Higgs plus one jet production in a hadron collider and study the next-to-leading power corrections on all partonic channels involving quark(s) and/or anti-quark(s). We also investigate the effect of next-to-soft gluon radiation in these channels employing the technique illustrated in \\cite{Pal:2023vec}. Analytical expressions of next-to-leading power leading logarithms thus obtained are simple and compact in nature. Further, we discuss the connection of such logarithms with that of the pseudo-scalar Higgs plus one jet production.","sentences":["We define soft quark operators that enable construction of colour-ordered helicity amplitudes out of the non-radiative ones.","We explore these operators for Higgs plus one jet production in a hadron collider and study the next-to-leading power corrections on all partonic channels involving quark(s) and/or anti-quark(s).","We also investigate the effect of next-to-soft gluon radiation in these channels employing the technique illustrated in \\cite{Pal:2023vec}.","Analytical expressions of next-to-leading power leading logarithms thus obtained are simple and compact in nature.","Further, we discuss the connection of such logarithms with that of the pseudo-scalar Higgs plus one jet production."],"url":"http://arxiv.org/abs/2405.06444v1","category":"hep-ph"}
{"created":"2024-05-10 12:17:04","title":"Dissociative recombination, and vibrational excitation of CO$^{+}$: model calculations and comparison with experiment","abstract":"The latest molecular data - potential energy curves and Rydberg$/$valence interactions - characterizing the super-excited electronic states of CO are reviewed, in order to provide inputs for the study of their fragmentation dynamics. Starting from this input, the main paths and mechanisms for CO$^+$ dissociative recombination are analyzed; its cross sections are computed using a method based on Multichannel Quantum Defect Theory. Convoluted cross sections, giving both isotropic and anisotropic Maxwellian rate-coefficients, are compared with merged-beam and storage-ring experimental results. The calculated cross sections underestimate the measured ones by a factor of $2$, but display a very similar resonant shape. These facts confirm the quality of our approach for the dynamics, and call for more accurate and more extensive molecular structure calculations.","sentences":["The latest molecular data - potential energy curves and Rydberg$/$valence interactions - characterizing the super-excited electronic states of CO are reviewed, in order to provide inputs for the study of their fragmentation dynamics.","Starting from this input, the main paths and mechanisms for CO$^+$ dissociative recombination are analyzed; its cross sections are computed using a method based on Multichannel Quantum Defect Theory.","Convoluted cross sections, giving both isotropic and anisotropic Maxwellian rate-coefficients, are compared with merged-beam and storage-ring experimental results.","The calculated cross sections underestimate the measured ones by a factor of $2$, but display a very similar resonant shape.","These facts confirm the quality of our approach for the dynamics, and call for more accurate and more extensive molecular structure calculations."],"url":"http://arxiv.org/abs/2405.06427v1","category":"physics.atom-ph"}
{"created":"2024-05-10 12:12:31","title":"QSOFEED: The relationship between star formation and AGN Feedback","abstract":"Large-scale cosmological simulations suggest that feedback from active galactic nuclei (AGN) plays a crucial role in galaxy evolution. In this study, we directly test this hypothesis utilising SDSS spectra of a sample of 48 low redshift (z<0.14) type 2 quasars (QSO2s). We characterised the kinematics of the warm ionised gas by performing a non-parametric analysis of the [OIII]$\\lambda 5007$ emission line, as well as constrain the properties of the young stellar populations (YSP) of their host galaxies through spectral synthesis modelling. These analyses revealed that 85% of the QSO2s display gas velocity dispersions larger than that of the stellar component of their host galaxies, indicating the presence of AGN-driven outflows. Comparing the gas kinematics to the AGN properties, we found a positive correlation between gas velocity dispersion and 1.4 GHz radio luminosity but not with AGN bolometric luminosity or Eddington ratio, suggesting that, either the radio luminosity is the key factor in driving outflows or that the outflows themselves are shocking the ISM and producing synchrotron emission. We found that 98% of the sample host YSPs to varying degrees, with star formation rates (SFR) $0 \\le SFR \\le 92 \\mbox{ M}_{\\odot} \\mbox{yr}^{-1}$, averaged over 100 Myr. We compared the gas kinematics and outflow properties to the SFRs to establish possible correlations which may suggest that the presence of the outflowing gas is impacting SF and find that none exists, leading to the conclusion that, on the scales probed by the SDSS fibre, AGN-driven outflows do not impact SF on the timescales probed in this study. However, we found a positive correlation between the light-weighted stellar ages of the QSO2s and the black hole mass, which may indicate that successive AGN episodes lead to the suppression of SF over the course of galaxy evolution.","sentences":["Large-scale cosmological simulations suggest that feedback from active galactic nuclei (AGN) plays a crucial role in galaxy evolution.","In this study, we directly test this hypothesis utilising SDSS spectra of a sample of 48 low redshift (z<0.14) type 2 quasars (QSO2s).","We characterised the kinematics of the warm ionised gas by performing a non-parametric analysis of the [OIII]$\\lambda 5007$ emission line, as well as constrain the properties of the young stellar populations (YSP) of their host galaxies through spectral synthesis modelling.","These analyses revealed that 85% of the QSO2s display gas velocity dispersions larger than that of the stellar component of their host galaxies, indicating the presence of AGN-driven outflows.","Comparing the gas kinematics to the AGN properties, we found a positive correlation between gas velocity dispersion and 1.4 GHz radio luminosity but not with AGN bolometric luminosity or Eddington ratio, suggesting that, either the radio luminosity is the key factor in driving outflows or that the outflows themselves are shocking the ISM and producing synchrotron emission.","We found that 98% of the sample host YSPs to varying degrees, with star formation rates (SFR) $0 \\le SFR \\le 92 \\mbox{ M}_{\\odot} \\mbox{yr}^{-1}$, averaged over 100 Myr.","We compared the gas kinematics and outflow properties to the SFRs to establish possible correlations which may suggest that the presence of the outflowing gas is impacting SF and find that none exists, leading to the conclusion that, on the scales probed by the SDSS fibre, AGN-driven outflows do not impact SF on the timescales probed in this study.","However, we found a positive correlation between the light-weighted stellar ages of the QSO2s and the black hole mass, which may indicate that successive AGN episodes lead to the suppression of SF over the course of galaxy evolution."],"url":"http://arxiv.org/abs/2405.06421v1","category":"astro-ph.GA"}
{"created":"2024-05-10 11:27:52","title":"Understanding two-dimensional tractor magnets: theory and realizations","abstract":"We present a comparative investigation of two-dimensional tractor magnet configurations, analyzing both theoretical predictions and experimental results with a focus on the minimal tractor magnet. The minimal tractor magnet consists of a rigid assembly of one attracting magnet (attractor), two repelling magnets (repulsors), and a fourth magnet (follower) that is magnetically stabilized in a local energy minimum. The theoretical framework relies on magnetostatics and stability analysis of stationary equilibria. To calculate the magnetostatic force and energy, we use a multipole method. In a first approximation, we derive analytical results from the point dipole approximation. The point dipole analysis defines an upper bound criterion for the magnetic moment ratio and provides analytical expressions for stability bounds in relation to geometry parameters. Our experimental results are consistent with the predictions from the fourth-order multipole expansion. Beyond the minimal tractor magnet, we introduce a more advanced configuration that allows for a higher magnetic binding energy and follower capture at larger distances.","sentences":["We present a comparative investigation of two-dimensional tractor magnet configurations, analyzing both theoretical predictions and experimental results with a focus on the minimal tractor magnet.","The minimal tractor magnet consists of a rigid assembly of one attracting magnet (attractor), two repelling magnets (repulsors), and a fourth magnet (follower) that is magnetically stabilized in a local energy minimum.","The theoretical framework relies on magnetostatics and stability analysis of stationary equilibria.","To calculate the magnetostatic force and energy, we use a multipole method.","In a first approximation, we derive analytical results from the point dipole approximation.","The point dipole analysis defines an upper bound criterion for the magnetic moment ratio and provides analytical expressions for stability bounds in relation to geometry parameters.","Our experimental results are consistent with the predictions from the fourth-order multipole expansion.","Beyond the minimal tractor magnet, we introduce a more advanced configuration that allows for a higher magnetic binding energy and follower capture at larger distances."],"url":"http://arxiv.org/abs/2405.06402v1","category":"physics.app-ph"}
{"created":"2024-05-10 10:44:16","title":"Probing superheavy dark matter through lunar radio observations of ultrahigh-energy neutrinos","abstract":"Ultrahigh-energy neutrinos (UHE$\\nu$s) can be used as a valuable probe of superheavy dark matter above $\\sim 10^9$ GeV, the latter being difficult to probe with collider and direct detection experiments due to the feebly interacting nature. Searching for radio emissions originating from the interaction of UHE$\\nu$s with the lunar regolith enables us to explore energies beyond $10^{12}$ GeV, which astrophysical accelerators cannot achieve. Taking into account the interaction of UHE$\\nu$s with the cosmic neutrino background and resulting standard neutrino cascades to calculate the neutrino flux on Earth, we investigate sensitivities of such lunar radio observations to very heavy dark matter. We show that the proposed ultra-long wavelength lunar radio telescope, as well as the existing low-frequency array, can provide the most stringent constraints on decaying or annihilating superheavy dark matter with masses at $\\gtrsim 10^{12}$ GeV. The limits are complementary to or even stronger than those from other UHE$\\nu$ detectors, such as the IceCube-Gen2 radio array and GRAND.","sentences":["Ultrahigh-energy neutrinos (UHE$\\nu$s) can be used as a valuable probe of superheavy dark matter above $\\sim 10^9$ GeV, the latter being difficult to probe with collider and direct detection experiments due to the feebly interacting nature.","Searching for radio emissions originating from the interaction of UHE$\\nu$s with the lunar regolith enables us to explore energies beyond $10^{12}$ GeV, which astrophysical accelerators cannot achieve.","Taking into account the interaction of UHE$\\nu$s with the cosmic neutrino background and resulting standard neutrino cascades to calculate the neutrino flux on Earth, we investigate sensitivities of such lunar radio observations to very heavy dark matter.","We show that the proposed ultra-long wavelength lunar radio telescope, as well as the existing low-frequency array, can provide the most stringent constraints on decaying or annihilating superheavy dark matter with masses at $\\gtrsim 10^{12}$ GeV.","The limits are complementary to or even stronger than those from other UHE$\\nu$ detectors, such as the IceCube-Gen2 radio array and GRAND."],"url":"http://arxiv.org/abs/2405.06382v1","category":"hep-ph"}
{"created":"2024-05-10 10:11:34","title":"Phase angle dependency of the dust cross section in a cometary coma","abstract":"Rosetta/OSIRIS took optical measurements of the intensity of scattered light from the coma of 67P/Churyumov-Gerasimenko over a wide range of phase angles. These data have been used to measure the phase angle dependent radiance profile of the dust coma.   We want to provide information about the column area densities of the dust coma as seen from Rosetta. This information in combination with the measured OSIRIS phase function can then be used to determine the scattering phase function of the dust particles.   We use a simple numerical model to calculate the dust density in the coma. For this we neglect all forces but solar gravitation and radiation pressure. As this cannot describe particles close to the surface of the comet, we assume starting conditions at a sufficient distance. We evaluate the column area density as observed from Rosetta/OSIRIS and compare the results for different spacecraft positions, dust sizes and surface activity distributions.   We find the phase angle dependence of the column area density to be largely independent of particle size and spacecraft positions. The determining factor is the activity distribution across the surface, especially the activity on the night side. For models with no night side activity, we find the column area density at high phase angles to be roughly two orders of magnitude larger than at low phase angles.   The radiance profile measured from inside a cometary coma results from the combined effects of a phase angle dependent column area density and the scattering phase function. The radiance profile is therefore strongly dependent on the surface activity distribution, and - unless the dust emission is isotropic - any attempt to infer particle properties (as expressed through the scattering phase function) from such data must take into account and de-bias for this spatial variation of the dust column area density.","sentences":["Rosetta/OSIRIS took optical measurements of the intensity of scattered light from the coma of 67P/Churyumov-Gerasimenko over a wide range of phase angles.","These data have been used to measure the phase angle dependent radiance profile of the dust coma.   ","We want to provide information about the column area densities of the dust coma as seen from Rosetta.","This information in combination with the measured OSIRIS phase function can then be used to determine the scattering phase function of the dust particles.   ","We use a simple numerical model to calculate the dust density in the coma.","For this we neglect all forces but solar gravitation and radiation pressure.","As this cannot describe particles close to the surface of the comet, we assume starting conditions at a sufficient distance.","We evaluate the column area density as observed from Rosetta/OSIRIS and compare the results for different spacecraft positions, dust sizes and surface activity distributions.   ","We find the phase angle dependence of the column area density to be largely independent of particle size and spacecraft positions.","The determining factor is the activity distribution across the surface, especially the activity on the night side.","For models with no night side activity, we find the column area density at high phase angles to be roughly two orders of magnitude larger than at low phase angles.   ","The radiance profile measured from inside a cometary coma results from the combined effects of a phase angle dependent column area density and the scattering phase function.","The radiance profile is therefore strongly dependent on the surface activity distribution, and - unless the dust emission is isotropic - any attempt to infer particle properties (as expressed through the scattering phase function) from such data must take into account and de-bias for this spatial variation of the dust column area density."],"url":"http://arxiv.org/abs/2405.06370v1","category":"astro-ph.EP"}
{"created":"2024-05-10 09:58:10","title":"Enhancing atomic ordering, magnetic and transport properties of Mn2VGa Heusler alloy thin films toward negatively spin-polarized charge injection","abstract":"Magnetic materials with negative spin polarization have attracted attention for their potential to increase the design freedom of spintronic devices. This study investigated the effects of off-stoichiometry on the atomic ordering, microstructure, and magneto-transport properties in Mn2+xV1-xGa (x = -0.2, 0, +0.2, +0.4) Heusler alloy films, which are predicted to have large negative spin polarization derived from a pseudo band gap in the majority spin channel. The Mn2+xV1-xGa films epitaxially grown on MgO(001) substrates exhibits variations of B2 and L21 order with the Mn concentration. A high-quality L21 ordered film was achieved in the Mn-rich composition (x = +0.2) with B2 and L21 order parameters of 0.97 and 0.86, respectively, and a saturation magnetization of 1.4 {\\mu}B/f.u, which agrees the Slater-Pauling rule. Scanning transmission electron microscopy observations showed that B2 and L21 phases coexist in Mn-poor and stoichiometric films, while the L21 phase is dominant in the Mn-rich film with small amounts of Mn-V and Mn-Ga disorders, as revealed by laboratory and anomalous X-ray diffraction. Combined first-principles calculations and anisotropic magnetoresistance analysis confirm that the addition of excess Mn preserves the high spin polarization by suppressing the formation of detrimental antisites of V atoms occupying Mn sites. Therefore, the Mn-rich composition is promising for negatively spin-polarized charge injection in Mn2VGa-based spintronic applications.","sentences":["Magnetic materials with negative spin polarization have attracted attention for their potential to increase the design freedom of spintronic devices.","This study investigated the effects of off-stoichiometry on the atomic ordering, microstructure, and magneto-transport properties in Mn2+xV1-xGa (x = -0.2, 0, +0.2, +0.4)","Heusler alloy films, which are predicted to have large negative spin polarization derived from a pseudo band gap in the majority spin channel.","The Mn2+xV1-xGa films epitaxially grown on MgO(001) substrates exhibits variations of B2 and L21 order with the Mn concentration.","A high-quality L21 ordered film was achieved in the Mn-rich composition (x = +0.2) with B2 and L21 order parameters of 0.97 and 0.86, respectively, and a saturation magnetization of 1.4 {\\mu}B/f.u, which agrees the Slater-Pauling rule.","Scanning transmission electron microscopy observations showed that B2 and L21 phases coexist in Mn-poor and stoichiometric films, while the L21 phase is dominant in the Mn-rich film with small amounts of Mn-V and Mn-Ga disorders, as revealed by laboratory and anomalous X-ray diffraction.","Combined first-principles calculations and anisotropic magnetoresistance analysis confirm that the addition of excess Mn preserves the high spin polarization by suppressing the formation of detrimental antisites of V atoms occupying Mn sites.","Therefore, the Mn-rich composition is promising for negatively spin-polarized charge injection in Mn2VGa-based spintronic applications."],"url":"http://arxiv.org/abs/2405.06362v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-10 09:54:31","title":"Anatomy of critical fluctuations in hadronic matter","abstract":"Critical phenomena in phase transitions of strongly interacting matter, governed by quantum chromodynamics, are inherently encoded in the fluctuations of conserved charges. In this work, we study the net-baryon number density fluctuations, including the lowest-lying nucleon and the baryonic resonance $\\Delta(1232)$, based on the parity doublet model in the mean-field approximation. We focus on the qualitative features of the second-order susceptibility of the net-baryon number density in dense hadronic matter and how the inclusion of $\\Delta(1232)$ affects it. We demonstrate that the fluctuations of the individual baryons do not necessarily reflect the total net-baryon number fluctuations at finite density, due to the non-trivial correlations between different particle species. Our results highlight the role of baryonic correlations in the interpretation of data from heavy ion collision experiments.","sentences":["Critical phenomena in phase transitions of strongly interacting matter, governed by quantum chromodynamics, are inherently encoded in the fluctuations of conserved charges.","In this work, we study the net-baryon number density fluctuations, including the lowest-lying nucleon and the baryonic resonance $\\Delta(1232)$, based on the parity doublet model in the mean-field approximation.","We focus on the qualitative features of the second-order susceptibility of the net-baryon number density in dense hadronic matter and how the inclusion of $\\Delta(1232)$ affects it.","We demonstrate that the fluctuations of the individual baryons do not necessarily reflect the total net-baryon number fluctuations at finite density, due to the non-trivial correlations between different particle species.","Our results highlight the role of baryonic correlations in the interpretation of data from heavy ion collision experiments."],"url":"http://arxiv.org/abs/2405.06360v1","category":"hep-ph"}
{"created":"2024-05-10 09:09:32","title":"Bayesian factor zero-inflated Poisson model for multiple grouped count data","abstract":"This paper proposes a computationally efficient Bayesian factor model for multiple grouped count data. Adopting the link function approach, the proposed model can capture the association within and between the at-risk probabilities and Poisson counts over multiple dimensions. The likelihood function for the grouped count data consists of the differences of the cumulative distribution functions evaluated at the endpoints of the groups, defining the probabilities of each data point falling in the groups. The combination of the data augmentation of underlying counts, the P\\'{o}lya-Gamma augmentation to approximate the Poisson distribution, and parameter expansion for the factor components is used to facilitate posterior computing. The efficacy of the proposed factor model is demonstrated using the simulated data and real data on the involvement of youths in the nineteen illegal activities.","sentences":["This paper proposes a computationally efficient Bayesian factor model for multiple grouped count data.","Adopting the link function approach, the proposed model can capture the association within and between the at-risk probabilities and Poisson counts over multiple dimensions.","The likelihood function for the grouped count data consists of the differences of the cumulative distribution functions evaluated at the endpoints of the groups, defining the probabilities of each data point falling in the groups.","The combination of the data augmentation of underlying counts, the P\\'{o}lya-Gamma augmentation to approximate the Poisson distribution, and parameter expansion for the factor components is used to facilitate posterior computing.","The efficacy of the proposed factor model is demonstrated using the simulated data and real data on the involvement of youths in the nineteen illegal activities."],"url":"http://arxiv.org/abs/2405.06335v1","category":"stat.ME"}
{"created":"2024-05-10 08:46:03","title":"Lifting of gap nodes by disorder in ultranodal superconductor candidate FeSe1-xSx","abstract":"The observation of time-reversal symmetry breaking and large residual density of states in tetragonal FeSe$_{1-x}$S$_x$ suggests a novel type of ultranodal superconducting state with Bogoliubov Fermi surfaces (BFSs). Although such BFSs in centrosymmetric superconductors are expected to be topologically protected, the impurity effect of this exotic superconducting state remains elusive experimentally. Here, we investigate the impact of controlled defects introduced by electron irradiation on the superconducting state of tetragonal FeSe$_{1-x}$S$_x$ ($0.18\\le x\\le 0.25$). The temperature dependence of magnetic penetration depth is initially consistent with a model with BFSs in the pristine sample. After irradiation, we observe a nonmonotonic evolution of low-energy excitations with impurity concentrations. This nonmonotonic change indicates a transition from nodal to nodeless, culminating in gapless with Andreev bound states, reminiscent of the nodal $s_\\pm$ case. This points to the accidental nature of the possible BFSs in tetragonal FeSe$_{1-x}$S$_x$, which are susceptible to disruption by the disorder.","sentences":["The observation of time-reversal symmetry breaking and large residual density of states in tetragonal FeSe$_{1-x}$S$_x$ suggests a novel type of ultranodal superconducting state with Bogoliubov Fermi surfaces (BFSs).","Although such BFSs in centrosymmetric superconductors are expected to be topologically protected, the impurity effect of this exotic superconducting state remains elusive experimentally.","Here, we investigate the impact of controlled defects introduced by electron irradiation on the superconducting state of tetragonal FeSe$_{1-x}$S$_x$ ($0.18\\le x\\le 0.25$).","The temperature dependence of magnetic penetration depth is initially consistent with a model with BFSs in the pristine sample.","After irradiation, we observe a nonmonotonic evolution of low-energy excitations with impurity concentrations.","This nonmonotonic change indicates a transition from nodal to nodeless, culminating in gapless with Andreev bound states, reminiscent of the nodal $s_\\pm$ case.","This points to the accidental nature of the possible BFSs in tetragonal FeSe$_{1-x}$S$_x$, which are susceptible to disruption by the disorder."],"url":"http://arxiv.org/abs/2405.06320v1","category":"cond-mat.supr-con"}
